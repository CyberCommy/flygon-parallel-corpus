- en: Linear Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性模型
- en: The family of linear models represents one of the most useful hypothesis classes.
    Many learning algorithms that are widely applied in algorithmic trading rely on
    linear predictors because they can be efficiently trained in many cases, they
    are relatively robust to noisy financial data, and they have strong links to the
    theory of finance. Linear predictors are also intuitive, easy to interpret, and
    often fit the data reasonably well or at least provide a good baseline.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型家族代表了最有用的假设类之一。许多广泛应用于算法交易的学习算法依赖于线性预测器，因为它们在许多情况下可以被有效地训练，相对而言，它们对嘈杂的金融数据相对稳健，并且它们与金融理论有着紧密的联系。线性预测器也直观，易于解释，并且通常能够很好地拟合数据，或者至少提供一个良好的基线。
- en: 'Linear regression has been known for over 200 years when Legendre and Gauss
    applied it to astronomy and began to analyze its statistical properties. Numerous
    extensions have since adapted the linear regression model and the baseline **ordinary
    least squares** (**OLS**) method to learn its parameters:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归已经被人们所熟知超过200年，当Legendre和Gauss将其应用于天文学并开始分析其统计特性。此后，许多扩展已经调整了线性回归模型和基线**普通最小二乘法**（**OLS**）方法来学习其参数：
- en: '**Generalized linear models** (**GLM**) expand the scope of applications by
    allowing for response variables that imply an error distribution other than the
    normal distribution. GLM include the probit or logistic models for **categorical
    response variables** that appear in classification problems.'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广义线性模型**（**GLM**）通过允许响应变量的应用扩展到除正态分布以外的误差分布。GLM包括用于分类问题中出现的**分类响应变量**的概率或逻辑模型。'
- en: More **robust estimation methods** enable statistical inference where the data
    violates baseline assumptions due to, for example, correlation over time or across
    observations. This is often the case with panel data that contains repeated observations
    on the same units such as historical returns on a universe of assets.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多**鲁棒估计方法**使统计推断能够适应数据违反基线假设的情况，例如，随时间或观察之间的相关性。这在包含对同一单位的重复观察的面板数据中经常发生，例如，资产组合的历史回报。
- en: '**Shrinkage methods** aim to improve the predictive performance of linear models.
    They use a complexity penalty that biases the coefficients learned by the model
    with the goal of reducing the model''s variance and improving out-of-sample predictive
    performance.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收缩方法**旨在改善线性模型的预测性能。它们使用复杂性惩罚来偏置模型学习的系数，以减少模型的方差并提高样本外的预测性能。'
- en: In practice, linear models are applied to regression and classification problems
    with the goals of inference and prediction. Numerous asset pricing models that
    have been developed by academic and industry researchers leverage linear regression.
    Applications include the identification of significant factors that drive asset
    returns, for example, as a basis for risk management, as well as the prediction
    of returns over various time horizons. Classification problems, on the other hand,
    include directional price forecasts.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，线性模型被应用于推断和预测的回归和分类问题。许多学术和行业研究人员开发的资产定价模型利用了线性回归。应用包括识别推动资产回报的重要因素，例如作为风险管理的基础，以及在不同时间范围内预测回报。另一方面，分类问题包括方向性价格预测。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: How linear regression works and which assumptions it makes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归的工作原理及其假设
- en: How to train and diagnose linear regression models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何训练和诊断线性回归模型
- en: How to use linear regression to predict future returns
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用线性回归来预测未来回报
- en: How use regularization to improve the predictive performance
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用正则化来提高预测性能
- en: How logistic regression works
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归的工作原理
- en: How to convert a regression into a classification problem
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将回归转化为分类问题
- en: For code examples, additional resources, and references, see the directory for
    this chapter in the online GitHub repository.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有关代码示例，附加资源和参考资料，请参阅在线GitHub存储库中本章的目录。
- en: Linear regression for inference and prediction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推断和预测的线性回归
- en: As the name suggests, linear regression models assume that the output is the
    result of a linear combination of the inputs. The model also assumes a random
    error that allows for each observation to deviate from the expected linear relationship.
    The reasons that the model does not perfectly describe the relationship between
    inputs and output in a deterministic way include, for example, missing variables,
    measurement, or data collection issues.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，线性回归模型假设输出是输入的线性组合的结果。该模型还假设了一个随机误差，允许每个观察结果偏离预期的线性关系。模型不能以确定性方式完美地描述输入和输出之间的关系的原因包括，例如，缺少变量，测量或数据收集问题。
- en: If we want to draw statistical conclusions about the true (but not observed)
    linear relationship in the population based on the regression parameters estimated
    from the sample, we need to add assumptions about the statistical nature of these
    errors. The baseline regression model makes the strong assumption that the distribution
    of the errors is identical across errors and that errors are independent of each
    other, that is, knowing one error does not help to forecast the next error. The
    assumption of **independent and identically distributed** (**iid**) errors implies
    that their covariance matrix is the identity matrix multiplied by a constant representing
    the error variance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要根据样本估计的回归参数对人口中真实（但未观察到）的线性关系进行统计结论，我们需要对这些误差的统计性质添加假设。基线回归模型做出了一个强烈的假设，即错误的分布在错误之间是相同的，并且错误是彼此独立的，也就是说，知道一个错误并不能帮助预测下一个错误。**独立同分布**（**iid**）误差的假设意味着它们的协方差矩阵是乘以代表误差方差的常数的单位矩阵。
- en: These assumptions guarantee that the OLS method delivers estimates that are
    not only unbiased but also efficient, that is, they have the lowest sampling error
    learning algorithms. However, these assumptions are rarely met in practice. In
    finance, we often encounter panel data with repeated observations on a given cross-section.
    The attempt to estimate the systematic exposure of a universe of assets to a set
    of risk factors over time typically surfaces correlation in the time or cross-sectional
    dimension, or both. Hence, alternative learning algorithms have emerged that assume
    more error covariance matrices that differ from multiples of the identity matrix.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, methods that learn biased parameters for a linear model may
    yield estimates with a lower variance and, hence, improve the predictive performance.
    **Shrinkage methods** reduce the model complexity by applying regularization that
    adds a penalty term to the linear objective function. The penalty is positively
    related to the absolute size of the coefficients so that these are shrunk relative
    to the baseline case. Larger coefficients imply a more complex model that reacts
    more strongly to variations in the inputs. Properly calibrated, the penalty can
    limit the growth of the model's coefficients beyond what an optimal bias-variance
    trade-off would suggest.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: We will introduce the baseline cross-section and panel techniques for linear
    models and important enhancements that produce accurate estimates when key assumptions
    are violated. We will then illustrate these methods by estimating factor models
    that are ubiquitous in the development of algorithmic trading strategies. Lastly,
    we will focus on regularization methods.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The multiple linear regression model
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will introduce the model's specification and objective function, methods
    to learn its parameters, statistical assumptions that allow for inference and
    diagnostics of these assumptions, as well as extensions to adapt the model to
    situations where these assumptions fail.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: How to formulate the model
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The multiple regression model defines a linear functional relationship between
    one continuous outcome variable and *p* input variables that can be of any type
    but may require preprocessing. Multivariate regression, in contrast, refers to
    the regression of multiple outputs on multiple input variables.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'In the population, the linear regression model has the following form for a
    single instance of the output *y*, an input vector ![](img/ca17efd5-4e85-4e63-a2b2-59e7e7afc866.png), and
    the error *ε*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c3c0ce60-a6be-4b9d-96c4-e0f29963c384.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: 'The interpretation of the coefficients is straightforward: the value of a coefficient ![](img/f2a0ef61-33a5-44a1-9f1a-fb132389afa3.png) is the
    partial, average effect of the variable *x[i ]*on the output, holding all other
    variables constant.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'The model can also be written more compactly in matrix form. In this case,
    *y* is a vector of *N* output observations, *X*is the design matrix with *N* rows
    of observations on the *p* variables plus a column of 1s for the intercept, and ![](img/681890d8-fe44-4a8b-8bc4-33b2159a51c5.png) is
    the vector containing the *P = p+1* coefficients ![](img/96e3b5c2-497d-449d-9c8e-bf3c4a038d2a.png):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18eef579-bc58-4079-a75f-b75ccb553879.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: The model is linear in its *p +1* parameters but can model non-linear relationships
    by choosing or transforming variables accordingly, for example by including a
    polynomial basis expansion or logarithmic terms. It can also use categorical variables
    with dummy encoding, and interactions between variables by creating new inputs
    of the form *x[i] . x[j]*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: To complete the formulation of the model from a statistical point of view so
    that we can test a hypothesis about the parameters, we need to make specific assumptions
    about the error term. We'll do this after first introducing the alternative methods
    to learn the parameters.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: How to train the model
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several methods to learn the model parameters ![](img/cabc1f80-1d1b-4793-8a1f-80370fd84fea.png) from
    the data: **ordinary least squares** (**OLS**), **maximum likelihood estimation**
    (**MLE**), and **stochastic gradient descent** (**SGD**).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Least squares
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The least squares method is the original method to learn the parameters of the
    hyperplane that best approximates the output from the input data. As the name
    suggests, the best approximation minimizes the sum of the squared distances between
    the output value and the hyperplane represented by the model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference between the model''s prediction and the actual outcome for a
    given data point is the residual (whereas the deviation of the true model from
    the true output in the population is called **error**). Hence, in formal terms,
    the least squares estimation method chooses the coefficient vector ![](img/b3139f6c-4371-4a74-9335-bbe76eeeba46.png) to
    minimize the **residual** **sum of squares** (**RSS**):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8123f8ed-b798-41e9-be38-a8c004d4d0ef.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: 'Hence, the least-squares coefficients ![](img/087b73ba-7e6b-4d5b-b7c6-fd90f2cf6d8d.png) are
    computed as:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8480afe1-ae4b-4e12-91db-e20143a84b40.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: 'The optimal parameter vector that minimizes RSS results from setting the derivatives
    of the preceding expression with respect to ![](img/90907b87-43c2-4724-ac11-b62f61852e2a.png) to zero.
    This produces a unique solution, assuming X has full column rank, that is, the
    input variables are not linearly dependent, as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53ae7775-f160-40be-b1ec-4a80c1fba44e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'When *y* and *X* have been de-meaned by subtracting their respective means, ![](img/90907b87-43c2-4724-ac11-b62f61852e2a.png) represents
    the ratio of the covariance between the inputs and the outputs ![](img/af4e8def-395c-49b0-a82e-25b2593dd732.png) and the
    output variance ![](img/181b1459-5eef-4d5b-b978-c00e00be4c71.png). There is also
    a geometric interpretation: the coefficients that minimize RSS ensure that the
    vector of residuals ![](img/7ff3850e-4378-4c76-b59a-72eb4c5aa04d.png) is orthogonal
    to the subspace of ![](img/540ecc41-c8d6-462b-8676-971ecc683b25.png)spanned by
    the columns of *X*, and the estimates ![](img/80dcda91-dc61-4571-8345-9f646a4e9b5a.png) are orthogonal
    projections into that subspace.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood estimation
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLE is an important general method to estimate the parameters of a statistical
    model. It relies on the likelihood function that computes how likely it is to
    observe the sample of output values for a given set of both input data as a function
    of the model parameters. The likelihood differs from probabilities in that it
    is not normalized to range from 0 to 1.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'We can set up the likelihood function for the linear regression example by
    assuming a distribution for the error term, such as the standard normal distribution:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60eff427-da2c-43a5-8d3e-7f13c71a89a9.png).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'This allows us to compute the conditional probability of observing a given
    output ![](img/ca374814-159c-47ee-beda-967393c46f27.png) given the corresponding
    input vector *x[i]* and the parameters, ![](img/3fb23a9c-71d9-4391-9ace-33350e36821b.png):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/219455c0-b5e2-43a9-9310-4472d95f6164.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: 'Assuming the output values are conditionally independent given the inputs,
    the likelihood of the sample is proportional to the product of the conditional
    probabilities of the individual output data points. Since it is easier to work
    with sums than with products, we apply the logarithm to obtain the log-likelihood
    function:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/197fca5b-2456-41f0-a988-fc5b646fa4a2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: 'The goal of MLE is to maximize the probability of the output sample that has
    in fact been observed by choosing model parameters, taking the observed inputs
    as given. Hence, the MLE parameter estimate results from maximizing the (log)
    likelihood function:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2814000-7a34-48a2-b790-2dd022c3944d.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Due to the assumption of normal distribution, maximizing the log-likelihood
    function produces the same parameter solution as least squares because the only
    expression that depends on the parameters is squared residual in the exponent.
    For other distributional assumptions and models, MLE will produce different results,
    and in many cases, least squares is not applicable, as we will see later for logistic
    regression.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于正态分布的假设，最大化对数似然函数产生与最小二乘相同的参数解，因为唯一依赖于参数的表达式是指数中的平方残差。对于其他分布假设和模型，MLE将产生不同的结果，在许多情况下，最小二乘不适用，正如我们将在后面看到的逻辑回归。
- en: Gradient descent
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Gradient descent is a general-purpose optimization algorithm that will find
    stationary points of smooth functions. The solution will be a global optimum if
    the objective function is convex. Variations of gradient descent are widely used
    in the training of complex neural networks, but also to compute solutions for
    MLE problems.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种通用的优化算法，可以找到平滑函数的稳定点。如果目标函数是凸的，解将是全局最优的。梯度下降的变体广泛用于复杂神经网络的训练，也用于计算MLE问题的解决方案。
- en: The algorithm uses the gradient of the objective function that contains its
    partial derivatives with respect to the parameters. These derivatives indicate
    how much the objective changes for infinitesimal steps in the direction of the
    corresponding parameters. It turns out that the maximal change of the function
    value results from a step in the direction of the gradient itself.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法使用包含相对于参数的偏导数的目标函数的梯度。这些导数指示了在相应参数的方向上无穷小步骤时目标的变化量。结果表明，函数值的最大变化来自于梯度本身的方向的步骤。
- en: Hence, when minimizing a function that describes, for example, the cost of a
    prediction error, the algorithm computes the gradient for the current parameter
    values using the training data and modifies each parameter according to the negative
    value of its corresponding gradient component. As a result, the objective function
    will assume a lower value and move the parameters move closer to the solution. The
    optimization stops when the gradient becomes small, and the parameter values change
    very little.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当最小化描述例如预测误差成本的函数时，该算法使用训练数据计算当前参数值的梯度，并根据其相应梯度分量的负值修改每个参数。结果，目标函数将假定一个较低的值，并将参数移动到解决方案附近。当梯度变得很小时，参数值几乎不变时，优化停止。
- en: The size of these steps is the learning rate, which is a critical parameter
    that may require tuning; many implementations include the option for this learning
    rate to increase with the number of iterations gradually. Depending on the size
    of the data, the algorithm may iterate many times over the entire dataset. Each
    such iteration is called an **epoch.** The number of epochs and the tolerance
    used to stop further iterations are hyperparameters you can tune.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤的大小是学习率，这是一个可能需要调整的关键参数；许多实现包括逐渐增加学习率的选项。根据数据的大小，算法可能会在整个数据集上进行多次迭代。每次迭代称为一个**时代**。您可以调整的超参数包括时代的数量和用于停止进一步迭代的容差。
- en: Stochastic gradient descent randomly selects a data point and computes the gradient
    for this data point as opposed to an average over a larger sample to achieve a
    speedup. There are also batch versions that use a certain number of data points
    for each step.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降随机选择一个数据点，并计算该数据点的梯度，而不是对更大样本的平均值进行加速。还有使用一定数量的数据点进行每一步的批处理版本。
- en: The Gauss—Markov theorem
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯-马尔可夫定理
- en: To assess the statistical of the model and conduct inference, we need to make
    assumptions about the residuals, that is, the properties of the unexplained part
    of the input. The **Gauss—Markov theorem** (**GMT**) defines the assumptions required
    for OLS to produce unbiased estimates of the model parameters ![](img/33eb8f33-f98e-4c29-82c8-76db82a7b46a.png), and
    when these estimates have the lowest standard error among all linear models for
    cross-sectional data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型的统计性并进行推断，我们需要对残差进行假设，即输入未解释部分的属性。**高斯-马尔可夫定理**（**GMT**）定义了OLS产生模型参数无偏估计所需的假设![](img/33eb8f33-f98e-4c29-82c8-76db82a7b46a.png)，并且当这些估计在横截面数据的所有线性模型中具有最低的标准误差时。
- en: 'The baseline multiple regression model makes the following GMT assumptions:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 基线多元回归模型做出以下GMT假设：
- en: In the population, **linearity** holds, ![](img/8feae49c-7c0f-492c-8581-fa3fb1b0d30a.png) where ![](img/ebb48f07-023f-4a4f-b46d-a0cf606d8ceb.png) are
    unknown but constant and ![](img/c6cfab11-33d7-4c05-81c8-fded5bb113da.png) is
    a random error
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在总体中，**线性性**成立，![](img/8feae49c-7c0f-492c-8581-fa3fb1b0d30a.png)，其中![](img/ebb48f07-023f-4a4f-b46d-a0cf606d8ceb.png)是未知但恒定的，![](img/c6cfab11-33d7-4c05-81c8-fded5bb113da.png)是随机误差
- en: The data for the input variables ![](img/ac422091-f903-413e-ab66-b8f884a4b0d8.png) are
    a **random sample** from the population
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入变量![](img/ac422091-f903-413e-ab66-b8f884a4b0d8.png)的数据是来自总体的**随机样本**
- en: No perfect **collinearity**—there are no exact linear relationships among the
    input variables
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有完美的**共线性**——输入变量之间没有精确的线性关系
- en: The **error has a conditional mean of zero** given any of the inputs: ![](img/fc56cd9c-b8ea-483b-b6c6-b6a01481a1df.png)
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误在任何输入条件下的条件均值为零：![](img/fc56cd9c-b8ea-483b-b6c6-b6a01481a1df.png)
- en: '**Homoskedasticity**, the error term ![](img/f0ded6ea-538b-40a8-9d3a-cb9f5483e17d.png) has
    constant variance given the inputs: ![](img/e5f9d3b1-a11c-4072-9cd9-d443cc1ce234.png)'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**同方差性**，误差项![](img/f0ded6ea-538b-40a8-9d3a-cb9f5483e17d.png)在给定输入时具有恒定方差：![](img/e5f9d3b1-a11c-4072-9cd9-d443cc1ce234.png)'
- en: 'The fourth assumption implies that no missing variable exists that is correlated
    with any of the input variables. Under the first four assumptions, the OLS method
    delivers **unbiased** estimates: including an irrelevant variable does not bias
    the intercept and slope estimates, but omitting a relevant variable will bias
    the OLS estimates. OLS is then also **consistent**: as the sample size increases,
    the estimates converge to the true value as the standard errors become arbitrary.
    The converse is unfortunately also true: if the conditional expectation of the
    error is not zero because the model misses a relevant variable or the functional
    form is wrong (that is, quadratic or log terms are missing), then all parameter
    estimates are biased. If the error is correlated with any of the input variables
    then OLS is also not consistent, that is, adding more data will not remove the
    bias.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: If we add the fifth assumptions, then OLS also produces the best linear, unbiased
    estimates (BLUE), where best means that the estimates have the lowest standard
    error among all linear estimators. Hence, if the five assumptions hold and statistical inference
    is the goal, then the OLS estimates is the way to go. If the goal, however, is
    to predict, then we will see that other estimators exist that trade off some bias
    for a lower variance to achieve superior predictive performance in many settings.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have introduced the basic OLS assumptions, we can take a look at
    inference in small and large samples.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: How to conduct statistical inference
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inference in the linear regression context aims to draw conclusions about the
    true relationship in the population from the sample data. This includes tests
    of hypothesis about the significance of the overall relationship or the values
    of particular coefficients, as well as estimates of confidence intervals.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: The key ingredient for statistical inference is a test statistic with a known
    distribution. We can use it to assume that the null hypothesis is true and compute
    the probability of observing the value for this statistic in the sample, familiar
    as the p-value. If the p-value drops below a significance threshold (typically
    five percent) then we reject the hypothesis because it makes the actual sample
    value very unlikely. At the same time, we accept that the p-value reflects the
    probability that we are wrong in rejecting what is, in fact, a correct hypothesis.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the five GMT assumptions, the classical linear model assumes **normality**—the population
    error is normally distributed and independent of the input variables. This assumption
    implies that the output variable is normally distributed, conditional on the input
    variables. This strong assumption permits the derivation of the exact distribution
    of the coefficients, which in turn implies exact distributions of the test statistics
    required for similarly exact hypotheses tests in small samples. This assumption
    often fails—asset returns, for instance, are not normally distributed—but, fortunately,
    the methods used under normality are also approximately valid.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the following distributional characteristics and test statistics, approximately
    under GMT assumptions 1–5, and exactly when normality holds:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: The parameter estimates follow a multivariate normal distribution: ![](img/41d4d2b8-7cf6-4a50-aa7a-b9ada8e5b6eb.png) .
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under GMT 1–5, the parameter estimates are already unbiased and we can get an unbiased
    estimate of ![](img/c255ed13-6923-4756-ba34-fc71c5b92cb3.png), the constant error
    variance, using ![](img/b5d5614e-5b53-416d-83d9-d168b96e8207.png).
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The t statistic for a hypothesis tests about an individual coefficient ![](img/fbfdf074-0b0d-4404-9e4b-80a87114d05a.png)is ![](img/60566dde-3959-4693-a060-9cac717f012b.png) and
    follows a t distribution with *N-p-1* degrees of freedom where ![](img/3bda39a2-09ae-4e22-9920-c0dfe0fa9414.png) is
    the j's element of the diagonal of ![](img/7a6b5101-d2be-4553-b96f-43949ce9508b.png).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *t* distribution converges to the normal distribution and since the 97.5
    quantile of the normal distribution is 1.96, a useful rule of thumb for a 95%
    confidence interval around a parameter estimate is ![](img/bb155050-7a1f-40b1-b171-eb5ca79d1622.png).
    An interval that includes zero implies that we can't reject the null hypothesis
    that the true parameter is zero and, hence, irrelevant for the model.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *F* statistic allows for tests of restrictions on several parameters, including
    whether the entire regression is significant. It measures the change (reduction)
    in the RSS that results from additional variables.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the **Lagrange Multiplier** (**LM**) test is an alternative to the
    *F* test to restrict multiple restrictions.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to diagnose and remedy problems
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Diagnostics validate the model assumptions and prevent wrong conclusions when
    interpreting the result and conducting statistical inference. They include measures
    of goodness of fit and various tests of the assumptions about the error term,
    including how closely the residuals match a normal distribution. Furthermore,
    diagnostics test whether the residual variance is indeed constant or exhibits
    heteroskedasticity, and if the errors are conditionally uncorrelated or exhibit
    serial correlation, that is, if knowing one error helps to predict consecutive
    errors.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the tests outlined as follows, it is always important to visually
    inspect the residuals to detect whether there are systematic patterns because
    these indicate that the model is missing one or more factors that drive the outcome.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Goodness of fit
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Goodness-of-fit measures assess how well a model explains the variation in the
    outcome. They help to assess the quality of model specification, for instance,
    to select among different model designs. They differ in how they evaluate the
    fit. The measures discussed here provide in-sample information; we will use out-of-sample
    testing and cross-validation when we focus on predictive models in the next section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'Prominent goodness-of-fit measures include the (adjusted) R² that should be
    maximized and is based on the least-squares estimate:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: R² measures the share of the variation in the outcome data explained by the
    model and is computed as ![](img/6a2363f4-83b4-42ce-85f0-3103f56a7a5a.png), where
    TSS is the sum of squared deviations of the outcome from its mean. It also corresponds
    to the squared correlation coefficient between the actual outcome values and those
    estimated (fitted) by the model. The goals is to maximize R² but it never decreases
    as the model adds more variables and, hence, encourages overfitting.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The adjusted R² penalizes R² for adding more variables; each additional variable
    needs to reduce RSS significantly to produce better goodness of fit.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alternatively, the Akaike (AIC) and the **Bayesian Information Criterion**
    (**BIC**) are to be minimized and are based on the maximum-likelihood estimate:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58a56311-c140-4adf-bcd0-adb48108167a.png), where ![](img/030f5c24-ec0a-47c9-9a82-265617c34044.png) is
    the value of the maximized likelihood function, k is the number of parameters'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/232898ca-2946-4ac5-8d69-1b9f4e32bfdb.png) where *N* is the sample size'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both metrics penalize for complexity, with BIC imposing a higher penalty so
    that it might underfit whereas AIC might overfit in relative terms. Conceptually, AIC
    aims at finding the model that best describes an unknown data-generating process,
    whereas BIC tries to find the best model among the set of candidates. In practice,
    both criteria can be used jointly to guide model selection when the goal is in-sample
    fit; otherwise, cross-validation and selection based on estimates of generalization
    error are preferable.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Heteroskedasticity
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GMT assumption 5 requires the residual covariance to take the shape ![](img/881489a2-b2d6-4688-a184-051c15fca6b3.png),
    that is, a diagonal matrix with entries equal to the constant variance of the
    error term. Heteroskedasticity occurs when the residual variance is not constant
    but differs across observations. If the residual variance is positively correlated
    with an input variable, that is, when errors are larger for input values that
    are far from their mean, then OLS standard error estimates will be too low, and,
    consequently, the t-statistic will be inflated leading to false discoveries of
    relationships where none actually exist.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: GMT假设5要求残差协方差采用形状![](img/881489a2-b2d6-4688-a184-051c15fca6b3.png)，即对角矩阵，其条目等于误差项的恒定方差。异方差性发生在残差方差不恒定，而是在观察中不同的情况。如果残差方差与输入变量呈正相关，即当误差较大时，OLS标准误差估计将过低，因此t统计量将被夸大，导致发现不存在的关系。
- en: Diagnostics starts with a visual inspection of the residuals. Systematic patterns
    in the (supposedly random) residuals suggest statistical tests of the null hypothesis
    that errors are homoscedastic against various alternatives. These tests include
    the Breusch—Pagan and White tests.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 诊断从对残差的视觉检查开始。在（假定随机的）残差中出现系统性模式，表明错误是同方差的零假设的统计检验，有多种替代方案。这些测试包括Breusch—Pagan和White测试。
- en: 'There are several ways to correct OLS estimates for heteroskedasticity:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以纠正OLS估计的异方差：
- en: Robust standard errors (sometimes called white standard errors) take heteroskedasticity
    into account when computing the error variance using a so-called **sandwich**
    **estimator**.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鲁棒标准误差（有时称为白色标准误差）在计算误差方差时考虑了异方差性，使用所谓的**夹心**估计量。
- en: Clustered standard errors assume that there are distinct groups in your data
    that are homoskedastic but the error variance differs between groups. These groups
    could be different asset classes or equities from different industries.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类标准误假设数据中存在不同的组，这些组是同方差的，但误差方差在组之间不同。这些组可能是不同的资产类别或来自不同行业的股票。
- en: 'Several alternatives to OLS estimate the error covariance matrix using different
    assumptions when ![](img/1fa8ab87-92ba-4bad-b08f-75d550e4d599.png). The following
    are available in `statsmodels`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种替代OLS的方法使用不同的假设来估计误差协方差矩阵，当![](img/1fa8ab87-92ba-4bad-b08f-75d550e4d599.png)。在`statsmodels`中提供以下选项：
- en: '**Weighted least squares** (**WLS**): For heteroskedastic errors where the
    covariance matrix has only diagonal entries as for OLS, but now the entries are
    allowed to vary'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权最小二乘法（WLS）：用于异方差误差，其中协方差矩阵只有对角条目，与OLS相同，但现在允许条目变化
- en: Feasible **generalized least squares** (**GLSAR**), for autocorrelated errors
    that follow an autoregressive AR (p) process (see the chapter on linear time series
    models)
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可行的广义最小二乘法（GLSAR），用于遵循自回归AR（p）过程的自相关误差（请参阅线性时间序列模型章节）
- en: '**Generalized least squares** (**GLS**) for arbitrary covariance matrix structure;
    yields efficient and unbiased estimates in the presence of heteroskedasticity
    or serial correlation'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广义最小二乘法（GLS）适用于任意协方差矩阵结构；在异方差性或序列相关性存在时产生高效和无偏估计
- en: Serial correlation
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列相关
- en: Serial correlation means that consecutive residuals produced by linear regression
    are correlated, which violates the fourth GMT assumption. Positive serial correlation
    implies that the standard errors are underestimated and the t-statistics will
    be inflated, leading to false discoveries if ignored. However, there are procedures
    to correct for serial correlation when calculating standard errors.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 序列相关意味着线性回归产生的连续残差是相关的，这违反了第四个GMT假设。正序列相关意味着标准误差被低估，t统计量被夸大，如果忽略，会导致虚假发现。然而，在计算标准误差时有纠正序列相关的程序。
- en: The Durbin—Watson statistic diagnoses serial correlation. It tests the hypothesis
    that the OLS residuals are not autocorrelated against the alternative that they
    follow an autoregressive process (that we will explore in the next chapter). The
    test statistic ranges from 0 to 4, and values near 2 indicate non-autocorrelation,
    lower values suggest positive, and higher values indicate negative autocorrelation.
    The exact threshold values depend on the number of parameters and observations
    and need to be looked up in tables.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Durbin—Watson统计量诊断序列相关性。它检验了OLS残差不自相关的零假设，而是遵循自回归过程的替代方案（我们将在下一章中探讨）。检验统计量范围从0到4，接近2的值表示非自相关，较低的值表示正相关，较高的值表示负相关。确切的阈值取决于参数和观测值的数量，需要在表中查找。
- en: Multicollinearity
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多重共线性
- en: 'Multicollinearity occurs when two or more independent variables are highly
    correlated. This poses several challenges:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个或更多的自变量高度相关时，就会出现多重共线性。这带来了几个挑战：
- en: It is difficult to determine which factors influence the dependent variable
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很难确定哪些因素影响了因变量
- en: The individual p values can be misleading—a p-value can be high even if the
    variable is important
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个别p值可能会误导-即使变量很重要，p值也可能很高
- en: The confidence intervals for the regression coefficients will be excessive,
    possibly even including zero, making it impossible to determine the effect of
    an independent variable on the outcome
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归系数的置信区间可能过大，甚至可能包括零，使得无法确定自变量对结果的影响
- en: There is no formal or theory-based solution that corrects for multicollinearity.
    Instead, try to remove one or more of the correlated input variables, or increase
    the sample size.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 没有正式的或基于理论的解决方案可以纠正多重共线性。相反，尝试去除一个或多个相关的输入变量，或增加样本量。
- en: How to run linear regression in practice
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在实践中运行线性回归
- en: 'The accompanying notebook `linear_regression_intro.ipynb` illustrates a simple
    and then a multiple linear regression, the latter using both OLS and gradient
    descent. For the multiple regression, we generate two random input variables *x[1 ]*and
    *x[2]* that range from -50 to +50, and an outcome variable calculated as a linear
    combination of the inputs plus random Gaussian noise to meet the normality assumption
    GMT 6:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 附带的笔记本`linear_regression_intro.ipynb`演示了简单的线性回归和多元线性回归，后者使用了OLS和梯度下降。对于多元回归，我们生成了两个随机输入变量*x[1]*和*x[2]*，范围从-50到+50，并且一个结果变量计算为输入的线性组合加上随机高斯噪声，以满足正态性假设GMT
    6：
- en: '![](img/e51fe442-ed74-43fd-82fe-bc610bc10f8e.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e51fe442-ed74-43fd-82fe-bc610bc10f8e.png)'
- en: OLS with statsmodels
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OLS with statsmodels
- en: 'We use `statsmodels` to estimate a multiple regression model that accurately
    reflects the data generating process as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`statsmodels`来估计一个准确反映数据生成过程的多元回归模型如下：
- en: '[PRE0]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This yields the following **OLS Regression Results** summary:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下**OLS回归结果**摘要：
- en: '![](img/bb6ff5b6-0bd0-4778-9951-91d5cc77e648.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb6ff5b6-0bd0-4778-9951-91d5cc77e648.png)'
- en: Summary ofOLS Regression Results
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: OLS回归结果摘要
- en: 'The upper part of the summary displays the dataset characteristics, namely
    the estimation method, the number of observations and parameters, and indicates
    that standard error estimates do not account for heteroskedasticity. The middle
    panel shows the coefficient values that closely reflect the artificial data generating
    process. We can confirm that the estimates displayed in the middle of the summary
    result can be obtained using the OLS formula derived previously:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要的上部显示了数据集的特征，即估计方法、观测值和参数的数量，并指出标准误差估计不考虑异方差性。中间面板显示了系数值，这些系数值与人工数据生成过程密切相关。我们可以确认摘要结果中间显示的估计可以使用先前推导的OLS公式获得：
- en: '[PRE1]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following diagram illustrates the hyperplane fitted by the model to the
    randomly generated data points:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了模型对随机生成的数据点拟合的超平面：
- en: '![](img/e3ec55b4-5d60-473b-ae25-2d5a9be6af8d.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3ec55b4-5d60-473b-ae25-2d5a9be6af8d.png)'
- en: Hyperplane
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 超平面
- en: The upper right part of the panel displays the goodness-of-fit measures just
    discussed, alongside the F-test that rejects the hypothesis that all coefficients
    are zero and irrelevant. Similarly, the t-statistics indicate that intercept and
    both slope coefficients are, unsurprisingly, highly significant.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 面板的右上部显示了刚才讨论的拟合优度指标，以及拒绝所有系数为零和不相关的F检验。同样，t统计量表明截距和斜率系数都是非常显著的，这并不奇怪。
- en: The bottom part of the summary contains the residual diagnostics. The left panel
    displays skew and kurtosis that are used to test the normality hypothesis. Both
    the Omnibus and the Jarque—Bera test fails to reject the null hypothesis that
    the residuals are normally distributed. The Durbin—Watson statistic tests for
    serial correlation in the residuals and has a value near 2 which, given 2 parameters
    and 625 observations, fails to reject the hypothesis of no serial correlation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 总结的底部包含了残差诊断。左侧面板显示了用于测试正态性假设的偏度和峰度。Omnibus和Jarque—Bera测试都未能拒绝残差正态分布的零假设。Durbin—Watson统计量测试残差的串行相关性，并且在2附近具有一个值，考虑到2个参数和625个观测值，未能拒绝无串行相关性的假设。
- en: 'Lastly, the condition number provides evidence about multicollinearity: it is
    the ratio of the square roots of the largest and the smallest eigenvalue of the design
    matrix that contains the input data. A value above 30 suggests that the regression
    may have significant multicollinearity.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，条件数提供了关于多重共线性的证据：它是包含输入数据的设计矩阵的最大和最小特征值的平方根的比值。值超过30表明回归可能存在显著的多重共线性。
- en: '`statsmodels` includes additional diagnostic tests that are linked in the notebook.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`statsmodels`包括与笔记本中链接的其他诊断测试。'
- en: Stochastic gradient descent with sklearn
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用sklearn的随机梯度下降
- en: 'The `sklearn` library includes an `SGDRegressor` model in its `linear_models`
    module. To learn the parameters for the same model using this method, we need
    to first standardize the data because the gradient is sensitive to the scale.
    We use `StandardScaler()` for this purpose that computes the mean and the standard
    deviation for each input variable during the fit step, and then subtracts the
    mean and divides by the standard deviation during the transform step that we can
    conveniently conduct in a single `fit_transform()` command:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`库在其`linear_models`模块中包括了一个`SGDRegressor`模型。要使用这种方法学习相同模型的参数，我们需要首先标准化数据，因为梯度对尺度敏感。我们使用`StandardScaler()`来计算每个输入变量的均值和标准差，在拟合步骤中减去均值并除以标准差，在转换步骤中进行方便的`fit_transform()`命令：'
- en: '[PRE2]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we instantiate the `SGDRegressor` using the default values except for
    a `random_state` setting to facilitate replication:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用默认值实例化`SGDRegressor`，除了设置`random_state`以便复制：
- en: '[PRE3]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we can fit the `sgd` model, create the in-sample predictions for both the
    OLS and the `sgd` models, and compute the root mean squared error for each:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以拟合`sgd`模型，为OLS和`sgd`模型创建样本内预测，并计算每个模型的均方根误差：
- en: '[PRE4]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As expected, both models yield the same result. We will now take on a more ambitious
    project using linear regression to estimate a multi-factor asset pricing model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，两个模型产生了相同的结果。现在我们将承担一个更有雄心的项目，使用线性回归来估计多因素资产定价模型。
- en: How to build a linear factor model
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何构建线性因子模型
- en: Algorithmic trading strategies use **linear factor models** to quantify the
    relationship between the return of an asset and the sources of risk that represent
    the main drivers of these returns. Each factor risk carries a premium, and the
    total asset return can be expected to correspond to a weighted average of these
    risk premia.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 算法交易策略使用**线性因子模型**来量化资产回报与代表这些回报主要驱动因素的风险来源之间的关系。每个因子风险都带有一个风险溢价，总资产回报可以预期对应于这些风险溢价的加权平均值。
- en: 'There are several practical applications of factor models across the portfolio
    management process from construction and asset selection to risk management and
    performance evaluation. The importance of factor models continues to grow as common
    risk factors are now tradeable:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: A summary of the returns of many assets by a much smaller number of factors
    reduces the amount of data required to estimate the covariance matrix when optimizing
    a portfolio
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An estimate of the exposure of an asset or a portfolio to these factors allows
    for the management of the resultant risk, for instance by entering suitable hedges
    when risk factors are themselves traded
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A factor model also permits the assessment of the incremental signal content
    of new alpha factors
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A factor model can also help assess whether a manager's performance relative
    to a benchmark is indeed due to skill in selecting assets and timing the market,
    or if instead, the performance can be explained by portfolio tilts towards known
    return drivers that can today be replicated as low-cost, passively managed funds
    without incurring active management fees
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following examples apply to equities, but risk factors have been identified
    for all asset classes (see references in the GitHub repository).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: From the CAPM to the Fama—French five-factor model
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Risk factors have been a key ingredient to quantitative models since the **Capital
    Asset Pricing Model **(**CAPM**) explained the expected returns of all *N* assets ![](img/31afacbc-b4b4-4f0f-97ac-fff357bc652a.png)
    using their respective exposure ![](img/a332fea0-7be6-4858-a873-73742a227f66.png)
    to a single factor, the expected excess return of the overall market over the
    risk-free rate ![](img/4f4b2d16-602f-4672-941b-3be7cc28d1a4.png). The model takes
    the following linear form:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a6152b2-582b-4452-a8d2-c70388fd9827.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: This differs from classic fundamental analysis a la Dodd and Graham where returns
    depend on firm characteristics. The rationale is that, in the aggregate, investors
    cannot eliminate this so-called systematic risk through diversification. Hence, in
    equilibrium, they require compensation for holding an asset commensurate with
    its systematic risk. The model implies that, given efficient markets where prices
    immediately reflect all public information, there should be no superior risk-adjusted
    returns, that is, the value of ![](img/f9d4110b-3746-4824-9340-1ec387bb81f6.png)
    should be zero.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'Empirical tests of the model use linear regression and have consistently failed,
    prompting a debate whether the efficient markets or the single factor aspect of
    the joint hypothesis is to blame. It turns out that both premises are probably
    wrong:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Joseph Stiglitz earned the 2001 Nobel Prize in economics in part for showing
    that markets are generally not perfectly efficient: if markets are efficient,
    there is no value in collecting data because this information is already reflected
    in prices. However, if there is no incentive to gather information, it is hard
    to see how it should be already reflected in prices.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, theoretical and empirical improvements on the CAPM suggest
    that additional factors help explain some of the anomalies that consisted in superior
    risk-adjusted returns that do not depend on overall market exposure, such as higher
    returns for smaller firms.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stephen Ross proposed the **Arbitrage Pricing Theory** (**APT**) in 1976 as
    an alternative that allows for several risk factors while eschewing market efficiency.
    In contrast to the CAPM, it assumes that opportunities for superior returns due
    to mispricing may exist but will quickly be arbitraged away. The theory does not
    specify the factors, but research by the author suggests that the most important
    are changes in inflation and industrial production, as well as changes in risk
    premia or the term structure of interest rates.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Kenneth French and Eugene Fama (who won the 2013 Nobel Prize) identified additional
    risk factors that depend on firm characteristics and are widely used today. In
    1993, the Fama—French three-factor model added the relative size and value of
    firms to the single CAPM source of risk. In 2015, the five-factor model further
    expanded the set to include firm profitability and level of investment that had
    been shown to be significant in the intervening years. In addition, many factor
    models include a price momentum factor.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'The Fama—French risk factors are computed as the return difference on diversified
    portfolios with high or low values according to metrics that reflect a given risk
    factor. These returns are obtained by sorting stocks according to these metrics
    and then going long stocks above a certain percentile while shorting stocks below
    a certain percentile. The metrics associated with the risk factors are defined
    as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '**Size**: **Market Equity** (**ME**)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value**: **Book Value of Equity** (**BE**) divided by ME'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operating Profitability (OP)**: Revenue minus cost of goods sold/assets'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Investment**: Investment/assets'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also unsupervised learning techniques for a data-driven discovery
    of risk factors using factors and principal component analysis that we will explore
    in [Chapter 12](c187906e-9fde-4f85-b709-df88dd0f7e88.xhtml), *Unsupervised Learning.*
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining the risk factors
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fama and French make updated risk factor and research portfolio data available
    through their website, and you can use the `pandas_datareader` library to obtain
    the data. For this application, refer to the `fama_macbeth.ipynb` notebook for
    additional detail.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we will be using the five Fama—French factors that result from
    sorting stocks first into three size groups and then into two for each of the
    remaining three firm-specific factors. Hence, the factors involve three sets of
    value-weighted portfolios formed as 3 x 2 sorts on size and book-to-market, size
    and operating profitability, and size and investment. The risk factor values computed
    as the average returns of the **portfolios** (**PF**) as outlined in the following
    table:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '| **Concept** | **Label** | **Name** | **Risk factor calculation** |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| Size | SMB | Small minus big | Nine small stock PF minus nine large stock
    PF |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| Value | HML | High minus low |  Two value PF minus two growth (with low BE/ME
    value) PF |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| Profitability | RMW | Robust minus weak | Two robust OP PF minus two weak
    OP PF |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| Investment | CMA | Conservative minus aggressive | Two conservative investment
    portfolios minus two aggressive investment portfolios |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| Market | Rm-Rf | Excess return on the market | Value-weight return of all
    firms incorporated in and listed on  major US exchanges with good data minus the
    one-month Treasury bill rate |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: 'We will use returns at a monthly frequency that we obtain for the period 2010
    – 2017 as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Fama and French also make available numerous portfolios that we can illustrate
    the estimation of the factor exposures, as well as the value of the risk premia
    available in the market for a given time period. We will use a panel of the 17
    industry portfolios at a monthly frequency. We will subtract the risk-free rate
    from the returns because the factor model works with excess returns:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We will now build a linear factor model based on this panel data using a method
    that addresses the failure of some basic linear regression assumptions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Fama—Macbeth regression
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given data on risk factors and portfolio returns, it is useful to estimate the
    portfolio's exposure, that is, how much the risk factors drive portfolio returns,
    as well as how much the exposure to a given factor is worth, that is, the what
    market's risk factor premium is. The risk premium then permits to estimate the
    return for any portfolio provided the factor exposure is known or can be assumed.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: More formally, we will have *i=1, ..., N* asset or portfolio returns over *t=1,
    ..., T* periods and each asset's excess period return will be denoted ![](img/65e5c37d-8bfd-47b8-85a9-07e538fd00f8.png). The
    goals is to test whether the *j=1, ..., M* factors ![](img/ca9ceac1-0c7c-4833-b6d1-dd15cb47660d.png)explain
    the excess returns and the risk premium associated with each factor. In our case,
    we have *N=17* portfolios and *M=5* factors, each with =96 periods of data.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Factor models are estimated for many stocks in a given period. Inference problems
    will likely arise in such cross-sectional regressions because the fundamental
    assumptions of classical linear regression may not hold. Potential violations
    include measurement errors, covariation of residuals due to heteroskedasticity
    and serial correlation, and multicollinearity.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'To address the inference problem caused by the correlation of the residuals,
    Fama and MacBeth proposed a two-step methodology for a cross-sectional regression
    of returns on factors. The two-stage Fama—Macbeth regression is designed to estimate
    the premium rewarded for the exposure to a particular risk factor by the market. The
    two stages consist of:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '**First stage**: *N* time-series regression, one for each asset or portfolio,
    of its excess returns on the factors to estimate the factor loadings. In matrix
    form, for each asset:'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/e187e1c1-3c7a-4dc4-a691-0e80d387a11b.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: '**Second stage**: T cross-sectional regression, one for each time period, to
    estimate the risk premium. In matrix form, we obtain a vector ![](img/8a5b412f-8c0f-40f0-97d5-d3f50789362e.png) of
    risk premia for each period:'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/ebf5ed03-1aaf-4cef-85aa-33cc4fb12ad1.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: Now we can compute the factor risk premia as the time average and get t-statistic
    to assess their individual significance, using the assumption that the risk premia
    estimates are independent over time:![](img/105fc0bb-9211-48d6-b2cb-c1ff1081bbf3.png).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: If we had a very large and representative data sample on traded risk factors
    we could use the sample mean as a risk premium estimate. However, we typically
    do not have a sufficiently long history to and the margin of error around the
    sample mean could be quite large. The Fama—Macbeth methodology leverages the covariance
    of the factors with other assets to determine the factor premia. The second moment
    of asset returns is easier to estimate than the first moment, and obtaining more
    granular data improves estimation considerably, which is not true of mean estimation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement the first stage to obtain the 17 factor loading estimates
    as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For the second stage, we run 96 regressions of the period returns for the cross
    section of portfolios on the factor loadings:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we compute the average for the 96 periods to obtain our factor risk
    premium estimates:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `linear_models` library extends `statsmodels` with various models for panel
    data and also implements the two-stage Fama—MacBeth procedure:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This provides us with the same result:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74d5d23a-9727-43c3-b1a1-3cb8ece117dd.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: LinearFactorModel Estimation Summary
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: The accompanying notebook illustrates the use of categorical variables by using
    industry dummies when estimating risk premia for a larger panel of individual
    stocks.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'Shrinkage methods: regularization for linear regression'
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The least squares methods to train a linear regression model will produce the
    best, linear, and unbiased coefficient estimates when the Gauss—Markov assumptions
    are met. Variations like GLS fare similarly well even when OLS assumptions about
    the error covariance matrix are violated. However, there are estimators that produce
    biased coefficients to reduce the variance to achieve a lower generalization error
    overall.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: When a linear regression model contains many correlated variables, their coefficients
    will be poorly determined because the effect of a large positive coefficient on
    the RSS can be canceled by a similarly large negative coefficient on a correlated
    variable. Hence, the model will have a tendency for high variance due to this
    wiggle room of the coefficients that increases the risk that the model overfits
    to the sample.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: How to hedge against overfitting
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One popular technique to control overfitting is that of **regularization**,
    which involves the addition of a penalty term to the error function to discourage
    the coefficients from reaching large values. In other words, size constraints
    on the coefficients can alleviate the resultant potentially negative impact on
    out-of-sample predictions. We will encounter regularization methods for all models
    since overfitting is such a pervasive problem.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will introduce shrinkage methods that address two motivations
    to improve on the approaches to linear models discussed so far:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction accuracy**: The low bias but high variance of least squares estimates
    suggests that the generalization error could be reduced by shrinking or setting
    some coefficients to zero, thereby trading off a slightly higher bias for a reduction
    in the variance of the model.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretation**: A large number of predictors may complicate the interpretation
    or communication of the big picture of the results. It may be preferable to sacrifice
    some detail to limit the model to a smaller subset of parameters with the strongest
    effects.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shrinkage models restrict the regression coefficients by imposing a penalty
    on their size. These models achieve this goal by adding a term to the objective function
    so that the coefficients of a shrinkage model minimize the RSS plus a penalty
    that is positively related to the (absolute) size of the coefficients. The added
    penalty turns finding the linear regression coefficients into a constrained minimization
    problem that, in general, takes the following Lagrangian form:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2dd72b9d-c072-47ce-834f-356b6c948b14.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: The regularization parameter λ determines the size of the penalty effect, that
    is, the strength of the regularization. As soon as λ is positive, the coefficients
    will differ from the unconstrained least squared parameters, which implies a biased
    estimate. The hyperparameter λ should be adaptively chosen using cross-validation
    to minimize an estimate of expected prediction error.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Shrinkage models differ by how they calculate the penalty, that is, the functional
    form of S. The most common versions are the ridge regression that uses the sum
    of the squared coefficients, whereas the lasso model bases the penalty on the
    sum of the absolute values of the coefficients.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: How ridge regression works
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ridge regression shrinks the regression coefficients by adding a penalty
    to the objective function that equals the sum of the squared coefficients, which
    in turn corresponds to the L² norm of the coefficient vector:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/119120f8-5beb-4dd6-9aaf-d613fb2edb63.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: 'Hence, the ridge coefficients are defined as:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2783010-31ca-4a32-8997-dd5c559c40e6.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: The intercept ![](img/71cf85ed-96c5-49f5-b91d-07099b5ed007.png) has been excluded
    from the penalty to make the procedure independent of the origin chosen for the
    output variable—otherwise, adding a constant to all output values would change
    all slope parameters as opposed to a parallel shift.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to standardize the inputs by subtracting from each input the
    corresponding mean and dividing the result by the input''s standard deviation because
    the ridge solution is sensitive to the scale of the inputs. There is also a closed
    solution for the ridge estimator that resembles the OLS case:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d2c6a4a-0e95-4314-932e-9cdec5694253.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: The solution adds the scaled identity matrix λ*I* to *X^TX* before inversion,
    which guarantees that the problem is non-singular, even if *X^T**X* does not have full
    rank. This was one of the motivations for using this estimator when it was originally
    introduced.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'The ridge penalty results in proportional shrinkage of all parameters. In the
    case of **orthonormal inputs**, the ridge estimates are just a scaled version
    of the least squares estimates, that is:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bba72082-ca29-4704-a72d-d811aa84a7c2.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: Using the **singular value decomposition** (**SVD**) of the input matrix *X*,
    we can gain insight into how the shrinkage affects inputs in the more common case
    where they are not orthonormal. The SVD of a centered matrix represents the principal
    components of a matrix (refer to [Chapter 11](2fbfa6b5-87f3-49c3-b13a-5ead63471370.xhtml),
    *Gradient Boosting Machines*, on unsupervised learning) that capture uncorrelated
    directions in the column space of the data in descending order of variance.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression shrinks coefficients on input variables that are associated
    with directions in the data that have less variance more than input variables
    that correlate with directions that exhibit more variance. Hence, the implicit
    assumption of ridge regression is that the directions in the data that vary the
    most will be most influential or most reliable when predicting the output.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: How lasso regression works
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The lasso, known as basis pursuit in signal processing, also shrinks the coefficients
    by adding a penalty to the sum of squares of the residuals, but the lasso penalty
    has a slightly different effect. The lasso penalty is the sum of the absolute
    values of the coefficient vector, which corresponds to its L¹ norm. Hence, the
    lasso estimate is defined by:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c05889b1-7c7b-415e-b0eb-280833978de1.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: Similarly to ridge regression, the inputs need to be standardized. The lasso
    penalty makes the solution nonlinear, and there is no closed-form expression for
    the coefficients as in ridge regression. Instead, the lasso solution is a quadratic
    programming problem and there are available efficient algorithms that compute
    the entire path of coefficients that result for different values of λ with the
    same computational cost as for ridge regression.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: The lasso penalty had the effect ofgradually reducing some coefficients to zero
    as the regularization increases. For this reason, the lasso can be used for the
    continuous selection of a subset of features.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: How to use linear regression to predict returns
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The notebook `linear_regression.ipynb` contains examples for the prediction
    of stock prices using OLS with `statsmodels` and `sklearn`, as well as ridge and
    lasso models. It is designed to run as a notebook on the Quantopian research platform
    and relies on the `factor_library` introduced in [Chapter 4](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml), *Alpha
    Factors Research*.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the data
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to select a universe of equities and a time horizon, build and transform
    alpha factors that we will use as features, calculate forward returns that we
    aim to predict, and potentially clean our data.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Universe creation and time horizon
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use equity data for the years 2014 and 2015 from a custom `Q100US`
    universe that uses built-in filters, factors, and classifiers to select the 100
    stocks with the highest average dollar volume of the last 200 trading days filtered
    by additional default criteria (see Quantopian docs linked on GitHub for detail).
    The universe dynamically updates based on the filter criteria so that, while there
    are 100 stocks at any given point, there may be more than 100 distinct equities
    in the sample:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Target return computation
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will test predictions for various `lookahead` periods to identify the best
    holding periods that generate the best predictability, measured by the information
    coefficient. More specifically, we compute returns for 1, 5, 10, and 20 days using
    the built-in `Returns` function, resulting in over 50,000 observations for the
    universe of 100 stocks over two years (that include approximately 252 trading
    days each):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Alpha factor selection and transformation
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use over 50 features that cover a broad range of factors based on market,
    fundamental, and alternative data. The notebook also includes custom transformations
    to convert fundamental data that is typically available in quarterly reporting
    frequency to rolling annual totals or averages to avoid excessive season fluctuations.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the factors have been computed through the various pipelines outlined
    in [Chapter 4](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml), *Alpha Factors Research*,
    we combine them using `pd.concat()`, assign index names, and create a categorical
    variable that identifies the asset for each data point:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Data cleaning – missing data
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a next step, we remove rows and columns that lack more than 20 percent of
    the observations, resulting in a loss of six percent of the observations and three
    columns:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'At this point, we have 51 features and the categorical identifier of the stock:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Data exploration
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For linear regression models, it is important to explore the correlation among
    the features to identify multicollinearity issues, and to check the correlation
    between the features and the target. The notebook contains a seaborn clustermap
    that shows the hierarchical structure of the feature correlation matrix. It identifies
    a small number of highly correlated clusters.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Dummy encoding of categorical variables
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to convert the categorical `stock` variable into a numeric format so
    that the linear regression can process it. For this purpose, we use dummy encoding
    that creates individual columns for each category level and flags the presence
    of this level in the original categorical column with an entry of `1`, and `0`
    otherwise. The pandas function `get_dummies()` automates dummy encoding. It detects
    and properly converts columns of type objects as illustrated next. If you need
    dummy variables for columns containing integers, for instance, you can identify
    them using the keyword `columns`:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'When converting all categories to dummy variables and estimating the model
    with an intercept (as you typically would), you inadvertently create multicollinearity:
    the matrix now contains redundant information and no longer has full rank, that
    is, becomes singular. It is simple to avoid this by removing one of the new indicator
    columns. The coefficient on the missing category level will now be captured by
    the intercept (which is always `1` when every other category dummy is `0`). Use
    the `drop_first` keyword to correct the dummy variables accordingly:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Applied to our combined features and returns, we obtain 181 columns because
    there are more than 100 stocks as the universe definition automatically updates
    the stock selection:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Creating forward returns
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal is to predict returns over a given holding period. Hence, we need
    to align the features with return values with the corresponding return data point
    1, 5, 10, or 20 days into the future for each equity. We achieve this by combining
    the pandas `.groupby()` method with the `.shift()` method as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: There are now different numbers of observations for each return series as the
    forward shift has created missing values at the tail end for each equity.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Linear OLS regression using statsmodels
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can estimate a linear regression model using OLS with `statsmodels` as demonstrated
    previously. We select a forward return, for example for a 10-day holding period,
    remove outliers below the 2.5% and above the 97.5% percentiles, and fit the model
    accordingly:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Diagnostic statistics
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The summary is available in the notebook to save some space due to the large
    number of variables. The diagnostic statistics show that, given the high p-value
    on the Jarque—Bera statistic, the hypothesis that the residuals are normally distributed
    cannot be rejected.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: However, the Durbin—Watson statistic is low at 1.5 so we can reject the null
    hypothesis of no autocorrelation comfortably at the 5% level. Hence, the standard
    errors are likely positively correlated. If our goal were to understand which
    factors are significantly associated with forward returns, we would need to rerun
    the regression using robust standard errors (a parameter in `statsmodels .fit()`
    method), or use a different method altogether such as a panel model that allows
    for more complex error covariance.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Linear OLS regression using sklearn
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since sklearn is tailored towards prediction, we will evaluate the linear regression
    model based on its predictive performance using cross-validation.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Custom time series cross-validation
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our data consists of grouped time series data that requires a custom cross-validation
    function to provide the train and test indices that ensure that the test data
    immediately follows the training data for each equity and we do not inadvertently
    create a look-ahead bias or leakage.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'We can achieve this using the following function that returns a `generator` yielding
    pairs of train and test dates. The set of train dates that ensure a minimum length
    of the training periods. The number of pairs depends on the parameter `nfolds`. The
    distinct test periods do not overlap and are located at the end of the period
    available in the data. After a test period is used, it becomes part of the training
    data that grow in size accordingly:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Select features and target
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to select the appropriate return series (we will again use a 10-day
    holding period) and remove outliers. We will also convert returns to log returns
    as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Cross-validating the model
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use 250 folds to generally predict about 2 days of forward returns
    following the historical training data that will gradually increase in length.
    Each iteration obtains the appropriate training and test dates from our custom
    cross-validation function, selects the corresponding features and targets, and
    then trains and predicts accordingly. We capture the root mean squared error as
    well as the Spearman rank correlation between actual and predicted values:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Test results – information coefficient and RMSE
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have captured the test predictions from the 250 folds and can compute both
    the overall and a 21-day rolling average:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We obtain the following chart that highlights the negative correlation of IC
    and RMSE and their respective values:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2f699e8-ecac-49db-b305-7af3946654b3.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: Chart highlighting the negative correlation of IC and RMSE
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'For the entire period, we see that the Information Coefficient measured by
    the rank correlation of actual and predicted returns is weakly positive and statistically
    significant:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33da13e6-51b6-4893-a3da-9a25c2f4dee5.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: Ridge regression using sklearn
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the ridge regression, we need to tune the regularization parameter with
    the keyword `alpha` that corresponds to the λ we used previously. We will try
    21 values from 10^(-5) to 10⁵ in logarithmic steps.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: The scale sensitivity of the ridge penalty requires us to standardize the inputs
    using the `StandardScaler`. Note that we always learn the mean and the standard
    deviation from the training set using the `.fit_transform()` method and then apply
    these learned parameters to the test set using the `.transform()` method.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the regularization parameters using cross-validation
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We then proceed to cross-validate the hyperparameter values again using `250`
    folds as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Cross-validation results and ridge coefficient paths
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now plot the information coefficient obtained for each hyperparameter
    value and also visualize how the coefficient values evolve as the regularization
    increases. The results show that we get the highest IC value for a value of λ=10\.
    For this level of regularization, the right-hand panel reveals that the coefficients
    have been already significantly shrunk compared to the (almost) unconstrained
    model with *λ=10^(-5)*:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83c16cfe-0936-4705-a073-fa1849cd7d82.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: Cross-validation results and ridge coefficient paths
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Top 10 coefficients
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The standardization of the coefficients allows us to draw conclusions about
    their relative importance by comparing their absolute magnitude. The 10 most relevant
    coefficients are:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36b556ea-a0b2-46e1-b01a-ac2185ac20df.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
- en: Top 10 coefficients
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regression using sklearn
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The lasso implementation looks very similar to the ridge model we just ran.
    The main difference is that lasso needs to arrive at a solution using iterative
    coordinate descent whereas ridge can rely on a closed-form solution:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Cross-validated information coefficient and Lasso Path
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As before, we can plot the average information coefficient for all test sets
    used during cross-validation. We see again that regularization improves the IC
    over the unconstrained model, delivering the best out-of-sample result at a level
    of *λ=10^(-5)*. The optimal regularization value is quite different from ridge
    regression because the penalty consists of the sum of the absolute, not the squared
    values of the relatively small coefficient values. We can also see that for this
    regularization level, the coefficients have been similarly shrunk, as in the ridge
    regression case:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/013530cc-20f7-463a-ac1d-354825bd10e3.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
- en: Cross-validated information coefficient and Lasso Path
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: In sum, ridge and lasso will produce similar results. Ridge often computes faster,
    but lasso also yields continuous features subset selection by gradually reducing
    coefficients to zero, hence eliminating features.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Linear classification
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The linear regression model discussed so far assumes a quantitative response
    variable. In this section, we will focus on approaches to modeling qualitative
    output variables for inference and prediction, a process that is known as **classification**
    and that occurs even more frequently than regression in practice.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Predicting a qualitative response for a data point is called **classifying**
    that observation because it involves assigning the observation to a category,
    or class. In practice, classification methods often predict probabilities for
    each of the categories of a qualitative variable and then use this probability
    to decide on the proper classification.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: We could approach the classification problem ignoring the fact that the output
    variable assumes discrete values, and apply the linear regression model to try
    to predict a categorical output using multiple input variables. However, it is
    easy to construct examples where this method performs very poorly. Furthermore,
    it doesn't make intuitive sense for the model to produce values larger than 1
    or smaller than 0 when we know that *y ∈ [0, 1]*.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: There are many different classification techniques, or classifiers, that are
    available to predict a qualitative response. In this section, we will introduce
    the widely used logistic regression which is closely related to linear regression. We
    will address more complex methods in the following chapters, on generalized additive
    models that include decision trees and random forests, as well as gradient boosting
    machines and neural networks.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: The logistic regression model
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The logistic regression model arises from the desire to model the probabilities
    of the output classes given a function that is linear in *x*, just like the linear
    regression model, while at the same time ensuring that they sum to one and remain
    in the [0, 1] as we would expect from probabilities.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we introduce the objective and functional form of the logistic
    regression model and describe the training method. We then illustrate how to use
    logistic regression for statistical inference with macro data using statsmodels,
    and how to predict price movements using the regularized logistic regression implemented
    by sklearn.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Objective function
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For illustration, we''ll use the output variable y that takes on the value
    1 if a stock return is positive over a given time horizon d, and 0 otherwise:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/959b63ca-ea20-4c97-bc7c-154d2341e74a.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
- en: 'We could easily extend y to three categories, where 0 and 2 reflect negative
    and positive price moves beyond a certain threshold, and 1 otherwise. Rather than
    modeling the output variable *y*, however, logistic regression models the probability
    that y belongs to either of the categories given a vector of alpha factors or
    features ![](img/90185215-12e0-4ea9-9ab1-2b277832e26c.png). In other words, the logistic
    regression models the probability that the stock price goes up, conditional on
    the values of the variables included in the model:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/767f64a0-9327-4123-8a14-68b4aa0fab23.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
- en: The logistic function
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To prevent the model from producing values outside the [0, 1] interval, we
    must model *p(x)* using a function that only gives outputs between 0 and 1 over
    the entire domain of *x*. The logistic function meets this requirement and always
    produces an S-shaped curve (see notebook examples), and so, regardless of the
    value of X, we will obtain a sensible prediction:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ca89660-1107-4c0a-a57d-adae0541bcee.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
- en: 'Here, the vector *x* includes a 1 for the intercept captured by the first component
    of ![](img/5218aa4d-ecfa-4abb-970a-67b11c0c557a.png), ![](img/565b1832-0731-474d-8eb8-9ad248442204.png).
    We can transform this expression to isolate the part that looks like a linear
    regression to arrive at:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30e97c9b-97c4-44a5-a528-d1e54384d4bc.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
- en: The quantity *p(x)/[1−p(x)]* is called the **odds**, an alternative way to express
    probabilities that may be familiar from gambling, and can take on any value odds
    between 0 and ∞, where low values also imply low probabilities and high values
    imply high probabilities.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: The logit is also called log-odds (since it is the logarithm of the odds). Hence,
    the logistic regression represents a logit that is linear in *x* and looks a lot
    like the preceding linear regression.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood estimation
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The coefficient vector ![](img/67a36c05-1101-4552-92e7-a069bab53116.png) must
    be estimated using the available training data. Although we could use (non-linear)
    least squares to fit the logistic regression model, the more general method of
    maximum likelihood is preferred, since it has better statistical properties. As
    we have just discussed, the basic intuition behind using maximum likelihood to
    fit a logistic regression model is to seek estimates for ![](img/67a36c05-1101-4552-92e7-a069bab53116.png)
    such that the predicted probability ![](img/86fcfb37-c25d-4cbd-b39d-a72674162df0.png)
    corresponds as closely as possible to the actual outcome. In other words, we try
    to find ![](img/6754827d-5689-4136-b222-c162c910b0c7.png) such that these estimates
    yield a number close to 1 for all cases where the stock price went up, and a number
    close to 0 otherwise. More formally, we are seeking to maximize the likelihood
    function:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ee6262d-4015-4968-a752-11c1a87ef6ba.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
- en: 'It is easier to work with sums than with products, so let''s take logs on both
    sides to get the log-likelihood function and the corresponding definition of the
    logistic regression coefficients:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/598f2e46-7876-403f-a502-2f045e6161e1.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
- en: Maximizing this equation by setting the derivatives of ![](img/a4a91a01-a2c2-47a4-8ed3-a2ba2408d686.png) with
    respect to ![](img/67a36c05-1101-4552-92e7-a069bab53116.png) to zero yields p+1
    so-called score equations that are nonlinear in the parameters that can be solved
    using iterative numerical methods for the concave log-likelihood function.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: How to conduct inference with statsmodels
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will illustrate how to use logistic regression with `statsmodels` based on
    a simple built-in dataset containing quarterly US macro data from 1959 – 2009
    (see the notebook `logistic_regression_macro_data.ipynb` for detail).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'The variables and their transformations are listed in the following table:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '| **Variable** | **Description** | **Transformation** |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: '| `realgdp` | Real gross domestic product | Annual Growth Rate |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: '| `realcons` | Real personal consumption expenditures | Annual Growth Rate
    |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
- en: '| `realinv` | Real gross private domestic investment | Annual Growth Rate |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: '| `realgovt` | Real federal expenditures and gross investment | Annual Growth
    Rate |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: '| `realdpi` | Real private disposable income | Annual Growth Rate |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: '| `m1` | M1 nominal money stock | Annual Growth Rate |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
- en: '| `tbilrate` | Monthly 3 treasury bill rate | Level |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
- en: '| `unemp` | Seasonally adjusted unemployment rate (%) | Level |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
- en: '| `infl` | Inflation rate | Level |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
- en: '| `realint` | Real interest rate | Level |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: To obtain a binary target variable, we compute the 20-quarter rolling average
    of the annual growth rate of quarterly real GDP. We then assign 1 if current growth
    exceeds the moving average and 0 otherwise. Finally, we shift the indicator variables
    to align next quarter's outcome with the current quarter.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'We use an intercept and convert the quarter values to dummy variables and train
    the logistic regression model as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This produces the following summary for our model with 198 observations and
    13 variables, including intercept:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8e2376b-b48e-4f53-8694-0f337f29b986.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
- en: Logit Regression results
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: The summary indicates that the model has been trained using maximum likelihood
    and provides the maximized value of the log-likelihood function at -67.9.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: The LL-Null value of -136.42 is the result of the maximized log-likelihood function
    when only an intercept is included. It forms the basis for the pseudo-R² statisticand
    the Log-**Likelihood Ratio** (**LLR**) test.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudo-R^(2 )statistic is a substitute for the familiar R² available under
    least squares. It is computed based on the ratio of the maximized log-likelihood
    function for the null model m[0] and the full model m[1] as follows:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58c66c64-3260-4ce4-83a2-6ed4ec540c23.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
- en: The values vary from 0 (when the model does not improve the likelihood) to 1
    where the model fits perfectly and the log-likelihood is maximized at 0\. Consequently,
    higher values indicate a better fit.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'The LLR test generally compares a more restricted model and is computed as:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0849e323-b0ee-4890-9cdd-9d93940804de.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
- en: The null hypothesis is that the restricted model performs better but the low
    p-value suggests that we can reject this hypothesis and prefer the full model
    over the null model. This is similar to the F-test for linear regression (where
    we can also use the LLR test when we estimate the model using MLE).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'The z-statistic plays the same role as the t-statistic in the linear regression
    output and is equally computed as the ratio of the coefficient estimate and its
    standard error. The p-values also indicate the probability of observing the test
    statistic assuming the null hypothesis *H[0] : β = 0* that the population coefficient
    is zero. We can reject this hypothesis for the `intercept`, `realcons`, `realinv`,
    `realgovt`, `realdpi`, and `unemp`.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: How to use logistic regression for prediction
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The lasso L[1] penalty and the ridge L[2] penalty can both be used with logistic
    regression. They have the same shrinkage effect as we have just discussed, and
    the lasso can again be used for variable selection with any linear regression
    model.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Just as with linear regression, it is important to standardize the input variables
    as the regularized models are scale sensitive. The regularization hyperparameter
    also requires tuning using cross-validation as in the linear regression case.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: How to predict price movements using sklearn
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We continue the price prediction example but now we binarize the outcome variable
    so that it takes on the value 1 whenever the 10-day return is positive and 0 otherwise;
    see the notebook `logistic_regression.ipynb` in the sub directory `stock_price_prediction`:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'With this new categorical outcome variable, we can now train a logistic regression
    using the default L[2] regularization. For logistic regression, the regularization
    is formulated inversely to linear regression: higher values for λ imply less regularization
    and vice versa. We evaluate 11 parameter values using cross validation as follows:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We then use the `roc_auc_score` discussed in the previous chapter to compare
    the predictive accuracy across the various regularization parameters:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can again plot the AUC result for the range of hyperparameter values alongside
    the coefficient path that shows the improvements in predictive accuracy as the
    coefficients are a bit shrunk at the optimal regularization value 10²:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2e88587-b9b2-4e2d-a204-712534c693d7.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
- en: AUC and Logistic Ridge path
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the first machine learning models using the important
    baseline case of linear models for regression and classification. We explored
    the formulation of the objective functions for both tasks, learned about various
    training methods, and learned how to use the model for both inference and prediction.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: We applied these new machine learning techniques to estimate linear factor models
    that are very useful to manage risks, assess new alpha factors, and attribute
    performance. We also applied linear regression and classification to accomplish
    the first predictive task of predicting stock returns in absolute and directional
    terms.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the important topic of linear time series
    models that are designed to capture serial correlation patterns in the univariate
    and multivariate case. We will also learn about new trading strategies as we explore
    pairs trading based on the concept of cointegration that captures dynamic correlation
    among two stock price series.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
