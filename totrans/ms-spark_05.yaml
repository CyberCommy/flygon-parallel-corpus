- en: Chapter 5. Apache Spark GraphX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, I want to examine the Apache Spark GraphX module, and graph
    processing in general. I also want to briefly examine graph-based storage by looking
    at the graph database called Neo4j. So, this chapter will cover the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: GraphX coding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mazerunner for Neo4j
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GraphX coding section, written in Scala, will provide a series of graph
    coding examples. The work carried out on the experimental Mazerunner product by
    Kenny Bastani, which I will also examine, ties the two topics together in one
    practical example. It provides an example prototype-based on Docker to replicate
    data between Apache Spark GraphX, and Neo4j storage.
  prefs: []
  type: TYPE_NORMAL
- en: Before writing code in Scala to use the Spark GraphX module, I think it would
    be useful to provide an overview of what a graph actually is in terms of graph
    processing. The following section provides a brief introduction using a couple
    of simple graphs as examples.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A graph can be considered to be a data structure, which consists of a group
    of vertices, and edges that connect them. The vertices or nodes in the graph can
    be objects or perhaps, people, and the edges are the relationships between them.
    The edges can be directional, meaning that the relationship operates from one
    node to the next. For instance, node A is the father of node B.
  prefs: []
  type: TYPE_NORMAL
- en: In the following diagram, the circles represent the vertices or nodes (**A**
    to **D**), whereas the thick lines represent the edges, or relationships between
    them (**E1** to **E6**). Each node, or edge may have properties, and these values
    are represented by the associated grey squares (**P1** to **P7**).
  prefs: []
  type: TYPE_NORMAL
- en: So, if a graph represented a physical route map for route finding, then the
    edges might represent minor roads or motorways. The nodes would be motorway junctions,
    or road intersections. The node and edge properties might be the road type, speed
    limit, distance, and the cost and grid locations.
  prefs: []
  type: TYPE_NORMAL
- en: There are many types of graph implementation, but some examples are fraud modeling,
    financial currency transaction modeling, social modeling (as in friend-to-friend
    connections on Facebook), map processing, web processing, and page ranking.
  prefs: []
  type: TYPE_NORMAL
- en: '![Overview](img/B01989_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The previous diagram shows a generic example of a graph with associated properties.
    It also shows that the edge relationships can be directional, that is, the **E2**
    edge acts from node **B** to node **C**. However, the following example uses family
    members, and the relationships between them to create a graph. Note that there
    can be multiple edges between two nodes or vertices. For instance, the husband-and-wife
    relationships between **Mike** and **Sarah**. Also, it is possible that there
    could be multiple properties on a node or edge.
  prefs: []
  type: TYPE_NORMAL
- en: '![Overview](img/B01989_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, in the previous example, the **Sister** property acts from node 6 **Flo**,
    to node 1, **Mike**. These are simple graphs to explain the structure of a graph,
    and the element nature. Real graph applications can reach extreme sizes, and require
    both, distributed processing, and storage to enable them to be manipulated. Facebook
    is able to process graphs, containing over 1 trillion edges using **Apache Giraph**
    (source: Avery Ching-Facebook). Giraph is an Apache Hadoop eco-system tool for
    graph processing, which has historically based its processing on Map Reduce, but
    now uses TinkerPop, which will be introduced in [Chapter 6](ch06.html "Chapter 6. Graph-based
    Storage"), *Graph-based Storage*. Although this book concentrates on Apache Spark,
    the number of edges provides a very impressive indicator of the size that a graph
    can reach.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, I will examine the use of the Apache Spark GraphX module
    using Scala.
  prefs: []
  type: TYPE_NORMAL
- en: GraphX coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will examine Apache Spark GraphX programming in Scala, using the
    family relationship graph data sample, which was shown in the last section. This
    data will be stored on HDFS, and will be accessed as a list of vertices and edges.
    Although this data set is small, the graphs that you build in this way could be
    very large. I have used HDFS for storage, because if your graph scales to the
    big data scale, then you will need some type of distributed and redundant storage.
    As this chapter shows by way of example, that could be HDFS. Using the Apache
    Spark SQL module, the storage could also be Apache Hive; see [Chapter 4](ch04.html
    "Chapter 4. Apache Spark SQL"), *Apache Spark SQL*, for details.
  prefs: []
  type: TYPE_NORMAL
- en: Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I have used the hadoop Linux account on the server `hc2nn` to develop the Scala-based
    GraphX code. The structure for SBT compilation follows the same pattern as the
    previous examples, with the code tree existing in a subdirectory named `graphx`,
    where an `sbt` configuration file called `graph.sbt` resides:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The source code lives, as expected, under a subtree of this level called `src/main/scala`,
    and contains five code samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In each graph-based example, the Scala file uses the same code to load data
    from HDFS, and to create a graph; but then, each file provides a different facet
    of GraphX-based graph processing. As a different Spark module is being used in
    this chapter, the `sbt` configuration file `graph.sbt` has been changed to support
    this work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The contents of the `graph.sbt` file are shown previously, via the Linux `more`
    command. There are only two changes here to note from previous examples—the value
    of name has changed to represent the content. Also, more importantly, the Spark
    GraphX 1.0.0 library has been added as a library dependency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two data files have been placed on HDFS, under the `/data/spark/graphx/` directory.
    They contain the data that will be used for this section in terms of the vertices,
    and edges that make up a graph. As the Hadoop file system `ls` command shows next,
    the files are called `graph1_edges.cvs` and `graph1_vertex.csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `vertex` file, shown next, via a Hadoop file system `cat` command, contains
    just six lines, representing the graph used in the last section. Each vertex represents
    a person, and has a vertex ID number, a name and an age value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The edge file contains a set of directed edge values in the form of source
    vertex ID, destination vertex ID, and relationship. So, record one forms a Sister
    relationship between `Flo` and `Mike`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Having explained the sbt environment, and the HDFS-based data, we are now ready
    to examine some of the GraphX code samples. As in the previous examples, the code
    can be compiled, and packaged as follows from the `graphx` subdirectory. This
    creates a JAR called `graph-x_2.10-1.0.jar` from which the example applications
    can be run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Creating a graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section will explain the generic Scala code, up to the point of creating
    a GraphX graph, from the HDFS-based data. This will save time, as the same code
    is reused in each example. Once this is explained, I will concentrate on the actual
    graph-based manipulation in each code example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The generic code starts by importing the Spark context, graphx, and RDD functionality
    for use in the Scala code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, an application is defined, which extends the `App` class, and the application
    name changes, for each example, from `graph1` to `graph5`. This application name
    will be used when running the application using `spark-submit`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The data files are defined in terms of the HDFS server and port, the path that
    they reside under in HDFS and their file names. As already mentioned, there are
    two data files that contain the `vertex` and `edge` information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The Spark Master URL is defined, as is the application name, which will appear
    in the Spark user interface when the application runs. A new Spark configuration
    object is created, and the URL and name are assigned to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'A new Spark context is created using the configuration that was just defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The vertex information from the HDFS-based file is then loaded into an RDD-based
    structure called `vertices` using the `sparkCxt.textFile` method. The data is
    stored as a long `VertexId`, and strings to represent the person''s name and age.
    The data lines are split by commas as this is CSV based-data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Similary, the HDFS-based edge data is loaded into an RDD-based data structure
    called `edges`. The CSV-based data is again split by comma values. The first two
    data values are converted into Long values, as they represent the source and destination
    vertex ID''s. The final value, representing the relationship of the edge, is left
    as a string. Note that each record in the RDD structure edges is actually now
    an `Edge` record:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'A default value is defined in case a connection, or a vertex is missing, then
    the graph is constructed from the RDD-based structures—`vertices`, `edges`, and
    the `default` record:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This creates a GraphX-based structure called `graph`, which can now be used
    for each of the examples. Remember that although these data samples are small,
    you can create extremely large graphs using this approach. Many of these algorithms
    are iterative applications, for instance, PageRank and Triangle Count, and as
    a result, the programs will generate many iterative Spark jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – counting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The graph has been loaded, and we know the data volumes in the data files,
    but what about the data content in terms of vertices, and edges in the actual
    graph itself? It is very simple to extract this information by using the vertices,
    and the edges count function as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the `graph1` example, using the example name and the JAR file created
    previously, will provide the count information. The master URL is supplied to
    connect to the Spark cluster, and some default parameters are supplied for the
    executor memory, and the total executor cores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The Spark cluster job called `graph1` provides the following output, which
    is as expected and also, it matches the data files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Example 2 – filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What happens if we need to create a subgraph from the main graph, and filter
    by the person''s age or relationships? The example code from the second example
    Scala file, `graph2`, shows how this can be done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The two example counts have been created from the main graph. The first filters
    the person-based vertices on the age, only taking those people who are greater
    than 40 years old. Notice that the `age` value, which was stored as a string,
    has been converted into a long for comparison. The previous second example filters
    the edges on the relationship property of `Mother` or `Father`. The two count
    values: `c1` and `c2` are created, and printed as the Spark output shows here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Example 3 – PageRank
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The PageRank algorithm provides a ranking value for each of the vertices in
    a graph. It makes the assumption that the vertices that are connected to the most
    edges are the most important ones. Search engines use PageRank to provide ordering
    for the page display during a web search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The previous example code creates a `tolerance` value, and calls the graph `pageRank`
    method using it. The vertices are then ranked into a new value ranking. In order
    to make the ranking more meaningful the ranking values are joined with the original
    vertices RDD. The `rankByPerson` value then contains the rank, vertex ID, and
    person's name.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PageRank result, held in `rankByPerson`, is then printed record by record,
    using a case statement to identify the record contents, and a format statement
    to print the contents. I did this, because I wanted to define the format of the
    rank value which can vary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the application is then shown here. As expected, `Mike` and
    `Sarah` have the highest rank, as they have the most relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Example 4 – triangle counting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The triangle count algorithm provides a vertex-based count of the number of
    triangles, associated with this vertex. For instance, vertex `Mike` (1) is connected
    to `Kate` (5), who is connected to `Sarah` (2); `Sarah` is connected to `Mike`
    (1) and so, a triangle is formed. This can be useful for route finding, where
    minimum, triangle-free, spanning tree graphs need to be generated for route planning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to execute a triangle count, and print it, is simple, as shown next.
    The graph `triangleCount` method is executed for the graph vertices. The result
    is saved in the value `tCount`, and then printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of the application job show that the vertices called, `Flo` (4)
    and `Jim` (6), have no triangles, whereas `Mike` (1) and `Sarah` (2) have the
    most, as expected, as they have the most relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Example 5 – connected components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a large graph is created from the data, it might contain unconnected subgraphs,
    that is, subgraphs that are isolated from each other, and contain no bridging
    or connecting edges between them. This algorithm provides a measure of this connectivity.
    It might be important, depending upon your processing, to know that all the vertices
    are connected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Scala code, for this example, calls two graph methods: `connectedComponents`,
    and `stronglyConnectedComponents`. The strong method required a maximum iteration
    count, which has been set to `1000`. These counts are acting on the graph vertices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The vertex counts are then joined with the original vertex records, so that
    the connection counts can be associated with the vertex information, such as the
    person''s name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected for the `connectedComponents` algorithm, the results show that
    for each vertex, there is only one component. This means that all the vertices
    are the members of a single graph, as the graph diagram earlier in the chapter
    showed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `stronglyConnectedComponents` method gives a measure of the connectivity
    in a graph, taking into account the direction of the relationships between them.
    The results for the `stronglyConnectedComponents` algorithm output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You might notice from the graph that the relationships, `Sister` and `Friend`,
    act from vertices `Flo` (6) and `Jim` (4), to `Mike` (1) as the edge and vertex
    data shows here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the strong method output shows that for most vertices, there is only one
    graph component signified by the `1` in the second column. However, vertices `4`
    and `6` are not reachable due to the direction of their relationship, and so they
    have a vertex ID instead of a component ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Mazerunner for Neo4j
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, you have been shown how to write Apache Spark graphx
    code in Scala to process the HDFS-based graph data. You have been able to execute
    the graph-based algorithms, such as PageRank, and triangle counting. However,
    this approach has a limitation. Spark does not have storage, and storing graph-based
    data in the flat files on HDFS does not allow you to manipulate it in its place
    of storage. For instance, if you had data stored in a relational database, you
    could use SQL to interrogate it in place. Databases such as Neo4j are graph databases.
    This means that their storage mechanisms and data access language act on graphs.
    In this section, I want to take a look at the work done on Mazerunner, created
    as a GraphX Neo4j processing prototype by Kenny Bastani.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure describes the Mazerunner architecture. It shows that data
    in Neo4j is exported to HDFS, and processed by GraphX via a notification process.
    The GraphX data updates are then saved back to HDFS as a list of key value updates.
    These changes are then propagated to Neo4j to be stored. The algorithms in this
    prototype architecture are accessed via a Rest based HTTP URL, which will be shown
    later. The point here though, is that algorithms can be run via processing in
    graphx, but the data changes can be checked via Neo4j database cypher language
    queries. Kenny''s work and further details can be found at: [http://www.kennybastani.com/2014/11/using-apache-spark-and-neo4j-for-big.html](http://www.kennybastani.com/2014/11/using-apache-spark-and-neo4j-for-big.html).'
  prefs: []
  type: TYPE_NORMAL
- en: This section will be dedicated to explaining the Mazerunner architecture, and
    will show, with the help of an example, how it can be used. This architecture
    provides a unique example of GraphX-based processing, coupled with graph-based
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mazerunner for Neo4j](img/B01989_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Installing Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process for installing the Mazerunner example code is described via [https://github.com/kbastani/neo4j-mazerunner](https://github.com/kbastani/neo4j-mazerunner).
  prefs: []
  type: TYPE_NORMAL
- en: 'I have used the 64 bit Linux Centos 6.5 machine `hc1r1m1` for the install.
    The Mazerunner example uses the Docker tool, which creates virtual containers
    with a small foot print for running HDFS, Neo4j, and Mazerunner in this example.
    First, I must install Docker. I have done this, as follows, using the Linux root
    user via `yum` commands. The first command installs the `docker-io` module (the
    docker name was already used for CentOS 6.5 by another application):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'I needed to enable the `public_ol6_latest` repository, and install the `device-mapper-event-libs`
    package, as I found that my current lib-device-mapper, which I had installed,
    wasn''t exporting the symbol Base that Docker needed. I executed the following
    commands as `root`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The actual error that I encountered was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'I can then check that Docker will run by checking the Docker version number
    with the following call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'I can start the Linux docker service using the following service command. I
    can also force Docker to start on Linux server startup using the following `chkconfig`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The three Docker images (HDFS, Mazerunner, and Neo4j) can then be downloaded.
    They are large, so this may take some time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Once downloaded, the Docker containers can be started in the order; HDFS, Mazerunner,
    and then Neo4j. The default Neo4j movie database will be loaded and the Mazerunner
    algorithms run using this data. The HDFS container starts as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The Mazerunner service container starts as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is long, so I will not include it all here, but you will see no
    errors. There also comes a line, which states that the install is waiting for
    messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to start the Neo4j container, I need the install to create a new Neo4j
    database for me, as this is a first time install. Otherwise on restart, I would
    just supply the path of the database directory. Using the `link` command, the
    Neo4j container is linked to the HDFS and Mazerunner containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'By checking the `neo4j/data` path, I can now see that a database directory,
    named `graph.db` has been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'I can then use the following `docker inspect` command, which the container-based
    IP address and the Docker-based Neo4j container is making available. The `inspect`
    command supplies me with the local IP address that I will need to access the Neo4j
    container. The `curl` command, along with the port number, which I know from Kenny''s
    website, will default to `7474`, shows me that the Rest interface is running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The Neo4j browser
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The rest of the work in this section will now be carried out using the Neo4j
    browser URL, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://172.17.0.5:7474/browser`.'
  prefs: []
  type: TYPE_NORMAL
- en: This is a local, Docker-based IP address that will be accessible from the `hc1r1m1`
    server. It will not be visible on the rest of the local intranet without further
    network configuration.
  prefs: []
  type: TYPE_NORMAL
- en: This will show the default Neo4j browser page. The Movie graph can be installed
    by following the movie link here, selecting the Cypher query, and executing it.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Neo4j browser](img/B01989_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The data can then be interrogated using Cypher queries, which will be examined
    in more depth in the next chapter. The following figures are supplied along with
    their associated Cypher queries, in order to show that the data can be accessed
    as graphs that are displayed visually. The first graph shows a simple Person to
    Movie relationship, with the relationship details displayed on the connecting
    edges.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Neo4j browser](img/B01989_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The second graph, provided as a visual example of the power of Neo4j, shows
    a far more complex cypher query, and resulting graph. This graph states that it
    contains 135 nodes and 180 relationships. These are relatively small numbers in
    processing terms, but it is clear that the graph is becoming complex.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Neo4j browser](img/B01989_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following figures show the Mazerunner example algorithms being called via
    an HTTP Rest URL. The call is defined by the algorithm to be called, and the attribute
    that it is going to act upon within the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://localhost:7474/service/mazerunner/analysis/{algorithm}/{attribute}`.'
  prefs: []
  type: TYPE_NORMAL
- en: So for instance, as the next section will show, this generic URL can be used
    to run the PageRank algorithm by setting `algorithm=pagerank`. The algorithm will
    operate on the `follows` relationship by setting `attribute=FOLLOWS`. The next
    section will show how each Mazerunner algorithm can be run along with an example
    of the Cypher output.
  prefs: []
  type: TYPE_NORMAL
- en: The Mazerunner algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section shows how the Mazerunner example algorithms may be run using the
    Rest based HTTP URL, which was shown in the last section. Many of these algorithms
    have already been examined, and coded in this chapter. Remember that the interesting
    thing occurring in this section is that data starts in Neo4j, it is processed
    on Spark with GraphX, and then is updated back into Neo4j. It looks simple, but
    there are underlying processes doing all of the work. In each example, the attribute
    that the algorithm has added to the graph is interrogated via a Cypher query.
    So, each example isn't so much about the query, but that the data update to Neo4j
    has occurred.
  prefs: []
  type: TYPE_NORMAL
- en: The PageRank algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first call shows the PageRank algorithm, and the PageRank attribute being
    added to the movie graph. As before, the PageRank algorithm gives a rank to each
    vertex, depending on how many edge connections it has. In this case, it is using
    the `FOLLOWS` relationship for processing.
  prefs: []
  type: TYPE_NORMAL
- en: '![The PageRank algorithm](img/B01989_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The following image shows a screenshot of the PageRank algorithm result. The
    text at the top of the image (starting with `MATCH`) shows the cypher query, which
    proves that the PageRank property has been added to the graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![The PageRank algorithm](img/B01989_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The closeness centrality algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The closeness algorithm attempts to determine the most important vertices in
    the graph. In this case, the `closeness` attribute has been added to the graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![The closeness centrality algorithm](img/B01989_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The following image shows a screenshot of the closeness algorithm result. The
    text at the top of the image (starting with `MATCH`) shows the Cypher query, which
    proves that the `closeness_centrality` property has been added to the graph. Note
    that an alias called `closeness` has been used in this Cypher query, to represent
    the `closeness_centrality` property, and so the output is more presentable.
  prefs: []
  type: TYPE_NORMAL
- en: '![The closeness centrality algorithm](img/B01989_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The triangle count algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `triangle_count` algorithm has been used to count triangles associated with
    vertices. The `FOLLOWS` relationship has been used, and the `triangle_count` attribute
    has been added to the graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![The triangle count algorithm](img/B01989_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The following image shows a screenshot of the triangle algorithm result. The
    text at the top of the image (starting with `MATCH`) shows the cypher query, which
    proves that the `triangle_count` property has been added to the graph. Note that
    an alias called **tcount** has been used in this cypher query, to represent the
    `triangle_count` property, and so the output is more presentable.
  prefs: []
  type: TYPE_NORMAL
- en: '![The triangle count algorithm](img/B01989_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The connected components algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The connected components algorithm is a measure of how many actual components
    exist in the graph data. For instance, the data might contain two subgraphs with
    no routes between them. In this case, the `connected_components` attribute has
    been added to the graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![The connected components algorithm](img/B01989_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The following image shows a screenshot of the connected component algorithm
    result. The text at the top of the image (starting with `MATCH`) shows the cypher
    query, which proves that the `connected_components` property has been added to
    the graph. Note that an alias called **ccomp** has been used in this cypher query,
    to represent the `connected_components` property, and so the output is more presentable.
  prefs: []
  type: TYPE_NORMAL
- en: '![The connected components algorithm](img/B01989_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The strongly connected components algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The strongly connected components algorithm is very similar to the connected
    components algorithm. Subgraphs are created from the graph data using the directional
    `FOLLOWS` relationship. Multiple subgraphs are created until all the graph components
    are used. These subgraphs form the strongly connected components. As seen here,
    a `strongly_connected_components` attribute has been added to the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The strongly connected components algorithm](img/B01989_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The following image shows a screenshot of the strongly connected component algorithm
    result. The text at the top of the image (starting with `MATCH`) shows the cypher
    query, which proves that the `strongly_connected_components` connected component
    property has been added to the graph. Note that an alias called **sccomp** has
    been used in this cypher query, to represent the `strongly_connected_components`
    property, and so the output is more presentable.
  prefs: []
  type: TYPE_NORMAL
- en: '![The strongly connected components algorithm](img/B01989_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has shown, with the help of examples, how the Scala-based code
    can be used to call GraphX algorithms in Apache Spark. Scala has been used, because
    it requires less code to develop the examples, which saves time. A Scala-based
    shell can be used, and the code can be compiled into Spark applications. Examples
    of the application compilation and configuration have been supplied using the
    SBT tool. The configuration and the code examples from this chapter will also
    be available for download with the book.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the Mazerunner example architecture (developed by Kenny Bastani while
    at Neo) for Neo4j and Apache Spark has been introduced. Why is Mazerunner important?
    It provides an example of how a graph-based database can be used for graph storage,
    while Apache Spark is used for graph processing. I am not suggesting that Mazerunner
    be used in a production scenario at this time. Clearly, more work needs to be
    done to make this architecture ready for release. However, graph-based storage,
    when associated with the graph-based processing within a distributed environment,
    offers the option to interrogate the data using a query language such as Cypher
    from Neo4j.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that you have found this chapter useful. The next chapter will delve
    into graph-based storage in more depth. You can now delve into further GraphX
    coding, try to run the examples provided, and try modifying the code, so that
    you become familiar with the development process.
  prefs: []
  type: TYPE_NORMAL
