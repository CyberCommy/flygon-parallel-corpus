- en: Extracting Patterns from Clickstream Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从点击流数据中提取模式
- en: When collecting real-world data between individual measures or events, there
    are usually very intricate and highly complex relationships to observe. The guiding
    example for this chapter is the observation of click events that users generate
    on a website and its subdomains. Such data is both interesting and challenging
    to investigate. It is interesting, as there are usually many *patterns* that groups
    of users show in their browsing behavior and certain *rules *they might follow.
    Gaining insights about user groups, in general, is of interest, at least for the
    company running the website and might be the focus of their data science team.
    Methodology aside, putting a production system in place that can detect patterns
    in real time, for instance, to find malicious behavior, can be very challenging
    technically. It is immensely valuable to be able to understand and implement both
    the algorithmic and technical sides.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集个别测量或事件之间的真实世界数据时，通常会有非常复杂和高度复杂的关系需要观察。本章的指导示例是用户在网站及其子域上生成的点击事件的观察。这样的数据既有趣又具有挑战性。它有趣，因为通常有许多*模式*显示出用户在其浏览行为中的行为和某些*规则*。至少对于运行网站的公司和可能成为他们数据科学团队的焦点，了解用户群体的见解是有趣的。方法论方面，建立一个能够实时检测模式的生产系统，例如查找恶意行为，技术上可能非常具有挑战性。能够理解和实施算法和技术两方面是非常有价值的。
- en: 'In this chapter, we will look into two topics in depth: doing *pattern mining *and
    working with *streaming data *in Spark. The chapter is split up into two main
    sections. In the first, we will introduce the three available pattern mining algorithms
    that Spark currently comes with and apply them to an interesting dataset. In the
    second, we will take a more technical view on things and address the core problems
    that arise when deploying a streaming data application using algorithms from the
    first part. In particular, you will learn the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入研究两个主题：在Spark中进行*模式挖掘*和处理*流数据*。本章分为两个主要部分。在第一部分中，我们将介绍Spark目前提供的三种可用模式挖掘算法，并将它们应用于一个有趣的数据集。在第二部分中，我们将更加技术化地看待问题，并解决使用第一部分算法部署流数据应用时出现的核心问题。特别是，您将学习以下内容：
- en: The basic principles of frequent pattern mining.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 频繁模式挖掘的基本原则。
- en: Useful and relevant data formats for applications.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序的有用和相关数据格式。
- en: How to load and analyze a clickstream data set generated from user activity
    on [http://MSNBC.com](http://MSNBC.com).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何加载和分析用户在[http://MSNBC.com](http://MSNBC.com)上生成的点击流数据集。
- en: Understanding and comparing three pattern mining algorithms available in Spark,
    namely *FP-growth, association rules*, and **prefix span.**
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark中了解和比较三种模式挖掘算法，即*FP-growth，关联规则*和**前缀跨度**。
- en: How to apply these algorithms to MSNBC click data and other examples to identify
    the relevant patterns.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将这些算法应用于MSNBC点击数据和其他示例以识别相关模式。
- en: The very basics of *Spark Streaming* and what use cases can be covered by it.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Spark Streaming*的基础知识以及它可以涵盖哪些用例。'
- en: How to put any of the preceding algorithms into production by deploying them
    with Spark Streaming.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过使用Spark Streaming将任何先前的算法投入生产。
- en: Implementing a more realistic streaming application with click events aggregated
    on the fly.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用实时聚合的点击事件实现更实际的流应用程序。
- en: By construction, this chapter is more technically involved towards the end,
    but with *Spark Streaming* it also allows us to introduce yet another very important
    tool from the Spark ecosphere. We start off by presenting some of the basic questions
    of pattern mining and then discuss how to address them.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通过构建，本章在技术上更加涉及到了末尾，但是通过*Spark Streaming*，它也允许我们介绍Spark生态系统中另一个非常重要的工具。我们首先介绍模式挖掘的一些基本问题，然后讨论如何解决这些问题。
- en: Frequent pattern mining
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 频繁模式挖掘
- en: 'When presented with a new data set, a natural sequence of questions is:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当面对一个新的数据集时，一个自然的问题序列是：
- en: What kind of data do we look at; that is, what structure does it have?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们看什么样的数据；也就是说，它有什么结构？
- en: Which observations in the data can be found frequently; that is, which patterns
    or rules can we identify within the data?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中可以经常发现哪些观察结果；也就是说，我们可以在数据中识别出哪些模式或规则？
- en: How do we assess what is frequent; that is, what are the good measures of relevance
    and how do we test for it?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何评估什么是频繁的；也就是说，什么是良好的相关性度量，我们如何测试它？
- en: On a very high level, frequent pattern mining addresses precisely these questions.
    While it's very easy to dive head first into more advanced machine learning techniques,
    these pattern mining algorithms can be quite informative and help build an intuition
    about the data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常高的层次上，频繁模式挖掘正是在解决这些问题。虽然很容易立即深入研究更高级的机器学习技术，但这些模式挖掘算法可以提供相当多的信息，并帮助建立对数据的直觉。
- en: To introduce some of the key notions of frequent pattern mining, let's first
    consider a somewhat prototypical example for such cases, namely shopping carts.
    The study of customers being interested in and buying certain products has been
    of prime interest to marketers around the globe for a very long time. While online
    shops certainly do help in further analyzing customer behavior, for instance,
    by tracking the browsing data within a shopping session, the question of what
    items have been bought and what patterns in buying behavior can be found applies
    to purely offline scenarios as well. We will see a more involved example of clickstream
    data accumulated on a website soon; for now, we will work under the assumption
    that only the events we can track are the actual payment transactions of an item.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了介绍频繁模式挖掘的一些关键概念，让我们首先考虑一个典型的例子，即购物车。对顾客对某些产品感兴趣并购买的研究长期以来一直是全球营销人员的主要关注点。虽然在线商店确实有助于进一步分析顾客行为，例如通过跟踪购物会话中的浏览数据，但已购买的物品以及购买行为中的模式的问题也适用于纯线下场景。我们很快将看到在网站上积累的点击流数据的更复杂的例子；目前，我们将在假设我们可以跟踪的事件中只有物品的实际支付交易的情况下进行工作。
- en: 'Just this given data, for instance, for groceries shopping carts in supermarkets
    or online, leads to quite a few interesting questions, and we will focus mainly
    on the following three:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于超市或在线杂货购物车的给定数据，会引发一些有趣的问题，我们主要关注以下三个问题：
- en: '*Which items are frequently bought together?* For instance, there is anecdotal
    evidence suggesting that beer and diapers are often bought together in one shopping
    session. Finding patterns of products that often go together may, for instance,
    allow a shop to physically place these products closer to each other for an increased
    shopping experience or promotional value even if they don''t belong together at
    first sight. In the case of an online shop, this sort of analysis might be the
    base for a simple recommender system.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*哪些物品经常一起购买？*例如，有传闻证据表明啤酒和尿布经常在同一次购物会话中一起购买。发现经常一起购买的产品的模式可能允许商店将这些产品放在彼此更近的位置，以增加购物体验或促销价值，即使它们乍一看并不属于一起。在在线商店的情况下，这种分析可能是简单推荐系统的基础。'
- en: Based on the previous question, *are there any interesting implications or rules
    to observe in shopping behaviour?,* continuing with the shopping cart example,
    can we establish associations such as *if bread and butter have been bought, we
    also often find cheese in the shopping cart*? Finding such association rules can
    be of great interest, but also need more clarification of what we consider to
    be *often*, that is, what does frequent mean.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于前面的问题，*在购物行为中是否有任何有趣的影响或规则？*继续以购物车为例，我们是否可以建立关联，比如*如果购买了面包和黄油，我们也经常在购物车中找到奶酪*？发现这样的关联规则可能非常有趣，但也需要更多澄清我们认为的*经常*是什么意思，也就是，频繁意味着什么。
- en: Note that, so far, our shopping carts were simply considered a *bag of items* without
    additional structure. At least in the online shopping scenario, we can endow data
    with more information. One aspect we will focus on is that of the *sequentiality *of
    items; that is, we will take note of the order in which the products have been
    placed into the cart. With this in mind, similar to the first question, one might
    ask, *which sequence of items can often be found in our transaction data? *For
    instance, larger electronic devices bought might be followed up by additional
    utility items.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，到目前为止，我们的购物车只是被简单地视为一个*物品袋*，没有额外的结构。至少在在线购物的情况下，我们可以为数据提供更多信息。我们将关注物品的*顺序性*;也就是说，我们将注意产品被放入购物车的顺序。考虑到这一点，类似于第一个问题，人们可能会问，*我们的交易数据中经常可以找到哪些物品序列？*例如，购买大型电子设备后可能会跟随购买额外的实用物品。
- en: The reason we focus on these three questions in particular is that Spark MLlib
    comes with precisely three pattern mining algorithms that roughly correspond to
    the aforementioned questions by their ability to answer them. Specifically, we
    will carefully introduce *FP-growth*, *association rules*, and *prefix span, *in
    that order, to address these problems and show how to solve them using Spark.
    Before doing so, let's take a step back and formally introduce the concepts we
    have been motivated for so far, alongside a running example. We will refer to
    the preceding three questions throughout the following subsection.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以特别关注这三个问题，是因为Spark MLlib正好配备了三种模式挖掘算法，它们大致对应于前面提到的问题，能够回答这些问题。具体来说，我们将仔细介绍*FP-growth*、*关联规则*和*前缀跨度*，以解决这些问题，并展示如何使用Spark解决这些问题。在这样做之前，让我们退一步，正式介绍到目前为止我们已经为之努力的概念，以及一个运行的例子。我们将在接下来的小节中提到前面的三个问题。
- en: Pattern mining terminology
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模式挖掘术语
- en: We will start with a set of items *I = {a[1], ..., a[n]}*, which serves as the
    base for all the following concepts. A *transaction* T is just a set of items
    in I, and we say that T is a transaction of length *l* if it contains *l* item.
    A *transaction database* D is a database of transaction IDs and their corresponding
    transactions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一组项目*I = {a[1], ..., a[n]}*开始，这将作为所有以下概念的基础。*事务* T只是I中的一组项目，如果它包含*l*个项目，则我们说T是长度为*l*的事务。*事务数据库*
    D是事务ID和它们对应的事务的数据库。
- en: 'To give a concrete example of this, consider the following situation. Assume
    that the full item set to shop from is given by *I = {bread, cheese, ananas, eggs,
    donuts, fish, pork, milk, garlic, ice cream, lemon, oil, honey, jam, kale, salt}*.
    Since we will look at a lot of item subsets, to make things more readable later
    on, we will simply abbreviate these items by their first letter, that is, we''ll
    write *I = {b, c, a, e, d, f, p, m, g, i, l, o, h, j, k, s}*. Given these items,
    a small transaction database D could look as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '| **Transaction ID** | **Transaction** |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| 1 | a, c, d, f, g, i, m, p |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| 2 | a, b, c, f, l, m, o |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| 3 | b, f, h, j, o |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| 4 | b, c, k, s, p |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| 5 | a, c, e, f, l, m, n, p |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: 'Table 1: A small shopping cart database with five transactions'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Frequent pattern mining problem
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given the definition of a transaction database, a *pattern* P is a *transaction
    contained in the transactions in D* and the support, *supp(P)*, of the pattern
    is the number of transactions for which this is true, divided or normalized by
    the number of transactions in D:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '*supp(s) = supp[D](s) = |{ s'' ∈ S | s <s''}| / |D|*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: We use the *<* symbol to denote *s* as a subpattern of *s'* or, conversely,
    call *s'* a superpattern of *s*. Note that in the literature, you will sometimes
    also find a slightly different version of support that does not normalize the
    value. For example, the pattern *{a, c, f}* can be found in transactions 1, 2,
    and 5\. This means that *{a, c, f}* is a pattern of support *0.6* in our database
    D of five items.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Support is an important notion, as it gives us a first example of measuring
    the frequency of a pattern, which, in the end, is what we are after. In this context,
    for a given minimum support threshold *t*, we say *P* is a frequent pattern if
    and only if *supp(P)* is at least *t*. In our running example, the frequent patterns
    of length 1 and minimum support *0.6* are *{a}*, *{b}*, *{c}*, *{p}*, and *{m}*
    with support 0.6 and *{f}* with support 0.8\. In what follows, we will often drop
    the brackets for items or patterns and write *f* instead of *{f}*, for instance.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Given a minimum support threshold, the problem of finding all the frequent patterns
    is called the *frequent pattern mining problem* and it is, in fact, the formalized
    version of the aforementioned first question. Continuing with our example, we
    have found all frequent patterns of length 1 for *t = 0.6* already. How do we
    find longer patterns? On a theoretical level, given unlimited resources, this
    is not much of a problem, since all we need to do is count the occurrences of
    items. On a practical level, however, we need to be smart about how we do so to
    keep the computation efficient. Especially for databases large enough for Spark
    to come in handy, it can be very computationally intense to address the frequent
    pattern mining problem.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'One intuitive way to go about this is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Find all the frequent patterns of length 1, which requires one full database
    scan. This is how we started with in our preceding example.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For patterns of length 2, generate all the combinations of frequent 1-patterns,
    the so-called candidates*,* and test if they exceed the minimum support by doing
    another scan of D.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Importantly, we do not have to consider the combinations of infrequent patterns,
    since patterns containing infrequent patterns can not become frequent. This rationale
    is called the **apriori principle***.*
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For longer patterns, continue this procedure iteratively until there are no
    more patterns left to combine.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This algorithm, using a generate-and-test approach to pattern mining and utilizing
    the apriori principle to bound combinations, is called the apriori algorithm.
    There are many variations of this baseline algorithm, all of which share similar
    drawbacks in terms of scalability. For instance, multiple full database scans
    are necessary to carry out the iterations, which might already be prohibitively
    expensive for huge data sets. On top of that, generating candidates themselves
    is already expensive, but computing their combinations might simply be infeasible.
    In the next section, we will see how a parallel version of an algorithm called
    *FP-growth*, available in Spark, can overcome most of the problems just discussed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种算法使用生成和测试方法进行模式挖掘，并利用先验原则来限制组合，称为先验算法。这种基线算法有许多变体，它们在可扩展性方面存在类似的缺点。例如，需要进行多次完整的数据库扫描来执行迭代，这对于庞大的数据集可能已经成本过高。此外，生成候选本身已经很昂贵，但计算它们的组合可能根本不可行。在下一节中，我们将看到Spark中的*FP-growth*算法的并行版本如何克服刚才讨论的大部分问题。
- en: The association rule mining problem
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关联规则挖掘问题
- en: 'To advance our general introduction of concepts, let''s next turn to *association
    rules, *as first introduced in *Mining Association Rules between Sets of Items
    in Large Databases*, available at [http://arbor.ee.ntu.edu.tw/~chyun/dmpaper/agrama93.pdf](http://arbor.ee.ntu.edu.tw/~chyun/dmpaper/agrama93.pdf).
    In contrast to solely counting the occurrences of items in our database, we now
    want to understand the rulesor implications of patterns. What I mean is, given
    a pattern *P[1]* and another pattern *P[2]*, we want to know whether *P[2]* is
    frequently present whenever *P[1]* can be found in *D*, and we denote this by
    writing *P[1 ]⇒ P[2]*. To make this more precise, we need a concept for rule frequency
    similar to that of support for patterns, namely *confidence. *For a rule *P[1 ]⇒ P[2]*,
    confidence is defined as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步介绍概念，让我们接下来转向*关联规则*，这是首次在*大型数据库中挖掘项集之间的关联规则*中引入的，可在[http://arbor.ee.ntu.edu.tw/~chyun/dmpaper/agrama93.pdf](http://arbor.ee.ntu.edu.tw/~chyun/dmpaper/agrama93.pdf)上找到。与仅计算数据库中项的出现次数相反，我们现在想要理解模式的规则或推论。我的意思是，给定模式*P[1]*和另一个模式*P[2]*，我们想知道在*D*中可以找到*P[1]*时，*P[2]*是否经常出现，我们用*P[1 ]⇒ P[2]*来表示这一点。为了更加明确，我们需要一个类似于模式支持的规则频率的概念，即*置信度*。对于规则*P[1 ]⇒ P[2]*，置信度定义如下：
- en: '*conf(P[1] ⇒ P[2]) = supp(P[1] ∪ P[2]) / supp(P[1])*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*conf(P[1] ⇒ P[2]) = supp(P[1] ∪ P[2]) / supp(P[1])*'
- en: This can be interpreted as the conditional support of *P[2]* given to *P[1]*;
    that is, if it were to restrict *D* to all the transactions supporting *P[1]*,
    the support of *P[2]* in this restricted database would be equal to *conf(P[1 ]⇒ P[2])*.
    We call *P[1 ]⇒ P[2]* a rule in *D* if it exceeds a minimum confidence threshold
    *t*, just as in the case of frequent patterns. Finding all the rules for a confidence
    threshold represents the formal answer to the second question, *association rule
    mining. *Moreover, in this situation, we call P[1 ]the *antecedent *and P[2] the *consequent *of
    the rule. In general, there is no restriction imposed on the structure of either
    the antecedent or the consequent. However, in what follows, we will assume that
    the consequent's length is 1, for simplicity.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以解释为*P[1]*给出*P[2]*的条件支持；也就是说，如果将*D*限制为支持*P[1]*的所有交易，那么在这个受限制的数据库中，*P[2]*的支持将等于*conf(P[1 ]⇒ P[2])*。如果它超过最小置信度阈值*t*，我们称*P[1 ]⇒ P[2]*为*D*中的规则，就像频繁模式的情况一样。找到置信度阈值的所有规则代表了第二个问题*关联规则挖掘*的正式答案。此外，在这种情况下，我们称*P[1 ]*为*前提*，*P[2]*为*结论*。通常，对前提或结论的结构没有限制。但在接下来的内容中，为简单起见，我们将假设结论的长度为1。
- en: 'In our running example, the pattern *{f, m}* occurs three times, while *{f,
    m, p}* is just present in two cases, which means that the rule *{f, m} **⇒ {p}*
    has confidence *2/3*. If we set the minimum confidence threshold to *t = 0.6*,
    we can easily check that the following association rules with an antecedent and
    consequent of length 1 are valid for our case:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的运行示例中，模式*{f，m}*出现了三次，而*{f，m，p}*只出现了两次，这意味着规则*{f，m}⇒{p}*的置信度为*2/3*。如果我们将最小置信度阈值设置为*t
    = 0.6*，我们可以轻松地检查以下具有长度为1的前提和结论的关联规则对我们的情况有效：
- en: '*{a} ⇒ {c}, **{a} ⇒ {f}, {a} ⇒ {m}, {a} ⇒ {p}** {c} ⇒ {a}, {c} ⇒ {f}, {c} ⇒
    {m}, {c} ⇒ {p}** {f} ⇒ {a}, {f} ⇒ {c}, {f} ⇒ {m}** {m} ⇒ {a}, {m} ⇒ {c}, {m} ⇒ {f}, {m}
    ⇒ {p}** {p} ⇒ {a}, {p} ⇒ {c}, {p} ⇒ {f}, {p} ⇒ {m}*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*{a}⇒{c}，{a}⇒{f}，{a}⇒{m}，{a}⇒{p}，{c}⇒{a}，{c}⇒{f}，{c}⇒{m}，{c}⇒{p}，{f}⇒{a}，{f}⇒{c}，{f}⇒{m}，{m}⇒{a}，{m}⇒{c}，{m}⇒{f}，{m}⇒{p}，{p}⇒{a}，{p}⇒{c}，{p}⇒{f}，{p}⇒{m}*'
- en: From the preceding definition of confidence, it should now be clear that it
    is relatively straightforward to compute the association rules once we have the
    support value of all the frequent patterns. In fact, as we will soon see, Spark's
    implementation of association rules is based on calculating frequent patterns
    upfront.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从置信度的前面定义可以清楚地看出，一旦我们有了所有频繁模式的支持值，计算关联规则就相对简单。实际上，正如我们将很快看到的那样，Spark对关联规则的实现是基于预先计算频繁模式的。
- en: At this point, it should be noted that while we will restrict ourselves to the
    measures of support and confidence, there are many other interesting criteria
    available that we can't discuss in this book; for instance, the concepts of *conviction,
    leverage, *or *lift. *For an in-depth comparison of the other measures, refer
    to [http://www.cse.msu.edu/~ptan/papers/IS.pdf](http://www.cse.msu.edu/~ptan/papers/IS.pdf).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此时应该指出的是，虽然我们将限制自己在支持和置信度的度量上，但还有许多其他有趣的标准可用，我们无法在本书中讨论；例如，*信念、杠杆、*或*提升*的概念。有关其他度量的深入比较，请参阅[http://www.cse.msu.edu/~ptan/papers/IS.pdf](http://www.cse.msu.edu/~ptan/papers/IS.pdf)。
- en: The sequential pattern mining problem
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顺序模式挖掘问题
- en: 'Let''s move on to formalizing, the third and last pattern matching question
    we tackle in this chapter. Let''s look at *sequences* in more detail. A sequence
    is different from the transactions we looked at before in that the order now matters.
    For a given item set *I*, a sequence *S* in *I* of length *l* is defined as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续正式化，这是我们在本章中处理的第三个也是最后一个模式匹配问题。让我们更详细地看一下*序列*。序列与我们之前看到的交易不同，因为现在顺序很重要。对于给定的项目集*I*，长度为*l*的序列*S*在*I*中定义如下：
- en: '*s = <s[1,] s[2],..., s[l]>*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*s = <s[1,] s[2],..., s[l]>*'
- en: 'Here, each individual *s[i]* is a concatenation of items, that is, *s[i] =
    (a[i1] ... a[im)]*, where *a[ij]* is an item in *I*. Note that we do care about
    the order of sequence items *s[i]* but not about the internal ordering of the
    individual *a[ij] *in *s[i]*. A sequence database *S* consists of pairs of sequence
    IDs and sequences, analogous to what we had before. An example of such a database
    can be found in the following table, in which the letters represent the same items
    as in our previous shopping cart example:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个单独的*s[i]*都是项目的连接，即*s[i] = (a[i1] ... a[im)]*，其中*a[ij]*是*I*中的一个项目。请注意，我们关心序列项*s[i]*的顺序，但不关心*s[i]*中各个*a[ij]*的内部顺序。序列数据库*S*由序列ID和序列的成对组成，类似于我们之前的内容。这样的数据库示例可以在下表中找到，其中的字母代表与我们之前的购物车示例中相同的项目：
- en: '| **Sequence ID** | **Sequence** |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| **序列ID** | **序列** |'
- en: '| 1 | *<a(abc)(ac)d(cf)>* |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 1 | *<a(abc)(ac)d(cf)>* |'
- en: '| 2 | *<(ad)c(bc)(ae)>* |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 2 | *<(ad)c(bc)(ae)>* |'
- en: '| 3 | *<(ef)(ab)(df)cb>* |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 3 | *<(ef)(ab)(df)cb>* |'
- en: '| 4 | *<eg(af)cbc>* |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 4 | *<eg(af)cbc>* |'
- en: 'Table 2: A small sequence database with four short sequences.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：一个包含四个短序列的小序列数据库。
- en: 'In the example sequences, note the round brackets to group individual items
    into a sequence item. Also note that we drop these redundant braces if the sequence
    item consists of a single item. Importantly, the notion of a subsequence requires
    a little more carefulness than for unordered structures. We call *u = (u[1], ...,
    u[n])* a subsequenceof *s = (s[1],..., s[l])* and write *u <s* if there are indices
    *1 **≤ i1 < i2 < ... < in ≤ m* so that we have the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例序列中，注意圆括号将单个项目分组为序列项。还要注意，如果序列项由单个项目组成，我们会省略这些冗余的大括号。重要的是，子序列的概念需要比无序结构更加小心。我们称*u
    = (u[1], ..., u[n])*为*s = (s[1],..., s[l])*的*子序列*，并写为*u <s*，如果存在索引*1 **≤ i1 <
    i2 < ... < in ≤ m*，使得我们有以下关系：
- en: '*u[1] < s[i1], ..., u[n] <s[in]*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*u[1] < s[i1], ..., u[n] <s[in]*'
- en: Here, the *<* signs in the last line mean that *u[j]* is a subpattern of*s[ij]*.
    Roughly speaking, *u* is a subsequence of *s* if all the elements of *u* are subpatterns
    of *s* in their given order. Equivalently, we call *s* a supersequence of *u*.
    In the preceding example, we see that *<a(ab)ac>* and *a(cb)(ac)dc>* are examples
    of subsequences of *<a(abc)(ac)d(cf)>* and that *<(fa)c>* is an example of a subsequence
    of *<eg(af)cbc>*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，最后一行中的*< *符号表示*u[j]*是*s[ij]*的子模式。粗略地说，如果*u*的所有元素按给定顺序是*s*的子模式，那么*u*就是*s*的子序列。同样地，我们称*s*为*u*的超序列。在前面的例子中，我们看到*<a(ab)ac>*和*a(cb)(ac)dc>*是*<a(abc)(ac)d(cf)>*的子序列的例子，而*<(fa)c>*是*<eg(af)cbc>*的子序列的例子。
- en: 'With the help of the notion of supersequences, we can now define the *support*
    of a sequence *s* in a given sequence database *S* as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 借助超序列的概念，我们现在可以定义给定序列数据库*S*中序列*s*的*支持度*如下：
- en: '*supp[S](s) = supp(s) = |{ s'' ∈ S | s <s''}| / |S|*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*supp[S](s) = supp(s) = |{ s'' ∈ S | s <s''}| / |S|*'
- en: Note that, structurally, this is the same definition as for plain unordered
    patterns, but the *<* symbol means something else, that is, a subsequence. As
    before, we drop the database subscript in the notation of *support* if the information
    is clear from the context. Equipped with a notion of *support*, the definition
    of sequential patterns follows the previous definition completely analogously.
    Given a minimum support threshold *t*, a sequence *s* in *S* is said to be a *sequential
    pattern* if *supp(s)* is greater than or equal to *t*. The formalization of the
    third question is called the *sequential pattern mining problem*, that is, find
    the full set of sequences that are sequential patterns in *S* for a given threshold
    *t*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，结构上，这与无序模式的定义相同，但*<*符号表示的是另一种含义，即子序列。与以前一样，如果上下文中的信息清楚，我们在*支持度*的表示法中省略数据库下标。具备了*支持度*的概念，顺序模式的定义完全类似于之前的定义。给定最小支持度阈值*t*，序列*S*中的序列*s*如果*supp(s)*大于或等于*t*，则称为*顺序模式*。第三个问题的形式化被称为*顺序模式挖掘问题*，即找到在给定阈值*t*下*S*中的所有顺序模式的完整集合。
- en: Even in our little example with just four sequences, it can already be challenging
    to manually inspect all the sequential patterns. To give just one example of a
    sequential pattern of *support 1.0*, a subsequence of length 2 of all the four
    sequences is *<ac>*. Finding all the sequential patterns is an interesting problem,
    and we will learn about the so-called *prefix span *algorithm that Spark employs
    to address the problem in the following section.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在我们只有四个序列的小例子中，手动检查所有顺序模式也可能是具有挑战性的。举一个*支持度为1.0*的顺序模式的例子，所有四个序列的长度为2的子序列是*<ac>*。找到所有顺序模式是一个有趣的问题，我们将在下一节学习Spark使用的所谓*前缀span*算法来解决这个问题。
- en: Pattern mining with Spark MLlib
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark MLlib进行模式挖掘
- en: After having motivated and introduced three pattern mining problems along with
    the necessary notation to properly talk about them, we will next discuss how each
    of these problems can be solved with an algorithm available in Spark MLlib. As
    is often the case, actually applying the algorithms themselves is fairly simple
    due to Spark MLlib's convenient `run` method available for most algorithms. What
    is more challenging is to understand the algorithms and the intricacies that come
    with them. To this end, we will explain the three pattern mining algorithms one
    by one, and study how they are implemented and how to use them on toy examples.
    Only after having done all this will we apply these algorithms to a real-life
    data set of click events retrieved from [http://MSNBC.com](http://MSNBC.com).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在激发和介绍了三个模式挖掘问题以及必要的符号来正确讨论它们之后，我们将讨论如何使用Spark MLlib中可用的算法解决这些问题。通常情况下，由于Spark
    MLlib为大多数算法提供了方便的`run`方法，实际应用算法本身相当简单。更具挑战性的是理解算法及其随之而来的复杂性。为此，我们将逐一解释这三种模式挖掘算法，并研究它们是如何实现以及如何在玩具示例中使用它们。只有在完成所有这些之后，我们才会将这些算法应用于从[http://MSNBC.com](http://MSNBC.com)检索到的点击事件的真实数据集。
- en: The documentation for the pattern mining algorithms in Spark can be found at [https://spark.apache.org/docs/2.1.0/mllib-frequent-pattern-mining.html](https://spark.apache.org/docs/2.1.0/mllib-frequent-pattern-mining.html).
    It provides a good entry point with examples for users who want to dive right
    in.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中模式挖掘算法的文档可以在[https://spark.apache.org/docs/2.1.0/mllib-frequent-pattern-mining.html](https://spark.apache.org/docs/2.1.0/mllib-frequent-pattern-mining.html)找到。它为希望立即深入了解的用户提供了一个很好的入口点。
- en: Frequent pattern mining with FP-growth
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用FP-growth进行频繁模式挖掘
- en: When we introduced the frequent pattern mining problem, we also quickly discussed
    a strategy to address it based on the apriori principle. The approach was based
    on scanning the whole transaction database again and again to expensively generate
    pattern candidates of growing length and checking their support. We indicated
    that this strategy may not be feasible for very large data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们介绍频繁模式挖掘问题时，我们还快速讨论了一种基于apriori原则来解决它的策略。这种方法是基于一遍又一遍地扫描整个交易数据库，昂贵地生成不断增长长度的模式候选项并检查它们的支持。我们指出，这种策略对于非常大的数据可能是不可行的。
- en: 'The so called *FP-growth algorithm*, where **FP** stands for **frequent pattern**,
    provides an interesting solution to this data mining problem. The algorithm was
    originally described in *Mining Frequent Patterns without Candidate Generation,* available
    at [https://www.cs.sfu.ca/~jpei/publications/sigmod00.pdf](https://www.cs.sfu.ca/~jpei/publications/sigmod00.pdf).
    We will start by explaining the basics of this algorithm and then move on to discussing
    its distributed version, *parallel FP-growth, *which has been introduced in *PFP:
    Parallel FP-Growth for Query Recommendation*, found at [https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/34668.pdf](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/34668.pdf).
    While Spark''s implementation is based on the latter paper, it is best to first
    understand the baseline algorithm and extend from there.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '所谓的*FP-growth算法*，其中**FP**代表**频繁模式**，为这个数据挖掘问题提供了一个有趣的解决方案。该算法最初是在*Mining Frequent
    Patterns without Candidate Generation*中描述的，可在[https://www.cs.sfu.ca/~jpei/publications/sigmod00.pdf](https://www.cs.sfu.ca/~jpei/publications/sigmod00.pdf)找到。我们将首先解释这个算法的基础知识，然后继续讨论其分布式版本*parallel
    FP-growth*，该版本在*PFP: Parallel FP-Growth for Query Recommendation*中介绍，可在[https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/34668.pdf](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/34668.pdf)找到。虽然Spark的实现是基于后一篇论文，但最好先了解基线算法，然后再进行扩展。'
- en: The core idea of FP-growth is to scan the transaction database D of interest
    precisely once in the beginning, find all the frequent patterns of length 1, and
    build a special tree structure called *FP-tree *from these patterns. Once this
    step is done, instead of working with D, we only do recursive computations on
    the usually much smaller FP-tree. This step is called the *FP-growth step *of
    the algorithm, since it recursively constructs trees from the subtrees of the
    original tree to identify patterns. We will call this procedure *fragment pattern
    growth*, which does not require us to generate candidates but is rather built
    on a *divide-and-conquer *strategy that heavily reduces the workload in each recursion
    step.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: FP-growth的核心思想是在开始时精确地扫描感兴趣的交易数据库D一次，找到所有长度为1的频繁模式，并从这些模式构建一个称为*FP-tree*的特殊树结构。一旦完成了这一步，我们不再使用D，而是仅对通常要小得多的FP-tree进行递归计算。这一步被称为算法的*FP-growth步骤*，因为它从原始树的子树递归构造树来识别模式。我们将称这个过程为*片段模式增长*，它不需要我们生成候选项，而是建立在*分而治之*策略上，大大减少了每个递归步骤中的工作量。
- en: 'To be more precise, let''s first define what an FP-tree is and what it looks
    like in an example. Recall the example database we used in the last section, shown
    in *Table 1*. Our item set consisted of the following 15 grocery items, represented
    by their first letter: *b*, *c*, *a*, *e*, *d*, *f*, *p*, *m*, *i*, *l*, *o*,
    *h*, *j*, *k*, *s*. We also discussed the frequent items; that is, patterns of
    length 1, for a minimum support threshold of *t = 0.6*, were given by *{f, c,
    b, a, m, p}*. In FP-growth, we first use the fact that the ordering of items does
    not matter for the frequent pattern mining problem; that is, we can choose the
    order in which to present the frequent items. We do so by ordering them by decreasing
    frequency. To summarize the situation, let''s have a look at the following table:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，让我们首先定义FP树是什么，以及在示例中它是什么样子。回想一下我们在上一节中使用的示例数据库，显示在*表1*中。我们的项目集包括以下15个杂货项目，用它们的第一个字母表示：*b*，*c*，*a*，*e*，*d*，*f*，*p*，*m*，*i*，*l*，*o*，*h*，*j*，*k*，*s*。我们还讨论了频繁项目；也就是说，长度为1的模式，对于最小支持阈值*t
    = 0.6*，由*{f, c, b, a, m, p}*给出。在FP-growth中，我们首先利用了一个事实，即项目的排序对于频繁模式挖掘问题并不重要；也就是说，我们可以选择呈现频繁项目的顺序。我们通过按频率递减的顺序对它们进行排序。总结一下情况，让我们看一下下表：
- en: '| **Transaction ID** | **Transaction** | **Ordered frequent items** |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '| 1 | *a, c, d, f, g, i, m, p* | *f, c, a, m, p* |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| 2 | *a, b, c, f, l, m, o* | *f, c, a, b, m* |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '| 3 | *b, f, h, j, o* | *f, b* |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| 4 | *b, c, k, s, p* | *c, b, p* |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| 5 | *a, c, e, f, l, m, n, p* | *f, c, a, m, p* |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Continuation of the example started with Table 1, augmenting the table
    by ordered frequent items.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, ordering frequent items like this already helps us to identify
    some structure. For instance, we see that the item set *{f, c, a, m, p}* occurs
    twice and is slightly altered once as *{f, c, a, b, m}*. The key idea of FP-growth
    is to use this representation to build a tree from the ordered frequent items
    that reflect the structure and interdependencies of the items in the third column
    of *Table 3*. Every FP-tree has a so-called *root *node that is used as a base
    for connecting ordered frequent items as constructed. On the right of the following
    diagram, we see what is meant by this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00139.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: FP-tree and header table for our frequent pattern mining running
    example.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'The left-hand side of *Figure 1* shows a header table that we will explain
    and formalize in just a bit, while the right-hand side shows the actual FP-tree.
    For each of the ordered frequent items in our example, there is a directed path
    starting from the root, thereby representing it. Each node of the tree keeps track
    of not only the frequent item itself but also of the number of paths traversed
    through this node. For instance, four of the five ordered frequent item sets start
    with the letter *f* and one with *c*. Thus, in the FP-tree, we see `f: 4` and
    `c: 1` at the top level. Another interpretation of this fact is that *f* is a
    *prefix* for four item sets and *c* for one. For another example of this sort
    of reasoning, let''s turn our attention to the lower left of the tree, that is,
    to the leaf node `p: 2`. Two occurrences of *p* tells us that precisely two identical
    paths end here, which we already know: *{f, c, a, m, p}* is represented twice.
    This observation is interesting, as it already hints at a technique used in FP-growth--starting
    at the leaf nodes of the tree, or the suffixes of the item sets, we can trace
    back each frequent item set, and the union of all these distinct root node paths
    yields all the paths--an important idea for parallelization.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: The header table you see on the left of *Figure 1* is a smart way of storing
    items. Note that by the construction of the tree, a node is not the same as a
    frequent item but, rather, items can and usually do occur multiple times, namely
    once for each distinct path they are part of. To keep track of items and how they
    relate, the header table is essentially a *linked list* of items, that is, each
    item occurrence is linked to the next by means of this table. We indicated the
    links for each frequent item by horizontal dashed lines in *Figure 1* for illustration
    purposes.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: With this example in mind, let's now give a formal definition of an FP-tree.
    An FP-tree *T* is a tree that consists of a root node together with frequent item
    prefix subtreesstarting at the root and a frequent item header table.Each node
    of the tree consists of a triple, namely the item name, its occurrence count,
    and a node link referring to the next node of the same name, or `null` if there
    is no such next node.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: To quickly recap, to build *T*, we start by computing the frequent items for
    the given minimum support threshold *t*, and then, starting from the root, insert
    each path represented by the sorted frequent pattern list of a transaction into
    the tree. Now, what do we gain from this? The most important property to consider
    is that all the information needed to solve the frequent pattern mining problem
    is encoded in the FP-tree *T* because we effectively encode all co-occurrences
    of frequent items with repetition. Since *T* can also have at most as many nodes
    as the occurrences of frequent items, *T* is usually much smaller than our original
    database D. This means that we have mapped the mining problem to a problem on
    a smaller data set, which in itself reduces the computational complexity compared
    with the naive approach sketched earlier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速回顾，构建*T*，我们首先计算给定最小支持阈值*t*的频繁项，然后，从根开始，将每个由事务的排序频繁模式列表表示的路径插入树中。现在，我们从中获得了什么？要考虑的最重要的属性是，解决频繁模式挖掘问题所需的所有信息都被编码在FP树*T*中，因为我们有效地编码了所有频繁项的重复共现。由于*T*的节点数最多与频繁项的出现次数一样多，*T*通常比我们的原始数据库D小得多。这意味着我们已经将挖掘问题映射到了一个较小的数据集上，这本身就降低了与之前草率方法相比的计算复杂性。
- en: 'Next, we''ll discuss how to grow patterns recursively from fragments obtained
    from the constructed FP tree. To do so, let''s make the following observation.
    For any given frequent item *x*, we can obtain all the patterns involving *x*
    by following the node links for *x*, starting from the header table entry for
    *x*, by analyzing at the respective subtrees. To explain how exactly, we further
    study our example and, starting at the bottom of the header table, analyze patterns
    containing *p*. From our FP-tree *T*, it is clear that *p* occurs in two paths:
    *(f:4, c:3, a:3, m:3, p:2)* and *(c:1, b:1, p:1)*, following the node links for *p*.
    Now, in the first path, *p* occurs only twice, that is, there can be at most two
    total occurrences of the pattern *{f, c, a, m, p}* in the original database D.
    So, conditional on *p* being present*, *the paths involving *p* actually read
    as follows: *(f:2, c:2, a:2, m:2, p:2)* and *(c:1, b:1, p:1)*. In fact, since
    we know we want to analyze patterns, given *p*, we can shorten the notation a
    little and simply write *(f:2, c:2, a:2, m:2)* and *(c:1, b:1)*. This is what
    we call the **conditional pattern base for p**. Going one step further, we can
    construct a new FP-tree from this conditional database. Conditioning on three
    occurrences of *p*, this new tree does only consist of a single node, namely *(c:3)*.
    This means that we end up with *{c, p}* as a single pattern involving *p*, apart
    from *p* itself. To have a better means of talking about this situation, we introduce
    the following notation: the conditional FP-tree for *p* is denoted by *{(c:3)}|p*.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何从构建的FP树中递归地从片段中生长模式。为此，让我们做出以下观察。对于任何给定的频繁项*x*，我们可以通过跟随*x*的节点链接，从*x*的头表条目开始，通过分析相应的子树来获得涉及*x*的所有模式。为了解释具体方法，我们进一步研究我们的例子，并从头表的底部开始，分析包含*p*的模式。从我们的FP树*T*来看，*p*出现在两条路径中：*(f:4,
    c:3, a:3, m:3, p:2)*和*(c:1, b:1, p:1)*，跟随*p*的节点链接。现在，在第一条路径中，*p*只出现了两次，也就是说，在原始数据库D中*{f,
    c, a, m, p}*模式的总出现次数最多为两次。因此，在*p*存在的条件下，涉及*p*的路径实际上如下：*(f:2, c:2, a:2, m:2)*和*(c:1,
    b:1)*。事实上，由于我们知道我们想要分析模式，给定*p*，我们可以简化符号，简单地写成*(f:2, c:2, a:2, m:2)*和*(c:1, b:1)*。这就是我们所说的**p的条件模式基**。再进一步，我们可以从这个条件数据库构建一个新的FP树。在*p*出现三次的条件下，这棵新树只包含一个节点，即*(c:3)*。这意味着我们最终得到了*{c,
    p}*作为涉及*p*的单一模式，除了*p*本身。为了更好地讨论这种情况，我们引入以下符号：*p*的条件FP树用*{(c:3)}|p*表示。
- en: 'To gain more intuition, let''s consider one more frequent item and discuss
    its conditional pattern base. Continuing bottom to top and analyzing *m*, we again
    see two paths that are relevant: *(f:4, c:3, a:3, m:2)* and *(f:4, c:3, a:3, b:1,
    m:1)*. Note that in the first path, we discard the *p:2* at the end, since we
    have already covered the case of *p*. Following the same logic of reducing all
    other counts to the count of the item in question and conditioning on *m*, we
    end up with the conditional pattern base *{(f:2, c:2, a:2), (f:1, c:1, a:1, b:1)}*.
    The conditional FP-tree in this situation is thus given by *{f:3, c:3, a:3}|m*.
    It is now easy to see that actually every possible combination of *m* with each
    of *f*, *c*, and *a* forms a frequent pattern. The full set of patterns, given
    *m*, is thus *{m}*, *{am}*, *{cm}*, *{fm}*, *{cam]*, *{fam}*, *{fcm}*, and *{fcam}*.
    By now, it should become clear as to how to continue, and we will not carry out
    this exercise in full but rather summarize the outcome of it in the following
    table:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观，让我们考虑另一个频繁项并讨论它的条件模式基。继续从底部到顶部并分析*m*，我们再次看到两条相关的路径：*(f:4, c:3, a:3, m:2)*和*(f:4,
    c:3, a:3, b:1, m:1)*。请注意，在第一条路径中，我们舍弃了末尾的*p:2*，因为我们已经涵盖了*p*的情况。按照相同的逻辑，将所有其他计数减少到所讨论项的计数，并在*m*的条件下，我们得到了条件模式基*{(f:2,
    c:2, a:2), (f:1, c:1, a:1, b:1)}*。因此，在这种情况下，条件FP树由*{f:3, c:3, a:3}|m*给出。现在很容易看出，实际上每个*m*与*f*、*c*和*a*的每种可能组合都形成了一个频繁模式。给定*m*，完整的模式集合是*{m}*、*{am}*、*{cm}*、*{fm}*、*{cam}*、*{fam}*、*{fcm}*和*{fcam}*。到目前为止，应该清楚如何继续了，我们不会完全进行这个练习，而是总结其结果如下表所示：
- en: '| **Frequent pattern** | **Conditional pattern base** | **Conditional FP-tree**
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| **频繁模式** | **条件模式基** | **条件FP树** |'
- en: '| *p* | *{(f:2, c:2, a:2, m:2), (c:1, b:1)}* | *{(c:3)}&#124;p* |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| *p* | *{(f:2, c:2, a:2, m:2), (c:1, b:1)}* | *{(c:3)}&#124;p* |'
- en: '| *m* | *{(f :2, c:2, a:2), (f :1, c:1, a:1, b:1)}* | *{f:3, c:3, a:3}&#124;m*
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| *m* | *{(f :2, c:2, a:2), (f :1, c:1, a:1, b:1)}* | *{f:3, c:3, a:3}&#124;m*
    |'
- en: '| *b* | *{(f :1, c:1, a:1), (f :1), (c:1)}* | null |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| *b* | *{(f :1, c:1, a:1), (f :1), (c:1)}* | null |'
- en: '| *a* | *{(f:3, c:3)}* | *{(f:3, c:3)}&#124;a* |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| *a* | *{(f:3, c:3)}* | *{(f:3, c:3)}&#124;a* |'
- en: '| *c* | *{(f:3)}* | *{(f:3)}&#124;c* |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| *c* | *{(f:3)}* | *{(f:3)}&#124;c* |'
- en: '| *f* | null | null |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| *f* | null | null |'
- en: 'Table 4: The complete list of conditional FP-trees and conditional pattern
    bases for our running example.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：我们运行示例的条件FP树和条件模式基的完整列表。
- en: 'As this derivation required a lot of attention to detail, let''s take a step
    back and summarize the situation so far:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the original FP-tree *T*, we iterated through all the items using
    node links.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each item *x*, we constructed its conditional pattern base and its conditional
    FP-tree. Doing so, we used the following two properties:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We discarded all the items following *x* in each potential pattern, that is,
    we only kept the *prefix* of *x*
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We modified the item counts in the conditional pattern base to match the count
    of *x*
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying a path using the latter two properties, we called the transformed
    prefix path of *x*.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To finally state the FP-growth step of the algorithm, we need two more fundamental
    observations that we have already implicitly used in the example. Firstly, the
    support of an item in a conditional pattern base is the same as that of its representation
    in the original database. Secondly, starting from a frequent pattern *x* in the
    original database and an arbitrary set of items *y*, we know that *xy* is a frequent
    pattern if and only if *y* is. These two facts can easily be derived in general,
    but should be clearly demonstrated in the preceding example.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'What this means is that we can completely focus on finding patterns in conditional
    pattern bases, as joining them with frequent patterns is again a pattern, andthis
    way, we can find all the patterns. This mechanism of recursively growing patterns
    by computing conditional pattern bases is therefore called pattern growth, which
    is why FP-growth bears its name. With all this in mind, we can now summarize the
    FP-growth procedure in pseudocode, as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With this procedure, we can summarize our description of the complete FP-growth
    algorithm as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Compute frequent items from D and compute the original FP-tree *T* from them
    (*FP-tree computation).*
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `fpGrowth(T, null)` (*FP-growth computation).*
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Having understood the base construction, we can now proceed to discuss a parallel
    extension of base FP-growth, that is, the basis of Spark''s implementation. **Parallel
    FP-growth**, or **PFP** for short, is a natural evolution of FP-growth for parallel
    computing engines such as Spark. It addresses the following problems with the
    baseline algorithm:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '*Distributed storage:* For frequent pattern mining, our database D may not
    fit into memory, which can already render FP-growth in its original form unapplicable.
    Spark does help in this regard for obvious reasons.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Distributed computing: *With distributed storage in place, we will have to
    take care of parallelizing all the steps of the algorithm suitably as well and
    PFP does precisely this.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adequate support values:* When dealing with finding frequent patterns, we
    usually do not want to set the minimum support threshold *t* too high so as to
    find interesting patterns in the long tail. However, a small *t* might prevent
    the FP-tree from fitting into memory for a sufficiently large D, which would force
    us to increase *t*. PFP successfully addresses this problem as well, as we will
    see.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The basic outline of PFP, with Spark for implementation in mind, is as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '**Sharding**: Instead of storing our database D on a single machine, we distribute
    it to multiple partitions. Regardless of the particular storage layer, using Spark
    we can, for instance, create an RDD to load D.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel frequent item count**: The first step of computing frequent items
    of D can be naturally performed as a map-reduce operation on an RDD.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Building groups of frequent items**: The set of frequent items is divided
    into a number of groups, each with a unique group ID.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel FP-growth**:The FP-growth step is split into two steps to leverage
    parallelism:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Map phase**:The output of a mapper is a pair comprising the group ID and
    the corresponding transaction.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduce phase**:Reducers collect data according to the group ID and carry
    out FP-growth on these group-dependent transactions.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregation**: The final step in the algorithm is the aggregation of results
    over group IDs.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In light of already having spent a lot of time with FP-growth on its own, instead
    of going into too many implementation details of PFP in Spark, let''s instead
    see how to use the actual algorithm on the toy example that we have used throughout:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The code is straightforward. We load the data into `transactions` and initialize
    Spark''s `FPGrowth` implementation with a minimum support value of *0.6* and *5*
    partitions. This returns a model that we can `run` on the transactions constructed
    earlier. Doing so gives us access to the patterns or frequent item sets for the
    specified minimum support, by calling `freqItemsets`, which, printed in a formatted
    way, yields the following output of 18 patterns in total:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00140.jpeg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: Recall that we have defined transactions as *sets*, and we often call them item
    sets. This means that within such an item set, a particular item can only occur
    once, and `FPGrowth` depends on this. If we were to replace, for instance, the
    third transaction in the preceding example by `Array("b", "b", "h", "j", "o")`,
    calling `run` on these transactions would throw an error message. We will see
    later on how to deal with such situations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: After having explained *association rules *and *prefix span* in a similar fashion
    to what we just did with FP-growth, we will turn to an application of these algorithms
    on a real-world data set.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Association rule mining
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall from the association rule introduction that in computing association
    rules, we are about halfway there once we have frequent item sets, that is, patterns
    for the specified minimum threshold. In fact, Spark's implementation of association
    rules assumes that we provide an RDD of `FreqItemsets[Item]`, which we have already
    seen an example of in the preceding call to `model.freqItemsets`. On top of that,
    computing association rules is not only available as a standalone algorithm but
    is also available through `FPGrowth`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Before showing how to run the respective algorithm on our running example,
    let''s quickly explain how association rules are implemented in Spark:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm is already provided with frequent item sets, so we don't have
    to compute them anymore.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each pair of patterns, X and *Y*, compute the frequency of both items X
    and Y co-occurring and store (*X*, (*Y*, supp(*X* ∪ *Y*)). We call such pairs
    of patterns *candidate pairs, *where *X* acts as a potential antecedent and *Y*
    as a consequent.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join all the patterns with the candidate pairs to obtain statements of the form, (X,
    ((Y, supp(*X* ∪ *Y*)), supp(*X*))).
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can then filter expressions of the form (X, ((Y, supp(*X* ∪ *Y*)), supp(*X*))) by
    the desired minimum confidence value to return all rules *X ⇒ Y* with that level
    of confidence.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Assuming we didn''t compute the patterns through FP-growth in the last section
    but, instead, were just given the full list of these item sets, we can create
    an RDD from a sequence of `FreqItemset` from scratch and then run a new instance
    of `AssociationRules` on it:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that after initializing the algorithm, we set the minimum confidence to
    `0.7` before collecting the results. Moreover, running `AssociationRules` returns
    an RDD of rules of the `Rule` type. These rule objects have accessors for `antecedent`,
    `consequent`, and `confidence`, which we use to collect the results that read
    as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00141.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'The reason we started this example from scratch is to convey the idea that
    association rules are indeed a standalone algorithm in Spark. Since the only built-in
    way to compute patterns in Spark is currently through FP-growth, and association
    rules depends on the concept of `FreqItemset` (imported from the `FPGrowth` submodule)
    anyway, this seems a bit unpractical. Using our results from the previous FP-growth
    example, we could well have written the following to achieve the same:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Interestingly, association rules can also be computed directly through the
    interface of `FPGrowth`. Continuing with the notation from the earlier example,
    we can simply write the following to end up with the same set of rules as before:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In practical terms, while both the formulations can be useful, the latter one
    will certainly be more concise.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Sequential pattern mining with prefix span
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Turning to sequential pattern matching, the *prefix span algorithm* is a little
    more complicated than association rules, so we need to take a step back and explain
    the basics first. Prefix span has first been described in [http://hanj.cs.illinois.edu/pdf/tkde04_spgjn.pdf](http://hanj.cs.illinois.edu/pdf/tkde04_spgjn.pdf) as
    a natural extension of the so-called *FreeSpan *algorithm. The algorithm itself
    represents a notable improvement over other approaches, such as **Generalized
    Sequential Patterns**(**GSP**). The latter is based on the apriori principle and
    all the drawbacks we discussed earlier regarding many algorithms based on it carry
    over to sequential mining as well, that is, expensive candidate generation, multiple
    database scans, and so on.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Prefix span, in its basic formulation, uses the same fundamental idea as FP-growth,
    which is, projecting the original database to a usually smaller structure to analyze*.*
    While in FP-growth, we recursively built new FP-trees for each *suffix *of a branch
    in the original FP-tree, prefix span grows or spans new structures by considering
    *prefixes*, as the name suggests.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first properly define the intuitive notions of prefix and suffix in
    the context of sequences. In what follows, we''ll always assume that the items
    within a sequence item are alphabetically ordered, that is, if *s =  <s[1,] s[2],..., s[l]> *is
    a sequence in *S* and each *s[i]* is a concatenation of items, that is, *s[i]
    = (a[i1] ... a[im])*, where a[ij] are items in *I*, we assume that all *a[ij]* are
    in the alphabetical order within *s[i]*. In such a situation, an element *s''
    = <s''[1,] s''[2],..., s''m> *is called a prefixof *s* if and only if the following
    three properties are satisfied:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: For all *i < m*, we have equality of sequence items, that is, *s'[i] = s[i]*
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s''[m] < s[m]*, that is, the last item of *s''* is a subpattern of *s[m]*'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we subtract *s'[m]* from *s[m]*, that is, delete the subpattern *s'[m]* from
    *s[m]*, all the frequent items left in *s[m] - s'[m]* have to come after all the
    elements in *s'[m]*, in alphabetical order
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the first two points come fairly naturally, the last one might seem a
    little strange, so let's explain it in an example. Given a sequence, *<a(abc)>*,
    from a database D, in which *a*, *b*, and *c* are indeed frequent, then, *<aa>*
    and *<a(ab)>* are prefixes for *<a(abc)>*, but *<ab>* is not, because in the difference
    of the last sequence items, *<(abc)> - <b> = <(ac)>*, the letter *a* does not
    alphabetically come after *b* from *<ab>*. Essentially, the third property tells
    us that a prefix can only cut out parts at the beginning of the last sequence
    item it affects.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'With the notion of the prefix defined, it is now easy to say what a suffixis.
    With the same notation as before, if *s''* is a prefix of *s*, then *s'''' = <(s[m]
    - s''[m]), s[m+1], ..., s[l]>* is a suffixfor this prefix, which we denote as
    *s'''' = s / s''*. Furthermore, we will write *s = s''s''''* in a product notation.
    For instance, given that *<a(abc)>* is the original sequence and *<aa>* is the
    prefix, we denote the suffix for this prefix as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '*<(_bc)> = <a(abc)> / <aa>*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Note that we use an underscore notation to denote the remainder of a sequence
    by a prefix.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the prefix and suffix notions are useful to split up or partition the
    original sequential pattern mining problem into smaller parts, as follows. Let
    *{<p[1]>, ...,<p[n]>}* be the complete set of sequential patterns of length 1\.
    Then, we can make the following observations:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: All the sequential patterns start with one of the *p[i]*. This, in turn, means
    that we can partition all sequential patterns into *n* disjoint sets, namely those
    starting with *p[i]*, for *i* between *1* and *n*.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Applying this reasoning recursively, we end up with the following statement:
    if *s* is a given sequential pattern of length 1 and *{s¹, ..., s^m}* is the complete
    list of length *l+1* sequential superpatterns of *s*, then all sequential patterns
    with the prefix *s* can be partitioned into *m* sets prefixed by *s^i*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both these statements are easy to arrive at but provide a powerful tool to subdivide
    the original problem set into disjointed smaller problems. Such a strategy is
    called *divide and conquer*. With this in mind, we can now proceed very similarly
    to what we did with conditioned databases in FP-growth, namely project databases
    with respect to a given prefix. Given a sequential pattern database S and a prefix
    *s*, the **s-projected database**, *S|[s]*, is the set of all the suffixes for
    *s* in S.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'We need one last definition to state and analyze the prefix span algorithm.
    If *s* is a sequential pattern in S and *x* is a pattern with a prefix *s*, then
    the *support count *of *x* in *S|[s]*, denoted by *supp[S|s](x)*, is the number
    of sequences *y* in *S|[s]*, so that *x < sy*; that is, we simply carry over the
    notion of support to s-projected databases. There are a few interesting properties
    we can derive from this definition that make our situation much easier. For instance,
    by definition, we see that for any sequence *x* with the prefix *s*, we have the
    following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '*supp[S](x) = supp[S|s](x)*'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: That is, it does not matter if we count the support in the original or projected
    database in this case. Moreover, if *s'* is a prefix of *s*, it is clear that
    *S|[s] = (S|[s'])|[s]*, meaning we can prefix consecutively without losing information.
    The last and most important statement from a computational complexity perspective
    is that a projected database cannot exceed its original size. This property should
    again be clear from the definitions, but it's immensely helpful to justify the
    recursive nature of prefix span.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Given all this information, we can now sketch the prefix span algorithm in
    pseudocode as follows. Note that we distinguish between an item `s''` being appended
    to the end of a sequential pattern `s` and the sequence `<s''>` generated from
    `s''` added to the end of `s`. To give an example, we could either add the letter
    *e* to *<a(abc)>* to form *<a(abce)>* or add *<e>* at the end to form *<a(abc)e>*:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The prefix span algorithm, as outlined, finds all sequential patterns; that
    is, it represents a solution to the sequential pattern mining problem. We cannot
    outline the proof of this statement here, but we hopefully have provided you with
    enough intuition to see how and why it works.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Turning to Spark for an example, note that we did not discuss how to effectively
    parallelize the baseline algorithm. If you are interested in knowing about the
    implementation details, see [https://github.com/apache/spark/blob/v2.2.0/mllib/src/main/scala/org/apache/spark/mllib/fpm/PrefixSpan.scala](https://github.com/apache/spark/blob/v2.2.0/mllib/src/main/scala/org/apache/spark/mllib/fpm/PrefixSpan.scala),
    as the parallel version is a little too involved would justify presenting it here.
    We will study the example first provided in *Table 2*, that is, the four sequences
    of *<a(abc)(ac)d(cf)>*, *<(ad)c(bc)(ae)>*, *<(ef)(ab)(df)cb>*, and *<eg(af)cbc>*.
    To encode the nested structure of sequences, we use arrays of arrays of strings
    and parallelize them to create an RDD from them. Initializing and running an instance
    of `PrefixSpan` works pretty much the same way as it did for the other two algorithms.
    The only thing noteworthy here is that, apart from setting the minimum support
    threshold to `0.7` via `setMinSupport`, we also specify the maximum length of
    the patterns to `5` through `setMaxPatternLength`. This last parameter is there
    to limit the recursion depth. Despite the clever implementation, the algorithm
    (and particularly, the computing database projections) can take a prohibitive
    amount of time:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Running this code in your Spark shell should yield the following output of
    14 sequential patterns:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00142.jpeg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: Pattern mining on MSNBC clickstream data
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having spent a considerable amount of time explaining the basics of pattern
    mining, let''s finally turn to a more realistic application. The data we will
    be discussing next comes from server logs from [http://msnbc.com](http://msnbc.com)
    (and in parts from [http://msn.com](http://msn.com), when news-related), and represents
    a full day''s worth of browsing activity in terms of page views of users of these
    sites. The data collected in September 1999 and has been made available for download
    at [http://archive.ics.uci.edu/ml/machine-learning-databases/msnbc-mld/msnbc990928.seq.gz](http://archive.ics.uci.edu/ml/machine-learning-databases/msnbc-mld/msnbc990928.seq.gz).
    Storing this file locally and unzipping it, the `msnbc990928.seq` file essentially
    consists of a header and space-separated rows of integers of varying length. The
    following are the first few lines of the file:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Each row in this file is a *sequence* of encoded page visits of users within
    that day. Page visits have not been collected to the most granular level but rather
    grouped into 17 news-related categories, which are encoded as integers. The category
    names corresponding to these categories are listed in the preceding header and
    are mostly self-explanatory (with the exception of `bbs`, which stands for **bulletin
    board service**). The n-th item in this list corresponds to category n; for example,
    `1` stands for `frontpage`, while `travel` is encoded as `17`. For instance, the
    fourth user in this file hit `opinion` once, while the third had nine page views
    in total, starting and ending with `tech`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the page visits in each row have indeed been stored
    *chronologically*, that is, this really is sequential data with respect to page
    visit order. In total, data for 989,818 users has been collected; that is, the
    data set has precisely that number of sequences. Unfortunately, it is unknown
    how many URLs have been grouped to form each category, but we do know it ranges
    rather widely from 10 to 5,000\. See the description available at [http://archive.ics.uci.edu/ml/machine-learning-databases/msnbc-mld/msnbc.data.html](http://archive.ics.uci.edu/ml/machine-learning-databases/msnbc-mld/msnbc.data.html) for
    more information.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Just from the description of this data set, it should be clear that all the
    three pattern mining problems we have discussed so far can be applied to this
    data--we can search for sequential patterns in this sequential database and, neglecting
    the sequentiality, analyze both frequent patterns and association rules. To do
    so, let''s first load the data using Spark. In what follows, we will assume that
    the header of the file has been removed and a Spark shell session has been created
    from the folder the sequence file is stored in:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We load the sequence file into an RDD of integer-valued arrays first. Recall
    from earlier sections that one of the assumptions of transactions in frequent
    pattern mining was that the item sets are, in fact, sets and thus contain no duplicates.
    To apply FP-growth and association rule mining, we therefore have to delete duplicate
    entries, as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Note that not only did we restrict to distinct items for each transaction but
    we also cached the resulting RDD, which is recommended for all the three pattern
    mining algorithms. This allows us to run FP-growth on this data, for which we
    have to find a suitable minimum support threshold *t*. So far, in the toy examples,
    we have chosen *t* to be rather large (between 0.6 and 0.8). It is not realistic
    to expect *any* patterns to have such large support values in larger databases.
    Although we only have to deal with 17 categories, browsing behaviors can vary
    drastically from user to user. Instead, we choose a support value of just 5 %
    to gain some insights:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output of this computation shows that for *t=0.05* we only recover 14 frequent
    patterns, as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00143.jpeg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: Not only are there, maybe, less patterns than you may have expected, but among
    those, all but one have a length of *1*. Less surprising is the fact that the *front
    page* is hit most often, with 31%, followed by the categories, *on-air* and *news. *Both
    the *front page* and *news *sites have been visited by only 7% of users on that
    day and no other pair of site categories was visited by more than 5% of the user
    base. Categories 5, 15, 16, and 17 don't even make the list. If we repeat the
    experiment with a *t* value of 1% instead, the number of patterns increases to
    a total of 74.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how many length-3 patterns are among them:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Running this on an `FPGrowth` instance with a minimum support value of *t=0.01*
    will yield the following result:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00144.jpeg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: As one could have guessed, the most frequent length-1 patterns are also predominant
    among the 3-patterns. Within these 11 patterns, 10 concern the *front page,* andnine
    the *news. *Interestingly, the category *misc*, while only visited 7% of the time,
    according to the earlier analysis, shows up in a total of four 3-patterns. If
    we had more information about the underlying user groups, it would be interesting
    to follow up on this pattern. Speculatively, users that have an interest in a
    lot of *miscellaneous* topics will end up in this mixed category, along with some
    other categories.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this up with an analysis of association rules is technically easy;
    we just run the following lines to get all the rules with confidence `0.4` from
    the existing FP-growth `model`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Note how we can conveniently access the antecedent, consequent, and confidence
    of the respective rules. The output of this is as follows; this time with the
    confidence rounded to two decimals:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00145.jpeg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'Again, naturally, the most frequent length-1 patterns show up in many of the
    rules, most notably, *frontpage *as a consequent. Throughout this example, we
    chose the support and confidence values so that the outputs are short and counts
    easy to validate manually, but let''s do some automated calculations on rule sets,
    regardless:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Executing these statements, we see that about two-thirds of the rules have *front
    page* as the consequent, that is, 14 of 22 rules in total, and among these, nine
    contain *news* in their antecedent.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to the sequence mining problem for this data set, we need to transform
    our original `transactions` to an RDD of the `Array[Array[Int]]` type first, since
    nested arrays are the way to encode sequences for prefix span in Spark, as we
    have seen before. While somewhat obvious, it is still important to point out that
    with sequences, we don't have to discard the additional information of repeating
    items, as we just did for FP-growth.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, we even gain more structure by imposing sequentiality on individual
    records. To do the transformation just indicated, we simply do the following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Again, we cache the result to improve the performance of the algorithm, this
    time, `prefixspan`. Running the algorithm itself is done as before:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We set the minimum support value very low at 0.5%, to get a slightly bigger
    result set this time. Note that we also search for patterns no longer than 15
    sequence items. Let''s analyze the distribution over a frequent sequence length
    by running the following:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In this chain of operations, we first map each sequence to a key-value pair
    consisting of its length and a count of 1\. We then proceed with a reduce operation
    that sums up the values by key, that is, we count how often this length occurs.
    The rest is just sorting and formatting, which yields the following result:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00146.jpeg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: As we can see, the longest sequence has a length of 14, which, in particular,
    means that our maximum value of 15 did not restrict the search space and we found
    all the sequential patterns for the chosen support threshold of `t=0.005`. Interestingly,
    most of the frequent sequential visits of users have a length between two and
    six touch points on [http://msnbc.com](http://msnbc.com).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete this example, let''s see what the most frequent pattern of each
    length is and what the longest sequential pattern actually looks like. Answering
    the second question will also give us the first, since there is only one length-14
    pattern. Computing this can be done as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Since this is one of the more complicated RDD operations we''ve considered
    so far, let''s discuss all the steps involved. We first map each frequent sequence
    to a pair consisting of its length and the sequence itself. This may seem a bit
    strange at first, but it allows us to group all the sequences by length, which
    we do in the next step. Each group consists of its key and an iterator over frequent
    sequences*.* We map each group to its iterator and reduce the sequences by only
    keeping the one with the largest frequency. To then properly display the result
    of this operation, we make use of `mkString` twice to create a string from the
    otherwise not readable nested arrays (when printed). The result of the preceding
    chain is as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00147.jpeg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: We discussed earlier that *front page* was the most frequent item by far, which
    makes a lot of sense intuitively, since it is the natural entry point to a website.
    However, it is a bit of a surprise that the most frequent sequences of all lengths,
    for the chosen threshold, consist of *front page* hits only. Apparently many users
    spend a lot of time, and clicks, in and around the front page, which might be
    a first indication of it's advertising value, as compared to the pages of the
    other categories. As we indicated in the introduction of this chapter, analyzing
    data like this, especially if enriched with other data sources, can be of a tremendous
    value for owners of the respective websites, and we hope to have shown how frequent
    pattern mining techniques can serve their part in doing so.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a pattern mining application
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The example developed in the last section was an interesting playground to apply
    the algorithms we have carefully laid out throughout the chapter, but we have
    to recognize the fact that *we were just handed the data.* At the time of writing
    this book, it was often part of the culture in building data products to draw
    a line in the sand between *data science* and *data engineering *at pretty much
    exactly this point, that is, between real-time data collection and aggregation,
    and (often offline) analysis of data, followed up by feeding back reports of the
    insights gained into the production system. While this approach has its value,
    there are certain drawbacks to it as well. By not taking the full picture into
    account, we might, for instance, not exactly know the details of how the data
    has been collected. Missing information like this can lead to false assumptions
    and eventually wrong conclusions. While specialization is both useful and necessary
    to a certain degree, at the very least, practitioners should strive to get a basic
    understanding of applications *end-to-end*.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'When we introduced the MSNBC data set in the last section, we said that it
    had been retrieved from the server logs of the website. We drastically simplified
    what this entails, so let us have a closer look:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '*High availability and fault tolerance: *Click events on a website need to
    be tracked without downtime at any point throughout the day. Some businesses,
    especially when it comes to any sort of payment transactions, for example, in
    online shops, can not afford to lose certain events.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*High throughput of live data and scalability: *We need a system that can store
    and process such events in real time and can cope with a certain load without
    slowing down. For instance, the roughly *one million* unique users in the MSNBC
    data set mean that, on average, there is activity of about 11 users per second*.* There
    are many more events to keep track of, especially keeping in mind that the only
    thing we have measured were page views.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Streaming data and batching thereof: *In principle, the first two points could
    be addressed by writing events to a sufficiently sophisticated log. However, we
    haven''t even touched the topic of aggregating data yet and we preferably need
    an online processing system to do so. First, each event has to be attributed to
    a user, which will have to be equipped with some sort of ID. Next, we will have
    to think about the concept of a user session. While the user data has been aggregated
    on a daily level in the MSNBC data set, this is not granular enough for many purposes.
    It makes sense to analyze users'' behavior for the time period they are actually
    active. For this reason, it is customary to consider* windows* of activities and
    aggregate clicks and other events as per such windows.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Analytics on streaming data: *Assuming we had a system like we just described
    and access to aggregated user session data in real time, what could we hope to
    achieve? We would need an analytics platform that allows us to apply algorithms
    and gain insights from this data.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark's proposal to address these problems is its **Spark Streaming**module,
    which we will briefly introduce next. Using Spark Streaming, we will build an
    application that can at least *mock *generating and aggregating events in order
    to then apply the pattern mining algorithms we studied to *streams of events.*
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: The Spark Streaming module
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is not enough time to give an in-depth introduction to Spark Streaming
    here, but we can, at the very least, touch on some of the key notions, provide
    some examples, and give some guidance to more advanced topics.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark Streaming is Spark''s module for stream data processing, and it is indeed
    equipped with all the properties we explained in the preceding list: it is a highly
    fault-tolerant, scalable, and high-throughput system for processing and analyzing
    streams of live data. Its API is a natural extension of Spark itself and many
    of the tools available for RDDs and DataFrames carry over to Spark Streaming.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: The core abstraction of Spark Streaming applications is the notion of *DStream,* which
    stands for *discretized stream. *To explain the nomenclature, we often think of
    data streams as a continuous flow of events,which is, of course, an idealization,
    since all we can ever measure are discrete events. Regardless, this continuous
    flow of data will hit our system, and for us to process it further, we *discretize*
    it into disjoint batches of data. This stream of discrete data batches is realized
    as DStream in Spark Streaming and is internally implemented as a *sequence of
    RDDs*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram gives a high-level overview of the data flow and transformation
    with Spark Streaming:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00148.jpeg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Input data is fed into Spark Streaming, which discretises this stream
    as a so called DStream. These sequences of RDDs can then be further transformed
    and processed by Spark and any module thereof.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: As the diagram shows, the data enters Spark Streaming through an input data
    stream. This data can be produced and ingested from many different sources, which
    we will discuss further later on. We speak of systems generating events that Spark
    Streaming can process as *sources*.Input DStreams take data from sources and do
    so via *receivers* for these sources. Once an input DStream has been created,
    it can be processed through a rich API that allows for many interesting transformations.
    It serves as a good mental model to think of DStreams as sequences or collections
    of RDDs, which we can operate on through an interface that is very close to that
    of RDDs in the Spark core. For instance, operations such as map-reduce, and filter
    are available for DStreams as well and simply carry over the respective functionality
    from the individual RDDs to sequences of RDDs. We will discuss all of this in
    more detail, but let's first turn to a basic example.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: As the first example to get started with Spark Streaming, let's consider the
    following scenario. Assume that we have already loaded the MSNBC data set from
    earlier and have computed the prefix span model (`psModel`) from it. This model
    was fit with data from a single day of user activity, say, yesterday's data. Today,
    new events of user activity come in. We will create a simple Spark Streaming application
    with a basic source that generates user data in precisely the schema we had for
    the MSNBC data; that is, we are given space-separated strings containing numbers
    between 1 and 17\. Our application will then pick up these events and create `DStream`
    from them. We can then apply our prefix span model to the data of `DStream` to
    find out if the new sequences fed into the system are indeed frequent sequences
    according to `psModel`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with a Spark Streaming application in the first place, we need to
    create a so-called `StreamingContext` API, which, by convention, will be instantiated
    as `ssc`. Assuming that we start an application from scratch, we create the following
    context:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If you work with the Spark shell, all but the first and last lines are not necessary,
    since, in such a case, you will be provided with a Spark context (`sc`) already.
    We include the creation of the latter regardless, since we aim at a self-contained
    application. The creation of a new `StreamingContext` API takes two arguments,
    namely a `SparkContext` and an argument called `batchDuration`, which we set to
    10 seconds. The batch duration is the value that tells us *how to discretize *data
    for a `DStream`, by specifying for how long the streaming data should be collected
    to form a batch within the `DStream`, that is, one of the RDDs in the sequence.
    Another detail we want to draw your attention to is that the Spark master is set
    to two cores by setting `local[2]`. Since we assume you are working locally, it
    is important to assign at least two cores to the application. The reason is that
    one thread will be used to receive input data, while the other will then be free
    to process it. Should you have more receivers in more advanced applications, you
    need to reserve one core for each.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we essentially repeat parts of the prefix span model for the sake of
    completeness of this application. As before, the sequences are loaded from a local
    text file. Note that this time, we assume the file is in the resources folder
    of your project, but you can choose to store it anywhere you want:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the last step of the preceding computation, we collect all the frequent sequences
    on the master and store them as `freqSequences`. The reason we do this is that
    we want to compare this data against the incoming data to see if the sequences
    of the new data are frequent with respect to the current model (`psModel`). Unfortunately,
    unlike many of the algorithms from MLlib, none of the three available pattern
    mining models in Spark are built to take new data once trained, so we have to
    do this comparison on our own, using `freqSequences`.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can finally create a `DStream` object of the `String` type. To this
    end, we call `socketTextStream` on our streaming context, which will allow us
    to receive data from a server, running on port `8000` of `localhost`, listening
    on a TCP socket:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'What we call `rawSequences` is the data received through that connection, discretized
    into 10-second intervals. Before we discuss *how to actually send data*, let''s
    first continue with the example of processing it once we have received it. Recall
    that the input data will have the same format as before, so we need to preprocess
    it in exactly the same way, as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The two `map` operations we use here are structurally the same as before on
    the original MSNBC data, but keep in mind that this time, `map` has a different
    context, since we are working with DStreams instead of RDDs. Having defined `sequences`,
    a sequence of RDDs of the `Array[Array[Int]]` type, we can use it to match against
    `freqSequences`. We do so by iterating over each RDD in sequences and then again
    over each array contained in these RDDs. Next, we count how often the respective array is
    found in `freqSequences`, and if it is found, we print that the sequence corresponding
    to `array` is indeed frequent:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that in the preceding code, we need to compare deep copies of arrays, since
    nested arrays can't be compared on the nose. To be more precise, one can check
    them for equality, but the result will always be false.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Having done the transformation, the only thing we are left with on the receiving
    side of the application is to actually tell it to start listening to the incoming
    data:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Through the streaming context, `ssc`, we tell the application to start and await
    its termination. Note that in our specific context and for most other applications
    of this fashion, we rarely want to terminate the program at all. By design, the
    application is intended as a *long-running job*, since, in principle, we want
    it to listen to and analyze new data indefinitely. Naturally, there will be cases
    of maintenance, but we may also want to regularly update (re-train) `psModel`
    with the newly acquired data.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: We have already seen a few operations on DStreams and we recommend you to refer
    to the latest Spark Streaming documentation ([http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html))
    for more details. Essentially, many of the (functional) programming functionalities
    available on basic Scala collections that we also know from RDDs carry over seamlessly
    to DStreams as well. To name a few, these are `filter`, `flatMap`, `map`, `reduce`,
    and `reduceByKey`. Other SQL-like functionalities, such as cogroup, `count`, `countByValue`,
    `join`, or `union`, are also at your disposal. We will see some of the more advanced
    functionalities later on in a second example.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have covered the receiving end, let''s briefly discuss how to create
    a data source for our app. One of the simplest ways to send input data from a
    command line over a TCP socket is to use *Netcat*, which is available for most
    operating systems, often preinstalled. To start Netcat locally on port `8000`,
    run the following command in a terminal separate from your Spark application or
    shell:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Assuming you already started the Spark Streaming application for receiving
    data from before, we can now type new sequences into the Netcat terminal window
    and confirm each by hitting *Enter*. For instance, type the following four sequences *within
    10 seconds*:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00149.jpeg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: 'You will see the following output:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00150.jpeg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: If you are either really slow at typing or so unlucky that you start typing
    when the 10-second window is almost over, the output might be split into more
    parts. Looking at the actual output, you will see that the often discussed categories *front
    page* and *news*,represented by categories 1 and 2, are frequent. Also, since
    23 is not a sequence item contained in the original data set, it can't be frequent.
    Lastly, the sequence <4, 5> is apparently also not frequent, which is something
    we didn't know before.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing Netcat for this example is a natural choice for the time and space
    given in this chapter, but you will never see it used for this purpose in serious
    production environments. In general, Spark Streaming has two types of sources
    available: basic and advanced. Basic sources can also be queues of RDDsand other
    custom sources apart from file streams, which the preceding example represents.
    On the side of advanced sources, Spark Streaming has a lot of interesting connectors
    to offer: Kafka, Kinesis, Flume, and advanced custom sources*.* This wide variety
    of advanced sources makes it attractive to incorporate Spark Streaming as a production
    component, integrating well with the other infrastructure components.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking a few steps back and considering what we have achieved by discussing
    this example, you may be inclined to say that apart from introducing Spark Streaming
    itself and working with data producers and receivers, the application itself did
    not solve many of our aforementioned concerns. This criticism is valid, and in
    a second example, we want to address the following remaining issues with our approach:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Input data for our DStreams had the same structure as our offline data, that
    is, it was already pre-aggregated with respect to users, which is not very realistic
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from the two calls to `map` and one to `foreachRDD`, we didn't see much
    in terms of functionality and added value in operating with DStreams
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We did not do any analytics on data streams but only checked them against a
    list of precomputed patterns
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To resolve these issues, let's slightly redefine our example setting. This time,
    let's assume that one event is represented by one user clicking on one site, where
    each such site falls under one of the categories 1-17, as before. Now, we cannot
    possibly simulate a complete production environment, so we make the simplifying
    assumption that each unique user has already been equipped with an ID. Given this
    information, let's say events come in as key-value pairs consisting of a user
    ID and a category of this click event.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: With this setup, we have to think about how to aggregate these events to generate
    sequences from them. For this purpose, we need to collect data points for each
    user ID in a given window*. *In the original data set, this window was obviously
    one full day, but depending on the application, it may make sense to choose a
    much smaller window. If we think about the scenario of a user browsing his favorite
    online shop, the click and other events that go back a few hours will unlikely
    influence his or her current desire to buy something. For this reason, a reasonable
    assumption made in online marketing and related fields is to limit the window
    of interest to about 20-30 minutes, a so-called *user session.* In order for us
    to see results much quicker, we will use an even smaller window of 20 seconds
    for our application. We call this the **window length***.*
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know how far back we want to analyze the data from a given point
    in time, we also have to define *how often* we want to carry out the aggregation
    step, which we will call the *sliding interval.* One natural choice would be to
    set both to the same amount of time, leading to disjoint windows of aggregation,
    that is, every 20 seconds. However, it might also be interesting to choose a shorter
    sliding window of 10 seconds, which would lead to aggregation data that overlaps
    10 seconds each. The following diagram illustrates the concepts we just discussed:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00151.jpeg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Visualisation of a window operation transforming a DStream to another.
    In this example the batch duration of the Spark Streaming application has been
    set to 10 seconds. The window length for the transformation operating on batches
    of data is 40 seconds and we carry out the window operation every 20 seconds,
    leading to an overlap of 20 seconds each and a resulting DStream that is batched
    in 20-second blocks.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'To turn this knowledge into a concrete example, we assume that the event data
    has the form *key:value*, that is, one such event could be `137: 2`, meaning that
    the user with ID `137` clicked on a page with the category *news*. To process
    these events, we have to modify our preprocessing like this:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'With these key-value pairs, we can now aim to do the aggregation necessary
    to group events by the user ID. As outlined earlier, we do this by aggregating
    on a given window of 20 seconds with a sliding interval of 10 seconds:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In the preceding code, we are using a more advanced operation on DStreams, namely
    `reduceByKeyAndWindow`, in which we specify an aggregation function on values
    of key-value pairs, as well as a window duration and sliding interval. In the
    last step of the computation, we strip the user IDs so that the structure of `rawSequences`
    is identical to the previous example. This means that we have successfully converted
    our example to work on unprocessed events, and it will still check against frequent
    sequences of our baseline model. We will not show more examples of how the output
    of this application looks, but we encourage you to play around with this application
    and see how the aggregation works on key-value pairs.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'To wrap up this example, and the chapter, let''s look at one more interesting
    way of aggregating event data. Let''s say we want to dynamically count how often
    a certain ID occurs in the event stream, that is, how many page clicks a user
    generates. We already have our `events` DStream defined previously, so we could
    approach the count as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In a way, this works as we intended; it counts events for IDs. However, note
    that what is returned is again a DStream, that is, we do not actually aggregate *across
    streaming windows* but just within the sequences of RDDs. To aggregate across
    the full stream of events, we need to keep track of count states since from the
    start. Spark Streaming offers a method on DStreams for precisely this purpose,
    namely `updateStateByKey`. It can be used by providing `updateFunction`, which
    takes the current state and new values as input and returns an updated state.
    Let''s see how it works in practice for our event count:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We first define our update function itself. Note that the signature of `updateStateByKey`
    requires us to return an `Option`, but in essence, we just compute the running
    sum of state and incoming values. Next, we provide `updateStateByKey` with an
    `Int` type signature and the previously created `updateFunction` method. Doing
    so, we get precisely the aggregation we wanted in the first place.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing, we introduced event aggregation, two more complex operations on
    DStreams (`reduceByKeyAndWindow` and `updateStateByKey`), and counted events in
    a stream with this example. While the example is still simplistic in what it does,
    we hope to have provided the reader with a good entry point for more advanced
    applications. For instance, one could extend this example to calculate moving
    averages over the event stream or change it towards computing frequent patterns
    on a per-window basis.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced a new class of algorithms, that is, frequent
    pattern mining applications, and showed you how to deploy them in a real-world
    scenario. We first discussed the very basics of pattern mining and the problems
    that can be addressed using these techniques. In particular, we saw how to implement
    the three available algorithms in Spark, *FP-growth*, *association rules*, and
    *prefix span*. As a running example for the applications we used clickstream data
    provided by MSNBC, which also helped us to compare the algorithms qualitatively.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Next, we introduced the basic terminology and entry points of Spark Streaming
    and considered a few real-world scenarios. We discussed how to deploy and evaluate
    one of the frequent pattern mining algorithms with a streaming context first.
    After that, we addressed the problem of aggregating user session data from raw
    streaming data. To this end, we had to find a solution to mock providing click
    data as streaming events.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
