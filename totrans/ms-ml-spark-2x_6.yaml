- en: Extracting Patterns from Clickstream Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When collecting real-world data between individual measures or events, there
    are usually very intricate and highly complex relationships to observe. The guiding
    example for this chapter is the observation of click events that users generate
    on a website and its subdomains. Such data is both interesting and challenging
    to investigate. It is interesting, as there are usually many *patterns* that groups
    of users show in their browsing behavior and certain *rules *they might follow.
    Gaining insights about user groups, in general, is of interest, at least for the
    company running the website and might be the focus of their data science team.
    Methodology aside, putting a production system in place that can detect patterns
    in real time, for instance, to find malicious behavior, can be very challenging
    technically. It is immensely valuable to be able to understand and implement both
    the algorithmic and technical sides.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look into two topics in depth: doing *pattern mining *and
    working with *streaming data *in Spark. The chapter is split up into two main
    sections. In the first, we will introduce the three available pattern mining algorithms
    that Spark currently comes with and apply them to an interesting dataset. In the
    second, we will take a more technical view on things and address the core problems
    that arise when deploying a streaming data application using algorithms from the
    first part. In particular, you will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The basic principles of frequent pattern mining.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Useful and relevant data formats for applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to load and analyze a clickstream data set generated from user activity
    on [http://MSNBC.com](http://MSNBC.com).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding and comparing three pattern mining algorithms available in Spark,
    namely *FP-growth, association rules*, and **prefix span.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to apply these algorithms to MSNBC click data and other examples to identify
    the relevant patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The very basics of *Spark Streaming* and what use cases can be covered by it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to put any of the preceding algorithms into production by deploying them
    with Spark Streaming.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a more realistic streaming application with click events aggregated
    on the fly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By construction, this chapter is more technically involved towards the end,
    but with *Spark Streaming* it also allows us to introduce yet another very important
    tool from the Spark ecosphere. We start off by presenting some of the basic questions
    of pattern mining and then discuss how to address them.
  prefs: []
  type: TYPE_NORMAL
- en: Frequent pattern mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When presented with a new data set, a natural sequence of questions is:'
  prefs: []
  type: TYPE_NORMAL
- en: What kind of data do we look at; that is, what structure does it have?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which observations in the data can be found frequently; that is, which patterns
    or rules can we identify within the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we assess what is frequent; that is, what are the good measures of relevance
    and how do we test for it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On a very high level, frequent pattern mining addresses precisely these questions.
    While it's very easy to dive head first into more advanced machine learning techniques,
    these pattern mining algorithms can be quite informative and help build an intuition
    about the data.
  prefs: []
  type: TYPE_NORMAL
- en: To introduce some of the key notions of frequent pattern mining, let's first
    consider a somewhat prototypical example for such cases, namely shopping carts.
    The study of customers being interested in and buying certain products has been
    of prime interest to marketers around the globe for a very long time. While online
    shops certainly do help in further analyzing customer behavior, for instance,
    by tracking the browsing data within a shopping session, the question of what
    items have been bought and what patterns in buying behavior can be found applies
    to purely offline scenarios as well. We will see a more involved example of clickstream
    data accumulated on a website soon; for now, we will work under the assumption
    that only the events we can track are the actual payment transactions of an item.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just this given data, for instance, for groceries shopping carts in supermarkets
    or online, leads to quite a few interesting questions, and we will focus mainly
    on the following three:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Which items are frequently bought together?* For instance, there is anecdotal
    evidence suggesting that beer and diapers are often bought together in one shopping
    session. Finding patterns of products that often go together may, for instance,
    allow a shop to physically place these products closer to each other for an increased
    shopping experience or promotional value even if they don''t belong together at
    first sight. In the case of an online shop, this sort of analysis might be the
    base for a simple recommender system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the previous question, *are there any interesting implications or rules
    to observe in shopping behaviour?,* continuing with the shopping cart example,
    can we establish associations such as *if bread and butter have been bought, we
    also often find cheese in the shopping cart*? Finding such association rules can
    be of great interest, but also need more clarification of what we consider to
    be *often*, that is, what does frequent mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that, so far, our shopping carts were simply considered a *bag of items* without
    additional structure. At least in the online shopping scenario, we can endow data
    with more information. One aspect we will focus on is that of the *sequentiality *of
    items; that is, we will take note of the order in which the products have been
    placed into the cart. With this in mind, similar to the first question, one might
    ask, *which sequence of items can often be found in our transaction data? *For
    instance, larger electronic devices bought might be followed up by additional
    utility items.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reason we focus on these three questions in particular is that Spark MLlib
    comes with precisely three pattern mining algorithms that roughly correspond to
    the aforementioned questions by their ability to answer them. Specifically, we
    will carefully introduce *FP-growth*, *association rules*, and *prefix span, *in
    that order, to address these problems and show how to solve them using Spark.
    Before doing so, let's take a step back and formally introduce the concepts we
    have been motivated for so far, alongside a running example. We will refer to
    the preceding three questions throughout the following subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Pattern mining terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start with a set of items *I = {a[1], ..., a[n]}*, which serves as the
    base for all the following concepts. A *transaction* T is just a set of items
    in I, and we say that T is a transaction of length *l* if it contains *l* item.
    A *transaction database* D is a database of transaction IDs and their corresponding
    transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give a concrete example of this, consider the following situation. Assume
    that the full item set to shop from is given by *I = {bread, cheese, ananas, eggs,
    donuts, fish, pork, milk, garlic, ice cream, lemon, oil, honey, jam, kale, salt}*.
    Since we will look at a lot of item subsets, to make things more readable later
    on, we will simply abbreviate these items by their first letter, that is, we''ll
    write *I = {b, c, a, e, d, f, p, m, g, i, l, o, h, j, k, s}*. Given these items,
    a small transaction database D could look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Transaction ID** | **Transaction** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | a, c, d, f, g, i, m, p |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | a, b, c, f, l, m, o |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | b, f, h, j, o |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | b, c, k, s, p |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | a, c, e, f, l, m, n, p |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: A small shopping cart database with five transactions'
  prefs: []
  type: TYPE_NORMAL
- en: Frequent pattern mining problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given the definition of a transaction database, a *pattern* P is a *transaction
    contained in the transactions in D* and the support, *supp(P)*, of the pattern
    is the number of transactions for which this is true, divided or normalized by
    the number of transactions in D:'
  prefs: []
  type: TYPE_NORMAL
- en: '*supp(s) = supp[D](s) = |{ s'' ∈ S | s <s''}| / |D|*'
  prefs: []
  type: TYPE_NORMAL
- en: We use the *<* symbol to denote *s* as a subpattern of *s'* or, conversely,
    call *s'* a superpattern of *s*. Note that in the literature, you will sometimes
    also find a slightly different version of support that does not normalize the
    value. For example, the pattern *{a, c, f}* can be found in transactions 1, 2,
    and 5\. This means that *{a, c, f}* is a pattern of support *0.6* in our database
    D of five items.
  prefs: []
  type: TYPE_NORMAL
- en: Support is an important notion, as it gives us a first example of measuring
    the frequency of a pattern, which, in the end, is what we are after. In this context,
    for a given minimum support threshold *t*, we say *P* is a frequent pattern if
    and only if *supp(P)* is at least *t*. In our running example, the frequent patterns
    of length 1 and minimum support *0.6* are *{a}*, *{b}*, *{c}*, *{p}*, and *{m}*
    with support 0.6 and *{f}* with support 0.8\. In what follows, we will often drop
    the brackets for items or patterns and write *f* instead of *{f}*, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: Given a minimum support threshold, the problem of finding all the frequent patterns
    is called the *frequent pattern mining problem* and it is, in fact, the formalized
    version of the aforementioned first question. Continuing with our example, we
    have found all frequent patterns of length 1 for *t = 0.6* already. How do we
    find longer patterns? On a theoretical level, given unlimited resources, this
    is not much of a problem, since all we need to do is count the occurrences of
    items. On a practical level, however, we need to be smart about how we do so to
    keep the computation efficient. Especially for databases large enough for Spark
    to come in handy, it can be very computationally intense to address the frequent
    pattern mining problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'One intuitive way to go about this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Find all the frequent patterns of length 1, which requires one full database
    scan. This is how we started with in our preceding example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For patterns of length 2, generate all the combinations of frequent 1-patterns,
    the so-called candidates*,* and test if they exceed the minimum support by doing
    another scan of D.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Importantly, we do not have to consider the combinations of infrequent patterns,
    since patterns containing infrequent patterns can not become frequent. This rationale
    is called the **apriori principle***.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For longer patterns, continue this procedure iteratively until there are no
    more patterns left to combine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This algorithm, using a generate-and-test approach to pattern mining and utilizing
    the apriori principle to bound combinations, is called the apriori algorithm.
    There are many variations of this baseline algorithm, all of which share similar
    drawbacks in terms of scalability. For instance, multiple full database scans
    are necessary to carry out the iterations, which might already be prohibitively
    expensive for huge data sets. On top of that, generating candidates themselves
    is already expensive, but computing their combinations might simply be infeasible.
    In the next section, we will see how a parallel version of an algorithm called
    *FP-growth*, available in Spark, can overcome most of the problems just discussed.
  prefs: []
  type: TYPE_NORMAL
- en: The association rule mining problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To advance our general introduction of concepts, let''s next turn to *association
    rules, *as first introduced in *Mining Association Rules between Sets of Items
    in Large Databases*, available at [http://arbor.ee.ntu.edu.tw/~chyun/dmpaper/agrama93.pdf](http://arbor.ee.ntu.edu.tw/~chyun/dmpaper/agrama93.pdf).
    In contrast to solely counting the occurrences of items in our database, we now
    want to understand the rulesor implications of patterns. What I mean is, given
    a pattern *P[1]* and another pattern *P[2]*, we want to know whether *P[2]* is
    frequently present whenever *P[1]* can be found in *D*, and we denote this by
    writing *P[1 ]⇒ P[2]*. To make this more precise, we need a concept for rule frequency
    similar to that of support for patterns, namely *confidence. *For a rule *P[1 ]⇒ P[2]*,
    confidence is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*conf(P[1] ⇒ P[2]) = supp(P[1] ∪ P[2]) / supp(P[1])*'
  prefs: []
  type: TYPE_NORMAL
- en: This can be interpreted as the conditional support of *P[2]* given to *P[1]*;
    that is, if it were to restrict *D* to all the transactions supporting *P[1]*,
    the support of *P[2]* in this restricted database would be equal to *conf(P[1 ]⇒ P[2])*.
    We call *P[1 ]⇒ P[2]* a rule in *D* if it exceeds a minimum confidence threshold
    *t*, just as in the case of frequent patterns. Finding all the rules for a confidence
    threshold represents the formal answer to the second question, *association rule
    mining. *Moreover, in this situation, we call P[1 ]the *antecedent *and P[2] the *consequent *of
    the rule. In general, there is no restriction imposed on the structure of either
    the antecedent or the consequent. However, in what follows, we will assume that
    the consequent's length is 1, for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our running example, the pattern *{f, m}* occurs three times, while *{f,
    m, p}* is just present in two cases, which means that the rule *{f, m} **⇒ {p}*
    has confidence *2/3*. If we set the minimum confidence threshold to *t = 0.6*,
    we can easily check that the following association rules with an antecedent and
    consequent of length 1 are valid for our case:'
  prefs: []
  type: TYPE_NORMAL
- en: '*{a} ⇒ {c}, **{a} ⇒ {f}, {a} ⇒ {m}, {a} ⇒ {p}** {c} ⇒ {a}, {c} ⇒ {f}, {c} ⇒
    {m}, {c} ⇒ {p}** {f} ⇒ {a}, {f} ⇒ {c}, {f} ⇒ {m}** {m} ⇒ {a}, {m} ⇒ {c}, {m} ⇒ {f}, {m}
    ⇒ {p}** {p} ⇒ {a}, {p} ⇒ {c}, {p} ⇒ {f}, {p} ⇒ {m}*'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding definition of confidence, it should now be clear that it
    is relatively straightforward to compute the association rules once we have the
    support value of all the frequent patterns. In fact, as we will soon see, Spark's
    implementation of association rules is based on calculating frequent patterns
    upfront.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it should be noted that while we will restrict ourselves to the
    measures of support and confidence, there are many other interesting criteria
    available that we can't discuss in this book; for instance, the concepts of *conviction,
    leverage, *or *lift. *For an in-depth comparison of the other measures, refer
    to [http://www.cse.msu.edu/~ptan/papers/IS.pdf](http://www.cse.msu.edu/~ptan/papers/IS.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: The sequential pattern mining problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s move on to formalizing, the third and last pattern matching question
    we tackle in this chapter. Let''s look at *sequences* in more detail. A sequence
    is different from the transactions we looked at before in that the order now matters.
    For a given item set *I*, a sequence *S* in *I* of length *l* is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*s = <s[1,] s[2],..., s[l]>*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, each individual *s[i]* is a concatenation of items, that is, *s[i] =
    (a[i1] ... a[im)]*, where *a[ij]* is an item in *I*. Note that we do care about
    the order of sequence items *s[i]* but not about the internal ordering of the
    individual *a[ij] *in *s[i]*. A sequence database *S* consists of pairs of sequence
    IDs and sequences, analogous to what we had before. An example of such a database
    can be found in the following table, in which the letters represent the same items
    as in our previous shopping cart example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sequence ID** | **Sequence** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | *<a(abc)(ac)d(cf)>* |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | *<(ad)c(bc)(ae)>* |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | *<(ef)(ab)(df)cb>* |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | *<eg(af)cbc>* |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: A small sequence database with four short sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example sequences, note the round brackets to group individual items
    into a sequence item. Also note that we drop these redundant braces if the sequence
    item consists of a single item. Importantly, the notion of a subsequence requires
    a little more carefulness than for unordered structures. We call *u = (u[1], ...,
    u[n])* a subsequenceof *s = (s[1],..., s[l])* and write *u <s* if there are indices
    *1 **≤ i1 < i2 < ... < in ≤ m* so that we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*u[1] < s[i1], ..., u[n] <s[in]*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the *<* signs in the last line mean that *u[j]* is a subpattern of*s[ij]*.
    Roughly speaking, *u* is a subsequence of *s* if all the elements of *u* are subpatterns
    of *s* in their given order. Equivalently, we call *s* a supersequence of *u*.
    In the preceding example, we see that *<a(ab)ac>* and *a(cb)(ac)dc>* are examples
    of subsequences of *<a(abc)(ac)d(cf)>* and that *<(fa)c>* is an example of a subsequence
    of *<eg(af)cbc>*.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the help of the notion of supersequences, we can now define the *support*
    of a sequence *s* in a given sequence database *S* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*supp[S](s) = supp(s) = |{ s'' ∈ S | s <s''}| / |S|*'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, structurally, this is the same definition as for plain unordered
    patterns, but the *<* symbol means something else, that is, a subsequence. As
    before, we drop the database subscript in the notation of *support* if the information
    is clear from the context. Equipped with a notion of *support*, the definition
    of sequential patterns follows the previous definition completely analogously.
    Given a minimum support threshold *t*, a sequence *s* in *S* is said to be a *sequential
    pattern* if *supp(s)* is greater than or equal to *t*. The formalization of the
    third question is called the *sequential pattern mining problem*, that is, find
    the full set of sequences that are sequential patterns in *S* for a given threshold
    *t*.
  prefs: []
  type: TYPE_NORMAL
- en: Even in our little example with just four sequences, it can already be challenging
    to manually inspect all the sequential patterns. To give just one example of a
    sequential pattern of *support 1.0*, a subsequence of length 2 of all the four
    sequences is *<ac>*. Finding all the sequential patterns is an interesting problem,
    and we will learn about the so-called *prefix span *algorithm that Spark employs
    to address the problem in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Pattern mining with Spark MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After having motivated and introduced three pattern mining problems along with
    the necessary notation to properly talk about them, we will next discuss how each
    of these problems can be solved with an algorithm available in Spark MLlib. As
    is often the case, actually applying the algorithms themselves is fairly simple
    due to Spark MLlib's convenient `run` method available for most algorithms. What
    is more challenging is to understand the algorithms and the intricacies that come
    with them. To this end, we will explain the three pattern mining algorithms one
    by one, and study how they are implemented and how to use them on toy examples.
    Only after having done all this will we apply these algorithms to a real-life
    data set of click events retrieved from [http://MSNBC.com](http://MSNBC.com).
  prefs: []
  type: TYPE_NORMAL
- en: The documentation for the pattern mining algorithms in Spark can be found at [https://spark.apache.org/docs/2.1.0/mllib-frequent-pattern-mining.html](https://spark.apache.org/docs/2.1.0/mllib-frequent-pattern-mining.html).
    It provides a good entry point with examples for users who want to dive right
    in.
  prefs: []
  type: TYPE_NORMAL
- en: Frequent pattern mining with FP-growth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we introduced the frequent pattern mining problem, we also quickly discussed
    a strategy to address it based on the apriori principle. The approach was based
    on scanning the whole transaction database again and again to expensively generate
    pattern candidates of growing length and checking their support. We indicated
    that this strategy may not be feasible for very large data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The so called *FP-growth algorithm*, where **FP** stands for **frequent pattern**,
    provides an interesting solution to this data mining problem. The algorithm was
    originally described in *Mining Frequent Patterns without Candidate Generation,* available
    at [https://www.cs.sfu.ca/~jpei/publications/sigmod00.pdf](https://www.cs.sfu.ca/~jpei/publications/sigmod00.pdf).
    We will start by explaining the basics of this algorithm and then move on to discussing
    its distributed version, *parallel FP-growth, *which has been introduced in *PFP:
    Parallel FP-Growth for Query Recommendation*, found at [https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/34668.pdf](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/34668.pdf).
    While Spark''s implementation is based on the latter paper, it is best to first
    understand the baseline algorithm and extend from there.'
  prefs: []
  type: TYPE_NORMAL
- en: The core idea of FP-growth is to scan the transaction database D of interest
    precisely once in the beginning, find all the frequent patterns of length 1, and
    build a special tree structure called *FP-tree *from these patterns. Once this
    step is done, instead of working with D, we only do recursive computations on
    the usually much smaller FP-tree. This step is called the *FP-growth step *of
    the algorithm, since it recursively constructs trees from the subtrees of the
    original tree to identify patterns. We will call this procedure *fragment pattern
    growth*, which does not require us to generate candidates but is rather built
    on a *divide-and-conquer *strategy that heavily reduces the workload in each recursion
    step.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be more precise, let''s first define what an FP-tree is and what it looks
    like in an example. Recall the example database we used in the last section, shown
    in *Table 1*. Our item set consisted of the following 15 grocery items, represented
    by their first letter: *b*, *c*, *a*, *e*, *d*, *f*, *p*, *m*, *i*, *l*, *o*,
    *h*, *j*, *k*, *s*. We also discussed the frequent items; that is, patterns of
    length 1, for a minimum support threshold of *t = 0.6*, were given by *{f, c,
    b, a, m, p}*. In FP-growth, we first use the fact that the ordering of items does
    not matter for the frequent pattern mining problem; that is, we can choose the
    order in which to present the frequent items. We do so by ordering them by decreasing
    frequency. To summarize the situation, let''s have a look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Transaction ID** | **Transaction** | **Ordered frequent items** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | *a, c, d, f, g, i, m, p* | *f, c, a, m, p* |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | *a, b, c, f, l, m, o* | *f, c, a, b, m* |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | *b, f, h, j, o* | *f, b* |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | *b, c, k, s, p* | *c, b, p* |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | *a, c, e, f, l, m, n, p* | *f, c, a, m, p* |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Continuation of the example started with Table 1, augmenting the table
    by ordered frequent items.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, ordering frequent items like this already helps us to identify
    some structure. For instance, we see that the item set *{f, c, a, m, p}* occurs
    twice and is slightly altered once as *{f, c, a, b, m}*. The key idea of FP-growth
    is to use this representation to build a tree from the ordered frequent items
    that reflect the structure and interdependencies of the items in the third column
    of *Table 3*. Every FP-tree has a so-called *root *node that is used as a base
    for connecting ordered frequent items as constructed. On the right of the following
    diagram, we see what is meant by this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00139.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: FP-tree and header table for our frequent pattern mining running
    example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The left-hand side of *Figure 1* shows a header table that we will explain
    and formalize in just a bit, while the right-hand side shows the actual FP-tree.
    For each of the ordered frequent items in our example, there is a directed path
    starting from the root, thereby representing it. Each node of the tree keeps track
    of not only the frequent item itself but also of the number of paths traversed
    through this node. For instance, four of the five ordered frequent item sets start
    with the letter *f* and one with *c*. Thus, in the FP-tree, we see `f: 4` and
    `c: 1` at the top level. Another interpretation of this fact is that *f* is a
    *prefix* for four item sets and *c* for one. For another example of this sort
    of reasoning, let''s turn our attention to the lower left of the tree, that is,
    to the leaf node `p: 2`. Two occurrences of *p* tells us that precisely two identical
    paths end here, which we already know: *{f, c, a, m, p}* is represented twice.
    This observation is interesting, as it already hints at a technique used in FP-growth--starting
    at the leaf nodes of the tree, or the suffixes of the item sets, we can trace
    back each frequent item set, and the union of all these distinct root node paths
    yields all the paths--an important idea for parallelization.'
  prefs: []
  type: TYPE_NORMAL
- en: The header table you see on the left of *Figure 1* is a smart way of storing
    items. Note that by the construction of the tree, a node is not the same as a
    frequent item but, rather, items can and usually do occur multiple times, namely
    once for each distinct path they are part of. To keep track of items and how they
    relate, the header table is essentially a *linked list* of items, that is, each
    item occurrence is linked to the next by means of this table. We indicated the
    links for each frequent item by horizontal dashed lines in *Figure 1* for illustration
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: With this example in mind, let's now give a formal definition of an FP-tree.
    An FP-tree *T* is a tree that consists of a root node together with frequent item
    prefix subtreesstarting at the root and a frequent item header table.Each node
    of the tree consists of a triple, namely the item name, its occurrence count,
    and a node link referring to the next node of the same name, or `null` if there
    is no such next node.
  prefs: []
  type: TYPE_NORMAL
- en: To quickly recap, to build *T*, we start by computing the frequent items for
    the given minimum support threshold *t*, and then, starting from the root, insert
    each path represented by the sorted frequent pattern list of a transaction into
    the tree. Now, what do we gain from this? The most important property to consider
    is that all the information needed to solve the frequent pattern mining problem
    is encoded in the FP-tree *T* because we effectively encode all co-occurrences
    of frequent items with repetition. Since *T* can also have at most as many nodes
    as the occurrences of frequent items, *T* is usually much smaller than our original
    database D. This means that we have mapped the mining problem to a problem on
    a smaller data set, which in itself reduces the computational complexity compared
    with the naive approach sketched earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll discuss how to grow patterns recursively from fragments obtained
    from the constructed FP tree. To do so, let''s make the following observation.
    For any given frequent item *x*, we can obtain all the patterns involving *x*
    by following the node links for *x*, starting from the header table entry for
    *x*, by analyzing at the respective subtrees. To explain how exactly, we further
    study our example and, starting at the bottom of the header table, analyze patterns
    containing *p*. From our FP-tree *T*, it is clear that *p* occurs in two paths:
    *(f:4, c:3, a:3, m:3, p:2)* and *(c:1, b:1, p:1)*, following the node links for *p*.
    Now, in the first path, *p* occurs only twice, that is, there can be at most two
    total occurrences of the pattern *{f, c, a, m, p}* in the original database D.
    So, conditional on *p* being present*, *the paths involving *p* actually read
    as follows: *(f:2, c:2, a:2, m:2, p:2)* and *(c:1, b:1, p:1)*. In fact, since
    we know we want to analyze patterns, given *p*, we can shorten the notation a
    little and simply write *(f:2, c:2, a:2, m:2)* and *(c:1, b:1)*. This is what
    we call the **conditional pattern base for p**. Going one step further, we can
    construct a new FP-tree from this conditional database. Conditioning on three
    occurrences of *p*, this new tree does only consist of a single node, namely *(c:3)*.
    This means that we end up with *{c, p}* as a single pattern involving *p*, apart
    from *p* itself. To have a better means of talking about this situation, we introduce
    the following notation: the conditional FP-tree for *p* is denoted by *{(c:3)}|p*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To gain more intuition, let''s consider one more frequent item and discuss
    its conditional pattern base. Continuing bottom to top and analyzing *m*, we again
    see two paths that are relevant: *(f:4, c:3, a:3, m:2)* and *(f:4, c:3, a:3, b:1,
    m:1)*. Note that in the first path, we discard the *p:2* at the end, since we
    have already covered the case of *p*. Following the same logic of reducing all
    other counts to the count of the item in question and conditioning on *m*, we
    end up with the conditional pattern base *{(f:2, c:2, a:2), (f:1, c:1, a:1, b:1)}*.
    The conditional FP-tree in this situation is thus given by *{f:3, c:3, a:3}|m*.
    It is now easy to see that actually every possible combination of *m* with each
    of *f*, *c*, and *a* forms a frequent pattern. The full set of patterns, given
    *m*, is thus *{m}*, *{am}*, *{cm}*, *{fm}*, *{cam]*, *{fam}*, *{fcm}*, and *{fcam}*.
    By now, it should become clear as to how to continue, and we will not carry out
    this exercise in full but rather summarize the outcome of it in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Frequent pattern** | **Conditional pattern base** | **Conditional FP-tree**
    |'
  prefs: []
  type: TYPE_TB
- en: '| *p* | *{(f:2, c:2, a:2, m:2), (c:1, b:1)}* | *{(c:3)}&#124;p* |'
  prefs: []
  type: TYPE_TB
- en: '| *m* | *{(f :2, c:2, a:2), (f :1, c:1, a:1, b:1)}* | *{f:3, c:3, a:3}&#124;m*
    |'
  prefs: []
  type: TYPE_TB
- en: '| *b* | *{(f :1, c:1, a:1), (f :1), (c:1)}* | null |'
  prefs: []
  type: TYPE_TB
- en: '| *a* | *{(f:3, c:3)}* | *{(f:3, c:3)}&#124;a* |'
  prefs: []
  type: TYPE_TB
- en: '| *c* | *{(f:3)}* | *{(f:3)}&#124;c* |'
  prefs: []
  type: TYPE_TB
- en: '| *f* | null | null |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: The complete list of conditional FP-trees and conditional pattern
    bases for our running example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As this derivation required a lot of attention to detail, let''s take a step
    back and summarize the situation so far:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the original FP-tree *T*, we iterated through all the items using
    node links.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each item *x*, we constructed its conditional pattern base and its conditional
    FP-tree. Doing so, we used the following two properties:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We discarded all the items following *x* in each potential pattern, that is,
    we only kept the *prefix* of *x*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We modified the item counts in the conditional pattern base to match the count
    of *x*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying a path using the latter two properties, we called the transformed
    prefix path of *x*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To finally state the FP-growth step of the algorithm, we need two more fundamental
    observations that we have already implicitly used in the example. Firstly, the
    support of an item in a conditional pattern base is the same as that of its representation
    in the original database. Secondly, starting from a frequent pattern *x* in the
    original database and an arbitrary set of items *y*, we know that *xy* is a frequent
    pattern if and only if *y* is. These two facts can easily be derived in general,
    but should be clearly demonstrated in the preceding example.
  prefs: []
  type: TYPE_NORMAL
- en: 'What this means is that we can completely focus on finding patterns in conditional
    pattern bases, as joining them with frequent patterns is again a pattern, andthis
    way, we can find all the patterns. This mechanism of recursively growing patterns
    by computing conditional pattern bases is therefore called pattern growth, which
    is why FP-growth bears its name. With all this in mind, we can now summarize the
    FP-growth procedure in pseudocode, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With this procedure, we can summarize our description of the complete FP-growth
    algorithm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute frequent items from D and compute the original FP-tree *T* from them
    (*FP-tree computation).*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `fpGrowth(T, null)` (*FP-growth computation).*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Having understood the base construction, we can now proceed to discuss a parallel
    extension of base FP-growth, that is, the basis of Spark''s implementation. **Parallel
    FP-growth**, or **PFP** for short, is a natural evolution of FP-growth for parallel
    computing engines such as Spark. It addresses the following problems with the
    baseline algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Distributed storage:* For frequent pattern mining, our database D may not
    fit into memory, which can already render FP-growth in its original form unapplicable.
    Spark does help in this regard for obvious reasons.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Distributed computing: *With distributed storage in place, we will have to
    take care of parallelizing all the steps of the algorithm suitably as well and
    PFP does precisely this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adequate support values:* When dealing with finding frequent patterns, we
    usually do not want to set the minimum support threshold *t* too high so as to
    find interesting patterns in the long tail. However, a small *t* might prevent
    the FP-tree from fitting into memory for a sufficiently large D, which would force
    us to increase *t*. PFP successfully addresses this problem as well, as we will
    see.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The basic outline of PFP, with Spark for implementation in mind, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sharding**: Instead of storing our database D on a single machine, we distribute
    it to multiple partitions. Regardless of the particular storage layer, using Spark
    we can, for instance, create an RDD to load D.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel frequent item count**: The first step of computing frequent items
    of D can be naturally performed as a map-reduce operation on an RDD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Building groups of frequent items**: The set of frequent items is divided
    into a number of groups, each with a unique group ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel FP-growth**:The FP-growth step is split into two steps to leverage
    parallelism:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Map phase**:The output of a mapper is a pair comprising the group ID and
    the corresponding transaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduce phase**:Reducers collect data according to the group ID and carry
    out FP-growth on these group-dependent transactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregation**: The final step in the algorithm is the aggregation of results
    over group IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In light of already having spent a lot of time with FP-growth on its own, instead
    of going into too many implementation details of PFP in Spark, let''s instead
    see how to use the actual algorithm on the toy example that we have used throughout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The code is straightforward. We load the data into `transactions` and initialize
    Spark''s `FPGrowth` implementation with a minimum support value of *0.6* and *5*
    partitions. This returns a model that we can `run` on the transactions constructed
    earlier. Doing so gives us access to the patterns or frequent item sets for the
    specified minimum support, by calling `freqItemsets`, which, printed in a formatted
    way, yields the following output of 18 patterns in total:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00140.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Recall that we have defined transactions as *sets*, and we often call them item
    sets. This means that within such an item set, a particular item can only occur
    once, and `FPGrowth` depends on this. If we were to replace, for instance, the
    third transaction in the preceding example by `Array("b", "b", "h", "j", "o")`,
    calling `run` on these transactions would throw an error message. We will see
    later on how to deal with such situations.
  prefs: []
  type: TYPE_NORMAL
- en: After having explained *association rules *and *prefix span* in a similar fashion
    to what we just did with FP-growth, we will turn to an application of these algorithms
    on a real-world data set.
  prefs: []
  type: TYPE_NORMAL
- en: Association rule mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall from the association rule introduction that in computing association
    rules, we are about halfway there once we have frequent item sets, that is, patterns
    for the specified minimum threshold. In fact, Spark's implementation of association
    rules assumes that we provide an RDD of `FreqItemsets[Item]`, which we have already
    seen an example of in the preceding call to `model.freqItemsets`. On top of that,
    computing association rules is not only available as a standalone algorithm but
    is also available through `FPGrowth`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before showing how to run the respective algorithm on our running example,
    let''s quickly explain how association rules are implemented in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm is already provided with frequent item sets, so we don't have
    to compute them anymore.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each pair of patterns, X and *Y*, compute the frequency of both items X
    and Y co-occurring and store (*X*, (*Y*, supp(*X* ∪ *Y*)). We call such pairs
    of patterns *candidate pairs, *where *X* acts as a potential antecedent and *Y*
    as a consequent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join all the patterns with the candidate pairs to obtain statements of the form, (X,
    ((Y, supp(*X* ∪ *Y*)), supp(*X*))).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can then filter expressions of the form (X, ((Y, supp(*X* ∪ *Y*)), supp(*X*))) by
    the desired minimum confidence value to return all rules *X ⇒ Y* with that level
    of confidence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Assuming we didn''t compute the patterns through FP-growth in the last section
    but, instead, were just given the full list of these item sets, we can create
    an RDD from a sequence of `FreqItemset` from scratch and then run a new instance
    of `AssociationRules` on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that after initializing the algorithm, we set the minimum confidence to
    `0.7` before collecting the results. Moreover, running `AssociationRules` returns
    an RDD of rules of the `Rule` type. These rule objects have accessors for `antecedent`,
    `consequent`, and `confidence`, which we use to collect the results that read
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00141.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The reason we started this example from scratch is to convey the idea that
    association rules are indeed a standalone algorithm in Spark. Since the only built-in
    way to compute patterns in Spark is currently through FP-growth, and association
    rules depends on the concept of `FreqItemset` (imported from the `FPGrowth` submodule)
    anyway, this seems a bit unpractical. Using our results from the previous FP-growth
    example, we could well have written the following to achieve the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Interestingly, association rules can also be computed directly through the
    interface of `FPGrowth`. Continuing with the notation from the earlier example,
    we can simply write the following to end up with the same set of rules as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In practical terms, while both the formulations can be useful, the latter one
    will certainly be more concise.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential pattern mining with prefix span
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Turning to sequential pattern matching, the *prefix span algorithm* is a little
    more complicated than association rules, so we need to take a step back and explain
    the basics first. Prefix span has first been described in [http://hanj.cs.illinois.edu/pdf/tkde04_spgjn.pdf](http://hanj.cs.illinois.edu/pdf/tkde04_spgjn.pdf) as
    a natural extension of the so-called *FreeSpan *algorithm. The algorithm itself
    represents a notable improvement over other approaches, such as **Generalized
    Sequential Patterns**(**GSP**). The latter is based on the apriori principle and
    all the drawbacks we discussed earlier regarding many algorithms based on it carry
    over to sequential mining as well, that is, expensive candidate generation, multiple
    database scans, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Prefix span, in its basic formulation, uses the same fundamental idea as FP-growth,
    which is, projecting the original database to a usually smaller structure to analyze*.*
    While in FP-growth, we recursively built new FP-trees for each *suffix *of a branch
    in the original FP-tree, prefix span grows or spans new structures by considering
    *prefixes*, as the name suggests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first properly define the intuitive notions of prefix and suffix in
    the context of sequences. In what follows, we''ll always assume that the items
    within a sequence item are alphabetically ordered, that is, if *s =  <s[1,] s[2],..., s[l]> *is
    a sequence in *S* and each *s[i]* is a concatenation of items, that is, *s[i]
    = (a[i1] ... a[im])*, where a[ij] are items in *I*, we assume that all *a[ij]* are
    in the alphabetical order within *s[i]*. In such a situation, an element *s''
    = <s''[1,] s''[2],..., s''m> *is called a prefixof *s* if and only if the following
    three properties are satisfied:'
  prefs: []
  type: TYPE_NORMAL
- en: For all *i < m*, we have equality of sequence items, that is, *s'[i] = s[i]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s''[m] < s[m]*, that is, the last item of *s''* is a subpattern of *s[m]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we subtract *s'[m]* from *s[m]*, that is, delete the subpattern *s'[m]* from
    *s[m]*, all the frequent items left in *s[m] - s'[m]* have to come after all the
    elements in *s'[m]*, in alphabetical order
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the first two points come fairly naturally, the last one might seem a
    little strange, so let's explain it in an example. Given a sequence, *<a(abc)>*,
    from a database D, in which *a*, *b*, and *c* are indeed frequent, then, *<aa>*
    and *<a(ab)>* are prefixes for *<a(abc)>*, but *<ab>* is not, because in the difference
    of the last sequence items, *<(abc)> - <b> = <(ac)>*, the letter *a* does not
    alphabetically come after *b* from *<ab>*. Essentially, the third property tells
    us that a prefix can only cut out parts at the beginning of the last sequence
    item it affects.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the notion of the prefix defined, it is now easy to say what a suffixis.
    With the same notation as before, if *s''* is a prefix of *s*, then *s'''' = <(s[m]
    - s''[m]), s[m+1], ..., s[l]>* is a suffixfor this prefix, which we denote as
    *s'''' = s / s''*. Furthermore, we will write *s = s''s''''* in a product notation.
    For instance, given that *<a(abc)>* is the original sequence and *<aa>* is the
    prefix, we denote the suffix for this prefix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*<(_bc)> = <a(abc)> / <aa>*'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we use an underscore notation to denote the remainder of a sequence
    by a prefix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the prefix and suffix notions are useful to split up or partition the
    original sequential pattern mining problem into smaller parts, as follows. Let
    *{<p[1]>, ...,<p[n]>}* be the complete set of sequential patterns of length 1\.
    Then, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: All the sequential patterns start with one of the *p[i]*. This, in turn, means
    that we can partition all sequential patterns into *n* disjoint sets, namely those
    starting with *p[i]*, for *i* between *1* and *n*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Applying this reasoning recursively, we end up with the following statement:
    if *s* is a given sequential pattern of length 1 and *{s¹, ..., s^m}* is the complete
    list of length *l+1* sequential superpatterns of *s*, then all sequential patterns
    with the prefix *s* can be partitioned into *m* sets prefixed by *s^i*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both these statements are easy to arrive at but provide a powerful tool to subdivide
    the original problem set into disjointed smaller problems. Such a strategy is
    called *divide and conquer*. With this in mind, we can now proceed very similarly
    to what we did with conditioned databases in FP-growth, namely project databases
    with respect to a given prefix. Given a sequential pattern database S and a prefix
    *s*, the **s-projected database**, *S|[s]*, is the set of all the suffixes for
    *s* in S.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need one last definition to state and analyze the prefix span algorithm.
    If *s* is a sequential pattern in S and *x* is a pattern with a prefix *s*, then
    the *support count *of *x* in *S|[s]*, denoted by *supp[S|s](x)*, is the number
    of sequences *y* in *S|[s]*, so that *x < sy*; that is, we simply carry over the
    notion of support to s-projected databases. There are a few interesting properties
    we can derive from this definition that make our situation much easier. For instance,
    by definition, we see that for any sequence *x* with the prefix *s*, we have the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*supp[S](x) = supp[S|s](x)*'
  prefs: []
  type: TYPE_NORMAL
- en: That is, it does not matter if we count the support in the original or projected
    database in this case. Moreover, if *s'* is a prefix of *s*, it is clear that
    *S|[s] = (S|[s'])|[s]*, meaning we can prefix consecutively without losing information.
    The last and most important statement from a computational complexity perspective
    is that a projected database cannot exceed its original size. This property should
    again be clear from the definitions, but it's immensely helpful to justify the
    recursive nature of prefix span.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given all this information, we can now sketch the prefix span algorithm in
    pseudocode as follows. Note that we distinguish between an item `s''` being appended
    to the end of a sequential pattern `s` and the sequence `<s''>` generated from
    `s''` added to the end of `s`. To give an example, we could either add the letter
    *e* to *<a(abc)>* to form *<a(abce)>* or add *<e>* at the end to form *<a(abc)e>*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The prefix span algorithm, as outlined, finds all sequential patterns; that
    is, it represents a solution to the sequential pattern mining problem. We cannot
    outline the proof of this statement here, but we hopefully have provided you with
    enough intuition to see how and why it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Turning to Spark for an example, note that we did not discuss how to effectively
    parallelize the baseline algorithm. If you are interested in knowing about the
    implementation details, see [https://github.com/apache/spark/blob/v2.2.0/mllib/src/main/scala/org/apache/spark/mllib/fpm/PrefixSpan.scala](https://github.com/apache/spark/blob/v2.2.0/mllib/src/main/scala/org/apache/spark/mllib/fpm/PrefixSpan.scala),
    as the parallel version is a little too involved would justify presenting it here.
    We will study the example first provided in *Table 2*, that is, the four sequences
    of *<a(abc)(ac)d(cf)>*, *<(ad)c(bc)(ae)>*, *<(ef)(ab)(df)cb>*, and *<eg(af)cbc>*.
    To encode the nested structure of sequences, we use arrays of arrays of strings
    and parallelize them to create an RDD from them. Initializing and running an instance
    of `PrefixSpan` works pretty much the same way as it did for the other two algorithms.
    The only thing noteworthy here is that, apart from setting the minimum support
    threshold to `0.7` via `setMinSupport`, we also specify the maximum length of
    the patterns to `5` through `setMaxPatternLength`. This last parameter is there
    to limit the recursion depth. Despite the clever implementation, the algorithm
    (and particularly, the computing database projections) can take a prohibitive
    amount of time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code in your Spark shell should yield the following output of
    14 sequential patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00142.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Pattern mining on MSNBC clickstream data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having spent a considerable amount of time explaining the basics of pattern
    mining, let''s finally turn to a more realistic application. The data we will
    be discussing next comes from server logs from [http://msnbc.com](http://msnbc.com)
    (and in parts from [http://msn.com](http://msn.com), when news-related), and represents
    a full day''s worth of browsing activity in terms of page views of users of these
    sites. The data collected in September 1999 and has been made available for download
    at [http://archive.ics.uci.edu/ml/machine-learning-databases/msnbc-mld/msnbc990928.seq.gz](http://archive.ics.uci.edu/ml/machine-learning-databases/msnbc-mld/msnbc990928.seq.gz).
    Storing this file locally and unzipping it, the `msnbc990928.seq` file essentially
    consists of a header and space-separated rows of integers of varying length. The
    following are the first few lines of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Each row in this file is a *sequence* of encoded page visits of users within
    that day. Page visits have not been collected to the most granular level but rather
    grouped into 17 news-related categories, which are encoded as integers. The category
    names corresponding to these categories are listed in the preceding header and
    are mostly self-explanatory (with the exception of `bbs`, which stands for **bulletin
    board service**). The n-th item in this list corresponds to category n; for example,
    `1` stands for `frontpage`, while `travel` is encoded as `17`. For instance, the
    fourth user in this file hit `opinion` once, while the third had nine page views
    in total, starting and ending with `tech`.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the page visits in each row have indeed been stored
    *chronologically*, that is, this really is sequential data with respect to page
    visit order. In total, data for 989,818 users has been collected; that is, the
    data set has precisely that number of sequences. Unfortunately, it is unknown
    how many URLs have been grouped to form each category, but we do know it ranges
    rather widely from 10 to 5,000\. See the description available at [http://archive.ics.uci.edu/ml/machine-learning-databases/msnbc-mld/msnbc.data.html](http://archive.ics.uci.edu/ml/machine-learning-databases/msnbc-mld/msnbc.data.html) for
    more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just from the description of this data set, it should be clear that all the
    three pattern mining problems we have discussed so far can be applied to this
    data--we can search for sequential patterns in this sequential database and, neglecting
    the sequentiality, analyze both frequent patterns and association rules. To do
    so, let''s first load the data using Spark. In what follows, we will assume that
    the header of the file has been removed and a Spark shell session has been created
    from the folder the sequence file is stored in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We load the sequence file into an RDD of integer-valued arrays first. Recall
    from earlier sections that one of the assumptions of transactions in frequent
    pattern mining was that the item sets are, in fact, sets and thus contain no duplicates.
    To apply FP-growth and association rule mining, we therefore have to delete duplicate
    entries, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that not only did we restrict to distinct items for each transaction but
    we also cached the resulting RDD, which is recommended for all the three pattern
    mining algorithms. This allows us to run FP-growth on this data, for which we
    have to find a suitable minimum support threshold *t*. So far, in the toy examples,
    we have chosen *t* to be rather large (between 0.6 and 0.8). It is not realistic
    to expect *any* patterns to have such large support values in larger databases.
    Although we only have to deal with 17 categories, browsing behaviors can vary
    drastically from user to user. Instead, we choose a support value of just 5 %
    to gain some insights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this computation shows that for *t=0.05* we only recover 14 frequent
    patterns, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00143.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Not only are there, maybe, less patterns than you may have expected, but among
    those, all but one have a length of *1*. Less surprising is the fact that the *front
    page* is hit most often, with 31%, followed by the categories, *on-air* and *news. *Both
    the *front page* and *news *sites have been visited by only 7% of users on that
    day and no other pair of site categories was visited by more than 5% of the user
    base. Categories 5, 15, 16, and 17 don't even make the list. If we repeat the
    experiment with a *t* value of 1% instead, the number of patterns increases to
    a total of 74.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how many length-3 patterns are among them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this on an `FPGrowth` instance with a minimum support value of *t=0.01*
    will yield the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00144.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As one could have guessed, the most frequent length-1 patterns are also predominant
    among the 3-patterns. Within these 11 patterns, 10 concern the *front page,* andnine
    the *news. *Interestingly, the category *misc*, while only visited 7% of the time,
    according to the earlier analysis, shows up in a total of four 3-patterns. If
    we had more information about the underlying user groups, it would be interesting
    to follow up on this pattern. Speculatively, users that have an interest in a
    lot of *miscellaneous* topics will end up in this mixed category, along with some
    other categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this up with an analysis of association rules is technically easy;
    we just run the following lines to get all the rules with confidence `0.4` from
    the existing FP-growth `model`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how we can conveniently access the antecedent, consequent, and confidence
    of the respective rules. The output of this is as follows; this time with the
    confidence rounded to two decimals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00145.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Again, naturally, the most frequent length-1 patterns show up in many of the
    rules, most notably, *frontpage *as a consequent. Throughout this example, we
    chose the support and confidence values so that the outputs are short and counts
    easy to validate manually, but let''s do some automated calculations on rule sets,
    regardless:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Executing these statements, we see that about two-thirds of the rules have *front
    page* as the consequent, that is, 14 of 22 rules in total, and among these, nine
    contain *news* in their antecedent.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to the sequence mining problem for this data set, we need to transform
    our original `transactions` to an RDD of the `Array[Array[Int]]` type first, since
    nested arrays are the way to encode sequences for prefix span in Spark, as we
    have seen before. While somewhat obvious, it is still important to point out that
    with sequences, we don't have to discard the additional information of repeating
    items, as we just did for FP-growth.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, we even gain more structure by imposing sequentiality on individual
    records. To do the transformation just indicated, we simply do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we cache the result to improve the performance of the algorithm, this
    time, `prefixspan`. Running the algorithm itself is done as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the minimum support value very low at 0.5%, to get a slightly bigger
    result set this time. Note that we also search for patterns no longer than 15
    sequence items. Let''s analyze the distribution over a frequent sequence length
    by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In this chain of operations, we first map each sequence to a key-value pair
    consisting of its length and a count of 1\. We then proceed with a reduce operation
    that sums up the values by key, that is, we count how often this length occurs.
    The rest is just sorting and formatting, which yields the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00146.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the longest sequence has a length of 14, which, in particular,
    means that our maximum value of 15 did not restrict the search space and we found
    all the sequential patterns for the chosen support threshold of `t=0.005`. Interestingly,
    most of the frequent sequential visits of users have a length between two and
    six touch points on [http://msnbc.com](http://msnbc.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete this example, let''s see what the most frequent pattern of each
    length is and what the longest sequential pattern actually looks like. Answering
    the second question will also give us the first, since there is only one length-14
    pattern. Computing this can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this is one of the more complicated RDD operations we''ve considered
    so far, let''s discuss all the steps involved. We first map each frequent sequence
    to a pair consisting of its length and the sequence itself. This may seem a bit
    strange at first, but it allows us to group all the sequences by length, which
    we do in the next step. Each group consists of its key and an iterator over frequent
    sequences*.* We map each group to its iterator and reduce the sequences by only
    keeping the one with the largest frequency. To then properly display the result
    of this operation, we make use of `mkString` twice to create a string from the
    otherwise not readable nested arrays (when printed). The result of the preceding
    chain is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00147.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We discussed earlier that *front page* was the most frequent item by far, which
    makes a lot of sense intuitively, since it is the natural entry point to a website.
    However, it is a bit of a surprise that the most frequent sequences of all lengths,
    for the chosen threshold, consist of *front page* hits only. Apparently many users
    spend a lot of time, and clicks, in and around the front page, which might be
    a first indication of it's advertising value, as compared to the pages of the
    other categories. As we indicated in the introduction of this chapter, analyzing
    data like this, especially if enriched with other data sources, can be of a tremendous
    value for owners of the respective websites, and we hope to have shown how frequent
    pattern mining techniques can serve their part in doing so.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a pattern mining application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The example developed in the last section was an interesting playground to apply
    the algorithms we have carefully laid out throughout the chapter, but we have
    to recognize the fact that *we were just handed the data.* At the time of writing
    this book, it was often part of the culture in building data products to draw
    a line in the sand between *data science* and *data engineering *at pretty much
    exactly this point, that is, between real-time data collection and aggregation,
    and (often offline) analysis of data, followed up by feeding back reports of the
    insights gained into the production system. While this approach has its value,
    there are certain drawbacks to it as well. By not taking the full picture into
    account, we might, for instance, not exactly know the details of how the data
    has been collected. Missing information like this can lead to false assumptions
    and eventually wrong conclusions. While specialization is both useful and necessary
    to a certain degree, at the very least, practitioners should strive to get a basic
    understanding of applications *end-to-end*.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we introduced the MSNBC data set in the last section, we said that it
    had been retrieved from the server logs of the website. We drastically simplified
    what this entails, so let us have a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: '*High availability and fault tolerance: *Click events on a website need to
    be tracked without downtime at any point throughout the day. Some businesses,
    especially when it comes to any sort of payment transactions, for example, in
    online shops, can not afford to lose certain events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*High throughput of live data and scalability: *We need a system that can store
    and process such events in real time and can cope with a certain load without
    slowing down. For instance, the roughly *one million* unique users in the MSNBC
    data set mean that, on average, there is activity of about 11 users per second*.* There
    are many more events to keep track of, especially keeping in mind that the only
    thing we have measured were page views.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Streaming data and batching thereof: *In principle, the first two points could
    be addressed by writing events to a sufficiently sophisticated log. However, we
    haven''t even touched the topic of aggregating data yet and we preferably need
    an online processing system to do so. First, each event has to be attributed to
    a user, which will have to be equipped with some sort of ID. Next, we will have
    to think about the concept of a user session. While the user data has been aggregated
    on a daily level in the MSNBC data set, this is not granular enough for many purposes.
    It makes sense to analyze users'' behavior for the time period they are actually
    active. For this reason, it is customary to consider* windows* of activities and
    aggregate clicks and other events as per such windows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Analytics on streaming data: *Assuming we had a system like we just described
    and access to aggregated user session data in real time, what could we hope to
    achieve? We would need an analytics platform that allows us to apply algorithms
    and gain insights from this data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark's proposal to address these problems is its **Spark Streaming**module,
    which we will briefly introduce next. Using Spark Streaming, we will build an
    application that can at least *mock *generating and aggregating events in order
    to then apply the pattern mining algorithms we studied to *streams of events.*
  prefs: []
  type: TYPE_NORMAL
- en: The Spark Streaming module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is not enough time to give an in-depth introduction to Spark Streaming
    here, but we can, at the very least, touch on some of the key notions, provide
    some examples, and give some guidance to more advanced topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark Streaming is Spark''s module for stream data processing, and it is indeed
    equipped with all the properties we explained in the preceding list: it is a highly
    fault-tolerant, scalable, and high-throughput system for processing and analyzing
    streams of live data. Its API is a natural extension of Spark itself and many
    of the tools available for RDDs and DataFrames carry over to Spark Streaming.'
  prefs: []
  type: TYPE_NORMAL
- en: The core abstraction of Spark Streaming applications is the notion of *DStream,* which
    stands for *discretized stream. *To explain the nomenclature, we often think of
    data streams as a continuous flow of events,which is, of course, an idealization,
    since all we can ever measure are discrete events. Regardless, this continuous
    flow of data will hit our system, and for us to process it further, we *discretize*
    it into disjoint batches of data. This stream of discrete data batches is realized
    as DStream in Spark Streaming and is internally implemented as a *sequence of
    RDDs*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram gives a high-level overview of the data flow and transformation
    with Spark Streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00148.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Input data is fed into Spark Streaming, which discretises this stream
    as a so called DStream. These sequences of RDDs can then be further transformed
    and processed by Spark and any module thereof.'
  prefs: []
  type: TYPE_NORMAL
- en: As the diagram shows, the data enters Spark Streaming through an input data
    stream. This data can be produced and ingested from many different sources, which
    we will discuss further later on. We speak of systems generating events that Spark
    Streaming can process as *sources*.Input DStreams take data from sources and do
    so via *receivers* for these sources. Once an input DStream has been created,
    it can be processed through a rich API that allows for many interesting transformations.
    It serves as a good mental model to think of DStreams as sequences or collections
    of RDDs, which we can operate on through an interface that is very close to that
    of RDDs in the Spark core. For instance, operations such as map-reduce, and filter
    are available for DStreams as well and simply carry over the respective functionality
    from the individual RDDs to sequences of RDDs. We will discuss all of this in
    more detail, but let's first turn to a basic example.
  prefs: []
  type: TYPE_NORMAL
- en: As the first example to get started with Spark Streaming, let's consider the
    following scenario. Assume that we have already loaded the MSNBC data set from
    earlier and have computed the prefix span model (`psModel`) from it. This model
    was fit with data from a single day of user activity, say, yesterday's data. Today,
    new events of user activity come in. We will create a simple Spark Streaming application
    with a basic source that generates user data in precisely the schema we had for
    the MSNBC data; that is, we are given space-separated strings containing numbers
    between 1 and 17\. Our application will then pick up these events and create `DStream`
    from them. We can then apply our prefix span model to the data of `DStream` to
    find out if the new sequences fed into the system are indeed frequent sequences
    according to `psModel`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with a Spark Streaming application in the first place, we need to
    create a so-called `StreamingContext` API, which, by convention, will be instantiated
    as `ssc`. Assuming that we start an application from scratch, we create the following
    context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: If you work with the Spark shell, all but the first and last lines are not necessary,
    since, in such a case, you will be provided with a Spark context (`sc`) already.
    We include the creation of the latter regardless, since we aim at a self-contained
    application. The creation of a new `StreamingContext` API takes two arguments,
    namely a `SparkContext` and an argument called `batchDuration`, which we set to
    10 seconds. The batch duration is the value that tells us *how to discretize *data
    for a `DStream`, by specifying for how long the streaming data should be collected
    to form a batch within the `DStream`, that is, one of the RDDs in the sequence.
    Another detail we want to draw your attention to is that the Spark master is set
    to two cores by setting `local[2]`. Since we assume you are working locally, it
    is important to assign at least two cores to the application. The reason is that
    one thread will be used to receive input data, while the other will then be free
    to process it. Should you have more receivers in more advanced applications, you
    need to reserve one core for each.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we essentially repeat parts of the prefix span model for the sake of
    completeness of this application. As before, the sequences are loaded from a local
    text file. Note that this time, we assume the file is in the resources folder
    of your project, but you can choose to store it anywhere you want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the last step of the preceding computation, we collect all the frequent sequences
    on the master and store them as `freqSequences`. The reason we do this is that
    we want to compare this data against the incoming data to see if the sequences
    of the new data are frequent with respect to the current model (`psModel`). Unfortunately,
    unlike many of the algorithms from MLlib, none of the three available pattern
    mining models in Spark are built to take new data once trained, so we have to
    do this comparison on our own, using `freqSequences`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can finally create a `DStream` object of the `String` type. To this
    end, we call `socketTextStream` on our streaming context, which will allow us
    to receive data from a server, running on port `8000` of `localhost`, listening
    on a TCP socket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'What we call `rawSequences` is the data received through that connection, discretized
    into 10-second intervals. Before we discuss *how to actually send data*, let''s
    first continue with the example of processing it once we have received it. Recall
    that the input data will have the same format as before, so we need to preprocess
    it in exactly the same way, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The two `map` operations we use here are structurally the same as before on
    the original MSNBC data, but keep in mind that this time, `map` has a different
    context, since we are working with DStreams instead of RDDs. Having defined `sequences`,
    a sequence of RDDs of the `Array[Array[Int]]` type, we can use it to match against
    `freqSequences`. We do so by iterating over each RDD in sequences and then again
    over each array contained in these RDDs. Next, we count how often the respective array is
    found in `freqSequences`, and if it is found, we print that the sequence corresponding
    to `array` is indeed frequent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note that in the preceding code, we need to compare deep copies of arrays, since
    nested arrays can't be compared on the nose. To be more precise, one can check
    them for equality, but the result will always be false.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having done the transformation, the only thing we are left with on the receiving
    side of the application is to actually tell it to start listening to the incoming
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Through the streaming context, `ssc`, we tell the application to start and await
    its termination. Note that in our specific context and for most other applications
    of this fashion, we rarely want to terminate the program at all. By design, the
    application is intended as a *long-running job*, since, in principle, we want
    it to listen to and analyze new data indefinitely. Naturally, there will be cases
    of maintenance, but we may also want to regularly update (re-train) `psModel`
    with the newly acquired data.
  prefs: []
  type: TYPE_NORMAL
- en: We have already seen a few operations on DStreams and we recommend you to refer
    to the latest Spark Streaming documentation ([http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html))
    for more details. Essentially, many of the (functional) programming functionalities
    available on basic Scala collections that we also know from RDDs carry over seamlessly
    to DStreams as well. To name a few, these are `filter`, `flatMap`, `map`, `reduce`,
    and `reduceByKey`. Other SQL-like functionalities, such as cogroup, `count`, `countByValue`,
    `join`, or `union`, are also at your disposal. We will see some of the more advanced
    functionalities later on in a second example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have covered the receiving end, let''s briefly discuss how to create
    a data source for our app. One of the simplest ways to send input data from a
    command line over a TCP socket is to use *Netcat*, which is available for most
    operating systems, often preinstalled. To start Netcat locally on port `8000`,
    run the following command in a terminal separate from your Spark application or
    shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming you already started the Spark Streaming application for receiving
    data from before, we can now type new sequences into the Netcat terminal window
    and confirm each by hitting *Enter*. For instance, type the following four sequences *within
    10 seconds*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00149.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00150.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If you are either really slow at typing or so unlucky that you start typing
    when the 10-second window is almost over, the output might be split into more
    parts. Looking at the actual output, you will see that the often discussed categories *front
    page* and *news*,represented by categories 1 and 2, are frequent. Also, since
    23 is not a sequence item contained in the original data set, it can't be frequent.
    Lastly, the sequence <4, 5> is apparently also not frequent, which is something
    we didn't know before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing Netcat for this example is a natural choice for the time and space
    given in this chapter, but you will never see it used for this purpose in serious
    production environments. In general, Spark Streaming has two types of sources
    available: basic and advanced. Basic sources can also be queues of RDDsand other
    custom sources apart from file streams, which the preceding example represents.
    On the side of advanced sources, Spark Streaming has a lot of interesting connectors
    to offer: Kafka, Kinesis, Flume, and advanced custom sources*.* This wide variety
    of advanced sources makes it attractive to incorporate Spark Streaming as a production
    component, integrating well with the other infrastructure components.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking a few steps back and considering what we have achieved by discussing
    this example, you may be inclined to say that apart from introducing Spark Streaming
    itself and working with data producers and receivers, the application itself did
    not solve many of our aforementioned concerns. This criticism is valid, and in
    a second example, we want to address the following remaining issues with our approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Input data for our DStreams had the same structure as our offline data, that
    is, it was already pre-aggregated with respect to users, which is not very realistic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from the two calls to `map` and one to `foreachRDD`, we didn't see much
    in terms of functionality and added value in operating with DStreams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We did not do any analytics on data streams but only checked them against a
    list of precomputed patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To resolve these issues, let's slightly redefine our example setting. This time,
    let's assume that one event is represented by one user clicking on one site, where
    each such site falls under one of the categories 1-17, as before. Now, we cannot
    possibly simulate a complete production environment, so we make the simplifying
    assumption that each unique user has already been equipped with an ID. Given this
    information, let's say events come in as key-value pairs consisting of a user
    ID and a category of this click event.
  prefs: []
  type: TYPE_NORMAL
- en: With this setup, we have to think about how to aggregate these events to generate
    sequences from them. For this purpose, we need to collect data points for each
    user ID in a given window*. *In the original data set, this window was obviously
    one full day, but depending on the application, it may make sense to choose a
    much smaller window. If we think about the scenario of a user browsing his favorite
    online shop, the click and other events that go back a few hours will unlikely
    influence his or her current desire to buy something. For this reason, a reasonable
    assumption made in online marketing and related fields is to limit the window
    of interest to about 20-30 minutes, a so-called *user session.* In order for us
    to see results much quicker, we will use an even smaller window of 20 seconds
    for our application. We call this the **window length***.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know how far back we want to analyze the data from a given point
    in time, we also have to define *how often* we want to carry out the aggregation
    step, which we will call the *sliding interval.* One natural choice would be to
    set both to the same amount of time, leading to disjoint windows of aggregation,
    that is, every 20 seconds. However, it might also be interesting to choose a shorter
    sliding window of 10 seconds, which would lead to aggregation data that overlaps
    10 seconds each. The following diagram illustrates the concepts we just discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00151.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Visualisation of a window operation transforming a DStream to another.
    In this example the batch duration of the Spark Streaming application has been
    set to 10 seconds. The window length for the transformation operating on batches
    of data is 40 seconds and we carry out the window operation every 20 seconds,
    leading to an overlap of 20 seconds each and a resulting DStream that is batched
    in 20-second blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To turn this knowledge into a concrete example, we assume that the event data
    has the form *key:value*, that is, one such event could be `137: 2`, meaning that
    the user with ID `137` clicked on a page with the category *news*. To process
    these events, we have to modify our preprocessing like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'With these key-value pairs, we can now aim to do the aggregation necessary
    to group events by the user ID. As outlined earlier, we do this by aggregating
    on a given window of 20 seconds with a sliding interval of 10 seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are using a more advanced operation on DStreams, namely
    `reduceByKeyAndWindow`, in which we specify an aggregation function on values
    of key-value pairs, as well as a window duration and sliding interval. In the
    last step of the computation, we strip the user IDs so that the structure of `rawSequences`
    is identical to the previous example. This means that we have successfully converted
    our example to work on unprocessed events, and it will still check against frequent
    sequences of our baseline model. We will not show more examples of how the output
    of this application looks, but we encourage you to play around with this application
    and see how the aggregation works on key-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To wrap up this example, and the chapter, let''s look at one more interesting
    way of aggregating event data. Let''s say we want to dynamically count how often
    a certain ID occurs in the event stream, that is, how many page clicks a user
    generates. We already have our `events` DStream defined previously, so we could
    approach the count as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In a way, this works as we intended; it counts events for IDs. However, note
    that what is returned is again a DStream, that is, we do not actually aggregate *across
    streaming windows* but just within the sequences of RDDs. To aggregate across
    the full stream of events, we need to keep track of count states since from the
    start. Spark Streaming offers a method on DStreams for precisely this purpose,
    namely `updateStateByKey`. It can be used by providing `updateFunction`, which
    takes the current state and new values as input and returns an updated state.
    Let''s see how it works in practice for our event count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We first define our update function itself. Note that the signature of `updateStateByKey`
    requires us to return an `Option`, but in essence, we just compute the running
    sum of state and incoming values. Next, we provide `updateStateByKey` with an
    `Int` type signature and the previously created `updateFunction` method. Doing
    so, we get precisely the aggregation we wanted in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing, we introduced event aggregation, two more complex operations on
    DStreams (`reduceByKeyAndWindow` and `updateStateByKey`), and counted events in
    a stream with this example. While the example is still simplistic in what it does,
    we hope to have provided the reader with a good entry point for more advanced
    applications. For instance, one could extend this example to calculate moving
    averages over the event stream or change it towards computing frequent patterns
    on a per-window basis.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced a new class of algorithms, that is, frequent
    pattern mining applications, and showed you how to deploy them in a real-world
    scenario. We first discussed the very basics of pattern mining and the problems
    that can be addressed using these techniques. In particular, we saw how to implement
    the three available algorithms in Spark, *FP-growth*, *association rules*, and
    *prefix span*. As a running example for the applications we used clickstream data
    provided by MSNBC, which also helped us to compare the algorithms qualitatively.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we introduced the basic terminology and entry points of Spark Streaming
    and considered a few real-world scenarios. We discussed how to deploy and evaluate
    one of the frequent pattern mining algorithms with a streaming context first.
    After that, we addressed the problem of aggregating user session data from raw
    streaming data. To this end, we had to find a solution to mock providing click
    data as streaming events.
  prefs: []
  type: TYPE_NORMAL
