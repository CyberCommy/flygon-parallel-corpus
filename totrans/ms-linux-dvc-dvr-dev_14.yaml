- en: '*Chapter 11*: Writing PCI Device Drivers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A PCI is more than just a bus. It is a standard with a complete set of specifications
    defining how different parts of a computer should interact. Over the years, a
    PCI bus has become the de facto bus standard for device inter-connections, such
    that almost every SoC has native support for such buses. The need for speed led
    to different versions and generations of this bus.
  prefs: []
  type: TYPE_NORMAL
- en: In the early days of the standard, the first bus that implemented the PCI standard
    was the PCI bus (the bus name is the same as the standard), as a replacement for
    the ISA bus. This improved (with 32-bit addressing and jumper-less autodetection
    and configuration) the address limitation encountered with ISA (limited to 24
    bits, and which occasionally necessitated playing with jumpers in order to route
    IRQs and so on). Compared with the previous bus implementation of the PCI standard,
    the major factor that has been improved is speed.
  prefs: []
  type: TYPE_NORMAL
- en: '**PCI Express** is the current family of PCI bus. It is a serial bus, while
    its ancestors were parallel. In addition to speed, PCIe extended the 32-bit addressing
    of its predecessors to 64 bits, with multiple improvements in the interrupt management
    system. This family is split into generations, GenX, which we will see in the
    following sections in this chapter. We will begin with an introduction to PCI
    buses and interfaces where we will learn about bus enumeration, and then we will
    look at the Linux kernel PCI APIs and core functionalities.'
  prefs: []
  type: TYPE_NORMAL
- en: The good news with all of this is that, whatever the family, almost everything
    is transparent to the driver developer. The Linux kernel will abstract and hide
    most of the mechanisms behind a reduced set of APIs that can be used to write
    reliable PCI device drivers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to PCI buses and interfaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Linux kernel PCI subsystem and data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCI and **Direct Memory Access** (**DMA**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A good overview of Linux memory management and memory mapping is required, as
    is a familiarity with the concept of interrupts and locking, especially with the
    Linux kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Linux kernel v4.19.X sources are available at [https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/refs/tags](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/refs/tags).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to PCI buses and interfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Peripheral Component Interconnect** (**PCI**) is a local bus standard used
    to attach peripheral hardware devices to the computer system. As a bus standard,
    it defines how different peripherals of a computer should interact. However, over
    the years, the PCI standard has evolved either in terms of features or in terms
    of speed. As of its creation until now, we have had several bus families implementing
    the PCI standard, such as PCI (yes, the bus with the same name as the standard),
    and **PCI Extended** (**PCI-X**), **PCI Express** (**PCIe** or **PCI-E**), which
    is the current generation of PCI. A bus that follows PCI standards is known as
    a PCI bus.'
  prefs: []
  type: TYPE_NORMAL
- en: From a software point of view, all these technologies are compatible and can
    be handled by the same kernel drivers. This means the kernel doesn't need to know
    which exact bus variant is used. PCIe greatly *extends* PCI with a lot of similarities
    from a software point of view (especially Read/Write I/O or Memory transactions).
    While both are software compatible, PCIe is a serial bus instead of parallel (prior
    to PCIe, every PCI bus family was parallel), which also means you can't have a
    PCI card installed in a PCIe slot, or a PCIe card installed in a PCI slot.
  prefs: []
  type: TYPE_NORMAL
- en: 'PCI Express is the most popular bus standard on computers these days, so we
    are going to target PCIe in this chapter while mentioning similarities with or
    differences from PCI when necessary. Apart from the preceding, the following are
    some of the improvements in PCIe:'
  prefs: []
  type: TYPE_NORMAL
- en: PCIe is a serial bus technology, whereas PCI (or other implementations) is parallel,
    thereby reducing the number of I/O lanes required for connecting devices, thus
    reducing the design complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCIe implements enhanced interrupt management features (providing Message-Based
    Interrupts, aka MSI, or its extended version, MSI-X), extending the number of
    interrupts a PCI device can deal with without increasing its latency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PCIe increases the transmission frequency and throughput: Gen1, Gen2, Gen3
    ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCI devices are types of memory-mapped devices. A device that is connected to
    any PCI bus is assigned address ranges in the processor's address space. These
    address ranges have a different meaning in the PCI address domain, which contains
    three different types of memory according to what they contain (control, data,
    and status registers for the PCI-based device) or to the way they are accessed
    (I/O port or memory mapped). These memory regions will be accessed by the device
    driver/kernel to control the particular device connected over the PCI bus and
    share information with it.
  prefs: []
  type: TYPE_NORMAL
- en: The PCI address domain contains the three different memory types that have to
    be mapped in the processor's address space.
  prefs: []
  type: TYPE_NORMAL
- en: Terminology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since the PCIe ecosystem is quite large, there are a number of terms we may
    need to be familiar with prior to going further. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Root complex** (**RC**): This refers to the PCIe host controller in the SoC.
    It can access the main memory without CPU intervening, which is a feature used
    by other devices to access the main memory. They are also known as Host-to-PCI
    bridges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`00h` configuration space headers. They never appear on a switch''s internal
    bus and have no downstream port.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lane**: This represents a set of differential signal pairs (one pair for
    Tx, one pair for Rx).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xN` (`x1`, `x2`, `x4`, `x8`, `x12`, `x16`, and `x32`), where `N` is the number
    of pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not all PCIe devices are endpoints. They may also be switches or bridges.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bridges**: These provide an interface to other buses, such as PCI or PCI
    X, or even another PCIe bus. A bridge can also provide an interface to the same
    bus. For example, a PCI-to-PCI bridge facilitates the addition of more loads to
    a bus by creating a completely separate secondary bus (we will see what a secondary
    bus is in forthcoming sections). The concept of bridges aids in understanding
    and implementing the concept of switches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Switches**: These provide an aggregation capability and allow more devices
    to be attached to a single root port. It goes without saying that switches have
    a single upstream port, but may have several downstream ports. They are smart
    enough to act as packet routers and recognize which path a given packet will need
    to take based on its address or other routing information, such as an ID. That
    being said, there is also implicit routing, which is used only for certain message
    transactions, such as broadcasts from the root complex and messages that always
    go to the root complex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Switch downstream ports are (kinds of virtual) PCI-PCI bridges bridging from
    the internal bus to buses representing the downstream PCI Express links of this
    PCI Express switch. You should keep in mind that only the PCI-PCI bridges representing
    the switch downstream ports may appear on the internal bus.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A PCI-to-PCI bridge provides a connectivity path between two peripheral component
    interconnect (PCI) buses. You should keep in mind that **only the downstream ports
    of PCI-PCI bridges are taken into account during bus enumeration**. This is very
    important in understanding the enumeration process.
  prefs: []
  type: TYPE_NORMAL
- en: PCI bus enumeration, device configuration, and addressing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCIe's most obvious improvement over PCI is its point-to-point bus topology.
    Each device sits on its own dedicated bus, which, in PCIe jargon, is known as
    a **link**. Understanding the enumeration process of PCIe devices requires some
    knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you look at the register space of the devices (in the header-type register),
    they will say whether they are a type `0` or type `1` register space. Typically,
    type `0` means an endpoint device and type `1` means a bridge device. The software
    has to identify whether it talks to an endpoint device or to a bridge device.
    Bridge device configuration differs from endpoint device configuration. During
    bridge device (type 1) enumeration, the software has to assign the following elements
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Primary Bus Number**: This is the upstream bus number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0xFF`, as `255` is the highest bus number. As and when enumeration continues,
    this field will be given the real value of how far downstream this bridge can
    go.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Device identification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Device identification consists of a few properties or parameters that make
    the device unique or addressable. In the PCI subsystem, these parameters are the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vendor ID**: This identifies the manufacturer of the device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Device ID**: This identifies the particular vendor device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding two elements may be enough, but you can also rely on the following
    elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Revision ID**: This specifies a device-specific revision identifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Class Code**: This identifies the generic function implemented by the device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Header Type**: This defines the layout of the header.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All those parameters can be read from the device configuration registers. That's
    what the kernel does to identify devices when enumerating buses.
  prefs: []
  type: TYPE_NORMAL
- en: Bus enumeration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Prior to delving into the PCIe bus enumeration function, there are some basic
    limitations we need to take care of:'
  prefs: []
  type: TYPE_NORMAL
- en: There can be `256` buses on the system (`0-255`) as there are `8` bits to identify
    them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can be `32` devices per bus (`0-31`) as there are `5` bits to identify
    them on each bus.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A device can have up to 8 functions (`0-7`), hence `3` bits to identify them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All external PCIe lanes, irrespective of whether they originate from the CPU
    or not, are behind PCIe bridges (and therefore get new PCIe bus numbers). Configuration
    software is able to enumerate up to `256` PCI buses on a given system. The number
    `0` is always assigned to the root complex. Remember that only the downstream
    ports (secondary sides) of PCI-PCI bridges are taken into account during bus enumeration.
  prefs: []
  type: TYPE_NORMAL
- en: The PCI enumeration process is based on the **Depth-first search** (**DFS**)
    algorithm, which normally starts at a random node (but in the case of PCI enumeration,
    this node is known in advance, and it is the RC in our case) and which explores
    as far as possible (actually looking for bridges) along each branch before backtracking.
  prefs: []
  type: TYPE_NORMAL
- en: Said like this, when a bridge is found, configuration software assigns a number
    to it, at least one larger than the bus number this bridge lives on. After this,
    the configuration software starts looking for new bridges on this new bus, and
    so on, before backtracking to this bridge's sibling (if the bridge was part of
    a multi-port switch) or neighbor bridge (in terms of topology).
  prefs: []
  type: TYPE_NORMAL
- en: 'Enumerated devices are identified with the BDF format, which stands for *Bus-Device-Function*,
    which uses triple bytes – in other words `XX:YY:ZZ` – in hexadecimal (without
    the `0x`) notation for identification. For instance, `00:01:03` would literally
    mean Bus `0x00: Device 0x01: Function 0x03`. We could interpret this as function
    `3` of device `1` on bus `0`. This notation helps in quickly locating a device
    within a given topology. In case a double-byte notation is used, this would mean
    the function has been omitted or does not matter, in other words, `XX:YY`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the topology of a PCIe fabric:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – PCI bus enumeration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.1_B10985.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 – PCI bus enumeration
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we describe the preceding topology diagram, keep repeating the following
    four statements until you become familiar with them:'
  prefs: []
  type: TYPE_NORMAL
- en: A PCI-to-PCI bridge facilitates the addition of more loads to a bus by creating
    a completely separate secondary bus. Thus, each bridge downstream port is a new
    bus, and must be given a bus number, at least +1 bigger than the bus number where
    it lives.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Switch downstream ports are (kinds of virtual) PCI-PCI (P2P) bridges bridging
    from the internal bus to buses representing the downstream PCI Express links of
    this PCI Express switch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The CPU is connected to the root complex through the Host-to-PCI bridge, which
    represents the upstream bridge in the root complex.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Only the downstream ports of PCI-PCI bridges are taken into account during bus
    enumeration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After applying the enumeration algorithm to the topology in the diagram, we
    can list 10 steps, from `0` along with two bridges (thus providing two buses),
    `00:00:00` and `00:01:00`. The following are descriptions of the steps in the
    enumeration process in the preceding topology diagram, although step **C** is
    where standardized enumeration logic starts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`00:00` being a (virtual) bridge, undoubtedly, its downstream port is a bus.
    It is then assigned the number `1` (remember, it is always greater than the bus
    number where the bridge lives, which is `0` in this case). Bus `1` is then enumerated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step `1` (one upstream virtual bridge that provides its internal bus and two
    downstream virtual bridges that expose its output buses). This switch's internal
    bus is given the number `2`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We immediately fall into step `3`, behind which there is an endpoint (no downstream
    port). According to the principle of the DFS algorithm, we reached the leaf node
    of this branch, so we can start backtracking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence step `4`, and there is a device behind it. Backtracking can happen again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then reach step `5`. There is a switch behind this bus (one upstream virtual
    bridge implementing the internal bus, which is given the bus number `6`, and `3`
    downstream virtual bridges representing its external buses, hence this is a 3-port
    switch).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step `7`, and there is an endpoint behind this bus. If we had to identify the
    function `0` of this endpoint using the BDF format, it would be `07:00:00` (function
    `0` of device `0` on bus `7`). Back to the DFS algorithm, we have reached the
    bottom of the branch. We can then start backtracking, which leads us to step **H**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In step `8`. There is a PCIe-to-PCI bridge behind this bus.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In step `9` downstream, and there is a 3-function endpoint behind this bus.
    In BDF notation, these would be identified as `09:00:00`, `09:00:01`, and `09:00:02`.
    Since endpoints mark the depth of a branch, it allows us to perform another backtrack,
    which leads us to step **J**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the backtracking phase, we enter step `10`. There is an endpoint behind this
    bus, which would have been identified as `0a:00:00` in BDF format. This marks
    the end of the enumeration process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCI(e) bus enumeration may look complicated at first glance, but it is quite
    simple. Reading the preceding material twice can be enough to understand the whole
    process.
  prefs: []
  type: TYPE_NORMAL
- en: PCI address spaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A PCI target can implement up to three different types of address spaces according
    to their content or by access method. These are **Configuration Address Space**,
    **Memory Address Space**, and **I/O Address Space**. Configuration and memory
    address spaces are memory mapped – they are assigned address ranges from the system
    address space, so that reads and writes to that range of addresses don't go to
    RAM, but are routed to the device directly from the CPU, while I/O address space
    is not. Without further ado, let's analyze the differences between them and their
    different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: PCI configuration space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the address space from where the configuration of the device can be
    accessed and which stores basic information about the device, and which is also
    used by the OS to program the device with its operational settings. There are
    `256` bytes for the configuration space on PCI. The PCIe extends this to `4` KB
    of register space. Since configuration address space is memory mapped, any address
    that points to configuration space is allocated from the system memory map. Thus,
    these `4` KB spaces allocate memory addresses from the system memory map, but
    the actual values/bits/content are generally implemented in registers on the peripheral
    device. For instance, when you read the vendor ID or device ID, the target peripheral
    device will return the data even though the memory address being used is from
    the system memory map.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part of this address space is standardized. Configuration address space is
    split as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first `64` bytes (`00h` – `3Fh`) represent the standard configuration header,
    which includes the PCI Bus ID, vendor ID, and device ID registers, to identify
    the device.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining `192` bytes (`40h` – `FFh`) make up the user-defined configuration
    space, such as the information specific to a PC card to be used by its accompanying
    software driver.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally speaking, the configuration space stores basic information about the
    device. It allows the central resource or OS to program a device with operational
    settings. There is no physical memory associated with configuration address space.
    It is a list of addresses used in the **TLP** (**Transaction Layer Packet**) in
    order to identify the target of the transaction.
  prefs: []
  type: TYPE_NORMAL
- en: Commands that are used to transfer data between each **Configuration Address
    Space** of PCI devices are known as either **Configuration Read** commands or
    **Configuration Write** commands.
  prefs: []
  type: TYPE_NORMAL
- en: PCI I/O address space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These days, I/O address space is used for compatibility with the x86 architecture's
    I/O port address space. The PCIe specification discourages the use of this address
    space. It will be no surprise if future revisions of the PCI Express specification
    deprecate the use of I/O address space. The only advantage of I/O mapped I/O is
    that, because of its separate address space, it does not steal address ranges
    from system memory space. As a result, computers can access the whole 4 GB of
    RAM on a 32-bit system.
  prefs: []
  type: TYPE_NORMAL
- en: '**I/O Read** and **I/O Write** commands are used to transfer data in **I/O
    Address Space**.'
  prefs: []
  type: TYPE_NORMAL
- en: PCI memory address space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the early days of computers, Intel defined a way to access registers in I/O
    devices via the so-called I/O address space. It made sense in those days because
    of the memory address space of the processor, which was quite limited (think of
    16-bit systems, for example) and it made little or even no sense to use some ranges
    of it for accessing devices. When the system memory space became less of a constraint
    (think of 32-bit systems, for example, where the CPU can address up to 4 GB),
    the separation between I/O address space and memory address space became less
    important, and even burdensome.
  prefs: []
  type: TYPE_NORMAL
- en: 'There were so many limitations and constraints on that address space that it
    resulted in registers in I/O devices being mapped directly to the system''s memory
    address space, hence, memory-mapped I/O, or MMIO. Those limitations and constraints
    included the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The need for a dedicated bus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A separate instruction set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And since it was implemented during the 16-bit systems era, the port address
    space was limited to `65536` ports (which corresponds to 216 ), although very
    old machines used 10 bits for I/O address space and had only 1024 unique port
    addresses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has therefore become more practical to take advantage of the benefits of
    memory-mapped I/O.
  prefs: []
  type: TYPE_NORMAL
- en: Memory-mapped I/O allows hardware devices to be accessed by simply reading or
    writing to those "special" addresses using the normal memory access instructions,
    though it is more expensive to decode up to 4 GB of address (or more) as compared
    to 65536\. That being said, PCI devices expose their memory regions through windows
    called BARs. A PCI device can have up to six BARs.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of BAR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**BAR** stands for **Base Address Register** and is a PCI concept with which
    a device tells the host how much memory it needs, as well as its type. This is
    memory space (grabbed from the system memory map), not actual physical RAM (you
    can actually think of RAM itself as a "specialized memory-mapped I/O device" whose
    job is just to save and give back data, although with today''s modern CPUs with
    caching and such, this is not physically straightforward). It is the responsibility
    of the BIOS or the OS to allocate the requested memory space to the target device.'
  prefs: []
  type: TYPE_NORMAL
- en: Once assigned, the BARs are seen as memory windows by the host system (CPUs)
    to talk to the device. The device itself doesn't write into that window. This
    concept can be seen as an indirection mechanism to access the real physical memory,
    which is internal and local to the PCI device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, the real physical address of the memory and the address of the input/output
    registers are internal to the PCI device. The following is how a host deals with
    the storage space of peripherals:'
  prefs: []
  type: TYPE_NORMAL
- en: The peripheral device tells the system by some means that it has several storage
    intervals and I/O address space, how big each interval is, and their respective
    local addresses. Obviously, these addresses are local and internal, all starting
    from `0`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the system software knows how many peripherals there are, and what kind
    of storage intervals they have, they can assign "physical addresses" to these
    intervals, and establish the connection between these intervals and the bus. These
    addresses are accessible. Obviously, the so-called "physical address" here is
    somewhat different from the real physical address. It is actually a logical address,
    so it often becomes a "bus address" because this is the address that the CPU sees
    on the bus. As you can imagine, there must be some sort of address mapping mechanism
    on the peripheral. The so-called "allocation of addresses for peripherals" is
    to assign bus addresses to them and establish mappings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interrupt distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here we will discuss the way in which interrupts are handled by a PCI device.
    There are three interrupt types in PCI Express. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Legacy interrupts, also called INTx interrupts, the only one mechanism available
    in the old PCI implementations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MSI** (**Message Based Interrupts**) extend the legacy mechanism, for example,
    by increasing the number of interrupts that are possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MSI-X (eXtended MSI) extends and enhances MSI, for example, by allowing the
    targeting of individual interrupts to different processors (helpful in some high-speed
    networking applications).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application logic in a PCI Express endpoint can implement one or more of
    the three methods enumerated above to signal an interrupt. Let's look at these
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: PCI legacy INT-X-based interrupts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Legacy interrupt management was based on PCI INT-X interrupt lines, made of
    up to four virtual interrupt wires, referred to as INTA, INTB, INTC, and INTD.
    These interrupt wires are shared by all the PCI devices in the system. The following
    are the steps that the legacy implementation had to go through in order to identify
    and handle the interrupt:'
  prefs: []
  type: TYPE_NORMAL
- en: The device asserts one of its INT# pins in order to generate an interrupt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The CPU acknowledges the interrupt and polls each device (actually its driver)
    connected to this INT# line (shared) by calling their interrupt handlers. The
    time needed to service the interrupt depends on the number of devices sharing
    the line.The device's interrupt service routine (ISR) may check whether the interrupt
    is originating from this device by reading the device's internal registers to
    identify the cause of the interrupt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ISR takes action to service the interrupt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the preceding method as well as the legacy method, interrupt lines are shared:
    Everyone answers the phone. Moreover, physical interrupt lines are limited. In
    the following section, we see how MSI addresses those issues and facilitates interrupt
    management.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The i.MX6 maps INTA/B/C/D to ARM GIC IRQ `155`/`154`/`153`/`152`, respectively.
    This allows a PCIe-to-PCI bridge to function properly. Refer to IMX6DQRM.pdf,
    page 225.
  prefs: []
  type: TYPE_NORMAL
- en: Message-based interrupt type – MSI and MSI-X
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two message-based interrupt mechanisms: MSI and MSI-X, the enhanced
    and extended version. MSI (or MSI-X) is simply a way of signaling interrupts using
    the PCI Express protocol layer, and the PCIe root complex (the host) takes care
    of interrupting the CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, a device is assigned pins as interrupt lines, which it has to
    assert when it wants to signal an interrupt to the CPU. This kind of signaling
    method is out-of-band since it uses yet another way (different from the main data
    path) to send such control information.
  prefs: []
  type: TYPE_NORMAL
- en: However, MSI allows the device to write a small amount of interrupt-describing
    data to a special memory-mapped I/O address, and the root complex then takes care
    of delivering the corresponding interrupt to the CPU. Once the endpoint device
    wants to generate the MSI interrupt, it issues a write request to (target) the
    address specified in the message address register with the data contents specified
    in the message data register. Since the data path is used for this, it is an in-band
    mechanism. Moreover, MSI increases the number of possible interrupts. This is
    described in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: PCI Express does not have a separate interrupt pin at all. However, it is compatible
    with legacy interrupts on a software level. For this, it requires MSI or MSI-X
    since it uses special in-band messages to allow pin assertion or de-assertion
    to be emulated. In other words, PCI Express emulates this capability by providing
    `assert_INTx` and `deassert_INTx`. Message packets are sent through the PCI Express
    serial link.
  prefs: []
  type: TYPE_NORMAL
- en: 'In an implementation using MSI, the following are the usual steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The device generates an interrupt by sending an MSI memory write upstream.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The CPU acknowledges the interrupt and calls the appropriate device ISR since
    this is known in advance based on the MSI vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ISR takes action to service the interrupt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MSIs are not shared, so an MSI that is assigned to a device is guaranteed to
    be unique within the system. It goes without saying that MSI implementation significantly
    reduces the total servicing time needed for the interrupt.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Most people think that MSI allows the device to send data to a processor as
    part of the interrupt. This is a misconception. The truth is that the data that
    is sent as part of the memory write transaction is exclusively used by the chipset
    (the root complex actually) to determine which interrupt to trigger on which processor;
    that data is not available for the device to communicate additional information
    to the interrupt handler.
  prefs: []
  type: TYPE_NORMAL
- en: MSI mechanism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MSI was originally defined as part of the PCI 2.2 standard, allowing a device
    to allocate either 1, 2, 4, 8, 16, or up to 32 interrupts. The device is programmed
    with an address to write to in order to signal an interrupt (generally, a control
    register in an interrupt controller), and a 16-bit data word to identify the device.
    The interrupt number is added to the data word to identify the interrupt.
  prefs: []
  type: TYPE_NORMAL
- en: A PCI Express endpoint may signal an MSI by sending a standard PCI Express posted
    write packet to the root port. The packet is made of a specific address (allocated
    by the host) and one of up to 32 data values (thus, 32 interrupts) provided by
    the host to the endpoint. The varying data values and the address value provide
    more detailed identification of interrupt events than legacy interrupts. Interrupt
    masking capability is optional in MSI specifications.
  prefs: []
  type: TYPE_NORMAL
- en: This approach does have some limitations. Only one address is used for the 32
    data values, which makes it difficult to target individual interrupts to different
    processors. This limitation is due to the fact that a memory-write operation associated
    with an MSI can only be distinguished from other memory-write operations by the
    address locations (not the data) they target, which are reserved by the system
    for interrupt delivery.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the MSI configuration steps, performed by the PCI controller
    driver for a PCI Express device:'
  prefs: []
  type: TYPE_NORMAL
- en: The bus enumeration process takes place during startup. It consists of kernel
    PCI core code scanning the PCI bus(es) in order to discover devices (in other
    words, it carries out configuration reads for valid vendor IDs). Upon discovering
    a PCI Express function, the PCI core code reads the capabilities list pointer
    to obtain the location of the first capability register within the chain of registers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The PCI core code then searches the capability register sets. It keeps doing
    this until it discovers the MSI capability register set (capability ID of `05h`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, the PCI core code configures the device, assigning a memory address
    to the device's message address register. This is the destination address of the
    memory write used when delivering an interrupt request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The PCI core code checks the Multiple Message Capable field in the device's
    message control register to determine how many event-specific messages the device
    would like assigned to it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The core code then allocates a number of messages equal to or less than what
    the device requested. As a minimum, one message will be allocated to the device.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The core code writes the base message data pattern into the device's message
    data register.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the PCI core code sets the MSI enable bit in the device's message control
    register, thereby enabling it to generate interrupts using MSI memory writes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MSI-X mechanism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`2048` address and data pairs. Thanks to this large number of address values
    available to each endpoint, it''s possible to route MSI-X messages to different
    interrupt consumers in a system, unlike the single address available to MSI packets.
    Moreover, endpoints with MSI-X capability also include application logic to mask
    and hold pending interrupts, as well as a memory table for the address and data
    pairs.'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the above, an MSI-X interrupt is identical to an MSI. However, optional
    features in MSI (such as 64-bit addressing and interrupt masking) are made mandatory
    with MSI-X.
  prefs: []
  type: TYPE_NORMAL
- en: Legacy INTx emulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because PCIe claimed backward compatibility with legacy parallel PCI, it also
    needed to support INTx-based interrupt mechanisms. But how can this be implemented?
    Actually, there were four INTx (INTA, INTB, INTC, and INTD) physical IRQ lines
    in classical PCI systems, which were all level-triggered, active low actually
    (in other words, the interrupt request is active as long as the physical INTx
    wire is at a low voltage). Then how is each IRQ transported in the emulated version?
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is that PCIe virtualizes PCI physical interrupt signals by using
    an in-band signaling mechanism, the so-called MSI. Since there were two levels
    (asserted and de-asserted) per physical line, PCIe provides two messages per line,
    known as `assert_INTx` and `deassert_INTx` messages. There are eight message types
    in total: `assert_INTA`, `deassert_INTA`, ... `assert_INTD`, `deassert_INTD`.
    In fact, they are simply known as INTx messages. This way, INTx interrupts are
    propagated across the PCIe link just like MSI and MSI-X.'
  prefs: []
  type: TYPE_NORMAL
- en: This backward compatibility exists mainly for PCI to PCIe bridge chips so that
    PCI devices will work properly in a PCIe system without modifying the drivers.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are familiar with interrupt distribution in the PCI subsystem. We have
    covered both legacy INT-X-based mechanisms and message-based mechanisms. Now it's
    time to dive into the code, from data structures to APIs.
  prefs: []
  type: TYPE_NORMAL
- en: The Linux kernel PCI subsystem and data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Linux kernel supports the PCI standard and provides APIs to deal with such
    devices. In Linux, the PCI implementation can be broadly divided into the following
    main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`arch/arm/kernel/bios32.c`. The PCI BIOS code interfaces with PCI Host Controller
    code as well as the PCI core in order to perform bus enumeration and the allocation
    of resources, such as memory and interrupts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The successful completion of BIOS execution guarantees that all the PCI devices
    in the system are assigned parts of available PCI resources and their respective
    drivers (referred to as slave or endpoint drivers) can take control of them using
    the facilities provided by the PCI core.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the kernel invokes the services of architecture and board-specific PCI
    functionalities. Two important tasks of PCI configuration are completed here.
    The first task is to scan for all PCI devices on the bus, configure them, and
    allocate memory resources. The second task is to configure the device. Configured
    here means a resource (memory) was reserved and IRQ assigned. It does not mean
    initialized. Initialization is device-specific and should be done by the device
    driver. PCI BIOS may optionally skip resource allocation (if they were assigned
    before Linux was booted, for example, in a PC scenario).
  prefs: []
  type: TYPE_NORMAL
- en: '`drivers/pci/host/`, in other words, `drivers/pci/controller/pcie-rcar.c` for
    r-car SoCs). However, some SoCs may implement the same PCIe IP block from a given
    vendor, such as Synopsys DesignWare. Such controllers can be found in the same
    directory, such as `drivers/pci/controller/dwc/` in the kernel source. For instance,
    the i.MX6 whose PCIe IP block is from this vendor has its driver implemented in
    `drivers/pci/controller/dwc/pci-imx6.c`.This part handles SoC (and sometimes board)-specific
    initialization and configuration and may invoke the PCI BIOS. However, it should
    provide PCI bus access and facility callback functions for BIOS as well as the
    PCI core, which will be called during PCI system initialization and while accessing
    the PCI bus for configuration cycles. Moreover, it provides resource information
    for available memory/IO space, INTx interrupt lines, and MSI. It should facilitate
    IO space access (as supported) and may also need to provide indirect memory access
    (if supported by hardware).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drivers/pci/probe.c`): This is in charge of creating and initializing the
    data structure tree for buses, devices, as well as bridges in the system. It handles
    bus/device numbering. It creates device entries and provides `proc/sysfs` information.
    It also provides services for PCI BIOS and slave (**End Point**) drivers and optionally
    hot plug support (if supported by h/w). It targets the (**EP**) driver interface
    query and initializes corresponding devices found during enumeration. It also
    provides an MSI interrupt handling framework and PCI Express port bus support.
    All of the above is enough to facilitate the development of device drivers in
    the Linux kernel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCI data structures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Linux kernel PCI framework aids development on PCI device drivers, which
    are built on top of the two main data structures: `struct pci_dev`, which represents
    a PCI device from within the kernel, and `struct pci_driver`, which represents
    a PCI driver.'
  prefs: []
  type: TYPE_NORMAL
- en: struct pci_dev
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is the structure with which the kernel instantiates each PCI device on
    the system. It describes the device and stores some of its state parameters. This
    structure is defined in `include/linux/pci.h` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding block, some elements have been removed for the sake of readability.
    For the remaining, the following elements have the following meanings:'
  prefs: []
  type: TYPE_NORMAL
- en: '`procent` is the device entry in `/proc/bus/pci/`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`slot` is the physical slot this device is in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vendor` is the vendor ID of the device manufacturer. The PCI Special Interest
    Group maintains a global registry of such numbers, and manufacturers must apply
    to have a unique number assigned to them. This ID is stored in a 16-bit register
    in the device configuration space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` is the ID that identifies this particular device once it is probed.
    This is vendor-dependent and there is no official registry as such. This is also
    stored in a 16-bit register.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subsystem_vendor` and `subsystem_device` specify the PCI subsystem vendor
    and subsystem device IDs. They can be used for further identification of the device,
    as we have seen earlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class` identifies the class this device belongs to. It is stored in a 16-bit
    register (in the device configuration space) whose top 8 bits identify the base
    class or group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pin` is the interrupt pin this device uses, in the case of the legacy INTx-based
    interrupt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`driver` is the driver associated with this device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dev` is the underlying device structure for this PCI device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cfg_size` is the size of the configuration space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`irq` is the field that is worth spending time on. When the device boots, MSI(-X)
    mode is not enabled and it remains unchanged until it is explicitly enabled by
    means of the `pci_alloc_irq_vectors()` API (old drivers use `pci_enable_msi()`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consequently, `irq` first corresponds to the default preassigned non-MSI IRQ.
    However, its value or usage may change according to one of the following situations:'
  prefs: []
  type: TYPE_NORMAL
- en: a) In MSI interrupt mode (upon a successful call to `pci_alloc_irq_vectors()`
    with the `PCI_IRQ_MSI` flag set), the (preassigned) value of this field is replaced
    by a new MSI vector. This vector corresponds to the base interrupt number of the
    allocated vectors, so that the IRQ number corresponding to vector X (index starting
    from 0) is equivalent to (the same as) `pci_dev->irq + X` (see the `pci_irq_vector()`
    function, which is intended to return the Linux IRQ number of a device vector).
  prefs: []
  type: TYPE_NORMAL
- en: b) In MSI-X interrupt mode (upon a successful call to `pci_alloc_irq_vectors()`
    with the `PCI_IRQ_MSIX` flag set), the (preassigned) value of this field is untouched
    (because of the fact that each MSI-X vector has its dedicated message address
    and message data pair, and this does not require the 1:1 vector-to-entry mapping).
    However, in this mode, `irq` is invalid. Using it in the driver to request a service
    interrupt may result in unpredictable behavior. Consequently, in case MSI(-X)
    is needed, the `pci_alloc_irq_vectors()` function (which enables MXI(-X) prior
    to allocating vectors) should be called before the driver calls `devm_equest_irq()`,
    because MSI(-X) is delivered via a vector that is different from the vector of
    a pin-based interrupt.
  prefs: []
  type: TYPE_NORMAL
- en: '`msi_enabled` holds the enabling state of the MSI IRQ mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`msix_enabled` holds the enabling state of the MSI-X IRQ mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enable_cnt` holds the number of times `pci_enable_device()` has been called.
    This helps in really disabling the device only after all callers of `pci_enable_device()`
    have called `pci_disable_device()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: struct pci_device_id
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While `struct pci_dev` describes the device, `struct pci_device_id` is intended
    to identify the device. This structure is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To understand how this structure is important for the PCI driver, let''s describe
    each of its elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '`vendor` and `device` represent the vendor ID and the device ID of the device,
    respectively. Both are paired to make a unique 32-bit identifier for a device.
    The driver relies on this 32-bit identifier to identify its devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subvendor` and `subdevice` represent the subsystem ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class`, `class_mask` are class-related PCI drivers that are intended to handle
    every device of a given class. For such drivers, `vendor` and `device` should
    be set to `PCI_ANY_ID`. The different classes of PCI devices are described in
    the PCI specification. These two values allow the driver to specify that it supports
    a type of PCI class device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`driver_data` is the data private to the driver. This field is not used to
    identify a device, but to pass different data to differentiate between devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are three macros that allow you to create specific instances of `struct
    pci_device_id`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PCI_DEVICE`: This macro is used to describe a specific PCI device by creating
    a `struct pci_device_id` that matches a specific PCI device with the vendor and
    device IDs given as parameters (`PCI_DEVICE(vend,dev)`), and with the sub-vendor,
    sub-device, and class-related fields set to `PCI_ANY_ID`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PCI_DEVICE_CLASS`: This macro is used to describe a specific PCI device class
    by creating a `struct pci_device_id` that matches a specific PCI class with `class`
    and `class_mask` given as parameters (`PCI_DEVICE_CLASS(dev_class,dev_class_mask)`).
    The vendor, device, sub-vendor, and sub-device fields will be set to `PCI_ANY_ID`.
    A typical example is `PCI_DEVICE_CLASS(PCI_CLASS_STORAGE_EXPRESS, 0xffffff)`,
    which corresponds to the PCI class for NVMe devices, and which will match any
    of these whatever the vendor and device IDs are.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PCI_DEVICE_SUB`: This macro is used to describe a specific PCI device with
    a subsystem by creating a `struct pci_device_id` that matches a specific device
    with subsystem information given as parameters (`PCI_DEVICE_SUB(vend, dev, subvend,
    subdev)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Every device/class supported by the driver should be fed into the same array
    for later use (there are two places where we are going to use it), as in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Each `pci_device_id` structure needs to be exported to user space in order
    to let the hotplug and device manager (`udev`, `mdev`, and so on ...) know what
    driver goes with what device. The first reason to feed them all in the same array
    is that they can be exported in a single shot. To achieve this, you should use
    the `MODULE_DEVICE_TABLE` macro, as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This macro creates a custom section with the given information. At compilation
    time, the build process (`depmod` to be more precise) extracts this information
    out of the driver and builds a human-readable table called `modules.alias`, located
    in the `/lib/modules/<kernel_version>/` directory. When the kernel tells the hotplug
    system that a new device is available, the hotplug system will refer to the `modules.alias`
    file to find the proper driver to load.
  prefs: []
  type: TYPE_NORMAL
- en: struct pci_driver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This structure represents an instance of a PCI device driver, whatever it is
    and whatever subsystem it belongs to. It is the main structure that every PCI
    driver must create and fill in order to be able to get them registered with the
    kernel. `struct pci_driver` is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Parts of the elements in this structure have been removed since they hold no
    interest for us. The following are the meanings of the remaining fields in the
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: This is the name of the driver. Since drivers are identified by their
    name, it must be unique among all PCI drivers in the kernel. It is common to set
    this field to the same name as the module name of the driver. If there is already
    a driver register with the same name in the same subsystem bus, the registration
    of your driver will fail. To see how it works under the hood, have a look at `driver_register()`
    at [https://elixir.bootlin.com/linux/v4.19/source/drivers/base/driver.c#L146](https://elixir.bootlin.com/linux/v4.19/source/drivers/base/driver.c#L146).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`id_table`: This should point to the `struct pci_device_id` table described
    earlier. It is the second and last place this structure is used in the driver.
    It must be non-NULL for the probe to be called.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`probe`: This is the pointer to the `probe` function of the driver. It is called
    by the PCI core when a PCI device matches (either by vendor/product IDs or class
    ID) an entry in `id_table` in the driver. This method should return `0` if it
    managed to initialize the device, or a negative error otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remove`: This is called by the PCI core when a device handled by this driver
    is removed from the system (disappears from the bus) or when the driver is being
    unloaded from the kernel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`suspend`, `resume`, and `shutdown`: These are optional but recommended power
    management functions. In those callbacks, you can use PCI-related power management
    helpers such as `pci_save_state()` or `pci_restore_state()`, `pci_disable_device()`
    or `pci_enable_device()`, `pci_set_power_state()`, and `pci_choose_state()`. These
    callbacks are invoked by the PCI core, respectively:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: – When the device is suspended, in which case the state is given as an argument
    to the callback.
  prefs: []
  type: TYPE_NORMAL
- en: – When the device is being resumed. This may occur only after `suspend` has
    been called.
  prefs: []
  type: TYPE_NORMAL
- en: – For a proper shutdown of the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a PCI driver structure being initialized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Registering a PCI driver
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Registering a PCI driver with the PCI core consists of calling `pci_register_driver()`,
    given an argument as a pointer to the `struct pci_driver` structure set up earlier.
    This should be done in the `init` method of the module, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`pci_register_driver()` returns `0` if everything went well while registering,
    or a negative error otherwise. This return value is handled by the kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, on the unloading path of the module, `struct pci_driver` needs to
    be unregistered so that the system does not try to use a driver whose corresponding
    module no longer exists. Thus, unloading a PCI driver entails calling `pci_unregister_driver()`,
    along with a pointer to the same structure as per the registration, shown as follows.
    This should be done in the module `exit` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'That being said, with those operations often being repeated in PCI drivers,
    the PCI core exposes the `module_pci_macro()` macro in order to handle registering/unregistering
    automatically, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This macro is safer, as it takes care of both registering and unregistering,
    preventing some developers from providing one and forgetting the other.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are familiar with the most important PCI data structures – `struct pci_dev`,
    `pci_device_id`, and `pci_driver`, as well as the Hyphenate helpers to deal with
    those data structures. The logical continuation is the driver structure, in which
    we learn where and how to use the previously enumerated data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the PCI driver structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While writing a PCI device driver, there are steps that need to be followed,
    some of which need to be done in a predefined order. Here, we try to discuss in
    detail each of these steps, explaining details where applicable.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the device
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before any operation can be performed on a PCI device (even for reading its
    configuration registers only), this PCI device must be enabled, and this has to
    be done explicitly by the code. The kernel provides `pci_enable_device()` for
    this purpose. This function initializes the device so that it can be used by a
    driver, asking low-level code to enable I/O and memory. It also handles PCI power
    management wake up, such that if the device was suspended, it will be woken up,
    too. The following is what `pci_enable_device()` looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Since `pci_enable_device()` can fail, the value it returns must be checked,
    as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Remember, `pci_enable_device()` will initialize both memory-mapped and I/O BARs.
    However, you may want to initialize one but not the other, either because your
    device does not support both, or because you'll not use both in the driver.
  prefs: []
  type: TYPE_NORMAL
- en: In order not to initialize I/O spaces, you can use another variant of the enabling
    method, `pci_enable_device_mem()`. On the other hand, if you need to deal with
    I/O space only, you can use the `pci_enable_device_io()` variant instead. The
    difference between both variants is that `pci_enable_device_mem()` will initialize
    only memory-mapped BARs, whereas `pci_enable_device_io()` will initialize I/O
    BARs. Note that if the device is enabled more than once, each operation will increment
    the `.enable_cnt` field in the `struct pci_dev` structure, but only the first
    operation will really act on the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the PCI device is to be disabled, you should adopt the `pci_disable_device()`
    method, whatever the enabling variant you used. This method signals to the system
    that the PCI device is no longer in use by the system. The following is its prototype:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`pci_disable_device()` also disables bus mastering on the device, if active.
    However, the device is not disabled until all the callers of `pci_enable_device()`
    (or one of its variants) have called `pci_disable_device()`.'
  prefs: []
  type: TYPE_NORMAL
- en: Bus mastering capability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A PCI device can, by definition, initiate transactions on the bus, at the very
    moment at which it becomes the bus master. After the device is enabled, you may
    want to enable bus mastering.
  prefs: []
  type: TYPE_NORMAL
- en: 'This actually consists of enabling DMA in the device, by setting the bus master
    bit in the appropriate configuration register. The PCI core provides `pci_set_master()`
    for this purpose. This method also invokes `pci_bios (pcibios_set_master()` actually)
    in order to perform the necessary arch-specific settings. `pci_clear_master()`
    will disable DMA by clearing the bus master bit. It is the reverse operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note that `pci_set_master()` must be invoked if the device is intended to perform
    DMA operations.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing configuration registers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the device is bound to the driver and after it has been enabled by the
    driver, it is common to access device memory spaces. The one often accessed first
    is the configuration space. Conventional PCI and PCI-X mode 1 devices have 256
    bytes of configuration space. PCI-X mode 2 and PCIe devices have 4,096 bytes of
    configuration space. It is primordial for the driver to be able to access the
    device configuration space, either to read information mandatory for the proper
    operation of the driver, or to set up some vital parameters. The kernel exposes
    standard and dedicated APIs (read and write) for different sizes of data configuration
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to read data from the device configuration space, you can use the
    following primitives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The above reads, respectively, as one, two, or four bytes in the configuration
    space of the PCI device represented here by the `dev` argument. The `read` value
    is returned to the `val` argument. When it comes to writing data to the device
    configuration space, you can use the following primitives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The above primitives write, respectively, one, two, or four bytes into the device
    configuration space. The `val` argument represents the value to write.
  prefs: []
  type: TYPE_NORMAL
- en: 'In either read or write cases, the `where` argument is the byte offset from
    the beginning of the configuration space. However, there exist in the kernel some
    commonly accessed configuration offsets identified by symbolically named macros,
    defined in `include/uapi/linux/pci_regs.h`. The following is a short excerpt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, to get the revision ID of a given PCI device, you could use the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the above, we used `pci_read_config_byte()` because the revision is represented
    by one byte only.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Since data is stored in (and read from) PCI devices in little endian format,
    read primitives (actually `word` and `dword` variants) take care of converting
    read data into the native endianness of the CPU, and write primitives (`word`
    and `dword` variants) take care of converting data from the native CPU byte order
    to little endian prior to writing data to the device.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing memory-mapped I/O resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Memory registers are used for just about everything else, for example, for burst
    transactions. Those registers actually correspond to the device memory BARs. Each
    of them is then assigned a memory region from the system address space so that
    any access to those regions is redirected to the corresponding device, targeting
    the right local (in the device) memory corresponding to the BAR. This is memory-mapped
    I/O.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Linux kernel memory-mapped I/O world, it is common to request (to claim
    actually) a memory region before creating a mapping for it. You can use `request_mem_region()`
    and `ioremap()` primitives for both purposes. The following are their prototypes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`request_mem_region()` is a pure reservation mechanism and does not perform
    any mapping. It relies on the fact that other drivers should be polite and should
    call `request_mem_region()`on their turns, which would prevent another driver
    from overlapping a memory region that has already been claimed. You should not
    map nor access the claimed region unless this call returns successfully. In its
    arguments, `name` represents the name to be given to the resource, `start` represents
    what address the mapping should be created for, and `n` indicates how large the
    mapping should be. To obtain this information for a given BAR, you can use `pci_resource_start()`,
    `pci_resource_len()`, or even `pci_resource_end()`, whose prototypes are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`unsigned long pci_resource_start (struct pci_dev *dev, int bar)`: This function
    returns the first address (memory address or I/O port number) associated with
    the BAR whose index is bar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unsigned long pci_resource_len (struct pci_dev *dev, int bar)`: This function
    returns the size of the BAR `bar`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unsigned long pci_resource_end (struct pci_dev *dev, int bar)`: This function
    returns the last address that is part of the I/O region number `bar`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unsigned long pci_resource_flags (struct pci_dev *dev, int bar)`: This function
    is not only related to the memory resource BAR. It actually returns the flags
    associated with this resource. `IORESOURCE_IO` would mean the BAR `bar` is an
    I/O resource (thus suitable for I/O mapped I/O), while `IORESOURCE_MEM` would
    mean it is a memory resource (to be used for memory-mapped I/O).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other side, `ioremap()` does create real mapping, and returns a memory-mapped
    I/O cookie on the mapped region. As an example, the following code shows how to
    map the `bar0` of a given device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code works well, but it is tedious, since we would do this for
    each BAR. In fact, `request_mem_region()` and `ioremap()` are quite basic primitives.
    The PCI framework provides many more PCI-related functions to facilitate such
    common tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding helpers can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pci_request_regions()` marks all PCI regions associated with the `pdev` PCI
    device as being reserved by the owner `res_name`. In its arguments, `pdev` is
    the PCI device whose resources are to be reserved and `res_name` is the name to
    be associated with the resource. `pci_request_region()`, on the other hand, targets
    a single BAR, identified by the `bar` argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pci_iomap()` creates a mapping for a BAR. You can access it using `ioread*()`
    and `iowrite*()`. `maxlen` specifies the maximum length to map. If you want to
    get access to the complete BAR without checking for its length first, pass `0`
    here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pci_iomap_range()` creates a mapping from starting from an offset in the BAR.
    The resulting mapping starts at `offset` and is `maxlen` wide. `maxlen` specifies
    the maximum length to map. If you want to get access to the complete BAR from
    `offset` to the end, pass `0` here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pci_ioremap_bar()` provides an error-proof way (relative to `pci_ioremap()`)
    to carry out a PCI memory remapping.. It makes sure the BAR is actually a memory
    resource, not an I/O resource. However, it maps the whole BAR size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pci_iounmap()` is the opposite of `pci_iomap()`, which undoes the mapping.
    Its `addr` argument corresponds to the cookie previously returned by `pci_iomap()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pci_release_regions()` is the opposite of `pci_request_regions()`. It releases
    reserved PCI I/O and memory resources previously claimed (reserved). `pci_release_region()`
    targets the single BAR variant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using these helpers, we can rewrite the same code as before, but for BAR1 this
    time. This would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: After the memory areas are claimed and mapped, `ioread*()` and `iowrite*()`
    APIs, which provide platform abstraction, access the mapped registers.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing I/O port resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'I/O port access requires going through the same steps as I/O memory, although
    the underlying mechanisms are different: requesting the I/O region, mapping the
    I/O region (this is not mandatory, it is just a matter of politeness), and accessing
    the I/O region.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first two steps have already been addressed without you noticing. Actually,
    `pci_requestregion*()` primitives handle both I/O port and I/O memory. It relies
    on the resource flags (`pci_resource_flags()`) in order to call the appropriate
    low-level helper (`(request_region()`) for I/O port or `request_mem_region()`
    for I/O memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Thus, whether the resource is I/O memory or I/O port, you can safely use either
    `pci_request_regions()` or its single bar variant, `pci_request_region()`.
  prefs: []
  type: TYPE_NORMAL
- en: The same applies to the I/O port mapping. `pci_iomap*()` primitives are able
    to deal with either I/O port or I/O memory. They rely on the resource flags too,
    and they invoke the appropriate helper to create the mapping. Based on the resource
    type, the underlying mapping functions are `ioremap()` for I/O memory, which are
    resources of the `IORESOURCE_MEM` type, and `__pci_ioport_map()` for I/O port,
    which corresponds to resources of the `IORESOURCE_IO` type. `__pci_ioport_map()`
    is an arch-dependent function (overridden by MIPS and SH architectures actually),
    which, most of the time, corresponds to `ioport_map()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm what we just said, we can have a look at the body of the `pci_iomap_range()`
    function, on which `pci_iomap()` relies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'However, when it comes to accessing I/O ports, APIs completely change. The
    following are helpers for accessing I/O ports. These functions hide the details
    of the underlying mapping and of what type they are. The following lists the functions
    provided by the kernel to access I/O ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding excerpt, the `in*()` family reads one, two, or four bytes,
    respectively, from the `port` location. The data fetched is returned by a value.
    On the other hand, the `out*()` family writes one, two, or four bytes, respectively,
    referred to as a `value` argument in the `port` location.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with interrupts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Drivers that need to service interrupts for a device need to request those interrupts
    first. It is common to request interrupts from within the `probe()` method. That
    being said, in order to deal with legacy and non-MSI IRQ, drivers can directly
    use the `pci_dev->irq` field, which is preassigned when the device is probed.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for a more generic approach, it is recommended to use the `pci_alloc_irq_vectors()`
    API. This function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The preceding function returns the number of vectors allocated (which might
    be smaller than `max_vecs`) if successful, or a negative error code in the event
    of an error. The number of allocated vectors is always at least up to `min_vecs`.
    If less than `min_vecs` interrupt vectors are available for `dev`, the function
    will fail with `-ENOSPC`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of this function is that it can deal with either legacy interrupts
    and MSI or MSI-X interrupts. Depending on the `flags` argument, the driver can
    instruct the PCI layer to set up the MSI or MSI-X capability for this device.
    This argument is used to specify the type of interrupt used by the device and
    the driver. Possible flags are defined in `include/linux/pci.h`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PCI_IRQ_LEGACY`: A single legacy IRQ vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PCI_IRQ_MSI`: On the success path, `pci_dev->msi_enabled` is set to `1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PCI_IRQ_MSIX`: On the success path, `pci_dev->msix_enabled` is set to `1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PCI_IRQ_ALL_TYPES`: This allows trying to allocate any of the above kinds
    of interrupt, but in a fixed order. MSI-X mode is always tried first and the function
    returns immediately in case of success. If MSI-X fails, then MSI is tried. The
    legacy mode is used as a fallback in case both MSI-X and MSI fail. The driver
    can rely on `pci_dev->msi_enabled` and `pci_dev->msix_enabled` to determine which
    mode was successful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PCI_IRQ_AFFINITY`: This allows affinity auto-assign. If set, `pci_alloc_irq_vectors()`
    will spread the interrupts around the available CPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To get the Linux IRQ numbers to be passed to `request_irq()` and `free_irq()`,
    which corresponds to a vector, use the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding, `dev` is the PCI device to operate on, and `nr` is the device-relative
    interrupt vector index (0-based). Let''s now look at how this function works more
    closely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding excerpt, we can see that MSI-X is the first attempt (`if (dev->msix_enabled)`).
    Additionally, the returned IRQ has nothing to do with the original `pci_dev->irq`
    preassigned at device probe time. But if MSI is enabled (`dev->msi_enabled` is
    true) instead, then this function will perform some sanity check and will return
    `dev->irq + nr`. This confirms the fact that `pci_dev->irq` is replaced with a
    new value when we operate in MSI mode, and that this new value corresponds to
    the base interrupt number of the allocated MSI vectors. Finally, you'll notice
    that there are no special checks for legacy mode.
  prefs: []
  type: TYPE_NORMAL
- en: Actually, in legacy mode, the preassigned `pci_dev->irq` remains untouched,
    and it is only a single allocated vector. Thus, `nr` should be `0` when operating
    in legacy mode. In this case, the vector returned is nothing but `dev->irq`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some devices might not support using legacy line interrupts, in which case
    the driver can specify that only MSI or MSI-X is acceptable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Note that MSI/MSI-X and legacy interrupts are mutually exclusive and the reference
    design supports legacy interrupts by default. Once MSI or MSI-X interrupts are
    enabled on a device, it stays in this mode until they are disabled again.
  prefs: []
  type: TYPE_NORMAL
- en: Legacy INTx IRQ assignment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The probe method of the PCI bus type (`struct bus_type pci_bus_type`) is `pci_device_probe()`,
    implemented in `drivers/pci/pci-driver.c`. This method is invoked each time a
    new PCI device is added to the bus or when a new PCI driver is registered with
    the system. This function calls `pci_assign_irq(pci_dev)` and then `pcibios_alloc_irq(pci_dev)`
    in order to assign an IRQ to the PCI device, the famous `pci_dev->irq`. The trick
    starts happening in `pci_assign_irq()`. `pci_assign_irq()` reads the pin to which
    the PCI device is connected, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The next steps rely on the PCI host bridge, whose driver should expose a number
    of callbacks, including a special one, `.map_irq`, whose purpose is to create
    IRQ mappings for devices according to their slot and the previously read pin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This is the first assignment of the IRQ during the device probing. Going back
    to the `pci_device_probe()` function, the next method invoked after `pci_assign_irq()`
    is `pcibios_alloc_irq()`. However, `pcibios_alloc_irq()` is defined as a weak
    and empty function, overridden only by AArch64 architecture, in `arch/arm64/kernel/pci.c`,
    and which relies on ACPI (if enabled) to mangle the assigned IRQ. Perhaps in the
    feature other architecture will want to override this function as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final code of `pci_device_probe()` is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The IRQ value contained in `PCI_INTERRUPT_LINE` is BAD until after `pci_enable_device()`
    is called. However, a peripheral driver should never alter `PCI_INTERRUPT_LINE`
    because it reflects how a PCI interrupt is connected to the interrupt controller,
    which is not changeable.
  prefs: []
  type: TYPE_NORMAL
- en: Emulated INTx IRQ swizzling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Note that most PCIe devices in legacy INTx mode will default to the local INTA
    "virtual wire output," and the same holds true for many physical PCI devices connected
    by PCIe/PCI bridges. OSes would end up sharing the INTA input among all the peripherals
    in the system; all devices sharing the same IRQ line – I will let you picture
    the disaster.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this is "virtual wire INTx IRQ swizzling." Back to the code
    of the `pci_device_probe()` function, it invokes `pci_assign_irq()`. If you look
    at the body of this function (in `drivers/pci/setup-irq.c`), you'll notice some
    swizzling operations, which are intended to solve this.
  prefs: []
  type: TYPE_NORMAL
- en: Locking considerations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is common for many device drivers to have a per-device spinlock that is taken
    in the interrupt handler. Since interrupts are guaranteed to be non-reentrant
    on a Linux-based system, it is not necessary to disable interrupts when you are
    working with pin-based interrupts or a single MSI. However, if a device uses multiple
    interrupts, the driver must disable interrupts while the lock is held. This would
    prevent deadlock if the device sends a different interrupt, whose handler will
    try to acquire the spinlock that is already locked by the interrupt being serviced.
    Thus, the locking primitives to use in such situations are `spin_lock_irqsave()`
    or `spin_lock_irq()`, which disable local interrupts and acquire the lock. You
    can refer to [*Chapter 1*](B10985_01_ePub_AM.xhtml#_idTextAnchor015)*, Linux Kernel
    concepts for Embedded Developers,* for more details on locking primitive and interrupt
    management.
  prefs: []
  type: TYPE_NORMAL
- en: A word on legacy APIs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are many drivers that still use the old and now deprecated MSI or MSI-X
    APIs, which are `pci_enable_msi()`, `pci_disable_msi()`, `pci_enable_msix_range()`,
    `pci_enable_msix_exact()`, and `pci_disable_msix()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previously listed APIs should not be used in new code at all. However,
    the following is an example of a code excerpt trying to use MSI and falling back
    to legacy interrupt mode in case MSI is not available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Since the preceding code contains deprecated APIs, it may be a good exercise
    to convert it to new APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are done with the generic PCI device driver structure and have addressed
    interrupt management in such drivers, we can move a step forward and leverage
    the direct memory access capabilities of the device.
  prefs: []
  type: TYPE_NORMAL
- en: PCI and Direct Memory Access (DMA)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to speed up data transfer and offload the CPU by allowing it not to
    perform heavy memory copy operations, both the controller and the device can be
    configured to perform Direct Memory Access (DMA), which is a means by which data
    is exchanged between device and host without the CPU being involved. Depending
    on the root complex, the PCI address space can be either 32 or 64 bits.
  prefs: []
  type: TYPE_NORMAL
- en: 'System memory regions that are the source or destination of DMA transfers are
    called DMA buffers. However, DMA buffer memory ranges depend on the size of the
    bus address. This originated from the ISA bus, which was 24-bits wide. In such
    a bus, DMA buffers could live only in the bottom 16 MB of system memory. This
    bottom memory is also referred to as `ZONE_DMA`. However, PCI buses do not have
    such limitations. While the classic PCI bus supports 32-bit addressing, PCIe extended
    this to 64 bits. Thus, two different address formats can be used: the 32-bit address
    format and the 64-bit address format. In order to pull the DMA API, the driver
    should contain `#include <linux/dma-mapping.h>`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To inform the kernel of any special needs of DMA-able buffers (which consist
    of specifying the width of the bus), you can use `dma_set_mask()`, defined as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This would help the system in terms of efficient memory allocation, especially
    if the device can directly address "consistent memory" in system RAM above 4 GB
    physical RAM. In the above helper, `dev` is the underlying device for the PCI
    device, and `mask` is the actual mask to use, which you can specify by using the
    `DMA_BIT_MASK` macro along with the actual bus width. `dma_set_mask()` returns
    `0` on success. Any other value means an error occurred.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example for a 32-bit (or 64-bit) bit system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: That being said, DMA transfer requires suitable memory mapping. This mapping
    consists of allocating DMA buffers and generating a bus address for each, which
    are of the type `dma_addr_t`. Since I/O devices view DMA buffers through the lens
    of the bus controller and any intervening I/O memory management unit (IOMMU),
    the resulting bus addresses will be given to the device in order for it to be
    notified of the location of the DMA buffers. Since each memory mapping also produces
    a virtual address, not only bus addresses but also virtual addresses will be generated
    for the mapping. In order for the CPU to be able to access the buffers, DMA service
    routines also map the kernel virtual address of DMA buffers to bus addresses.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of (PCI) DMA mapping: coherent ones and streaming ones.
    For either, the kernel provides a healthy API that masks many of the internal
    details of dealing with the DMA controller.'
  prefs: []
  type: TYPE_NORMAL
- en: PCI coherent (aka consistent) mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Such mapping is called consistent because it allocates uncached (coherent) and
    unbuffered memory for the device to perform DMA operation. Since a write by either
    the device or the CPU can be immediately read by either without worrying about
    cache coherency, such mappings are also synchronous. All this makes consistent
    mapping too expensive for the system, although most devices require it. However,
    in terms of code, it is easier to implement.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function sets up a coherent mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'With the above, memory allocated for the mapping is guaranteed to be physically
    contiguous. `size` is the length of the region you need to allocate. This function
    returns two values: the virtual address that you can use to access it from the
    CPU and `dma_handle`, the third argument, which is an output parameter and which
    corresponds to the bus address the function call generated for the allocated region.
    The bus address is actually the one you pass to the PCI device.'
  prefs: []
  type: TYPE_NORMAL
- en: Do note that `pci_alloc_consistent()` is actually a dumb wrapper of `dma_alloc_coherent()`
    with the `GFP_ATOMIC` flag set, meaning allocation does not sleep and it is safe
    to call it from within an atomic context. You may want to use `dma_alloc_coherent()`
    (which is strongly encouraged) instead if you wish to change the allocation flags,
    for example, using `GFP_KERNEL` instead of `GFP_ATOMIC`.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that mapping is expensive, and the minimum it can allocate is a
    page. Under the hood, it only allocates the number of pages in the power of 2\.
    The order of pages is obtained with `int order = get_order(size)`. Such a mapping
    is to be used for buffers that last the lifetime of the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'To unmap and free such a DMA region, you can call `pci_free_consistent()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Here, `cpu_addr` and `dma_handle` correspond to the kernel virtual address and
    to the bus address returned by `pci_alloc_consistent()`. Though the mapping function
    can be called from an atomic context, this one may not be called in such a context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Do also note that `pci_free_consistent()` is a simple wrapper of `dma_free_coherent()`,
    which can be used if the mapping has been done using `dma_alloc_coherent()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code shows how to perform a DMA mapping and send the resulting
    bus address to the device. In the real world, an interrupt may be raised. You
    should then handle it from within the driver.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming DMA mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Streaming mapping, on the other hand, has more constraints in terms of code.
    First of all, such mappings need to work with a buffer that has already been allocated.
    Additionally, a buffer that has been mapped belongs to the device and not to the
    CPU anymore. Thus, before the CPU can use the buffer, it should be unmapped first,
    in order to address possible caching issues.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to initiate a write transaction (CPU to device), the driver should
    place data in the buffer prior to the mapping. Moreover, the direction the data
    should move into has to be specified, and the data should only be used based on
    this direction.
  prefs: []
  type: TYPE_NORMAL
- en: The reason the buffers must be unmapped before they can be accessed by the CPU
    is because of the cache. It goes without saying that CPU mapping is cacheable.
    The `dma_map_*()` family functions (actually wrapped by `pci_map_*()` functions),
    which are used for streaming mappings, will first clean/invalidate the caches
    related to the buffers and will rely on the CPU not to access those buffers until
    the corresponding `dma_unmap_*()` (wrapped by `pci_unmap_*()` functions). Those
    unmappings will then invalidate (if necessary) the caches again, in case of any
    speculative fetches in the meantime, before the CPU may read any data written
    to memory by the device. Only at this time can the CPU access the buffers.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are streaming mappings that can accept several non-contiguous and scattered
    buffers. We can then enumerate two forms of streaming mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: Single buffer mapping, which allows only single-page mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scatter/gather mapping, which allows the passing of several buffers (scattered
    over memory)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of them is introduced in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Single buffer mapping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This consists of mapping a single buffer. It is for occasional mapping. That
    being said, you can set up a single buffer with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '`direction` should be either `PCI_DMA_BIDIRECTION`, `PCI_DMA_TODEVICE`, `PCI_DMA_FROMDEVICE`,
    `or PCI_DMA_NONE`. `ptr` is the kernel virtual address of the buffer, and `dma_addr_t`
    is the returned bus address that can be sent to the device. You should make sure
    to use the direction that really matches the way data is intended to move, not
    just always `DMA_BIDIRECTIONAL`. `pci_map_single()` is a dumb wrapper of `dma_map_single()`,
    with the directions mapping to `DMA_TO_DEVICE`, `DMA_FROM_DEVICE`, or `DMA_BIDIRECTIONAL`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should free the mapping with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This is a wrapper around `dma_unmap_single()`. `dma_addr` should be the same
    as the one returned by `pci_map_single()` (or the one returned by `dma_map_single()`
    in case you used it). `direction` and `size` should match what you have specified
    in the mapping.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows a simplified example of streaming mapping (actually a single
    buffer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, `buffer` is supposed to be already allocated and
    to contain the data. It is then mapped, its bus address is sent to the device,
    and the DMA operation is started. The next code sample (implemented as an interrupt
    handler for the DMA transaction) demonstrates how to deal with the buffer from
    the CPU side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding, the mapping is released before the CPU can play with the buffer.
  prefs: []
  type: TYPE_NORMAL
- en: Scatter/gather mapping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scatter/gather mapping is the second family of streaming DMA mapping with which
    you can transfer several (not necessarily physically contiguous) buffer regions
    in a single shot instead of mapping each buffer individually and transferring
    them one by one. In order to set up a `scatterlist` mapping, you should allocate
    your scattered buffers first, which must be of page size, except the last one,
    which may have a different size. After this, you should allocate an array of `scatterlist`
    and fill it with the previously allocated buffers using `sg_set_buf()`. Finally,
    you must call `dma_map_sg()` on the `scatterlist` array. Once done with DMA, call
    `dma_unmap_sg()` on the array to unmap the `scatterlist` entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'While you can send contents of several buffers over DMA one by one by mapping
    each one of them, scatter/gather can send them all at once by sending the pointer
    to `scatterlist` to the device, along with a length, which is the number of entries
    in the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'First of all, do note that `pci_map_sg()` is a dumb wrapper of `dma_map_sg()`.
    In the preceding code, we used `sg_init_table()`, which results in a statically
    allocated table. We could have used `sg_alloc_table()` for a dynamic allocation.
    Moreover, we could have used the `for_each_sg()` macro, in order to loop over
    each `sg` (`sg_set_page()` helper in order to set the page to which this scatterlist
    is bound (you should never assign the page directly). The following is an example
    involving such helpers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding block, pages should have been allocated and should obviously
    be of `PAGE_SIZE` size. `st` is an output parameter that will be set up appropriately
    on the success path of this function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, note that scatterlist entries must be of page size (except the last
    entry, which may have a different size). For each buffer in the input scatterlist,
    `dma_map_sg()` determines the proper bus address to give to the device. The bus
    address and length of each buffer are stored in the struct scatterlist entries,
    but their location in the structure varies from one architecture to another. Thus,
    there are two macros that you can use to make your code portable:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dma_addr_t sg_dma_address(struct scatterlist *sg)`: This returns the bus (DMA)
    address from this scatterlist entry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unsigned int sg_dma_len(struct scatterlist *sg)`: This returns the length
    of this buffer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dma_map_sg()` and `dma_unmap_sg()` take care of cache coherency. However,
    if you have to access (read/write) the data between the DMA transfers, the buffers
    must be synchronized between each transfer in an appropriate manner, by means
    of either `dma_sync_sg_for_cpu()` if the CPU needs to access the buffers, or `dma_sync_sg_for_device()`
    if it is the device that needs access. Similar functions for single region mapping
    are `dma_sync_single_for_cpu()` and `dma_sync_single_for_device()`.'
  prefs: []
  type: TYPE_NORMAL
- en: Given all of the above, we can conclude that coherent mappings are simple to
    code but expensive to use, whereas streaming mappings have the reverse characteristics.
    Streaming mapping is to be used when the I/O device owns the buffer for long durations.
    Streamed DMA is common for asynchronous operations when each DMA operates on a
    different buffer, such as network drivers, where each `skbuf` data is mapped and
    unmapped on the fly. However, the device may have the last word on what method
    you should use. That being said, if you had the choice, you should use streaming
    mapping when you can and coherent mapping when you must.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have dealt with PCI specification buses and implementations,
    as well as its support in the Linux kernel. We went through the enumeration process
    and how the Linux kernel allows different address spaces to be accessed. We then
    followed a detailed step-by-step guide on how to write a PCI device driver, from
    the device table population to the module's `exit` method. We took a deeper look
    at the interrupt mechanisms and their underlying behaviors as well as the differences
    between them. Now you are able to write a PCI device driver on your own, and you
    are familiar with their enumeration process. Moreover, you understand their interrupt
    mechanisms and are aware of the differences between them (MSI or not). Finally,
    you learned how to access their respective memory regions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will deal with the NVMEM framework, which helps to develop
    drivers for non-volatile storage devices such as EEPROM. This will serve to end
    the complexity that we have experienced so far while learning about PCI device
    drivers.
  prefs: []
  type: TYPE_NORMAL
