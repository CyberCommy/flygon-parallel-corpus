- en: GPU Programming Using OpenACC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every processor architecture provides different approaches to writing code to
    run on the processor. CUDA is no exception; it also provides different approaches
    to coding. One such approach, which has become very popular in recent years, is
    making use of OpenACC, which fundamentally is directive-based programming.
  prefs: []
  type: TYPE_NORMAL
- en: OpenACC is basically a standard which exposes heterogeneous computing as a first-class
    citizen. The standard fundamentally dictates that there are two kinds of processor,
    that is, a host and a device/accelerator, which is very similar to the concepts
    that the CUDA programming model states.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA programming, using languages such as C, C++, Fortran, and Python, is the
    preferred way to express parallelism for programmers who want to get the best
    performance. Programming languages require a programmer to recreate their sequential
    program from scratch, while maintaining both serial and parallel versions of their
    key operations. Programmers can micromanage everything about their program and
    often use device-specific features, which are too specific for higher-level approaches,
    to achieve the best performance. Parallel programs created in a parallel programming
    language tend to only work on a very small number of platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Compiler directives blend the flexibility of programming languages with the
    easy use of libraries. The programmer annotates the code with high-level instructions
    that a compiler can use to parallelize the code, or can safely ignore. This means
    that code with compiler directives can be compiled for many different parallel
    platforms, and there’s no need to maintain separate serial and parallel versions
    of the code. Also, there is sometimes a need to quickly test and prototype an
    application to run on a GPU. One such example is converting code bases such as
    weather code, which has millions of lines of code, to run on a GPU; doing this
    using popular languages will take a lot of effort. In such a scenario, OpenACC
    becomes a logical choice. In OpenACC the developers provide hints to the compiler
    in the form of directives. The compiler takes these hints and generates an architecture-specific
    accelerator code.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenACC standard also provides vendor neutrality to the developers of code.
    Single-source code with OpenACC directives can be recompiled for different devices.
    For example, the PGI compiler currently supports OpenACC backends such as Intel
    CPU multi-core, NVIDIA GPU, Intel Xeon Phi, and **F****ield-Programmable Gate
    Array** (**FPGA**) / **Application Specific Integrated Circuit** (**ASIC**) architectures.
    This is a really attractive proposition to developers who want to write vendor-neutral
    code. Key applications in **high-processing computing** (**HPC**) such as **Vienna
    Ab-initio Simulation Package** (**VASP**) (molecular dynamics/quantum chemistry), **Weather
    Research and Forecasting** (**WRF**), and ANSYS Fluent **Computational Fluid Dynamics**
    (**CFD**) make use of the OpenACC programming model to target the NVIDIA GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize the key takeaways for OpenACC:'
  prefs: []
  type: TYPE_NORMAL
- en: The OpenACC standard was developed when heterogeneous computing was considered
    to be the new programming model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenACC provides performance portability across various accelerators.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenACC is not an alternative to the CUDA programming language. When the targeted
    processor is chosen as NVIDIA, the OpenACC compilers generate CUDA code behind
    the scenes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In recent years, the OpenMP standard has also started incorporating heterogeneous
    computing APIs. But to date, there is no compiler that supports different processor
    architectures, and so we have chosen with stick to OpenACC in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenACC directives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asynchronous programming in OpenACC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional important directives and clauses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Linux/Windows PC with a modern NVIDIA GPU (Pascal architecture onward) is
    required for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the introduction, OpenACC is a standard and this standard is
    implemented by different compilers such as the GCC, PGI, and CRAY compilers. The
    compiler that we will be using for this chapter is PGI. The PGI compiler has been
    really popular in the Fortran community and has always been ahead of the curve
    in implementing the OpenACC latest specifications, and it provides a community
    edition, which can be downloaded from the PGI website for free. The good part
    is that fundamentally there is no change in functionality between the community
    edition and a paid-for version of the PGI compiler. For this chapter, you will
    be required to download the PGI community edition.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter''s code is also available on GitHub at: [https://github.com/PacktPublishing/Learn-CUDA-Programming](https://github.com/PacktPublishing/Learn-CUDA-Programming).'
  prefs: []
  type: TYPE_NORMAL
- en: Sample code examples are developed and tested with version 19.4 of the PGI community
    edition. But it is recommended you use the latest PGI version.
  prefs: []
  type: TYPE_NORMAL
- en: Image merging on a GPU using OpenACC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to understand the OpenACC concept, we have chosen a simple computer
    vision algorithm for merging two images. Fundamentally in this code, we are trying
    to merge two images, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18bb51b7-333b-4c18-8f05-de66809037a2.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding image demonstrates a computer vision algorithm merging two images.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will talk more about the code structure later in the chapter. To start, configure
    the environment according to the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare your GPU application. As an example, we will use a kernel algorithm
    for merging two images. This code can be found at `09_openacc/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compile your application with the `pgc++` compiler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding commands will create a binary named `blurring.out`. As you might
    have observed we are using the `pgc++` compiler to compile our code. Also, we
    pass a few arguments to our code. Let''s understand them in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-acc`: This flag tells the compiler to parse the OpenACC directives provided
    in the code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-ta`: Stands for the target architecture that the device code should be generated
    for. Note that `-ta=tesla` means we are targeting a NVIDIA GPU. Some examples
    of other targets include `-ta=multi-core`, which targets multi-core as the device,
    `-ta=radeaon`, which targets AMD GPUs, and a few others. Additionally, we can
    add device-specific flags; for example, we added a pinned flag to a GPU that allocates
    all CPU memory as pinned (non-pageable).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-Minfo`: This option tells the compiler to provide us with more information
    about steps taken by the compiler to make our code parallel. By saying `-Minfo-accel`,
    we are asking the compiler to provide us with more information related to the
    accelerator region only. We can change the flag to `-Minfo=all` to provide details
    of the non-accelerator region also. The following output shows part of the output
    of adding a `Minfo` flag to our code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To understand this compilation output, we need to understand OpenACC pragmas,
    which we will do in the next section. We will revisit this compilation output
    later. Further details on other available flags can be found using `pgc++ --help`*.*
  prefs: []
  type: TYPE_NORMAL
- en: 'The sample output after running the binary is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding output shows that we are reading an image of size 1536*2048\.
    The code has one serial implementation and three parallel implementations using
    OpenACC pragmas. The timings of each of the implementations are shown in the preceding
    output. The last implementation with the pipeline approach shows the best timing:
    `0.0008 seconds`. We will take an incremental approach and go into the details
    of each implementation in the next sections.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The serial implementation of this algorithm is very simple, and shown in the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: There is nothing fancy about the code; basically, it takes two input image data
    (`in1` and `in2`), performs the average operation to merge both inputs, and finally
    stores the output. The key thing for us, with respect to parallelism, is that
    the loop is embarrassingly parallel and suitable for architecture such as GPUs.
    As shown in the preceding code output, serial implementation took `0.0028` seconds.
    Please note that the timings may vary slightly, based on the system that you are
    running the code on.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce you to the OpenACC directives necessary
    to convert the sample code to run on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: OpenACC directives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will try to understand the syntax of OpenACC pragmas, and
    implement basic parallel and data directives for the merge operation. The basic
    syntax of the OpenACC pragma is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command is explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`#pragma` in C/C++ is what''s known as a "compiler hint." These are very similar
    to programmer comments; however, the compiler will actually read our pragmas.
    If the compiler does not understand the pragma, it can ignore it, rather than
    throw a syntax error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`acc` is an addition to our pragma. It specifies that this is an OpenACC pragma.
    Any non-OpenACC compiler will ignore this pragma.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`directive` is a command in OpenACC that will tell the compiler to perform
    some operation. For now, we will only use directives that allow the compiler to
    parallelize our code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clauses` are additions/alterations to our directives. These include, but are
    not limited to, optimizations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are three directives we will cover in this section: *parallel*, *loop,*
    and *data*. We will showcase each of them and finally apply them to our merge
    algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Parallel and loop directives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The parallel directive is the most straightforward of the directives. It will
    mark a region of the code for parallelization (this usually only involves parallelizing
    a single `for` loop), as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We may also define a parallel region. The parallel region may have multiple
    loops (though this is often not recommended!). The parallel region is everything
    contained within the outer most curly braces, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'It is extremely important to include the loop; otherwise, you will not be parallelizing
    the loop properly. The parallel directive tells the compiler to parallelize the
    code redundantly, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ed25634-1b03-4d94-b6e8-f63049177ff5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The loop directive specifically tells the compiler that we want the loop parallelized,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03886faa-19f3-4ad8-9401-a3ebd22606d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The loop directive has two major uses:'
  prefs: []
  type: TYPE_NORMAL
- en: To mark a single loop for parallelization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To allow us to explicitly define optimizations/alterations for the loop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will cover loop optimizations later on in the chapter, along with gang and
    vector; for now, we will focus on the parallelization aspect. For the loop directive
    to work properly, it must be contained within the parallel directive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When using the parallel directive, you must include the loop directive for
    the code to function properly. We may also use the loop directive to parallelize
    multidimensional loop nests. In the following code snippet, we see a nested loop
    and we mention the loop clause explicitly for the second loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that, in the preceding code snippet, we do not put the parallel clause
    again in the inner loop as we have already mentioned it in the scope that starts
    from the outer loop.
  prefs: []
  type: TYPE_NORMAL
- en: Data directive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The OpenACC parallel model states that we have a host, which runs our sequential
    code (mostly it would be a CPU). Then we have our device, which is some sort of
    parallel hardware. The host and device usually (though not always) have separate
    memories, and the programmer can use OpenACC to move data between the two memories.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the first chapter, the GPU and CPU architectures are fundamentally
    different. The GPU, being a throughput-based architecture, has a high number of
    computational units along with high-speed memory bandwidth. The CPU, on the other
    hand, is a latency-reducing architecture, has a large cache hierarchy, and also
    provides a large main memory. Any data that needs to be operated on needs to be
    first copied to the GPU memory. (Note that even in the case of unified memory
    the data gets copied behind the scenes in the form of pages by the driver.)
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in the following diagram, the data transfer between the two
    architectures (CPU and GPU) happens via an I/O bus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac932965-3f84-440d-be0a-1f8428eb7e48.png)'
  prefs: []
  type: TYPE_IMG
- en: Our goal when using a GPU as the target architecture in OpenACC is to only use
    it to offload our parallel code, and the sequential code will continue to run
    on our CPU. The OpenACC standard allows the programmer to explicitly define data
    management by using the OpenACC **data directive and data clauses**. Data clauses
    allow the programmer to specify data transfers between the host and device (or,
    in our case, the CPU and the GPU).
  prefs: []
  type: TYPE_NORMAL
- en: '**I****mplicit data management**: We can leave the transfer of data to the
    compiler as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the compiler will understand that the `A` vector needs
    to be copied from the GPU, and generate an implicit transfer for the developer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Explicit data management**: It is good practice to make use of explicit data
    transfers to gain more control over the transfers, as shown in the following code
    where we are using the copy data clause:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet we make use of the copy data clause. The following
    diagram explains the steps executed when runtime reached the copy data directive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52965e3f-256c-4efd-925f-94fd52a84cd5.png)'
  prefs: []
  type: TYPE_IMG
- en: We will go into the details of these steps with the help of the merge code where
    we will be applying the data clauses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other available data clauses are as listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Data clause** | **Description** | **Key usage** |'
  prefs: []
  type: TYPE_TB
- en: '| `copy(list)` |'
  prefs: []
  type: TYPE_TB
- en: Allocates memory on the device
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copies data from the host to the device when entering the region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copies data to the host when exiting the region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| This is the default for input data structures that are modified and then
    returned from function |'
  prefs: []
  type: TYPE_TB
- en: '| `copyin(list)` |'
  prefs: []
  type: TYPE_TB
- en: Allocates memory on the device
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copies data from the host to the device when entering the region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Vectors that are just input to a subroutine |'
  prefs: []
  type: TYPE_TB
- en: '| `copyout(list)` |'
  prefs: []
  type: TYPE_TB
- en: Allocates memory on the device
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copies data to the host when exiting the region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| A result that doesn''t overwrite the input data structure |'
  prefs: []
  type: TYPE_TB
- en: '| `create(list)` |'
  prefs: []
  type: TYPE_TB
- en: Only allocates memory on the device
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No copy is made
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Temporary arrays |'
  prefs: []
  type: TYPE_TB
- en: To maximize performance, the programmer should avoid all unnecessary data transfers,
    and hence explicit memory management is preferred over implicit data management.
  prefs: []
  type: TYPE_NORMAL
- en: '**Array shaping: **Array shaping is how you specify the size of the array.
    If you do not specify a shape, then the compiler will try to assume the size.
    This works well in Fortran, since Fortran tracks the size of the array; however,
    it will most likely not work in C/C++. Array shaping is also the only way to copy
    a portion of data from the array (for example, if you only need to copy half of
    the array, this can be a performance boost, cutting out unnecessary copies), as
    shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This would copy all of the elements of `A` except for the first and last elements.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the parallel, loop, and data directive to merge image code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now try to apply the parallel, loop, and data directive to the merge
    sequential code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We have made both the loops (height: `y` and width: `x`) parallel using the
    parallel loop directive. Also, we have explicitly added data clauses to copy the
    data. Note that, since the `in1` and `in2` vectors are input only, they are copied
    using the `copyin()` data clause. The `out` vector is the output and is copied
    using the `copyout()` data clause. Let''s try to understand the compiler output
    for this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding compiler output shows that for the `merge_parallel_pragma` function
    the following actions have been generated by the compiler:'
  prefs: []
  type: TYPE_NORMAL
- en: At line 30, `copyin` was generated for the `in1` and `in2 `variables. The array
    size copied to the GPU before the kernel launch will be `[0:w*h]`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At line 30, `copyout` was generated for the `out` variable. The array size that
    will be copied after the GPU kernel launch will be `[0:w*h]`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At lines 30 and 32, Tesla kernel code was generated:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At line 30, the outer loop was parallelized with gang-level parallelism.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At line 32, the inner loop was parallelized with vector-level parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When the code is run on V100, the time taken by this whole kernel is `0.0010s`.
    This is basically approximately twice as fast as the serial code. This may not
    sound impressive. The reason for that is that most time is spent on data transfers
    rather than kernel computation. In order to confirm this, let''s make use of `nvprof`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can observe in the preceding profiling output, 94% of the time is spent
    on data transfers while only 5% of the time (45 microseconds) is spent on kernel
    execution. The query you might have is: How do I know which kernel this is? If
    you look closely at the name of the GPU kernel, `merge_parallel_pragma_30_gpu`,
    the PGI compiler generated a CUDA kernel in the `merge_parallel_pragma` function at
    line 30, and that is how we can relate it back to the pragmas that put in a function
    at that line number.'
  prefs: []
  type: TYPE_NORMAL
- en: So we know the problem, but what about the solution? The optimization technique
    that we will use to hide this latency is blocking. We will cover more about the
    blocking technique, and using the asynchronous clause to overlap this transfer,
    in upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming in OpenACC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to achieve better performance for merging parallel code, we will make
    use of a concept called blocking. Blocking basically means that, rather than transferring
    the whole input and output arrays in one shot, we can create blocks of the array
    which can be transferred and operated in parallel. The following diagram demonstrates
    creating blocks and overlapping data transfers with the kernel execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5521327f-6b8b-4e44-b04c-abbd69131c84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram shows that different blocks are transferred and the kernel
    execution of these blocks can be independent of each block. In order for this
    to happen, we need the data transfer commands and kernel calls to be fired and
    executed asynchronously. In order to achieve blocking, we will be introducing
    more directives/clauses in this section: the structured/unstructured data directive
    and `async` clause. We will showcase each of them and finally apply them to our
    basic OpenACC merge parallel code.'
  prefs: []
  type: TYPE_NORMAL
- en: Structured data directive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The OpenACC data directives allow the programmer to explicitly manage the data
    on the device (in our case, the GPU). The following code snippet shows an example
    of marking a structured data region:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Device memory allocation happens at the beginning of the region, and device
    memory deallocation happens at the end of the region. Additionally, any data movement
    from the host to the device (CPU to GPU) happens at the beginning of the region,
    and any data movement from the device to the host (GPU to CPU) happens at the
    end of the region. Memory allocation/deallocation and data movement are defined
    by which clauses the programmer includes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Encompassing multiple compute regions: **A single data region can contain
    any number of parallel/kernels regions, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Unstructured data directive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two unstructured data directives:'
  prefs: []
  type: TYPE_NORMAL
- en: '**enter data**: Handles device memory allocation, and copies from the host
    to the device. The two clauses that you may use with enter data are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create`: This will only perform device memory allocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`copyin`: This will perform allocation along with a memory copy to the device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**exit data**: Handles device memory deallocation, and copies from the device
    to the host. The two clauses that you may use with exit data are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delete`: This will perform only device memory deallocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`copyout`: This will first do a memory copy from the device to the host, followed
    by device memory deallocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unstructured data directives do not mark a data region as you are able to have
    multiple enter data and exit data directives in your code. It is better to think
    of them purely as memory allocation and deallocation. The largest advantage of
    using unstructured data directives is their ability to branch across multiple
    functions. You may allocate your data in one function, and deallocate it in another.
    We can look at a simple example of that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet shows that allocation happens in the separate `allocate()`
    function, and deletion happens in `deallocate()`. You can link the same concept
    to `enter data create` as part of the constructor and `exit data delete` as part
    of the destructor in C++.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming in OpenACC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By default, all OpenACC calls are synchronous in nature. Which means that,
    after every call to a data transfer or every kernel call to the GPU, a synchronization
    gets added implicitly. The CPU will wait till the OpenACC call has finished and
    then start executing the next instruction. To make the call asynchronous, we can
    make use of the `async` clause along with the data and parallel directive, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The primary benefits of using `async` can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If we want to execute host and device code simultaneously, we can launch our
    device code with `async`, and while that executes we can go back to the host to
    continue unrelated (non-device dependent) code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can *queue up* multiple device kernel launches so that they execute back
    to back, which in some cases can reduce the overhead associated with launching
    device kernels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can perform device computation at the same time as data movement between
    host and device**.** This is the optimization we will be applying to our code,
    and is the most general use case of `async`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the hood, whenever we use the `async` clause, we are adding some *work* to
    a queue. Work that is submitted to different queues can execute *asynchronously*,
    and work that is in the same queue will execute *sequentially* (one after the
    other). When we use `async`, we are able to specify a queue number. If no queue
    number is specified, then a default will automatically be used.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the unstructured data and async directives to merge image code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now try to apply data directives along with the `async` clause to merge
    parallel code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We have made use of data directives and also an `async` clause to implement
    the blocking concept. Let''s break down the overall implementation, which will
    make it simpler to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enter data region**: The `enter data create` clause allocates memory for
    the `in1` and `in2` variables and `out` in the GPU.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Creates blocks**: We decided that we will split the image into eight blocks.
    The blocks are split across rows. The outer `for` loop for the block gets added
    for this reason.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Transfer data from host to device asynchronously**: `acc update device` basically
    copies data from the host to the device asynchronously as we have added an `async`
    clause to the same.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Launch parallel loop** **asynchronously**: The `async` clause is added to
    the parallel clause to launch the GPU kernel asynchronously.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Transfer data from device to host asynchronously**: `acc update self` basically
    copies the data from the device to the host asynchronously as we have added an
    `async` clause to the same.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Wait**: `acc wait` will make sure the CPU waits till all the OpenACC launches
    have finished, prior to moving forward in all the queues.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Exit data region**: `acc exit data delete` will delete the `in1` and `in2`
    vectors and `out`, which were allocated in the `enter data` clause.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s try to understand the compiler output of the `merge_async_pipelined `function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding compiler output shows that, for the `merge_async_pipelined` function,
    the following actions have been generated by the compiler:'
  prefs: []
  type: TYPE_NORMAL
- en: At line `67` , the `data create` region has been generated for the `in1`, `in2`
    and `out` variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At line `74` , `update device` is called for `in1` and `in2`, and the transfer
    of data to the device is restricted to block the upper and lower bounds: `in1[w*lower:w*(upper-lower)],in2[w*lower:w*(upper-lower)]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At lines `74` and `76` , the Tesla kernel code has been generated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At line `81`, `update self` is called for the `out` variable, and the transfer
    of data from the device is restricted to block the upper and lower bounds: `out[w*lower:w*(upper-lower)]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At line `84`, the data region ends, and `delete` is called to free up the memory
    allocated on the GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When the code is run on V100, the time taken by this whole kernel is 0.0008
    seconds. To understand this in more detail, let''s go back to the profiler. This
    time we will visualize the output by making use of the NVIDIA Visual Profiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d3b6f80-eaf4-4292-ac0f-e1321f6907bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Output by using NVIDIA Visual Profiler
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshot showstheVisual Profiler output after using `async`
    and blocking. The key message from the profiler window is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We see three streams being created and used. This is because our code uses `async(block%2)`,
    which means that we have requested max `2` queues. The third queue is the default
    queue and is not used during the pipeline execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We see that the host-to-device and device-to-host transfer also overlaps as
    the GPU has two **Direct Memory Access** (**DMA**) engines, and hence the data
    transfer in the opposite direction can be overlapped.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also see that our kernel execution overlaps with the data transfer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So far we have seen key directives that helped us to make sequential code for image
    merging to run on a GPU. In the next section, we will introduce you to more clauses
    which will help you to optimize your OpenACC code further.
  prefs: []
  type: TYPE_NORMAL
- en: Additional important directives and clauses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover other important, widely used directives that
    we can apply to our merge algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Gang/vector/worker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gang/worker/vector defines the various levels of parallelism we can achieve
    with OpenACC. This parallelism is most useful when parallelizing multi-dimensional
    loop nests. OpenACC allows us to define a generic gang/worker/vector model that
    will be applicable to a variety of hardware, but we will focus more on a GPU-specific
    implementation. The following diagram shows an OpenACC parallel programming model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd25f916-e4c0-4563-8add-eda78ea46a13.png)'
  prefs: []
  type: TYPE_IMG
- en: This preceding diagram represents a single gang. When parallelizing our `for`
    loops, the loop iterations will be broken up evenly among a number of gangs. Each
    gang will contain a number of threads. These threads are organized into blocks.
    A worker is a row of threads.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, there are three workers, which means that there are
    three rows of threads. The vector refers to how long each row is. So in the preceding
    graphic, the vector is eight, because each row is eight threads long. By default,
    when programming for a GPU, gang and vector parallelism is automatically applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'As OpenACC is an open standard and targets multiple hardware; it provides generic
    constructs. But how does this construct get mapped to a particular target device?
    The answer is simple; it depends on the architecture and compiler, and hence providing
    performance portability. If we were to map how the current PGI compiler maps this
    concept to CUDA (NVIDIA GPU), it would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The OpenACC gang maps to a CUDA block.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The worker essentially maps to a CUDA warp.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenACC vector maps to `threadIdx.x` and (X dimension).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenACC worker maps to `threadIdx.y` (Y dimension).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Again it is important to reiterate that this is how the PGI compiler maps the
    OpenACC constructs. Other compilers might map this differently. Specifically for
    NVIDIA GPUs, the gang worker vector will define the organization of our GPU threads.
    By adding the following clauses, the developer can tell the compiler which levels
    of parallelism to use on given loops:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gang`: Marks the loop for gang parallelism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`worker`: Marks the loop for worker parallelism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vector`: Marks the loop for vector parallelism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code snippet has three loops, and each loop parallelism has been
    explicitly defined: the outer loop as `gang`, the middle loop as the `worker`
    loop, and the innermost loop as the `vector` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**Adjusting gangs, workers, and vectors: **The compiler will choose a number
    of gangs and workers and a vector length for you, but you can change it with the
    following clauses:'
  prefs: []
  type: TYPE_NORMAL
- en: '`num_gangs(N)`: Generates the `N` gangs for the parallel region'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers(M)`: Generates `M` workers for the parallel region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vector_length(Q)`: Uses a vector length of `Q` for the parallel region'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For an example in the following code snippet we have set the number of gangs
    to `2`, the number of workers to `2` and the vector length to `32`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: It is rarely a good idea to set the number of gangs in your code—let the compiler
    decide. Most of the time you can effectively tune a loop nest by adjusting only
    the vector length. Also, it is rare to use a worker loop for the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Managed memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenACC provides an option to allow the compiler to handle memory management.
    We will be able to achieve better performance by managing memory ourselves; however,
    allowing the compiler to use the managed memory is very simple. We do not need
    to make any changes to our code to get the managed memory working.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to make use of the managed memory, we can pass the managed flag to
    the `pgc++` compiler like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: After adding the managed clause, the compiler will basically ignore the data
    clauses, and the managed memory is used to transfer data between the CPU and GPU.
    Note that the managed memory is only for heap data and not stack/static data.
    The unified memory concept that we covered in the previous chapter will remain
    the same.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel directive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The kernel directive allows the programmer to step back and rely solely on
    the compiler. Some sample code using a kernel directive is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Just like in the parallel directive example, we are parallelizing a single loop.
    Recall that, when using the parallel directive, it must always be paired with
    the loop directive; otherwise, the code will be improperly parallelized. The kernel
    directive does not follow the same rule; in some compilers, adding the loop directive
    may limit the compiler's ability to optimize the code.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel directive is the exact opposite of the parallel directive. This means
    that the compiler is making a lot of assumptions, and may even override the programmer's
    decision to parallelize the code. Also, by default, the compiler will attempt
    to optimize the loop. The compiler is generally pretty good at optimizing loops,
    and sometimes may be able to optimize the loop in a way that the programmer cannot
    describe. However, usually programmers will be able to achieve better performance
    by optimizing the loop themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run into a situation where the compiler refuses to parallelize a loop,
    you may override the compiler''s decision. (However, keep in mind that by overriding
    the compiler''s decision, you are taking responsibility for any mistakes that
    occur from parallelizing the code!) In this code segment, we are using the independent
    clause to assure the compiler that we think the loop is parallelizable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the most telling advantages of the kernel directive is its ability to
    parallelize many loops at once. For example, in the following code segment, we
    are able to effectively parallelize two loops at once by utilizing a kernel region:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Collapse clause
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **collapse clause** allows us to transform multi-dimensional loop nests
    into a single-dimensional loop. This process is helpful for increasing the overall
    length (which usually increases parallelism) of our loops, and will often help
    with memory locality. Let''s look at the syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The code will combine the three-dimensional loop nest into a single one-dimensional
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: Tile clause
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **tile clause** allows us to break up a multi-dimensional loop into *tiles*,
    or *blocks*. This is often useful for increasing memory locality in some code.
    Let''s look at the syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will break our loop iterations up into 32 x 32 tiles (or
    blocks), and then execute those blocks in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA interoperability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier in the chapter, OpenACC is not an alternative to CUDA languages;
    in fact, developers can start making use of OpenACC to port hotspots to a GPU.
    They can start integrating CUDA kernels for the most critical function only. There
    are several ways to turn an OpenACC/CUDA into interoperable code. We will look
    at some of them in this section.
  prefs: []
  type: TYPE_NORMAL
- en: DevicePtr clause
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This clause can be used to map the CUDA device pointer allocated using `cudaMalloc`
    and pass it to OpenACC. The following code snippet shows the use of the `deviceptr` clause:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Normally, the OpenACC runtime expects to be given a host pointer, which will
    then be translated to some associated device pointer. The `deviceptr` clause is
    a way to tell the OpenACC runtime that a given pointer should not be translated
    since it is already a device pointer.
  prefs: []
  type: TYPE_NORMAL
- en: Routine directive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last topic to discuss is using CUDA device functions within OpenACC parallel
    and kernel regions. These are functions that are compiled to be called by the
    accelerator within a GPU kernel or OpenACC region. To use CUDA `__device__` functions
    within our OpenACC loops, we can also use the routine directive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Please note that this chapter provides a practical approach to making use of
    OpenACC and does not cover the whole standard API. For extensive API information,
    see [https://www.openacc.org/.](https://www.openacc.org/)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we provided you with an alternative approach to making use
    of a GPU. This directive-based programming approach using OpenACC is really popular
    for legacy applications, and also for new applications it provides a very easy
    and portable approach. Using this approach, you can see how compilers have become
    more advanced. User feedback on directives has been used by making use of directives
    can generate optimal parallel code for different architectures.
  prefs: []
  type: TYPE_NORMAL
- en: We covered parallel directives that provide an instruction/hint to the compiler
    about which part in the code to make parallel. We also made use of data directives
    to take control of the data transfer instead of relying on managed memory. With
    the use of an asynchronous clause, we also tried optimizing our application by
    overlapping kernels and data transfers. We explored mapping OpenACC constructs
    to the CUDA hierarchy, and also how OpenACC and CUDA C/C++ code can interoperate.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start applying our knowledge of CUDA to deep learning.
  prefs: []
  type: TYPE_NORMAL
