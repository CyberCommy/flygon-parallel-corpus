- en: Protecting Your Web Scraper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have built a web scraper that is capable of autonomously collecting
    information from various websites, there are a few things you should do to make
    sure it operates safely. A number of important measures should be taken to protect
    your web scraper. As you should be aware, nothing on the internet should be fully
    trusted if you do not have complete ownership of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the following tools and techniques you will
    need to ensure your web scraper''s safety:'
  prefs: []
  type: TYPE_NORMAL
- en: Virtual private servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proxies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtual private networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whitelists and blacklists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtual private servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you make an HTTP request for a website, you are making a direct connection
    between your computer and the targeted server. By doing this, you are providing
    them with your machine's public IP address, which can be used to determine your
    general location, and your **Internet Service Provider** (**ISP**). Although this
    can't be tied directly back to your exact location, it could be used maliciously
    if its finds its way into the wrong hands. With this in mind, it is preferable
    to not expose any of your personal assets to untrusted servers.
  prefs: []
  type: TYPE_NORMAL
- en: Running your web scraper on a computer that is far removed from your physical
    location, with some sort of remote access, is a good way to decouple your web
    scraper from your personal computer. You can rent **Virtual Private Server** (**VPS**)
    instances from various providers on the web.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the more notable companies include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon Web Services** (**AWS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft Azure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DigitalOcean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These companies will allow you to create a virtual machine and provide you with
    credentials for accessing the instance. They have various offerings, depending
    on the size of machine you require, and most of them offer some free resources
    if they are below a certain size.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to deploy your web scraping code onto these machines and run
    the program from within your VPS. This book will not cover the packaging and deployment
    of Go applications in detail, but the following are a few techniques for getting
    you started:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Secure Copy Protocol** (**SCP**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ansible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Puppet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By operating the web scraper on a VPS, you will have the peace of mind that,
    should your machine become exposed, it can be safely destroyed without compromising
    your personal computer. Also, running your scraper in a VPS allows you to easily
    scale to suit your needs as you begin scraping more websites. You can spin up
    multiple VPS instances to run your scrapers in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Proxies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The role of a proxy is to provide an additional layer of protection on top of
    your system. At its core, a proxy is a server that sits in between your web scraper
    and the target web server, and passes communication between the two. Your web
    scraper sends a request to the proxy server, which then forwards the request to
    the website. From the point of view of the website, the request only comes from
    the proxy server, without any knowledge of the origin of the request. There are
    many types of proxy available, each with its own pros and cons
  prefs: []
  type: TYPE_NORMAL
- en: Public and shared proxies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some proxies are open to the public to use. However, they can be shared by
    many different people. This jeopardizes your reliability because, if other users
    compromise the proxy through misuse, it could endanger your web scraper. Speed
    is another concern for public proxies: the more traffic flows through a proxy,
    the less bandwidth will be available. On the other hand, these proxies are free
    to use and could be useful during your testing and debugging stages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A few sites listing public proxies are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://free-proxy-list.net](https://free-proxy-list.net)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://hidemyna.me](https://hidemyna.me)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://proxy-list.download](https://proxy-list.download)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the varying success of public proxies that are available, you will need
    to ensure you do your research before trying them out in production. You will
    need to consider whether or not these proxies are reliable, and can reach your
    target website. You'll also need to ensure that your information is being protected
    as you connect through them.
  prefs: []
  type: TYPE_NORMAL
- en: Dedicated proxies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dedicated proxies are a great way to ensure that only you are in control of
    the traffic flowing through the proxy servers. There are many companies that sell
    dedicated proxies both on-demand and in bulk. Some companies worth considering
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Storm proxies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blazing SEO
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghost proxies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oxylabs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a few things to consider when selecting a company.
  prefs: []
  type: TYPE_NORMAL
- en: Price
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pricing models for dedicated proxies vary from company to company. In most cases,
    you pay per IP address that you use and you will be able to use that IP address
    as much as you want. There are a number of companies that have a pool of IP addresses
    and will charge you based on your bandwidth instead. In this pricing model, you
    will need to ensure that you are making the most efficient calls possible.
  prefs: []
  type: TYPE_NORMAL
- en: The cost of a per-IP proxy can range between $1 and $6 per month. Usually, you
    will get larger discounts for buying in bulk. Some companies may also limit your
    bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: Location
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On occasion, the location of the proxy may be important to you. Many proxy companies
    distribute their servers throughout the world to allow for greater coverage. If
    you are scraping websites in different countries, it might make sense for you
    to run your scraper through a proxy located in that country in order to avoid
    firewalls or unusual traffic signatures. Different countries also may have different
    laws about what is permissible through the internet in that country, so you should
    always consult local laws before pursuing this route
  prefs: []
  type: TYPE_NORMAL
- en: Type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two main types of proxy that you should be aware of: residential
    and data center proxies. Residential proxies have IP addresses that are assigned
    by an ISP that is registered in a residential area. The IP addresses are directly
    related to a specific region and many websites can estimate where you are based
    on these IP addresses. This is how Google Analytics knows where web traffic to
    your website is coming from. From a web scraping point of view, it may make a
    difference if the web traffic is coming from San Francisco as opposed to London.
    If your content changes based on your location, you may need residential proxies
    in the right places.'
  prefs: []
  type: TYPE_NORMAL
- en: The second type of proxy is data center proxies. These proxies are assigned
    by ISPs that are related to data centers, such as VSP providers. When you create
    a new virtual machine, the IP address assigned to that machine is most likely
    a data center IP. These addresses may be intentionally blocked by websites to
    prevent access from non-residential visitors.
  prefs: []
  type: TYPE_NORMAL
- en: Anonymity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anonymity should be considered quite high on the list when selecting proxy providers.
    Not all proxies completely hide the originating source of the request when they
    pass data to the target servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transparent proxies provide the target server with information about who you
    are, and should be avoided in most cases. These proxies pass HTTP headers to the
    target server, such as `X-Forwarded-For`: `<your_ip_address>`, to identify the
    originating source of the request, and `Via`: `<proxy_server>` to identify the
    proxy itself.'
  prefs: []
  type: TYPE_NORMAL
- en: Anonymous proxies provide the same headers as transparent proxies, but they
    may provide false information in order to hide your true identity. In this case,
    the target server will be aware that the connection is being made through a proxy,
    but the true source of the request is unknown.
  prefs: []
  type: TYPE_NORMAL
- en: Elite proxies are the highest level of anonymity that you can achieve from a
    proxy. Elite proxies do not forward any information about the originating source,
    nor do they disclose that the request is coming from a proxy. Instead, the request
    appears to the web server to be a normal request originating at the IP address
    of the proxy.
  prefs: []
  type: TYPE_NORMAL
- en: Proxies in Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you've received a list of proxy addresses to use, configuring your Go HTTP
    client is very simple. The Go HTTP client contains an object called a **transport**.
    The transport is responsible for low-level communication with web servers, including
    opening and closing connections, sending and receiving data, and handling HTTP
    1XX response codes. You can set the `Proxy()` method of transport by setting a
    function that accepts an `*http.Request`, and returns the proxy address as a `*url.URL`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of setting a `Proxy()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `GetProxy()` function randomly chooses between the three configured proxies
    and converts the string to a `*url.URL`. By configuring the `http.DefaultTransport.Proxy`
    function, each time the `http.DefaultClient` is used, `GetProxy` will determine
    which random proxy to use. You could also use different proxies for different
    hosts, by inspecting the `*http.Request` and returning your desired proxy based
    on the hostname provided.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual private networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on your need, you may need to connect to a **Virtual Private Network**
    (**VPN**) in order to ensure that all of your web scraping traffic is hidden.
    Where proxies provide a layer of protection by masking the IP address of your
    web scraper, a VPN also masks the data that flows between your scraper and the
    target site through an encrypted tunnel. This will make the content that you are
    scraping invisible to ISPs and anyone else with access to your network.
  prefs: []
  type: TYPE_NORMAL
- en: VPNs are not legal in all countries. Please comply with local laws.
  prefs: []
  type: TYPE_NORMAL
- en: There are many companies that offer VPN access, with costs typically ranging
    from $5 to $15 per month.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some recommended companies are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Vypr VPN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Express VPN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IPVanish VPN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nord VPN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring your web scraper to use the VPN is different from proxies. VPNs
    usually require a specific client to connect your machine to their network, which
    is not done through code. The advantage is that the code you write with your scraper
    will work independently of any network configuration. Unfortunately, you will
    not be able to incorporate on-the-fly changes in your network in code without
    using shell commands.
  prefs: []
  type: TYPE_NORMAL
- en: Follow the instructions supplied by your VPN provider in order to connect to
    a VPN network.
  prefs: []
  type: TYPE_NORMAL
- en: Boundaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you are crawling a website, you may not always know where you will end
    up. Many links in web pages take you to external sites that you may not trust
    as much as your target sites. These linked pages could contain irrelevant information
    or could be used for malicious purposes. It is important to define boundaries
    for your web scraper to safely navigate through unknown sources.
  prefs: []
  type: TYPE_NORMAL
- en: Whitelists
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Whitelisting** domains is a process by means of which you explicitly allow
    your scraper to access certain websites. Any site listed on the whitelist is OK
    for the web scraper to access, whereas any site that is not listed is automatically
    skipped. This is a simple way to ensure that your scraper only accesses pages
    for a small set of specific sites, which helps in the collection of very focused
    information. You can take this even further by only allowing access to paths of
    a website.'
  prefs: []
  type: TYPE_NORMAL
- en: Building a whitelist in Go is fairly simple with the use of the URL and path
    packages. Let's take an example of indexing articles on the Packt Hub site ([https://hub.packtpub.com/](https://hub.packtpub.com/)).
    Many of the articles posted here contain links to external websites for the purpose
    of noting their sources of information. However, if we are only interested in
    finding other articles on Packt Hub, we would whitelist only [hub.packtpub.com](http://hub.packtpub.com)
    URLs.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example article link you may encounter would look something like this: [https://hub.packtpub.com/8-programming-languages-to-learn-in-2019/](https://hub.packtpub.com/8-programming-languages-to-learn-in-2019/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the GoLang URL package, we can look at the hostname to determine whether
    it is a link worth following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You could then verify that this matches by using the `path.Match()` function,
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Blacklists
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contrary to whitelists, **blacklists** define websites where your scraper should
    definitely not venture. Sites that you will want to include here may be places
    that you know do not contain any relevant information, or you are just not interested
    in their content. You might also temporarily blacklist sites that are experiencing
    performance issues, such as a high number of 5XX errors, as discussed in [Chapter
    2](3bb44efb-a981-4717-b7ec-8eb196a1754c.xhtml), *The Request/Response Cycle*.
    You can match your link URLs to their hostname in the same way as in the preceding
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only change that is required is to modify the last `if` block, shown as
    follows, so it runs only if `doesMatch` is false:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reviewed a number of different techniques to ensure that
    we and our web scrapers are protected while browsing the internet. By using VPS,
    we are protecting our personal assets from malicious activity and discoverability
    on the internet. Proxies also help restrict information about the source of internet
    traffic, providing a layer of anonymity. VPNs add an extra layer of security over
    proxies by creating an encrypted tunnel for our data to flow through. Finally,
    creating whitelists and blacklists ensures that your scraper will not venture
    too deep into uncharted and undesirable places.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 7](1f846c4e-a180-4341-bced-ebf1ed109211.xhtml), *Scraping with Concurrency*,
    we will look at how to use concurrency in order to increase the scale of our web
    scraper without the added cost of incorporating extra resources.
  prefs: []
  type: TYPE_NORMAL
