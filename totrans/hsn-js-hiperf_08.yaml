- en: Data Formats - Looking at Different Data Types Other Than JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have almost finished our discussion on server-side JavaScript. One topic
    that seems to fly under the radar, but does come up quite a bit when interfacing
    with other systems or even making things faster, is transmitting data in different
    formats. One of the most common, if not the most common, formats is JSON. JSON
    is quite easily one of the easiest data formats to interface with, especially
    in JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: 'In JavaScript, we do not have to worry about JSON objects that do not match
    a class. If we were utilizing a strongly typed language such as Java (or TypeScript
    for those that are using it), we would have to worry about the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a class that mimics the format of the JSON object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a map structure that keeps nesting based on how many nested objects
    there are.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating on-the-fly classes based on the JSON that we get.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: None of these are necessarily hard, but it can add to speed and complexity when
    we are interfacing with systems that are written in these languages. With other
    data formats, we may get some major speed benefits; not only from possibly smaller
    data transfers, but also from the other languages being able to parse the objects
    more easily. There are even more benefits when we move to a schema-based data
    format, such as versioning, which can make backward compatibility easier.
  prefs: []
  type: TYPE_NORMAL
- en: With all of this in mind, let's go ahead and take a look at JSON and see some
    of the benefits, but also the losses that we get with utilizing it. On top of
    this, we will take a look at a new custom format that we will create for our services
    to transfer data at a hopefully smaller size. After this, we will take a look
    at a schema-less data format such as JSON and, finally, take a look at a schema-based
    format.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter might be lighter than almost all of the other chapters, but it
    is one that will prove useful when developing enterprise applications or interfacing
    with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Using JSON
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding in JSON
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoding in JSON
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A look at data formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In TypeScript, we could just use the `any` type if we wanted to, but that would
    partially defeat the purpose of TypeScript. While we will not be looking at TypeScript
    in this book, it is good to know that it is out there, and it is easy to see how
    developers may run into it when developing backend applications.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following tools are needed to complete this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An editor or IDE, preferably VS Code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An operating system that supports Node.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code found at [https://github.com/PacktPublishing/Hands-On-High-Performance-Web-Development-with-JavaScript/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-High-Performance-Web-Development-with-JavaScript/tree/master/Chapter08).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As stated previously, JSON provides an easy-to-use and easy-to-operate interface
    for sending and receiving messages between services. For those that do not know,
    JSON stands for **JavaScript Object Notation**, which is one of the reasons that
    it interfaces with JavaScript so well. It mimics a lot of the behavior that JavaScript's
    objects do, except for some fundamental types (things such as functions). This
    also makes it quite easy to parse. We could use something such as the built-in `JSON.parse` function
    to turn stringified versions of JSON into objects, or `JSON.stringify` to turn
    one of our objects into its over-the-wire format.
  prefs: []
  type: TYPE_NORMAL
- en: 'So what are some of the disadvantages when utilizing JSON? We first have the
    problem that the format can get very verbose when sending data over the wire.
    Consider an array of objects that have the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This is probably a common sight for those that have worked with contact forms
    or customer information. Now, while we should involve paging of some kind for
    a website, we still might grab `100` or even `500` of these at a time. This could
    easily lead to a huge wire transfer cost. We can simulate this with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: By using this method, we get the byte length of the buffer that would come out
    of stringifying `100` entries for the data that we are sending. We will see that
    it comes out to around 22 KB worth of data. If we up this to `500`, we can deduce
    that it would be around 110 KB worth of data. While this may not seem like a lot
    of data, we could see this type of data being sent to a smartphone, where we want
    to limit the amount of data that we transfer in order that we do not drain the
    battery.
  prefs: []
  type: TYPE_NORMAL
- en: We have not heavily discussed cellular phones and our applications, especially
    on the frontend, but it is something we need to increasingly be conscious of since
    we are becoming more and more of a remote business world. Many users, even if
    there is not a mobile version of the application, will still try to utilize it.
    One personal anecdote is utilizing email services that are meant for desktop applications
    because of some functionality that was not available in the mobile version of
    the application. We always need to be conscious of the amount of data we are transferring,
    but mobile has made that idea become a primary objective.
  prefs: []
  type: TYPE_NORMAL
- en: One way around this is to utilize some type of compression/decompression format.
    One fairly well-known format is `gzip`. This format is quite fast, has no loss
    in data quality (some compression formats have this, such as JPEG), and is ubiquitous
    with web pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and `gzip` this data by utilizing the `zlib` module in Node.js.
    The following code showcases an easy-to-use `gzip` method inside of `zlib`, and
    showcases the size difference between the original and the gzipped version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We will now see that the gzipped version is only 301 bytes and, for the 500-length
    array, we see a gzipped version of around 645 bytes. That is quite a saving! However,
    there are a couple of points to remember here. First, we are using the exact same
    object for each item in the array. Compression algorithms are based on patterns,
    so seeing the exact same object over and over again is giving us a false sense
    of the original to the compressed form. This does not mean that this is not indicative
    of the size differences between uncompressed versus compressed data, but it is
    something to keep in mind when testing out various formats. Based on various sites,
    a compression ratio of 4-10 times the original is what we would see (this means
    if the original was 1 MB, we would see a compression size of anywhere from 250
    KB to 100 KB).
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of utilizing JSON, we can create a format ourselves that should represent
    the data in a much more compact way. First, we are going to only support three
    item types: a whole number, a floating number, and strings. Second, we will store
    all of the keys in the header of the message.'
  prefs: []
  type: TYPE_NORMAL
- en: A schema can be best described as the definition of the data that is coming
    through. This means that we will know how to interpret the data that is coming
    through and not have to look for special encoding symbols to tell us when the
    end of the payload is (even though our format will use an end-of-body signal).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our schema is going to look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We will use wrapper bytes for both the header and body of the message. The header
    will be denoted with the `0x10` byte and the body with the `0x11` byte.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will support the following types and their conversions will look similar
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Whole number: `0x01` followed by a 32-bit integer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Floating-point number: `0x02` followed by a 32-bit integer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'String: `0x03` followed by the length of the string followed by the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This should be good enough to get the concept of data formats and how they might
    work differently than just encoding and decoding JSON. In the next two sections,
    we will see how we might implement an encoder and decoder utilizing streams.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to implement both the encoder and decoder utilizing transform
    streams. This will give us the most flexibility in terms of actually implementing
    the streams, and it already has a lot of the behavior that we need since we are
    technically transforming the data. First, we will need some generic helpers for
    both the encoding and decoding of our specific data types, and we will put all
    of these methods in a `helpers.js` helper file. The encoding functions will look
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Encoding the string takes in the string and outputs the buffer that will hold
    the information for the decoder to work on. First, we will change the string to
    the `Buffer` format. Next, we create a buffer to hold the length of the string.
    Then, we store the length of the buffer utilizing the `writeUInt32BE` method.
  prefs: []
  type: TYPE_NORMAL
- en: For those that do not know byte/bit conversions, 8 bits of information (a bit
    is either a 1 or 0- the lowest form of data we can supply) makes up 1 byte. A
    32-bit integer, what we are trying to write, is then made up of 4 bytes (32/8).
    The U portion of that method means it is unsigned. Unsigned means we only want
    positive numbers (lengths can only be 0 or positive in our case). With this information,
    we can see why we allocated 4 bytes for this operation and why we are utilizing
    this specific method. For more information on both the write/read portions for
    buffers, go to [https://nodejs.org/api/buffer.html](https://nodejs.org/api/buffer.html)
    as it explains in depth the buffer operations we have access to. We will only
    explain the operations that we will be utilizing.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the string turned into the buffer format and the length of the
    string, we will write out a buffer that has `type` as the first byte, in our case
    the `0x03` byte; the length of the string, so we know how much of the incoming
    buffer is the string; and then finally, the string itself. This one should be
    the most complicated out of the two helper methods, but from a decoding perspective,
    it should make sense. When we are reading the buffer, we do not know how long
    a string is going to be. Because of this, we need some information in the prefix
    of this type to know how much to actually read. In our case, the `0x03` tells
    us that the type is a string and we know, based on our data type protocol that
    we established previously, that the next 4 bytes will be the length of the string.
    Finally, we can use this information to read so far ahead in the buffer to grab
    the string and decode it back to a string.
  prefs: []
  type: TYPE_NORMAL
- en: The `encodeNumber` method is much easier to understand. First, we check whether
    the rounding of the number equals itself. If it does, then we know that we are
    dealing with a whole number, otherwise, we treat it as a floating-point number.
    For those that are unaware, in most cases, knowing this information does not matter
    too much in JavaScript (though there are certain optimizations that the V8 engine
    utilizes when it knows that it is dealing with whole numbers), but if we want
    to use this data format with other languages, then the difference matters.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we allocated 4 bytes since we are only going to write out 32-bit signed
    integers. Signed means they will support both positive and negative numbers (again,
    we won't go into the big difference between the two, but for those that are curious,
    we actually limit the maximum value we can store in here if we utilize signed
    integers since we have to utilize one of the bits to tell us whether the number
    is negative or not). We then write out the final buffer, which consists of our
    type and then the number in buffer format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, with the helper methods and the following constants in the `helper.js`
    file, proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create our `encoder.js` file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary dependencies and also create the shell of our `SimpleSchemaWriter`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the constructor and make sure that `objectMode` is always turned on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a private `#encode` helper function that will do the underlying data check
    and conversion for us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the main `_transform` function for our `Transform` stream. Details of
    this stream will be explained as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Overall, the `transform` function should look familiar to previous `_transform`
    methods we have implemented, with some exceptions:'
  prefs: []
  type: TYPE_NORMAL
- en: Our first portion of the encoding is wrapping our headers (the keys of the object).
    This means that we need to write out our delineator for headers, which is the
    `0x10` byte.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will run through all of the keys of our object. From here, we will utilize
    the `private` method, `encode`. This method will check the data type of the key
    and return the encoding utilizing one of the helper methods that we discussed
    previously. If it does not get a type that it understands, it will return `null`.
    We will then give back an `Error` since our data protocol does not understand
    the type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we have run through all of the keys, we will write out the `0x10` byte
    again, stating that we are done with the headers, and write out the `0x11` byte
    to tell the decoder that we are starting with the body of our message. (We could
    have utilized the constants from the `helpers.js` file here and we probably should,
    but this should help with understanding the underlying protocol. The decoder will
    utilize these constants to showcase better programming practices.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now run through the values of the object and run them through the same
    encoding system that we did with the headers, and also return an `Error` if we
    do not understand the data type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we are finished with the body, we will push the `0x11` byte again to say
    that we are done with the body. This will be the signal to the decoder to stop
    converting this object and to send out the object it has been converting. We will
    then push all of this data to the `Readable` portion of our `Transform` stream
    and use the callback to say that we are ready to process more data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are some problems with the overall structure of our encoding scheme (we
    shouldn't be using singular bytes for our wrappers since they can easily be misconstrued
    by our encoder and decoder) and we should support more data types, but this should
    give a nice understanding as to how an encoder can be built for more generally
    used data formats.
  prefs: []
  type: TYPE_NORMAL
- en: Right now, we will not be able to test this, other than that it spits out the
    correct encoding, but once we have the decoder up and running, we will be able
    to test to see whether we get the same object on both sides. Let's now take a
    look at the decoder for this system.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The decoder has quite a bit more state to it than the encoder and this is usually
    true of data formats. When dealing with raw bytes, trying to parse the information
    out of it is usually more difficult than writing the data out as that raw format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the helper methods that we will use to decode the data
    types we support:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `decodeString` method showcases how we could handle errors in the case of
    incorrectly formatted data, the `decodeNumber` method does not showcase this.
    For the `decodeString` method, we need to grab the length of the string from the
    buffer and we know this is the second byte of the buffer that would be passed
    in. Based on this, we know we can grab the string by starting at the fifth byte
    in the buffer (the first byte is the one that tells us that this is a string;
    the next four are the length of the string), and we grab everything until we get
    to the length of the string. We then run this buffer through the `toString` method.
  prefs: []
  type: TYPE_NORMAL
- en: '`decodeNumber` is quite simple since we only have to read the 4 bytes after
    the first byte telling us it is a number (again, we should do a check here, but
    we are keeping it simple). This showcases the two main helper methods that we
    need to decode the data types that we support. Next, we will take a look at the
    actual decoder. It will look something like the following.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated previously, the decoding process is a bit more involved. This is
    for a number of reasons, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We are working directly on the bytes so we have to do quite a bit of processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are dealing with a header and body section. If we created a non-schema-based
    system, we may be able to write a decoder without as much state as we have in
    this one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, since we are dealing with the buffers directly, all of the data may not
    come in at once, so we need to handle this case. The encoder does not have to
    worry about this since we are operating the Writable stream in object mode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this in mind, let''s run through the decoding stream:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will set up our decode stream with all of the same types of setup that we
    have done with `Transform` streams in the past. We will set up a few private variables
    to track the state as we move through the decoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we are going to utilize an index throughout the decoding process. We
    are not able to simply read a byte at a time since the decoding process runs through
    the buffer at different speeds (when we read a number, we are reading 5 bytes;
    when we read a string, we read at least 6 bytes). Because of this, a `while` loop
    will be better:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we do a check on the current byte to see whether it is a header or body
    delineation mark. This will let us know whether we are working on the object keys
    or on the object values. If we detect the `headers` flag, we will set the `#inHeaders` Boolean
    stating that we are in the headers. If we are in the body, we have more work to
    do:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Next, the paragraphs that follow will explain the process of getting the headers
    and the values of each JSON object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, we will change our body Boolean to the opposite of what it is currently
    at. Next, if we are going from inside the body to outside of the body, this means
    that we are done with this object. Because of this, we can push out the object
    that we are currently working on and reset all of our internal state variables
    (the temporary object, `#obj`; the temporary set of `#keys` that we get from the
    header; and the `#currKey` to know which key we are working on when we are in
    the body). Once we have this, we can run the callback (we are returning here so
    we don't run through more of our main body). If we do not do this, we will keep
    going through the loop and we will be in a bad state.
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, we have gone through the headers of our payload and have reached
    the values for each object. We will set our private `#keys` variable to the keys
    of the object (since, at this point, the headers should have grabbed all of the
    keys from the headers). We can now start to see the decoding process.
  prefs: []
  type: TYPE_NORMAL
- en: If we are in the headers, we will run our private `#decode` method and not utilize
    the third argument since the default is to run the method as if we are in headers.
    Otherwise, we will run it like we are in the body and pass a third argument to
    state that we are in the body. Also, if we are in the body, we will increment
    our `#currKey` variable.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can take a look at the heart of the decoding process, the `#decode`
    method. We grab the item based on the first byte in the buffer, which will tell
    us which decoding helper method we should run. Then, if we are running this method
    in header mode, we will set a new key for our temporary object, and we will set
    its value to null since that will be filled in once we get to the body. If we
    are in body mode, we will set the value of the key corresponding to the `#currKey`
    index in our `#keys` array that we are looping through once we are in the body.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that code explanation, the basic process that is happening can be summed
    up in a few basic steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to go through the headers and set the object's keys to these values.
    We are temporarily setting the values for each of these keys to null since they
    will be filled in later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we move out of the header section and we go to the body section, we can
    grab all of the keys from the temporary object, and the decode run we do at that
    time should correspond to the key at the current key's index in the array.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we are out of the body, we reset all of the temporary variables for the
    state and send out the corresponding object since we are finished with the decoding
    process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This may seem confusing, but all we are doing is lining up the header at some
    index with the body element at that same index. It would be similar to the following
    code if we wanted to put an array of keys and values together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This code is almost exactly the same as what we were doing with the preceding
    buffer, except we have to work with the raw bytes instead of higher-level items
    such as strings, arrays, and objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'With both the decoder and the encoder finished, we can now run an object through
    our encoder and decoder to see whether we get the same value out. Let''s run the
    following test harness code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll use the following test object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see that we will spit the same object out as we pipe the data through
    the encoder into the decoder. Now, it''s great that we created our own encoding
    and decoding scheme, but how does it hold up to the transfer size compared to
    JSON (since we are trying to do better than just stringifying and parsing)? With
    this payload, we are actually increasing the size! If we think about it, this
    makes sense. We have to add in all of our special encoding items (all of the information
    other than the data such as the `0x10` and `0x11` bytes), but we now start to
    add more numerical items to our list that are quite large. We will see that we
    start to beat the basic `JSON.stringify` and `JSON.parse`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This is happening because stringified numbers are turned into just that, string
    versions of the numbers, so when we get numbers that are larger than 5 bytes,
    we are starting to save on bytes (1 byte for the data type and 4 bytes for the
    32-bit number encoding). With strings, we will never see savings since we are
    always adding an extra 5 bytes of information (1 byte for the data type and 4
    bytes for the length of the string).
  prefs: []
  type: TYPE_NORMAL
- en: In most encoding and decoding schemes, this is the case. The way they handle
    data has trade-offs depending on the type of data that is being passed. In our
    case, if we are sending large, highly numerical data over the wire, our scheme
    will probably work better, but if we are sending strings across, we are not going
    to benefit at all from this encoding and decoding scheme. Keep this thought in
    mind as we take a look at some data formats that are used quite heavily out in
    the wild.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, this encoding and decoding scheme is not meant to be used in actual
    environments as it is riddled with issues. However, it does showcase the underlying
    theme of building out data formats. While most of us will never have to build
    data formats, it is nice to understand what goes on when building them out, and
    where data formats may have to specialize their encoding and decoding schemes
    based on the type of data that they are primarily working with.
  prefs: []
  type: TYPE_NORMAL
- en: A look at data formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have looked at our own data format, let's go ahead and take a look
    at some fairly popular data formats that are currently out there. This is not
    an exhaustive look at these, but more an introduction to data formats and what
    we may find out in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: The first data format that we will look at is a schema-less format. As stated
    previously, schema-based formats either send ahead of time the schema for data,
    or they will send the schema with the data itself. This allows, usually, a more
    compact form of the data to come in, while also making sure both endpoints agree
    on the way the data will be received. The other form is schema-less, where we
    send the data in a new form, but all of the information to decode it is done through
    the specification.
  prefs: []
  type: TYPE_NORMAL
- en: JSON is one of these formats. When we send JSON, we have to encode it and then
    decode it once we are on the other side. Another schema-less data format is XML.
    Both of these should be quite familiar to web developers as we utilize JSON extensively
    and we use a form of XML when putting together our frontends (HTML).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another popular format is `MessagePack` ([https://msgpack.org/index.html](https://msgpack.org/index.html)).
    `MessagePack` is a format that is known for producing smaller payloads than JSON.
    What is also nice about `MessagePack` is the number of languages that have the
    library written natively for them. We will take a look at the Node.js version,
    but just note that this could be used on both the frontend (in the browser) and
    on the server. So let''s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will `npm install` the `what-the-pack` extension by utilizing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have done this, we can start to utilize this library. With the following
    code, we can see how easy it is to utilize this data format over the wire:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: What we see here is a slightly modified version of the example that is on the
    page for `what-the-pack` ([https://www.npmjs.com/package/what-the-pack](https://www.npmjs.com/package/what-the-pack)).
    We import the package and then we initialize the library. One way this library
    is different is that we need to initialize a buffer for the encoding and decoding
    process. This is what the `2**22` is doing in the `initialize` method. We are
    initializing a buffer that is 2 to the power of 22 bytes large. This way, it can
    easily slice the buffer and copy it without having expensive array-based operations.
    Another thing keen observers will note is that the library is not based on streaming.
    They have most likely done this to be compatible between the browser and Node.js.
    Other than these small issues, the overall library works just like we think it
    would.
  prefs: []
  type: TYPE_NORMAL
- en: The first console log shows us that the encoded buffer is 5 bytes less than
    the JSON version. While this does showcase that the library gives us a more compact
    form, it should be noted that there are cases where `MessagePack` may not be smaller
    than the corresponding JSON. It also may run slower than the built-in `JSON.stringify`
    and `JSON.parse` methods. Remember, everything is a trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of schema-less data formats out there and each of them has
    their own tricks to try to make the encoding/decoding time faster and to make
    the over-the-wire data smaller. However, when we are dealing with enterprise systems,
    we will most likely see a schema-based data format being used.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of ways to define a schema but, in our case, we will use
    the proto file format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and create a **proto** file to simulate the `test.json` file
    that we had. The schema could look something like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: What we are declaring here is that this message called `TestData` is going to
    live in a package called `exampleProtobuf`. The package is mainly there to group
    like items (this is heavily utilized in languages such as Java and C#). The syntax
    tells our encoder and decoder that the protocol we are going to use is `proto3`.
    There were other versions of the protocol and this one is the most up-to-date
    stable version.
  prefs: []
  type: TYPE_NORMAL
- en: We then declare a new message called `TestData` that has three entries. One
    will be called `item1` and will be of type `string`, one will be a whole number
    called `item2`, and the final one will be a floating-point number called `item3`.
    We are also giving them IDs as this makes it easier for things such as indexing
    and for self-reference types (also because it is mandatory for `protobuf` to work).
    We will not go into exactly what this does, but note that it can help with the
    encoding and decoding process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can write some code that can use this to create a `TestData` object
    in our code that can specifically handle these messages. This would look like
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Notice that this is similar to most of the code we have seen except for some
    verification and creation processes. First, the library needs to read in the proto
    file that we have and make sure it is actually correct. Next, we create the object
    based on the namespace and name we gave it. Now, we verify our payload and create
    a message from this. We then run it through the encoder specific to this data
    type. Finally, we decode the message and test to make sure that we got the same
    data that we put in.
  prefs: []
  type: TYPE_NORMAL
- en: Two things should be noticeable from this example. First, the data size is quite
    small! This is one advantage that schema-based/protobuf has over schema-less data
    formats. Since we know ahead of time what the types should be, we do not need
    to encode that information into the message itself. Second, we will see that the
    floating-point number did not come back out as 3.3\. This is due to precision
    errors and it is something that we should be on the lookout for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we do not want to read in proto files like this, we could build the
    message in the code like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This should resemble the message that we created in the proto file, but we will
    go over each line to show that it is the same `protobuf` object. We are first
    creating a new type called `TestType` in this case (instead of `TestData`). Next,
    we add three fields, each with their own label, an index number, and the type
    of data that is stored in it. If we run this through the same type of verification,
    create, encode, decode process, we will get the same results as before.
  prefs: []
  type: TYPE_NORMAL
- en: While this has not been a comprehensive overview of different data formats,
    it should help to recognize when we might use schema-less (when we don't know
    what the data may look like) and when to use schemas (when communicating between
    unknown systems or we need a decrease in payload size).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While most of our starting applications will be done utilizing JSON to pass
    data between different servers, or even different parts of our applications, it
    should be noticeable where we may not want to use it. By utilizing other data
    formats, we can make sure that we get as much speed out of our application as
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen what building our own data format could entail and then we took
    a look at other popular formats that are currently out there. This should be the
    last piece of information that we need to build highly performant server applications
    in Node.js. While we will use some libraries for data formats, we should also
    note that we have really only used the vanilla libraries that come with Node.js.
  prefs: []
  type: TYPE_NORMAL
- en: We will next take a look at a practical example of a static server that caches
    information. From here, we will utilize all of the previous concepts to create
    a highly available and speedy static server.
  prefs: []
  type: TYPE_NORMAL
