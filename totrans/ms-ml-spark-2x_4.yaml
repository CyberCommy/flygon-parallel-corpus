- en: Predicting Movie Reviews Using NLP and Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will take an in-depth look at the field of **Natural Language
    Processing** (**NLP**), not to be confused with Neuro-Linguistic Programming!
    NLP helps analyze raw textual data and extract useful information such as sentence
    structure, sentiment of text, or even translation of text between languages. Since
    many sources of data contain raw text, (for example, reviews, news articles, and
    medical records). NLP is getting more and more popular, thanks to providing an
    insight into the text and helps make automatized decisions easier.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, NLP is often using machine-learning algorithms to extract and
    model the structure of text. The power of NLP is much more visible if it is applied
    in the context of another machine method, where, for example, text can represent
    one of the input features.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will apply NLP to analyze the *sentiment* of movie reviews.
    Based on annotated training data, we will build a classification model that is
    going to distinguish between positive and negative movie reviews. It is important
    to mention that we do not extract sentiment directly from the text (based on words
    such as love, hate, and so on), but utilize a binary classification that we have
    already explored in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to accomplish this, we will take raw movie reviews that have been
    manually scored beforehand and train an ensemble-a set of models-which are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Process the movie reviews to synthesize the features for our model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, we will explore the various features we can create with text data ranging
    from a bag-of-words approach to a weighted bag-of-words (for example, TF-IDF)
    and then briefly explore the word2vec algorithm, which we will explore in detail
    in [Chapter 5](part0101.html#30A8Q0-d18ba71168a441bd917775fac13ca893), Word2vec
    for Prediction and Clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Alongside this, we will look at some basic ways of feature selection/omission,
    which include removing stopwords and punctuation, or stemming.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the generated features, we will run a variety of supervised, binary classification
    algorithms to help us classify positive and negative reviews, which include the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classification decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosted trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging the combined predictive power of the four different learning algorithms,
    we will create a super-learner model, which takes all four "guesses" of the models
    as meta-features to train a deep neural network to output a final prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we will create a Spark machine learning pipeline for this process,
    which does the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extracts features from new movie reviews
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comes up with a prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs this prediction inside of a Spark streaming application (yes, you will
    build your first machine learning application in every chapter for the remainder
    of this book!)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If this sounds a tad ambitious, take heart! We will step through each one of
    these tasks in a manner that is both methodical and purposeful so that you can
    have the confidence to build your own NLP application; but first, a little background
    history and some theory behind this exciting field.
  prefs: []
  type: TYPE_NORMAL
- en: NLP - a brief primer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just like artificial neural networks, NLP is a relatively "old" subject, but
    one that has garnered a massive amount of attention recently due to the rise of
    computing power and various applications of machine learning algorithms for tasks
    that include, but are not limited to, the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine translation** (**MT**): In its simplest form, this is the ability
    of machines to translate one language of words to another language of words. Interestingly,
    proposals for machine translation systems pre-date the creation of the digital
    computer. One of the first NLP applications was created during World War II by
    an American scientist named Warren Weaver whose job was to try and crack German
    code. Nowadays, we have highly sophisticated applications that can translate a
    piece of text into any number of different languages we desire!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech recognition** (**SR**): These methodologies and technologies attempt
    to recognize and translate spoken words into text using machines. We see these
    technologies in smartphones nowadays that use SR systems in tasks ranging from
    helping us find directions to the nearest gas station to querying Google for the
    weekend''s weather forecast. As we speak into our phones, a machine is able to
    recognize the words we are speaking and then translate these words into text that
    the computer can recognize and perform some task if need be.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information retrieval** (**IR**): Have you ever read a piece of text, such
    as an article on a news website, for example, and wanted to see similar news articles
    like the one you just read? This is but one example of an information retrieval
    system that takes a piece of text as an "input" and seeks to obtain other relevant
    pieces of text similar to the input text. Perhaps the easiest and most recognizable
    example of an IR system is doing a search on a web-based search engine. We give
    some words that we want to "know" more about (this is the "input"), and the output
    are the search results, which are hopefully relevant to our input search query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information extraction** (**IE**): This is the task of extracting structured
    bits of information from unstructured data such as text, video and pictures. For
    example, when you read a blog post on some website, often, the post is tagged
    with a few keywords that describe the general topics about this posting, which
    can be classified using information extraction systems. One extremely popular
    avenue of IE is called *Visual Information Extraction,* which attempts to identify
    complex entities from the visual layout of a web page, for example, which would
    not be captured in typical NLP approaches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text summarization** (**darn, no acronym here!**): This is a hugely popular
    area of interest. This is the task of taking pieces of text of various length
    and summarizing them by identifying topics, for example. In the next chapter,
    we will explore two popular approaches to text summarization via topic models
    such as **Latent Dirichlet Allocation** (**LDA**) and **Latent Semantic Analysis**
    (**LSA**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will use NLP techniques to help us solve a binary classification
    problem for rating movie reviews from **International Movie Database** (**IMDb**).
    Let's now shift our attention to the dataset we will use and learn more about
    feature extraction techniques with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Large Movie Review Database, originally published in the paper, *Learning
    Word Vectors for Sentiment Analysis,* by Andrew L. Maas et al, can be downloaded
    from [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/).
  prefs: []
  type: TYPE_NORMAL
- en: The downloaded archive contains two folders labeled *train* and *test*. For
    train, there are 12,500 positive reviews and 12,500 negative reviews that we will
    train a classifier on. The test dataset contains the same amount of positive and
    negative reviews for a grand total of 50,000 positive and negative reviews amongst
    the two files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example of one review to see what the data looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Bromwell High is nothing short of brilliant. Expertly scripted and perfectly
    delivered, this searing parody of  students and teachers at a South London Public
    School leaves you literally rolling with laughter. It''s vulgar, provocative,
    witty and sharp. The characters are a superbly caricatured cross-section of British
    society (or to be more accurate, of any society). Following the escapades of Keisha,
    Latrina, and Natella, our three "protagonists", for want of a better term, the
    show doesn''t shy away from parodying every imaginable subject. Political correctness
    flies out the window in every episode. If you enjoy shows that aren''t afraid
    to poke fun of every taboo subject imaginable, then Bromwell High will not disappoint!"'
  prefs: []
  type: TYPE_NORMAL
- en: It appears that the only thing we have to work with is the raw text from the
    movie review and review sentiment; we know nothing about the date posted, who
    posted the review, and other data that may/may not be helpful to us aside from
    the text.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before running any data manipulation, we need to prepare the Spark environment
    as we did in the previous chapters. Let''s start the Spark shell and request enough
    memory to process the downloaded dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To avoid too much logging output from Spark, it is possible to control logging
    level at runtime directly by calling `setLogLevel` on SparkContext:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sc.setLogLevel("WARN")`'
  prefs: []
  type: TYPE_NORMAL
- en: The command decreases the verbosity of the Spark output.
  prefs: []
  type: TYPE_NORMAL
- en: The next challenge is to read in the training dataset, which is composed of
    25,000 positive and negative movie reviews. The following lines of code will read
    in these files and then create our binary labels of 0 for a negative review and
    1 for a positive review.
  prefs: []
  type: TYPE_NORMAL
- en: 'We directly utilize the exposed Spark `sqlContext` method, `textFile`, that
    allows for reading multiple files and returns Dataset[String]. This is the difference
    from the method mentioned in the previous chapters, which were using the method
    called `wholeTextFiles` and producing RDD[String]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can directly show the first five lines using the dataset method `show` (you
    can modify the truncate parameter to show the full text of the review):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00092.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will do the same thing for the negative reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00093.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now, each of the *positiveReview* and *negativeReviews* variables represents
    RDD of loaded reviews. Each row of dataset contains a string representing a single
    review. However, we still need to generate corresponding labels and merge both
    loaded datasets together.
  prefs: []
  type: TYPE_NORMAL
- en: 'The labeling is easy, since we loaded negative and positive reviews as separated
    Spark DataFrames. We can directly append a constant column representing the label
    0 for negative reviews and 1 for positive reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00094.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, we used the `withColumn` method, which appends a new column to
    an existing dataset. The definition of a new column `lit(1.0)` means a constant
    column defined by a numeric literal *1.0*. We need to use a real number to define
    the target value, since the Spark API expects it. Finally, we merged both datasets
    together using the `union` method.
  prefs: []
  type: TYPE_NORMAL
- en: We also appended the magic column `row_id`, which uniquely identifies each row
    in the dataset. This trick simplifies our workflow later when we need to join
    the output of several algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Why did we use a double value instead of a string label representation? In the
    code labeling individual reviews, we defined a constant column with numeric literals
    representing double numbers. We could use also *lit("positive")* to label positive
    reviews, but using plain text labels would force us to transform the string value
    into numeric value in the later steps anyway. Hence, in this example, we will
    make our life easier using double value labels directly. Furthermore, we used
    double values directly since it is required by the Spark API.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this stage, we have only a raw text representing reviews, which is not sufficient
    to run any machine learning algorithm. We need to transform the text into a numeric
    format, aka perform the so-called "feature extraction" (it is as it sounds; we
    are taking the input data and extracting features which we will use to train a
    model). The method generates some new features based on input feature(s). There
    are many methods regarding how the text can be transformed into numeric features.
    We can count the number of words, length of text, or number of punctuations. However,
    to represent text in a systematic way that would reflect a text structure, we
    need more elaborate methods.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction method– bag-of-words model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have ingested our data and created our labels, it''s time to extract
    our features to build our binary classification model. As its name suggests, the
    bag-of-words approach is a very common feature-extraction technique whereby we
    take a piece of text, in this case a movie review, and represent it as a bag (aka
    multiset) of its words and grammatical tokens. Let''s look at an example using
    a few movie reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Review 1:** *Jurassic World was such a flop!*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Review 2:** *Titanic ... an instant classic. Cinematography was as good as
    the acting!!*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each token (can be a word and/or punctuation), we will create a feature
    and then count the occurrence of that token throughout the document. Here''s what
    our bag-of-words dataset would look like for the first review:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Review ID** | **a** | **Flop** | **Jurassic** | **such** | **World** |
    **!** |'
  prefs: []
  type: TYPE_TB
- en: '| Review 1 | 1 | 1 | 1 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'First, notice the arrangement of this dataset, often called a *document-term
    matrix* (each document [row] is composed of a certain set of words [terms] that
    make up this two-dimensional matrix). We can also arrange this differently and
    transpose the rows and columns to create-you guessed it-a *term-document matrix*
    whereby the columns now show the documents that have that particular term and
    the numbers inside the cells are the counts. Also, realize that the order of the
    words is alphabetical, which means we lose any sense of word order. This implies
    that the word "flop" is equidistant in similarity to the word "Jurassic," and
    while we know this is not true, this highlights one of the limitations of the
    bag-of-words approach: *the* *word order is lost, and sometimes, different documents
    can have the same representation but mean totally different things.*'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about an extremely powerful learning algorithm
    pioneered at Google and included in Spark called  **word-to-vector** (**word2vec**),
    which essentially digitizes terms to "encode" their meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Second, notice that for our given review of six tokens (including punctuation),
    we have six columns. Suppose we added the second review to our document-term-matrix;
    how would our original bag-of-words change?
  prefs: []
  type: TYPE_NORMAL
- en: '| **Review ID** | **a** | **acting** | **an** | **as** | **Cinematography**
    | **classic** | **flop** | **good** | **instant** | **Jurassic** | **such** |
    **Titanic** | **was** | **World** | **.** | **!** |'
  prefs: []
  type: TYPE_TB
- en: '| Review 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 1
    |'
  prefs: []
  type: TYPE_TB
- en: '| Review 2 | 0 | 1 | 1 | 2 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 1 | 1 | 0 | 1 | 2
    |'
  prefs: []
  type: TYPE_TB
- en: We tripled our original number of features from five to 16 tokens, which brings
    us to another consideration with this approach. Given that we must create a feature
    for every token, it's not difficult to see we will soon have an extremely wide
    and very sparse matrix representation (sparse because one document will certainly
    not contain every word/symbol/emoticon, and so on, and therefore, most of the
    cell inputs will be zero). This poses some interesting problems with respect to
    the dimensionality for our algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the situation where we are trying to train a random forest using a
    bag-of-words approach on a text document that has +200k tokens, whereby most of
    the inputs will be zero. Recall that in a tree-based learner, it is making determinations
    to "go left or go right", which is dependent on the feature type. In a bag-of-words
    example, we can count features as true or false (that is, the document has the
    term or not) or the occurrence of a term (that is, how many times does the document
    have this term). For each successive branch in our tree, the algorithm must consider
    all these features (or at least the square root of the number of features in the
    case of a random forest), which can be extremely wide and sparse, and make a decision
    that influences the overall outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, you are about to learn how Spark deals with this type of dimensionality
    and sparsity along with some steps we can take to reduce the number of features
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Text tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To perform feature extraction, we still need to provide individual words-tokens
    that are composing the original text. However, we do not need to consider all
    the words or characters. We can, for example, directly skip punctuations or unimportant
    words such as prepositions or articles, which mostly do not bring any useful information.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, a common practice is to regularize tokens to a common representation.
    This can include methods such as unification of characters (for example, using
    only lowercase characters, removing diacritics, using common character encoding
    such as utf8, and so on) or putting words into a common form (so-called stemming,
    for example, "cry"/"cries"/"cried" is represented by "cry").
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we will perform this process using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Lowercase all words ("Because" and "because" are the same word).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove punctuation symbols with a regular expression function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove stopwords. These are essentially injunctions and conjunctions such as
    *in*, *at*, *the*, *and*, *etc*, and so on, that add no contextual meaning to
    the review that we want to classify.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find "rare tokens" that have a total number of occurrences less than three times
    in our corpus of reviews.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, remove all "rare tokens."
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each of the steps in the preceding sequence represent our best practices when
    doing sentiment classification on text. For your situation, you may not want to
    lowercase all words (for example, "Python", the language and "python", the snake
    type, is an important distinction!). Furthermore, your stopwords list-if you choose
    to include one-may be different and incorporate more business logic given your
    task. One website that has done a fine job in collecting lists of stopwords is
    [http://www.ranks.nl/stopwords](http://www.ranks.nl/stopwords).
  prefs: []
  type: TYPE_NORMAL
- en: Declaring our stopwords list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we can directly reuse the list of generic English stopwords provided
    by Spark. However, we can enrich it by our specific stopwords:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As stated earlier, this is an extremely delicate task and highly dependent on
    the business problem you are looking to solve. You may wish to add to this list
    terms that are relevant to your domain that will not help the prediction task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Declare a tokenizer that tokenizes reviews and omits all stopwords and words
    that are too short:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at this function step by step to see what it''s doing. It
    accepts a single review as an input and then calls the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.split("""\W+""")`: This splits movie review text into tokens that are represented
    by alphanumeric characters only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.map(_.toLowerCase.replaceAll("[^\\p{IsAlphabetic}]", ""))`: As a best practice,
    we lowercase the tokens so that at index time, *Java = JAVA = java*. However,
    this unification is not always the case, and it''s important that you are aware
    of the implications lowercasing your text data can have on the model. As an example,
    "Python," the computing language would lowercase to "python," which is also a
    snake. Clearly, the two tokens are not the same; however, lowercasing would make
    it so! We will also filter out all numeric characters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.filter(w =>w.length>minTokenLen)`: Only keep those tokens whose length is
    greater than a specified limit (in our case, three characters).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.filter(w => !stopWords.contains(w))`: Using the stopwords list that we declared
    beforehand, we can remove these terms from our tokenized data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can now directly apply the defined function on the corpus of reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we are marking the function `toTokens` as a Spark user-defined
    function by calling the `udf` marker, which exposes a common Scala function to
    be used in the context of the Spark DataFrame. After that, we can directly apply
    the defined `udf` function on the `reviewText` column in the loaded dataset. The
    output from the function creates a new column called `reviewTokens`.
  prefs: []
  type: TYPE_NORMAL
- en: We separated `toTokens` and `toTokensUDF` definitions since it would be easier
    to define them in one expression. This is a common practice that allows you to
    test the `toTokens` method in separation without the need of using and knowing
    Spark infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, you can reuse the defined `toTokens` method among different projects,
    which do not necessarily need to be Spark-based.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code finds all the rare tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Rare tokens computation is a complex operation. In our example, the input is
    represented by rows containing a list of tokens. However, we need to compute all
    the unique tokens and their occurrences.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we flatten the structure into a new dataset where each row represents
    a token by using the `flatMap` method.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we can use the same strategy that we used in the previous chapters. We
    can generate key-value pairs *(word, 1)* for each word.
  prefs: []
  type: TYPE_NORMAL
- en: The pair is expressing the number of occurrences of the given word. Then, we
    will just group all the pairs with the same word together (the `groupByKey` method)
    and compute the total number of occurrences of the word representing a group (`reduceGroups`).
    The following steps just filter out all too frequent words and finally collect
    the result as a list of words.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next goal is to find rare tokens. In our example, we will consider each
    token with occurrences less than three as rare:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00095.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have our tokenization function, it is time to filter out rare tokens
    by defining another Spark UDF, which we will directly apply on the `reviewTokens` input
    data column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Movie reviews tokens are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00096.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Depending on your particular task, you may wish to add or perhaps delete some
    stopwords or explore different regular expression patterns (teasing out email
    addresses using regular expressions, for example, is quite common). For now, we
    will take the tokens that we have to and use it to build our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming and lemmatization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One extremely popular step in NLP is to stem words back to their root form.
    For example, "accounts" and "accounting" would both be stemmed to "account," which
    at first blush seems very reasonable. However, stemming falls prey to the following
    two areas, which you should be aware of:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. **Over-stemming**: This is when stemming fails to keep two words with distinct
    meanings separate. For example, stem ("general," "genetic") = "gene".'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. **Under-stemming**: This is the inability to reduce words with the same
    meaning to their root forms. For example, stem ("jumping," "jumpiness") = *jumpi* but
    stem ("jumped," "jumps") = "jump." In this example, we know that each of the preceding
    terms are simply an inflection of the root word "jump;" however, depending on
    the stemmer you choose to employ (the two most common stemmers are Porter [oldest
    and most common] and Lancaster), you may fall into this error.'
  prefs: []
  type: TYPE_NORMAL
- en: Given the possibilities of over and under-stemming words in your corpus, NLP
    practitioners cooked up the notion of lemmatization to help combat these known
    issues. The word "Lemming," is taking the canonical (dictionary) form of a *set
    of related words* based on the context of the word. For example, lemma ("paying,"
    "pays, "paid") = "pay." Like stemming, lemmatization tries to group related words,
    but goes one step further by trying to group words by their word sense because,
    after all, the same two words can have entirely different meanings depending on
    the context! Given the depth and complexity of this chapter already, we will refrain
    from performing any lemmatization techniques, but interested parties can read
    further about this topic at [http://stanfordnlp.github.io/CoreNLP/](http://stanfordnlp.github.io/CoreNLP/).
  prefs: []
  type: TYPE_NORMAL
- en: Featurization - feature hashing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, it is time to transform string representation into a numeric one. We adopt
    a bag-of-words approach; however, we use a trick called feature hashing. Let's
    look in more detail at how Spark employs this powerful technique to help us construct
    and access our tokenized dataset efficiently. We use feature hashing as a time-efficient
    implementation of a bag-of-words, as explained earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, feature hashing is a fast and space-efficient method to deal with
    high-dimensional data-typical in working with text-by converting arbitrary features
    into indices within a vector or matrix. This is best described with an example
    text. Suppose we have the following two movie reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The movie Goodfellas was well worth the money spent. Brilliant acting!*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Goodfellas is a riveting movie with a great cast and a brilliant plot-a must
    see for all movie lovers!*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each token in these reviews, we can apply a "hashing trick," whereby we
    assign the distinct tokens a number. So, the set of unique tokens (after lowercasing
    + text processing) in the preceding two reviews would be in alphabetical order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then apply the hashes to create the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The matrix from the feature hashing is constructed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Rows *represent* the movie review numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Columns *represent* the features (not the actual words!). The feature space
    is represented by a range of used hash functions. Note that for each row, there
    is the same number of columns and not just one ever-growing, wide matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, every entry in the matrix (*i, j)* = *k* means that in row *i,* feature
    *j,* appears *k* times. So, for example, the token "movie" which is hashed on
    feature 9, appears twice in the second review; therefore, matrix (2, 9) = 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The used hashing function makes gaps. If the hashing function hashes a small
    set of words into large numeric space, the resulting matrix will have high sparsity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One important consideration to think about is the notion of hashing collisions,
    which is where two different features (tokens, in this case) are hashed into the
    same index number in our feature matrix. A way to guard against this is to choose
    a large number of features to hash, which is a parameter we can control in Spark
    (the default setting for this in Spark is 2^20 ~ 1 million features).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we can employ Spark's hashing function, which will map each token to a
    hash index that will make up our feature vector/matrix. As always, we will start
    with our imports of the necessary classes we will need and then change the default
    value for the number of features to create hashes against to roughly 4096 (2^12).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code, we will use the `HashingTF` transformer from the Spark ML package
    (you will learn more about transformations later in this chapter). It requires
    the names of input and output columns. For our dataset `movieReviews`, the input
    column is `reviewTokens`, which holds the tokens created in the previous steps.
    The result of the transformation is stored in a new column called `tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00097.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: After invoking the transformation, the resulting `tfTokens` dataset contains
    alongside original data a new column called `tf`, which holds an instance of `org.apache.spark.ml.linalg`.
    Vector for each input row. The vector in our case is a sparse vector (because
    the hash space is much larger than the number of unique tokens).
  prefs: []
  type: TYPE_NORMAL
- en: Term Frequency - Inverse Document Frequency (TF-IDF) weighting scheme
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now use Spark ML to apply a very common weighting scheme called a TF-IDF
    to convert our tokenized reviews into vectors, which will be inputs to our machine
    learning models. The math behind this transformation is relatively straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00098.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For each token:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the term frequency within a given document (in our case, a movie review).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiply this count by the log of the inverse document frequency that looks
    at how common the token occurs among all of the documents (commonly referred to
    as the corpus).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Taking the inverse is useful, in that it will penalize tokens that occur too
    frequently in the document (for example, "movie") and boost those tokens that
    do not appear as frequently.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we can scale terms based on the inverse term document frequency formula
    explained earlier. First, we need to compute a model-a prescription about how
    to scale term frequencies. In this case, we use the Spark `IDF` estimator to create
    a model based on the input data produced by the previous step `hashingTF`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will build a Spark estimator that we trained (fitted) on the input
    data (= output of transformation in the previous step). The IDF estimator computes
    weights of individual tokens. Having the model, it is possible to apply it on
    any data that contains a column defined during fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00099.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look in more detail at a single row and the difference between `hashingTF` and
    `IDF` outputs. Both operations produced a sparse vector the same length. We can
    look at non-zero elements and verify that both rows contain non-zero values at
    the same locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also print a few non-zero values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00100.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You can directly see that tokens with the same frequency in the sentence can
    have different resulting scores based on their frequencies over all the sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Let's do some (model) training!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we have a numeric representation of textual data, which captures
    the structure of reviews in a simple way. Now, it is time for model building.
    First, we will select columns that we need for training and split the resulting
    dataset. We will keep the generated `row_id` column in the dataset. However, we
    will not use it as an input feature, but only as a simple unique row identifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we have created four different subsets of our data: a training
    dataset, testing dataset, transfer dataset, and a final validation dataset. The
    transfer dataset will be explained later on in the chapter, but everything else
    should appear very familiar to you already from the previous chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, the cache call is important since the majority of the algorithms are going
    to iteratively query the dataset data, and we want to avoid repeated evaluation
    of all the data preparation operations.
  prefs: []
  type: TYPE_NORMAL
- en: Spark decision tree model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s start with a simple decision tree and perform a grid search over
    a few of the hyper-parameters. We will follow the code from [Chapter 2](part0038.html#147LC0-d18ba71168a441bd917775fac13ca893), *Detecting
    Dark Matter: The Higgs-Boson Particle* to build our models that are trained to
    maximize the AUC statistic. However, instead of using models from the MLlib library,
    we will adopt models from the Spark ML package. The motivation of using the ML
    package will be clearer later when we will need to compose the models into a form
    of pipeline. Nevertheless, in the following code, we will use `DecisionTreeClassifier`,
    which we fit to `trainData`, generate prediction for `testData`, and evaluate
    the model''s AUC performance with the help of `BinaryClassificationEvaluato`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00101.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'After selecting the best model, we will write it into a file. This is a useful
    trick since model training can be time and resource expensive, and the next time,
    we can load the model directly from the file instead of retraining it again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Spark Naive Bayes model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, let's look to employ Spark's implementation of Naive Bayes. As a reminder,
    we purposely stay away from going into the algorithm itself as this has been covered
    in many machine learning books; instead, we will focus on the parameters of the
    model and ultimately, how we can "deploy" these models in a Spark streaming application
    later on in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark''s implementation of Naive Bayes is relatively straightforward, with
    just a few parameters we need to keep in mind. They are mainly as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**getLambda**: Sometimes referred to as "additive smoothing" or "laplace smoothing,"
    this parameter allows us to smooth out the observed proportions of our categorical
    variables to create a more uniform distribution. This parameter is especially
    important when the number of categories you are trying to predict is very low
    and you don''t want entire categories to be missed due to low sampling. Enter
    the lambda parameter that "helps" you combat this by introducing some minimal
    representation of some of the categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**getModelType**: There are two options here: "*multinomial"* (default) or
    "*Bernoulli."* The *Bernoulli *model type would assume that our features are binary,
    which in our text example would be "*does review have word _____? Yes or no?*"
    The *multinomial* model type, however, takes discrete word counts. One other model
    type that is not currently implemented in Spark for Naive Bayes but is appropriate
    for you to know is a Gaussian model type. This gives our model features the freedom
    to come from a normal distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given that we only have one hyper-parameter to deal with in this case, we will
    simply go with the default value for our lamda, but you are encouraged to try
    a grid search approach as well for optimal results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is interesting to compare the performance of different models for the same
    input dataset. Often, it turns out that even a simple Naive Bayes algorithm lends
    itself very well to text classification tasks. The reason partly has to do with
    the first adjective of this algorithm: "naive." Specifically, this particular
    algorithm assumes that our features-which in this case are globally weighted term
    frequencies-are mutually independent. Is this true in the real world? More often,
    this assumption is often violated; however, this algorithm still could perform
    just as well, if not better, than more complex models.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark random forest model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will move on to our random forest algorithm, which, as you will recall
    from the previous chapters, is an ensemble of various decision trees whereby we
    perform a grid search again alternating between various depths and other hyper-parameters,
    which will be familiar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00103.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From our grid search, the highest AUC we are seeing is `0.769`.
  prefs: []
  type: TYPE_NORMAL
- en: Spark GBM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we will move on to our **gradient boosting machine** (**GBM**), which
    will be the final model in our ensemble of models. Note that in the previous chapters,
    we used H2O''s version of GBM, but now, we will stick with Spark and use Spark''s
    implementation of GBM as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00104.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'So now, we have trained up four different learning algorithms: a (single) decision
    tree, a random forest, Naive Bayes, and a gradient boosted machine. Each provides
    a different AUC as summarized in the table here. We can see that the best performing
    model is RandomForest followed by GBM. However, it is fair to say that we did
    not perform any exhausted search for the GBM model nor did we use a high number
    of iterations as is usually recommended:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Decision tree | 0.659 |'
  prefs: []
  type: TYPE_TB
- en: '| Naive Bayes | 0.484 |'
  prefs: []
  type: TYPE_TB
- en: '| Random forest | 0.769 |'
  prefs: []
  type: TYPE_TB
- en: '| GBM | 0.755 |'
  prefs: []
  type: TYPE_TB
- en: Super-learner model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will combine the prediction power for all of these algorithms to generate
    a "super-learner" with the help of a neural network, which takes each model''s
    prediction as input and then, tries to come up with a better prediction, given
    the guesses of the individually trained models. At a high level, the architecture
    would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00105.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We will explain further the intuition behind building a "super-learner" and
    the benefits of this approach, and teach you how to build your Spark streaming
    application, which will take in your text (that is, a movie review that you will
    write) and run it through the prediction engine of each of your models. Using
    these predictions as input into your neural network, we will yield a positive
    or negative sentiment using the combined power of the various algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Super learner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the preceding sections, we trained several models. Now, we will compose
    them into an ensemble called a super learner using a deep learning model. The
    process to build a super learner is straightforward (see the preceding figure):'
  prefs: []
  type: TYPE_NORMAL
- en: Select base algorithms (for example, GLM, random forest, GBM, and so on).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a meta-learning algorithm (for example, deep learning).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train each of the base algorithms on the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform K-fold cross-validation on each of these learners and collect the cross-validated
    predicted values from each of the base algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The N cross-validated predicted values from each of the L-base algorithms can
    be combined to form a new NxL matrix. This matrix, along with the original response
    vector, is called the "level-one" data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the meta-learning algorithm on the level-one data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The super learner (or so-called "ensemble model") consists of the L-base learning
    models and the meta-learning model, which can then be used to generate predictions
    on a test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The key trick of ensembles is to combine a diverse set of strong learners together.
    We already discussed a similar trick in the context of the random forest algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The PhD thesis of Erin LeDell contains much more detailed information about
    super learners and their scalability. You can find it at [http://www.stat.berkeley.edu/~ledell/papers/ledell-phd-thesis.pdf](http://www.stat.berkeley.edu/~ledell/papers/ledell-phd-thesis.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we will simplify the whole process by skipping cross-validation
    but using a single hold-out dataset. It is important to mention that this is not
    the recommended approach!
  prefs: []
  type: TYPE_NORMAL
- en: As the first step, we use trained models and a transfer dataset to get predictions
    and compose them into a new dataset, augmenting it by the actual labels.
  prefs: []
  type: TYPE_NORMAL
- en: This sounds easy; however, we cannot use the *DataFrame#withColumn* method directly
    and create a new `DataFrame` from multiple columns from different datasets, since
    the method accepts columns only from the left-hand side `DataFrame` or constant
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, we have already prepared the dataset for this situation by assigning
    a unique ID to each row. In this case, we will use it and join individual model
    predictions based on `row_id`. We also need to rename each model prediction column
    to uniquely identify the model prediction inside the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00106.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The table is composed of the models' prediction and annotated by the actual
    label. It is interesting to see how individual models agree/disagree on the predicted
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the same transformation to prepare a validation dataset for our
    super learner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build our meta-learner algorithm. In this case, we will use the
    deep learning algorithm provided by the H2O machine learning library. However,
    it needs a little bit of preparation-we need to publish the prepared train and
    test data as H2O frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to transform the `label` column into a categorical column. This
    is necessary; otherwise, the H2O deep learning algorithm would perform regression
    since the `label` column is numeric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build an H2O deep learning model. We can directly use the Java
    API of the algorithm; however, since we would like to compose all the steps into
    a single Spark pipeline, we will utilize a wrapper exposing the Spark estimator
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we directly specified the validation dataset, we can explore the performance
    of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Alternatively, we can open the H2O Flow UI (by calling `hc.openFlow`) and explore
    its performance in the visual form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You can easily see that the AUC for this model on the validation dataset is
    0.868619 - the value higher than all the AUC values of the individual models.
  prefs: []
  type: TYPE_NORMAL
- en: Composing all transformations together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we developed individual steps using Spark primitives,
    that is, UDFs, native Spark algorithms, and H2O algorithms. However, to invoke
    all these transformation on unseen data requires a lot of manual effort. Hence,
    Spark introduces the concept of pipelines, mainly motivated by Python scikit pipelines
    ([http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about the design decision behind Python, we recommend that you
    read the excellent paper "API design for machine learning software: experiences
    from the scikit-learn project" by Lars Buitinck et al ([https://arxiv.org/abs/1309.0238](https://arxiv.org/abs/1309.0238)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline is composed of stages that are represented by estimators and transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Estimators**: These are the core elements that expose a fit method that creates
    a model. Most of the classification and regression algorithms are represented
    as an estimator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformers**: These transform an input dataset into a new dataset. The
    transformers expose the method `transform`, which implements the logic of transformation.
    The transformers can produce single on multiple vectors. Most of the models produced
    by estimators are transformers-they transform an input dataset into a new dataset
    representing the prediction. Another example can be the TF transformer used in
    this section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pipeline itself exposes the same interface as the estimator. It has the
    fit method, so it can be trained and produces a "pipeline model", which can be
    used for data transformation (it has the same interface as transformers). Hence,
    the pipelines can be combined hierarchically together. Furthermore, the individual
    pipeline stages are invoked in a sequential order; however, they can still represent
    a directed acyclic graph (for example, a stage can have two input columns, each
    produced by a different stage). In this case, the sequential order has to follow
    the topological ordering of the graph.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we will compose all the transformation together. However, we
    will not define a training pipeline (that is, a pipeline that will train all the
    models), but we will use the already trained models to set up the pipeline stages.
    Our motivation is to define a pipeline that we can use to score a new movie review.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, let''s start from the beginning of our example-the first operation that
    we applied on the input data was a simple tokenizer. It was defined by a Scala
    function that we wrapped into a form of Spark UDF. However, to use it as a part
    of the pipeline we need to wrap the defined Scala function into a transformation.
    Spark does not provide any simple wrapper to do that, so it is necessary to define
    a generic transformation from scratch. We know that we will transform a single
    column into a new column. In this case, we can use `UnaryTransformer`, which exactly
    defines one-to-one column transformation. We can be a little bit more generic
    and define a generic wrapper for Scala functions (aka Spark UDFs):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `UDFTransformer` class wraps a function `f`, which accepts a generic type
    `T`, and produces type `U`. At the Spark dataset level, it transforms an input
    column (see `UnaryTransformer`) of type `inType` into a new output column (again,
    the field is defined by `UnaryTransformer`) of the `outType` type. The class also
    has a dummy implementation of the trait `MLWritable`, which supports serialization
    of the transformer into a file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we just need to define our tokenizer transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The defined transformer accepts a string column (that is, a movie review) and
    produces a new column that contains an array of strings representing movie review
    tokens. The transformer is directly using the `toTokens` function, which we used
    at the beginning of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next transformation should remove rare words. In this case, we will use
    a similar approach as in the previous step and utilize the defined `UDFTransformer` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This transformer accepts a column containing an array of tokens and produces
    a new column containing a filtered array of tokens. It is using the already defined
    `rareTokensFilter` Scala function.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have not specified any input data dependencies, including names of
    input columns. We will keep it for the final pipeline definition.
  prefs: []
  type: TYPE_NORMAL
- en: The next steps include vectorization with the help of the `TF` method hashing
    string tokens into a large numeric space and followed by a transformation based
    on the built `IDF` model. Both transformations are already defined in the expected
    form-the first `hashingTF` transformation is already a transformer translating
    a set of tokens into numeric vectors, the second one `idfModel` accepts the numeric
    vector and scales it based on the computed coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: These steps provide input for the trained binomial models. Each base model represents
    a transformer producing several new columns such as prediction, raw prediction,
    and probabilities. However, it is important to mention that not all models provide
    the full set of columns. For example, Spark GBM currently (Spark version 2.0.0)
    provides only the prediction column. Nevertheless, it is good enough for our example.
  prefs: []
  type: TYPE_NORMAL
- en: 'After generating predictions, our dataset contains many columns; for example,
    input columns, columns with tokens, transformed tokens, and so on. However, to
    apply the generated meta-learner, we need only columns with prediction generated
    by the base models. Hence, we will define a column selector transformation, which
    drops all the unnecessary columns. In this case, we have a transformation-accepting
    dataset with N-columns and producing a new dataset with M-columns. Therefore,
    we cannot use `UnaryTransformer` defined earlier, and we need to define a new
    ad-hoc transformation called `ColumnSelector`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`ColumnSelector` represents a generic transformer that selects only the given
    columns from the input dataset. It is important to mention the overall two-stages
    concept-the first stage transforms the schema (that is, the metadata associated
    with each dataset) and the second transforms the actual dataset. The separation
    allows Spark to invoke early checks on transformers to find incompatibilities
    before invoking actual data transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to define the actual column selector transformer by creating an instance
    of `columnSelector`-be aware of specifying the right columns to keep:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, our transformers are ready to be composed into the final "super-learning"
    pipeline. The API of the pipeline is straightforward-it accepts individual stages
    that are invoked sequentially. However, we still need to specify dependencies
    between individual stages. Mostly the dependency is described by input and output
    column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few important concepts worth mentioning:'
  prefs: []
  type: TYPE_NORMAL
- en: The `tokenizerTransformer` and `rareTokensFilterTransformer` are connected via
    the column `allReviewTokens`-the first one is the column producer, and the second
    one is the column consumer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `dtModel`, `nbModel`, `rfModel`, and `gbmModel` models have the same input
    column defined as `idf.getOutputColumn`. In this case, we have effectively used
    computation DAG, which is topologically ordered into a sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the models have the same output columns (with some exceptions, in the case
    of GBM), which cannot be appended into the resulting dataset all together since
    the pipeline expects unique names of columns. Hence, we need to rename the output
    columns of the models by calling `setPredictionCol`, `setRawPredictionCol`, and
    `setProbabilityCol`. It is important to mention that the GBM does not produce
    raw prediction and probabilities columns right now.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we can fit the pipeline to get the pipeline model. This is, in fact, an
    empty operation, since our pipeline is composed only of transformers. However,
    we still need to call the `fit` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Voila, we have our super-learner model, composed of multiple Spark models and
    orchestrated by the H2O deep learning model. It is time to use the model to make
    a prediction!
  prefs: []
  type: TYPE_NORMAL
- en: Using the super-learner model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The usage of the model is easy-we need to provide a dataset with a single column
    called `reviewText` and transform it with `superLearnerModel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned prediction `reviewPrediction` is a dataset with the following
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The first column contains the predicted value, which was decided based on the
    F1 threshold. The columns `p0` and `p1` represent probabilities of individual
    prediction classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we explore the content of the returned dataset, it contains a single row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter demonstrated three powerful concepts: the processing of text,
    Spark pipelines, and super learners.'
  prefs: []
  type: TYPE_NORMAL
- en: The text processing is a powerful concept that is waiting to be largely adopted
    by the industry. Hence, we will go deeper into the topic in the following chapters
    and look at other approaches of natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: The same holds for Spark pipelines, which have become an inherent part of Spark
    and the core of the Spark ML package. They offer an elegant way of reusing the
    same concepts during training and scoring time. Hence, we would like to use the
    concept in the upcoming chapters as well.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, with super learners, aka ensembles, you learned the basic concept of
    how to benefit from ensembling multiple models together with the help of a meta-learner.
    This offers a simple but powerful way of building strong learners, which are still
    simple enough to understand.
  prefs: []
  type: TYPE_NORMAL
