- en: Chapter 11. Anomaly Detection on Sentiment Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we look back at the year 2016, we will surely remember it as a time of
    many significant geo-political events ranging from Brexit, Great Britain''s vote
    to leave the European Union, to the untimely passing of many beloved celebrities,
    including the sudden death of the singer David Bowie (covered in [Chapter 6](ch06.xhtml
    "Chapter 6. Scraping Link-Based External Data"), *Scraping Link-Based External
    Data* and [Chapter 7](ch07.xhtml "Chapter 7. Building Communities"), *Building
    Communities*). However, perhaps the most notable occurrence of the year was the
    tense US presidential election and its eventual outcome, the election of President
    Donald Trump. A campaign that will long be remembered, not least for its unprecedented
    use of social media, and the stirring up of passion among its users, most of whom
    made their feelings known through the use of hashtags: either positive ones, such
    as *#MakeAmericaGreatAgain* or *#StrongerTogether*, or conversely negative ones,
    such as *#DumpTrump* or *#LockHerUp*. Since this chapter is about sentiment analysis,
    the election presents the ideal use case. However, instead of trying to predict
    the outcome itself, we will aim to detect abnormal tweets during the US election
    using a real-time Twitter feed. We will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Acquiring Twitter data in real-time and batch
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting sentiment using Stanford NLP
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing sentiment time series in *Timely*
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deriving features from only 140 characters using *Word2Vec*
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the concepts of graph *ergodicity* and *shortest paths*
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a KMeans model to detect potential anomalies
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing models with *Embedding Projector* from *TensorFlow*
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following the US elections on Twitter
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On November 8, 2016, American citizens went in millions to polling stations
    to cast their votes for the next President of the United States. Counting began
    almost immediately and, although not officially confirmed until sometime later,
    the forecasted result was well known by the next morning. Let's start our investigation
    a couple of days before the major event itself, on November 6, 2016, so that we
    can preserve some context in the run-up. Although we do not exactly know what
    we will find in advance, we know that *Twitter* will play an oversized role in
    the political commentary given its influence in the build-up, and it makes sense
    to start collecting data as soon as possible. In fact, data scientists may sometimes
    experience this as a *gut feeling* - a strange and often exciting notion that
    compels us to commence working on something without a clear plan or absolute justification,
    just a sense that it will pay off. And actually, this approach can be vital since,
    given the normal time required to formulate and realize such a plan and the transient
    nature of events, a major news event may occur (refer to [Chapter 10](ch10.xhtml
    "Chapter 10. Story De-duplication and Mutation"), *Story De-duplication and Mutation*),
    a new product may have been released, or the stock market may be trending differently
    (see [Chapter 12](ch12.xhtml "Chapter 12. TrendCalculus"), *TrendCalculus*); by
    this time, the original dataset may no longer be available
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Acquiring data in stream
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first action is to start acquiring Twitter data. As we plan to download
    more than 48 hours worth of tweets, the code should be robust enough to not fail
    somewhere in the middle of the process; there is nothing more frustrating than
    a fatal `NullPointerException` occurring after many hours of intense processing.
    We know we will be working on sentiment analysis at some point down the line,
    but for now we do not wish to over-complicate our code with large dependencies
    as this can decrease stability and lead to more unchecked exceptions. Instead,
    we will start by collecting and storing the data and subsequent processing will
    be done offline on the collected data, rather than applying this logic to the
    live stream.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: We create a new Streaming context reading from Twitter 1% firehose using the
    utility methods created in [Chapter 9](ch09.xhtml "Chapter 9.  News Dictionary
    and Real-Time Tagging System") *, News Dictionary and Real-Time Tagging System*.
    We also use the excellent GSON library to serialize Java class `Status` (Java
    class embedding Twitter4J records) to JSON objects.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We read Twitter data every 5 minutes and have a choice to optionally supply
    Twitter filters as command line arguments. Filters can be keywords such as ***Trump***
    , ***Clinton* **or ***#MAGA*** , ***#StrongerTogether*** . However, we must bear
    in mind that by doing this we may not capture all relevant tweets as we can never
    be fully up to date with the latest hashtag trends (such as ***#DumpTrump*** ,
    ***#DrainTheSwamp*** , ***#LockHerUp*** , or *#LoveTrumpsHate*) and many tweets
    will be overlooked with an inadequate filter, so we will use an empty filter list
    to ensure that we catch everything.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We serialize our `Status` class using the GSON library and persist our JSON
    objects in HDFS. Note that the serialization occurs within a `Try` clause to ensure
    that unwanted exceptions are not thrown. Instead, we return JSON as an optional
    `String`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Finally, we run our Spark Streaming context and keep it alive until a new president
    has been elected, no matter what happens!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Acquiring data in batch
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Only 1% of tweets are retrieved through the Spark Streaming API, meaning that
    99% of records will be discarded. Although able to download around 10 million
    tweets, we can potentially download more data, but this time only for a selected
    hashtag and within a small period of time. For example, we can download all tweets
    related to the ***#LockHerUp*** or ***#BuildTheWall*** hashtags.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: The search API
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For that purpose, we consume Twitter historical data through the `twitter4j`
    Java API. This library comes as a transitive dependency of `spark-streaming-twitter_2.11`.
    To use it outside of a Spark project, the following maven dependency should be
    used:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We create a Twitter4J client as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we consume the `/search/tweets` service through the `Query` object:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finally, we get a list of `Status` objects that can easily be serialized using
    the GSON library introduced earlier.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Rate limit
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Twitter is a fantastic resource for data science, but it is far from a non-profit
    organization, and as such, they know how to value and price data. Without any
    special agreement, the search API is limited to a few days retrospective, a maximum
    of 180 queries per 15 minute window and 450 records per query. This limit can
    be confirmed on both the Twitter DEV website ([https://dev.twitter.com/rest/public/rate-limits](https://dev.twitter.com/rest/public/rate-limits))
    and from the API itself using the `RateLimitStatus` class:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Unsurprisingly, any queries on popular terms, such as ***#MAGA*** on November
    9, 2016, hit this threshold. To avoid a rate limit exception, we have to page
    and throttle our download requests by keeping track of the maximum number of tweet
    IDs processed and monitor our status limit after each search request.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With around half a billion tweets a day, it will be optimistic, if not Naive,
    to gather all US-related data. Instead, the simple ingest process detailed earlier
    should be used to intercept tweets matching specific queries only. Packaged as
    main class in an assembly jar, it can be executed as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, the `twitter.properties` file contains your Twitter API keys:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Analysing sentiment
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After 4 days of intense processing, we extracted around 10 million tweets; representing
    approximately 30 GB worth of JSON data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Massaging Twitter data
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the key reasons Twitter became so popular is that any message has to
    fit into a maximum of 140 characters. The drawback is also that every message
    has to fit into a maximum of 140 characters! Hence, the result is massive increase
    in the use of abbreviations, acronyms, slang words, emoticons, and hashtags. In
    this case, the main emotion may no longer come from the text itself, but rather
    from the emoticons used ([http://dl.acm.org/citation.cfm?id=1628969](http://dl.acm.org/citation.cfm?id=1628969)),
    though some studies showed that the emoticons may sometimes lead to inadequate
    predictions in sentiment ([https://arxiv.org/pdf/1511.02556.pdf](https://arxiv.org/pdf/1511.02556.pdf)).
    Emojis are even broader than emoticons as they include pictures of animals, transportation,
    business icons, and so on. Also, while emoticons can easily be retrieved through
    simple regular expressions, emojis are usually encoded in Unicode and are more
    difficult to extract without a dedicated library.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `Emoji4J` library is easy to use (although computationally expensive) and
    given some text with emojis/emoticons, we can either `codify` - replace Unicode
    values with actual code names - or `clean` - simply remove any emojis.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![Massaging Twitter data](img/B05261_11_01.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Emoji parsing'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'So firstly, let''s clean our text from any junk (special characters, emojis,
    accents, URLs, and so on) to access plain English content:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s also codify and extract all emojis and emoticons and keep them aside
    as a list:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Writing these methods inside an *implicit class* means that they can be applied
    directly a String through a simple import statement.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![Massaging Twitter data](img/image_11_002.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Twitter parsing'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Using the Stanford NLP
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our next step is to pass our cleaned text through a *Sentiment Annotator*.
    We use the Stanford NLP library for that purpose:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We create a Stanford `annotator` that tokenizes content into sentences (`tokenize`),
    splits sentences (`ssplit`), tags elements (`pos`), and lemmatizes each word (`lemma`)
    before analyzing the overall sentiment:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Any word is replaced by its most basic form, that is, *you're* is replaced with
    *you be* and *aren't you doing* replaced with *be not you do*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: A sentiment spans from *Very Negative* (0.0) to *Very Positive* (4.0) and is
    averaged per sentence. As we do not get more than 1 or 2 sentences per tweet,
    we expect a very small variance; most of the tweets should be *Neutral* (around
    2.0), with only extremes to be scored (below ~1.5 or above ~2.5).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Building the Pipeline
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For each of our Twitter records (stored as JSON objects), we do the following
    things:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Parse the JSON object using `json4s` library
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the date
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the text
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the location and map it to a US state
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clean the text
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract emojis
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lemmatize text
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze sentiment
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We then wrap all these values into the following `Tweet` case class:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As mentioned in previous chapters, creating a new NLP instance wouldn''t scale
    for each record out of our dataset of 10 million records. Instead, we create only
    one `annotator` per `Iterator` (which means one per partition):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Using Timely as a time series database
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we are able to transform raw information into a clean series of Twitter
    sentiment with parameters such as hashtags, emojis, or US states, such a time
    series should be stored reliably and made available for fast query lookups.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: In the Hadoop ecosystem, *OpenTSDB* ([http://opentsdb.net/](http://opentsdb.net/))
    is the default database for storing millions of chronological data points. However,
    instead of using the obvious candidate, we will introduce one you may not have
    come across before, called *Timely* ([https://nationalsecurityagency.github.io/timely/](https://nationalsecurityagency.github.io/timely/)).
    Timely is a recently open sourced project started by the **National Security Agency**
    (**NSA**), as a clone of OpenTSDB, which uses Accumulo instead of HBase for its
    underlying storage. As you may recall, Accumulo supports cell-level security,
    and we will see this later on.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Storing data
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each record is composed of a metric name (for example, hashtag), timestamp,
    metric value (for example, sentiment), an associated set of tags (for example,
    state), and a cell visibility:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For this exercise, we will filter out data for tweets only mentioning Trump
    or Clinton:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we build a `Metric` object with names `io.gzet.state.clinton` and `io.gzet.state.trump`
    and an associated visibility. For the purpose of this exercise, we will assume
    that a junior analyst without the `SECRET` permission will not be granted access
    to highly negative tweets. This allows us to demonstrate Accumulo''s excellent
    cell-level security:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In addition, we will also need to handle *duplicate records*. In the event
    where multiple tweets are received at the exact same time (with potentially different
    sentiments), they will override an existing cell on Accumulo:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We insert data either from a `POST` request or simply by piping data through
    an opened socket back to the Timely server:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Our data is now securely stored in Accumulo and available to anyone with the
    correct access permissions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'We have created a series of input formats to retrieve Timely data back into
    a Spark job. This will not be covered here but can be found in our GitHub repository:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Tip
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the time of writing, Timely is still under active development and, as such,
    does not yet have a clean input/output format that can be used from Spark/MapReduce.
    The only ways to send data are via HTTP or Telnet.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Using Grafana to visualize sentiment
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Timely does not come with a visualization tool as such. However, it does integrate
    well, and securely, with *Grafana* ([https://grafana.net/](https://grafana.net/))
    using the timely-grafana plugin. More information can be found on the Timely website.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Number of processed tweets
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a first simple visualization, we display the number of tweets for both the
    candidates on November 8 and 9, 2016 (UTC):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![Number of processed tweets](img/B05261_11_03.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Timely-processed tweets'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: We observe more and more tweets related to Trump as the results of the election
    are published. On average, we observe around 6 times more Trump-related tweets
    than Clinton-related tweets.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Give me my Twitter account back
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A quick study of the sentiment shows that it's relatively negative (1.3 on an
    average) and there's no significant difference between the tweets of both the
    candidates that would have helped predict the outcome of the US election.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![Give me my Twitter account back](img/B05261_11_04.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Timely-timeseries'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: However, on closer inspection, we find a truly interesting phenomenon. On November
    8, 2016, around 1pm GMT (8am EST, that is, when the first polling stations opened
    in New York), we observe a massive drop-off in the *sentiment variance*. An oddity,
    seen in the preceding figure, which can't be completely explained. We can speculate
    that either the first vote cast officially marked the end of the turbulent presidential
    campaign and was the starting point of a retrospective period after the election
    - perhaps, a more *fact-based* dialog than before - or maybe Trump's advisors
    taking away his Twitter account really was their greatest idea.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we give an example of the versatility of Accumulo security by logging into
    Grafana as another user, this time with no `SECRET` authorization granted. As
    expected, in the proceeding image , the sentiment looks much more positive (as
    extremely negative sentiment is hidden), hence confirming the visibility settings
    on Timely; the elegance of Accumulo speaks for itself:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![Give me my Twitter account back](img/image_11_005.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Timely-timeseries for non-SECRET'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: An example of how to create an Accumulo user can be found in [Chapter 7](ch07.xhtml
    "Chapter 7. Building Communities"), *Building Communities*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the swing states
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The last interesting feature we will leverage from Timely and Grafana is tree
    map aggregations. As all the US states'' names are stored as part of the metric
    attributes, we will create a simple tree map for both the candidates. The size
    of each box corresponds to the number of observations, while the color is relative
    to the observed sentiment:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![Identifying the swing states](img/image_11_006.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Timely - Tree map of the US states for Hillary Clinton'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: When we used the 2-day sentiment average previously, we couldn't differentiate
    between the republican and democrat states as the sentiment was statistically
    flat and relatively bad (1.3 on average). However, if we consider only the day
    prior to the election, then it seems much more interesting because we observed
    much more variance in our sentiment data. In the preceding image, we see Florida,
    North Carolina, and Pennsylvania - 3 of the 12 swing states-showing unexpectedly
    bad sentiment for Hillary Clinton. Could this pattern be an early indicator of
    the election outcome?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Twitter and the Godwin point
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our text content properly cleaned up, we can feed a *Word2Vec* algorithm
    and attempt to understand the words in their actual *context*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Learning context
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As it says on the tin, the *Word2Ve*c algorithm transforms a word into a vector.
    The idea is that similar words will be embedded into similar vector spaces and,
    as such, will look close to one another contextually.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More information about `Word2Vec` algorithm can be found at [https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Well integrated into Spark, a `Word2Vec` model can be trained as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here we extract each tweet as a sequence of words, only keeping records with
    at least `4` distinct words. Note that the list of all words needs to fit in memory
    as it is collected back to the driver as a map of word and vector (as an array
    of float). The vector size and learning rate can be tuned through the `setVectorSize`
    and `setLearningRate` methods respectively.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we use a Zeppelin notebook to interact with our model, sending different
    words and asking the model to obtain the closest synonyms. The results are quite
    impressive:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'While hashtags generally pass through standard NLP unnoticed, they do have
    a major contribution to make to tone and emotion. A tweet marked as neutral can
    be, in fact, much worse than it sounds using hashtags like *#HillaryForPrison*
    or ***#LockHerUp*** . So, let''s attempt to take this into account using an interesting
    feature called *word-vector association*. A common example of this association
    given by the original *Word2Vec* algorithm is shown here:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This can be translated as the following vector:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The nearest point should therefore be `[WOMEN]`. Technically speaking, this
    can be translated as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Saving/retrieving this model can be done as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Visualizing our model
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As our vectors are 100 dimensions wide, they are difficult to represent in a
    graph using traditional methods. However, you may have come across the *Tensor
    Flow* project and its recently open sourced *Embedding Projector* ([http://projector.tensorflow.org/](http://projector.tensorflow.org/)).
    This project offers a nice way to visualize our models due to its ability to quickly
    render high-dimensional data. It's easy to use as well - we simply export our
    vectors as tab-separated data points, load them into a web browser, and voila!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing our model](img/image_11_007.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Embedding project, neighbours of Computer'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '*Embedding Projector* projects high-dimensional vector onto 3D space, where
    each dimension represents one of the first three **principal components** (**PCA**).
    We can also build our own projection where we basically stretch our vectors toward
    four specific directions. In the following representation, we stretch our vectors
    left, right, up, and down to [`Trump`], [`Clinton`], [`Love`], and [`Hate`]:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing our model](img/B05261_11_08.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Embedding project, custom projection'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a greatly simplified vector space, we can more easily understand
    each word and how it relates to its neighbors (`democrat` versus `republican`
    and `love` versus `hate`). For example, with the French election coming up next
    year, we see that France is closer to Trump than it is to Clinton. Could this
    be seen as an early indicator of the upcoming election?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Word2Graph and Godwin point
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You don''t have to play around with the Twitter *Word2Vec* model for very long
    before you come across sensitive terms and references to World War II. In fact,
    this is an occurrence that was originally asserted by Mike Godwin in 1990 as Godwin''s
    Law ([https://www.wired.com/1994/10/godwin-if-2/](https://www.wired.com/1994/10/godwin-if-2/)),
    which states as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '*As an online discussion grows longer, the probability of a comparison involving
    Nazis or Hitler approaches 1*'
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As of 2012, it is even part of the Oxford English Dictionary.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![Word2Graph and Godwin point](img/image_11_009.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The Godwin law'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Building a Word2Graph
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although more of a rhetorical device than an actual mathematical law, Godwin''s
    Law remains a fascinating anomaly and seems to be relevant to the US election.
    Naturally, we will decide to explore the idea further using the graph theory.
    The first step is to broadcast our model back to the executors and parallelize
    our list of words. For each word, we output the top five synonyms and build an
    `Edge` object with word similarity as edge weight. Let''s take a look:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To prove Godwin''s law, we will have to prove that no matter the input node,
    we can always find a path from that node to the *Godwin point*. In mathematical
    terms, this assumes the graph to be *ergodic*. With more than one connected component,
    our graph cannot be ergodic as some nodes will never lead to the Godwin point.
    Therefore:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As we only have one connected component, the next step is to compute the shortest
    path for each node to that Godwin point:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The shortest path algorithm is quite simple and can be easily implemented with
    *Pregel* using the same techniques described in [Chapter 7](ch07.xhtml "Chapter 7. Building
    Communities"), *Building Communities*. The basic approach is to start Pregel on
    the target node (our Godwin point) and send a message back to its incoming edges,
    incrementing a counter at each hop. Each node will always keep the smallest possible
    counter and propagate this value downstream to its incoming edges. The algorithm
    stops when no further edge is found.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'We normalize this distance using a Godwin depth of 16, calculated as the maximum
    of each of the shortest paths:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following figure shows a depth of 4 - we normalize the scores of 0, 1,
    2, 3, and 4 to **0.0**, **0.25**, **0.5**, **0.75**, and **1.0** respectively:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![Building a Word2Graph](img/image_11_010.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The normalized Godwin distance'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we collect each vertex with its associated distance as a map. We can
    easily sort this collection from the most to the least-sensitive word, but we
    will not report our findings here (for obvious reasons!).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: On November 7 and 8, 2016, this map contained all the words from our Twitter
    dictionary, implying a full ergodicity. According to Godwin's Law, any word, given
    enough time, can lead to the Godwin point. We will use this map later in the chapter
    when we build features from Twitter text content.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Random walks
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One way to simulate random walks through the *Word2Vec* algorithm is to treat
    the graph as a series of **Markov chains**. Assuming *N* random walks and a transition
    matrix *T*, we compute the transition matrix *T^N*. Given a state, *S[1]* (meaning
    a word *w[1]*), we extract the probability distribution to jump from *S[1]* to
    an *S[N]* state in *N* given transitions. In practice, given a dictionary of ~100k
    words, a dense representation of such a transition matrix will require around
    50 GB to fit in memory. We can easily build a sparse representation of *T* using
    the `IndexedRowMatrix` class from MLlib:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Unfortunately, there is no built-in method in Spark to perform matrix multiplication
    with sparse support. Therefore, the m2 matrix needs to be dense and must fit in
    memory. A solution can be to decompose this matrix (using SVD) and play with the
    symmetric property of the word2vec matrix (if word *w[1]* is a synonym to *w[2]*,
    then *w[2]* is a synonym to *w[1]*) in order to simplify this process. Using simple
    matrix algebra, one can prove that given a matrix *M*:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![Random walks](img/B05261_11_11.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: and *M* symmetric, then
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![Random walks](img/B05261_11_12.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: for even and odd value of *n* respectively. In theory, we only need to compute
    the multiplication of *S* that is a diagonal matrix. In practice, this requires
    lot of effort and is computationally expensive for no real value (all we want
    is to generate random word association). Instead, we generate random walks using
    our Word2Vec graph, the Pregel API, and a Monte Carlo simulation. This will generate
    word associations starting from a seed `love`. The algorithm stops after 100 iterations
    or when a path reaches our Godwin point. The detail of this algorithm can be found
    in our code repository.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Tip
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is also worth mentioning that a Matrix, *M*, is said to be ergodic (hence
    also proving the Godwin Law) if there exists an integer, *n*, such that M^n> 0.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: A Small Step into sarcasm detection
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detecting sarcasm is an active area of research ([http://homes.cs.washington.edu/~nasmith/papers/bamman+smith.icwsm15.pdf](http://homes.cs.washington.edu/~nasmith/papers/bamman+smith.icwsm15.pdf)).
    In fact, detecting sarcasm is often not easy for humans, so how can it be easy
    for computers? If I say "*We will make America great again*"; without knowing
    me, observing me, or hearing the tone I'm using, how could you know if I really
    meant what I said? Now, if you were to read a tweet from me that says "*We will
    make America great again :(:(:(*", does it help in a sense?
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Building features
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We believe that sarcasm cannot be detected using plain English text only, especially
    not when the plain text fits into less than 140 characters. However, we showed
    in this chapter that emojis can play a major role in the definition of emotion.
    A naive assumption is that a tweet with both positive sentiment and negative emojis
    can potentially lead to sarcasm. In addition to the tone, we also found that some
    words were closer to some ideas/ideologies that can be classified as fairly negative.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '#LoveTrumpsHates'
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have demonstrated that any word can be represented in a highly dimensional
    space between words such as [`clinton`], [`trump`], [`love`], and [`hate`]. Therefore,
    for our first extractor, we build features using the average cosine similarity
    between these words:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We expose this method as a user-defined function so that each tweet can be
    scored against each of these four dimensions:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Scoring Emojis
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can extract all emojis and run a basic word count to retrieve only the most
    used emojis. We can then categorize them into five different groups: `love`, `joy`,
    `joke`, `sad`, and `cry`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提取所有表情符号并运行基本的词频统计，以检索只使用最多的表情符号。然后我们可以将它们分类为五个不同的组：`爱`，`喜悦`，`笑话`，`悲伤`和`哭泣`：
- en: '[PRE39]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Again, we expose this method as a UDF that can be applied to a DataFrame. An
    emoji score of 1.0 will be extremely positive, and 0.0 will be highly negative.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将此方法公开为可以应用于DataFrame的UDF。表情符号得分为1.0将非常积极，而0.0将非常消极。
- en: Training a KMeans model
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练KMeans模型
- en: 'With the UDFs set, we get our initial Twitter DataFrame and build the feature
    vectors:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 设置了UDF后，我们获得了我们的初始Twitter DataFrame并构建了特征向量：
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We normalize our vectors using the `Normalizer` class and feed a KMeans algorithm
    with only five clusters. Compared to [Chapter 10](ch10.xhtml "Chapter 10. Story
    De-duplication and Mutation"), *Story De-duplication and Mutation*, the KMeans
    optimization (in terms of *k*) does not really matter here as we are not interested
    in grouping tweets into categories, but rather detecting outliers (tweets that
    are far away from any cluster center):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`Normalizer`类对向量进行归一化，并将KMeans算法的输入限制为只有五个簇。与[第10章](ch10.xhtml "第10章。故事去重和变异")相比，*故事去重和变异*，这里KMeans优化（以*k*表示）并不重要，因为我们不感兴趣将推文分组到类别中，而是检测异常值（远离任何簇中心的推文）：
- en: '[PRE41]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We recommend the use of the ML package instead of MLlib. There have been huge
    improvements to this package over the past few versions of Spark in terms of dataset
    adoption and catalyst optimization. Unfortunately, there is a major limitation:
    all ML classes are defined as private and cannot be extended. As we want to extract
    the distance alongside the predicted cluster, we will have to build our own Euclidean
    measure as a UDF function:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议使用ML包而不是MLlib。在过去几个Spark版本中，这个包在数据集采用和催化剂优化方面有了巨大的改进。不幸的是，存在一个主要限制：所有ML类都被定义为私有的，不能被扩展。因为我们想要提取预测的簇旁边的距离，我们将不得不构建我们自己的欧几里得测量作为UDF函数：
- en: '[PRE42]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, we predict our clusters and Euclidean distances from our *featured
    tweets* DataFrame and register this DataFrame as a persistent Hive table:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们从我们的*特色推文* DataFrame中预测我们的簇和欧几里得距离，并将此DataFrame注册为持久的Hive表：
- en: '[PRE43]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Detecting anomalies
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测异常
- en: We consider a tweet as abnormal if its feature vector is too far from any known
    cluster center (in terms of Euclidean distance). Since we stored our predictions
    as a Hive table, we can sort all points through a simple SQL statement and only
    take the first few records.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征向量与任何已知簇中心的距离太远（以欧几里得距离表示），我们将认为推文是异常的。由于我们将预测存储为Hive表，我们可以通过简单的SQL语句对所有点进行排序，并只取前几条记录。
- en: 'An example is reported, as follows, when querying Hive from our Zeppelin notebook:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的Zeppelin笔记本查询Hive时，报告了一个示例，如下所示：
- en: '![Detecting anomalies](img/B05261_11_13.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![检测异常](img/B05261_11_13.jpg)'
- en: 'Figure 11: Zeppelin notebook for detecting anomalies'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：用于检测异常的Zeppelin笔记本
- en: 'Without getting into too much detail (abnormal tweets can be sensitive), a
    few examples extracted from Hive queries are listed here:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 不详细介绍（异常推文可能会敏感），以下是从Hive查询中提取的一些示例：
- en: 'good luck today america #vote #imwithher [grimacing]'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '今天祝你好运，美国 #投票 #我和她在一起 [鬼脸]'
- en: this is so great we be america great again [cry, scream]
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这太棒了，我们让美国再次变得伟大 [哭泣，尖叫]
- en: we love you sir thank you for you constant love [cry]
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们爱你先生，谢谢你的不断爱 [哭泣]
- en: 'i can not describe how incredibly happy i am right now #maga [cry, rage]'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '我无法描述我现在有多么开心 #maga [哭泣，愤怒]'
- en: Note, however, that the outliers we found were not all sarcastic tweets. We
    have only just begun our study of sarcasm, and lots of refining (including manual
    work) and probably more advanced models (such as *neural networks*) will be needed
    in order to write a comprehensive detector.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，我们发现的异常值并不都是讽刺性的推文。我们刚刚开始研究讽刺，需要进行大量的细化（包括手动工作），可能还需要更先进的模型（如*神经网络*）才能编写全面的检测器。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'The purpose of this chapter was to cover different topics around time series,
    word embedding, sentiment analysis, graph theory, and anomaly detection. It''s
    worth noting that the tweets used to illustrate the examples in no way reflect
    the authors'' own opinions: "Whether or not America will be great again is out
    of scope here":(:( - sarcasm or not?'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是涵盖关于时间序列、词嵌入、情感分析、图论和异常检测的不同主题。值得注意的是，用来说明示例的推文绝不反映作者自己的观点：“美国是否会再次变得伟大超出了本书的范围”：（：（-讽刺与否？
- en: In the next chapter, we will cover an innovative approach to detect trends out
    of Time Series data using the *TrendCalculus* method. This will be used against
    market data, but can easily be applied in different use cases, including the *Sentiment
    Time Series* we built here.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍一种创新的方法，使用*TrendCalculus*方法从时间序列数据中检测趋势。这将用于市场数据，但可以轻松应用于不同的用例，包括我们在这里构建的*情感时间序列*。
