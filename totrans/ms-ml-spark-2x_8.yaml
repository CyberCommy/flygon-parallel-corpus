- en: Lending Club Loan Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are almost at the end of the book, but the last chapter is going to utilize
    all the tricks and knowledge we covered in the previous chapters. We showed you
    how to utilize the power of Spark for data manipulation and transformation, and
    we showed you the different methods for data modeling, including linear models,
    tree models, and model ensembles. Essentially, this chapter will be the *kitchen
    sink* of chapters, whereby we will deal with many problems all at once, ranging
    from data ingestion, manipulation, preprocessing, outlier handling, and modeling,
    all the way to model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: One of our main goals is to provide a realistic picture of a data scientists'
    daily life--start with almost raw data, explore the data, build a few models,
    compare them, find the best model, and deploy into production--if only it were
    this easy all the time! In this final chapter, we will borrow a real-life scenario
    from Lending Club, a company that provides peer-to-peer loans. We will apply all
    the skills you learned to see if we can build a model that determines the riskiness
    of a loan. Furthermore, we will compare the results with actual Lending Club data
    to evaluate our process.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Lending Club goal is to minimize the investment risk of providing bad loans,
    the loans with a high probability of defaulting or being delayed, but also to
    avoid rejecting good loans and hence losing profits. Here, the main criterion
    is driven by accepted risk - how much risk Lending Club can accept to be still
    profitable.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, for prospective loans, Lending Club needs to provide an appropriate
    interest rate reflecting risk and generating income or provide loan adjustments.
    Therefore, it follows that if a given loan has a high interest rate, we can possibly
    infer that there is more inherent risk than a loan with a lower interest rate.
  prefs: []
  type: TYPE_NORMAL
- en: In our book, we can benefit from the Lending Club experience since they provide
    historical tracking of not only good loans but also bad loans. Furthermore, all
    historical data is available, including final loan statuses representing a unique
    opportunity to fit into the role of a Lending Club data scientist and try to match
    or even beat their prediction models.
  prefs: []
  type: TYPE_NORMAL
- en: We can even go one step further-we can imagine an "autopilot mode". For each
    submitted loan,we can define the investment strategy (that is, how much risk we
    want to accept). The autopilot will accept/reject the loan and propose a machine-generated
    interest rate and compute expected return. The only condition is if you make some
    money using our models, we expect a cut of the profits!
  prefs: []
  type: TYPE_NORMAL
- en: Goal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The overall goal is to create a machine learning application that will be able
    to train models respecting given investment strategy and deploy these models as
    a callable service, processing incoming loan applications. The service will be
    able to decide about a given loan application and compute an interest rate. We
    can define our intentions with a top-down approach starting from business requirements.
    Remember, a good data scientist has a firm understanding of the question(s) being
    asked, which is dependent on understanding the business requirement(s), which
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to define what the investment strategy means and how it optimizes/influences
    our machine learning model creation and evaluation. Then, we will take the model's
    findings and apply them to our portfolio of loans to best optimize our profits
    based on specified investment strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to define a computation of expected return based on the investment strategy,
    and the application should provide the expected return of a lender. This is an
    important loan attribute for investors since it directly connects the loan application,
    investment strategy (that is, risk), and possible profit. We should keep this
    fact in mind, since in real life, the modeling pipelines are used by users who
    are not experts in data science or statistics and who are more interested in more
    high-level interpretation of modeling outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Furthermore, we need means to design and realize a loan prediction pipeline,
    which consists of the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model that is based on loan application data and investment strategy decides
    about the loan status-if the loan should be accepted or rejected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model needs to be robust enough to reject all bad loans (that is, loans
    that would lead to an investment loss), but on the other hand, do not miss any
    good loans (that is, do not miss any investment opportunity).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model should be interpretable-it should provide an explanation as to why
    a loan was rejected. Interestingly, there is a lot of research regarding this
    subject; the interpretability of models with key stakeholders who want something
    more tangible than just *the model said so*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For those interested in further reading regarding model interpretability, Zachary
    Lipton (UCSD) has an outstanding paper titled *The Mythos of Model Interpretability**, *[https://arxiv.org/abs/1606.03490](https://arxiv.org/abs/1606.03490)
    which directly addresses this topic. This is an especially useful paper for those
    data scientists who are constantly in the hot seat of explaining all their magic!
  prefs: []
  type: TYPE_NORMAL
- en: There is another model that recommends the interest rate for accepted loans.
    Based on the specified loan application, the model should decide the best interest
    rate-not too high lose a borrower, but not too low to miss a profit.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we need to decide how to deploy this complex, multi-faceted machine
    learning pipeline. Much like our previous chapter, which combines multiple models
    in a single pipeline, we will take all the inputs we have in our dataset-which
    we will see are very different types-and perform processing, feature extraction,
    model prediction, and recommendations based on our investment strategy: a tall
    order but one that we will accomplish in this chapter!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lending Club provides all available loan applications and their results publicly.
    The data for years 2007-2012 and 2013-2014 can be directly downloaded from [https://www.lendingclub.com/info/download-data.action](https://www.lendingclub.com/info/download-data.action).
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the DECLINED LOAN DATA, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00173.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The downloaded files contain `filesLoanStats3a.CSV` and `LoanStats3b.CSV`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The file we have contains approximately 230 k rows that are split into two
    sections:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Loans that meet the credit policy: 168 k'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loans that do not meet the credit policy: 62 k (note the imbalanced dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As always, it is advisable to look at the data by viewing a sample row or perhaps
    the first 10 rows; given the size of the dataset we have here, we can use Excel
    see at what a row looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00174.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Be careful since the downloaded file can contain a first line with a Lending
    Club download system comment. The best way is to remove it manually before loading
    into Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Data dictionary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Lending Club download page also provides a data dictionary that contains
    explanations of individual columns. Specifically, the dataset contains 115 columns
    with specific meanings, collecting data about borrowers, including their bank
    history, credit history, and their loan application. Furthermore, for accepted
    loans, data includes payment progress or the final state of the loan-if it was
    fully paid or defaulted. One reason why it''s crucial to study the data dictionary
    is to prevent using a column that can possibly pre-hint at the result you are
    trying to predict and thereby result in a model that is inaccurate. The message
    is clear but very important: study and know your data!'
  prefs: []
  type: TYPE_NORMAL
- en: Preparation of the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this Chapter, instead of using Spark shell, we will build two standalone
    Spark applications using Scala API: one for model preparation and the second for
    model deployment. In the case of Spark, the Spark application is a normal Scala
    application with a main method that serves as an entry point for execution. For
    example, here is a skeleton of application for model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Moreover, we will try to extract parts, which can be shared between both applications,
    into a library. This will allow us to follow the DRY (do-not-repeat-yourself)
    principle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Data load
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As usual, the first step involves the loading of data into memory. At this
    point, we can decide to use Spark or H2O data-loading capabilities. Since data
    is stored in the CSV file format, we will use the H2O parser to give us a quick
    visual insight into the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The loaded dataset can be directly explored in the H2O Flow UI. We can directly
    verify the number of rows, columns, and size of data stored in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00175.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Exploration – data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, it is time to explore the data. There are many questions that we can ask,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What target features would we like to model supporting our goals?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the useful training features for each target feature?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which features are not good for modeling since they leak information about target
    features (see the previous section)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which features are not useful (for example, constant features, or features containing
    lot of missing values)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to clean up data? What to do with missing values? Can we engineer new features?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic clean up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'During data exploration, we will execute basic data clean up. In our case,
    we can utilize the power of booth tools together: we use the H2O Flow UI to explore
    the data, find suspicious parts of the data, and transform them directly with
    H2O, or, even better, with Spark.'
  prefs: []
  type: TYPE_NORMAL
- en: Useless columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step is to remove columns that contain unique values per line. Typical
    examples of this are user IDs or transaction IDs. In our case, we will identify
    them manually based on data description:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00176.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next step is to identify useless columns, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Constant columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bad columns (containing only missing values)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code will help us do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00177.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: String columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, it is time to explore different type of columns within our dataset. The
    easy step is to look at columns containing strings-these columns are like ID columns
    since they hold unique values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00178.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The question is whether the `url` feature contains any useful information that
    we can extract. We can explore data directly in H2O Flow and look at some samples
    of data in the feature column in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00179.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see directly that the `url` feature contains only pointers to the Lending
    Club site using the application ID that we already dropped. Hence, we can decide
    to drop it.
  prefs: []
  type: TYPE_NORMAL
- en: Loan progress columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our target goal is to make a prediction of inherent risk based on loan application
    data, but some of the columns contain information about loan payment progress
    or they were assigned by Lending Club itself. In this example, for simplicity,
    we will drop them and focus only on columns that are part of the loan-application
    process. It is important to mention that in real-life scenarios, even these columns
    could carry interesting information (for example, payment progress) usable for
    prediction. However, we wanted to build our model based on the initial application
    of the loan and not when a loan has already been a) accepted and b) there is historical
    payment history that would not be known at the time of receiving the application.
    Based on the data dictionary, we detected the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can directly record all the columns that we need to remove since they
    do not bring any value for modelling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Categorical columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the next step, we will explore categorical columns. The H2O parser marks
    a column as a categorical column only if it contains a limited set of string values.
    This is the main difference from columns that are marked as string columns. They
    contain more than 90 percent of unique values (see, for example, the `url` column
    that we explored in the previous paragraph). Let''s collect a list of all the
    categorical columns in our dataset and also the sparsity of individual features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00180.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can explore individual columns. For example, the "purpose" column contains
    13 categories, and the main purpose of it is debt consolidation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00181.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This column seems valid, but now, we should focus on suspicious columns, that
    is, first high-cardinality columns: `emp_title`, `title`, `desc`. There are several
    observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The highest value for each column is an empty "value". That can mean a missing
    value. However, for these types of column (that is, columns representing a set
    of values) a dedicated level for a missing value makes very good sense. It just
    represents another possible state, "missing". Hence, we can keep it as it is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The "title" column overlaps with the purpose column and can be dropped.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `emp_title` and `desc` columns are purely textual descriptions. In this
    case, we will not treat them as categorical, but apply NLP techniques to extract
    important information later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we will focus on columns starting with "mths_", As the name of the column
    suggests, the column should contain numeric values, but our parser decided that
    the columns are categorical. That could be caused by inconsistencies in collected
    data. For example, when we explore the domain of the "mths_since_last_major_derog"
    column, we can easily spot a reason:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00182.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The most common value in the column is an empty value (that is, the same deficiency
    that we already explored earlier). In this case, we need to decide how to replace
    this value to transform a column to a numeric column: should it be replaced by
    the missing value?'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to experiment with different strategies, we can define a flexible
    transformation for this kind of column. In this situation, we will leave the H2O
    API and switch to Spark and define our own Spark UDF. Hence, as in the previous
    chapters, we will define a function. In this case, a function which for a given
    replacement value and a string, produces a float value representing given string
    or returns the specified value if string is empty. Then, the function is wrapped
    into the Spark UDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: A good practice is to keep our code flexible enough to allow for experimenting,
    but do not make it over complicated. In this case, we simply keep an open door
    for cases that we expect to be explored in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two more columns that need our attention: `int_rate` and `revol_util`.
    Both should be numeric columns expressing percentages; however, if we explore
    them, we can easily see a problem--instead of a numeric value, the column contains
    the "%" sign. Hence, we have two more candidates for column transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00183.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, we will not process the data directly but define the Spark UDF transformation,
    which will transform the string-based rate into a numeric rate. However, in definition
    of our UDF, we will simply use information provided by H2O, which is confirming
    that the list of categories in both columns contains only data suffixed by the
    percent sign:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The defined UDF will be applied later with the rest of the Spark transformations.
    Furthermore, we need to realize that these transformations need to be applied
    during training as well as scoring time. Hence, we will put them into our shared
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Text columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we identified the `emp_title` and `desc` columns as
    targets for text transformation. Our theory is that these columns can carry useful
    information that could help distinguish between good and bad loans.
  prefs: []
  type: TYPE_NORMAL
- en: Missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last step in our data-exploration journey is to explore missing values.
    We already observed that some columns contain a value that represents a missing
    value; however, in this section, we will focus on pure missing values. First,
    we need to collect them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The list contains 111 columns with the number of missing values varying from
    0.2 percent to 86 percent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00184.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There are plenty of columns with five missing values, which can be caused by
    wrong data collection, and we can easily filter them out if they are represented
    in a pattern. For more "polluted columns" (for example, where there are many missing
    values), we need to figure out the right strategy per column based on the column
    semantics described in the data dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: In all these cases, H2O Flow UI allows us to easily and quickly explore basic
    properties of data or even execute basic data cleanup. However, for more advanced
    data manipulations, Spark is the right tool to utilize because of a provided library
    of pre-cooked transformations and native SQL support.
  prefs: []
  type: TYPE_NORMAL
- en: Whew! As we can see, the data clean up, while being fairly laborious, is an
    extremely important task for the data scientist and one that will-hopefully-yield
    good answers to well thought out questions. This process must be carefully considered
    before each and every new problem that is looking to be solved. As the old ad
    age goes, "G*arbage in, garbage out" - *if the inputs are not right, our model
    will suffer the consequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, it is possible to compose all the identified transformations
    together into shared library functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The method takes a Spark DataFrame as an input and applies all identified cleanup
    transformations. Now, it is time to build some models!
  prefs: []
  type: TYPE_NORMAL
- en: Prediction targets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After performing our data cleanup, it''s time to examine our prediction targets.
    Our ideal modeling pipeline includes two models: one that controls acceptance
    of the loan and one that estimates interest rate. Already you should be thinking
    that the first model is a binary classification problem (accept or reject the
    loan) while the second model is a regression problem, where the outcome is a numeric
    value.'
  prefs: []
  type: TYPE_NORMAL
- en: Loan status model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first model needs to distinguish between bad and good loans. The dataset
    already provides the `loan_status`column, which is the best feature representation
    of our modeling target. Let's look at the column in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loan status is represented by a categorical feature that has seven levels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fully paid: borrower paid the loan and all interest'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Current: the loan is actively paid in accordance with a plan'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In grace period: late payment 1-15 days'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Late (16-30 days): late payment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Late (31-120 days): late payment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Charged off: a loan is 150 days past the due date'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Default: a loan was lost'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the first modeling goal, we need to distinguish between good and bad loans.
    Good loans could be the loans that were fully paid. The rest of the loans could
    be considered as bad loans with the exception of current loans that need more
    attention (for example, survival analysis) or we could simply remove all rows
    that contain the "Current" status. For transformation of the `loan_status` feature
    into a binary feature, we will define a Spark UDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can explore the distribution of individual categories in more detail. In
    the following screenshot,we can also see that the ratio between good and bad loans
    is highly unbalanced. We need to keep this fact during the training and evaluation
    of the model, since we would like to optimize the recall probability of detection
    of the bad loan:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00185.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Properties of the loan_status column.
  prefs: []
  type: TYPE_NORMAL
- en: Base model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we have prepared the target prediction column and cleaned up
    the input data, and we can now build a base model. The base model gives us basic
    intuition about data. For this purpose, we will use all columns except columns
    detected as being useless. We will also skip handling of missing values, since
    we will use H2O and the RandomForest algorithm, which can handle missing values.
    However, the first step is to prepare a dataset with the help of defined Spark
    transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We will simply drop all known columns that are correlated with our target prediction
    column, all high categorical columns that carry a text description (except `title` and
    `desc`, which we will use later), and apply all basic the cleanup transformation
    we identified in the sections earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step involves splitting data into two parts. As usual, we will keep
    the majority of data for training and rest for model validation and transforming
    into a form that is accepted by H2O model builders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: With the cleanup data, we can easily build a model. We will blindly use the
    RandomForest algorithm since it gives us direct insight into data and importance
    of individual features. We say "blindly" because as you recall from [Chapter 2](part0038.html#147LC0-d18ba71168a441bd917775fac13ca893),
    *Detecting Dark Matter - The Higgs-Boson Particle,* a RandomForest model can take
    inputs of many different types and build many different trees using different
    features, which gives us confidence to use this algorithm as our out-of-the-box
    model, given how well it performs when including all our features. Thus, the model
    also defines a baseline that we would like to improve by engineering new features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use default settings. RandomForest brings out-of-the-box validation
    schema based on out-of-bag samples, so we can skip cross-validation for now. However,
    we will increase the number of constructed trees, but limit the model builder
    execution by a `Logloss`-based stopping criterion. Furthermore, we know that the
    prediction target is imbalanced where the number of good loans is much higher
    than bad loans, so we will ask for upsampling minority class by enabling the `balance_classes` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'When the model is built, we can explore its quality, as we did in the previous
    chapters, but our first look will be at the importance of the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00186.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The most surprising fact is that the zip_code and **collection_recovery_fee** features
    have a much higher importance than the rest of the columns. This is suspicious
    and could indicate direct correlation between the column and the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: We can revisit the data dictionary, which describes the **zip_code** column
    as "the first three numbers of the zip code provided by the borrower in the loan
    application" and the second column as "post-charge off collection fee". The latter
    one indicates a direct connection to the response column since "good loans" will
    have a value equal to zero. We can also validate this fact by exploring the data.
    In the case of zip_code, there is no obvious connection to the response column.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will therefore do one more model run, but in this case, we will try to ignore
    both the `zip_code` and `collection_recovery_fee` columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After the model is built, we can explore the variable importance graph again
    and see a more meaningful distribution of the importance between the variables.
    Based on the graph, we can decide to use only top 10 input features to simplify
    the model''s complexity and decrease modeling time. It is important to say that
    we still need to consider the removed columns as relevant input features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00187.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Base model performance**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can look at the model performance of the created model. We need to
    keep in mind that in our case, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: The performance of the model is reported on out-of-bag samples, not on unseen
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used fixed parameters as the best guess; however, it would be beneficial
    to perform a random parameter search to see how the input parameters influence
    the model's performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00188.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the AUC measured on out-of-bag sample of data is quite high.
    Even individual class errors are for a selected threshold, which minimizes individual
    classes accuracy, low. However, let''s explore the performance of the model on
    the unseen data. We will use the prepared part of the data for validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00189.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The computed model metrics can be explored visually in the Flow UI as well.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the AUC is lower, and individual class errors are higher, but
    are still reasonably good. However, all the measured statistical properties do
    not give us any notion of "business" value of the model-how much money was lent,
    how much money was lost for defaulted loans, and so on. In the next step, we will
    try to design ad-hoc evaluation metrics for the model.
  prefs: []
  type: TYPE_NORMAL
- en: What does it mean by the statement that the model made a wrong prediction? It
    can consider a good loan application as bad, which will result in the rejection
    of the application. That also means the loss of profit from the loan interest.
    Alternatively, the model can recommend a bad loan application as good, which will
    cause the loss of the full or partial amount of lent money. Let's look at both
    situations in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'The former situation can be described by the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The function returns the amount of money lost if a model predicted a bad loan,
    but the actual data indicated that the loan was good. The returned amount considers
    the predicted interest rate and term. The important variables are `predGoodLoanProb`,
    which holds the model's predicted probability of considering the actual loan as
    a good loan, and `predThreshold`, which allows us to set up a bar when the probability
    of predicting a good loan is good enough for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a similar way, we will describe the latter situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: It is good to realize that we are just following the confusion matrix definition
    for false positives and false negatives and applying our domain knowledge of input
    data to define ad-hoc model evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it is time to utilize both functions and define `totalLoss`-how much money
    we can lose on accepting bad loans and missing good loans if we follow our model''s
    recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `totalLoss` function is defined for a Spark DataFrame and a threshold.
    The Spark DataFrame holds actual validation data and prediction composed of three
    columns: actual prediction for default threshold, the probability of a good loan,
    and the probability of a bad loan. The threshold helps us define the right bar
    for the good loan probability; that is, if the good loan probability is higher
    than threshold, we can consider that the model recommends to accept the loan.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the function for different thresholds, including one that minimizes
    individual class errors, we will get the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00190.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From the table, we can see that the lowest total loss for our metrics is based
    on threshold `0.85`, which represents quite a conservative strategy, which focusing
    on avoiding bad loans.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can even define a function that finds the minimal total loss and corresponding
    threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00191.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Based on the reported results, we can see that the model minimizes the total
    loss for threshold ~ `0.85`, which is higher than the default threshold identified
    by the model (F1 = 0.66). However, we still need to realize that this is just
    a base naive model; we did not perform any tuning and searching of right training
    parameters. We still have two fields, `title` and `desc`, which we can utilize.
    It's time for model improvements!
  prefs: []
  type: TYPE_NORMAL
- en: The emp_title column transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first column, `emp_title`, describes the employment title. However, it is
    not unified-there are multiple versions with the same meaning ("Bank of America"
    versus "bank of america") or a similar meaning ("AT&T" and "AT&T Mobility"). Our
    goal is to unify the labels into a basic form, detect similar labels, and replace
    them by a common title. The theory is the employment title has a direct impact
    on the ability to pay back the loan.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic unification of labels is a simple task-transform labels into lowercase
    form and drop all non-alphanumeric characters ("&" or "."). For this step, we
    will use the Spark API for user-defined functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step defines a tokenizer, a function that splits a sentence into individual
    tokens and drops useless and stop words (for example, too short words or conjunctions).
    In our case, we will make the minimal token length and list of stop words flexible
    as input parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: It is important to mention that Spark API provides a list of stop words already
    as part of `StopWordsRemover` transformation. Our definition of `tokenizeUdf` directly
    utilizes the provided list of English stop words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it is time to look at the column in more detail. We will start by selecting
    the `emp_title` column from the already created DataFrame, `loanStatusBaseModelDf`,
    and apply the two functions defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have a Spark DataFrame with two important columns: the first contains
    unified `emp_title` and the second one is represented by a list of tokens. With
    the help of Spark SQL API, we can easily compute the number of unique values in
    the `emp_title` column or the number of unique tokens with a frequency of more
    than 100 (that is, it means the word was used in more than 100 `emp_titles`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00192.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You can see that there are many unique values in the `emp_title` column. On
    the other hand, there are only `717` tokens that are repeated over and over. Our
    goal to *compress* the number of unique values in the column and group similar
    values together. We can experiment with different methods. For example, encode
    each `emp_title` with a representative token or use a more advanced technique
    based on the Word2Vec algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we combined DataFrame query capabilities with the computation
    power of raw RDDs. Many queries can be expressed with powerful SQL-based DataFrame
    APIs; however, if we need to process structured data (such as the sequence of
    string tokens in the preceding example), often the RDD API is a quick way to go.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the second option. The Word2Vec algorithm transforms text features
    into a vector space where similar words are closed together with respect to cosine
    distance of corresponding vectors representing the words. That's a nice property;
    however, we still need to detect "groups of similar words". For this task, we
    can simply use the KMeans algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to create the Word2Vec model. Since we have data in a Spark
    DataFrame, we will simply use the Spark implementation from the `ml` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The algorithm input is defined by a sequence of tokens representing sentences
    stored in the "tokens" column. The `outputCol` parameter defines the output of
    the model if it is used to transform the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00193.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From the output of transformation, you can directly see that the DataFrame output
    contains not only the `emp_title` and `emp_title_tokens` input columns, but also
    the `emp_title_w2vVector` column, which represents the output of the w2vModel
    transformation.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to mention that the Word2Vec algorithm is defined only for words,
    but the Spark implementation transforms sentences (that is, the sequence of words)
    into a vector as well by averaging all the word vectors that the sentence represents.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, we will build a K-means model to partition a vector space
    representing individual employment titles into a predefined number of clusters.
    Before doing this, it''s important to think about why this would be a good thing
    to do in the first place. Think about the many different variations of saying
    "Software Engineer" that you know of: Programmer Analyst, SE, Senor Software Engineer,
    and so on. Given these variations that all essentially mean the same thing and
    will be represented by similar vectors, clustering provides us with a means to
    group similar titles together. However, we need to specify how many K clusters
    we should detect-this needs more experimentation, but for simplicity, we will
    try `500` clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The model allows us to transform the input data and explore the clusters. The
    cluster number is stored in a new column called `emp_title_cluster`.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying the number of clusters is tricky given that we are dealing with the
    unsupervised world of machine learning. Often, practitioners will use a simple
    heuristic known as the elbow method( refer the following link: [https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)),
    which basically runs through many K-means models, increasing the number of K-clusters
    as a function of the heterogeneity (uniqueness) among each of the clusters. Usually,
    there is a diminishing gain as we increase the number of K-clusters and the trick
    is to find where the increase becomes marginal to the point where the benefit
    is no longer "worth" the run time.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, there are some information criteria statistics known as **AIC**
    (**Akaike Information Criteria**) ([https://en.wikipedia.org/wiki/Akaike_information_criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion))
    and **BIC** (**Bayesian Information Criteria**) ([https://en.wikipedia.org/wiki/Bayesian_information_criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion))
    that those of you who are interested should look into for further insight. Note
    that at of the time of writing this book, Spark has yet to implement these information
    criteria, and hence, we will not cover this in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00194.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Additionally, we can explore words associated with a random cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00195.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Look at the preceding cluster and ask yourself, "Do these titles seem like a
    logical cluster?" Perhaps more training may be required, or perhaps we need to
    consider further feature transformations, such as running an n-grammer, which
    can identify sequences of words that occur with a high degree of frequency. Interested
    parties can check out the n-grammer section in Spark here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, the `emp_title_cluster` column defines a new feature that we will
    use to replace the original `emp_title` column. We also need to remember all the
    steps and models we used in the process of the column preparation, since we will
    need to reproduce them to enrich the new data. For this purpose, the Spark pipeline
    is defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The first two pipeline steps represent the application of user-defined functions.
    We used the same trick that was used in [Chapter 4](part0080.html#2C9D00-d18ba71168a441bd917775fac13ca893), *Predicting
    Movie Reviews Using NLP and Spark Streaming,* to wrap an UDF into a Spark pipeline
    transformer with help of the defined `UDFTransformer` class. The remaining steps
    represent models that we built.
  prefs: []
  type: TYPE_NORMAL
- en: The defined `UDFTransformer` class is a nice way to wrap UDF into Spark pipeline
    transformer, but for Spark, it is a black box and it cannot perform all the powerful
    transformations. However, it could be replaced by an existing concept of the Spark
    SQLTransformer, which can be understood by the Spark optimizer; on the other hand,
    its usage is not so straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline still needs to be fit; however, in our case, since we used only
    Spark transformers, the fit operation bundles all the defined stages into the
    pipeline model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it is time to evaluate the impact of the new feature on the model quality.
    We will repeat the same steps we did earlier during the evaluation of the quality
    of the base model:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare training and validation parts and enrich them with a new feature, `emp_title_cluster`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute total the money loss and find the minimal loss.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the first step, we will reuse the prepared train and validation parts;
    however, we need to transform them with the prepared pipeline and drop the "raw"
    column, `desc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'When we have the data ready, we can repeat the model training with the same
    parameters we used for the base model training, except that we use the prepared
    input training part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can evaluate the model on the validation data and compute our evaluation
    metrics based on the total money loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00196.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that employing an NLP technique to detect a similar job title slightly
    improves the quality of the model, resulting in decreasing the total dollar loss
    computed on the unseen data. However, the question is whether we can improve our
    model even more based on the `desc` column, which could include useful information.
  prefs: []
  type: TYPE_NORMAL
- en: The desc column transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next column we will explore is `desc`. Our motivation is still to mine any
    possible information from it and improve model's quality. The `desc` column contains
    purely textual descriptions for why the lender wishes to take out a loan. In this
    case, we are not going to treat them as categorical values since most of them
    are unique. However, we will apply NLP techniques to extract important information.
    In contrast to the `emp_title` column, we will not use the Word2Vec algorithm,
    but we will try to find words that are distinguishing bad loans from good loans.
  prefs: []
  type: TYPE_NORMAL
- en: For this goal, we will simply decompose descriptions into individual words (that
    is, tokenization) and assign weights to each used word with the help of tf-idf
    and explore which words are most likely to represent good loans or bad loans.
    Instead of tf-idf, we could help just word counts, but tf-idf values are a better
    separation between informative words (such as "credit") and common words (such
    as "loan").
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the same procedure we performed in the case of the `emp_title` column-defining
    transformations that transcribe the `desc` column into a list of unified tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The transformation prepares a `desc_tokens` column that contains a list of
    words for each input `desc` value. Now, we need to translate string tokens into
    numeric form to build the tf-idf model. In this context, we will use `CountVectorizer`,
    which extracts the vocabulary of used words and generates a numeric vector for
    each row. A position in a numeric vector corresponds to a single word in the vocabulary
    and the value represents the number of occurrences. Un which g tokens into a numeric
    vector, since we would like to keep the relation between a number in the vector
    and token representing it. In contrast to Spark HashingTF, `CountVectorizer` preserves
    bijection between a word and the number of its occurrences in a generated vector.
    We will reuse this capability later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the IDF model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'When we put all the defined transformations into a single pipeline, we can
    directly train it on input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have a pipeline model that can transform a numeric vector for each
    input `desc` value. Furthermore, we can inspect the pipeline model''s internals
    and extract vocabulary from the computed `CountVectorizerModel` and individual
    word weights from `IDFModel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00197.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, we know individual word weights; however, we still need to compute
    which words are used by "good loans" and "bad loans". For this purpose, we will
    utilize information about word frequencies computed by the prepared pipeline model
    and stored in the `desc_vector` column (in fact, this is an output of `CountVectorizer`).
    We will sum all these vectors separately for good and then for bad loans:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Having computed values, we can easily find words that are used only by good/bad
    loans and explore their computed IDF weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00198.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The produced information does not seem helpful, since we got only very rare
    words that allow us detect only a limited number of highly specific loan descriptions.
    However, we would like to be more generic and find more common words that are
    used by both loan types, but will still allow us to distinguish between bad and
    good loans.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we need to design a word score that will target words with high-frequency
    usage in good (or bad) loans but penalize rare words. For example, we can define
    it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'If we apply the word score method on each word in the vocabulary, we will get
    a sorted list of words based on the descending score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00199.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on the produced list, we can identify interesting words. We can take
    10 or 100 of them. However, we still need to figure out what to do with them.
    The solution is easy; for each word, we will generate a new binary feature-1 if
    a word is present in the `desc` value; otherwise, 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can test our idea on the prepared training and validation sample and measure
    the quality of the model. Again, the first step is to prepare the augmented data
    with a new feature. In this case, a new feature is a vector that contains binary
    features generated by descWordEncoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we just need to compute the model''s quality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00200.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the new feature helps and improves the precision of our model.
    On the other hand, it also opens a lot of space for experimentation-we can select
    different words, or even use IDF weights instead of binary values if the word
    is part of the `desc` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize our experiments, we will compare the computed results for the
    three models we produced: (1) the base model, (2) the model trained on the data
    augmented by the `emp_title` feature, and (3) the model trained on the data enriched
    by the `desc` feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00201.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Our small experiments demonstrated the powerful concept of feature generation.
    Each newly generated feature improved the quality of the base model with respect
    to our model-evaluation criterion.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can finish with exploration and training of the first model
    to detect good/bad loans. We will use the last model we prepared since it gives
    us the best quality. There are still many ways to explore data and improve our
    model quality; however, now, it is time to build our second model.
  prefs: []
  type: TYPE_NORMAL
- en: Interest RateModel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second model predicts the interest rate of accepted loans. In this case,
    we will use only the part of the training data that corresponds to good loans,
    since they have assigned a proper interest rate. However, we need to understand
    that the remaining bad loans could carry useful information related to the interest
    rate prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the rest of the cases, we will start with the preparation of training
    data. We will use initial data, filter out bad loans, and drop string columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: In the next step, we will use the capabilities of H2O random hyperspace search
    to find the best GBM model in a defined hyperspace of parameters. We will also
    constrain the search by additional stopping criteria based on the requested model
    precision and overall search time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to define common GBM model builder parameters, such as training,
    validation datasets, and response column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step involves definition of hyperspace of parameters to explore. We
    can encode any interesting values, but keep in mind that the search could use
    any combination of parameters, even those that are useless:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will define how to traverse the defined hyperspace of parameters. H2O
    provides two strategies: a simple cartesian search that step-by-step builds the
    model for each parameter''s combination or a random search that randomly picks
    the parameters from the defined hyperspace. Surprisingly, the random search has
    quite a good performance, especially if it is used to explore a huge parameter
    space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we will also limit the search by two stopping conditions: the
    model performance based on RMSE and the maximum runtime of the whole grid search.
    At this point, we have defined all the necessary inputs, and it is time to launch
    the hyper search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the search is a set of models called `grid`. Let''s find one
    with the lowest RMSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00202.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can define our evaluation criteria and select the right model not only
    based on selected model metrics, but also consider the term and difference between
    predicted and actual value, and optimize the profit. However, instead of that,
    we will trust our search strategy that it found the best possible model and directly
    jump into deploying our solution.
  prefs: []
  type: TYPE_NORMAL
- en: Using models for scoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we explored different data processing steps, and built
    and evaluated several models to predict the loan status and interest rates for
    the accepted loans. Now, it is time to use all built artifacts and compose them
    together to score new loans.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple steps that we need to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: Data cleanup
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `emp_title` column preparation pipeline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `desc` column transformation into a vector representing significant words
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The binomial model to predict loan acceptance status
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The regression model to predict loan interest rate
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To reuse these steps, we need to connect them into a single function that accepts
    input data and produces predictions involving loan acceptance status and interest
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scoring functions is easy-it replays all the steps that we did in the previous
    chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: We use all definitions that we prepared before-`basicDataCleanup` method, `empTitleTransformer`,
    `loanStatusModel`, `intRateModel`-and apply them in the corresponding order.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the definition of the `scoreLoan` functions, we do not need to
    remove any columns. All the defined Spark pipelines and models use only features
    they were defined on and keep the rest untouched.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method uses all the generated artifacts. For example, we can score the
    input data in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00203.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, to score new loans independently from our training code, we still
    need to export trained models and pipelines in some reusable form. For Spark models
    and pipelines, we can directly use Spark serialization. For example, the defined
    `empTitleTransormer` can be exported in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: We also defined the transformation for the `desc` column as a `udf` function,
    `descWordEncoderUdf`. However, we do not need to export it, since we defined it
    as part of our shared library.
  prefs: []
  type: TYPE_NORMAL
- en: 'For H2O models, the situation is more complicated since there are several ways
    of model export: binary, POJO, and MOJO. The binary export is similar to the Spark
    export; however, to reuse the exported binary model, it is necessary to have a
    running instance of the H2O cluster. This limitation is removed by the other methods.
    The POJO exports the model as Java code, which can be compiled and run independently
    from the H2O cluster. Finally, the MOJO export model is in a binary form, which
    can be interpreted and used without running the H2O cluster. In this chapter,
    we will use the MOJO export, since it is straightforward and also the recommended
    method for model reuse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also export the Spark schema that defines the input data. This will
    be useful for the definition of a parser of the new data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `saveSchema` method processes a given schema and removes all metadata.
    This is not common practice. However, in this case, we will remove them to save
    space.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to mention that the data-creation process from the H2O
    frame implicitly attaches plenty of useful statistical information to the resulting
    Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Model deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model deployment is the most important part of model life cycle. At this
    stage, the model is fed by real-life data and produce results that can support
    decision making (for example, accepting or rejecting a loan).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will build a simple application combining the Spark streaming
    the models we exported earlier and shared code library, which we defined while
    writing the model-training application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The latest Spark 2.1 introduces structural streaming, which is built upon the
    Spark SQL and allows us to utilize the SQL interface transparently with the streaming
    data. Furthermore, it brings a strong feature in the form of "exactly-once" semantics,
    which means that events are not dropped or delivered multiple times. The streaming
    Spark application has the same structure as a "regular" Spark application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three important parts: (1) The creation of input stream, (2) The
    transformation of the created stream, and (3) The writing resulted stream.'
  prefs: []
  type: TYPE_NORMAL
- en: Stream creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several ways to create a stream, described in the Spark documentation
    ([https://spark.apache.org/docs/2.1.1/structured-streaming-programming-guide.html)](https://spark.apache.org/docs/2.1.1/structured-streaming-programming-guide.html)),
    including socket-based, Kafka, or file-based streams. In this chapter, we will
    use file-based streams, streams that are pointed to a directory and deliver all
    the new files that appear in the directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, our application will read CSV files; thus, we will connect the stream
    input with the Spark CSV parser. We also need to configure the parser with the
    input data schema, which we exported from the mode-training application. Let''s
    load the schema first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: The `loadSchema` method modifies the loaded schema by marking all the loaded
    fields as nullable. This is a necessary step to allow input data to contain missing
    values in any column, not only in columns that contained missing values during
    model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, we will directly configure a CSV parser and the input stream
    to read CSV files from a given data folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The CSV parser needs a minor configuration to set up the format for timestamp
    features and representation of missing values. At this point, we can even explore
    the structure of the stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00204.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Stream transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The input stream publishes a similar interface as a Spark DataSet; thus, it
    can be transformed via a regular SQL interface or machine learning transformers.
    In our case, we will reuse all the trained models and transformation that were
    saved in the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will load `empTitleTransformer`-it is a regular Spark pipeline transformer
    that can be loaded with help of the Spark `PipelineModel` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The `loanStatus` and `intRate` models were saved in the H2O MOJO format. To
    load them, it is necessary to use the `MojoModel` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we have all the necessary artifacts ready; however, we cannot
    use H2O MOJO models directly to transform Spark streams. However, we can wrap
    them into a Spark transformer. We have already defined a transformer called UDFTransfomer
    in [Chapter 4](part0080.html#2C9D00-d18ba71168a441bd917775fac13ca893), *Predicting
    Movie Reviews Using NLP and Spark Streaming* so we will follow a similar pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The defined `MojoTransformer` supports binomial and regression MOJO models.
    It accepts a Spark dataset and enriches it by new columns: two columns holding
    true/false probabilities for binomial models and a single column representing
    the predicted value of the regression model. This is reflected in `transform` method,
    which is using the MOJO wrapper `modelUdf` to transform the input dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'dataset.select(*col*(**"*"**), *modelUdf*(*struct*(args: _*)).as(*outputCol*))'
  prefs: []
  type: TYPE_NORMAL
- en: The `modelUdf` model implements the transformation from the data represented
    as Spark Row into a format accepted by MOJO, the call of MOJO, and the transformation
    of the MOJO prediction into a Spark Row format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The defined `MojoTransformer` allows us to wrap the loaded MOJO models into
    the Spark transformer API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we have all the necessary building blocks ready, and we can
    apply them on the input stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The code first calls the shared library function `basicDataCleanup` and then
    transform the `desc` column with another shared library function, `descWordEncoderUdf`:
    both cases are implemented on top of Spark DataSet SQL interfaces. The remaining
    steps will apply defined transformers. Again, we can explore the structure of
    the transformed stream and verify that it contains fields introduced by our transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00205.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that there are several new fields in the schema: representation
    of the empTitle cluster, the vector of denominating words, and model predictions.
    Probabilities are from the loab status model and the real value from the interest
    rate model.'
  prefs: []
  type: TYPE_NORMAL
- en: Stream output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark provides the so-called "Output Sinks" for streams. The sink defines how
    and where the stream is written; for example, as a parquet file or as a in-memory
    table. However, for our application, we will simply show the stream output in
    the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code directly starts the stream processing and waits until the
    termination of the application. The application simply process every new file
    in a given folder (in our case, given by the environment variable, `APPDATADIR`).
    For example, given a file with five loan applications, the stream produces a table
    with five scored events:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00206.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The important part of the event is represented by the last columns, which contain
    predicted values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00207.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we write another file with a single loan application into the folder, the
    application will show another scored batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00208.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this way, we can deploy trained models and corresponding data-processing
    operations and let them score actual events. Of course, we just demonstrated a
    simple use case; a real-life scenario would be much more complex involving a proper
    model validation, A/B testing with the currently used models, and the storing
    and versioning of the models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter summarizes everything you learned throughout the book with end-to-end
    examples. We analyzed the data, transformed it, performed several experiments
    to figure out how to set up the model-training pipeline, and built models. The
    chapter also stresses on the need for well-designed code, which can be shared
    across several projects. In our example, we created a shared library that was
    used at the time of training as well as being utilized during the scoring time.
    This was demonstrated on the critical operation called "model deployment" when
    trained models and related artifacts are used to score unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also brings us to the end of the book. Our goal was to show that
    solving machine learning challenges with Spark is mainly about experimentation
    with data, parameters, models, debugging data / model-related issues, writing
    code that can be tested and reused, and having fun by getting surprising data
    insights and observations.
  prefs: []
  type: TYPE_NORMAL
