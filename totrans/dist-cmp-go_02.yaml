- en: Understanding Goroutines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software development and programming has advanced quite a lot in the past decade.
    Many concepts that were previously considered academic and inefficient are beginning
    to find a place among modern software solutions. Two such concepts are coroutines
    (goroutines in Go) and channels. Conceptually, they have evolved over time and
    they have been implemented differently in each programming language. In many programming
    languages such as Ruby or Clojure, they are implemented as libraries, but in Go,
    they are implemented within the language as a native feature. As we shall see,
    this makes the language really modern, quite efficient, and an advanced programming
    language.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will try to gain an understanding of Go by looking at goroutines
    and the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency and parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go's runtime scheduler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gotchas when using goroutines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency and parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computer and software programs are useful because they do a lot of laborious
    work very fast and can also do multiple things at once. We want our programs to
    be able to do multiple things simultaneously, that is, multitask, and the success
    of a programming language can depend on how easy it is to write and understand
    multitasking programs.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency and parallelism are two terms that we are bound to come across often
    when looking into multitasking and they are often used interchangeably. However,
    they mean two distinctly different things.
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard definitions given on the Go blog ([https://blog.golang.org/concurrency-is-not-parallelism](https://blog.golang.org/concurrency-is-not-parallelism))
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Concurrency**: *Concurrency is about dealing with lots of things at once*.
    This means that we manage to get multiple things done at once in a given period
    of time. However, we will only be doing a single thing at a time. This tends to
    happen in programs where one task is waiting and the program decides to run another
    task in the idle time. In the following diagram, this is denoted by running the
    yellow task in idle periods of the blue task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelism**: *Parallelism is about doing lots of things at once*. This
    means that even if we have two tasks, they are continuously working without any
    breaks in between them. In the diagram, this is shown by the fact that the green
    task is running independently and is not influenced by the red task in any manner:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/66b44862-4bcb-4fcb-a816-4125f4fdf8ca.png)'
  prefs: []
  type: TYPE_IMG
- en: It is important to understand the difference between these two terms. Let's
    look at a few concrete examples to further elaborate upon the difference between
    the two.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's look at the concept of concurrency using a simple example of a few daily
    routine tasks and the way we can perform them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine you start your day and need to get six things done:'
  prefs: []
  type: TYPE_NORMAL
- en: Make hotel reservation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Book flight tickets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Order a dress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pay credit card bills
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write an email
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listen to an audiobook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The order in which they are completed doesn''t matter, and for some of the
    tasks, such as  writing an email or listening to an audiobook, you need not complete
    them in a single sitting. Here is one possible way to complete the tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Order a dress.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write one-third of the email.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make hotel reservation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Listen to 10 minutes of audiobook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pay credit card bills.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write another one-third of the email.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Book flight tickets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Listen to another 20 minutes of audiobook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete writing the email.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue listening to audiobook until you fall asleep.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In programming terms, we have executed the above tasks **concurrently**. We
    had a complete day and we chose particular tasks from our list of tasks and started
    to work on them. For certain tasks, we even decided to break them up into pieces
    and work on the pieces between other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will eventually write a program which does all of the preceding steps concurrently,
    but let''s take it one step at a time. Let''s start by building a program that
    executes the tasks sequentially, and then modify it progressively until it is
    purely concurrent code and uses goroutines. The progression of the program will
    be in three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Serial task execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Serial task execution with goroutines.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Concurrent task execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code overview
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The code will consist of a set of functions that print out their assigned tasks
    as completed. In the cases of writing an email or listening to an audiobook, we
    further divide the tasks into more functions. This can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`writeMail`, `continueWritingMail1`, `continueWritingMail2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`listenToAudioBook`*,* `continueListeningToAudioBook`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serial task execution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s first implement a program that will execute all the tasks in a linear
    manner. Based on the code overview we discussed previously, the following code
    should be straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We take each of the main tasks and start executing them in simple sequential
    order. Executing the preceding code should produce unsurprising output, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Serial task execution with goroutines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We took a list of tasks and wrote a program to execute them in a linear and
    sequential manner. However, we want to execute the tasks concurrently! Let''s
    start by first introducing goroutines for the split tasks and see how it goes.
    We will only show the code snippet where the code actually changed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a possible output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Whoops! That's not what we were expecting. The output from the `continueWritingMail1`,
    `continueWritingMail2`, and `continueListeningToAudioBook` functions is missing;
    the reason being that we are using goroutines. Since goroutines are not waited
    upon, the code in the `main` function continues executing and once the control
    flow reaches the end of the `main` function, the program ends. What we would really
    like to do is to wait in the `main` function until all the goroutines have finished
    executing. There are two ways we can do this—using channels or using `WaitGroup`.
    Since we have [Chapter 3](../Text/Ch03.xhtml), *Channels and Messages*, dedicated
    to channels, let's use `WaitGroup` in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use `WaitGroup`, we have to keep the following in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Use `WaitGroup.Add(int)` to keep count of how many goroutines we will be running
    as part of our logic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `WaitGroup.Done()` to signal that a goroutine is done with its task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `WaitGroup.Wait()` to wait until all goroutines are done.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass `WaitGroup` instance to the goroutines so they can call the `Done()` method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on these points, we should be able to modify the source code to use `WaitGroup`.
    The following is the updated code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is one possible output order; notice how `continueWritingMail1` and `continueWritingMail2`
    were executed at the end after `listenToAudioBook` and `continueListeningToAudioBook`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Concurrent task execution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the final output of the previous section, we can see that all the tasks
    in `listOfTasks` are being executed in serial order, and the last step for maximum
    concurrency would be to let the order be determined by Go runtime instead of the
    order in `listOfTasks`. This might sound like a laborious task, but in reality
    this is quite simple to achieve. All we need to do is add the `go` keyword in
    front of `task(&waitGroup)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is a possible output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If we look at this possible output, the tasks were executed in the following
    order:'
  prefs: []
  type: TYPE_NORMAL
- en: Listen to audiobook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Book flight tickets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Order a dress.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pay credit card bills.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write an email.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make hotel reservations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have a good idea on what concurrency is and how to write concurrent
    code using `goroutines` and `WaitGroup`, let's dive into parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imagine that you have to write a few emails. They are going to be long and
    laborious, and the best way to keep yourself entertained is to listen to music
    while writing them, that is, listening to music "in parallel" to writing the emails.
    If we wanted to write a program that simulates this scenario, the following is
    one possible implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the program might be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The numbers represent the time in terms of `Hour:Minutes:Seconds` and, as can
    be seen, they are being executed in parallel. You might have noticed that the
    code for parallelism looks almost identical to the code for the final concurrency
    example. However, in the function `listenForever`, we are printing `Listening...`
    in an infinite loop. If the preceding example was written without goroutines,
    the output would keep printing `Listening...` and never reach the `writeMail`
    function calls.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how goroutine can be used to run concurrent programs,
    let's look at how Go is allowing us to do this. We shall next look at the scheduler
    used by Go runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Go's runtime scheduler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Go program, along with the runtime, is managed and executed on multiple
    OS threads. The runtime uses a scheduler strategy known as **M:N** scheduler,
    which will schedule M number of goroutines on N number of OS threads. As a result,
    whenever we need to run or switch to a different goroutine, the context switching
    will be fast, and this also enables us to use multiple cores of the CPU for parallel
    computing.
  prefs: []
  type: TYPE_NORMAL
- en: A solid understanding of Go's runtime and scheduler would be quite interesting
    and useful, and now would be a good time to look at them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the Go scheduler''s perspective, there are primarily three entities:'
  prefs: []
  type: TYPE_NORMAL
- en: Goroutine (G)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OS thread or machine (M)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context or processor (P)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at what they do. We will also be looking the partial struct definitions
    of these entities to provide a better idea of how scheduling is implemented and
    how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Goroutine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is the logical unit of execution that contains the actual instructions for
    our program/functions to run. It also contains other important information regarding
    the goroutine, such as the stack memory, which machine (M) it is running on, and
    which Go function called it. The following are some of the elements in the goroutine
    struct that might come in handy for this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: An interesting thing to know is that when our Go program starts, a goroutine
    called main goroutine is first launched, and it takes care of setting up the runtime
    space before starting our program. A typical runtime setup might include things
    such as maximum stack size, enabling garbage collector, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: OS thread or machine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Initially, the OS threads or machines are created by and managed by the OS.
    Later on, the scheduler can request for more OS threads or machines to be created
    or destroyed. It is the actual resource upon which a goroutine will be executed.
    It also maintains information about the main goroutine, the G currently being
    run on it, **thread local storage** (**tls**), and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Context or processor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have a global scheduler which takes care of bringing up new M, registering
    G, and handling system calls. However, it does not handle the actual execution
    of goroutines. This is done by an entity called **Processor**, which has its own
    internal scheduler and a queue called runqueue (`runq` in code) consisting of
    goroutines that will be executed in the current context. It also handles switching
    between various goroutines and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: From Go 1.5 onwards, a Go runtime can have a maximum number of `GOMAXPROCS`
    Ps running at any given point in the program's lifetime. Of course, we can change
    this number by either setting the `GOMAXPROCS` environment variable or by calling
    the `GOMAXPROCS()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling with G, M, and P
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By the time the program is ready to start executing, the runtime has machines
    and processors set up. The runtime would request the OS to start an ample number
    of Machines (M), GOMAXPROCS number of Processors to execute goroutines (G). It
    is important to understand that M is the actual unit of execution and G is the
    logical unit of execution. However, they require P to actually execute G against
    the M. Let''s look at a possible scenario to better explain the scheduling process.
    First let''s look at the components we shall be using for the scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a set of M ready to run: M1...Mn'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also have two Ps: P1 and P2 with runqueues—runq1 and runq2 respectively'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last but not least, we also have 20 goroutines, G1...G20, which we want to execute
    as part of the program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Go''s runtime and all of the components, M1...Mn, P1 and P2, and G1...G20,
    are represented as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9389276b-2d9b-4ead-9efb-8f28f5ffec93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Given that we have two Processors, the global scheduler would ideally distribute
    the goroutines between the two Processors equally. Assume that P1 is assigned
    to work on G1...G10 and and puts them into its runqueue, and similarly P2 puts
    G11...G20 in its runqueue. Next, P1''s scheduler pops a goroutine from its runqueue
    to run, G1, picks a machine to run it on, M1, and similarly P2 runs G11 on M2\.
    This can be illustrated by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5f54f92-848d-4ee8-bd6b-bb0d8874a3e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A process''s internal scheduler is also responsible for switching out the current
    goroutine with the next one that it wants to execute. If everything is going well,
    the scheduler will switch the current goroutine for one of three possible reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Time slice for current execution is over: A process will use **schedtick**
    (it is incremented every time the scheduler is called) to keep track of how long
    the current goroutine has been executing and, once a certain time limit is reached,
    the current goroutine will be put back in the runqueue and the next goroutine
    is picked up for execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Done with execution: Simply put, the goroutine is done executing all of its
    instructions. In this case, it will not be put back in the runqueue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Waiting on system call: In some cases, the goroutine might need to make a system
    system call, and as a result, the goroutine will be blocked. Given that we have
    a handful of processors, it doesn''t make sense to block such an expensive resource.
    The good news is that in Go, the processor is not required to wait on the system
    call; instead it can leave the waiting M and G combo, which will be picked up
    by the global scheduler after the system call. In the meantime, the processor
    can pick another M from the available machines, pick another goroutine from its
    runqueue, and start executing it. This is explained with the help of the following
    diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/27d5e056-2609-4609-8d1c-e0c9437d8e90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous diagram explains that the processor P1 is running goroutine G1
    on machine M1\. G1 will now begin making a system call. This can be illustrated
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1445b62-98e8-44cb-b0f0-74679a42fb4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous diagram explains that the processor P1 detaches itself from machine
    M1 and goroutine G1 due to a system call. P1 picks a new machine, M5, and a new
    goroutine, G9, to execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d547d370-30c7-4e37-b22a-28b74e49ea40.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous diagram, the G1-M1 system call is completed. Now G1 is put back
    in the P1 runqueue and M1 is added to the set of idle machines.
  prefs: []
  type: TYPE_NORMAL
- en: In the last part of this section, we are going to discuss another strategy implemented
    in the scheduler, called **work-stealing**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that processor P1 has 10 goroutines and P2 has 10 goroutines. However,
    it turns out that the goroutines in P1 were quickly completed and now there are
    zero goroutines in P1''s runqueue. It would be a tragedy if P1 were idle and waiting
    for the global scheduler to provide it with more work. With the help of the work-stealing
    strategy, P1 starts checking with other processors and, if another processor has
    goroutines in its runqueue, it will "steal" half of them and start executing them.
    This ensures that we are maximizing the CPU usage for our program. Let''s ask
    two interesting questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What if a processor realizes that it can't steal any more tasks? The procesor
    will wait for a little while expecting new goroutines and, if none are created,
    the processor is killed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can a processor steal more than half of a runqueue? Even if we have many processors
    at work, work-stealing will always take half of the target processor's runqueue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This can be illustrated with the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4bbff37-03f6-4395-a197-b2506b88d511.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram shows two processors, P1 and P2, executing a goroutine
    each from their runqueue on two machines. Let''s consider that the tasks for processor
    P2 complete while P1 is running. This is shown in the figure here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6b8362d-5400-407c-bfca-f5b4146abd5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Processor P2 has exhausted its runqueue, and does not have any more goroutines
    to execute. Thanks to the work-stealing strategy, P2 has "stolen" half of the
    goroutines from P1''s runqueue and can start executing them, as shown in the figure
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5fdef3cb-e16d-4e5c-8e08-ca3fb705debc.png)'
  prefs: []
  type: TYPE_IMG
- en: Gotchas when using goroutines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By this point, we should have developed a good understanding of how goroutines
    and the scheduler works. Let's now look at a few things that may catch us by surprise
    while working with goroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Single goroutine halting the complete program
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We know that goroutines run across multiple threads and multiple cores. So
    what happens when we have a panic in one of the threads? Here is an example that
    would let us simulate such a situation. We will create a lot of similar goroutines,
    whose sole purpose is to take a number and divide it by itself after subtracting
    10 from the denominator. This will work fine for the majority of cases, except
    when the number is `10`. The following code implements the described functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous code can be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Essentially, a lot of goroutines were put in the runqueue, and upon being executed
    in random order, their outputs were printed to the console. However, as soon as
    the goroutine with index == 10 was executed, it raised a panic which was not handled
    by the function, and this resulted in the complete program halting and exiting
    with status code `2`. This shows that even a single error or panic that hasn't
    been handled will halt the complete program!
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it wouldn''t make sense to crash the program because we faced a panic
    that we might have been able to handle graciously. Go allows us to recover from
    a panic with an appropriately named function called `recover`. Let''s look at
    how to use `recover` in the previous code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the preceding code can be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Goroutines aren't predictable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we started by looking at how Go enables us to write code that
    is concurrent and, to an extent, parallel. Then we followed up with a discussion
    on how Go schedules goroutines over machines and processors. It is possible that
    we might be able to reason how the goroutines are going to be distributed over
    machines and processors, which in turn might let us write non-standard or hacky
    Go code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the code from the *Parallelism* section, where we tried to simulate
    listening to music while writing a few emails. Here is the output of the code
    for quick reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can now easily infer that there were at least two Ps, and one of them was
    being used by the goroutine printing `Listening...`, while the other P was handling
    the goroutines related to writing emails.
  prefs: []
  type: TYPE_NORMAL
- en: This is all well and good, however consider the case where `GOMAXPROCS` is set
    to `1` or the system has low hardware capabilities which might result in fewer
    machines. It is possible that this might lead to the goroutine printing `Listening...` run
    forever and never giving control to the other goroutines. In reality, the Go compiler
    should detect this case and accordingly plan the scheduling of goroutines. However,
    it would be better to plan our code so that we do not have to rely on Go's scheduler
    and its current implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Goroutines are concurrent and, to an extent, parallel; however, we should think
    of them as being concurrent. The order of execution of goroutines is not predictable
    and we should not rely on them to be executed in any particular order.
  prefs: []
  type: TYPE_NORMAL
- en: We should also take care to handle errors and panics in our goroutines because
    even though they are being executed in parallel, a panic in one goroutine will
    crash the complete program. Finally, goroutines can block on system calls, however
    this will not block the execution of the program nor slow down the performance
    of the overall program.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at a few of the design concepts behind Go's runtime scheduler to understand
    why all of this happens.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why we haven't discussed channels in this chapter. The
    reason is that by not relying on channels we were able to look at goroutines in
    their most elemental form. This allowed us to dive deeper into the concept and
    implementation of goroutines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we shall be looking at channels and how they further empower
    goroutines.
  prefs: []
  type: TYPE_NORMAL
