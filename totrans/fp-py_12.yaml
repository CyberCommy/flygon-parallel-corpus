- en: Chapter 12. The Multiprocessing and Threading Modules
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we eliminate complex, shared state and design around non-strict processing,
    we can leverage parallelism to improve performance. In this chapter, we'll look
    at the multiprocessing and multithreading techniques that are available to us.
    Python library packages become particularly helpful when applied to algorithms
    that permit lazy evaluation.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: The central idea here is to distribute a functional program across several threads
    within a process or across several processes. If we've created a sensible functional
    design, we don't have complex interactions among application components; we have
    functions that accept argument values and produce results. This is an ideal structure
    for a process or a thread.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: We'll focus on the `multiprocessing` and `concurrent.futures` modules. These
    modules allow a number of parallel execution techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: We'll also focus on process-level parallelism instead of multithreading. The
    idea behind process parallelism allows us to ignore Python's **Global Interpreter
    Lock** (**GIL**) and achieve outstanding performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: For more information on Python's GIL, see [https://docs.python.org/3.3/c-api/init.html#thread-state-and-the-global-interpreter-lock](https://docs.python.org/3.3/c-api/init.html#thread-state-and-the-global-interpreter-lock).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: We won't emphasize features of the `threading` module. This is often used for
    parallel processing. If we have done our functional programming design well, any
    issues that stem from multithreaded write access should be minimized. However,
    the presence of the GIL means that multithreaded applications in **CPython** suffer
    from some small limitations. As waiting for I/O doesn't involve the GIL, it's
    possible that some I/O bound programs might have unusually good performance.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: The most effective parallel processing occurs where there are no dependencies
    among the tasks being performed. With some careful design, we can approach parallel
    programming as an ideal processing technique. The biggest difficulty in developing
    parallel programs is coordinating updates to shared resources.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: When following functional design patterns and avoiding stateful programs, we
    can also minimize concurrent updates to shared objects. If we can design software
    where lazy, non-strict evaluation is central, we can also design software where
    concurrent evaluation is possible.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Programs will always have some strict dependencies where ordering of operations
    matters. In the `2*(3+a)` expression, the `(3+a)` subexpression must be evaluated
    first. However, when working with a collection, we often have situations where
    the processing order among items in the collection doesn't matter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following two examples:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Both of these commands have the same result even though the items are evaluated
    in the reverse order.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, even this following command snippet has the same result:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The evaluation order is random. As the evaluation of each item is independent,
    the order of evaluation doesn't matter. This is the case with many algorithms
    that permit non-strict evaluation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: What concurrency really means
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a small computer, with a single processor and a single core, all evaluations
    are serialized only through the core of the processor. The operating system will
    interleave multiple processes and multiple threads through clever time-slicing
    arrangements.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: On a computer with multiple CPUs or multiple cores in a single CPU, there can
    be some actual concurrent processing of CPU instructions. All other concurrency
    is simulated through time slicing at the OS level. A Mac OS X laptop can have
    200 concurrent processes that share the CPU; this is far more processes than the
    number of available cores. From this, we can see that the OS time slicing is responsible
    for most of the apparently concurrent behavior.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The boundary conditions
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's consider a hypothetical algorithm which has ![The boundary conditions](graphics/B03652_12_01.jpg).
    Assume that there is an inner loop that involves 1,000 bytes of Python code. When
    processing 10,000 objects, we're executing 100 billion Python operations. This
    is the essential processing budget. We can try to allocate as many processes and
    threads as we feel might be helpful, but the processing budget can't change.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个假设的算法，其中有![边界条件](graphics/B03652_12_01.jpg)。假设有一个涉及1000字节Python代码的内部循环。在处理10000个对象时，我们执行了1000亿次Python操作。这是基本的处理预算。我们可以尝试分配尽可能多的进程和线程，但处理预算是不能改变的。
- en: The individual CPython bytecode doesn't have a simple execution timing. However,
    a long-term average on a Mac OS X laptop shows that we can expect about 60 MB
    of code to be executed per second. This means that our 100 billion bytecode operation
    will take about 1,666 seconds, or 28 minutes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 单个CPython字节码没有简单的执行时间。然而，在Mac OS X笔记本上的长期平均值显示，我们可以预期每秒执行大约60MB的代码。这意味着我们的1000亿字节码操作将需要大约1666秒，或28分钟。
- en: 'If we have a dual processor, four-core computer, then we might cut the elapsed
    time to 25 percent of the original total: 7 minutes. This presumes that we can
    partition the work into four (or more) independent OS processes.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一台双处理器、四核的计算机，那么我们可能将经过时间缩短到原始总时间的25%：7分钟。这假设我们可以将工作分成四个（或更多）独立的操作系统进程。
- en: The important consideration here is that our budget of 100 billion bytecodes
    can't be changed. Parallelism won't magically reduce the workload. It can only
    change the schedule to, perhaps, reduce the elapsed time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重要考虑因素是我们的1000亿字节码的预算是不能改变的。并行性不会神奇地减少工作量。它只能改变时间表，也许可以减少经过时间。
- en: Switching to a better algorithm which is ![The boundary conditions](graphics/B03652_12_02.jpg)
    can reduce the workload to 132 MB of operations. At 60 MBps, this workload is
    considerably smaller. Parallelism won't have the kind of dramatic improvements
    that algorithm change will have.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 切换到一个更好的算法可以将工作量减少到132MB的操作。以60MBps的速度，这个工作量要小得多。并行性不会像算法改变那样带来戏剧性的改进。
- en: Sharing resources with process or threads
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与进程或线程共享资源
- en: The OS assures that there is little or no interaction between processes. For
    two processes to interact, some common OS resource must be explicitly shared.
    This can be a common file, a specific shared memory object, or a semaphore with
    a shared state between the processes. Processes are inherently independent, interaction
    is exceptional.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统确保进程之间几乎没有交互。要使两个进程交互，必须显式共享一些公共的操作系统资源。这可以是一个共享文件，一个特定的共享内存对象，或者是进程之间共享状态的信号量。进程本质上是独立的，交互是例外。
- en: Multiple threads, on the other hand, are part of a single process; all threads
    of a process share OS resources. We can make an exception to get some thread-local
    memory that can be freely written without interference from other threads. Outside
    thread-local memory, operations that write to memory can set the internal state
    of the process in a potentially unpredictable order. Explicit locking must be
    used to avoid problems with these stateful updates. As noted previously, the overall
    sequence of instruction executions is rarely, strictly speaking, concurrent. The
    instructions from concurrent threads and processes are generally interleaved in
    an unpredictable order. With threading comes the possibility of destructive updates
    to shared variables and the need for careful locking. With parallel processing
    come the overheads of OS-level process scheduling.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，多个线程是单个进程的一部分；进程的所有线程共享操作系统资源。我们可以例外地获得一些线程本地内存，可以自由写入而不受其他线程干扰。除了线程本地内存，写入内存的操作可能以潜在的不可预测顺序设置进程的内部状态。必须使用显式锁定来避免这些有状态更新的问题。正如之前所指出的，指令执行的整体顺序很少是严格并发的。并发线程和进程的指令通常以不可预测的顺序交错执行。使用线程会带来对共享变量的破坏性更新的可能性，需要仔细的锁定。并行处理会带来操作系统级进程调度的开销。
- en: Indeed, even at the hardware level, there are some complex memory write situations.
    For more information on issues in memory writes, visit [http://en.wikipedia.org/wiki/Memory_disambiguation](http://en.wikipedia.org/wiki/Memory_disambiguation).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，即使在硬件级别，也存在一些复杂的内存写入情况。有关内存写入问题的更多信息，请访问[http://en.wikipedia.org/wiki/Memory_disambiguation](http://en.wikipedia.org/wiki/Memory_disambiguation)。
- en: The existence of concurrent object updates is what raises havoc with trying
    to design multithreaded applications. Locking is one way to avoid concurrent writes
    to shared objects. Avoiding shared objects is another viable design technique.
    This is more applicable to functional programming.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 并发对象更新的存在是设计多线程应用程序时所面临的困难。锁定是避免对共享对象进行并发写入的一种方法。避免共享对象是另一种可行的设计技术。这更适用于函数式编程。
- en: In CPython, the GIL is used to assure that OS thread scheduling will not interfere
    with updates to Python data structures. In effect, the GIL changes the granularity
    of scheduling from machine instructions to Python virtual machine operations.
    Without the GIL, it's possible that an internal data structure might be corrupted
    by the interleaved interaction of competing threads.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPython中，GIL用于确保操作系统线程调度不会干扰对Python数据结构的更新。实际上，GIL将调度的粒度从机器指令改变为Python虚拟机操作。没有GIL，内部数据结构可能会被竞争线程的交错交互所破坏。
- en: Where benefits will accrue
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利益将会产生的地方
- en: A program that does a great deal of calculation and relatively little I/O will
    not see much benefit from concurrent processing. If a calculation has a budget
    of 28 minutes of computation, then interleaving the operations in different ways
    won't have very much impact. Switching from strict to non-strict evaluation of
    100 billion bytecodes won't shrink the elapsed execution time.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: However, if a calculation involves a great deal of I/O, then interleaving CPU
    processing and I/O requests can have an impact on performance. Ideally, we'd like
    to do our computations on some pieces of data while waiting for the OS to complete
    input of the next pieces of data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two approaches to interleaving computation and I/O. They are as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: We can try to interleave I/O and calculation for the entire problem as a whole.
    We might create a pipeline of processing with read, compute, and write as operations.
    The idea is to have individual data objects flowing through the pipe from one
    stage to the next. Each stage can operate in parallel.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can decompose the problem into separate, independent pieces that can be processed
    from the beginning to the end in parallel.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The differences between these approaches aren't crisp; there is a blurry middle
    region that's not clearly one or the other. For example, multiple parallel pipelines
    are a hybrid mixture of both designs. There are some formalisms that make it somewhat
    easier to design concurrent programs. The **Communicating Sequential Processes**
    (**CSP**) paradigm can help design message-passing applications. Packages such
    as `pycsp` can be used to add CSP formalisms to Python.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: I/O-intensive programs often benefit from concurrent processing. The idea is
    to interleave I/O and processing. CPU-intensive programs rarely benefit from attempting
    concurrent processing.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Using multiprocessing pools and tasks
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make non-strict evaluation available in a larger context, the `multiprocessing`
    package introduces the concept of a `Pool` object. We can create a `Pool` object
    of concurrent worker processes, assign tasks to them, and expect the tasks to
    be executed concurrently. As noted previously, this creation does not actually
    mean simultaneous creation of `Pool` objects. It means that the order is difficult
    to predict because we've allowed OS scheduling to interleave execution of multiple
    processes. For some applications, this permits more work to be done in less elapsed
    time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: To make the most use of this capability, we need to decompose our application
    into components for which non-strict concurrent execution is beneficial. We'd
    like to define discrete tasks that can be processed in an indefinite order.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: An application that gathers data from the Internet via web scraping is often
    optimized through parallel processing. We can create a `Pool` object of several
    identical website scrapers. The tasks are URLs to be analyzed by the pooled processes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: An application that analyzes multiple logfiles is also a good candidate for
    parallelization. We can create a `Pool` object of analytical processes. We can
    assign each logfile to an analyzer; this allows reading and analysis to proceed
    in parallel among the various workers in the `Pool` object. Each individual worker
    will involve serialized I/O and computation. However, one worker can be analyzing
    the computation while other workers are waiting for I/O to complete.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Processing many large files
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is an example of a multiprocessing application. We''ll scrape **Common
    Log Format** (**CLF**) lines in web logfiles. This is the generally used format
    for an access log. The lines tend to be long, but look like the following when
    wrapped to the book''s margins:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We often have large numbers of large files that we'd like to analyze. The presence
    of many independent files means that concurrency will have some benefit for our
    scraping process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll decompose the analysis into two broad areas of functionality. The first
    phase of any processing is the essential parsing of the logfiles to gather the
    relevant pieces of information. We''ll decompose this into four stages. They are
    as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: All the lines from multiple source logfiles are read.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, create simple namedtuples from the lines of log entries in a collection
    of files.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The details of more complex fields such as dates and URLs are parsed.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uninteresting paths from the logs are rejected; we can also think of this as
    passing only the interesting paths.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once past the parsing phase, we can perform a large number of analyses. For
    our purposes in demonstrating the `multiprocessing` module, we'll look at a simple
    analysis to count occurrences of specific paths.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: The first portion, reading from source files, involves the most input processing.
    The Python use of file iterators will translate into lower-level OS requests for
    buffering of data. Each OS request means that the process must wait for the data
    to become available.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, we want to interleave the other operations so that they are not waiting
    for I/O to complete. We can interleave operations along a spectrum from individual
    rows to whole files. We'll look at interleaving whole files first, as this is
    relatively simple to implement.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'The functional design for parsing Apache CLF files can look as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We've decomposed the larger parsing problem into a number of functions that
    will handle each portion of the parsing problem. The `local_gzip()` function reads
    rows from locally-cached GZIP files. The `access_iter()` function creates a simple
    `namedtuple` object for each row in the access log. The `access_detail_iter()`
    function will expand on some of the more difficult to parse fields. Finally, the
    `path_filter()` function will discard some paths and file extensions that aren't
    of much analytical value.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Parsing log files – gathering the rows
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is the first stage in parsing a large number of files: reading each file
    and producing a simple sequence of lines. As the logfiles are saved in the `.gzip`
    format, we need to open each file with the `gzip.open()` function instead of the
    `io.open()` function or the `__builtins__.open()` function.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'The `local_gzip()` function reads lines from locally cached files, as shown
    in the following command snippet:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding function iterates through all files. For each file, the yielded
    value is a generator function that will iterate through all lines within that
    file. We've encapsulated several things, including wildcard file matching, the
    details of opening a logfile compressed with the `.gzip` format, and breaking
    a file into a sequence of lines without any trailing `\n` characters.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The essential design pattern here is to yield values that are generator expressions
    for each file. The preceding function can be restated as a function and a mapping
    that applies that function to each file.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several other ways to produce similar output. For example, here is
    an alternative version of the inner `for` loop in the preceding example. The `line_iter()`
    function will also emit lines of a given file:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `line_iter()` function applies the `gzip.open()` function and some line
    cleanup. We can use a mapping to apply the `line_iter()` function to all files
    that match a pattern as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: While this alternative mapping is succinct, it has the disadvantage of leaving
    open file objects lying around waiting to be properly garbage-collected when there
    are no more references. When processing a large number of files, this seems like
    a needless bit of overhead. For this reason, we'll focus on the `local_gzip()`
    function shown previously.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: The previous alternative mapping has the distinct advantage of fitting well
    with the way the `multiprocessing` module works. We can create a worker pool and
    map tasks (such as file reading) to the pool of processes. If we do this, we can
    read these files in parallel; the open file objects will be part of separate processes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的替代映射具有与“多进程”模块配合良好的明显优势。我们可以创建一个工作进程池，并将任务（如文件读取）映射到进程池中。如果这样做，我们可以并行读取这些文件；打开的文件对象将成为单独的进程的一部分。
- en: An extension to this design will include a second function to transfer files
    from the web host using FTP. As the files are collected from the web server, they
    can be analyzed using the `local_gzip()` function.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对这种设计的扩展将包括第二个函数，用于使用FTP从Web主机传输文件。当从Web服务器收集文件时，可以使用`local_gzip()`函数对其进行分析。
- en: The results of the `local_gzip()` function are used by the `access_iter()` function
    to create namedtuples for each row in the source file that describes a file access.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`local_gzip()`函数的结果被`access_iter()`函数使用，为源文件中描述文件访问的每一行创建命名元组。'
- en: Parsing log lines into namedtuples
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将日志行解析为命名元组
- en: Once we have access to all of the lines of each logfile, we can extract details
    of the access that's described. We'll use a regular expression to decompose the
    line. From there, we can build a `namedtuple` object.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们可以访问每个日志文件的所有行，我们就可以提取描述的访问的详细信息。我们将使用正则表达式来分解行。从那里，我们可以构建一个`namedtuple`对象。
- en: 'Here is a regular expression to parse lines in a CLF file:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是解析CLF文件中行的正则表达式：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can use this regular expression to break each row into a dictionary of nine
    individual data elements. The use of `[]`and `"` to delimit complex fields such
    as the `time`, `request`, `referrer`, and `user_agent` parameters are handled
    gracefully by the namedtuple pattern.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个正则表达式将每一行分解为九个单独的数据元素的字典。使用`[]`和`"`来界定复杂字段（如`time`、`request`、`referrer`和`user_agent`参数）的方式由命名元组模式优雅地处理。
- en: 'Each individual access can be summarized as a `namedtuple()` function as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单独的访问可以总结为一个`namedtuple()`函数，如下所示：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We've taken pains to assure that the `namedtuple` function's fields match the
    regular expression group names in the `(?P<name>)` constructs for each portion
    of the record. By making sure the names match, we can very easily transform the
    parsed dictionary into a tuple for further processing.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经费心确保`namedtuple`函数的字段与`(?P<name>)`构造中每条记录的正则表达式组名匹配。通过确保名称匹配，我们可以非常容易地将解析的字典转换为元组以进行进一步处理。
- en: 'Here is the `access_iter()` function that requires each file to be represented
    as an iterator over the lines of the file:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`access_iter()`函数，它要求每个文件都表示为文件行的迭代器：
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output from the `local_gzip()` function is a sequence of sequences. The
    outer sequence consists of individual logfiles. For each file, there is an iterable
    sequence of lines. If the line matches the given pattern, it's a file access of
    some kind. We can create an `Access` namedtuple from the `match` dictionary.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`local_gzip()`函数的输出是一个序列的序列。外部序列由单独的日志文件组成。对于每个文件，都有一个可迭代的行序列。如果行与给定模式匹配，它就是某种文件访问。我们可以从`match`字典中创建一个`Access`命名元组。'
- en: The essential design pattern here is to build a static object from the results
    of a parsing function. In this case, the parsing function is a regular expression
    matcher.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的基本设计模式是从解析函数的结果构建静态对象。在这种情况下，解析函数是一个正则表达式匹配器。
- en: 'There are some alternative ways to do this. For example, we can revise the
    use of the `map()` function as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些替代方法可以做到这一点。例如，我们可以修改`map()`函数的使用如下：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding alternative function embodies just the essential parse and builds
    an `Access` object processing. It will either return an `Access` or a `None` object.
    This differs from the version above that also filters items that don't match the
    regular expression.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的替代函数仅包含基本的解析和构建`Access`对象的处理。它将返回一个`Access`或`None`对象。这与上面的版本不同，后者还过滤了不匹配正则表达式的项目。
- en: 'Here is how we can use this function to flatten logfiles into a single stream
    of the `Access` objects:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们如何使用此函数将日志文件展平为`Access`对象的单个流：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This shows how we can transform the output from the `local_gzip()` function
    into a sequence of the `Access` instances. In this case, we apply the `access_builder()`
    function to the nested iterator of iterable structure that results from reading
    a collection of files.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了我们如何将`local_gzip()`函数的输出转换为`Access`实例的序列。在这种情况下，我们将`access_builder()`函数应用于从读取文件集合中产生的嵌套迭代器的可迭代结构。
- en: Our point here is to show that we have a number of functional styles for parsing
    files. In [Chapter 4](ch04.html "Chapter 4. Working with Collections"), *Working
    with Collections* we showed very simple parsing. Here, we're performing more complex
    parsing, using a variety of techniques.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的重点在于展示我们有许多解析文件的功能样式。在[第4章](ch04.html "第4章。与集合一起工作")中，*与集合一起工作*，我们展示了非常简单的解析。在这里，我们正在执行更复杂的解析，使用各种技术。
- en: Parsing additional fields of an Access object
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解析访问对象的其他字段
- en: The initial `Access` object created previously doesn't decompose some inner
    elements in the nine fields that comprise an access log line. We'll parse those
    items separately from the overall decomposition into high-level fields. It keeps
    the regular expressions for parsing somewhat simpler if we break this down into
    separate parsing operations.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 先前创建的初始`Access`对象并没有分解组成访问日志行的九个字段中的一些内部元素。我们将这些项目分别从整体分解成高级字段。如果我们将这个分解成单独的解析操作，可以使解析正则表达式变得更简单。
- en: 'The resulting object is a `namedtuple` object that will wrap the original `Access`
    tuple. It will have some additional fields for the details parsed separately:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 结果对象是一个`namedtuple`对象，它将包装原始的`Access`元组。它将具有一些额外的字段，用于单独解析的细节：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `access` attribute is the original `Access` object. The `time` attribute
    is the parsed `access.time` string. The `method`, `url`, and `protocol` attributes
    come from decomposing the `access.request` field. The `referrer` attribute is
    a parsed URL. The `agent` attribute can also be broken down into fine-grained
    fields. Here are the fields that comprise agent details:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: These fields reflect the most common syntax for agent descriptions. There is
    considerable variation in this area, but this particular subset of values seems
    to be reasonably common.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll combine three detailed parser functions into a single overall parsing
    function. Here is the first part with the various detail parsers:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We've written three parsers for the HTTP request, the time stamp, and the user
    agent information. The request is usually a three-word string such as `GET /some/path
    HTTP/1.1`. The `parse_request()` function extracts these three space-separated
    values. In the unlikely event that the path has spaces in it, we'll extract the
    first word and the last word as the method and protocol; all the remaining words
    are part of the path.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Time parsing is delegated to the `datetime` module. We've simply provided the
    proper format in the `parse_time()` function.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Parsing the user agent is challenging. There are many variations; we've chosen
    a common one for the `parse_agent()` function. If the user agent matches the given
    regular expression, we'll have the attributes of an `AgentDetails` namedtuple.
    If the user agent information doesn't match the regular expression, we'll simply
    use the `None` value instead.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use these three parsers to build `AccessDetails` instances from the
    given `Access` objects. The main body of the `access_detail_iter()` function looks
    as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We've used a similar design pattern to the previous `access_iter()` function.
    A new object is built from the results of parsing some input object. The new `AccessDetails`
    object will wrap the previous `Access` object. This technique allows us to use
    immutable objects, yet still contain more refined information.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is essentially a mapping from an `Access` object to an `AccessDetails`
    object. We can imagine changing the design to use `map()` as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We've changed the construction of the `AccessDetails` object to be a function
    that returns a single value. We can map that function to the iterable input stream
    of the `Access` objects. This also fits nicely with the way the `multiprocessing`
    module works.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: In an object-oriented programming environment, these additional parsers might
    be method functions or properties of a class definition. The advantage of this
    design is that items aren't parsed unless they're needed. This particular functional
    design parses everything, assuming that it's going to be used.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: A different function design might rely on the three parser functions to extract
    and parse the various elements from a given `Access` object as needed. Rather
    than using the `details.time` attribute, we'd use the `parse_time(access.time)`
    parameter. The syntax is longer, but the attribute is only parsed as needed.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Filtering the access details
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll look at several filters for the `AccessDetails` objects. The first is
    a collection of filters that reject a lot of overhead files that are rarely interesting.
    The second filter will be part of the analysis functions, which we'll look at
    later.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'The `path_filter()` function is a combination of three functions:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Exclude empty paths.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exclude some specific filenames.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exclude files that have a given extension.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'An optimized version of the `path_filter()` function looks as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: For each individual `AccessDetails` object, we'll apply three filter tests.
    If the path is essentially empty or the part includes one of the excluded names
    or the path's final name has an excluded extension, the item is quietly ignored.
    If the path doesn't match any of these criteria, it's potentially interesting
    and is part of the results yielded by the `path_filter()` function.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: This is an optimization because all of the tests are applied using an imperative
    style `for` loop body.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'The design started with each test as a separate first-class filter-style function.
    For example, we might have a function like the following to handle empty paths:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This function simply assures that the path contains a name. We can use the
    `filter()` function as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can write similar tests for the `non_excluded_names()` and `non_excluded_ext()`
    functions. The entire sequence of `filter()` functions will look as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This applies each `filter()` function to the results of the previous `filter()`
    function. The empty paths are rejected; from this subset, the excluded names and
    the excluded extensions are rejected. We can also state the preceding example
    as a series of assignment statements as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This version has the advantage of being slightly easier to expand when we add
    new filter criteria.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The use of generator functions (such as the `filter()` function) means that
    we aren't creating large intermediate objects. Each of the intermediate variables,
    `ne`, `nx_name`, and `nx_ext`, are proper lazy generator functions; no processing
    is done until the data is consumed by a client process.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: While elegant, this suffers from a small inefficiency because each function
    will need to parse the path in the `AccessDetails` object. In order to make this
    more efficient, we will need to wrap a `path.split('/')` function with the `lru_cache`
    attribute.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the access details
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll look at two analysis functions we can use to filter and analyze the individual
    `AccessDetails` objects. The first function, a `filter()` function, will pass
    only specific paths. The second function will summarize the occurrences of each
    distinct path.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll define the `filter()` function as a small function and combine this
    with the built-in `filter()` function to apply the function to the details. Here
    is the composite `filter()` function:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We've defined a rule, the `book_in_path()` attribute, that we'll apply to each
    `AccessDetails` object. If the path is not empty and the first-level attribute
    of the path is `book`, then we're interested in these objects. All other `AccessDetails`
    objects can be quietly rejected.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the final reduction that we''re interested in:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This function will produce a `Counter()` object that shows the frequency of
    each path in an `AccessDetails` object. In order to focus on a particular set
    of paths, we'll use the `reduce_total(book_filter(details))` method. This provides
    a summary of only items that are passed by the given filter.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The complete analysis process
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is the composite `analysis()` function that digests a collection of logfiles:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding command snippet will work with a single filename or file pattern.
    It applies a standard set of parsing functions, `path_filter()`, `access_detail_iter()`,
    `access_iter()`, and `local_gzip()`, to a filename or file pattern and returns
    an iterable sequence of the `AccessDetails` objects. It then applies our analytical
    filter and reduction to that sequence of the `AccessDetails` objects. The result
    is a `Counter` object that shows the frequency of access for certain paths.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: A specific collection of saved `.gzip` format logfiles totals about 51 MB. Processing
    the files serially with this function takes over 140 seconds. Can we do better
    using concurrent processing?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Using a multiprocessing pool for concurrent processing
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One elegant way to make use of the `multiprocessing` module is to create a processing
    `Pool` object and assign work to the various processes in that pool. We will use
    the OS to interleave execution among the various processes. If each of the processes
    has a mixture of I/O and computation, we should be able to assure that our processor
    is very busy. When processes are waiting for I/O to complete, other processes
    can do their computation. When an I/O completes, a process will be ready to run
    and can compete with others for processing time.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`multiprocessing`模块的一个优雅的方法是创建一个处理`Pool`对象，并将工作分配给该池中的各个进程。我们将使用操作系统在各个进程之间交错执行。如果每个进程都有I/O和计算的混合，我们应该能够确保我们的处理器非常忙碌。当进程等待I/O完成时，其他进程可以进行计算。当I/O完成时，一个进程将准备好运行，并且可以与其他进程竞争处理时间。
- en: 'The recipe for mapping work to a separate process looks as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 将工作映射到单独的进程的方法如下：
- en: '[PRE25]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We've created a `Pool` object with four separate processes and assigned this
    `Pool` object to the `workers` variable. We've then mapped a function, `analysis`,
    to an iterable queue of work to be done, using the pool of processes. Each process
    in the `workers` pool will be assigned items from the iterable queue. In this
    case, the queue is the result of the `glob.glob(pattern)` attribute, which is
    a sequence of file names.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个具有四个独立进程的`Pool`对象，并将该`Pool`对象分配给`workers`变量。然后，我们将一个名为`analysis`的函数映射到要执行的工作的可迭代队列上，使用进程池。`workers`池中的每个进程将被分配来自可迭代队列的项目。在这种情况下，队列是`glob.glob(pattern)`属性的结果，它是文件名的序列。
- en: As the `analysis()` function returns a result, the parent process that created
    the `Pool` object can collect those results. This allows us to create several
    concurrently-built `Counter` objects and merge them into a single, composite result.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`analysis()`函数返回一个结果，创建`Pool`对象的父进程可以收集这些结果。这使我们能够创建几个并发构建的`Counter`对象，并将它们合并成一个单一的复合结果。
- en: If we start *p* processes in the pool, our overall application will include
    *p+1* processes. There will be one parent process and *p* children. This often
    works out well because the parent process will have little to do after the subprocess
    pools are started. Generally, the workers will be assigned to separate CPUs (or
    cores) and the parent will share a CPU with one of the children in the `Pool`
    object.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在池中启动*p*个进程，我们的整个应用程序将包括*p+1*个进程。将有一个父进程和*p*个子进程。这通常效果很好，因为在子进程池启动后，父进程将几乎没有什么要做。通常情况下，工作进程将被分配到单独的CPU（或核心），而父进程将与`Pool`对象中的一个子进程共享一个CPU。
- en: Note
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The ordinary Linux parent/child process rules apply to the subprocesses created
    by this module. If the parent crashes without properly collecting final status
    from the child processes, then "zombie" processes can be left running. For this
    reason, a process `Pool` object is a context manager. When we use a pool via the
    `with` statement, at the end of the context, the children are properly terminated.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由该模块创建的子进程遵循普通的Linux父/子进程规则。如果父进程在没有正确收集子进程的最终状态的情况下崩溃，那么可能会留下“僵尸”进程在运行。因此，进程`Pool`对象是一个上下文管理器。当我们通过`with`语句使用进程池时，在上下文结束时，子进程会被正确终止。
- en: By default, a `Pool` object will have a number of workers based on the value
    of the `multiprocessing.cpu_count()` function. This number is often optimal, and
    simply using the `with multiprocessing.Pool() as workers:` attribute might be
    sufficient.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`Pool`对象将具有基于`multiprocessing.cpu_count()`函数值的工作进程数。这个数字通常是最佳的，只需使用`with
    multiprocessing.Pool() as workers:`属性可能就足够了。
- en: In some cases, it can help to have more workers than CPUs. This might be true
    when each worker has I/O-intensive processing. Having many worker processes waiting
    for I/O to complete can improve the elapsed running time of an application.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，有时比CPU更多的工作进程可能会有所帮助。当每个工作进程都有I/O密集型处理时，这可能是真的。有许多工作进程等待I/O完成可以改善应用程序的运行时间。
- en: If a given `Pool` object has *p* workers, this mapping can cut the processing
    time to almost ![Using a multiprocessing pool for concurrent processing](graphics/B03652_12_03.jpg)
    of the time required to process all of the logs serially. Pragmatically, there
    is some overhead involved with communication between the parent and child processes
    in the `Pool` object. Therefore, a four-core processor might only cut the processing
    time in half.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果给定的`Pool`对象有*p*个工作进程，这种映射可以将处理时间减少到几乎处理所有日志的时间的![使用多进程池进行并发处理](graphics/B03652_12_03.jpg)。实际上，在`Pool`对象中父进程和子进程之间的通信涉及一些开销。因此，一个四核处理器可能只能将处理时间减少一半。
- en: 'The multiprocessing `Pool` object has four map-like methods to allocate work
    to a pool: `map()`, `imap()`, `imap_unordered()`, and `starmap()`. Each of these
    is a variation on the common theme of mapping a function to a pool of processes.
    They differ in the details of allocating work and collecting results.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 多进程`Pool`对象有四种类似map的方法来分配工作给进程池：`map()`、`imap()`、`imap_unordered()`和`starmap()`。每个方法都是将函数映射到进程池的变体。它们在分配工作和收集结果的细节上有所不同。
- en: The `map(function, iterable)` method allocates items from the iterable to each
    worker in the pool. The finished results are collected in the order they were
    allocated to the `Pool` object so that order is preserved.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`map(function, iterable)`方法将可迭代对象中的项目分配给池中的每个工作进程。完成的结果按照它们分配给`Pool`对象的顺序进行收集，以保持顺序。'
- en: The `imap(function, iterable)` method is described as "lazier" than map. By
    default, it sends each individual item from the iterable to the next available
    worker. This might involve more communication overhead. For this reason, a chunk
    size larger than 1 is suggested.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`imap(function, iterable)` 方法被描述为比 map 方法“更懒”。默认情况下，它会将可迭代对象中的每个单独项目发送给下一个可用的工作进程。这可能涉及更多的通信开销。因此建议使用大于1的块大小。'
- en: The `imap_unordered(function, iterable)` method is similar to the `imap()` method,
    but the order of the results is not preserved. Allowing the mapping to be processed
    out of order means that, as each process finishes, the results are collected.
    Otherwise, the results must be collected in order.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The `starmap(function, iterable)` method is similar to the `itertools.starmap()`
    function. Each item in the iterable must be a tuple; the tuple is passed to the
    function using the `*` modifier so that each value of the tuple becomes a positional
    argument value. In effect, it's performing `function(*iterable[0])`, `function(*iterable[1])`,
    and so on.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is one of the variations on the preceding mapping theme:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We've created a `Counter()` function that we'll use to consolidate the results
    from each worker in the pool. We created a pool of subprocesses based on the number
    of available CPUs and used the `Pool` object as a context manager. We then mapped
    our `analysis()` function to each file in our file-matching pattern. The resulting
    `Counter` objects from the `analysis()` function are combined into a single resulting
    counter.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: This takes about 68 seconds. The time to analyze the logs was cut in half using
    several concurrent processes.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: We've created a two-tiered map-reduce process with the `multiprocessing` module's
    `Pool.map()` function. The first tier was the `analysis()` function, which performed
    a map-reduce on a single logfile. We then consolidated these reductions in a higher-level
    reduce operation.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Using apply() to make a single request
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to the `map()` function''s variants, a pool also has an `apply(function,
    *args, **kw)` method that we can use to pass one value to the worker pool. We
    can see that the `map()` method is really just a `for` loop wrapped around the
    `apply()` method, we can, for example, use the following command:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: It's not clear, for our purposes, that this is a significant improvement. Almost
    everything we need to do can be expressed as a `map()` function.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Using map_async(), starmap_async(), and apply_async()
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The behavior of the `map()`, `starmap()`, and `apply()` functions is to allocate
    work to a subprocess in the `Pool` object and then collect the response from the
    subprocess when that response is ready. This can cause the child to wait for the
    parent to gather the results. The `_async()` function's variations do not wait
    for the child to finish. These functions return an object that can be queried
    to get the individual results from the child processes.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a variation using the `map_async()` method:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We've created a `Counter()` function that we'll use to consolidate the results
    from each worker in the pool. We created a pool of subprocesses based on the number
    of available CPUs and used this `Pool` object as a context manager. We then mapped
    our `analysis()` function to each file in our file-matching pattern. The response
    from the `map_async()` function is a `MapResult` object; we can query this for
    results and overall status of the pool of workers. In this case, we used the `get()`
    method to get the sequence of the `Counter` objects.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The resulting `Counter` objects from the `analysis()` function are combined
    into a single resulting `Counter` object. This aggregate gives us an overall summary
    of a number of logfiles. This processing is not any faster than the previous example.
    The use of the `map_async()` function allows the parent process to do additional
    work while waiting for the children to finish.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: More complex multiprocessing architectures
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `multiprocessing` package supports a wide variety of architectures. We can
    easily create multiprocessing structures that span multiple servers and provide
    formal authentication techniques to create a necessary level of security. We can
    pass objects from process to process using queues and pipes. We can share memory
    between processes. We can also share lower-level locks between processes as a
    way to synchronize access to shared resources such as files.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Most of these architectures involve explicitly managing state among several
    working processes. Using locks and shared memory, in particular, are imperative
    in nature and don't fit well with a functional programming approach.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: We can, with some care, treat queues and pipes in a functional manner. Our objective
    is to decompose a design into producer and consumer functions. A producer can
    create objects and insert them into a queue. A consumer will take objects out
    of a queue and process them, perhaps putting intermediate results into another
    queue. This creates a network of concurrent processors and the workload is distributed
    among these various processes. Using the `pycsp` package can simplify the queue-based
    exchange of messages among processes. For more information, visit [https://pypi.python.org/pypi/pycsp](https://pypi.python.org/pypi/pycsp).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: This design technique has some advantages when designing a complex application
    server. The various subprocesses can exist for the entire life of the server,
    handling individual requests concurrently.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Using the concurrent.futures module
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the `multiprocessing` package, we can also make use of the `concurrent.futures`
    module. This also provides a way to map data to a concurrent pool of threads or
    processes. The module API is relatively simple and similar in many ways to the
    `multiprocessing.Pool()` function's interface.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example to show just how similar they are:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The most significant change between the preceding example and previous examples
    is that we're using an instance of the `concurrent.futures.ProcessPoolExecutor`
    object instead of the `multiprocessing.Pool` method. The essential design pattern
    is to map the `analysis()` function to the list of filenames using the pool of
    available workers. The resulting `Counter` objects are consolidated to create
    a final result.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The performance of the `concurrent.futures` module is nearly identical to the
    `multiprocessing` module.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Using concurrent.futures thread pools
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `concurrent.futures` module offers a second kind of executor that we can
    use in our applications. Instead of creating a `concurrent.futures.ProcessPoolExecutor`
    object, we can use the `ThreadPoolExecutor` object. This will create a pool of
    threads within a single process.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The syntax is otherwise identical to using a `ProcessPoolExecutor` object. The
    performance, however, is remarkably different. The logfile processing is dominated
    by I/O. All of the threads in a process share the same OS scheduling constraints.
    Due to this, the overall performance of multithreaded logfile analysis is about
    the same as processing the logfiles serially.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'Using sample logfiles and a small four-core laptop running Mac OS X, these
    are the kinds of results that indicate the difference between threads that share
    I/O resources and processes:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Using the `concurrent.futures` thread pool, the elapsed time was 168 seconds
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a process pool, the elapsed time was 68 seconds
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both cases, the `Pool` object's size was 4\. It's not clear which kind of
    applications benefit from a multithreading approach. In general, multiprocessing
    seems to be best for Python applications.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Using the threading and queue modules
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Python `threading` package involves a number of constructs helpful for building
    imperative applications. This module is not focused on writing functional applications.
    We can make use of thread-safe queues in the `queue` module to pass objects from
    thread to thread.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: The `threading` module doesn't have a simple way to distribute work to various
    threads. The API isn't ideally suited to functional programming.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: As with the more primitive features of the `multiprocessing` module, we can
    try to conceal the stateful and imperative nature of locks and queues. It seems
    easier, however, to make use of the `ThreadPoolExecutor` method in the `concurrent.futures`
    module. The `ProcessPoolExecutor.map()` method provides us with a very pleasant
    interface to concurrent processing of the elements of a collection.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: The use of the `map()` function primitive to allocate work seems to fit nicely
    with our functional programming expectations. For this reason, it's best to focus
    on the `concurrent.futures` module as the most accessible way to write concurrent
    functional programs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Designing concurrent processing
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From a functional programming perspective, we''ve seen three ways to use the
    `map()` function concept applied to data items concurrently. We can use any one
    of the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '`multiprocessing.Pool`'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`concurrent.futures.ProcessPoolExecutor`'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`concurrent.futures.ThreadPoolExecutor`'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are almost identical in the way we interact with them; all three have
    a `map()` method that applies a function to items of an iterable collection. This
    fits elegantly with other functional programming techniques. The performance is
    different because of the nature of concurrent threads versus concurrent processes.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'As we stepped through the design, our log analysis application decomposed into
    two overall areas:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'The lower-level parsing: This is generic parsing that will be used by almost
    any log analysis application'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The higher-level analysis application: This is more specific filtering and
    reduction focused on our application needs'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The lower-level parsing can be decomposed into four stages:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Reading all the lines from multiple source logfiles. This was the `local_gzip()`
    mapping from file name to a sequence of lines.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating simple namedtuples from the lines of log entries in a collection of
    files. This was the `access_iter()` mapping from text lines to Access objects.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing the details of more complex fields such as dates and URLs. This was
    the `access_detail_iter()` mapping from `Access` objects to `AccessDetails` objects.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rejecting uninteresting paths from the logs. We can also think of this as passing
    only the interesting paths. This was more of a filter than a map operation. This
    was a collection of filters bundled into the `path_filter()` function.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We defined an overall `analysis()` function that parsed and analyzed a given
    logfile. It applied the higher-level filter and reduction to the results of the
    lower-level parsing. It can also work with a wild-card collection of files.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the number of mappings involved, we can see several ways to decompose
    this problem into work that can be mapped to into a pool of threads or processes.
    Here are some of the mappings we can consider as design alternatives:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Map the `analysis()` function to individual files. We use this as a consistent
    example throughout this chapter.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refactor the `local_gzip()` function out of the overall `analysis()` function.
    We can now map the revised `analysis()` function to the results of the `local_gzip()`
    function.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refactor the `access_iter(local_gzip(pattern))` function out of the overall
    `analysis()` function. We can map this revised `analysis()` function against the
    iterable sequence of the `Access` objects.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refactor the `access_detail_iter(access-iter(local_gzip(pattern)))` function
    into a separate iterable. We will then map the `path_filter()` function and the
    higher-level filter and reduction against the iterable sequence of the `AccessDetail`
    objects.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also refactor the lower-level parsing into a function that is separate
    from the higher-level analysis. We can map the analysis filter and reduction against
    the output from the lower-level parsing.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these are relatively simple restructurings of the example application.
    The benefit of using functional programming techniques is that each part of the
    overall process can be defined as a mapping. This makes it practical to consider
    different architectures to locate an optimal design.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: In this case, however, we need to distribute the I/O processing to as many CPUs
    or cores as we have available. Most of these potential refactorings will perform
    all of the I/O in the parent process; these will only distribute the computations
    to multiple concurrent processes with little resulting benefit. Then, we want
    to focus on the mappings, as these distribute the I/O to as many cores as possible.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: It's often important to minimize the amount of data being passed from process
    to process. In this example, we provided just short filename strings to each worker
    process. The resulting `Counter` object was considerably smaller than the 10 MB
    of compressed detail data in each logfile. We can further reduce the size of each
    `Counter` object by eliminating items that occur only once; or we can limit our
    application to only the 20 most popular items.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: The fact that we can reorganize the design of this application freely doesn't
    mean we should reorganize the design. We can run a few benchmarking experiments
    to confirm our suspicion that logfile parsing is dominated by the time required
    to read the files.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ve looked at two ways to support concurrent processing
    of multiple pieces of data:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'The `multiprocessing` module: Specifically, the `Pool` class and the various
    kinds of mappings available to a pool of workers.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `concurrent.futures` module: Specifically the `ProcessPoolExecutor` and
    `ThreadPoolExecutor` class. These classes also support a mapping that will distribute
    work among workers that are threads or processes.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've also noted some alternatives that don't seem to fit well with functional
    programming. There are numerous other features of the `multiprocessing` module,
    but they're not a good fit with functional design. Similarly, the `threading`
    and `queue` modules can be used to build multithreaded applications, but the features
    aren't a good fit with functional programs.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll look at the `operator` module. This can be used to
    simplify some kinds of algorithms. We can use a built-in operator function instead
    of defining a lambda form. We'll also look at some techniques to design flexible
    decision making and allow expressions to be evaluated in a non-strict order.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
