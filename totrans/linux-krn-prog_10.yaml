- en: Kernel Memory Allocation for Module Authors - Part 1
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 模块作者的内核内存分配-第1部分
- en: In the previous two chapters, one on kernel internal aspects and architecture
    and the other on the essentials of memory management internals, we covered key
    aspects that serve as required background information for this and the following
    chapter. In this and the next chapter, we will get down to the actual allocation
    and freeing of kernel memory by various means. We will demonstrate this via kernel
    modules that you can test and tweak, elaborate on the whys and hows of it, and
    provide many real-world tips and tricks to enable a kernel or driver developer
    like you to gain maximum efficiency when working with memory within your kernel
    module.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，一章介绍了内核内部方面和架构，另一章介绍了内存管理内部的基本知识，我们涵盖了为本章和下一章提供所需的背景信息的关键方面。在本章和下一章中，我们将着手实际分配和释放内核内存的各种方式。我们将通过您可以测试和调整的内核模块来演示这一点，详细说明其中的原因和方法，并提供许多实用的技巧，以使像您这样的内核或驱动程序开发人员在处理内核模块内存时能够获得最大的效率。
- en: In this chapter, we will cover the kernel's two primary memory allocators –
    the **Page Allocator** (**PA**) (aka **Buddy System Allocator** (**BSA**)) and
    the slab allocator. We will delve into the nitty-gritty of working with their
    APIs within kernel modules. Actually, we will go well beyond simply seeing how
    to use the APIs, clearly demonstrating why all is not optimal in all cases, and
    how to overcome these situations. [Chapter 9](dbb888a2-8145-4132-938c-1313a707b2f2.xhtml),
    *Kernel Memory Allocation for Module Authors – Part 2*, will continue our coverage
    of the kernel memory allocators, delving into a few more advanced areas.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍内核的两个主要内存分配器——**页面分配器**（**PA**）（又称**Buddy System Allocator**（**BSA**））和slab分配器。我们将深入研究在内核模块中使用它们的API的细节。实际上，我们将远远超出简单地了解如何使用API，清楚地展示在所有情况下都不是最佳的原因，以及如何克服这些情况。[第9章](dbb888a2-8145-4132-938c-1313a707b2f2.xhtml)，*模块作者的内核内存分配-第2部分*，将继续介绍内核内存分配器，深入探讨一些更高级的领域。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing kernel memory allocators
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍内核内存分配器
- en: Understanding and using the kernel page allocator (or BSA)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解和使用内核页面分配器（或BSA）
- en: Understanding and using the kernel slab allocator
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解和使用内核slab分配器
- en: Size limitations of the kmalloc API
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kmalloc API的大小限制
- en: Slab allocator - a few additional details
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Slab分配器-一些额外的细节
- en: Caveats when using the slab allocator
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用slab分配器时的注意事项
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: I assume that you have gone through [Chapter 1](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml),
    *Kernel Workspace Setup*, and have appropriately prepared a guest **Virtual Machine**
    (**VM**) running Ubuntu 18.04 LTS (or a later stable release) and installed all
    the required packages. If not, I highly recommend you do this first.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设您已经阅读了[第1章](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml)，*内核工作空间设置*，并已经适当准备了一个运行Ubuntu
    18.04 LTS（或更高稳定版本）的虚拟机，并安装了所有必需的软件包。如果没有，我强烈建议您首先这样做。
- en: To get the most out of this book, I strongly recommend you first set up the
    workspace
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用本书，我强烈建议您首先设置好工作空间
- en: environment, including cloning this book's GitHub repository ([https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming)) for
    the code, and work on it in a hands-on fashion.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 环境，包括克隆本书的GitHub存储库（[https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming)）以获取代码，并进行实际操作。
- en: 'Refer to *Hands-On System Programming with Linux*, Kaiwan N Billimoria, Packt
    ([https://www.packtpub.com/networking-and-servers/hands-system-programming-linux](https://www.packtpub.com/networking-and-servers/hands-system-programming-linux))
    as a prerequisite to this chapter (essential reading, really):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*Hands-On System Programming with Linux*，Kaiwan N Billimoria, Packt ([https://www.packtpub.com/networking-and-servers/hands-system-programming-linux](https://www.packtpub.com/networking-and-servers/hands-system-programming-linux))作为本章的先决条件（确实是必读的）：
- en: '*Chapter 1*, *Linux System Architecture*'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第1章*，*Linux系统架构*'
- en: '*Chapter 2*, *Virtual Memory*'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第2章*，*虚拟内存*'
- en: Introducing kernel memory allocators
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍内核内存分配器
- en: The Linux kernel, like any other OS, requires a sturdy algorithm and implementation
    to perform a really key task – the allocation and subsequent deallocation of memory
    or page frames (RAM). The primary (de)allocator engine in the Linux OS is referred
    to as the PA, or the BSA. Internally, it uses a so-called buddy system algorithm
    to efficiently organize and parcel out free chunks of system RAM. We will find
    more on the algorithm in the *Understanding and using the kernel page allocator
    (or BSA)* section.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 像任何其他操作系统一样，Linux内核需要一个稳固的算法和实现来执行一个非常关键的任务——分配和释放内存或页面帧（RAM）。Linux操作系统中的主要（de）分配器引擎被称为PA或BSA。在内部，它使用所谓的伙伴系统算法来高效地组织和分配系统RAM的空闲块。我们将在*理解和使用内核页面分配器（或BSA）*部分找到更多关于该算法的信息。
- en: 'In this chapter and in this book, when we use the notation *(de)allocate*,
    please read it as both words: *allocate* and *deallocate*.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和本书中，当我们使用*(de)allocate*这种表示法时，请将其理解为*allocate*和*deallocate*两个词。
- en: Of course, being imperfect, the page allocator is not the only or always the
    best way to obtain and subsequently release system memory. Other technologies
    exist within the Linux kernel to do so. High on the list of them is the kernel's **slab
    allocator** or **slab cache** system (we use the word *slab* here as the generic
    name for this type of allocator as it originated with this name; in practice,
    though, the internal implementation of the modern slab allocator used by the Linux
    kernel is called SLUB (the unqueued slab allocator); more on this later).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，作为不完美的，页面分配器并不是获取和释放系统内存的唯一或总是最佳方式。Linux内核中存在其他技术来实现这一点。其中之一是内核的**slab分配器**或**slab缓存**系统（我们在这里使用*slab*这个词作为这种类型分配器的通用名称，因为它起源于这个名称；实际上，Linux内核使用的现代slab分配器的内部实现称为SLUB（无队列slab分配器）；稍后会详细介绍）。
- en: 'Think of it this way: the slab allocator solves some issues and optimizes performance
    with the page allocator. What issues exactly? We shall soon see. For now, though,
    it''s really important to understand that the only way in which to actually (de)allocate
    physical memory is via the page allocator. The page allocator is the primary engine
    for memory (de)allocation on the Linux OS!'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 可以这样理解：slab分配器解决了一些问题，并通过页面分配器优化了性能。到底解决了哪些问题？我们很快就会看到。不过，现在，真的很重要的是要理解，实际（de）分配物理内存的唯一方式是通过页面分配器。页面分配器是Linux操作系统上内存（de）分配的主要引擎！
- en: To avoid confusion and repetition, we will from now on refer to this primary
    allocation engine as the page allocator. *Y*ou will understand that it's also
    known as the BSA (derived from the name of the algorithm that drives it).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免混淆和重复，我们从现在开始将这个主要分配引擎称为页面分配器。*您将了解到它也被称为BSA（源自驱动它的算法的名称）。*
- en: 'Thus, the slab allocator is layered upon (or above) the page allocator. Various
    core kernel subsystems, as well as non-core code within the kernel, such as device
    drivers, can allocate (and deallocate) memory either directly via the page allocator
    or indirectly via the slab allocator; the following diagram illustrates this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，slab分配器是建立在页面分配器之上的。各种核心内核子系统以及内核中的非核心代码，如设备驱动程序，都可以直接通过页面分配器或间接通过slab分配器分配（和释放）内存；以下图表说明了这一点：
- en: '![](img/5ac7cdd2-8784-4148-a456-149595e71aed.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ac7cdd2-8784-4148-a456-149595e71aed.png)'
- en: Figure 8.1 – Linux's page allocator engine with the slab allocator layered above
    it
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 - Linux的页面分配器引擎，上面是slab分配器
- en: 'A few things to be clear about at the outset:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，有几件事要澄清：
- en: The entire Linux kernel and all of its core components and subsystems (excluding
    the memory management subsystem itself) ultimately use the page allocator (or
    BSA) for memory (de)allocation. This includes non-core stuff, such as kernel modules
    and device drivers.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个Linux内核及其所有核心组件和子系统（不包括内存管理子系统本身）最终都使用页面分配器（或BSA）进行内存（de）分配。这包括非核心内容，如内核模块和设备驱动程序。
- en: The preceding systems reside completely in kernel (virtual) address space and
    are not directly accessible from user space.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前面的系统完全驻留在内核（虚拟）地址空间中，不可直接从用户空间访问。
- en: The page frames (RAM) from where the page allocator gets memory is within the
    kernel lowmem region, or the direct-mapped RAM region of the kernel segment (we
    covered the kernel segment in detail in the previous chapter)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 页面帧（RAM）从页面分配器获取内存的地方位于内核低内存区域，或内核段的直接映射RAM区域（我们在上一章节详细介绍了内核段）
- en: The slab allocator is ultimately a user of the page allocator, and thus gets
    its memory from there itself (which again implies from the kernel lowmem region)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: slab分配器最终是页面分配器的用户，因此它的内存也是从那里获取的（这再次意味着从内核低内存区域获取）
- en: User space dynamic memory allocation with the familiar `malloc` family of APIs
    does not directly map to the preceding layers (that is, calling `malloc(3)` in
    user space does *not *directly result in a call to the page or slab allocator).
    It does so indirectly. How exactly? You will learn how; patience! (This key coverage
    is found in two sections of the next chapter, in fact, involving demand paging; look
    out for it as you cover that chapter!)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户空间使用熟悉的`malloc`系列API进行动态内存分配并不直接映射到前面的层（也就是说，在用户空间调用`malloc(3)`并不直接导致对页面或slab分配器的调用）。它是间接的。具体是如何？您将会学到；请耐心等待！（这个关键内容实际上在下一章的两个部分中找到，涉及到需求分页；在您学习那一章时要注意！）
- en: Also, to be clear, Linux kernel memory is non-swappable. It can never be swapped
    out to disk; this was decided in the early Linux days to keep performance high.
    User space memory pages are always swappable by default; this can be changed by
    the system programmer via the `mlock()`/`mlockall()` system calls.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另外，要明确的是，Linux内核内存是不可交换的。它永远不会被交换到磁盘上；这是在早期Linux时代决定的，以保持性能高。用户空间内存页面默认是可交换的；系统程序员可以通过`mlock()`/`mlockall()`系统调用来改变这一点。
- en: Now, fasten your seatbelts! With this basic understanding of the page allocator
    and slab allocator, let's begin the journey on learning (the basics on) how the
    Linux kernel's memory allocators work and, more importantly, how to work well
    with them.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，系好安全带！有了对页面分配器和slab分配器的基本理解，让我们开始学习Linux内核内存分配器的工作原理，更重要的是，如何与它们良好地配合工作。
- en: Understanding and using the kernel page allocator (or BSA)
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解和使用内核页面分配器（或BSA）
- en: 'In this section, you will learn about two aspects of the Linux kernel''s primary
    (de)allocator engine:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，您将了解Linux内核主要（de）分配器引擎的两个方面：
- en: First, we will cover the fundamentals of the algorithm behind this software
    (called the buddy system).
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们将介绍这个软件背后算法的基础知识（称为伙伴系统）。
- en: Then, we will cover the actual and practical usage of the APIs it exposes to
    the kernel or driver developer.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将介绍它向内核或驱动程序开发人员公开的API的实际使用。
- en: Understanding the basics of the algorithm behind the page allocator is important.
    You will then be able to understand the pros and cons of it, and thus, when and
    which APIs to use in which situation. Let's begin with its inner workings. Again,
    remember that the scope of this book with regard to the internal memory management
    details is limited. We will cover it to a depth deemed sufficient and no more.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 理解页面分配器背后的算法的基础知识是重要的。然后您将能够了解其优缺点，以及在哪种情况下使用哪些API。让我们从它的内部工作原理开始。再次提醒，本书关于内部内存管理细节的范围是有限的。我们将涵盖到足够的深度，不再深入。
- en: The fundamental workings of the page allocator
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 页面分配器的基本工作原理
- en: We will break up this discussion into a few relevant parts. Let's begin with
    how the kernel's page allocator tracks free physical page frames via its freelist data
    structures.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个讨论分成几个相关的部分。让我们从内核的页面分配器如何通过其freelist数据结构跟踪空闲物理页面帧开始。
- en: Freelist organization
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Freelist组织
- en: The key to the page allocator (buddy system) algorithm is its primary internal
    metadata structure. It's called the buddy system freelist and consists of an array
    of pointers to (the oh-so-common!) doubly linked circular lists. The index of
    this array of pointers is called the order of the list – it's the power to which
    to raise 2 to. The array length is from `0` to `MAX_ORDER-1`. The value of `MAX_ORDER`
    is arch-dependent. On the x86 and ARM, it's 11, whereas on a large-ish system
    such as the Itanium, it's 17\. Thus, on the x86 and ARM, the order ranges from
    2⁰ to 2^(10) ; that is, from 1 to 1,024\. What does that mean? Do read on...
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Each doubly linked circular list points to free physical contiguous page frames
    of size *2^(order)*. Thus (assuming a 4 KB page size), we end up with lists of
    the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 2⁰ = 1 page = 4 KB chunks
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2¹ = 2 pages = 8 KB chunks
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2² = 4 pages = 16 KB chunks
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2³ = 8 pages = 32 KB chunks
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2^(10) = 1024 pages = 1024*4 KB = 4 MB chunks
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram is a simplified conceptual illustration of (a single
    instance of) the page allocator freelist:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72c111f2-5fee-43ad-91e8-e1ee9d18eabf.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Buddy system/page allocator freelist on a system with 4 KB page
    size and MAX_ORDER of 11
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, each memory "chunk" is represented by a square box (to
    keep it simple, we use the same size in our diagram). Internally, of course, these
    aren't the actual memory pages; rather, the boxes represent metadata structures
    (struct page) that point to physical memory frames. On the right side of the figure,
    we show the size of each physically contiguous free memory chunk that could be
    enqueued on the list to the left.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'The kernel gives us a convenient (summarized) view into the current state of
    the page allocator via the `proc` filesystem (on our Ubuntu guest VM with 1 GB
    RAM):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12346cfe-59e4-438a-bea4-dced0a622b0e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Annotated screenshot of sample /proc/buddyinfo output
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Our guest VM is a pseudo-NUMA box with one node (`Node 0`) and two zones (`DMA`
    and `DMA32`). The numbers following `zone XXX` are the number of free (physically
    contiguous!) page frames in order 0, order 1, order 2, right up to `MAX_ORDER-1` (here,
    *11 – 1 = 10*). So, let''s take a couple of examples from the preceding output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: There are 35 single-page free chunks of RAM in the order `0` list for node `0`, zone
    DMA.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In node `0`, zone DMA32, order `3`, the number shown in *Figure 8.3* here is 678;
    now, take *2^(order) = 2^(3 )**= 8* *page frames = 32 KB* (assuming a page size
    of 4 KB); this implies that there are 678 32 KB physically contiguous free chunks
    of RAM on that list.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that **each chunk is guaranteed to be physically contiguous
    RAM in and of itself**. Also, notice that the size of the memory chunks on a given
    order is always double that of the previous order (and half that of the next one).
    This is, of course, as they're all powers of 2.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Note that `MAX_ORDER` can (and does) vary with the architecture. On regular
    x86 and ARM systems, it's `11`, yielding a largest chunk size of 4 MB of physically
    contiguous RAM on order 10 of the freelists. On high-end enterprise server class
    systems running the Itanium (IA-64) processor, `MAX_ORDER` can be as high as `17` (implying
    a largest chunk size on order (17-1), thus of *2^(16) = 65,536 pages = 512 MB
    chunks* of physically contiguous RAM on order 16 of the freelists, for a 4 KB
    page size). The IA-64 MMU supports up to eight page sizes ranging from a mere
    4 KB right up to 256 MB. As another example, with a page size of 16 MB, the order
    16 list could potentially have physically contiguous RAM chunks of size *65,536
    * 16 MB = 1 TB* each!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'Another key point: the kernel keeps **multiple BSA freelists – one for every node:zone that
    is present on the system!** This lends a natural way to allocate memory on a NUMA
    system.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how the kernel instantiates multiple freelists
    – *one per node:zone present on the system* (diagram credit: *Professional Linux
    Kernel Architecture*, Mauerer, Wrox Press, Oct 2008):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了内核如何实例化多个*空闲列表-系统上每个节点：区域一个*（图表来源：*Professional Linux Kernel Architecture*，Mauerer，Wrox
    Press，2008年10月）：
- en: '![](img/998a8480-eae3-4f0e-bb2b-4ecd15a07d4b.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/998a8480-eae3-4f0e-bb2b-4ecd15a07d4b.png)'
- en: Figure 8.4 – Page allocator (BSA) "freelists," one per node:zone on the system;
    diagram credit: Professional Linux Kernel Architecture, Mauerer, Wrox Press, Oct
    2008
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4-页面分配器（BSA）“空闲列表”，系统上每个节点：区域一个；图表来源：*Professional Linux Kernel Architecture*，Mauerer，Wrox
    Press，2008年10月
- en: Furthermore, as can be seen in Figure 8.5, when the kernel is called upon to
    allocate RAM via the page allocator, it picks the optimal freelist to allocate
    memory from – the one associated with the node upon which the thread asking the
    request is running (recall the NUMA architecture from the previous chapter). If
    this node is out of memory or cannot allocate it for whatever reason, the kernel
    then uses a fallback list to figure out which freelist to attempt to allocate
    memory from. (In reality, the real picture is even more complex; we provide a
    few more details in the *Page allocator internals – a few more details* section.)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如图8.5所示，当内核被调用以通过页面分配器分配RAM时，它会选择最佳的空闲列表来分配内存-与请求的线程所在的*节点*相关联的列表（回想一下前一章的NUMA架构）。如果该节点没有内存或由于某种原因无法分配内存，内核将使用备用列表来确定从哪个空闲列表尝试分配内存（实际上，实际情况更加复杂；我们在*页面分配器内部-更多细节*部分提供了一些更多的细节）。
- en: Let's now understand (in a conceptual way) how all of this actually works.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们以概念方式了解所有这些实际上是如何工作的。
- en: The workings of the page allocator
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 页面分配器的工作原理
- en: 'The actual (de)allocation strategy can be explained by using a simple example.
    Let''s say a device driver requests 128 KB of memory. To fulfill this request,
    the (simplified and conceptual) page allocator algorithm will do this:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的（解）分配策略可以通过一个简单的例子来解释。假设一个设备驱动程序请求128 KB的内存。为了满足这个请求，（简化和概念化的）页面分配器算法将执行以下操作：
- en: The algorithm expresses the amount to be allocated (128 KB here) in pages. Thus,
    here, it's (assuming a page size of 4 KB) *128/4 = 32 pages*.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该算法以页面的形式表示要分配的数量（这里是128 KB），因此，这里是（假设页面大小为4 KB）*128/4=32页*。
- en: Next, it determines to what power 2 must be raised to get 32\. That's *log**[2]**32*,
    which is 5 (as 2⁵ is 32).
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，它确定2必须被提高到多少次方才能得到32。这就是*log*[2]*32*，结果是5（因为2⁵等于32）。
- en: Now, it checks the list on order 5 of the appropriate *node:zone *page allocator
    freelist. If a memory chunk is available (it will be of size *2**⁵ **pages = 128
    KB*), dequeue it from the list, update the list, and allocate it to the requester.
    Job done! Return to caller.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，它检查适当的*节点：区域*页面分配器空闲列表上的顺序5列表。如果有可用的内存块（大小为*2**⁵**页=128 KB*），则从列表中出列，更新列表，并分配给请求者。任务完成！返回给调用者。
- en: 'Why do we say *of the appropriate node:zone **page allocator freelist*? Does
    that mean there''s more than one of them? Yes, indeed! We repeat: the reality
    is that there will be several freelist data structures, one each per *node:zone *on
    the system. (Also see more details in the section *Page allocator internals –
    a few more details*.)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们说*适当的节点：区域**页面分配器空闲列表*？这是否意味着有不止一个？是的，确实如此！我们再次重申：实际情况是系统上将有几个空闲列表数据结构，每个*节点：区域*一个（还可以在*页面分配器内部-更多细节*部分中查看更多细节）。
- en: If no memory chunk is available on the order 5 list (that is, if it's null),
    then it checks the list on the next order; that is, the order 6-linked list (if
    it's not empty, it will have *2⁶** pages = 256 KB* memory chunks enqueued on it,
    each chunk being double the size of what we want).
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果顺序5列表上没有可用的内存块（即为空），那么它将检查下一个顺序的列表；也就是顺序6的链表（如果不为空，它将有*2⁶**页=256 KB*的内存块排队，每个块的大小是我们想要的两倍）。
- en: 'If the order 6 list is non-null, then it will take (dequeue) a chunk of memory
    from it (which will be 256 KB in size, double of what''s required), and do the
    following:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果顺序6列表不为空，那么它将从中取出（出列）一个内存块（大小为256 KB，是所需大小的两倍），并执行以下操作：
- en: Update the list to reflect the fact that one chunk is now removed.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新列表以反映现在已经移除了一个块。
- en: Cut the chunk in half, thus obtaining two 128 KB halves or **buddies**! (Please
    see the following information box.)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这个块切成两半，从而得到两个128 KB的半块或**伙伴**！（请参阅下面的信息框。）
- en: Migrate (enqueue) one half (of size 128 KB) to the order 5 list.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一半（大小为128 KB）迁移（入列）到顺序5列表。
- en: Allocate the other half (of size 128 KB) to the requester.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将另一半（大小为128 KB）分配给请求者。
- en: Job done! Return to caller.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务完成！返回给调用者。
- en: If the order 6 list is also empty, then it repeats the preceding process with
    the order 7 list, and so on, until it succeeds.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果顺序6列表也是空的，那么它将使用顺序7列表重复前面的过程，直到成功为止。
- en: If all the remaining higher-order lists are empty (null), it will fail the request.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果所有剩余的高阶列表都为空（null），则请求将失败。
- en: We can cut or slice a memory chunk in half because every chunk on the list is
    guaranteed to be physically contiguous memory. Once cut, we have two halves; each
    is called a **buddy block**, hence the name of this algorithm. Pedantically, it's
    called the binary buddy system as we use power-of-2-sized memory chunks. A buddy
    block is defined as a block that is of the same size and physically adjacent to
    another.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将内存块切成两半，因为列表上的每个块都保证是物理上连续的内存。切割后，我们得到两个半块；每个都被称为**伙伴块**，因此这个算法的名称。从学术角度来说，它被称为二进制伙伴系统，因为我们使用2的幂大小的内存块。**伙伴块**被定义为与另一个相同大小且物理相邻的块。
- en: You will understand that the preceding description is conceptual. The actual
    code implementation is certainly more complex and optimized. By the way, the code
    – the *heart of the zoned buddy allocator*,as its comment mentions, is here: `mm/page_alloc.c:__alloc_pages_nodemask()`.
    Being beyond the scope of this book, we won't attempt to delve into the code-level
    details of the allocator.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你会明白前面的描述是概念性的。实际的代码实现当然更复杂和优化。顺便说一句，代码-作为**分区伙伴分配器的核心**，正如它的注释所提到的，就在这里：`mm/page_alloc.c:__alloc_pages_nodemask()`。超出了本书的范围，我们不会尝试深入研究分配器的代码级细节。
- en: Working through a few scenarios
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过几种情景来工作
- en: 'Now that we have the basics of the algorithm, let''s consider a few scenarios:
    first, a simple straightforward case, and after that, a couple of more complex
    cases.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了算法的基础，让我们考虑一些情景：首先是一个简单直接的情况，然后是一些更复杂的情况。
- en: '**The simplest case**'
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**最简单的情况**'
- en: Let's say that a kernel-space device driver (or some core code) requests 128
    KB and receives a memory chunk from the order 5 list of one of the freelist data
    structures. At some later point in time, it will necessarily free the memory chunk
    by employing one of the page allocator free APIs. Now, this API's algorithm calculates
    – via its order – that the just-freed chunk belongs on the order 5 list; thus,
    it enqueues it there.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个内核空间设备驱动程序（或一些核心代码）请求128 KB，并从一个空闲列表数据结构的order 5列表中接收到一个内存块。在以后的某个时间点，它将必然通过使用页面分配器的一个free
    API来释放内存块。现在，这个API的算法通过它的order计算出刚刚释放的块属于order 5列表；因此，它将其排队在那里。
- en: '**A more complex case**'
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**更复杂的情况**'
- en: Now, let's say that, unlike the previous simple case, when the device driver
    requests 128 KB, the order 5 list is null; thus, as per the page allocator algorithm,
    we go to the list on the next order, 6, and check it. Let's say it's non-null;
    the algorithm now dequeues a 256 KB chunk and splits (or cuts) it in half. Now,
    one half (of size 128 KB) goes to the requester, and the remaining half (again,
    of size 128 KB) is enqueued on to the order 5 list.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设与之前的简单情况不同，当设备驱动程序请求128 KB时，order 5列表为空；因此，根据页面分配器算法，我们转到下一个order 6的列表并检查它。假设它不为空；算法现在出列一个256
    KB的块并将其分割（或切割）成两半。现在，一半（大小为128 KB）发送给请求者，剩下的一半（同样大小为128 KB）排队到order 5列表。
- en: The really interesting property of the buddy system is what happens when the
    requester (the device driver), at some later point in time, frees the memory chunk.
    As expected, the algorithm calculates (via its order) that the just-freed chunk
    belongs on the order 5 list. But before blindly enqueuing it there, **it looks
    for its buddy block**, and in this case, it (possibly) finds it! It now merges
    the two buddy blocks into a single larger block (of size 256 KB) and places (enqueues)
    the merged block on the *order 6 *list. This is fantastic – it has actually helped defragment
    memory!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 伙伴系统的真正有趣的特性是当请求者（设备驱动程序）在以后的某个时间点释放内存块时会发生什么。正如预期的那样，算法通过它的order计算出刚刚释放的块属于order
    5列表。但在盲目地将其排队到那里之前，**它会寻找它的伙伴块**，在这种情况下，它（可能）找到了！现在它将两个伙伴块合并成一个更大的块（大小为256 KB）并将合并后的块排队到*order
    6*列表。这太棒了-它实际上帮助了**碎片整理内存**！
- en: '**The downfall case**'
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**失败的情况**'
- en: Let's make it interesting now by not using a convenient rounded power-of-2 size
    as the requirement. This time, let's say that the device driver requests a memory
    chunk of size 132 KB. What will the buddy system allocator do? As, of course,
    it cannot allocate less memory than requested, it allocates more – you guessed
    it (see *Figure 8.2*), the next available memory chunk is on order 7, of size
    256 KB. But the consumer (the driver) is only going to see and use the first 132
    KB of the 256 KB chunk allocated to it. The remaining (124 KB) is wasted (think
    about it, that's close to 50% wastage!).This is called **internal fragmentation
    (or wastage)** and is the critical failing of the binary buddy system!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过不使用方便的2的幂大小作为需求来增加趣味性。这一次，假设设备驱动程序请求大小为132 KB的内存块。伙伴系统分配器会怎么做？当然，它不能分配比请求的内存更少，它会分配更多-你猜到了（见*图8.2*），下一个可用的内存块是大小为256
    KB的order 7。但消费者（驱动程序）只会看到并使用分配给它的256 KB块的前132 KB。剩下的（124 KB）是**浪费**的（想想看，接近50%的浪费！）。这被称为**内部碎片（或浪费）**，是二进制伙伴系统的关键失败！
- en: 'You will learn, though, that there is indeed a mitigation to this: a patch
    was contributed to deal with similar scenarios (via the `alloc_pages_exact() /
    free_pages_exact()` APIs). We will cover the APIs to use the page allocator shortly.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，对于这种情况确实有一种缓解方法：有一个补丁用于处理类似的情况（通过`alloc_pages_exact() / free_pages_exact()`
    API）。我们将很快介绍使用页面分配器的API。
- en: Page allocator internals – a few more details
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 页面分配器内部-更多细节
- en: 'In this book, we do not intend to delve into code-level detail on the internals
    of the page allocator. Having said that, here''s the thing: in terms of data structures,
    the `zone` structure contains an array of `free_area` structures. This makes sense;
    as you''ve learned, there can be (and usually are) multiple page allocator freelists
    on the system, one per node:zone:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们不打算深入研究页面分配器内部的代码级细节。话虽如此，事实是：在数据结构方面，`zone`结构包含一个`free_area`结构的数组。这是有道理的；正如你所学到的，系统上可以有（通常有）多个页面分配器空闲列表，每个节点：区域一个：
- en: '[PRE0]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `free_area` structure is the implementation of the doubly-linked circular
    lists (of free memory page frames within that node:zone) along with the number
    of page frames that are currently free:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`free_area`结构是双向循环链表的实现（在该节点：区域内的空闲内存页框中）以及当前空闲的页框数量：'
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Why is it an array of linked lists and not just one list? Without delving into
    the details, we''ll mention that, in reality, the kernel layout for the buddy
    system freelists is more complex than let on until now: from the 2.6.24 kernel,
    each freelist we have seen is actually further broken up into multiple freelists
    to cater to different *page migration types*. This was required to deal with complications
    when trying to keep memory defragmented.Besides that, as mentioned earlier, these
    freelists exist per *node:zone* on the system. So, for example, on an actual NUMA
    system with 4 nodes and 3 zones per node, there will be 12 (4 x 3) freelists.
    Not just that, each freelist is actually further broken down into 6 freelists,
    one per migration type. Thus, on such a system, a total of *6 x 12 = 72* freelist
    data structures would exist system-wide!'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么是一个链表数组而不是一个链表？不深入细节，我们将提到，实际上，到目前为止，伙伴系统空闲列表的内核布局比表面上的更复杂：从2.6.24内核开始，我们看到的每个空闲列表实际上进一步分解为多个空闲列表，以满足不同的*页面迁移类型*。这是为了处理在尝试保持内存碎片整理时出现的复杂情况。除此之外，如前所述，这些空闲列表存在于系统上的每个*节点：区域*。因此，例如，在一个实际的NUMA系统上，每个节点有4个区域，每个节点有3个区域，将有12（4
    x 3）个空闲列表。不仅如此，每个空闲列表实际上进一步分解为6个空闲列表，每个迁移类型一个。因此，在这样的系统上，整个系统将存在*6 x 12 = 72*个空闲列表数据结构！
- en: If you are interested, dig into the details and check out the output of `/proc/buddyinfo` – a
    nice summary view of the state of the buddy system freelists (as Figure 8.3 shows).
    Next, for a more detailed and realistic view (of the type mentioned previously,
    showing *all *the freelists), look up `/proc/pagetypeinfo` (requires root access)
    – it shows all the freelists (broken up into page migration types as well).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您感兴趣，请深入了解细节，并查看`/proc/buddyinfo`的输出-这是伙伴系统空闲列表状态的一个很好的总结视图（如图8.3所示）。接下来，为了获得更详细和更现实的视图（如前面提到的类型，显示*所有*空闲列表），查看`/proc/pagetypeinfo`（需要root访问）-它显示所有空闲列表（也分解为页面迁移类型）。
- en: The design of the page allocator (buddy system) algorithm is one of the best-fit class.
    It confers the major benefit of actually helping to defragment physical memory
    as the system runs. Briefly, its pros and cons are as follows.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 页面分配器（伙伴系统）算法的设计是*最佳适配*类之一。它的主要优点是实际上有助于在系统运行时整理物理内存。简而言之，它的优缺点如下。
- en: 'The pros of the page allocator (buddy system) algorithm are as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 页面分配器（伙伴系统）算法的优点如下：
- en: Helps defragment memory (external fragmentation is prevented)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有助于碎片整理内存（防止外部碎片）
- en: Guarantees the allocation of a physically contiguous memory chunk
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保证分配物理连续的内存块
- en: Guarantees CPU cache line-aligned memory blocks
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保证CPU缓存行对齐的内存块
- en: Fast (well, fast enough; the algorithmic time complexity is *O(log n)*)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速（足够快；算法时间复杂度为*O(log n)*）
- en: On the other hand, by far the biggest downside is that internal fragmentation
    or wastage can be much too high.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，迄今为止最大的缺点是内部碎片或浪费可能过高。
- en: 'Okay, great! We have covered a good deal of background material on the internal
    workings of the page or buddy system allocator. Time to get hands on: let''s now
    dive into actually understanding and using the page allocator APIs to allocate
    and free memory.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，很棒！我们已经涵盖了页面或伙伴系统分配器内部工作的大量背景材料。现在是动手的时候：让我们现在深入了解并使用页面分配器API来分配和释放内存。
- en: Learning how to use the page allocator APIs
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何使用页面分配器API
- en: 'The Linux kernel provides (exposes to the core and modules) a set of APIs to
    allocate and deallocate memory (RAM) via the page allocator. These are often referred
    to as the low-level (de)allocator routines. The following table summarizes the
    page allocation APIs; you''ll notice that all the APIs or macros that have two
    parameters, the first parameter is called the *GFP flags or bitmask*; we shall
    explain it in detail shortly, please ignore it for now. The second parameters
    is the `order`- the order of the freelist, that is, the amount of memory to allocate
    is 2^(order) page frames. All prototypes can be found in `include/linux/gfp.h`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Linux内核提供了一组API来通过页面分配器分配和释放内存（RAM），这些通常被称为低级（de）分配器例程。以下表格总结了页面分配API；您会注意到所有具有两个参数的API或宏，第一个参数称为*GFP标志或位掩码*；我们将很快详细解释它，请现在忽略它。第二个参数是`order`-空闲列表的顺序，即要分配的内存量为2^(order)页帧。所有原型都可以在`include/linux/gfp.h`中找到：
- en: '| **API or macro name** | **Comments** | **API signature or macro** |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| **API或宏名称** | **评论** | **API签名或宏** |'
- en: '| `__get_free_page()` | Allocates exactly one page frame. The allocated memory
    will have random content; it''s a wrapper around the `__get_free_pages()` API.
    The return value is a pointer to the just-allocated memory''s kernel logical address.
    | `#define __get_free_page(gfp_mask)  \ __get_free_pages((gfp_mask), 0)`​ |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| `__get_free_page()` | 分配一个页面帧。分配的内存将具有随机内容；它是`__get_free_pages()`API的包装器。返回值是刚分配的内存的内核逻辑地址的指针。
    | `#define __get_free_page(gfp_mask) \ __get_free_pages((gfp_mask), 0)`​ |'
- en: '| `__get_free_pages()` | Allocates *2^(order)* physically contiguous page frames.
    Allocated memory will have random content; the return value is a pointer to the
    just-allocated memory''s kernel logical address. | `unsigned long __get_free_pages(gfp_t
    gfp_mask, unsigned int order);` |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| `__get_free_pages()` | 分配*2^(order)*个物理连续的页面帧。分配的内存将具有随机内容；返回值是刚分配的内存的内核逻辑地址的指针。
    | `unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order);` |'
- en: '| `get_zeroed_page()` | Allocates exactly one page frame; its contents are
    set to ASCII zero (`NULL`; that is, it''s zeroed out); the return value is a pointer
    to the just-allocated memory''s kernel logical address. | `unsigned long get_zeroed_page(gfp_t
    gfp_mask);` |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| `get_zeroed_page()` | 分配一个页面帧；其内容设置为ASCII零（`NULL`；即，它被清零）；返回值是刚分配的内存的内核逻辑地址的指针。
    | `unsigned long get_zeroed_page(gfp_t gfp_mask);` |'
- en: '| `alloc_page()` | Allocates exactly one page frame. The allocated memory will
    have random content; a wrapper over the `alloc_pages()` API; the return value
    is a pointer to the just-allocated memory''s `page` metadata structure; can convert
    it into a kernel logical address via the `page_address()` function. | `#define
    alloc_page(gfp_mask)  \ alloc_pages(gfp_mask, 0)` |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| `alloc_page()` | 分配一个页面帧。分配的内存将具有随机内容；是`alloc_pages()` API的包装器；返回值是指向刚分配的内存的`page`元数据结构的指针；可以通过`page_address()`函数将其转换为内核逻辑地址。
    | `#define alloc_page(gfp_mask) \ alloc_pages(gfp_mask, 0)` |'
- en: '| `alloc_pages()` | Allocates *2^(order)* physically contiguous page frames.
    The allocated memory will have random content; the return value is a pointer to
    the start of the just-allocated memory''s `page` metadata structure; can convert
    it into a kernel logical address via the `page_address()` function. | `struct
    page * alloc_pages(gfp_t gfp_mask, unsigned int order);` |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| `alloc_pages()` | 分配*2^(order)*个物理连续页面帧。分配的内存将具有随机内容；返回值是指向刚分配的内存的`page`元数据结构开头的指针；可以通过`page_address()`函数将其转换为内核逻辑地址。
    | `struct page * alloc_pages(gfp_t gfp_mask, unsigned int order);` |'
- en: Table 8.1 – Low-level (BSA/page) allocator – popular exported allocation APIs
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 - 低级（BSA/page）分配器 - 流行的导出分配API
- en: All the preceding APIs are exported (via the `EXPORT_SYMBOL()` macro), and hence
    available to kernel module and device driver developers. Worry not, you will soon
    see a kernel module that demonstrates using them.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 所有先前的API都是通过`EXPORT_SYMBOL()`宏导出的，因此可供内核模块和设备驱动程序开发人员使用。不用担心，您很快就会看到一个演示如何使用它们的内核模块。
- en: 'The Linux kernel considers it worthwhile to maintain a (small) metadata structure
    to track every single page frame of RAM. It''s called the `page` structure. The
    point here is, be careful: unlike the usual semantics of returning a pointer (a
    virtual address) to the start of the newly allocated memory chunk, notice how
    both the `alloc_page()` and `alloc_pages()` APIs mentioned previously return a
    pointer to the start of the newly allocated memory''s page structure, not the
    memory chunk itself (as the other APIs do). You must obtain the actual pointer
    to the start of the newly allocated memory by invoking the `page_address()` API
    on the page structure address that is returned. Example code in the *Writing a
    kernel module to demo using the page allocator APIs* sectionwill illustrate the
    usage of all of the preceding APIs.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Linux内核认为维护一个（小）元数据结构来跟踪每个RAM页面帧是值得的。它被称为`page`结构。关键在于，要小心：与通常的返回指向新分配的内存块开头的指针（虚拟地址）的语义不同，注意先前提到的`alloc_page()`和`alloc_pages()`
    API都返回指向新分配的内存的`page`结构开头的指针，而不是内存块本身（其他API所做的）。您必须通过调用返回的页面结构地址上的`page_address()`
    API来获取新分配的内存开头的实际指针。在*编写内核模块以演示使用页面分配器API*部分的示例代码将说明所有先前API的用法。
- en: Before we can make use of the page allocator APIs mentioned here, though, it's
    imperative to understand at least the basics regarding the **Get Free Page** (**GFP**) flags,
    which are the topic of the section that follows.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里提到的页面分配器API之前，至关重要的是了解至少关于**获取空闲页面**（GFP）标志的基础知识，这是接下来的部分的主题。
- en: Dealing with the GFP flags
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理GFP标志
- en: 'You will notice that the first parameter to all the previous allocator APIs
    (or macros) is `gfp_t gfp_mask`. What does this mean? Essentially, these are GFP flags*.*
    These are flags (there are several of them) used by the kernel''s internal memory
    management code layers. For all practical purposes, for the typical kernel module
    (or device driver) developer, just two GFP flags are crucial (as mentioned before,
    the rest are for internal usage). They are as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到所有先前的分配器API（或宏）的第一个参数是`gfp_t gfp_mask`。这是什么意思？基本上，这些是GFP标志。这些是内核内部内存管理代码层使用的标志（有几个）。对于典型的内核模块（或设备驱动程序）开发人员来说，只有两个GFP标志至关重要（如前所述，其余是用于内部使用）。它们如下：
- en: '`GFP_KERNEL`'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GFP_KERNEL`'
- en: '`GFP_ATOMIC`'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GFP_ATOMIC`'
- en: 'Deciding which of these to use when performing memory allocation via the page
    allocator APIs is important; a key rule to always remember is the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过页面分配器API执行内存分配时决定使用哪个是重要的；始终记住的一个关键规则是：
- en: '*If in process context and it is safe to sleep, use the GFP_KERNEL flag. If
    it is unsafe to sleep (typically, when in any type of atomic or interrupt context), you must use
    the GFP_ATOMIC flag.*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果在进程上下文中并且可以安全休眠，则使用GFP_KERNEL标志。如果不安全休眠（通常在任何类型的原子或中断上下文中），必须使用GFP_ATOMIC标志。*'
- en: Following the preceding rule is critical. Getting this wrong can result in the
    entire machine freezing, kernel crashes, and/or random bad stuff happening. So,
    what exactly do the statements *safe/unsafe to sleep* really mean? For this and
    more, we defer to the *The GFP flags – digging deeper *section that follows. It *is*
    reallyimportant though, so I definitely recommend you read it.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循上述规则至关重要。搞错了会导致整个机器冻结、内核崩溃和/或发生随机的不良情况。那么*安全/不安全休眠*这些陈述到底意味着什么？为此以及更多内容，我们推迟到接下来的*深入挖掘GFP标志*部分。尽管如此，这真的很重要，所以我强烈建议您阅读它。
- en: '**Linux Driver Verification** (**LDV**) project: back in [Chapter 1](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml),
    *Kernel Workspace Setup*, in the The LDV - Linux Driver Verification - project section,
    we mentioned that this project has useful "rules" with respect to various programming
    aspects of Linux modules (drivers, mostly) as well as the core kernel.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**Linux驱动程序验证**（LDV）项目：回到[第1章](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml)，*内核工作空间设置*，在*LDV
    - Linux驱动程序验证*项目部分，我们提到该项目对于Linux模块（主要是驱动程序）以及核心内核的各种编程方面有有用的“规则”。'
- en: 'With regard to our current topic, here''s one of the rules, a negative one,
    implying that you *cannot *do this: *"Using a blocking memory allocation when
    spinlock is held"* ([http://linuxtesting.org/ldv/online?action=show_rule&rule_id=0043](http://linuxtesting.org/ldv/online?action=show_rule&rule_id=0043)).
    When holding a spinlock, you''re not allowed to do anything that might block;
    this includes kernel-space memory allocations. Thus, very important, you must
    use the `GFP_ATOMIC` flag when performing a memory allocation in any kind of atomic
    or non-blocking context, like when holding a spinlock (you will learn that this
    isn''t the case with the mutex lock; you are allowed to perform blocking activities
    while holding a mutex). Violating this rule leads to instability and even raises
    the possibility of (an implicit) deadlock. The LDV page mentions a device driver
    that was violating this very rule and the subsequent fix ([https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=5b0691508aa99d309101a49b4b084dc16b3d7019](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=5b0691508aa99d309101a49b4b084dc16b3d7019)).
    Take a look: the patch clearly shows (in the context of the `kzalloc()` API, which
    we shall soon cover) the `GFP_KERNEL` flag being replaced with the `GFP_ATOMIC`
    flag.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我们当前的主题，这里有一个规则，一个否定的规则，暗示着你*不能*这样做：“在持有自旋锁时使用阻塞内存分配”([http://linuxtesting.org/ldv/online?action=show_rule&rule_id=0043](http://linuxtesting.org/ldv/online?action=show_rule&rule_id=0043))。持有自旋锁时，你不允许做任何可能会阻塞的事情；这包括内核空间内存分配。因此，非常重要的是，在任何原子或非阻塞上下文中执行内存分配时，必须使用`GFP_ATOMIC`标志，比如在持有自旋锁时（你会发现这在互斥锁中并不适用；在持有互斥锁时，你可以执行阻塞活动）。违反这个规则会导致不稳定，甚至可能引发（隐式）死锁的可能性。LDV页面提到了一个违反这个规则的设备驱动程序以及随后的修复([https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=5b0691508aa99d309101a49b4b084dc16b3d7019](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=5b0691508aa99d309101a49b4b084dc16b3d7019))。看一下：补丁清楚地显示了（在我们即将介绍的`kzalloc()`API的上下文中）`GFP_KERNEL`标志被替换为`GFP_ATOMIC`标志。
- en: Another GFP flag commonly used is `__GFP_ZERO`. Its usage implies to the kernel
    that you want zeroed-out memory pages. It's often bitwise-ORed with `GFP_KERNEL`
    or `GFP_ATOMIC` flags in order to return memory initialized to zero.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用的GFP标志是`__GFP_ZERO`。它的使用向内核表明你想要零化的内存页面。它经常与`GFP_KERNEL`或`GFP_ATOMIC`标志按位或操作，以返回初始化为零的内存。
- en: 'The kernel developers do take the trouble to document the GFP flags in detail.
    Take a look in `include/linux/gfp.h`. Within it, there''s a long and detailed
    comment; it''s headed `DOC: Useful GFP flag combinations`.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '内核开发人员确实费心详细记录了GFP标志。在`include/linux/gfp.h`中有一个长而详细的注释；标题是`DOC: 有用的GFP标志组合`。'
- en: For now, and so that we get off the ground quickly, just understand that using
    the Linux kernel's memory allocation APIs with the `GFP_KERNEL` flag is indeed
    the common case for kernel-internal allocations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，为了让我们快速入门，只需了解使用`GFP_KERNEL`标志与Linux内核的内存分配API确实是内核内部分配的常见情况。
- en: Freeing pages with the page allocator
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用页面分配器释放页面
- en: 'The flip side of allocating memory is freeing it, of course. Memory leakage
    in the kernel is definitely not something you''d like to contribute to. For the
    page allocator APIs shown in *Table 8.1**,* here are the corresponding free APIs:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，分配内存的另一面是释放内存。内核中的内存泄漏绝对不是你想要贡献的东西。在*表8.1*中显示的页面分配器API中，这里是相应的释放API：
- en: '| **API or macro name** | **Comment** | **API signature or macro** |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **API或宏名称** | **评论** | **API签名或宏** |'
- en: '| `free_page()` | Free a (single) page that was allocated via the `__get_free_page()`,
    `get_zeroed_page()`, or `alloc_page()` APIs; it''s a simple wrapper over the `free_pages()`
    API | `#define free_page(addr) __free_pages((addr), 0)` |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| `free_page()` | 释放通过`__get_free_page()`、`get_zeroed_page()`或`alloc_page()`API分配的（单个）页面；它只是`free_pages()`API的简单包装
    | `#define free_page(addr) __free_pages((addr), 0)` |'
- en: '| `free_pages()` | Free multiple pages that were allocated via the `__get_free_pages()` or `alloc_pages()` APIs
    (it''s actually a wrapper over `__free_pages()`.)  | `void free_pages(unsigned
    long addr, unsigned int order)` |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| `free_pages()` | 释放通过`__get_free_pages()`或`alloc_pages()`API分配的多个页面（实际上是`__free_pages()`的包装）
    | `void free_pages(unsigned long addr, unsigned int order)` |'
- en: '| `__free_pages()` | (*Same as the preceding row, plus*) it''s the underlying
    routine where the work gets done; also, note that the first parameter is a pointer
    to the `page` metadata structure. | `void __free_pages(struct page *page, unsigned
    int order)` |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| `__free_pages()` | （与前一行相同，另外）这是执行实际工作的基础例程；还要注意第一个参数是指向`page`元数据结构的指针。 |
    `void __free_pages(struct page *page, unsigned int order)` |'
- en: Table 8.2 – Common free page(s) APIs to use with the page allocator
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.2 - 与页面分配器一起使用的常见释放页面API
- en: You can see that the actual underlying API in the preceding functions is `free_pages()`,
    which itself is just a wrapper over the `mm/page_alloc.c:__free_pages()` code.
    The first parameter to the `free_pages()` API is the pointer to the start of the
    memory chunk being freed; this, of course, being the return value from the allocation
    routine. However, the first parameter to the underlying API, `__free_pages()`,
    is the pointer to the *page* metadata structure of the start of the memory chunk
    being freed.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到前面函数中实际的基础API是`free_pages()`，它本身只是`mm/page_alloc.c:__free_pages()`代码的包装。`free_pages()`API的第一个参数是指向被释放内存块的起始指针；当然，这是分配例程的返回值。然而，基础API`__free_pages()`的第一个参数是指向被释放内存块的*page*元数据结构的指针。
- en: Generally speaking, unless you really know what you are doing, you're definitely
    advised to invoke the `foo()` wrapper routine and not its internal `__foo()` routine.
    One reason to do so is simply correctness (perhaps the wrapper uses some necessary
    synchronization mechanism - like a lock - prior to invoking the underlying routine).
    Another reason to do so is validity checking (which helps code remain robust and
    secure). *O*ften, the `__foo()` routines bypass validity checks in favor of speed.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，除非您真的知道自己在做什么，您肯定应该调用`foo()`包装例程而不是其内部的`__foo()`例程。这样做的一个原因是简单的正确性（也许包装器在调用底层例程之前使用了一些必要的同步机制
    - 比如锁）。另一个原因是有效性检查（这有助于代码保持健壮和安全）。通常，`__foo()`例程会绕过有效性检查以换取速度。
- en: As all experienced C/C++ application developers know, allocating and subsequently
    freeing memory is a rich source of bugs! This is primarily because C is an unmanaged
    language, as far as memory is concerned; hence, you can hit all sorts of memory
    bugs. These include the well-known memory leakage, buffer overflows/underflows
    for both read/write, double-free, and **Use After Free** (**UAF**) bugs.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所有有经验的C/C++应用程序开发人员所知，分配和随后释放内存是错误的丰富来源！这主要是因为C是一种无管理语言，就内存而言；因此，您可能会遇到各种各样的内存错误。这些包括众所周知的内存泄漏，读/写的缓冲区溢出/下溢，双重释放和**使用后释放**（UAF）错误。
- en: 'Unfortunately, it''s no different in kernel space; it''s just that the consequences
    are (much) worse! Be extra careful! Please do take care to ensure the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在内核空间中也没有什么不同；只是后果会更严重！要特别小心！请务必确保以下内容：
- en: Favor routines that initialize the memory allocated to zero.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏爱初始化分配的内存为零的例程。
- en: 'Think about and use the appropriate GFP flag when performing an allocation
    – more on this in the *The GFP flags – digging deeper* section, but briefly, note
    the following:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在执行分配时考虑并使用适当的GFP标志 - 更多内容请参阅*GFP标志 - 深入挖掘*部分，但简而言之，请注意以下内容：
- en: When in process context where it's safe to sleep, use `GFP_KERNEL`.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可以安全休眠的进程上下文中，使用`GFP_KERNEL`。
- en: When in an atomic context, such as when processing an interrupt, use `GFP_ATOMIC`.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在原子上下文中，比如处理中断时，使用`GFP_ATOMIC`。
- en: When using the page allocator (as we're doing now), try as much as possible
    to keep the allocation size as rounded power-of-2 pages (again, the rationale
    behind this and ways to mitigate this – when you don't require so much memory,
    the typical case – are covered in detail in the coming sections of this chapter).
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用页面分配器时（就像我们现在正在做的那样），尽量保持分配大小为圆整的2的幂页（关于这一点的原因以及在不需要这么多内存时如何减轻这一点 - 典型情况下
    - 将在本章后续部分详细介绍）。
- en: You only ever attempt to free memory that you allocated earlier; needless to
    say, don't miss freeing it, and don't double-free it.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您只会尝试释放您之前分配的内存；不用说，不要忘记释放它，也不要重复释放它。
- en: Keep the original memory chunk's pointer safe from reuse, manipulation (`ptr
    ++` or something similar), and corruption, so that you can correctly free it when
    done.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保原始内存块的指针不受重用、操纵（`ptr ++`或类似的操作）和破坏，以便在完成时正确释放它。
- en: Check (and recheck!) the parameters passed to APIs. Is a pointer to the previously
    allocated block required, or to its underlying `page` structure?
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查（并再次检查！）传递给API的参数。是否需要指向先前分配的块或其底层`page`结构的指针？
- en: Finding it difficult and/or worried about issues in production? Don't forget,
    you have help! Do learn how to use powerful static analysis tools found within
    the kernel itself (Coccinelle,  `sparse` and others, such as `cppcheck` or `smatch`).
    For dynamic analysis, learn how to install and use **KASAN** (the **Kernel Address
    Sanitizer**).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中发现困难和/或担心问题？别忘了，您有帮助！学习如何使用内核内部的强大静态分析工具（Coccinelle、`sparse`和其他工具，如`cppcheck`或`smatch`）。对于动态分析，学习如何安装和使用**KASAN**（内核地址消毒剂）。
- en: Recall the Makefile template I provided in [Chapter 5](408b6f9d-42dc-4c59-ab3d-1074d595f9e2.xhtml),
    *Writing Your First Kernel Module – LKMs Part 2*, in the *A better Makefile template* section.
    It contains targets that use several of these tools; please do use it!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我在[第5章](408b6f9d-42dc-4c59-ab3d-1074d595f9e2.xhtml)中提供的Makefile模板，*编写您的第一个内核模块
    - LKMs第2部分*，在*A better Makefile template*部分。它包含使用了几种这些工具的目标；请使用它！
- en: Alright, now that we've covered both the (common) allocation and free APIs of
    the page allocator, it's time to put this learning to use. Let's write some code!
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，既然我们已经涵盖了页面分配器的（常见的）分配和释放API，现在是时候将这些知识付诸实践了。让我们写一些代码！
- en: Writing a kernel module to demo using the page allocator APIs
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写一个内核模块来演示使用页面分配器API
- en: Let's now get hands on with the low-level page allocator and free APIs that
    we've learned about so far. In this section, we will show relevant code snippets,
    followed by an explanation where warranted, from our demo kernel module (`ch8/lowlevel_mem/lowlevel_mem.c`).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们动手使用我们迄今为止学到的低级页面分配器和释放API。在本节中，我们将展示相关的代码片段，然后在必要时进行解释，来自我们的演示内核模块（`ch8/lowlevel_mem/lowlevel_mem.c`）。
- en: 'In the primary worker routine, `bsa_alloc()`, of our small LKM, we highlighted
    (in bold font) the code comments that show what we are trying to achieve. A few
    points to note:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们小型LKM的主要工作例程`bsa_alloc()`中，我们用粗体字突出显示了显示我们试图实现的代码注释。需要注意的几点：
- en: 'First, we do something very interesting: we use our small kernel "library"
    function `klib_llkd.c:show_phy_pages()` to literally show you how physical RAM
    page frames are identity mapped to kernel virtual pages in the kernel lowmem region!
    (The exact working of the `show_phy_pages()` routine is discussed very shortly):'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们做了一些非常有趣的事情：我们使用我们的小内核“库”函数`klib_llkd.c:show_phy_pages()`，直接向您展示物理RAM页框如何在内核低端内存区域与内核虚拟页进行身份映射！（`show_phy_pages()`例程的确切工作将很快讨论）：
- en: '[PRE2]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we allocate one page of memory via the underlying `__get_free_page()`
    page allocator API (that we saw previously in *Table 8.1*):'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过底层的`__get_free_page()`页面分配器API分配一页内存（我们之前在*表8.1*中看到过）：
- en: '[PRE3]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice how we emit a `printk` function showing the kernel's logical address. Recall
    from the previous chapter that this is page allocator memory that lies very much
    in the direct-mapped RAM or lowmem region of the kernel segment/VAS.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Now, for security, we should consistently, and only, use the `%pK` format specifier
    when printing kernel addresses so that a hashed value and not the real virtual
    address shows up in the kernel logs. However, here, in order to show you the actual
    kernel virtual address, we also use the `%px` format specifier (which, like the
    `%pK`, is portable as well; for security, please don't use the `%px` format specifier
    in production!).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Next, notice the detailed comment just after the first `__get_free_page()` API
    (in the preceding snippet) is issued. It mentions the fact that you don't really
    have to print an out-of-memory error or warning messages. (Curious? To find out
    why, visit [https://lkml.org/lkml/2014/6/10/382](https://lkml.org/lkml/2014/6/10/382).)
    In this example module (as with several earlier ones and more to follow), we code
    our printk's (or `pr_foo()` macro) instances for portability by using appropriate
    printk format specifiers (like the `%zd`, `%zu`, `%pK`, `%px`, and `%pa`).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s move on to our second memory allocation using the page allocator; see
    the following code snippet:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding code snippet (see the code comments), we have allocated 2³ –
    that is, 8 – pages of memory via the page allocator's `__get_free_pages()` API
    (as the default value of our module parameter, `bsa_alloc_order`, is `3`).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'An aside: notice that we use the `GFP_KERNEL|__GFP_ZERO` GFP flags to ensure
    that the allocated memory is zeroed out, a best practice. Then again, zeroing
    out large memory chunks can result in a slight performance hit.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we ask ourselves the question: is there a way to verify that the memory
    is really physically contiguous (as promised)? It turns out that yes, we can actually
    retrieve and print out the physical address of the start of each allocated page
    frame and retrieve its **Page Frame Number** **(PFN****)** as well.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'The **PFN** is a simple concept: it''s just the index or page number – for
    example, the PFN of physical address 8192 is 2 (*8192/4096*). As we''ve shown
    how to (and importantly, when you can) translate kernel virtual addresses to their
    physical counterparts earlier (and vice versa; this coverage is in [Chapter 7](06ee05b5-3e71-482d-93b8-235c27ce23bc.xhtml),
    *Memory Management Internals – Essentials*, in the *Direct-mapped RAM and address
    translation* section), we won''t repeat it here.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this work of translating virtual addresses to physical addresses and
    checking for contiguity, we write a small "library" function, which is kept in
    a separate C file in the root of this book''s GitHub source tree, `klib_llkd.c`. Our
    intent is to modify our kernel module''s Makefile to link in the code of this
    library file as well! (Doing this properly was  covered back in [Chapter 5](408b6f9d-42dc-4c59-ab3d-1074d595f9e2.xhtml), *Writing
    Your First Kernel Module – LKMs Part 2*, in the *Performing library emulation
    via multiple source files* section.) Here''s our invocation of our library routine
    (just as was done in step 0):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following is the code of our library routine (in the `<booksrc>/klib_llkd.c`
    source file; again, for clarity, we won''t show the entire code here):'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Study the preceding function. We walk through our given memory range, (virtual)
    page by (virtual) page, obtaining the physical address and PFN, which we then
    emit via printk (notice how we use the `%pa` format specifier to port-ably print
    a *physical address* - it requires it to be passed by reference though). Not only
    that, if the third parameter, `contiguity_check`*,* is `1`, we check whether the
    PFNs are just a single digit apart, thus checking that the pages are indeed physically
    contiguous or not. (By the way, the simple `powerof()` function that we make use
    of is also within our library code.)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Hang on, though, a key point: having kernel modules working with physical addresses
    is *highly discouraged*. Only the kernel''s internal memory management code works
    directly with physical addresses. There are very few real-world cases of even
    hardware device drivers using physical memory directly (DMA is one, and using
    the `*ioremap*` APIs another).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，有一个关键点：让内核模块与物理地址一起工作是*极不鼓励*的。只有内核的内部内存管理代码直接使用物理地址。甚至硬件设备驱动程序直接使用物理内存的真实案例非常少见（DMA是其中之一，使用`*ioremap*`API是另一个）。
- en: We only do so here to prove a point – that the memory allocated by the page
    allocator (with a single API call) is physically contiguous. Also, do realize
    that the `virt_to_phys()`(and friends) APIs that we employ are guaranteed to work
    *only* on direct-mapped memory (the kernel lowmem region) and nothing else (not
    the `vmalloc` range, the IO memory ranges, bus memory, DMA buffers, and so on).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只在这里这样做是为了证明一点-由页面分配器分配的内存（通过单个API调用）是物理连续的。此外，请意识到我们使用的`virt_to_phys()`（和其他）API保证仅在直接映射内存（内核低内存区域）上工作，而不是在`vmalloc`范围、IO内存范围、总线内存、DMA缓冲区等其他地方。
- en: 'Now, let''s continue with the kernel module code:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们继续进行内核模块代码：
- en: '[PRE7]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As seen in the preceding snippet, we allocate a single page of memory but ensure
    it's zeroed out by employing the PA `get_zeroed_page()` API. `pr_info()` shows
    the hashed and actual KVAs (using the `%pK` or `%px` has the addresses printed
    in a port-able fashion as well, irrespective of your running on a 32 or 64-bit
    system.)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码片段所示，我们分配了一页内存，但通过使用PA `get_zeroed_page()` API确保它被清零。`pr_info()`显示了哈希和实际的KVA（使用`%pK`或`%px`以便地址以便以可移植的方式打印，无论你是在32位还是64位系统上运行）。
- en: 'Next, we allocate one page with the `alloc_page()` API. Careful! It does not
    return the pointer to the allocated page, but rather the pointer to the metadata
    structure `page` representing the allocated page; here''s the function signature:
    `struct page * alloc_page(gfp_mask)`. Thus, we use the `page_address()` helper
    to convert it into a kernel logical (or virtual) address:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用`alloc_page()` API分配一页。小心！它不会返回分配页面的指针，而是返回代表分配页面的元数据结构`page`的指针；这是函数签名：`struct
    page * alloc_page(gfp_mask)`。因此，我们使用`page_address()`助手将其转换为内核逻辑（或虚拟）地址：
- en: '[PRE8]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding code snippet, we allocate one page of memory via the `alloc_page()`
    PA API. As explained, we need to convert the page metadata structure returned
    by it into a KVA (or kernel logical address) via the `page_address()` API.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们通过`alloc_page()` PA API分配了一页内存。正如所解释的，我们需要将其返回的页面元数据结构转换为KVA（或内核逻辑地址）通过`page_address()`
    API。
- en: 'Next, allocate and `init` *2^3 = 8 pages* with the `alloc_pages()` API. The
    same warning as the preceding code snippet applies here too:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`alloc_pages()` API分配和`init` *2^3 = 8页*。与前面的代码片段一样，这里也适用相同的警告：
- en: '[PRE9]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding code snippet, we combine `alloc_pages()` wrapped within a `page_address()`
    API to allocate *2^3 = 8* pages of memory!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们将`alloc_pages()`包装在`page_address()` API中，以分配*2^3 = 8*页内存！
- en: Interestingly, we use several local `goto` statements in the code (do peek at
    the code in the repo). Looking carefully at it, you will notice that it actually
    keeps error handling code paths clean and logical. This is indeed part of the
    Linux kernel coding style guidelines.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们在代码中使用了几个本地的`goto`语句（请在存储库中查看代码）。仔细观察，你会注意到它实际上保持了*错误处理代码路径*的清晰和逻辑。这确实是Linux内核*编码风格*指南的一部分。
- en: Usage of the (sometimes controversial) `goto` is clearly documented right here: [https://www.kernel.org/doc/html/v5.4/process/coding-style.html#centralized-exiting-of-functions](https://www.kernel.org/doc/html/v5.4/process/coding-style.html#centralized-exiting-of-functions).
    I urge you to check it out! Once you understand the usage pattern, you'll find
    that it helps reduce the all-too-typical memory leakage (and similar) cleanup
    errors!
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对（有时有争议的）`goto`的使用在这里清楚地记录在这里：[https://www.kernel.org/doc/html/v5.4/process/coding-style.html#centralized-exiting-of-functions](https://www.kernel.org/doc/html/v5.4/process/coding-style.html#centralized-exiting-of-functions)。我敦促你去查看！一旦你理解了使用模式，你会发现它有助于减少所有太典型的内存泄漏（等等）清理错误！
- en: Finally, in the cleanup method, prior to being removed from kernel memory, we
    free up all the memory chunks we just allocated in the cleanup code of the kernel
    module.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在清理方法中，在从内核内存中删除之前，我们释放了在内核模块的清理代码中刚刚分配的所有内存块。
- en: 'In order to link our library `klib_llkd` code with our `lowlevel_mem`kernel
    module, the Makefile changes to have the following (recall that we learned about
    compiling multiple source files into a single kernel module in [Chapter 5](408b6f9d-42dc-4c59-ab3d-1074d595f9e2.xhtml), *Writing
    Your First Kernel Module – LKMs Part 2*, in the *Performing library emulation
    via multiple source files* section):'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将我们的库`klib_llkd`代码与我们的`lowlevel_mem`内核模块链接起来，`Makefile`更改为以下内容（回想一下，我们在[第5章](408b6f9d-42dc-4c59-ab3d-1074d595f9e2.xhtml)中学习了如何将多个源文件编译成单个内核模块，*编写你的第一个内核模块-LKMs第2部分*，在*通过多个源文件执行库模拟*部分）：
- en: '[PRE10]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Again, in this sample LKM we often used the `%px` printk format specifier so
    that we can see the actual virtual address and not a hashed value (kernel security
    feature). It's okay here, but don't do this in production.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在这个示例LKM中，我们经常使用`%px` printk格式说明符，以便我们可以看到实际的虚拟地址而不是哈希值（内核安全功能）。在这里可以，但在生产中不要这样做。
- en: Phew! That was quite a bit to cover. Do ensure you understand the code, and
    then read on to see it in action.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！这是相当多的内容。确保你理解了代码，然后继续看它的运行。
- en: Deploying our lowlevel_mem_lkm kernel module
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署我们的lowlevel_mem_lkm内核模块
- en: Okay, time to see our kernel module in action! Let's build and deploy it on
    both a Raspberry Pi 4 (running the default Raspberry Pi OS) and on an x86_64 VM
    (running Fedora 31).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，是时候看看我们的内核模块在运行中的情况了！让我们在树莓派4（运行默认的树莓派OS）和x86_64 VM（运行Fedora 31）上构建和部署它。
- en: 'On the Raspberry Pi 4 Model B (here running Raspberry Pi kernel version 5.4.79-v7l+),
    we build and then `insmod(8)` our `lowlevel_mem_lkm`kernel module. The following
    screenshot shows the output:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在Raspberry Pi 4 Model B上（运行Raspberry Pi内核版本5.4.79-v7l+），我们构建然后`insmod(8)`我们的`lowlevel_mem_lkm`内核模块。以下截图显示了输出：
- en: '![](img/08a93d27-6112-4b32-8314-20969b47d182.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08a93d27-6112-4b32-8314-20969b47d182.png)'
- en: Figure 8.5 – The lowlevel_mem_lkm kernel module's output on a Raspberry Pi 4
    Model B
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 - 在Raspberry Pi 4 Model B上的lowlevel_mem_lkm内核模块的输出
- en: Check it out! In step 0 of the output in Figure 8.6 our `show_phy_pages()` library
    routine clearly shows that KVA `0xc000 0000` has PA `0x0`, KVA `0xc000 1000` has
    pa `0x1000`, and so on, for five pages (along with the PFN on the right); you
    can literally see the 1:1 identity mapping of physical RAM page frames to kernel
    virtual pages (in the lowmem region of the kernel segment)!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 看看！在图8.6的输出的第0步中，我们的`show_phy_pages()`库例程清楚地显示KVA `0xc000 0000`具有PA `0x0`，KVA
    `0xc000 1000`具有PA `0x1000`，依此类推，共五页（右侧还有PFN）；你可以清楚地看到物理RAM页框与内核虚拟页（在内核段的lowmem区域）的1:1身份映射！
- en: Next, the initial memory allocation with the `__get_free_page()` API goes through
    as expected. More interesting is our case 2\. Here, we can clearly see that the physical
    address and PFN of each allocated page (from 0 to 7, for a total of 8 pages) are
    consecutive, showing that the memory pages allocated are indeed physically contiguous!
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用`__get_free_page()`API进行初始内存分配如预期进行。更有趣的是我们的第2种情况。在这里，我们可以清楚地看到每个分配的页面（从0到7，共8页）的物理地址和PFN是连续的，显示出分配的内存页面确实是物理上连续的！
- en: 'We build and run the same module on an x86_64 VM running Ubuntu 20.04 (running
    our custom 5.4 ''debug'' kernel). The following screenshot shows the output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在运行我们自定义的5.4 'debug'内核的Ubuntu 20.04上的x86_64 VM上构建和运行相同的模块。以下截图显示了输出：
- en: '![](img/90c13860-8203-43b7-a2d7-7926b1256fd8.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90c13860-8203-43b7-a2d7-7926b1256fd8.png)'
- en: Figure 8.6 – The lowlevel_mem_lkm kernel module's output on a x86_64 VM running
    Ubuntu 20.04
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 - 在运行Ubuntu 20.04的x86_64 VM上的lowlevel_mem_lkm内核模块的输出
- en: 'This time (refer Figure 8.7), with the `PAGE_OFFSET` value being a 64-bit quantity
    (the value here is `0xffff 8880 0000 0000`), you can again clearly see the identity
    mapping of physical RAM frames to kernel virtual addresses (for 5 pages). Let''s
    take a moment and look carefully at the kernel logical addresses returned by the
    page allocator APIs. In Figure 8.7, you can see that they are all in the range `0xffff
    8880 .... ....`. The following snippet is from the kernel source tree at `Documentation/x86/x86_64/mm.txt`,
    documenting (a part of) the virtual memory layout on the x86_64:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次（参见图8.7），由于`PAGE_OFFSET`值是64位数量（这里的值是`0xffff 8880 0000 0000`），你可以再次清楚地看到物理RAM页框与内核虚拟地址的身份映射（5页）。让我们花点时间仔细看看页分配器API返回的内核逻辑地址。在图8.7中，你可以看到它们都在`0xffff
    8880 .... ....`范围内。以下片段来自x86_64的内核源树中的`Documentation/x86/x86_64/mm.txt`，记录了x86_64上的虚拟内存布局（部分）：
- en: If this all seems new and strange to you, please refer to [Chapter 7](06ee05b5-3e71-482d-93b8-235c27ce23bc.xhtml),
    *Memory Management Internals – Essentials*, particularly the *Examining the kernel
    segment *and *Direct-mapped RAM and address translation* sections.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这一切对你来说都很新奇，请参考[第7章](06ee05b5-3e71-482d-93b8-235c27ce23bc.xhtml)，*内存管理内部-基本知识*，特别是*检查内核段*和*直接映射的RAM和地址转换*部分。
- en: '[PRE11]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It's quite clear, isn't it? The page allocator memory (the buddy system free
    lists) maps directly onto free physical RAM within the direct-mapped or lowmem region
    of the kernel VAS. Thus, it obviously returns memory from this region. You can
    see this region in the preceding documentation output (highlighted in bold font)
    – the kernel direct-mapped or lowmem region. Again, I emphasize the fact that
    the specific address range used is very arch-specific. In the preceding code,
    it's the (maximum possible) range on the x86_64.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 很清楚，不是吗？页分配器内存（伙伴系统空闲列表）直接映射到内核VAS的直接映射或lowmem区域内的空闲物理RAM。因此，它显然会从这个区域返回内存。你可以在前面的文档输出中看到这个区域（用粗体字突出显示）-
    内核直接映射或lowmem区域。再次强调特定的地址范围非常与架构相关。在前面的代码中，这是x86_64上的（最大可能的）范围。
- en: Though tempting to claim that you're now done with the page allocator and its
    APIs, the reality is that this is (as usual) not quite the case. Do read on to
    see why – it's really important to understand these aspects.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然很想宣称你现在已经完成了页分配器及其API，但现实情况是（像往常一样）并非完全如此。请继续阅读，看看为什么-理解这些方面真的很重要。
- en: The page allocator and internal fragmentation
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 页分配器和内部碎片
- en: 'Though all looks good and innocent on the surface, I urge you to delve a bit
    deeper. Just under the surface, a massive (unpleasant!) surprise might await you:
    the blissfully unaware kernel/driver developer. The APIs we covered previously
    regarding the page allocator (see *Table 8.1*) have the dubious distinction of
    being able to internally fragment – in simpler terms, **waste** – very significant
    portions of kernel memory!'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然表面上看起来一切都很好，但我敦促你深入一点。在表面之下，一个巨大的（不愉快的！）惊喜可能在等待着你：那些毫不知情的内核/驱动程序开发人员。我们之前介绍的有关页分配器的API（参见*表8.1*）有能力在内部产生碎片-简单来说，**浪费**-内核内存的非常重要部分！
- en: To understand why this is the case, you must understand at least the basics
    of the page allocator algorithm and its freelist data structures. The section *The
    fundamental workings of the page allocator* covered this (just in case you haven't
    read it, please do so).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么会这样，你必须至少了解页分配器算法及其空闲列表数据结构的基础知识。*页分配器的基本工作*部分涵盖了这一点（以防你还没有阅读，请务必阅读）。
- en: 'In the *Working through a few scenarios* section, you would have seen that
    when we make an allocation request of convenient, perfectly rounded power-of-two-size
    pages, it goes very smoothly. However, when this isn''t the case – let''s say
    the driver requests 132 KB of memory – then we end up with a major issue: the internal
    fragmentation or wastage is very high. This is a serious downside and must be
    addressed. We will see how, in two ways, in fact. Do read on!'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在*通过几种情景*部分，你会看到当我们请求方便的、完全舍入的二次幂大小的页面时，情况会非常顺利。然而，当情况不是这样时——比如说驱动程序请求132 KB的内存——那么我们就会遇到一个主要问题：内部碎片或浪费非常高。这是一个严重的缺点，必须加以解决。我们将看到实际上有两种方法。请继续阅读！
- en: The exact page allocator APIs
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 确切的页面分配器API
- en: Realizing the vast potential for wastage within the default page allocator (or
    BSA), a developer from Freescale Semiconductor (see the information box) contributed
    a patch to the kernel page allocator that extends the API, adding a couple of
    new ones.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 意识到默认页面分配器（或BSA）内存在浪费的巨大潜力后，来自Freescale Semiconductor的开发人员（请参见信息框）为内核页面分配器贡献了一个扩展API的补丁，添加了一些新的API。
- en: In the 2.6.27-rc1 series, on 24 July 2008, Timur Tabi submitted a patch to mitigate
    the page allocator wastage issue. Here's the relevant commit: [https://github.com/torvalds/linux/commit/2be0ffe2b29bd31d3debd0877797892ff2d91f4c](https://github.com/torvalds/linux/commit/2be0ffe2b29bd31d3debd0877797892ff2d91f4c).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在2.6.27-rc1系列中，2008年7月24日，Timur Tabi提交了一个补丁来减轻页面分配器浪费问题。这是相关的提交：[https://github.com/torvalds/linux/commit/2be0ffe2b29bd31d3debd0877797892ff2d91f4c](https://github.com/torvalds/linux/commit/2be0ffe2b29bd31d3debd0877797892ff2d91f4c)。
- en: 'Using these APIs leads to more efficient allocations for large-ish chunks (multiple
    pages) of memory **with far less wastage**. The new (well, it *was *new back in
    2008, at least) pair of APIs to allocate and free memory are as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些API可以更有效地分配大块（多个页面）内存，**浪费要少得多**。用于分配和释放内存的新（嗯，至少在2008年是*新的*）API对如下：
- en: '[PRE12]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The first parameter to the `alloc_pages_exact()` API, `size`, is in bytes, the
    second is the "usual" GFP flags value discussed earlier (in the *Dealing with
    the GFP flags* section; `GFP_KERNEL` for the might-sleep process context cases,
    and `GFP_ATOMIC` for the never-sleep interrupt or atomic context cases).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`alloc_pages_exact()`API的第一个参数`size`是以字节为单位的，第二个参数是之前讨论过的“通常”的GFP标志值（在*处理GFP标志*部分；对于可能休眠的进程上下文情况，使用`GFP_KERNEL`，对于永不休眠的中断或原子上下文情况，使用`GFP_ATOMIC`）。'
- en: Note that the memory allocated by this API is still guaranteed to be physically
    contiguous. Also, the amount that can be allocated at a time (with one function
    call) is limited by `MAX_ORDER`; in fact, this is true of all the other regular
    page allocation APIs that we have seen so far. We will discuss a lot more about
    this aspect in the upcoming section, *Size limitations of the kmalloc API.* There,
    you'll realize that the discussion is in fact not limited to the slab cache but
    to the page allocator as well!
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由此API分配的内存仍然保证是物理上连续的。此外，一次（通过一个函数调用）可以分配的数量受到`MAX_ORDER`的限制；事实上，这也适用于我们迄今为止看到的所有其他常规页面分配API。我们将在即将到来的*kmalloc
    API的大小限制*部分中讨论更多关于这方面的内容。在那里，你会意识到讨论实际上不仅限于slab缓存，还包括页面分配器！
- en: The `free_pages_exact()` API must only be used to free memory allocated by its
    counterpart, `alloc_pages_exact()`. Also, note that the first parameter to the "free"
    routine is of course the value returned by the matching 'alloc' routine (the pointer
    to the newly allocated memory chunk).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`free_pages_exact()` API只能用于释放由其对应的`alloc_pages_exact()`分配的内存。此外，要注意“free”例程的第一个参数当然是匹配的“alloc”例程返回的值（指向新分配的内存块的指针）。'
- en: 'The implementation of `alloc_pages_exact()` is simple and clever: it first
    allocates the entire memory chunk requested "as usual" via the `__get_free_pages()`
    API. Then, it loops – from the end of the memory to be used to the amount of actually
    allocated memory (which is typically far greater) – freeing up those unnecessary
    memory pages! So, in our example, if you allocate 132 KB via the `alloc_pages_exact()`
    API, it will actually first internally allocate 256 KB via `__get_free_pages()`,
    but will then free up memory from 132 KB to 256 KB!'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`alloc_pages_exact()`的实现很简单而巧妙：它首先通过`__get_free_pages()`API“通常”分配整个请求的内存块。然后，它循环——从要使用的内存的末尾到实际分配的内存量（通常远大于此）——释放那些不必要的内存页面！因此，在我们的例子中，如果通过`alloc_pages_exact()`API分配了132
    KB，它实际上会首先通过`__get_free_pages()`分配256 KB，然后释放从132 KB到256 KB的内存！'
- en: Another example of the beauty of open source! A demo of using these APIs can
    be found here: `ch8/page_exact_loop`; we will leave it to you to try it out.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 开源之美的又一个例子！可以在这里找到使用这些API的演示：`ch8/page_exact_loop`；我们将留给你来尝试。
- en: Before we began this section, we mentioned that there were two ways in which
    the wastage issue of the page allocator can be addressed. One is by using the
    more efficient `alloc_pages_exact()` and `free_pages_exact()` APIs, as we just
    learned; the other is by using a different layer to allocate memory – the *slab
    allocator*. We will soon cover it; until then, hang in there. Next, let's cover
    more, *crucial to understand*, details on the (typical) GFP flags and how you,
    the kernel module or driver author, are expected to use them.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始这一部分之前，我们提到了解决页面分配器浪费问题的两种方法。一种是使用更有效的`alloc_pages_exact()`和`free_pages_exact()`API，就像我们刚刚学到的那样；另一种是使用不同的层来分配内存——*slab分配器*。我们很快就会涉及到它；在那之前，请耐心等待。接下来，让我们更详细地了解（典型的）GFP标志以及你作为内核模块或驱动程序作者应该如何使用它们，这一点非常重要。
- en: The GFP flags – digging deeper
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GFP标志——深入挖掘
- en: With regard to our discussions on the low-level page allocator APIs, the first
    parameter to every function is the so-called GFP mask. When discussing the APIs
    and their usage, we mentioned a *key rule*.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我们对低级页面分配器API的讨论，每个函数的第一个参数都是所谓的GFP掩码。在讨论API及其使用时，我们提到了一个*关键规则*。
- en: If in *process context and it is safe to sleep,* use the `GFP_KERNEL` flag. If
    it is *unsafe to **sleep* (typically, when in any type of interrupt context or
    when holding some types of locks), you *must* use the `GFP_ATOMIC` flag.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: We elaborate on this in the following sections.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Never sleep in interrupt or atomic contexts
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What does the phrase *safe to sleep *actually mean? To answer this, think of blocking
    calls (APIs):a *blocking call *is one where the calling process (or thread) is
    put into a sleep state because it is waiting on something, an *event*, and the
    eventit is waiting on has not occurred yet. Thus, it waits – it "sleeps." When,
    at some future point in time, the event it is waiting on occurs or arrives, it
    is woken up by the kernel and proceeds forward.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: One example of a user space blocking API includes `sleep(3)`. Here, the event
    it is waiting on is the elapse of a certain amount of time. Another example is `read(2)` and
    its variants, where the event being waited on is storage or network data becoming
    available. With `wait4(2)`, the event being waited on is the death or stoppage/continuing
    of a child process, and so on.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: So, any function that might possibly block can end up spending some time asleep
    (while asleep, it's certainly off the CPU run queues, and in a wait queue). Invoking
    this *possibly blocking* functionality when in kernel mode (which, of course,
    is the mode we are in when working on kernel modules)is *only allowed when in
    process context.* **It is a bug to invoke a blocking call of any sort in a context
    where it is unsafe to sleep, such as an interrupt or atomic context***. *Think
    of this as a golden rule. This is also known as sleeping in an atomic context
    – it's wrong, it's buggy, and it must *never *happen.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'You might wonder, *how can I know in advance if my code will ever enter an
    atomic or interrupt context*? In one way, the kernel helps us out: when configuring
    the kernel (recall `make menuconfig` from [Chapter 2](e0b89a37-18a3-424d-8983-58c4ac0725f6.xhtml),
    *Building the 5.x Linux Kernel from Source - Part 1*), under the `Kernel Hacking
    / Lock Debugging` menu, there is a Boolean tunable called `"Sleep inside atomic
    section checking"`. Turn it on! (The config option is named `CONFIG_DEBUG_ATOMIC_SLEEP`;
    you can always grep your kernel config file for it. Again, in [Chapter 5](408b6f9d-42dc-4c59-ab3d-1074d595f9e2.xhtml),
    *Writing Your First Kernel Module - LKMs Part 2*, under the Configuring a "debug"
    kernel section, this is something you should definitely turn on.)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Another way to think of this situation is howexactly do you put a process or
    thread to sleep? The short answer is by having it invoke the scheduling code –
    the `schedule()` function. Thus, by implication of what we have just learned (as
    a corollary), `schedule()` must only be called from within a context where it's
    safe to sleep; process context usually is safe, interrupt context never is.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: This is really important to keep in mind! (We briefly covered what process and
    interrupt context are in [Chapter 4](1c494ebd-e7ec-4a78-8695-5b97bdc3d6be.xhtml),
    *Writing Your First Kernel Module – LKMs Part 1*, in the *Process and interrupt
    contexts* section*, *and how the developer can use the `in_task()` macro to determine
    whether the code is currently running in a process or interrupt context.) Similarly,
    you can use the `in_atomic()` macro; if the code is an *atomic context* – where
    it must typically run to completion without interruption – it returns `True`;
    otherwise, `False`. You can be in process context but atomic at the same time
    – for example, when holding certain kinds of locks (spinlocks; we will, of course,
    cover this in the chapters on *synchronization* later); the converse cannot happen.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Besides the GFP flags we're focused upon - the `GFP_KERNEL` and `GFP_ATOMIC` ones
    - the kernel has several other `[__]GFP_*` flags that are used internally; several
    for the express purpose of reclaiming memory*.* These include (but are not limited
    to) `__GFP_IO`, `__GFP_FS`, `__GFP_DIRECT_RECLAIM`, `__GFP_KSWAPD_RECLAIM`, `__GFP_RECLAIM`,
    `__GFP_NORETRY`, and so on. In this book, we do not intend to delve into these
    details. I refer you to the detailed comment in `include/linux/gfp.h` that describes
    them (also see the *Further reading* section).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '**Linux Driver Verification** (**LDV**) project: back in [Chapter 1](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml),* Kernel Workspace
    Setup*, we mentioned that this project has useful "rules" with respect to various
    programming aspects of Linux modules (drivers, mostly) as well as the core kernel.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'With regard to our current topic, here''s one of the rules, a negative one,
    implying that you *cannot *do this: *Not disabling IO during memory allocation
    while holding a USB device lock* ([http://linuxtesting.org/ldv/online?action=show_rule&rule_id=0077](http://linuxtesting.org/ldv/online?action=show_rule&rule_id=0077)).
    Some quick background: when you specify the `GFP_KERNEL` flag, it implicitly means
    (among other things) that the kernel can start an IO (Input/Output; reads/writes)
    operation to reclaim memory. The trouble is, at times this can be problematic
    and should not be done; to get over this, you''re expected use the `GFP_NOIO`
    flag as part of the GFP bitmask when allocating kernel memory.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s precisely the case that this LDV ''rule'' is referring to: here, between
    the `usb_lock_device()` and `usb_unlock_device()` APIs, the `GFP_KERNEL` flag
    shouldn''t be used and the `GFP_NOIO` flag should be used instead. (You can see
    several instances of this flag being used in this code: `drivers/usb/core/message.c`).
    The LDV page mentions the fact that a couple of USB-related code driver code source
    files were fixed to adhere to this rule.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: All right, now that you're armed with a good amount of detail on the page allocator
    (it is, after all, the internal "engine" of RAM (de)allocation!), its APIs, and
    how to use them, let's move on to a very important topic – the motivation(s) behind
    the slab allocator, its APIs, and how to use them.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and using the kernel slab allocator
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As seen in the first section of this chapter, *Introducing kernel memory allocators,* the *slab
    allocator* or *slab cache* is layered above the page allocator (or BSA; refer
    back to *Figure 8.1*). The slab allocator justifies its very existence with two
    primary ideas or purposes:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '**Object caching**: Here, it serves as a cache of common "objects," and the
    allocation (and subsequent freeing) of frequently allocated data structures within
    the Linux kernel, for high performance.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitigate the high wastage (internal fragmentation) of the page allocator by
    providing small, conveniently sized caches, typically **fragments of a page**.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now examine these ideas in a more detailed manner.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The object caching idea
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Okay, we begin with the first of these design ideas – the notion of a cache
    of common objects. A long time ago, a SunOS developer, Jeff Bonwick, noticed that
    certain kernel objects – data structures, typically – were allocated and deallocated
    frequently within the OS. He thus had the idea of *pre-allocating* them in a cache
    of sorts. This evolved into what we call the *slab cache*.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, on the Linux OS as well, the kernel (as part of the boot time initialization)
    pre-allocates a fairly large number of objects into several slab caches. The reason:
    performance! When core kernel code (or a device driver) requires memory for one
    of these objects, it directly requests the slab allocator. If cached, the allocation
    is almost immediate (the converse being true as well at deallocation). You might
    wonder, *is all this really necessary*? Indeed it is!'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: A good example of high performance being required is within the critical code
    paths of the network and block IO subsystems. Precisely for this reason, several
    network and block IO data structures (the network stack's socket buffer, `sk_buff`,
    the block layer's `biovec`, and, of course, the core `task_struct` data structures
    or objects, being a few good examples) are *auto-cached *(*pre-allocated*) by
    the kernel within the slab caches. Similarly, filesystem metadata structures (such
    as the `inode` and `dentry` structures, and so on), the memory descriptor (`struct
    mm_struct`), and several more are *pre-allocated* on slab caches. Can we see these
    cached objects? Yes, just a bit further down, we will do precisely this (via `/proc/slabinfo`).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: The other reason that the slab (or, more correctly now, the SLUB) allocator
    has far superior performance is simply that traditional heap-based allocators
    tend to allocate and deallocate memory often, creating "holes" (fragmentation).
    Because the slab objects are allocated once (at boot) onto the caches, and freed
    back there (thus not really "freed" up), performance remains high. Of course,
    the modern kernel has the intelligence to, in a graceful manner, start freeing
    up the slab caches when the memory pressure gets too high.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'The current state of the slab caches – the object caches, the number of objects
    in a cache, the number in use, the size of each object, and so on – can be looked
    up in several ways: a raw view via the `proc` and `sysfs` filesystems, or a more
    human-readable view via various frontend utilities, such as `slabtop(1)`, `vmstat(8)`,
    and `slabinfo`. In the following code snippet, on a native x86_64 (with 16 GB
    of RAM) running Ubuntu 18.04 LTS, we peek at the top 10 lines of output from `/proc/slabinfo`:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'A few points to note:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Even reading `/proc/slabinfo` requires root access (hence, we use `sudo(8)`).
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the preceding output, the leftmost column is the name of the slab cache.
    It often, but not always, matches the name of the actual data structure within
    the kernel that it caches.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then follows, for each cache, information in this format: `<statistics> : <tunables>
    : <slabdata>`. The meaning of each of the fields shown in the header line is explained
    in the man page for `slabinfo(5)` (look it up with `man 5 slabinfo`).'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Incidentally, the `slabinfo` utility is one example of user space C code *within*
    the kernel source tree under the `tools/` directory (as are several others). It
    displays a bunch of slab layer statistics (try it with the `-X` switch). To build
    it, do the following:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'A question you might have at this point is, *how much memory in total is the
    slab cache currently using*? This is easily answered by grepping `/proc/meminfo` for
    the `Slab:` entry, as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As is apparent, significant amounts of memory can be used by the slab caches!
    This, in fact, is a common feature on Linux that puzzles those new to it: the
    kernel can and will use RAM for cache purposes, thus greatly improving performance.
    It is, of course, designed to intelligently throttle down the amount of memory
    used for caching as the memory pressure increases. On a regular Linux system,
    a significant percentage of memory can go toward caching (especially the *page
    cache; *it''s used to cache the content of files as IO is performed upon them).
    This is fine, *as long as* *memory pressure is low*. The `free(1)` utility clearly
    shows this (again, on my x86_64 Ubuntu box with 16 GB of RAM, in this example):'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `buff/cache` column indicates two caches that the Linux kernel employs –
    the buffer and page caches. In reality, among the various caches that the kernel
    employs, the *page cache* is a key one and often accounts for a majority of memory
    usage.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Look up `/proc/meminfo` for fine-granularity detail on system memory usage;
    the fields displayed are numerous. The man page on `proc(5)` describes them under
    the `/proc/meminfo` section.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand the motivation behind the slab allocator (there's more
    on this too), let's dive into learning how to use the APIs it exposes for both
    the core kernel as well as module authors.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Learning how to use the slab allocator APIs
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have noticed that, so far, we haven't explained the second "design idea"
    behind the slab allocator (cache), namely, *mitigate the high wastage (internal
    fragmentation) of the page allocator by providing small, conveniently sized caches,
    typically, fragments of a page*. We will see what exactly this means in a practical
    fashion, along with the kernel slab allocator APIs.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Allocating slab memory
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Though several APIs to perform memory allocation and freeing exist within the
    slab layer, there are just a couple of really key ones, with the rest falling
    into a "convenience or helper" functions category (which we will of course mention
    later). The key slab allocation APIs for the kernel module or device driver author
    are as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Be sure to include the `<linux/slab.h>` header file when using any slab allocator
    APIs.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The `kmalloc()` and `kzalloc()` routines tend to be the **most frequently used
    APIs for memory allocation** within the kernel. A quick check – we're not aiming
    to be perfectly precise – with the very useful `cscope(1)` code browsing utility
    on the 5.4.0 Linux kernel source tree reveals the (approximate) frequency of usage: `kmalloc()` is called
    around 4,600 times and `kzalloc()` is called over 11,000 times!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'Both functions have two parameters: the first parameter to pass is the size of
    the memory allocation required in bytes, while the second is the type of memory
    to allocate, specified via the now familiar GFP flags(we already covered this
    topic in earlier sections, namely, *Dealing with the **GFP flags* and *The GFP
    flags – digging deeper. *If you''re not familiar with them, I suggest you read
    those sections first).'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'To mitigate the risk of **Integer Overflow** (**IoF**) bugs, you should avoid
    dynamically calculating the size of memory to allocate (the first parameter).
    The kernel documentation warns us regarding precisely this (link:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kernel.org/doc/html/latest/process/deprecated.html#open-coded-arithmetic-in-allocator-arguments](https://www.kernel.org/doc/html/latest/process/deprecated.html#open-coded-arithmetic-in-allocator-arguments)).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, always avoid using deprecated stuff documented here: *Deprecated
    Interfaces, Language Features, Attributes, and Conventions* (link: [https://www.kernel.org/doc/html/latest/process/deprecated.html#deprecated-interfaces-language-features-attributes-and-conventions](https://www.kernel.org/doc/html/latest/process/deprecated.html#deprecated-interfaces-language-features-attributes-and-conventions)).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon successful allocation, the return value is a pointer, the *kernel logical
    address* (remember, it''s still a virtual address, *not* physical) of the start
    of the memory chunk (or slab) just allocated. Indeed, you should notice that but
    for the second parameter, the `kmalloc()` and `kzalloc()` APIs closely resemble
    their user space counterpart, the all-too-familiar glibc `malloc(3)` (and friends)
    APIs. Don''t get the wrong idea, though: they''re completely different. `malloc()` returns
    a user space virtual address and, as mentioned earlier, there is no direct correlation
    between the user-mode `malloc(3)` and the kernel-mode `k[m|z]alloc()` (so no,
    a call to `malloc()` does *not *result in an immediate call to `kmalloc()`; more
    on this later!).'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Next, it's important to understand that the memory returned by these slab allocator
    APIs **is guaranteed to be physically contiguous***. *Furthermore, and another
    key benefit, the return address is guaranteed to be on a CPU cacheline boundary;
    that is, it will be **cacheline-aligned**. Both of these are important performance-enhancing
    benefits.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Every CPU reads and writes data (from and to CPU caches <-> RAM) in an atomic
    unit called the **CPU cacheline***.* The size of the cacheline varies with the
    CPU. You can look this up with the `getconf(1)` utility – for example, try doing `getconf
    -a|grep LINESIZE`. On modern CPUs, the cachelines for instructions and data are
    often separated out (as are the CPU caches themselves). A typical CPU cacheline
    size is 64 bytes.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: The content of a memory chunk immediately after allocation by `kmalloc()` is
    random (again, like `malloc(3)`). Indeed, the reason why `kzalloc()` is the preferred
    and recommended API to use is that it *sets to zero* the allocated memory. Some
    developers argue that the initialization of the memory slab takes some time, thus
    reducing performance. Our counter argument is that unless the memory allocation
    code is in an extremely time-critical code path (which, you could reasonably argue,
    is not good design in the first place, but sometimes can't be helped), you should,
    as a best practice, *initialize your memory upon allocation*. A whole slew of
    memory bugs and security side effects can thereby be avoided.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Many parts of the Linux kernel core code certainly use the slab layer for memory.
    Within these, there *are* timecritical code paths – good examples can be found
    within the network and block IO subsystems. For maximizing performance, the slab
    (actually SLUB) layer code has been written to be *lo*ckless (via a lock-free
    technology called per-CPU variables). See more on the performance challenges and
    implementation details in the *Further reading *section.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Freeing slab memory
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Of course, you must free the allocated slab memory you allocated at some point
    in the future (thus not leaking memory); the  `kfree()` routine serves this purpose.
    Analogous to the user space `free(3)` API, `kfree()` takes a single parameter
    – the pointer to the memory chunk to free. It must be a valid kernel logical (or
    virtual) address and must have been initialized by, that is, the return value
    of, one of the slab layer APIs (`k[m|z]alloc()` or one of its helpers). Its API
    signature is simple:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Just as with `free(3)`, there is no return value. As mentioned before, take
    care to ensure that the parameter to `kfree()` is the precise value returned by
    `k[m|z]alloc()`. Passing an incorrect value will result in memory corruption,
    ultimately leading to an unstable system.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: There are a few additional points to note.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume we have allocated some slab memory with `kzalloc()`:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Later, after usage, we would like to free it, so we do the following:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This code – checking that the value of `kptr` is not `NULL` before freeing it
    – *is unnecessary*; just perform `kfree(kptr);` and it's done.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example of *incorrect* code (pseudo-code) is shown as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Interesting: here, from the second loop iteration onward, the programmer has *assumed *that
    the `kptr` pointer variable will be set to `NULL` upon being freed! This is definitely
    not the case (it would have been quite a nice semantic to have though; also, the
    same argument applies to the "usual" user space library APIs). Thus, we hit a
    dangerous bug: on the loop''s second iteration, the `if` condition will likely
    turn out to be false, thus skipping the allocation. Then, we hit the `kfree()`,
    which, of course, will now corrupt memory (due to a double-free bug)! (We provide
    a demo of this very case in the LKM here: `ch8/slab2_buggy`).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: With regard to *initializing* memory buffers after (or during) allocation, just
    as we mentioned with regard to allocations, the same holds true for freeing memory.
    You should realize that the `kfree()` API merely returns the just-freed slab to
    its corresponding cache, leaving the internal memory content exactly as it was!
    Thus, just prior to freeing up your memory chunk, a (slightly pedantic) best practice
    is to *wipe out (overwrite)* the memory content. This is especially true for security
    reasons (such as in the case of an "info-leak," where a malicious attacker could
    conceivably scan freed memory for "secrets"). The Linux kernel provides the `kzfree()` API
    for this express purpose (the signature is identical to that of `kfree()`).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '*Careful!* In order to overwrite "secrets," a simple `memset()` of the target
    buffer might just not work. Why not? The compiler might well optimize away the
    code (as the buffer is no longer to be used). David Wheeler, in his excellent
    work *Secure Programming HOWTO* ([https://dwheeler.com/secure-programs/](https://dwheeler.com/secure-programs/)),
    mentions this very fact and provides a solution: "One approach that seems to work
    on all platforms is to write your own implementation of memset with internal "volatilization"
    of the first argument." (This code is based on a workaround proposed by Michael
    Howard):'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '`void *guaranteed_memset(void *v,int c,size_t n)`'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '`{ volatile char *p=v; while (n--) *p++=c; return v; }`'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '"Then place this definition into an external file to force the function to
    be external (define the function in a corresponding `.h` file, and `#include`
    the file in the callers, as is usual). This approach appears to be safe at any
    optimization level (even if the function gets inlined)."'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: The kernel's `kzfree()` API should work just fine. Take care when doing similar
    stuff in user space.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Data structures – a few design tips
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the slab APIs for memory allocation in kernel space is highly recommended.
    For one, it guarantees both physically contiguous as well as cacheline-aligned
    memory. This is very good for performance; in addition, let's check out a few
    quick tips that can yield big returns.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '*CPU caching* can provide tremendous performance gains. Thus, especially for
    time-critical code, take care to design your data structures for best performance:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Keep the most important (frequently accessed, "hot") members  together and at
    the top of the structure. To see why, imagine there are five important members
    (of a total size of say, 56 bytes) in your data structure; keep them all together
    and at the top of the structure. Say the CPU cacheline size is 64 bytes. Now,
    when your code accesses *any one* of these five important members (for anything,
    read/write), *all five members will be fetched into the CPU cache(s) as the CPU's
    memory read/writes work in an atomic unit of CPU cacheline size; *this optimizes
    performance (as working on the cache is typically multiple times faster than working
    on RAM).
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try and align the structure members such that a single member does not "fall
    off a cacheline." Usually, the compiler helps in this regard, but you can even
    use compiler attributes to explicitly specify this.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Accessing memory sequentially results in high performance due to effective
    CPU caching. However, we can''t seriously push the case for making all our data
    structures arrays! Experienced designers and developers know that using linked
    lists is extremely common. But doesn''t that actually hurt performance? Well,
    yes, to some extent. Thus, a suggestion: use linked lists. Keep the "node" of
    the list as a large data structure (with "hot" members at the top and together).
    This way, we try and maximize the best of both cases as the large structure is
    essentially an array. (Think about it, the list of task structures that we saw
    in [Chapter 6](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml), *Kernel Internals
    Essentials – Processes and Threads*, – the *task list – *is a perfect real-world
    example of a linked list with large data structures as nodes).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The upcoming section deals with a key aspect: we learn exactly which slab caches
    the kernel uses when allocating (slab) memory via the popular `k[m|z]alloc()`
    APIs.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: The actual slab caches in use for kmalloc
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ll take a quick deviation – very important, though – before trying out
    a kernel module using the basic slab APIs. It''s important to understand where
    exactly the memory allocated by the `k[m|z]alloc()` APIs is coming from. Well,
    it''s from the slab caches, yes, but which ones exactly? A quick `grep` on the
    output of `sudo vmstat -m` reveals this for us (the following screenshot is on
    our x86_64 Ubuntu guest):'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec7fdc35-1bda-4de1-b8fb-fed9b2cce797.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Screenshot of sudo vmstat -m showing the kmalloc-n slab caches
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: That's very interesting! The kernel has a slew of dedicated slab caches for
    generic `kmalloc` memory of varying sizes, *ranging from 8,192 bytes down to a
    mere 8 bytes!* This tells us something – with the page allocator, if we had requested,
    say, 12 bytes of memory, it would have ended up giving us a whole page (4 KB)
    – the wastage is just too much. Here, with the slab allocator, an allocation request
    for 12 bytes ends up actually allocating just 16 bytes (from the second-to-last
    cache seen in Figure 8.8)! Fantastic.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, note the following:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Upon `kfree()`, the memory is freed back into the appropriate slab cache.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The precise sizing of the slab caches for `kmalloc` varies with the architecture.
    On our Raspberry Pi system (an ARM CPU, of course), the generic memory `kmalloc-N` caches
    ranged from 64 bytes to 8,192 bytes.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding screenshot also reveals a clue. Often, the demand is for small-to-tiny
    fragments of memory. As an example, in the preceding screenshot the column labelled
    `Num` represents the *Number of currently active objects*, the maximum number
    is from the 8- and 16-byte `kmalloc` slab caches (of course, this may not always
    be the case. Quick tip: use the `slabtop(1)` utility (you''ll need to run it as
    root): the rows towards the top reveal the current frequently used slab caches.)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linux keeps evolving, of course. As of the 5.0 mainline kernel, there is a
    newly introduced `kmalloc` cache type, called the reclaimable cache (the naming
    format is `kmalloc-rcl-N`). Thus, performing a grep as done previously on a 5.x
    kernel will also reveal these caches:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The new `kmalloc-rcl-N` caches help internally with more efficiencies (to reclaim
    pages under pressure and as an anti-fragmentation measure). However, a module
    author like you need not be concerned with these details. (The commit for this
    work can be viewed here: [https://github.com/torvalds/linux/commit/1291523f2c1d631fea34102fd241fb54a4e8f7a0](https://github.com/torvalds/linux/commit/1291523f2c1d631fea34102fd241fb54a4e8f7a0).)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '`vmstat -m` is essentially a wrapper over the kernel''s `/sys/kernel/slab`
    content (more on this follows). Deep internal details of the slab caches can be
    seen using utilities such as `slabtop(1)`, as well as the powerful `crash(1)`
    utility (on a "live" system, the relevant crash command is `kmem -s` (or `kmem
    -S`)).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Right! Time to again get hands on with some code to demonstrate the usage of
    the slab allocator APIs!
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Writing a kernel module to use the basic slab APIs
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the following code snippet, take a look at the demo kernel module code (found
    at `ch8/slab1/`). In the `init` code, we merely perform a couple of slab layer
    allocations (via the `kmalloc()` and `kzalloc()` APIs), print some information,
    and free the buffers in the cleanup code path (of course, the full source code
    is accessible at this book's GitHub repository). Let's look at the relevant parts
    of the code step by step.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'At the start of the `init` code of this kernel module, we initialize a global
    pointer (`gkptr`) by allocating 1,024 bytes to it (*remember: pointers have no
    memory!*) via the `kmalloc()` slab allocation API. Notice that, as we''re certainly
    running in process context here, and it is thus "safe to sleep," we use the `GFP_KERNEL` flag
    for the second parameter (just in case you want to refer back, the earlier section, *The
    GFP flags – digging deeper*, has it covered):'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In the preceding code, also notice that we use the `print_hex_dump_bytes()` kernel
    convenience routine as a convenient way to dump the buffer memory in a human-readable
    format. Its signature is:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Where `prefix_str` is any string you would like to prefix to each line of the
    hex dump; `prefix_type` is one of `DUMP_PREFIX_OFFSET`, `DUMP_PREFIX_ADDRESS`,
    or `DUMP_PREFIX_NONE`, `buf` is the source buffer to hex-dump; and `len` is the
    number of bytes to dump.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'Up next is a typical strategy (*a best practice*) followed by many device drivers:
    they keep all their required or context information in a single data structure,
    often termed the *driver context* structure. We mimic this by declaring a (silly/sample)
    data structure called `myctx`, as well as a global pointer to it called `ctx` (the
    structure and pointer definition is in the preceding code block):'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: After the data structure, we then allocate and initialize `ctx` to the size
    of the `myctx` data structure via the useful `kzalloc()` wrapper API. The subsequent
    *hexdump* will show that it is indeed initialized to all zeroes (for readability,
    we will only "dump" the first 32 bytes).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Do notice how we handle the error paths using `goto`; this has already been
    mentioned a few times earlier in this book, so we won''t repeat ourselves here. Finally,
    in the cleanup code of the kernel module, we `kfree()` both buffers, preventing
    any memory leakage:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'A screenshot of a sample run on my Raspberry Pi 4 follows. I used our `../../lkm`
    convenience script to build, load, and do `dmesg`:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca0f2ab4-3db3-48cf-8ae9-d5ff027ddcdf.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Partial screenshot of our slab1.ko kernel module in action on a
    Raspberry Pi 4
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Okay, now that you have a grip on the basics of using the common slab allocator
    APIs, `kmalloc(), kzalloc()`, and `kfree()`, let's go further. In the next section,
    we will dive into a really key concern – the reality of size limitations on the
    memory you can obtain via the slab (and page) allocators. Read on!
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Size limitations of the kmalloc API
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the key advantages of both the page and slab allocators is that the memory
    chunk they provide upon allocation is not only virtually contiguous (obviously)
    but is also guaranteed to be *physically contiguous memory*. Now that is a big
    deal and will certainly help performance.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: But (there's always a *but*, isn't there!), precisely because of this guarantee,
    it becomes impossible to serve up any given large size when performing an allocation.
    In other words, there is a definite limit to the amount of memory you can obtain
    from the slab allocator with a single call to our dear `k[m|z]alloc()` APIs. What
    is the limit? (This is indeed a really frequently asked question.)
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, you should understand that, technically, the limit is determined by
    two factors:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: One, the system page size (determined by the `PAGE_SIZE` macro)
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two, the number of "orders" (determined by the `MAX_ORDER` macro); that is,
    the number of lists in the page allocator (or BSA) freelist data structures (see
    Figure 8.2)
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a standard 4 KB page size and a MAX_ORDER value of 11, the maximum amount
    of memory that can be allocated with a single `kmalloc()` or `kzalloc()` API call
    is 4 MB. This is the case on both the x86_64 and ARM architectures.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'You might wonder, *how exactly is this 4 MB limit arrived at*? Think about
    it: once a slab allocation request exceeds the maximum slab cache size that the
    kernel provides (often 8 KB), the kernel simply passes the request down to the
    page allocator. The page allocator''s maximum allocable size is determined by
    `MAX_ORDER`. With it set to `11`, the maximum allocable buffer size is *2^((MAX_ORDER-1)) =**2^(10)
    pages = 1024 pages = 1024 * 4K = 4 MB*!'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Testing the limits – memory allocation with a single call
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A really key thing for developers (and everyone else, for that matter) is to
    **be empirical** in your work! The English word *empirical *means based on what
    is experienced or seen, rather than on theory. This is a critical rule to always
    follow – do not simply assume things or take them at face value. Try them out
    for yourself and see.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do something quite interesting: write a kernel module that allocates
    memory from the (generic) slab caches (via the `kmalloc()` API, of course). We
    will do so in a loop, allocating – and freeing – a (calculated) amount on each
    loop iteration. The key point here is that we will keep increasing the amount
    allocated by a given "step" size. The loop terminates when `kmalloc()` fails;
    this way, we can test just how much memory we can actually allocate with a single
    call to `kmalloc()`(you''ll realize, of course, that `kzalloc()`, being a simple
    wrapper over `kmalloc()`, faces precisely the same limits).'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we show the relevant code. The `test_maxallocsz()`
    function is called from the `init` code of the kernel module:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: By the way, notice how our `printk()` function uses the `%zu` format specifier
    for the `size_t` (essentially an unsigned integer) variable? `%zu` is a portability
    aid; it makes the variable format correct for both 32- and 64-bit systems!
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build (cross-compile on the host) and insert this kernel module on our
    Raspberry Pi device running our custom-built 5.4.51-v7+ kernel; almost immediately, upon
    `insmod(8)`, you will see an error message, `Cannot allocate memory`, printed
    by the `insmod` process; the following (truncated) screenshot shows this:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5fd4b019-8594-448d-9c41-7f0cb0040383.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – The first insmod(8) of our slab3_maxsize.ko kernel module on a
    Raspberry Pi 3 running a custom 5.4.51 kernel
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: This is expected! Think about it, the `init` function of our kernel module code
    has indeed failed with `ENOMEM` after all. Don't get thrown by this; looking up
    the kernel log reveals what actually transpired. The fact is that on the very
    first test run of this kernel module, you will find that at the place where `kmalloc()` fails,
    the kernel dumps some diagnostic information, including a pretty lengthy kernel
    stack trace. This is due to it invoking a `WARN()` macro.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our slab memory allocations worked, up to a point. To clearly see the failure
    point, simply scroll down in the kernel log (`dmesg`) display. The following screenshot
    shows this:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df1b5ab9-fc13-4aa8-889f-e1e0294c79ea.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Partial screenshot showing the lower part of the dmesg output
    (of our slab3_maxsize.ko kernel module) on a Raspberry Pi 3
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'Aha, look at the last line of output (Figure 8.11): the `kmalloc()` fails on
    an allocation above 4 MB (at 4,200,000 bytes), precisely as expected; until then,
    it succeeds.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'As an interesting aside, notice that we have (quite deliberately) performed
    the very first allocation in the loop with size `0`; it does not fail:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '`kmalloc(0, GFP_xxx);` returns the zero pointer; on x86[_64], it''s the value `16` or `0x10` (see `include/linux/slab.h` for
    details). In effect, it''s an invalid virtual address living in the page `0` `NULL`
    pointer trap. Accessing it will, of course, lead to a page fault (originating
    from the MMU).'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, attempting `kfree(NULL);` or `kfree()` of the zero pointer results
    in `kfree()` becoming a no-op.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hang on, though – an extremely important point to note: in the *The actual
    slab caches in use for kmalloc* section, we saw that the slab caches that are
    used to allocate memory to the caller are the `kmalloc-n` slab caches, where `n` ranges
    from `64` to `8192` bytes (on the Raspberry Pi, and thus the ARM for this discussion).
    Also, FYI, you can perform a quick  `sudo vmstat -m | grep -v "\-rcl\-" | grep
    --color=auto "^kmalloc"` to verify this.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'But clearly, in the preceding kernel module code example, we have allocated
    via `kmalloc()` much larger quantities of memory (right from 0 bytes to 4 MB).
    The way it really works is that the `kmalloc()` API only uses the `kmalloc-''n''`
    slab caches for memory allocations less than or equal to 8,192 bytes (if available);
    any allocation request for larger memory chunks is then passed to the underlying page
    (or buddy system) allocator! Now, recall what we learned in the previous chapter:
    the page allocator uses the buddy system freelists (on a per *node:zone* basis) *and *the
    maximum size of memory chunks enqueued on the freelists are *2^((MAX_ORDER-1)) =
    2^(10)* *pages*, which, of course, is 4 MB (given a page size of 4 KB and `MAX_ORDER` of `11`).
    This neatly ties in with our theoretical discussions.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'So, there we have it: both in theory and in practice, you can now see that (again,
    given a page size of 4 KB and `MAX_ORDER` of `11`), the maximum size of memory
    that can be allocated via a single call to `kmalloc()` (or `kzalloc()`) is 4 MB.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Checking via the /proc/buddyinfo pseudo-file
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It''s really important to realize that although we figured out that 4 MB of
    RAM is the maximum we can get at one shot, it definitely doesn''t mean that you
    will always get that much. No, of course not. It completely depends upon the amount
    of free memory present within the particular freelist at the time of the memory
    request. Think about it: what if you are running on a Linux system that has been
    up for several days (or weeks). The likelihood of finding physically contiguous
    4 MB chunks of free RAM is quite low (again, this depends upon the amount of RAM
    on the system and its workload).'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'As a rule of thumb, if the preceding experiment did not yield a maximum allocation
    of what we have deemed to be the maximum size (that is, 4 MB), why not try it
    on a freshly booted guest system? Now, the chances of having physically contiguous
    4 MB chunks of free RAM are a lot better. Unsure about this? Let''s get empirical
    again and look up the content of `/proc/buddyinfo` – both on an in-use and a freshly
    booted system – to figure out whether the memory chunks are available. In the
    following code snippet, on our in-use x86_64 Ubuntu guest system with just 1 GB
    of RAM, we look it up:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As we learned earlier (in the *Freelist organization* section), the numbers
    seen in the preceding code block are in the sequence order `0` to `MAX_ORDER-1`
    (typically, *0* to *11 – 1 = 10*), and they represent the number of *2^(order)*
    contiguous free page frames in that order.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding output, we can see that we do *not* have free blocks on the
    order `10` list (that is, the 4 MB chunks; it''s zero). On a freshly booted Linux
    system, the chances are high that we will. In the following output, on the same
    system that''s just been rebooted, we see that there are seven chunks of free
    physically contiguous 4 MB RAM available in node `0`, zone DMA32:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Reiterating this very point, on a Raspberry Pi that has been up for just about
    a half hour, we have the following:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, there are 160 4 MB chunks of physically contiguous RAM available (free).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there's more to explore. In the following section, we cover more
    on using the slab allocator – the resource-managed API alternative, additional
    slab helper APIs that are available, and a note on cgroups and memory in modern
    Linux kernels.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Slab allocator – a few additional details
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A few more key points remain to be explored. First, some information on using
    the kernel's resource-managed versions of the memory allocator APIs, followed
    by a few additionally available slab helper routines within the kernel, and then
    a brief look at cgroups and memory. We definitely recommend you go through these
    sections as well. Please, do read on!
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Using the kernel's resource-managed memory allocation APIs
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Especially useful for device drivers, the kernel provides a few managed APIs
    for memory allocation. These are formally referred to as the device resource-managed
    or devres APIs (the link to kernel documentation on this is [https://www.kernel.org/doc/Documentation/driver-model/devres.txt](https://www.kernel.org/doc/Documentation/driver-model/devres.txt)).
    They are all prefixed with `devm_`; though there are several of them, we will
    focus on only one common use case here – that of using these APIs in place of
    the usual `k[m|z]alloc()` ones. They are as follows:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '`void * devm_kmalloc(struct device *dev, size_t size, gfp_t gfp);`'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`void * devm_kzalloc(struct device *dev, size_t size, gfp_t gfp);`'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reason why these resource-managed APIs are useful is that there is *no need
    for the developer to explicitly free the memory allocated by them*. The kernel
    resource management framework guarantees that it will automatically free the memory
    buffer upon driver detach, or if a kernel module, when the module is removed (or
    the device is detached, whichever occurs first). This feature immediately enhances
    code robustness. Why? Simple, we're all human and make mistakes. Leaking memory
    (especially on error code paths) is indeed a pretty common bug!
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'A few relevant points regarding the usage of these APIs:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: A key point – please do not attempt to blindly replace `k[m|z]alloc()` with
    the corresponding `devm_k[m|z]alloc()`! These resource-managed allocations are
    really designed to be used only in the `init` and/or `probe()` methods of a device
    driver (all drivers that work with the kernel's unified device model will typically
    supply the `probe()` and `remove()` (or `disconnect()`) methods. We will not delve
    into these aspects here).
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`devm_kzalloc()` is usually preferred as it initializes the buffer as well.
    Internally (as with `kzalloc()`), it is merely a thin wrapper over the `devm_kmalloc()` API.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second and third parameters are the usual ones, as with the `k[m|z]alloc()` APIs – the
    number of bytes to allocate and the GFP flags to use. The first parameter, though,
    is a pointer to `struct device`. Quite obviously, it represents the *device *that
    your driver is driving.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the memory allocated by these APIs is auto-freed (on driver detach or module
    removal), you don't have to do anything. It can, though, be freed via the `devm_kfree()` API.
    You doing this, however, is usually an indication that the managed APIs are the
    wrong ones to use...
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Licensing: The managed APIs are exported (and thus available) only to modules
    licensed under the GPL (in addition to other possible licenses).'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional slab helper APIs
  id: totrans-389
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several helper slab allocator APIs, friends of the `k[m|z]alloc()` API
    family. These include the `kcalloc()` and `kmalloc_array()` APIs for allocating
    memory for an array, as well as `krealloc()`, whose behavior is analogous to `realloc(3)`, the
    familiar user space API.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'In conjunction with allocating memory for an array of elements, the `array_size()`
    and `struct_size()` kernel helper routines can be very helpful. In particular,
    `struct_size()` has been heavily used to prevent (and indeed fix) many integer
    overflow (and related) bugs when allocating an array of structures, a common task
    indeed. As a quick example, here''s a small code snippet from `net/bluetooth/mgmt.c`:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: It's worth browsing through the `include/linux/overflow.h` kernel header file.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '`kzfree()` is like `kfree()` but zeroes out the (possibly larger) memory region
    being freed. (Why larger? This will be explained in the next section.) Note that
    this is considered a security measure but might hurt performance.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'The resource-managed versions of these APIs are also available: `devm_kcalloc()`
    and `devm_kmalloc_array()`.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: Control groups and memory
  id: totrans-396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Linux kernel supports a very sophisticated resource management system called **cgroups **(**control
    groups**), which, in a nutshell, are used to hierarchically organize processes
    and perform resource management (more on cgroups, with an example of cgroups v2
    CPU controller usage, can be found in [Chapter 11](d6e5ebd3-1f04-40e8-a240-2607c58b1299.xhtml), *The
    CPU Scheduler - Part 2*, on CPU scheduling).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Among the several resource controllers is one for memory bandwidth. By carefully
    configuring it, the sysadmin can effectively regulate the distribution of memory
    on the system. Memory protection is possible, both as (what is called) hard and
    best-effort protection via certain `memcg` (memory cgroup) pseudo-files (particularly,
    the `memory.min` and `memory.low` files). In a similar fashion, within a cgroup,
    the `memory.high` and `memory.max` pseudo-files are the main mechanism to control
    the memory usage of a cgroup. Of course, as there is a lot more to it than is
    mentioned here, I refer you to the kernel documentation on the new cgroups (v2)
    here: [https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: Right, now that you have learned how to use the slab allocator APIs better,
    let's dive a bit deeper still. The reality is, there are still a few important
    caveats regarding the size of the memory chunks allocated by the slab allocator
    APIs. Do read on to find out what they are!
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Caveats when using the slab allocator
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will split up this discussion into three parts. We will first re-examine
    some necessary background (which we covered earlier), then actually flesh out
    the problem with two use cases – the first being very simple, and the second a
    more real-world case of the issue at hand.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Background details and conclusions
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, you have learned some key points:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: The *page* (or *buddy system*) *allocator* allocates power-of-2 pages to the
    caller. The power to raise 2 to is called the *order*; it typically ranges from
    `0` to `10` (on both x86[_64] and ARM).
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is fine, except when it's not. When the amount of memory requested is very
    small, the *wastage* (or internal fragmentation) can be huge.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requests for fragments of a page (less than 4,096 bytes) are very common. Thus,
    the *slab allocator, layered upon the page allocator *(see Figure 8.1) is designed
    with object caches, as well as small generic memory caches, to efficiently fulfill
    requests for small amounts of memory.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The page allocator guarantees physically contiguous page and cacheline-aligned
    memory.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The slab allocator guarantees physically contiguous and cacheline-aligned memory.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, fantastic – this leads us to conclude that when the amount of memory required
    is large-ish and a perfect (or close) power of 2, use the page allocator. When
    it''s quite small (less than a page), use the slab allocator. Indeed, the kernel
    source code of `kmalloc()` has a comment that neatly sums up how the `kmalloc()` API
    should be used (reproduced in bold font as follows):'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Sounds great, but there is still a problem! To see it, let''s learn how to
    use another useful slab API, `ksize()`. Its signature is as follows:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The parameter to `ksize()` is a pointer to an existing slab cache (it must be
    a valid one). In other words, it's the return address from one of the slab allocator
    APIs (typically, `k[m|z]alloc()`). The return value is the actual number of bytes
    allocated.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: Okay, now that you know what `ksize()` is for, let's use it in a more practical
    fashion, first with a simple use case and then with a better one!
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Testing slab allocation with ksize() – case 1
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand what we''re getting at, consider a small example (for readability,
    we will not show essential validity checks. Also, as this is a tiny code snippet,
    we haven''t provided it as a kernel module in the book''s code base):'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The resulting output on my x86_64 Ubuntu guest system is as follows:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: So, we attempted to allocate 20 bytes with `kzalloc()`, but actually obtained
    32 bytes (thus incurring a wastage of 12 bytes, or 60%!). This is expected. Recall
    the `kmalloc-n` slab caches – on x86, there is one for 16 bytes and another for
    32 bytes (among the many others). So, when we ask for an amount in between the
    two, we obviously get memory from the higher of the two. (Incidentally, and FYI,
    on our ARM-based Raspberry Pi system, the smallest slab cache for `kmalloc` is
    64 bytes, so, of course, we get 64 bytes when we ask for 20 bytes.)
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `ksize()` API works only on allocated slab memory; you cannot
    use it on the return value from any of the page allocator APIs (which we saw in
    the *Understanding and u**sing the kernel page allocator (or BSA)* section).
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: Now for the second, and more interesting, use case.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Testing slab allocation with ksize() – case 2
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Okay, now, let''s extend our previous kernel module (`ch8/slab3_maxsize`) to `ch8/slab4_actualsize`.
    Here, we will perform the same loop, allocating memory with `kmalloc()` and freeing
    it as before, but this time, we will also document the actual amount of memory
    allocated to us in each loop iteration by the slab layer, by invoking the `ksize()` API:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output of this kernel module is indeed interesting to scan! In the following
    figure, we show a partial screenshot of the output I got on my x86_64 Ubuntu 18.04
    LTS guest running our custom built 5.4.0 kernel:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdf32434-baf3-4fa3-a64f-7327729d0c8a.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Partial screenshot of our slab4_actualsize.ko kernel module in
    action
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: The module's printk output can be clearly seen in the preceding screenshot. The
    remainder of the screen is diagnostic information from the kernel – this is emitted
    as a kernel-space memory allocation request failed. All this kernel diagnostic
    information is a  result of the first invocation of the kernel calling the `WARN_ONCE()` macro,
    as the underlying page allocator code, `mm/page_alloc.c:__alloc_pages_nodemask()` –
    the "heart" of the buddy system allocator, as it's known -  failed! This should
    typically never occur, hence the diagnostics (the details on the kernel diagnostics
    is beyond this book's scope, so we will leave this aside. Having said that, we
    do examine the kernel stack backtrace to some extent in coming chapters).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the output from case 2
  id: totrans-430
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Look closely at the preceding screenshot (Figure 8.12; here, we will simply
    ignore the kernel diagnostics emitted by the `WARN()` macro, which got invoked
    because a kernel-level memory allocation failed!). The Figure 8.12 output has
    five columns, as follows:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: The timestamp from `dmesg(1)`; we ignore it.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kmalloc(n)`: The number of bytes requested by `kmalloc()` (where `n` is the
    required amount).'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The actual number of bytes allocated by the slab allocator (revealed via `ksize()`).
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The wastage (bytes): The difference between the actual and required bytes.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The wastage as a percentage.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As an example, in the second allocation, we requested 200,100 bytes, but actually
    obtained 262,144 bytes (256 KB). This makes sense, as this is the precise size
    of one of the page allocator lists on a buddy system freelist (it's *order 6*,
    as *2⁶ = 64 pages = 64 x 4 = 256 KB*; see *Figure 8.2*). Hence, the delta, or
    wastage really, is *262,144 - 200,100 = 62,044 bytes*, which, when expressed as
    a percentage, is 31%.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s like this: the closer the requested (or required) size gets to the kernel''s
    available (or actual) size, the less the wastage will be; the converse is true
    as well. Let''s look at another example from the preceding output (the snipped
    output is reproduced as follows for clarity):'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'From the preceding output, you can see that when `kmalloc()` requests 1,600,100
    bytes (around 1.5 MB), it actually gets 2,097,152 bytes (exactly 2 MB), and the
    wastage is 31%. The wastage then successively *reduces as we get closer to an
    allocation "boundary" or threshold* (the actual size of the kernel''s slab cache
    or page allocator memory chunk) as it were: to 16%, then down to  4%. But look:
    with the next allocation, when we cross that threshold, asking for *just over* 2
    MB (2,200,100 bytes), we actually get 4 MB, *a wastage of 90%*! Then, the wastage
    again drops as we move closer to the 4 MB memory size...'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: This is important! You might think you're being very efficient by mere use of
    the slab allocator APIs, but in reality, the slab layer invokes the page allocator
    when the amount of memory requested is above the maximum size that the slab layer
    can provide (typically, 8 KB, which is often the case in our preceding experiments).
    Thus, the page allocator, suffering from its usual wastage issues, ends up allocating
    far more memory than you actually require, or indeed ever use. What a waste!
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: 'The moral: *check and recheck your code that allocates memory with the slab
    APIs*. Run trials on it using `ksize()` to figure out how much memory is actually
    being allocated, not how much you think is being allocated.'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: 'There are no shortcuts. Well, there is one: if you require less than a page
    of memory (a very typical use case), just use the slab APIs. If you require more,
    the preceding discussion comes into play. Another thing: using the `alloc_pages_exact()
    / free_pages_exact()` APIs (covered in the *One Solution – the exact page allocator
    APIs* section) should help reduce wastage as well.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: Graphing it
  id: totrans-444
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As an interesting aside, we use the well-known `gnuplot(1)` utility to plot
    a graph from the previously gathered data. Actually, we have to minimally modify
    the kernel module to only output what we''d like to graph: the required (or requested)
    amount of memory to allocate (*x* axis), and the percentage of waste that actually
    occurred at runtime (*y* axis). You can find the code of our slightly modified
    kernel module in the book''s GitHub repository here: `ch8/slab4_actualsz_wstg_plot` ([https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/ch8/slab4_actualsize](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/ch8/slab4_actualsize)).'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we build and insert this kernel module, "massage" the kernel log, saving
    the data in an appropriate column-wise format as required by `gnuplot` (in a file
    called `2plotdata.txt`). While we do not intend to delve into the intricacies
    of using `gnuplot(1)` here (refer to the *Further reading* section for a tutorial
    link), in the following code snippet, we show the essential commands to generate
    our graph:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Lo and behold, the plot:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0680fcb-f728-4941-a8ea-c00a301d1e8b.png)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – A graph showing the size requested by kmalloc() (x axis) versus
    the wastage incurred (as a percentage; y axis)
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: This "saw-tooth"-shaped graph helps visualize what you just learned. The closer
    a `kmalloc()` (or `kzalloc()`, or indeed *any* page allocator API) allocation
    request size is to any of the kernel's predefined freelist sizes, the less wastage
    there is. But the moment this threshold is crossed, the wastage zooms up (spikes)
    to close to 100% (as seen by the literally vertical lines in the preceding graph).
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: 'So, with this, we''ve covered a significant amount of stuff. As usual, though,
    we''re not done: the next section very briefly highlights the actual slab layer
    implementations (yes, there are several) within the kernel. Let''s check it out!'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: Slab layer implementations within the kernel
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In closing, we mention the fact that there are at least three different mutually
    exclusive kernel-level implementations of the slab allocator; only one of them
    can be in use at runtime. The one to be used at runtime is selected at the time
    of *configuring* the kernel (you learned this procedure in detail in [Chapter
    2](e0b89a37-18a3-424d-8983-58c4ac0725f6.xhtml), *Building the 5.x Linux Kernel
    from Source – Part 1*). The relevant kernel configuration options are as follows:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '`CONFIG_SLAB`'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONFIG_SLUB`'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONFIG_SLOB`'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first (`SLAB`) is the early, well-supported (but quite under-optimized)
    one; the second one (`SLUB`*, the unqueued allocator*) is a major improvement
    on the first, in terms of memory efficiency, performance, and better diagnostics,
    and is the one selected by default. The `SLOB` allocator is a drastic simplification
    and, as per the kernel config help, "does not perform well on large systems."
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-459
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned – to a good level of detail – how both the page
    (or buddy system) as well as the slab allocators work. Recall that the actual
    "engine" of allocating (and freeing) RAM within the kernel is ultimately the *page
    (or buddy system) allocator*,the slab allocator being layered on top of it to
    provide optimization for typical less-than-a-page-in-size allocation requests
    and to efficiently allocate several well-known kernel data structures ('objects').
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: You learned how to efficiently use the APIs exposed by both the page and slab
    allocators, with several demo kernel modules to help show this in a hands-on manner.
    A good deal of focus was (quite rightly) given to the real issue of the developer
    issuing a memory request for a certain *N* number of bytes, but you learned that
    it can be very sub-optimal, with the kernel actually allocating much more (the
    wastage can climb to very close to 100%)! You now know how to check for and mitigate
    these cases. Well done!
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: The following chapter covers more on optimal allocation strategies, as well
    as some more advanced topics on kernel memory allocation, including the creation
    of custom slab caches, using the `vmalloc` interfaces, what the *OOM killer *is
    all about, and more. So, first ensure you've understood the content of this chapter
    and worked on the kernel modules and assignments (as follows). Then, let's get
    you on to the next one!
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-463
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we conclude, here is a list of questions for you to test your knowledge
    regarding this chapter''s material: [https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions).
    You will find some of the questions answered in the book''s GitHub repo: [https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn).'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-465
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To help you delve deeper into the subject with useful materials, we provide
    a rather detailed list of online references and links (and at times, even books)
    in a Further reading document in this book's GitHub repository. The *Further reading*
    document is available here: [https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md).
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
