- en: Performance
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common reasons to choose C++ as a key programming language for
    a project is due to performance requirements. C++ has a clear edge over the competition
    when it comes to performance, but achieving the best results requires understanding
    relevant problems. This chapter focuses on increasing the performance of C++ software.
    We'll start by showing you tools for measuring performance. We'll show you a few
    techniques for increasing single-threaded compute speed. Then we'll discuss how
    to make use of parallel computing. Finally, we'll show how you can use C++20's
    coroutines for non-preemptive multitasking.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helping the compiler generate performant code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelizing computations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using coroutines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, let's specify what you'll need to run the examples in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To replicate the examples from this chapter, you should install the following:'
  prefs: []
  type: TYPE_NORMAL
- en: CMake 3.15+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A compiler supporting C++20's ranges and coroutines, for instance, GCC 10+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source code snippets from the chapter can be found at [https://github.com/PacktPublishing/Software-Architecture-with-Cpp/tree/master/Chapter11](https://github.com/PacktPublishing/Software-Architecture-with-Cpp/tree/master/Chapter11).
  prefs: []
  type: TYPE_NORMAL
- en: Measuring performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To effectively improve the performance of your code, you must start by measuring
    how it performs. Without knowing where the actual bottlenecks are, you will end
    up optimizing the wrong places, losing time, and getting surprised and frustrated
    that your hard work gave little to no gains. In this section, we'll show how to
    properly measure performance using benchmarks, how to successfully profile your
    code, and how to gain insights into performance in distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: Performing accurate and meaningful measurements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For accurate and repeatable measurements, you might also want to put your machine
    into performance mode instead of the usual default power-saving one. If you require
    low latency from your system, you might want to disable power saving permanently
    on both the machines you benchmark on and in your production environment. Many
    times this may mean going into BIOS and configuring your server properly. Note
    that this may not be possible if you use a public cloud provider. If you have
    root/admin permissions on your machine, the OS can often steer some of the settings
    too. For instance, you can force your CPU to run with its maximum frequency on
    a Linux system by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Moreover, to obtain meaningful results, you might want to perform measurements
    on a system that as closely resembles your production environment as possible.
    Aside from configuration, aspects such as the different speeds of RAM, the number
    of CPU caches, and the microarchitecture of your CPUs can also skew your results
    and lead you to incorrect conclusions. The same goes for the hard drive setup
    and even the network topology and hardware used. The software you build on also
    plays a crucial role: from the firmware used, through the OS and kernel, all the
    way up the software stack to your dependencies. It''s best to have a second environment
    that''s identical to your production one and governed using the same tools and
    scripts.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a solid environment for taking measurements, let's see what
    we can actually measure.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging different types of measuring tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several ways to measure performance, each focusing on a different
    scope. Let's go through them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmarks can be used to time the speed of your system in a pre-made test.
    Usually, they result in either a time to finish or another performance metric
    such as orders processed per second. There are several types of benchmarks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Microbenchmarks,** which you can use to measure the execution of a small
    code fragment. We''ll cover them in the next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simulations,** which are synthetic tests on a larger scale with artificial
    data. They can be useful if you don''t have access to the target data or your
    target hardware. For instance, when you are planning to check the performance
    of hardware that you''re working on, but it doesn''t exist yet, or when you plan
    to handle incoming traffic, but can only assume how the traffic will look.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replays,** which can be a very accurate way of measuring performance under
    the real-life workload. The idea is to record all the requests or workloads coming
    into the production system, often with timestamps. Such dumps can then later be
    "replayed" into the benchmarked system, respecting the time differences between
    them, to check how it performs. Such benchmarks can be great to see how potential
    changes to code or the environment can influence the latency and throughput of
    your system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Industry-standard**, which is a good way to see how our product performs
    compared to its competitors. Examples of such benchmarks include SuperPi for CPUs,
    3D Mark for graphic cards, and ResNet-50 for artificial intelligence processors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aside from benchmarking, another type of tool that is invaluable when it comes
    to measuring performance is profilers. Instead of just giving you overall performance
    metrics, profilers allow you to examine what your code is doing and look for bottlenecks.
    They're useful for catching unexpected things that slow your system down. We'll
    cover them in more detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last way to grasp your system''s performance is tracing. Tracing is essentially
    a way to log your system''s behavior during execution. By monitoring how long
    it takes for a request to complete various steps of processing (such as being
    handled by different types of microservices), you can gain insight into what parts
    of your system need to improve their performance, or how well your system deals
    with different kinds of requests: either different types or those that get accepted
    or rejected. We''ll cover tracing later in this chapter â€“ right after profiling.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now say a few more words on microbenchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Using microbenchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microbenchmarks are used to measure how fast a "micro" fragment of code can
    perform. If you're wondering how to implement a given functionality or how fast
    different third-party libraries deal with the same task, then they're the perfect
    tool for the job. While they're not representative of a realistic environment,
    they're well suited to perform such small experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s show how to run such experiments using one of the most commonly used
    frameworks to create microbenchmarks in C++: Google Benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Google Benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s start by introducing the library into our code by using Conan. Put the
    following in your `conanfile.txt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re going to use the CMakeDeps generator as it''s the recommended CMake
    generator in Conan 2.0\. It relies on CMake''s `find_package` feature to use the
    packages installed by our barbaric dependency manager. To install the dependencies
    in their release versions, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you're using a custom Conan profile, remember to add it here as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using it from your `CMakeLists.txt` file is also pretty straightforward, as
    shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: First, we add our build directory to `CMAKE_PREFIX_PATH` so that CMake can find
    the config and/or target files produced by Conan. Next, we just use them to find
    our dependency.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we''re going to create several microbenchmarks, we could use a CMake function
    to help us with defining them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The function will be able to create single-translation-unit microbenchmarks,
    each using C++20 and linked to the Google Benchmark library. Let''s now use it
    to create our first microbenchmark executable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now we're ready to put some code in our source file.
  prefs: []
  type: TYPE_NORMAL
- en: Writing your first microbenchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ll try to benchmark how much faster a lookup takes when it''s done using
    bisection in a sorted vector as compared to just going through it linearly. Let''s
    start with code that will create the sorted vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Our vector will contain size elements with all the numbers from 0 to size -
    1 in ascending order. Let''s now specify the element we''re looking for and the
    container size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we''ll benchmark how long it takes to find a needle in a haystack.
    The simple linear search can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see the first use of Google Benchmark. Each microbenchmark should
    accept `State` as an argument. This special type does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Contains information about the iterations performed and the time spent on the
    measured computation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counts the bytes processed if wanted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can return other state information, such as the need to run further (through
    the `KeepRunning()` member function)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be used to pause and resume the timing of an iteration (through the `PauseTiming()`
    and `ResumeTiming()` member functions, respectively)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code in our loop will be measured, making as many iterations as desired,
    based on the total allowed time to run this particular benchmark. The creation
    of our haystack is outside the loop and won't be measured.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the loop, there's a sink helper named `DoNotOptimize`. Its purpose is
    to ensure the compiler doesn't get rid of our computations as it can prove that
    they are irrelevant outside of this scope. In our case, it will mark the result
    of `std::find` necessary, so the actual code to find the needle is not optimized
    away. Using tools such as objdump or sites such as Godbolt and QuickBench allows
    you to peek if the code you want to run wasn't optimized out. QuickBench has the
    additional advantage of running your benchmarks in the cloud and sharing their
    results online.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back to our task at hand, we have a microbenchmark for the linear search, so
    let''s now time the binary search in another microbenchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Our new benchmark is pretty similar. It only differs in the function used:
    `lower_bound` will perform a binary search. Note that similar to our base example,
    we don''t even check if the iterator returned points to a valid element in the
    vector, or to its end. In the case of `lower_bound`, we could check if the element
    under the iterator is actually the one we''re looking for.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the microbenchmark functions, let''s create actual benchmarks
    out of them by adding the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If the default benchmark settings are okay with you, that''s all you need to
    pass. As the last step, let''s add a `main()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Simple as that! Alternatively, you can link our program with `benchmark_main`
    instead. Using Google Benchmark''s `main()` function has the advantage of providing
    us with some default options. If you compile our benchmark and run it passing
    `--help` as a parameter, you''ll see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This is a nice set of features to use. For example, when designing experiments,
    you can use the `benchmark_format` switch to get a CSV output for easier plotting
    on a chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see our benchmark in action by running the compiled executable with
    no command-line arguments. A possible output from running `./microbenchmark_1`
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Starting with some data about the running environment (the time of benchmarking,
    the executable name, the server's CPUs, and the current load), we get to the results
    of each benchmark we defined. For each benchmark, we get the average wall time
    per iteration, the average CPU time per iteration, and the number of iterations
    that the benchmark harness ran for us. By default, the longer a single iteration,
    the fewer iterations it will go through. Running more iterations ensures you get
    more stable results.
  prefs: []
  type: TYPE_NORMAL
- en: Passing arbitrary arguments to a microbenchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we were to test more ways of dealing with our problem at hand, we could look
    for a way to reuse the benchmark code and just pass it to the function used to
    perform the lookup. Google Benchmark has a feature that we could use for that.
    The framework actually lets us pass any arguments we want to the benchmark by
    adding them as additional parameters to the function signature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how a unified signature for our benchmark could look with this feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can notice the new `finder` parameter to the function, which is used in
    the spot where we previously called either `find` or `lower_bound`. We can now
    make our two microbenchmarks using a different macro than we did last time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `BENCHMARK_CAPTURE` macro accepts the function, a name suffix, and the
    arbitrary number of parameters. If we wanted more, we could just pass them here.
    Our benchmark function could be a regular function or a template â€“ both are supported.
    Let''s now see what we get when running the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As you can see the arguments passed to the functions are not part of the name,
    but the function name and our suffix are.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see how we can further customize our benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Passing numeric arguments to a microbenchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A common need when designing experiments like ours is to check it on different
    sizes of arguments. Such needs can be addressed in Google Benchmark in a number
    of ways. The simplest is to just add a call to `Args()` on the object returned
    by the `BENCHMARK` macros. This way, we can pass a single set of values to use
    in a given microbenchmark. To use the passed value, we''d need to change our benchmark
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The call to `state.range(0)` will read the 0-th argument passed. An arbitrary
    number can be supported. In our case, it''s used to parameterize the haystack
    size. What if we wanted to pass a range of value sets instead? This way, we could
    see how changing the size influences the performance more easily. Instead of calling
    `Args`, we could call `Range` on the benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the range boundaries using a predefined minimum and maximum. We
    then tell the benchmark harness to create the ranges by multiplying by 10 instead
    of the default value. When we run such benchmarks, we could get the following
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: When analyzing those results, you might be wondering why the linear search doesn't
    show us linear growth. That's because we look for a constant value of the needle
    that can be spotted at a constant position. If the haystack contains our needle,
    we need the same number of operations to find it regardless of the haystack size,
    so the execution time stops growing (but can still be subject to small fluctuations).
  prefs: []
  type: TYPE_NORMAL
- en: Why not play with the needle position as well?
  prefs: []
  type: TYPE_NORMAL
- en: Generating the passed arguments programmatically
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generating both the haystack sizes and needle positions might be the easiest
    when done in a simple function. Google Benchmark allows such scenarios, so let's
    show how they work in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first rewrite our benchmark function to use two parameters passed in
    each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, `state.range(0)` will mark our needle position, while `state.range(1)`
    will be the haystack size. This means we need to pass two values each time. Let''s
    create a function that generates them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Instead of using `Range` and `RangeMultiplier`, we write a loop to generate
    the haystack sizes, this time increasing them by 100 each time. When it comes
    to the needles, we use three positions in proportionate positions of the haystack
    and one that falls outside of it. We call `Args` on each loop iteration, passing
    both the generated values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s apply our generator function to the benchmarks we define:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Using such functions makes it easy to pass the same generator to many benchmarks.
    Possible results of such benchmarks are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now we have a pretty well-defined experiment for performing the searches. As
    an exercise, run the experiment on your own machine to see the complete results
    and try to draw some conclusions from the results.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing what to microbenchmark and optimize
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Running such experiments can be educative and even addictive. However, keep
    in mind that microbenchmarks shouldn''t be the only type of performance testing
    in your project. As Donald Knuth famously said:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We should forget about small efficiencies, say about 97% of the time: premature
    optimization is the root of all evil*'
  prefs: []
  type: TYPE_NORMAL
- en: This means that you should microbenchmark only code that matters, especially
    code on your hot path. Larger benchmarks, along with tracing and profiling, can
    be used to see where and when to optimize instead of guessing and optimizing prematurely.
    First, understand how your software executes.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: There''s one more point we want to make regarding the quote above. It
    doesn''t mean you should allow premature *pessimization*. Poor choice of data
    structures or algorithms, or even small inefficiencies that spread all your code,
    can sometimes influence the overall performance of your system. For instance,
    performing unnecessary dynamic allocations, although it might not look that bad
    at first, can lead to heap fragmentation over time and cause you serious trouble
    if your app should run for long periods of time. Overuse of node-based containers
    can lead to more cache misses too. Long story short, if it''s not a big effort
    to write efficient code instead of less efficient code, go for it.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now learn what to do if your project has spots that need to maintain good
    performance over time.
  prefs: []
  type: TYPE_NORMAL
- en: Creating performance tests using benchmarks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to having unit tests for precise testing and functional tests for larger-scale
    testing of your code's correctness, you can use microbenchmarks and larger benchmarks
    to test your code's performance.
  prefs: []
  type: TYPE_NORMAL
- en: If you have tight constraints on the execution time for certain code paths,
    having a test that ensures the limit is met can be very useful. Even if you don't
    have such specific constraints, you might be interested in monitoring how the
    performance changes across code changes. If after a change your code runs slower
    than before by a certain threshold, the test could be marked as failed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although also a useful tool, remember that such tests are prone to the boiling
    frog effect: degrading the performance slowly over time can go unnoticed, so be
    sure to monitor the execution times occasionally. When introducing performance
    tests to your CI, be sure to always run them in the same environment for stable
    results.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now discuss the next type of tools in our performance shed.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While benchmarks and tracing can give you an overview and specific numbers for
    a given scope, profilers can help you analyze where those numbers came from. They
    are an essential tool if you need to gain insight into your performance and improve
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the type of profiler to use
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two types of profilers available: instrumentation profilers and sampling
    ones. One of the better-known instrumentation profilers is Callgrind, part of
    the Valgrind suite. Instrumentation profilers have lots of overhead because they
    need to, well, instrument your code to see what functions you call and how much
    each of them takes. This way, the results they produce contain even the smallest
    functions, but the execution times can be skewed by this overhead. It also has
    the drawback of not always catching **input/output** (**I/O**) slowness and jitters.
    They slow down the execution, so while they can tell you how often you call a
    particular function, they won''t tell you if the slowness is due to waiting on
    a disk read to finish.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the flaws of instrumentation profilers, it's usually better to use sampling
    profilers instead. Two worth mentioning are the open source perf for profiling
    on Linux systems and Intel's proprietary tool called VTune (free for open source
    projects). Although they can sometimes miss key events due to the nature of sampling,
    they should usually give you a much better view of where your code spends time.
  prefs: []
  type: TYPE_NORMAL
- en: If you decide to use perf, you should know that you can either use it by invoking
    `perf stat`, which gives you a quick overview of statistics like CPU cache usage,
    or `perf record -g` and `perf report -g` to capture and analyze profiling results.
  prefs: []
  type: TYPE_NORMAL
- en: If you want a solid overview of perf, please watch Chandler Carruth's video,
    which shows the tool's possibilities and how to use it, or take a look at its
    tutorial. Both are linked in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the profiler and processing the results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When analyzing profiling results, you may often want to perform some preparation,
    cleanup, and processing. For instance, if your code mostly spends time spinning
    around, you might want to filter that out. Before even starting the profiler,
    be sure to compile or download as many debug symbols as you can, both for your
    code, your dependencies, even the OS libraries, and kernel. Also, it's essential
    you disable frame pointer optimizations. On GCC and Clang, you can do so by passing
    the `-fno-omit-frame-pointer` flag. It won't affect performance much but will
    give you much more data about the execution of your code. When it comes to post-processing
    of the results, when using perf, it's usually a good idea to create flame graphs
    from the results. Brendan Gregg's tool from the *Further reading* section is great
    for that. Flame graphs are a simple and effective tool to see where the execution
    takes too much time, as the width of each item on the graph corresponds to the
    resource usage. You can have flame graphs for CPU usage, as well as for resources
    such as memory usage, allocations, and page faults, or the time spent when the
    code is not executing such as staying blocked during system calls, on mutexes,
    I/O operations, and the like. There are also ways to perform diffs on the generated
    flame graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Keep in mind that not all performance issues will show up on such graphs and
    not all can be found using profilers. While with some experience you''ll be able
    to see that you could benefit from setting affinity to your threads or changing
    which threads execute on specific NUMA nodes, it might not always be that obvious
    to see that you''ve forgotten to disable power-saving features or would benefit
    from enabling or disabling hyper-threading. Information about the hardware you''re
    running on is useful, too. Sometimes you might see the SIMD registers of your
    CPU being used, but the code still doesn''t run at its full speed: you might be
    using SSE instructions instead of AVX ones, AVX instead of AVX2, or AVX2 instead
    of AVX512\. Knowing what specific instructions your CPU is capable of running
    can be golden when you analyze the profiling results.'
  prefs: []
  type: TYPE_NORMAL
- en: Solving performance issues also requires a bit of experience. On the other hand,
    sometimes experience can lead you to false assumptions. For instance, in many
    cases, using dynamic polymorphism will hurt your performance; there are cases
    where it doesn't slow down your code. Before jumping to conclusions, it might
    be worth profiling the code and gaining knowledge about the various ways a compiler
    can optimize code and the limits of those techniques. Talking specifically about
    virtualization, it's often beneficial to mark your classes of virtual member functions
    as final when you don't want other types to inherit and override them, respectively.
    This tends to help the compilers in lots of cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compilers can also optimize much better if they "see" what type the object
    is: if you create a type in scope and call its virtual member function, the compiler
    should be able to deduce which function should be called. GCC tends to devirtualize
    better than other compilers. For more information on this, you can refer to Arthur
    O''Dwyer''s blog post from the *Further reading* section.'
  prefs: []
  type: TYPE_NORMAL
- en: As with other types of tools presented in this section, try not to rely only
    on your profiler. Improvements in profiling results are not a guarantee that your
    system got faster. A better-looking profile can still not tell you the whole story.
    And the better performance of one component doesn't necessarily mean the whole
    system's performance improved. This is where our last type of tool can come in
    use.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last technique we'll discuss in this section is meant for distributed systems.
    When looking at the overall system, often deployed in the cloud, profiling your
    software on one box won't tell you the whole story. In such a scope, your best
    bet would be to trace the requests and responses flowing through your system.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing is a way to log the execution of your code. It's often used when a request
    (and sometimes its response) has to flow through many parts of your system. Usually,
    such messages are being traced along the route, with timestamps being added at
    interesting points of execution.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation IDs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One common addition to timestamps is correlation IDs. Basically, they're unique
    identifiers that get assigned to each traced message. Their purpose is to correlate
    the logs produced by different components of your system (like different microservices)
    during the processing of the same incoming request and sometimes for the events
    it caused, too. Such IDs should be passed with the message everywhere it goes,
    for example, by appending to its HTTP header. Even when the original request is
    gone, you could add its correlation ID to each of the responses produced.
  prefs: []
  type: TYPE_NORMAL
- en: By using correlation IDs, you can track how messages for a given request propagate
    through the system and how long it took for different parts of your system to
    process it. Often you'll want additional data to be gathered along the way, like
    the thread that was used to perform the computation, the type, and count of responses
    produced for a given request, or the names of the machines it went through.
  prefs: []
  type: TYPE_NORMAL
- en: Tools like Jaeger and Zipkin (or other OpenTracing alternatives) can help you
    to add tracing support to your system fast.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now tackle a different subject and say a few words about code generation.
  prefs: []
  type: TYPE_NORMAL
- en: Helping the compiler generate performant code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many things that can help your compiler generate efficient code for
    you. Some boil down to steering it properly, others require writing your code
    in a compiler-friendly way.
  prefs: []
  type: TYPE_NORMAL
- en: It's also important to know what you need to do on your critical path and to
    design it efficiently. For instance, try to avoid virtual dispatch there (unless
    you can prove it's being devirtualized), and try not to allocate new memory on
    it. Often, the clever design of code to avoid locking (or at least using lock-free
    algorithms) is helpful. Generally speaking, everything that can worsen your performance
    should be kept outside your hot path. Having both your instruction and data caches
    hot is really going to pay out. Even attributes such as `[[likely]]` and `[[unlikely]]`
    that hint to the compiler which branch it should expect to be executed can sometimes
    change a lot.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing whole programs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An interesting way to increase the performance of many C++ projects is to enable
    **link-time optimization** (**LTO**). During compilation, your compiler doesn''t
    know how the code will get linked with other object files or libraries. Many opportunities
    to optimize arise only at this point: when linking, your tools can see the bigger
    picture of how the parts of your program interact with each other. By enabling
    LTO, you can sometimes grab a significant improvement in performance with very
    little cost. In CMake projects, you can enable LTO by setting either the global
    `CMAKE_INTERPROCEDURAL_OPTIMIZATION` flag or by setting the `INTERPROCEDURAL_OPTIMIZATION`
    property on your targets.'
  prefs: []
  type: TYPE_NORMAL
- en: One drawback of using LTO is that it makes the building process longer. Sometimes
    a lot longer. To mitigate this cost for developers, you may want to only enable
    this optimization for builds that undergo performance testing or are meant to
    be released.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing based on real-world usage patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another interesting way to optimize your code is to use **Profile-Guided Optimization**
    (**PGO**). This optimization is actually a two-step one. In the first step, you
    need to compile your code with additional flags that cause the executable to gather
    special profiling information during runtime. You should then execute it under
    the expected production load. Once you're done with it, you can use the gathered
    data to compile the executable a second time, this time passing a different flag
    that instructs the compiler to use the gathered data to generate code better suited
    for your profile. This way, you'll end up with a binary that's prepared and tuned
    to your specific workload.
  prefs: []
  type: TYPE_NORMAL
- en: Writing cache-friendly code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Both those types of optimization can be of use, but there''s one more important
    thing that you need to keep in mind when working on performant systems: cache
    friendliness. Using flat data structures instead of node-based ones means that
    you need to perform less pointer chasing at runtime, which helps your performance.
    Using data that''s contiguous in memory, regardless of whether you''re reading
    it forward or backward, means your CPU''s memory prefetcher can load it before
    it''s used, which can often make a huge difference. Node-based data structures
    and the mentioned pointer chasing cause random memory access patterns that can
    "confuse" the prefetcher and make it impossible for it to prefetch correct data.'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to see some performance results, please refer to the *C++ Containers
    Benchmark* linked in the *Further reading* section. It compares various usage
    scenarios of `std::vector`, `std::list`, `std::deque`, and `plf::colony`. If you
    don't know that last one, it's an interesting "bag"-type container with great
    fast insertion and deletion of large data.
  prefs: []
  type: TYPE_NORMAL
- en: When choosing from associative containers, you'll most often want to use "flat"
    implementations instead of node-based ones. This means that instead of using `std::unordered_map`
    and `std::unordered_set`, you might want to try out ones like `tsl::hopscotch_map`
    or Abseil's `flat_hash_map` and `flat_hash_set`.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques such as putting colder instructions (such as exception handling code)
    in a non-inline function can help to increase the hotness of your instruction
    cache. This way, lengthy code for handling rare cases will not be loaded in the
    instruction cache, leaving space for more code that should be there, which can
    also improve your performance.
  prefs: []
  type: TYPE_NORMAL
- en: Designing your code with data in mind
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to help your caches, another technique that can be helpful is data-oriented
    design. Often, it's a good idea to store members used more often close to each
    other in memory. Colder data can often be placed in another struct and just be
    connected with the hotter data by an ID or a pointer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, instead of the more commonly spotted arrays of objects, using objects
    of arrays can yield better performance. Instead of writing your code in an object-oriented
    manner, split your object''s data member across a few arrays, each containing
    data for multiple objects. In other words, take the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'And consider replacing it with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This way, when processing a specific set of data points against some objects,
    the cache hotness increases and so does the performance. If you don't know whether
    this will yield more performance from your code, measure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes even reordering members of your types can give you better performance.
    You should take into account the alignment of your types of data members. If performance
    matters, usually it''s a good idea to order them so that the compiler doesn''t
    need to insert too much padding between the members. Thanks to that, the size
    of your data type can be smaller, so many such objects can fit into one cache
    line. Consider the following example (let''s assume we''re compiling for the x86_64
    architecture):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Despite the sizes being 8 bytes each and chars being just 1 byte each, we end
    up with 32 bytes in total! That's because `second_size` must start on an 8-byte
    aligned address, so after `first_char`, we get 7 bytes of padding. The same goes
    for `second_char`, as types need to be aligned with respect to their largest data
    type member.
  prefs: []
  type: TYPE_NORMAL
- en: 'Can we do better? Let''s try switching the order of our members:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: By simply putting the biggest members first, we were able to cut the size of
    our structure by 8 bytes, which is 25% of its size. Not bad for such a trivial
    change. If your goal is to pack many such structs in a contiguous block of memory
    and iterate through them, you could see a big performance boost of that code fragment.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now talk about another way to improve your performance.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing computations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll discuss a few different ways to parallelize computations.
    We will start with a comparison between threads and processes, after which we'll
    show you the tools available in the C++ standard, and last but not least, we'll
    say a few words about the OpenMP and MPI frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start, let's say a few words on how to estimate the maximum possible
    gains you can have from parallelizing your code. There are two laws that can help
    us here. The first is Amdahl's law. It states that if we want to speed up our
    program by throwing more cores at it, then the part of our code that must remain
    sequential (cannot be parallelized) will limit our scalability. For instance,
    if 90% of your code is parallelizable, then even with infinite cores you can still
    get only up to a 10x speedup. Even if we cut down the time to execute that 90%
    to zero, the 10% of the code will always remain there.
  prefs: []
  type: TYPE_NORMAL
- en: The second law is Gustafson's law. It states that every large-enough task can
    be efficiently parallelized. This means that by increasing the size of the problem,
    we can obtain better parallelization (assuming we have free computing resources
    to use). In other words, sometimes it's better to add more capabilities to be
    run in the same time frame instead of trying to reduce the execution time of existing
    code. If you can cut the time of a task by half by doubling the cores, at some
    point, doubling them again and again will get you diminishing returns, so their
    processing power can be better spent elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the differences between threads and processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To parallelize computations efficiently, you need to also understand when to
    use processes to perform computation and when threads are the better tool for
    the job. Long story short, if your only target is to actually parallelize work,
    then it's best to start with adding extra threads up to the point where they don't
    bring extra benefits. At such a point, add more processes on other machines in
    your network, each with multiple threads too.
  prefs: []
  type: TYPE_NORMAL
- en: Why is that? Because processes are more heavyweight than threads. Spawning a
    process and switching between them takes longer than creating and switching between
    threads. Each process requires its own memory space, while threads within the
    same process share their memory. Also, inter-process communication is slower than
    just passing variables between threads. Working with threads is easier than it
    is with processes, so the development will be faster too.
  prefs: []
  type: TYPE_NORMAL
- en: Processes, however, also have their uses in the scope of a single application.
    They're great for isolating components that can independently run and crash without
    taking down the whole application with them. Having separate memory also means
    one process can't snoop another one's memory, which is great when you need to
    run third-party code that could turn out to be malicious. Those two reasons are
    why they're used in web browsers, among other apps. Aside from that, it's possible
    to run different processes with different OS permissions or privileges, which
    you can't achieve with multiple threads.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now discuss a simple way to parallelize work in the scope of a single
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: Using the standard parallel algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If the computations you perform can be parallelized, there are two ways you
    can use that to your advantage. One is by replacing your regular calls to standard
    library algorithms with parallelizable ones. If you''re not familiar with parallel
    algorithms, they were added in C++17 and in essence are the same algorithms, but
    you can pass each of them an execution policy. There are three execution policies:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::execution::seq`: The sequenced policy for the plain-old execution of
    an algorithm in a non-parallelized way. This one we know too well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::par`: A parallel policy that signals that the execution *may*
    be parallelized, usually using a thread pool under the hood.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::par_unseq`: A parallel policy that signals that the execution
    *may* be parallelized and vectorized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::unseq`: A C++20 addition to the family. This policy signals
    that the execution can be vectorized, but not parallelized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the preceding policies are not enough for you, additional ones may be provided
    by a standard library implementation. Possible future additions may include ones
    for CUDA, SyCL, OpenCL, or even artificial intelligence processors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see the parallel algorithms in action. As an example, to sort a
    vector in a parallel way, you can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Simple and easy. Although in many cases this will yield better performance,
    in some cases you might be better off executing the algorithms in the traditional
    way. Why? Because scheduling work on more threads requires additional work and
    synchronization. Also, depending on the architecture of your app, it may influence
    the performance of other already existing threads and flush their cores' data
    caches. As always, measure first.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing computations using OpenMP and MPI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative to using the standard parallel algorithms would be to leverage
    OpenMP's pragmas. They're an easy way to parallelize many types of computations
    by just adding a few lines of code. And if you want to distribute your code across
    a cluster, you might want to see what MPI can do for you. Those two can also be
    joined together.
  prefs: []
  type: TYPE_NORMAL
- en: With OpenMP, you can use various pragmas to easily parallelize code. For instance,
    you can write `#pragma openmp parallel for` before a `for` loop to get it executed
    using parallel threads. The library can do much more, such as executing computations
    on GPUs and other accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating MPI into your project is harder than just adding an appropriate
    pragma. Here, you'll need to use the MPI API in your code base to send or receive
    data between processes (using calls such as `MPI_Send` and `MPI_Recv`), or perform
    various gather and reduce operations (calling `MPI_Bcast` and `MPI_Reduce`, among
    other functions in this family). Communication can be done point to point or to
    all clusters using objects called communicators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on your algorithm implementation, MPI nodes can all execute the same
    code or it can vary when needed. The node will know how it should behave based
    on its rank: a unique number assigned when the computations start. Speaking of
    which, to start a process using MPI, you should run it through a wrapper, like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This would read hosts from said file one by one, connect to each of them, and
    run four instances of `my_command` on each with the args passed.
  prefs: []
  type: TYPE_NORMAL
- en: There are many implementations of MPI. One of the most notable is OpenMPI (don't
    confuse that with OpenMP). Among some useful features, it offers fault tolerance.
    After all, it's not uncommon for a node to go down.
  prefs: []
  type: TYPE_NORMAL
- en: The last tool we'd like to mention in this section is GNU Parallel, which you
    might find useful if you want to easily span processes that perform work by spawning
    parallel processes. It can be used both on a single machine and across a compute
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Speaking about different ways to execute code, let''s now discuss one more
    big topic from C++20: coroutines.'
  prefs: []
  type: TYPE_NORMAL
- en: Using coroutines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Coroutines are functions that can suspend their execution and resume it later
    on. They allow writing asynchronous code in a very similar manner to how you would
    write synchronous code. Compared to writing asynchronous code with `std::async`,
    this allows writing cleaner code that's easier to understand and maintain. There's
    no need to write callbacks anymore, and no need to deal with the verbosity of
    `std::async` with promises and futures.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from all that, they can also often provide you with much better performance.
    `std::async` based code usually has more overhead for switching threads and waiting.
    Coroutines can resume and suspend very cheaply even compared to the overhead of
    calling functions, which means they can yield better latency and throughput. Also,
    one of their design goals was to be highly scalable, even to billions of concurrent
    coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/527abf17-78b4-4414-aa38-5bd846f4d1c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 â€“ Calling and executing coroutines is different from using regular
    functions as they can be suspended and resumed
  prefs: []
  type: TYPE_NORMAL
- en: 'C++ coroutines are stackless, which means their state is not stored on the
    calling thread''s stack. This gives them an interesting property: several different
    threads can pick up the execution of a coroutine. In other words, even though
    it looks like the coroutine function body would be executed sequentially, parts
    of it can be executed in different threads. This makes it possible to leave parts
    of the function to be executed on dedicated threads. For instance, I/O operations
    can be done in a dedicated I/O thread.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To check whether a function is a C++ coroutine, you need to look for one of
    the following keywords in its body:'
  prefs: []
  type: TYPE_NORMAL
- en: '`co_await`, which suspends the coroutine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`co_yield` for returning a value to the caller and suspending the coroutine.
    Similar to Python''s `yield` keyword used in generators. Allows generating values
    lazily.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`co_return`, which returns a value and finishes executing the coroutine. It''s
    a coroutine equivalent of the `return` keyword.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whenever a function body has one of those keywords, the function automatically
    becomes a coroutine. Although this means it''s an implementation detail, there''s
    one more hint that you can use: coroutine return types must satisfy certain requirements,
    which we''ll discuss later on.'
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines are first-class citizens in the C++ world. This means you can get
    their address, use them as function arguments, return them from functions, and
    store them in objects.
  prefs: []
  type: TYPE_NORMAL
- en: In C++, you could write coroutines even before C++20\. This was possible thanks
    to libraries such as Boost.Coroutine2, or Bloomberg's Quantum. The latter was
    even used to implement CoroKafka â€“ a library for efficiently dealing with Kafka
    streams using coroutines. With the advent of standard C++ coroutines, new libraries
    started popping up. Now, we're going to show you one of them.
  prefs: []
  type: TYPE_NORMAL
- en: Distinguishing between cppcoro utilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It's hard to write coroutine-based code from scratch. C++20 only offers the
    fundamental utilities for writing coroutines, so we need a set of primitives to
    use when writing our own coroutines. The cppcoro library created by Lewis Baker
    is one of the most commonly used coroutine frameworks for C++. In this section,
    we'll showcase the library and demonstrate how to use it when writing coroutine-based
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with an overview of the coroutine types the library offers us:'
  prefs: []
  type: TYPE_NORMAL
- en: '`task<>`: For scheduling work to be executed later â€“ starts executing when
    it''s `co_awaited` for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shared_task<>`: A task that multiple coroutines can await. It can be copied
    so that multiple coroutines reference the same result. Doesn''t offer any thread-safety
    on its own.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator`: Produces a sequence of Ts lazily and synchronously. It''s effectively
    a `std::range`: it has a `begin()` returning an iterator and an `end()` returning
    a sentinel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recursive_generator`: Similar to `generator<T>`, but can yield either a T
    or `recursive_generator<T>`. Has some extra overhead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`async_generator`: Similar to `generator<T>`, but values may be produced asynchronously.
    This means that, as opposed to generator, asynchronous generators can use `co_await`
    in their bodies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should use those types as return types for your coroutines. Usually, in
    your generators (coroutines returning one of the preceding generator types), you'd
    want to return values using `co_yield` (similar to in Python generators). In your
    tasks, however, usually, you'll want to schedule work with `co_await`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The library actually offers many more programming abstractions than just the
    preceding coroutine types. It also provides the following types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Awaitables** types that you can `co_await` on, such as coroutine-flavored
    events and synchronization primitives: mutexes, latches, barriers, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cancellation-related utilities**, essentially allowing you to cancel the
    execution of your coroutines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Schedulers** â€“ objects allowing you to schedule work through them, such as
    `static_thread_pool`, or ones for scheduling work on a specific thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**I/O and networking utilities**, allowing you to read from and write to files
    and IP sockets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Meta-functions and concepts**, such as `awaitable_traits`, `Awaitable`, and
    `Awaiter`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aside from the preceding utilities, cppcoro offers us functions â€“ utilities
    for using other classes and steering execution, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sync_wait`: Block until the passed awaitable completes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`when_all, when_all_ready`: Return an awaitable that completes when all the
    passed awaitables complete. The difference between those two is in handling failures
    of the sub-awaitables. `when_all_ready` will complete even in the event of failures
    and the caller can examine each result, while `when_all` will rethrow an exception
    if any of the sub-awaitables throws one (it''s impossible to know which one did,
    though). It will also cancel any incomplete tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fmap`: Similarly to functional programming, applies a function to an awaitable.
    You can think of it as transforming a task of one type into a task of another.
    For example, you can serialize types returned by your coroutines by calling `fmap(serialize,
    my_coroutine())`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resume_on`: Instructs the coroutine which scheduler to use to continue execution
    once some work is completed. This enables you to execute certain work in certain
    execution contexts, such as running I/O-related tasks on a dedicated I/O thread.
    Note that this means a single C++ function (coroutine) can execute its parts on
    separate threads. Can be "piped" with computations similarly to `std::ranges`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`schedule_on`: Instructs the coroutine which scheduler to use to start some
    work. Commonly used as `auto foo = co_await schedule_on(scheduler, do_work());`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we start using those utilities together, let's say a few more words about
    awaitables.
  prefs: []
  type: TYPE_NORMAL
- en: Looking under the hood of awaitables and coroutines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Aside from cppcoro, the standard library offers two more trivial awaitables:
    `suspend_never` and `suspend_always`. By looking at them, we can see how to implement
    our own awaitables when needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'When typing `co_await`, you tell the compiler to first call the awaiter''s
    `await_ready()`. If it says the awaiter is ready by returning true, `await_resume()`
    will get called. The return type of `await_resume()` should be the type the awaiter
    is actually producing. If the awaiter was not ready, the program will instead
    execute `await_suspend()`. After it''s done, we have three cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '`await_suspend` returns `void`: The execution will always suspend afterwards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`await_suspend` returns `bool`: The execution will suspend or not depending
    on the returned value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`await_suspend` returns `std::coroutine_handle<PromiseType>`: Another coroutine
    will get resumed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's much more going on with coroutines under the hood. Even though coroutines
    don't use the `return` keyword, the compiler will generate code under the hood
    to make them compile and work. When using keywords such as `co_yield`, it will
    rewrite them to calls to the appropriate member functions of helper types. For
    instance, a call to `co_yield x` is equivalent to `co_await` `promise.yield_value(x)`.
    If you want to learn more about what's happening exactly and write your own coroutine
    types, refer to the *Your First Coroutine* article from the *Further reading*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, let's now use all this knowledge to write our own coroutines. We'll create
    a simple application that mimics doing meaningful work. It will use a thread pool
    to fill a vector with some numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our CMake target will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We'll link to the cppcoro library. In our case, we're using Andreas Buhr's fork
    of cppcoro, as it is a well-maintained fork of Lewis Baker's repository and supports
    CMake.
  prefs: []
  type: TYPE_NORMAL
- en: We'll also link to the excellent `{fmt}` library for text formatting. If your
    standard library offers C++20's string formatting, you can use that instead.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, we're going to need a threading library â€“ after all, we
    want to use multiple threads in a pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start our implementation with some constants and a `main` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to produce five items using three pooled threads. cppcoro''s thread
    pool is a neat way to schedule work. By default, it creates as many threads as
    your machine has hardware ones. Moving onward, we need to specify our work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We'll sprinkle our code with log messages so you can better see what's going
    on in which thread. This will help us better understand how coroutines work. We
    create work by calling a coroutine named `do_routine_work`. It returns us the
    coroutine, which we run using the `sync_wait` blocking function. A coroutine won't
    start executing until it is actually being awaited. This means that our actual
    work will start inside this function call.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have our results, let''s log them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'No voodoo magic here. Let''s define our `do_routine_work` coroutine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'It returns a task, which produces some integers. Because we''re going to use
    the thread pool, let''s use cppcoro''s `async_mutex` to synchronize the threads.
    Let''s now start using the pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: You might be surprised that the `schedule()` call doesn't pass in any callable
    to execute. In the coroutine's case, we're actually making our current thread
    suspend the coroutine and start executing its caller. This means it will now wait
    for the coroutine to finish (somewhere in the `sync_wait` call).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the meantime, a thread from our pool will resume the coroutine â€“ simply
    continuing to execute its body. Here''s what we''ve prepared for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We create a vector of tasks to execute. Each task fills one number in `ints`
    under the mutex. The `schedule_on` call runs the filling coroutine using another
    thread from our pool. Finally, we wait for all the results. At this point, our
    tasks start executing. Finally, as our coroutine is a task, we use `co_return`
    .
  prefs: []
  type: TYPE_NORMAL
- en: Don't forget to `co_return` the produced value. If we removed the `co_return
    ints;` line from our example, we would simply return a default constructed vector.
    The program would run, happily print the empty vector, and exit with code 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our last step is to implement the coroutine that will produce a number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This one is a task that doesn''t return any value. Instead, it will add it
    to our vector. Its hard work will actually be done by dozing off for a number
    of milliseconds. After the wake-up, the coroutine will continue with more productive
    endeavors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: It will lock the mutex. In our case, it's just an `await`. When the mutex is
    locked, it will add a number to our vector â€“ the same number it was called with.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: Remember to `co_await`. If you forget and your awaitable allows that
    (perhaps because its okay to not consume each awaitable), then you might skip
    some essential computations. In our example, this could mean not locking a mutex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s finish the coroutine''s implementation now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Just a simple `status print` and a `co_return` to mark the coroutine as complete.
    Once it returns, the coroutine frame can be destroyed, freeing the memory occupied
    by it.
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s all. Let''s now run our code and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Our main thread was used to fire up the work on the pool and then waited for
    the results to come. Then, our three threads from the pool were producing numbers.
    The last task scheduled was actually the first one that ran, producing the number
    4\. This is because it was the one that continued executing `do_routine_work`
    all the time: first, it scheduled all other tasks on the pool, then started performing
    the first task when `when_all_ready` was called. Later on, the execution continued
    with the first free thread taking the next task scheduled on the pool until the
    whole vector was filled. Finally, the execution returned to our main thread.'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our short example. And with it, we conclude our last section
    of this chapter. Let's now summarize what we've learned.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've learned what types of tools can help us achieve better
    performance with our code. We learned how to perform experiments, write performance
    tests, and look for performance bottlenecks. You're now able to write microbenchmarks
    using Google Benchmark. Moreover, we discussed how to profile your code and how
    (and why) to implement distributed tracing of your system. We also discussed parallelizing
    your computations using both standard library utilities and external solutions.
    Last but not least, we introduced you to coroutines. You now know what C++20 brings
    to the coroutine table, as well as what you can find in the cppcoro library. You've
    also learned how to write your own coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important lesson from this chapter is: when it comes to performance,
    measure first and optimize later. This will help you maximize the impact of your
    work.'
  prefs: []
  type: TYPE_NORMAL
- en: That's it for performance â€“ the last of quality attributes we wanted to discuss
    in our book. In the next chapter, we'll start moving into the world of services
    and the cloud. We'll start by discussing service-oriented architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What can we learn from the performance results from this chapter's microbenchmarks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is how we traverse a multi-dimensional array important for performance? Why/why
    not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In our coroutines example, why can't we create our thread pool inside the `do_routine_work`
    function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we rework our coroutine example so it uses a generator instead of just
    tasks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When can the C++ compiler devirtualize a call?, blog post, Arthur O'Dwyer, [https://quuxplusone.github.io/blog/2021/02/15/devirtualization/](https://quuxplusone.github.io/blog/2021/02/15/devirtualization/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CppCon 2015: Chandler Carruth "Tuning C++: Benchmarks, and CPUs, and Compilers!
    Oh My!", YouTube video, [https://www.youtube.com/watch?v=nXaxk27zwlk](https://www.youtube.com/watch?v=nXaxk27zwlk)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tutorial, Perf Wiki, [https://perf.wiki.kernel.org/index.php/Tutorial](https://perf.wiki.kernel.org/index.php/Tutorial)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU Flame Graphs, Brendan Gregg, [http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html](http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C++ Containers Benchmark, blog post, Baptiste Wicht, [https://baptiste-wicht.com/posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html](https://baptiste-wicht.com/posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your First Coroutine, blog post, Dawid Pilarski, [https://blog.panicsoftware.com/your-first-coroutine](https://blog.panicsoftware.com/your-first-coroutine)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
