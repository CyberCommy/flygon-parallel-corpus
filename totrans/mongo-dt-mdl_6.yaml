- en: Chapter 6. Managing the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Planning a database operation is one of the most important phases of data model
    maintenance. In MongoDB, depending on the nature of the data, we can segregate
    the application's operations by functionality or by geographic groups.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will review some concepts already introduced in [Chapter
    5](ch05.html "Chapter 5. Optimizing Queries"), *Optimizing Queries*, such as read
    preferences and write concerns. But this time we will focus on understanding how
    these functionalities can help us to split the operations through MongoDB deployments,
    for instance, separating read and write operations, or ensuring information consistency
    using the write propagation through replica set nodes, considering the application's
    characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: You will also see how it is possible to have collections that support a high
    read/write throughput—which is essential for some applications—by exploring special
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, in this chapter, you will learn about:'
  prefs: []
  type: TYPE_NORMAL
- en: Operational segregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capped collections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data self-expiration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operational segregation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how our application's queries can influence, in general,
    our decisions regarding document design. However, there is more to the read preferences
    and write concern concepts than we have already explored.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB offers us a series of functionalities that allow us to segregate the
    application operations by functional or geographic groups. When using the functional
    segregation, we can direct an application responsible for report generation to
    use only a certain MongoDB deployment. The geographic segregation means that we
    can target operations considering the geographic distance from a MongoDB deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Giving priority to read operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is not that hard to imagine that once an application is built, marketing
    or commercial people will ask for a new report of the application's data, and
    by the way, this will be the essential report. We know how dangerous it can be
    to build and plug such applications in our main database just for the purpose
    of reporting. Besides the data concurrence with other applications, we know that
    this type of application can overload our database by making complex queries and
    manipulating a huge amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: This is the reason why we must target dedicated MongoDB deployments to the operations
    that handle huge volumes of data and need heavier processing from the database.
    We will make applications target the right MongoDB deployments through read preferences
    as you already saw in [Chapter 5](ch05.html "Chapter 5. Optimizing Queries"),
    *Optimizing Queries*.
  prefs: []
  type: TYPE_NORMAL
- en: By default, an application will always read the first node from our replica
    set. This behavior guarantees that the application will always read the most recent
    data, which ensures the data's consistency. Although, if the intention is to reduce
    the throughput in the first node, and we can accept eventual consistence, it is
    possible to redirect the read operations to secondary nodes from the replica set
    by enabling the `secondary` or `secondaryPreferred` mode.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the function of throughput reduction on the primary node, giving preference
    to read operations in a secondary node is crucial when we have application distributed
    in multiple datacenters and consequently, we have replica sets distributed geographically.
    This is because we make it possible to choose the nearest node or a node with
    the lowest latency to execute the read operation by setting the nearest mode.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can substantially increase the database's availability by allowing
    the read operations to be executed in any replica set node using the `primaryPreferred`
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: But what if, in addition to the read preference specification, primary or secondary,
    we could specify in which instance we will target an operation? For instance,
    think of a replica set that is distributed in two different locations and each
    instance has a different type of physical storage. In addition to this, we want
    to ensure that the write operation will be performed in at least one instance
    of each datacenter that has an **ssd** disk. Is this possible? The answer is *yes*!
  prefs: []
  type: TYPE_NORMAL
- en: This is possible due to **tag sets**. Tags sets are a configuration property
    that give you control over write concerns and read preferences for a replica set.
    They consist of a document containing zero or more tags. We will store this configuration
    in the replica set configuration document in the `members[n].tags` field.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of read preferences, the tag sets grant you target read operations
    for a specific member of a replica set. The tag sets values are applied when the
    replica set member for the read process is chosen.
  prefs: []
  type: TYPE_NORMAL
- en: The tag sets will affect only one of the read preference modes, which are `primaryPreferred`,
    `secondary`, `secondaryPreferred`, and `nearest`. The tag sets will have no effect
    on the `primary` mode, meaning that it will only impact the choice of a replica
    set secondary member, unless used in combination with the `nearest` mode, where
    the closest node or the less latency node can be the primary node.
  prefs: []
  type: TYPE_NORMAL
- en: Before we see how to do this configuration, you need to understand how the replica
    set member is chosen. The client driver that will perform the operation makes
    the choice, or in the case of a sharded cluster, the choice is done by the **mongos**
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the choice process is done in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: A list of the members, both primary and secondary, is created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If a tag set is specified, the members that do not match the specification are
    skipped.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The client that is nearest to the application is determined.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A list of the other replica set members is created considering the latency among
    the other members. This latency can be defined as soon as the write operation
    is performed through the `secondaryAcceptableLatencyMS` property. In the case
    of a sharded cluster, it is set through the `--localThreshold` or `localPingThresholdMs`
    options. If none of these configurations are set, the default value will be 15
    milliseconds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can find more about this configuration in the MongoDB manual reference at
    [http://docs.mongodb.org/manual/reference/configuration-options/#replication.localPingThresholdMs](http://docs.mongodb.org/manual/reference/configuration-options/#replication.localPingThresholdMs).
  prefs: []
  type: TYPE_NORMAL
- en: The host that will be chosen to perform the operation is randomly selected and
    the read operation is performed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The tag set configuration is as easy and simple as any other MongoDB configuration.
    As always, we use documents to create a configuration, and as stated before, the
    tag sets are a field of the replica set configuration document. This configuration
    document can be retrieved by running the `conf()` method on a replica set member.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can find out more about the `conf()` method in the MongoDB documentation
    at [http://docs.mongodb.org/manual/reference/method/rs.conf/#rs.conf](http://docs.mongodb.org/manual/reference/method/rs.conf/#rs.conf).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following document shows a tag set example for a read operation, after
    an execution of the `rs.conf()` command on the mongod shell of the `rs1`, which
    is our replica set''s primary node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a tag set configuration for each node of the replica set, we must
    do the following sequence of commands in the primary mongod shell:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will get the replica set configuration document and store it in the
    `cfg` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, by using the `cfg` variable, we will set a document as a new value to
    the `members[n].tags` field for each one of our three replica set members:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we call the `reconfig()` method, passing in our new configuration
    document stored in the `cfg` variable to reconfigure our replica set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything is correct, then we must see this output in the mongod shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To check the configuration, we can re-execute the command `rs.conf()`. This
    will return the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, consider the following `customer` collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following read operations will use the tags created in our replica set''s
    instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The preceding configuration is an example of *segregation by application operation*.
    We created tag sets, marking the application's nature and what the media type
    that will be read will be.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen before, tag sets are very useful when we need to separate our
    application geographically. Suppose that we have MongoDB applications and instances
    of our replica set in two different datacenters. Let''s create tags that will
    indicate in which datacenter our instances are present by running the following
    sequence on the replica set primary node mongod shell. First, we will get the
    replica set configuration document and store it in the `cfg` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, by using the `cfg` variable, we will set a document as a new value to
    the `members[n].tags` field, for each one of our three replica set members:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we call the `reconfig()` method, passing our new configuration document
    stored in the `cfg` variable to reconfigure our replica set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything is correct, then we will see this output in the mongod shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of our configuration can be checked by executing the command `rs.conf()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to target a read operation to a given datacenter, we must specify
    it in a new tag inside our query. The following queries will use the tags and
    each one will be executed in its own datacenter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In write operations, the tag sets are not used to choose the replica set member
    that will be available to write. Although, it is possible to use tag sets in write
    operations through the creation of custom write concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get back to the requirement raised at the beginning of this section.
    How can we ensure that a write operation will be spread over at least two instances
    of a geographic area? By running the sequence of the following commands on the
    replica set primary node mongod shell, we will configure a replica set with five
    instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The tags `riodc` and `spdc` represent which localities our instances are physically
    present in.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's create a custom `writeConcern` MultipleDC using the property `getLastErrorModes`.
    This will ensure that the write operation will be spread to at least one location
    member.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we will execute the preceding sequence, where we set a document
    representing our custom write concern on the `settings` field of our replica set
    configuration document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output in the mongod shell should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we call the `reconfig()` method, passing the new configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If the execution was successful, the output in the mongod shell is a document
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'From this moment, we can use a `writeConcern` MultipleDC in order to ensure
    that the write operation will be performed in at least one node of each data center
    shown, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Back to our requirement, if we want the write operation to be performed in
    at least two instances of each datacenter, we must configure it in the following
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'And, fulfilling our requirement, we can create a `writeConcern` MultipleDC
    called `ssd`. This will ensure that the write operation will happen in at least
    one instance that has this type of disk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following query, we see how using a `writeConcern` MultipleDC requires
    the write operation to be present in at least one instance that has `ssd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: It is not a simple task to make an operational segregation in our database.
    However, it is very useful for the database's management and maintenance. The
    early implementation of this kind of task requires a good knowledge of our data
    model, as details about the storage our database resides in are highly important.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how to plan collections for applications that
    need high throughput and fast response times.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to learn more about how to configure replica set tag sets, you can
    visit the MongoDB reference manual at [http://docs.mongodb.org/manual/tutorial/configure-replica-set-tag-sets/#replica-set-configuration-tag-sets](http://docs.mongodb.org/manual/tutorial/configure-replica-set-tag-sets/#replica-set-configuration-tag-sets).
  prefs: []
  type: TYPE_NORMAL
- en: Capped collections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Non-functional requirements are often related to the application's response
    time. Especially nowadays when we are connected to news feeds all the time and
    we want fresh information to be available in the shortest response time.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB has a special type of collection that meets this non-functional requirement,
    capped collections. Capped collections are fixed size collections that support
    high read and write throughput. This is because the documents are inserted in
    their natural order, without the need for an index to perform the write operation.
  prefs: []
  type: TYPE_NORMAL
- en: The natural insertion order is guaranteed by MongoDB, which writes the data
    like it is written on the disk. Therefore, updates that increase the document
    size are not allowed during the document's lifecycle. As soon as the collection
    reaches maximum size, MongoDB automatically cleans old documents so that new documents
    can be inserted.
  prefs: []
  type: TYPE_NORMAL
- en: One very common use case is the persistence of application logs. MongoDB itself
    uses the replica set operation log, `oplog.rs`, as a capped collection. In [Chapter
    8](ch08.html "Chapter 8. Logging and Real-time Analytics with MongoDB"), *Logging
    and Real-time Analytics with MongoDB*, you will see another practical example
    of this.
  prefs: []
  type: TYPE_NORMAL
- en: Another very common usage of MongoDB is as a publisher/subscriber system, especially
    if we use tailable cursors. Tailable cursors are cursors that remain open even
    after the client reads all the returned records. So, when new documents are inserted
    into the collection, the cursor returns it to the client.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command creates the `ordersQueue` collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We used the `util` command `createCollection` to create our capped collection,
    passing to it the name `ordersQueue` and a collection with the `capped` property
    with the value `true` and `size` with a value of `10000`. If the `size` property
    is less than 4,096, MongoDB adjusts it to 4,096 bytes. On the other hand, if it
    is greater than 4,096, MongoDB raises the size and adjusts it to be a multiple
    of 256.
  prefs: []
  type: TYPE_NORMAL
- en: 'Optionally, we can set the maximum document number that a collection can have
    by using the `max` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we need to convert a collection into a capped collection, we should use
    the `convertToCapped` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'As we have already seen, MongoDB keeps the documents in a natural order, in
    other words, the order in which they are inserted into MongoDB. Consider the following
    documents, inserted in the `ordersQueue` collection as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The query `db.ordersQueue.find()` produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If we use the `$natural` operator as shown in the following query, we will
    have the same result as shown in the preceding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'But if we need the last insertion first, we must execute the command with a
    `-1` value on the `$natural` operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We must be careful when creating a capped collection as:'
  prefs: []
  type: TYPE_NORMAL
- en: We cannot shard a capped collection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cannot update a document in a capped collection; otherwise, the document
    grows in size. If we need to update a document in a capped collection, then we
    must make sure that the size will remain the same. And for better performance,
    we should create an index to avoid a collection scan when updating.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cannot delete a document in a capped collection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capped collections are a good tool when we have a high read/write throughput
    as a non-functional requirement, or we need to limit the collection size in bytes
    or by document number.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, if we need to automatically expire data, based on a time frame,
    we should use the **time to live** (**TTL**) function.
  prefs: []
  type: TYPE_NORMAL
- en: Data self-expiration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you already saw in [Chapter 4](ch04.html "Chapter 4. Indexing"), *Indexing*,
    MongoDB offers us an index type that helps us to remove data from a collection
    after a certain amount of time in seconds, or by a specific date.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the TTL is a background thread that executes on a mongod instance that
    looks for documents with date typed fields on the index, and removes them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a `customers` collection with the following document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To expire the documents in this collection after 360 seconds, we should create
    the following index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To expire the documents at exactly 2015-01-11 20:27:02, we should create the
    following index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'When using the TTL function, we must take extra care and keep the following
    points in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: We cannot create a TTL index on a capped collection because MongoDB will not
    be able to remove documents from the collection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A TTL index cannot have a field that is part of another index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The field of the index should be a Date or array of a Date type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite having the background thread in every replica set node, which removes
    the documents when we have a TTL index, it will only remove them from the primary
    one. The replication process will delete the documents from the secondary nodes
    of the replica set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you saw that besides thinking of our schema design based on
    our queries, it is also important to think about the planning of the operation
    and maintenance for the creation of our collections.
  prefs: []
  type: TYPE_NORMAL
- en: You learned how to use tag sets to deal with datacenter-aware operations and
    why we limit the amount of documents stored in our collections by doing a capped
    collection. In the same manner, you saw how TTL indexes can be useful in real-life
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will see how we can scale our MongoDB instance by creating
    shards.
  prefs: []
  type: TYPE_NORMAL
