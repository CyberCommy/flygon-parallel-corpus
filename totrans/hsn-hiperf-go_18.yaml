- en: Comparing Code Quality Across Versions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After you've written, debugged, profiled, and monitored your Go code, you need
    to monitor your application in the long term for performance regressions. Adding
    new features to your code is useless if you can't continue to deliver a level
    of performance that other systems in your infrastructure depend on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the Go Prometheus exporter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application performance monitoring** (**APM**) tools'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service-level indicators** and **service-level objectives** (**SLIs** and
    **SLOs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding these concepts should help drive you to write performant code
    over the longer term. When working on larger-scale projects, work often doesn't
    scale well. Having 10 times the number of engineers often does not guarantee 10
    times the output. Being able to programmatically quantify code performance is
    important as software teams grow and features are added to products. Evangelizing
    performant code is something that is always taken in a positive light, and using
    some of the techniques described in this chapter will help you to improve your
    code performance in the long term, whether you're working in an enterprise setting
    or on a small open source project.
  prefs: []
  type: TYPE_NORMAL
- en: Go Prometheus exporter – exporting data from your Go application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the best ways to track long-term changes to your application is to use
    time-series data to monitor and alert us about important changes. Prometheus
  prefs: []
  type: TYPE_NORMAL
- en: '([https://prometheus.io/](https://prometheus.io/)) is a great way to perform
    this task. Prometheus is an open source time-series monitoring tool that utilizes
    a pull model via HTTP in order to drive monitoring and alerting. It is written
    in Go and has first-class client libraries for Go programs. The following steps
    show a very simple implementation of the Go Prometheus HTTP library:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we instantiate our package and import our necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in our `main` function, we instantiate a new server and have it serve
    a `NewServeMux` that returns a Prometheus handler (`promhttp.Handler()`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After we do this, we can see that we return values from the default Prometheus
    exporter. These are all well commented, and include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Go garbage collection information
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Goroutine information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go environment version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go memory statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go CPU utilization statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HTTP handler statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We next build the binary for our Go service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We next create a docker network for linking our services together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We then follow this by creating our Prometheus exporter service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we run our Prometheus exporter service on our Docker host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, we can see a truncated output of this response.
    Excluded are the comments and the built-in Go statistics for brevity. You can
    see the key–value responses in the response from the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/acfa975f-460e-4fe2-8bc0-63a759934df5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After we have set up this server, we can monitor it at a given cadence. We
    can run both our metrics service and Prometheus in containers and let them talk
    to each other. We can use a simple `prometheus.yml` definition for our Prometheus
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a9cdc6c-b28c-4191-996e-3a8e0d80a7e0.png)'
  prefs: []
  type: TYPE_IMG
- en: You can replace `promExporter` within the `scrape_configs`->`static_configs`->`targets`
    portion of the YAML with an IP address or hostname if you'd like to use something
    besides your docker host for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have our binary build, we can create two separate Dockerfiles: one
    for the container that will contain our Prometheus exporter service and one that
    will contain our Prometheus service. Our Dockerfile for our Prometheus service
    takes the baseline Prometheus image and adds our YAML configuration to the appropriate
    place in our image. Our `Dockerfile.promservice` configuration is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have our `Dockerfile.promservice` created, we can build our Prometheus
    service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then run our Prometheus service on our Docker host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now we have a Prometheus instance running on our local environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have our Prometheus service up and running, we can go to `http://[IPADDRESS]:9090/`
    and we''ll see our Prometheus instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a63a59c1-4eee-478d-aa04-d6950f0f1dc1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can validate that we are scraping our target by looking at the `/targets`
    path in the same URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0135088e-d9d5-45b6-82cf-14fefaa31be6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we can make a couple of requests to our host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can see the results of our `curl` in our Prometheus instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/206eb2a0-6f9f-48b1-a63a-2cef366a27f1.png)'
  prefs: []
  type: TYPE_IMG
- en: With these results, we can see the total number HTTP responses that we have
    served with a 200, 500, and 503 status code. Our example is simple, but we can
    use many different types of metrics here in order to validate any assumptions
    that we have. We will do a more involved metric-gathering example in our SLI/SLO
    example later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to talk about APM and how it can be used in
    maintaining a performant distributed system.
  prefs: []
  type: TYPE_NORMAL
- en: APM – watching your distributed system performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many APM tools on the market today. They are frequently used to monitor
    the performance and reliability of software over time. Some of the products available
    for Go at the time of writing this book are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Elastic APM agent**: [https://www.elastic.co/guide/en/apm/agent/go/current/index.html](https://www.elastic.co/guide/en/apm/agent/go/current/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**New Relic APM**: [https://newrelic.com/golang](https://newrelic.com/golang)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Datadog**: [https://docs.datadoghq.com/tracing/setup/go/](https://docs.datadoghq.com/tracing/setup/go/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SignalFX**: [https://docs.signalfx.com/en/latest/apm/apm-instrument/apm-go.html](https://docs.signalfx.com/en/latest/apm/apm-instrument/apm-go.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AppDynamics** : [https://www.appdynamics.com/supported-technologies/go](https://www.appdynamics.com/supported-technologies/go)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Honeycomb APM**: [https://docs.honeycomb.io/getting-data-in/go/](https://docs.honeycomb.io/getting-data-in/go/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS XRay**: [https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-go.html](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-go.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google''s suite of APM products**: [https://cloud.google.com/apm/](https://cloud.google.com/apm/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of these tools are closed source and paid services. Aggregating distributed
    traces is a difficult value proposition. The vendors listed here (as well as some
    that are not mentioned) combine data storage, aggregation, and analysis in order
    to have a one-stop shop for APM. We can also use the OpenCensus/Zipkin open source
    example that we created in [Chapter 13](ec12b9e7-c528-45c2-b0b8-dea297659b3e.xhtml), *Tracing
    Go Code*, to perform distributed tracing in our system. Implementing spans around
    particular bits of our code base can help us to monitor long-term application
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at an example of Google's APM solutions. At the time of writing,
    Google Cloud offers 2.5 million span ingestions and 25 million span scans per
    month, which is more than adequate for an example.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud environment setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing we need to do is to create a GCP project and retrieve the application
    credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll log into [https://console.cloud.google.com/](https://console.cloud.google.com/).
    Once logged in, we can hit the project selector dropdown at the top of the page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ecb57374-6123-4223-9163-a94127915405.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then create a new project for our particular application at the top
    right of the screen, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ca13a97e-ed0e-4c20-88f2-d249eb7fcb3e.png)'
  prefs: []
  type: TYPE_IMG
- en: We can then visit the service account key page at [https://console.cloud.google.com/apis/credentials/serviceaccountkey](https://console.cloud.google.com/apis/credentials/serviceaccountkey),
    which will let us create a service account key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can create a service account key for our application. Make sure you select
    the Cloud Trace Agent, as this is necessary for us to add traces to Google Cloud
    Trace. This is depicted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ced82271-b1aa-421a-83c2-fe958418d501.png)'
  prefs: []
  type: TYPE_IMG
- en: After we click Create, the browser will prompt us to download our new credentials.
    For reference, we will call this key `high-performance-in-go-tracing.json`. You
    can name your key whatever you like.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we have this key saved locally, we can turn it into an environment variable.
    In your Terminal, enter the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This will save your service account credentials as a special environment variable,
    `GOOGLE_APPLICATION_CREDENTIALS`, which we will use in our next example.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Trace code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have our application credentials all set up; we are off to the races
    to write our first trace that will be captured by our APM:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we instantiate the necessary packages and set a server host/port constant:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, in our `init()` function, we set up our StackDriver exporter and register
    our tracer to sample every web request that comes in. In production, we should
    probably sample fewer requests, as sampling adds additional latency to our requests:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we are going to have a sleep function that takes a context, sleeps, and
    writes a message to the end user. In this function, I am deferring the end of
    the span to the end of the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Our GitHub request function makes a request to [https://github.com](https://github.com)
    and returns the status to our end user. In this function, I''m explicitly calling
    the end of the span:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Our main function sets up an HTTP handler function that performs the `githubRequest`
    and `sleep` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After we execute our main function, we make a request to `localhost:1234` and
    see a response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/46cc3921-ed8c-4c9f-8dbc-9428236fd65d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After this, we visit the Google Cloud console and select the trace that we
    made:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/053fbe63-4e43-4fb1-8723-5f9cb5640f14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this trace example, we can see all sorts of relevant details:'
  prefs: []
  type: TYPE_NORMAL
- en: All of the trace samples that have been taken (I added a bunch of different
    samples here to populate the fields).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our waterfall graph of the request flow. This is a bit small for our example
    with just the web request and the sleep, but as we pass context around in a distributed
    system, this graph quickly gets much bigger.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The summary for each trace. If you click on one of the tracing bars in the graph,
    you can see more details about the particular trace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adding distributed tracing as an APM solution can be extremely helpful in determining
    the location of the web request that takes the most time. Finding real-life bottlenecks
    can often be much more practical than diving through logs. Google''s APM also
    gives you the ability to run reports based on the traces you''ve made. After you
    have made more than 100 requests, you can execute an analysis report and see the
    results. The density distribution latency chart shows you where your request latencies
    lie in a graph. Our example should have mostly similar results, as we performed
    a long sleep and made one solitary request to an external service. We can see
    the density distribution graph in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fffa4640-94a6-4dd9-9539-07377ae888fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also look at the cumulative latency in this portal, which will show
    us the percentage of requests that are shorter than the value on the *x* axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73ccd8c4-1aef-4706-b576-9a8e5eae5700.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also see latency profiles with correlated requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3204ba3b-bc64-4f0e-8b5e-2c19fa41d0af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Furthermore, we can also see the perceived bottlenecks within the distributed
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/586c57f2-b4e7-4790-84d2-9ff3b0245fbe.png)'
  prefs: []
  type: TYPE_IMG
- en: These analysis tools help us to deduce where we can make improvements in our
    distributed system. APMs help many companies to deliver performant applications
    to their customers. These tools are extra valuable because they look at performance
    through the lens of customer experience. In the next section, we will talk about
    setting goals with SLIs and SLOs.
  prefs: []
  type: TYPE_NORMAL
- en: SLIs and SLOs – setting goals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SLIs and SLOs are two paradigms that were brought to the computer science world
    by Google. They are defined in the SRE workbook
  prefs: []
  type: TYPE_NORMAL
- en: '([https://landing.google.com/sre/sre-book/chapters/service-level-objectives/](https://landing.google.com/sre/sre-book/chapters/service-level-objectives/))
    and are an excellent way to measure actionable items within your computing system.
    These measurements normally follow Google''s four golden signals:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency**: The amount of time a request takes to complete (usually measured
    in milliseconds)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traffic**: The volume of traffic that your service is receiving (usually
    measured in requests per second)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Errors**: The percentage of failed requests over total requests (usually
    measured with a  percentage)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Saturation**: The measure of hardware saturation (usually measured by queued
    request counts)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These measurements can then be used to create one or more SLAs. These are frequently
    delivered to customers who expect a specific level of service from your application.
  prefs: []
  type: TYPE_NORMAL
- en: We can use Prometheus to measure these metrics. Prometheus has a bunch of different
    methodologies for counting things, including gauges, counters, and histograms.
    We will use all of these different tools to measure these metrics within our system.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test our system, we''ll use the `hey` load generator. This is a tool that
    is similar to `ab`, which we used in previous chapters, but it''ll show our distribution
    a little better for this particular scenario. We can grab it by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to need to stand up our Prometheus service in order to read some
    of these values. If yours isn''t still standing from our previous example, we
    can perform the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will get our Prometheus instance to stand up and measure requests:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our code starts by instantiating the `main` package and importing the necessary
    Prometheus packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We then gather our saturation, requests, and latency numbers in our `main`
    function. We use a gauge for saturation, a counter for requests, and a histogram
    for latency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We then create our `goldenSignalHandler`, which randomly generates a latency
    from 0 to 1 seconds. For added visibility of our signals, if the random number
    is divisible by 4, we return a 404 error status, and if it's divisible by 5, we
    return a 500 error. We then return a response and log that the request has been
    completed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our `goldenSignalChain` ties these metrics together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We then register all of our measurements (saturation, requests, and latency)
    with Prometheus, handle our HTTP requests, and start our HTTP server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After we start our HTTP server by executing `go run SLISLO.go`, we can then
    make a `hey` request to our HTTP server. The output from our `hey` call is visible
    in the following screenshot. Remember that these are all random values and will
    be different if you execute this same test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2dbed159-6c95-40f8-8952-de203f66852e.png)'
  prefs: []
  type: TYPE_IMG
- en: We can then take a look at our individual golden signals.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring traffic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To measure our traffic, we can use the Prometheus query `sum(rate(requests[1m]))`.
  prefs: []
  type: TYPE_NORMAL
- en: We can measure the rate at any given interval. Configure this rate in a couple
    of different ways and see which is most conducive to your system's requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring latency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To measure latency, we can look at the `latency_bucket` Prometheus query. Our
    requests were lumped into a histogram with different latency numbers, and this
    query reflects that.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To measure the number of errors in our system, we need to find the ratio of
    requests that had a successful response code to those that did not have a successful
    response code. We can find this with the following query `sum(requests {code!="200"})
    / (sum(requests {code="200"})) + sum(requests {code!="200"})`.
  prefs: []
  type: TYPE_NORMAL
- en: This ratio is important to monitor. Computer systems fail and people make incorrect
    requests, but your ratio of 200 responses to non-200 responses should be relatively
    small.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring saturation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can measure saturation using the `saturation` **Prometheus** query. We want
    to validate that our system isn't saturated, and this query can help us to perform
    this action.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can encapsulate all of these golden signals into a Grafana dashboard. We
    can run Grafana locally by invoking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to log into the Grafana portal by visiting `http://localhost:3000` and
    using the default username and password combination:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Username**: admin'
  prefs: []
  type: TYPE_NORMAL
- en: '**Password**: admin'
  prefs: []
  type: TYPE_NORMAL
- en: We can then set up a new password to our liking after we've logged in.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we''ve logged in, we click Add data source at the top of the page and
    select Prometheus on the next page. We then enter our local IP address and click
    Save & Test. If all works as expected, we should see the Data source is working
    popup at the bottom of the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7de906b-9d13-4a64-8582-231fed7be4cc.png)'
  prefs: []
  type: TYPE_IMG
- en: After we complete this, we visit [http://localhost:3000/dashboard/import](http://localhost:3000/dashboard/import).
  prefs: []
  type: TYPE_NORMAL
- en: We then choose Upload .json file in the top-right corner, and we upload the
    JSON file that has been created for this dashboard at [https://github.com/bobstrecansky/HighPerformanceWithGo/blob/master/15-code-quality-across-versions/SLISLO/four_golden_signals_grafana_dashboard.json](https://github.com/bobstrecansky/HighPerformanceWithGo/blob/master/15-code-quality-across-versions/SLISLO/four_golden_signals_grafana_dashboard.json).
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have uploaded this JSON file, we import this data source, and we''ll
    be able to see our Request Rate, Duration Latency Buckets, Error Rate, and Saturation
    charts, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89634aef-43de-4594-bf92-fa0ed2edecd9.png)'
  prefs: []
  type: TYPE_IMG
- en: Having visibility into these statistics can be helpful in maintaining a stable
    system. After you have these statistics captured, you can use Prometheus Alertmanager
    to set alerts on thresholds you're interested in monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: More information on configuring Alertmanager can be found at
  prefs: []
  type: TYPE_NORMAL
- en: '[https://prometheus.io/docs/alerting/alertmanager/](https://prometheus.io/docs/alerting/alertmanager/).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to keep track of our data, also known
    as logging.
  prefs: []
  type: TYPE_NORMAL
- en: Logging – keeping track of your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logging, the act of recording events that occur in a system, is essential to
    creating performant software systems. Being able to record and validate events
    within a programming system is a great way to ensure that you're maintaining code
    quality across versions of your applications. Logs can often quickly show a bug
    in your software, and being able to consume that information in a quick fashion
    can often help lower your **mean time to recovery** (**MTTR**).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many different logging packages for Go. A few of the most popular
    packages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The standard built-in log package provided by the Go maintainers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Glog package**: [https://github.com/golang/glog](https://github.com/golang/glog)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uber''s Zap package**: [https://github.com/uber-go/zap](https://github.com/uber-go/zap)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Zero Allocation JSON logger**: [https://github.com/rs/zerolog](https://github.com/rs/zerolog)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Logrus package**: [https://github.com/sirupsen/logrus](https://github.com/sirupsen/logrus)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are going to use the Zap package as our example, as benchmarks have shown.
    Using the standard library logger is often sufficient (if you''ve noticed, this
    is the package that I''ve used for logging in the book thus far). Having a structured
    logging package such as Zap available can make up for a pleasant experience because
    it offers a couple of features that the standard library logger doesn''t offer
    out of the box, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Logging levels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured logs (JSON in particular)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typed logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It also performs best in comparison benchmarks among the loggers. Zap has two
    different types of logging available, the sugared logger and the structured logger.
    The structured logger is slightly more performant, whereas the sugared logger
    is more loosely typed. As this is a book about performance, we are going to take
    a look at the structured logger as it's more performant, but both logging options
    are more than adequate for production use.
  prefs: []
  type: TYPE_NORMAL
- en: Having a logger that has different logging levels is important because it allows
    you to determine which logs need dire attention and which logs are just returning
    information. This also lets you set a priority for your team depending on how
    urgent a fix is when you hit a logging inflection point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having logs that can be structured is helpful for ingestion into other systems.
    JSON logging is quickly becoming more and more popular because log aggregation
    tools such as the following accept JSON logging:'
  prefs: []
  type: TYPE_NORMAL
- en: The ELK Stack (ElasticSearch, Logstash, and Kibana)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loggly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splunk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sumologic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datadog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Stackdriver Logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we saw with our APM solutions, we can utilize these logging services to aggregate
    large groupings of logs in a centralized location, whether it be on premise or
    in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Having typed logs allows you to organize your logging data in a way that makes
    sense to your program or business. Maintaining consistency in your logging can
    allow for your system operators and site reliability engineers to more quickly
    diagnose problems, resulting in a shorter MTTR for production incidents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at a logging example with Zap:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we instantiate our package and import the `time` package and the Zap
    logger:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We then set up a logging production configuration that will return logs to
    `stdout` (following the twelve-factor app process). These can often go to log
    routers such as Fluentd ([https://www.fluentd.org/](https://www.fluentd.org/)),
    and we can test all of the different log levels available in Zap:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After we run our logger, we can see some pretty clean JSON output. We can also
    use a utility such as jq ([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/))
    in order to make this easily consumable in your local environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec3d0e6f-18db-4a17-8be5-8e0916416543.png)'
  prefs: []
  type: TYPE_IMG
- en: As we mentioned, having a structured, leveled logger in your Go application
    will help you to troubleshoot more quickly and effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed the different methodologies of comparing code
    quality across versions:'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the Go Prometheus exporter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: APM tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SLIs and SLOs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing all of these techniques can help you to determine where your application
    isn't performing as it is expected to. Knowing these things can help you to iterate
    rapidly and produce the best software that you can.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the course of this book, you've learned about application performance
    and how it pertains to Go. I hope this book will help you to think about web performance
    while you are writing applications. Always keep performance in the forefront of
    your mind. Everyone enjoys performant applications, and hopefully this book will
    help you do your part as a developer in making them.
  prefs: []
  type: TYPE_NORMAL
