- en: Log-Based Artifact Recipes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following recipes are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: About time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing IIS weblogs with RegEx
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going spelunking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpreting the daily out log
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding `daily.out` parsing to Axiom
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scanning for indicators with YARA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These days it is not uncommon to encounter modern systems equipped with some
    form of event or activity monitoring software. This software may be implemented
    to assist with security, debugging, or compliance requirements. Whatever the situation,
    this veritable treasure trove of information can be, and commonly is, leveraged
    in all types of cyber investigations. A common issue with log analysis can be
    the huge amount of data one is required to sift through for the subset of interest.
    Through the recipes in this chapter, we will explore various logs with great evidentiary
    value and demonstrate ways to quickly process and review them. Specifically, we
    will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Converting different timestamp formats (UNIX, FILETIME, and so on) to human-readable
    formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing web server access logs from an IIS platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingesting, querying, and exporting logs with Splunk's Python API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting drive usage information from macOS `daily.out` logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing our `daily.out` log parser from Axiom
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bonus recipe for identifying files of interest with YARA rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visit [www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)
    to download the code bundle for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: About time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Easy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7 or 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: One important element of any good log file is the timestamp. This value conveys
    the date and time of the activity or event noted in the log. These date values
    can come in many formats and may be represented as numbers or hexadecimal values.
    Outside of logs, different files and artifacts store dates in different manners,
    even if the data type remains the same. A common differentiating factor is the
    epoch value, which is the date that the format counts time from. A common epoch
    is January 1, 1970, though other formats count from January 1, 1601\. Another
    factor that differs between formats is the interval used for counting. While it
    is common to see formats that count seconds or milliseconds, some formats count
    blocks of time, such as the number of 100-nanoseconds since the epoch. Because
    of this, the recipe developed here can take the raw datetime input and provide
    a formatted timestamp as its output.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All libraries used in this script are present in Python's standard library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To interpret common date formats in Python, we perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up arguments to take the raw date value, the source of the date, and the
    data type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Develop a class that provides a common interface for data across different date
    formats.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Support processing of Unix epoch values and Microsoft `FILETIME` dates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin by importing libraries for argument handling and parsing dates. Specifically,
    we need the `datetime` class from the `datetime` library to read the raw date
    values and the `timedelta` class to specify timestamp offsets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler takes three positional arguments, `date_value`,
    `source`, and `type`, which represent the date value to process, the source of
    date value (UNIX, FILETIME, and so on), and the type (integer or hexadecimal value),
    respectively. We use the choices keyword for the source and type arguments to
    limit the options the user can supply. Notice, that the source argument uses a
    custom `get_supported_formats()` function rather than a predefined list of supported
    date formats. We then take these arguments and initiate an instance of the `ParseDate`
    class and call the `run()` method to handle the conversion process before printing
    its `timestamp` attribute to the console.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at how the `ParseDate` class works. By using a class, we can easily
    extend and implement this code in other scripts. From the command-line arguments,
    we accept arguments for the date value, date source, and the value type. These
    values and the output variable, `timestamp`, are defined in the `__init__` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `run()` method is the controller, much like the `main()` function of many
    of our recipes, and selects the correct method to call based on the date source.
    This allows us to easily extend the class and add new support with ease. In this
    version, we only support three date types: Unix epoch second, Unix epoch millisecond,
    and Microsoft''s FILETIME. To reduce the number of methods we would need to write,
    we will design the Unix epoch method to handle both second - and millisecond -
    formatted timestamps.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To help those wanting to use this library in the future, we add a method for
    viewing what formats are supported. By using the `@classmethod` decorator, we
    expose this function without needing to initialize the class first. This is the
    reason we can use the `get_supported_formats()` method in the command-line handler.
    Just remember to update this as new features are added!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `parse_unix_epoch()` method handles processing Unix epoch time. We specify
    an optional argument, `milliseconds`, to switch this method between processing
    second and millisecond values. First we must determine if the data type is `"hex"`
    or `"number"`. If it is `"hex"`, we convert it to an integer and if it is a `"number"`
    we convert it to a float. If we do not recognize or support the data type for
    this method, such as a `string`, we throw an error to the user and exit the script.
  prefs: []
  type: TYPE_NORMAL
- en: After converting the value, we evaluate if this should be treated as a millisecond
    value and, if so, divide it by `1,000` before handling it further. Following this,
    we use the `fromtimestamp()` method of the `datetime` class to convert the number
    to a `datetime` object. Lastly, we format this date to a human-readable format
    and store this string in the `timestamp` property.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `parse_windows_filetime()` class method handles the `FILETIME` format, commonly
    stored as a hex value. Using a similar block of code as before, we convert the
    `"hex"` or `"number"` value into a Python object and raise an error for any other
    provided formats. The one difference is that we divide the date value by `10`
    rather than `1,000` before processing them further.
  prefs: []
  type: TYPE_NORMAL
- en: 'While in the previous method the `datetime` library handled the epoch offset,
    we need to handle this offset separately this time. Using the `timedelta` class,
    we specify the millisecond value and add that to a `datetime` object representing
    the FILETIME format''s epoch. The resulting `datetime` object is now ready for
    us to format and output for the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this script, we can provide a timestamp and see the converted value
    in an easy-to-read format, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00076.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided one or more recommendations
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Add support for other types of timestamps (OLE, WebKit, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add time zone support through `pytz`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handle the formatting of hard-to-read dates with `dateutil`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing IIS web logs with RegEx
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: Logs from web servers are very useful for generating user statistics, providing
    us with insightful information about the devices used and the geographical locations
    of the visitors. They also provide clarification to examiners looking for users
    attempting to exploit the web server or otherwise unauthorized use. While these
    logs store important details, they do so in a manner inconvenient to analyze efficiently.
    If you were to attempt to do so manually, the field names are specified at the
    top of the file and would require you to remember the order of the fields as you
    read through the text file. Fortunately, there is a better way. Using the following
    script, we show how to iterate through each line, map the values to the fields,
    and create a spreadsheet of properly displayed results - making it much easier
    to quickly analyze the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All libraries used in this script are present in Python's standard library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To properly form this recipe, we need to take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Accept arguments for an input log file and output CSV file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define regular expression patterns for each of the log's columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate through each line in the log and prepare each line in a manner that
    we can parse individual elements and handle quoted space characters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Validate and map each value to its respective column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write mapped columns and values to a spreadsheet report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin with by importing libraries for argument handling and logging, followed
    by the built-in libraries we need to parse and validate the log information. These
    include the `re` regular expression library and `shlex` lexical analyzer library.
    We also include `sys` and `csv` for handling the output of log messages and reports.
    We initialize the recipe's logging object by calling the `getLogger()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Following the imports, we define patterns for the fields we will parse from
    the logs. This information may vary a bit between logs, though the patterns expressed
    here should cover most elements in a log.
  prefs: []
  type: TYPE_NORMAL
- en: You may need to add, remove, or reorder some of the patterns defined as follows
    to properly parse the IIS log you are working with. These patterns should cover
    the common elements found in IIS logs.
  prefs: []
  type: TYPE_NORMAL
- en: We build these patterns as a list of tuples called `iis_log_format`, where the
    first tuple element is the column name and the second is the regular expression
    pattern to validate the expected content. By using a regular expression pattern,
    we can define a set of rules that the data must follow in order to be valid. It
    is critical that these columns are expressed in the order they appear in the log;
    otherwise, the code won't be able to properly map values to columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler takes two positional arguments, `iis_log`
    and `csv_report`, which represent the IIS log to process and the desired CSV path,
    respectively. Additionally, this recipe also accepts an optional argument, `l`,
    specifying the output path for the recipe's log file.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we initialize the recipe's logging utility and configure it for console
    and file-based logging. This is important as we should note in a formal manner
    when we fail to parse a line for the user. In this manner, if something fails
    they shouldn't be working under the mistaken assumption that all lines were parsed
    successfully and are displayed in the resulting CSV spreadsheet. We also want
    to record runtime messages, including the version of the script and the supplied
    arguments. At this point, we are ready to call the `main()` function and kick
    off the script. Refer to the logging recipe in [Chapter 1](part0029.html#RL0A0-260f9401d2714cb9ab693c4692308abe),
    *Essential Scripting and File Information Recipes* for a more detailed explanation
    of setting up a logging object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `main()` function handles the bulk of the logic in this script. We create
    a list, `parsed_logs`, to store the parsed lines before iterating over the lines
    in the log file. Inside the `for` loop, we strip the line and create a storage
    dictionary, `log_entry`, for the record. We speed up our processing, and prevent
    errors in column matching, by skipping lines beginning with the comment (or pound)
    character or if the line is empty.
  prefs: []
  type: TYPE_NORMAL
- en: While IIS logs are stored as space-delimited values, they use double quotes
    to escape strings that contain spaces. For example, a `useragent` string is a
    single value but generally, contains one or more spaces. Using the `shlex` module,
    we can parse the line with the `shlex()` method, and handle quote escaped spaces
    automatically by delimiting the data correctly on space values. This library can
    slow down processing, so we only use it on lines containing a double-quote character.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: With the line properly delimited, we use the `enumerate` function to step through
    each element in the record and extract the corresponding column name and pattern.
    Using the pattern, we call the `match()` method on the value and, if it matches,
    create an entry in the `log_entry` dictionary. If the value doesn't match the
    pattern, we log an error and provide the whole line in the log file. After iterating
    through each of the columns, we append the record dictionary to the initial list
    of parsed log records and repeat this process for the remaining lines.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Once all lines have been processed, we print a status message to the console
    prior to preparing for the `write_csv()` method. We use a simple list comprehension
    expression to extract the first element of each tuple, which represents a column
    name, within the `iis_log_format` list. With the columns extracted, let's look
    at the report writer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The report writer creates a CSV file using the methods we have previously explored.
    Since we stored the lines as a list of dictionaries, we can easily create the
    report with four lines of code using the `csv.DictWriter` class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'When we look at the CSV report generated by the script, we see the following
    fields in the sample output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00077.jpeg)![](../images/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. Here is a recommendation:'
  prefs: []
  type: TYPE_NORMAL
- en: While we can define regex patterns as seen at the start of the script, we can
    make our lives easier using regular expression management libraries instead. One
    example is the `grok` library, which is used to create variable names for patterns.
    This allows us to organize and extend patterns with ease, as we can express them
    by name instead of a string value. This library is used by other platforms, such
    as the ELK stack, for management and implementation of regular expressions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going spelunking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: Log files can quickly become quite sizable due to the level of detail and time
    frame preserved. As you may have noticed, the CSV report from the prior recipe
    can easily become too large for our spreadsheet application to open or browse
    efficiently. Rather than analyzing this data in a spreadsheet, one alternative
    would be to load the data into a database.
  prefs: []
  type: TYPE_NORMAL
- en: '**Splunk** is a platform that incorporates a NoSQL database with an ingestion
    and query engine, making it a powerful analysis tool. Its database operates in
    a manner like Elasticsearch or MongoDB, permitting the storage of documents or
    structured records. Because of this, we do not need to provide records with a
    consistent key-value mapping to store them in the database. This is what makes
    NoSQL databases so useful for log analysis, as log formats can be variable depending
    on the event type.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we learn to index the CSV report from the previous recipe into
    Splunk, allowing us to interact with the data inside the platform. We also design
    the script to run queries against the dataset and to export the resulting subset
    of data responsive to the query to a CSV file. These processes are handled in
    separate stages so we can independently query and export data as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe requires the installation of the third-party library `splunk-sdk`.
    All other libraries used in this script are present in Python's standard library.
    Additionally, we must install Splunk on the host operating system and, due to
    limitations of the `splunk-sdk` library, run the script using Python 2.
  prefs: []
  type: TYPE_NORMAL
- en: To install Splunk, we need to navigate to [Splunk.com](https://www.splunk.com/),
    fill out the form, and select the Splunk Enterprise free trial download. This
    enterprise trial allows us to practice with the API and gives us the ability to
    upload 500 MB per day. Once we have downloaded the application, we need to launch
    it to configure the application. While there are a lot of configurations we could
    change, launch it with the defaults, for now, to keep things simple and focus
    on the API. In doing so, the default address for the server will be `localhost:8000`.
    By navigating to this address in a browser, we can log in for the first time,
    set up accounts and (*please do this*) change the administrator password.
  prefs: []
  type: TYPE_NORMAL
- en: The default username and password for a new Splunk install is *admin* and *changeme*.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the Splunk instance active, we can now install the API library. This library
    handles the conversion from the REST API into Python objects. At the time of writing
    of this book, the Splunk API is only available in Python 2\. The `splunk-sdk`
    library can be installed with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the `splunk-sdk` library, visit [http://dev.splunk.com/python](http://dev.splunk.com/python).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that the environment is properly configured, we can begin to develop the
    code. This script will index new data to Splunk, run queries on that data, and
    export subsets of data responsive to our queries to a CSV file. To accomplish
    this, we need to:'
  prefs: []
  type: TYPE_NORMAL
- en: Develop a robust argument-handling interface allowing the user to specify these
    options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a class to handle operations with the various properties' methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create methods to handle the process of indexing new data and creating the index
    for data storage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up methods for running Splunk queries in a manner that allows for informative
    reports.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide a mechanism for exporting reports to a CSV format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin by importing the required libraries for this script, including the
    newly installed `splunklib`. To prevent unnecessary errors arising due to user
    ignorance, we use the `sys` library to determine the version of Python executing
    the script and raise an error if it is not Python 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The next logical block to develop is the recipe's command-line argument handler.
    As we have many options and operations to execute in this code, we need to spend
    some extra time on this section. And because this code is class based, we must
    set up some additional logic in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'This recipe''s command-line handler takes one positional input, `action`, which
    represents the action to run (index, query, or export). This recipe also supports
    seven optional arguments: `index`, `config`, `file`, `query`, `cols`, `host`,
    and `port`. Let''s start looking at what all of these options do.'
  prefs: []
  type: TYPE_NORMAL
- en: The `index` argument, which is actually a required argument, is used to specify
    the name of the Splunk index to ingest, query, or export the data from. This can
    be an existing or new `index` name. The `config` parameter refers to the configuration
    file containing your username and password for the Splunk instance. This file,
    as described in the argument's help, should be protected and stored outside of
    the location where the code is executed. In an enterprise environment, you may
    need to further protect these credentials.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `file` parameter will be used to provide a path to the `file` to `index`
    into the platform or be used to specify the filename to write the exported `query`
    data to. For example, we will use the `file` parameter to point to the CSV spreadsheet
    we wish to ingest from the previous recipe. The `query` parameter also serves
    a dual purpose, it can be used to run a query from Splunk or to specify a query
    ID to export as CSV. This means that the `index` and `query` actions require only
    one of these parameters, but the `export` action requires both.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The last block of arguments allows the user to modify default properties of
    the recipe. The `cols` argument, for example, can be used to specify what columns
    from the source data to export and in what order. As we will be querying and exporting
    IIS logs, we already know what columns are available and are of interest to us.
    You may want to specify alternative default columns based on what type of data
    is being explored. Our last two arguments include the `host` and `port` parameters,
    each defaulting to a local server but can be configured to allow you to interact
    with alternate instances.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: With our arguments specified, we can parse them and verify that all requirements
    are met prior to executing the recipe. First, we must open and read the `config`
    file containing the authentication credentials, where the `username` is on the
    first line and the `password` is on the second line. Using this information, we
    create a dictionary, `conn_dict`, containing the login details and server location.
    This dictionary is passed to the `splunklib` `client.connect()` method. Notice
    how we delete, using the `del()` method, the variables containing this sensitive
    information. While the username and password are still accessible through the
    `service` object, we want to limit the number of areas in which those details
    are stored. Following the creation of the `service` variable, we test if any applications
    are installed in Splunk, as by default there is at least one, and use that as
    a test of successful authentication.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We continue processing the supplied arguments by converting the columns into
    a list and creating the `Spelunking` class instance. To initialize the class,
    we must supply it the `service` variable, the action to take, the index name,
    and the columns. Using this, our class instance is now ready for use.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Next, we use a series of `if-elif-else` statements to handle the three various
    actions we expect to encounter. If the user supplied the `index` action, we first
    confirm that the optional `file` parameter is present, raising an error if it
    is not. If we do find it, we assign the value to the corresponding property of
    the `Spelunking` class instance. This type of logic is repeated for the `query`
    and `export` actions, confirming that they also were used with the correct optional
    arguments. Notice how we assign the absolute path of the file for the class using
    the `os.path.abspath()` function. This allows `splunklib` to find the correct
    file on the system. And, in what perhaps may be the longest argument handling
    section in the book, we have completed the requisite logic and can now call the
    class `run()` method to kick off the processing for the specific action requested.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: With the arguments now behind us, let's dive into the class responsible for
    handling the operations requested by the user. This class takes four arguments,
    including the `service` variable, the `action` specified by the user, the Splunk
    index name, and the columns to use. All other properties are set to `None` and,
    as seen in the previous code block, will be appropriately initialized at the time
    of execution if they were supplied. This is done to limit the number of arguments
    required by the class and to handle the situations where certain properties are
    unused. All of these properties are initialized at the start of our class to ensure
    we have assigned default values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `run()` method is responsible for obtaining the `index` object from the
    Splunk instance using the `get_or_create_index()` method. It also checks which
    action was specified at the command-line and calls the corresponding class instance
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `get_or_create_index()` method, as the name suggests, first tests whether
    the specified index exists and makes a connection to it, or creates a new index
    if none is found by that name. Since this information is stored in the `indexes`
    property of the `service` variable as a dictionary-like object, we can easily
    test for the existence of the index by name.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: To ingest the data from a file, such as a CSV file, we can use a one-line statement
    to send information to the instance in the `index_data()` method. This method
    uses the `upload()` method of the `splunk_index` object to send the file to Splunk
    for ingestion. While a CSV file is a simplistic example of how we can import data,
    we could also use some of the logic from the previous recipe to read the raw log
    into the Splunk instance without the intermediate CSV step. For that, we would
    want to use a different method of the `index` object that would allow us to send
    each parsed event individually.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `query_index()` method is a little more involved, as we first need to modify
    the query provided from the user. As seen in the following snippet, we need to
    add the columns specified by the user to the initial query. This will make fields
    not used in the query available during the export stage. Following this modification,
    we create a new job in the Splunk system with the `service.jobs.create()` method
    and record the query SID. This SID will be used in the exporting phase to export
    the results of the specific query job. We print this information, along with the
    time before the job expires from the Splunk instance. By default, this time-to-live
    value is `300` seconds, or five minutes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As previously alluded to, the `export_report()` method uses the SID mentioned
    in the prior method to check whether the job is complete and to retrieve the data
    for export. in order to do this, we iterate through the available jobs, and if
    ours is not present, raise a warning. If the job is found, but the `is_ready()`
    method returns `False`, the job is still processing and not ready to export results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: If the job passes these two tests, we extract the data from Splunk and write
    it to a CSV file using the `write_csv()` method. Before we can do that, we need
    to initialize a list to store the job results. Next, we retrieve the results,
    specifying the columns of interest, and read this raw data into the `job_results`
    variable. Luckily, `splunklib` provides a `ResultsReader` that converts the `job_results`
    variable into a list of dictionaries. We iterate through this list and append
    each of these dictionaries to the `export_data` list. Finally, we provide the
    file path, column names, and dataset to export to the CSV writer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `write_csv()` method in this class is a `@staticmethod`. This decorator
    allows us to use a generalized method in the class without needing to specify
    an instance. This method will no doubt look familiar to those used elsewhere in
    the book, where we open the output file, create a `DictWriter` object, then write
    the column headers and data to file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In our hypothetical use case, the first stage will be to index the data contained
    in the CSV spreadsheet from the previous recipe. As seen in the following snippet,
    we supply the CSV file from our previous recipe and add it to the Splunk index.
    Next, we look for all entries where the user agent is an iPhone. Finally, the
    last stage involves taking the output from the query and creating a CSV report.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00079.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'With these three commands successfully executed, we can open and review the
    filtered output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00080.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided one or more recommendations
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: The Splunk API for Python (and in general) has many other features. Additionally,
    more advanced querying techniques can be used to generate data that we can manipulate
    into graphics for technical and non-technical end users alike. Learn more about
    the many features that the Splunk API affords you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpreting the daily.out log
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: Operating system logs generally reflect events for software, hardware, and services
    on the system. These details can assist us in our investigations as we look into
    an event, such as the use of removable devices. One example of a log that can
    prove useful in identifying this activity is `daily.out` log found on macOS systems.
    This log records a lot of information, including what drives are connected to
    the machine and the amount of storage available and used daily. While we can also
    learn about shutdown times, network states, and other information from this log,
    we will focus on drive usage over time.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All libraries used in this script are present in Python's standard library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script will leverage the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up arguments to accept the log file and a path to write the report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a class that handles the parsing of the log's various sections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a method to extract the relevant section and pass it for further processing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract disk information from these sections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a CSV writer to export the extracted details.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin by importing libraries necessary for argument handling, date interpretation,
    and the writing spreadsheets. One of the great things about processing text files
    in Python is that you rarely need a third-party library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler accepts two positional arguments, `daily_out`
    and `output_report`, which represent the path to the daily.out log file and the
    desired output path for the CSV spreadsheet, respectively. Notice how we pass
    an open file object for processing through the `argparse.FileType` class. Following
    this, we initialize the `ProcessDailyOut` class with the log file and call the
    `run()` method and store the returned results in the `parsed_events` variable.
    We then call the `write_csv()` method to write the results to a spreadsheet in
    the desired output directory using defined columns from the `processor` class
    object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `ProcessDailyOut` class, we set up the properties supplied by the user
    and define the columns used for the report. Notice how we add two different sets
    of columns: the `disk_status_columns` and the `report_columns`. The `report_columns`
    are simply the `disk_status_columns` with two additional fields to identify the
    entry date and time zone.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `run()` method begins by iterating over the provided log file. After stripping
    whitespace characters from the start and end of each line, we validate the content
    to identify breaks in sections. The `"-- End of daily output --"` string breaks
    each entry in the log file. Each entry contains several sections of data broken
    up by new lines. For this reason, we must use several blocks of code to split
    and process each section separately.
  prefs: []
  type: TYPE_NORMAL
- en: In this loop, we gather all lines from a single event and pass it to the `process_event()`
    method and append the processed results to the `parsed_events` list that is eventually
    returned.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `process_event()` method, we will define variables that will allow us
    to split sections of an event for further processing. To better understand this
    next segment of code, please take a moment to review the following example of
    an event:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00081.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Within this event, we can see that the first element is the date value and time
    zone, followed by a series of subsections. Each of the subsection headers is a
    line that ends with a colon; we use this to split the various data elements within
    this file, as seen in the following code. We create a dictionary, `event_data`,
    using the section headers as a key and their content, if present, as the value
    before further processing each subsection.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: If the section header line does not end with a colon, we check if there are
    exactly two colons in the line. If so, we try to validate this line as a date
    value. To handle this date format with built-in libraries, we need to extract
    the time zone separately from the rest of the date as there is a known bug in
    versions of Python 3 with parsing time zones with the `%Z` formatter. For the
    curious, more information on this bug can be found at [https://bugs.python.org/issue22377](https://bugs.python.org/issue22377).
  prefs: []
  type: TYPE_NORMAL
- en: To separate the time zone from the date value, we delimit the string on the
    space value, place the time zone value (element `4` in this example) in its own
    variable, then join the remaining time values into a new string that we can parse
    with the `datetime` library. This may raise an `IndexError` if the string does
    not have a minimum of `5` elements or a `ValueError` if the `datetime` format
    string is invalid. If either of these error types are not raised, we assign the
    date to the `event_data` dictionary. If we do receive either of these errors,
    the line will be appended to the `section_data` list and the next loop iteration
    will continue. This is important as a line may contain two colons and not be a
    date value, and so we wouldn't want to then disregard the line by removing it
    from the script's consideration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The last piece of this conditional appends any line that has content to the
    `section_data` variable for further processing as needed. This prevents blank
    lines from finding their way in and allows us to capture all information between
    two section headers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We close this function by calling to any subsection processors. At this time,
    we only handle the disk information subsection, with the `process_disk()` method,
    though one could develop code to extract other values of interest. This method
    accepts as its input the event information and the event date. The disk information
    is returned as a list of processed disk information elements which we return to
    the `run()` method and add the values to the processed event list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: To process a disk subsection, we iterate through each of the lines, if there
    are any, and extract the relevant event information. The `for` loop starts by
    checking the iteration number and skipping row zero as it contains the data's
    column headers. For any other line, we use list comprehension and split the line
    on a single space, strip whitespace, and filter out any fields that are blank.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Next, we initialize a dictionary, `disk_info`, that holds the event information
    with the date and time zone details for this snapshot. The `for` loop uses the
    `enumerate()` function to map values to their column names. If the column name
    contains `"/Volumes/"` (the standard mount point for drive volumes), we will join
    the remainder of the split items. This ensures that volumes with spaces in their
    names are preserved appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The innermost `for` loop ends by appending the disk information to the `processed_data`
    list. Once all lines in the disk section have been processed, we return the `processed_data`
    list to the parent function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, we briefly touch on the `write_csv()` method, which uses the `DictWriter`
    class to open the file and write the header rows and the content to the CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this script, we can see the extracted details in the CSV report.
    An example of this output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00082.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Adding daily.out parsing to Axiom
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Easy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: Using the code we just developed to parse macOS `daily.out` logs, we add this
    functionality into Axiom, developed by *Magnet Forensics*, for the automatic extraction
    of these events. As Axiom supports the processing of forensic images and loose
    files, we can either provide it a full acquisition or just an export of the `daily.out`
    log for this example. Through the API made available by this tool, we can access
    and process files found by its engine and return results for review directly within
    Axiom.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Magnet Forensics team developed an API for both Python and XML to add support
    for creating custom artifacts within Axiom. The Python API, at of the writing
    of this book, is only available through `IronPython` running Python version 2.7\.
    While we have developed our code outside of this platform, we can easily integrate
    it into Axiom following the steps laid out in this recipe. We used Axiom version
    1.1.3.5726 to test and develop this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: We first need to install Axiom in a Windows instance and ensure that our code
    is stable and portable. Additionally, our code needs to be sandbox friendly. The
    Axiom sandbox limits the use of third-party libraries and access to some Python
    modules and functions that could cause code to interact with the system outside
    of the application. For this reason, we designed our `daily.out` parser to only
    use built-in libraries that are safe in the sandbox to demonstrate the ease of
    develop with these custom artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To develop and implement a custom artifact, we will need to:'
  prefs: []
  type: TYPE_NORMAL
- en: Install Axiom in on a Windows machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the script we developed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the `Artifact` class and define the parser metadata and columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Develop the `Hunter` class to handle the artifact processing and result reporting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this script, we import the `axiom` library and the datetime library. Notice,
    we have removed the previous `argparse` and `csv` imports are they are unnecessary
    here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Next, we must paste in the `ProcessDailyOut` class from the prior recipe, not
    including the `write_csv` or argument handling code, to use in this script. Since
    the current version of the API does not allow imports, we have to bundle all the
    code we need into a single script. To save pages and avoid redundancy, we will
    omit the code block in this section (though it exists as you'd expect in the code
    file bundled with this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: The next class is the `DailyOutArtifact`, a subclass of the `Artifact` class
    provided by the Axiom API. We call the `AddHunter()` method, providing our (not
    yet shown) `hHunter` class, before defining the plugin's name within the `GetName()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The last method of this class, `CreateFragments()`, specifies how to handle
    a single entry of the processed daily.out log results. A fragment, with respect
    to the Axiom API, is the term used to describe a single entry of an artifact.
    This code block allows us to add custom column names and assign the proper categories
    and data types for those columns. The categories include date, location, and other
    special values defined by the tool. The majority of columns for our artifact will
    be in the `None` category, as they don't display a specific kind of information.
  prefs: []
  type: TYPE_NORMAL
- en: 'One important categorical difference is `DateTimeLocal` versus `DateTime`:
    the `DateTime` will present the date as a UTC value to the user, so we need to
    be conscious about selecting the proper date category. Because we extracted the
    time zone from the daily.out log entries, we use the `DateTimeLocal` category
    in this recipe. The `FragmentType` property is a string for all of the values,
    as the class does not convert values from strings into another data type.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The next class is our `Hunter`. This parent class is used to run the processing
    code and, as you will see, specifies the platform and content that will be provided
    to the plugin by the Axiom engine. In this instance, we only want to run this
    against the computer platform and a file that goes by a single name. The `RegisterFileName()`
    method is one of several options for specifying what files will be requested by
    the plugin. We can also use regular expressions or file extensions to select the
    files we would like to process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The `Hunt()` method is where the magic happens. To start, we get a temporary
    path where the file can be read within the sandbox and assign it to the `temp_daily_out`
    variable. With this open file, we hand the file object to the `ProcessDailyOut`
    class and use the `run()` method to parse the file, just like in the last recipe.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'After gathering the parsed event information, we are ready to "publish" the
    data to the software and display it to the user. In the `for` loop, we first initiate
    a `Hit()` object to add data to a new fragment using the `AddValue()` method.
    Once we have assigned the event values to a hit, we publish the hit to the platform
    with the `PublishHit()` method and continue the loop until all parsed events have
    been published:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The last bit of code checks to see if the file is not `None` and will close
    it if so. This is the end of the processing code, which may be called again if
    another `daily.out` file is discovered on the system!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The last line registers our hard work with Axiom's engine to ensure it is included
    and called by the framework.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the newly developed artifact in Axiom, we need to take a few more steps
    to import and run the code against an image. First, we need to launch Axiom Process.
    This is where we will load, select, and run the artifact against the provided
    evidence. Under the Tools menu, we select the Manage custom artifacts option:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00083.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Within the Manage custom artifacts window, we will see any existing custom
    artifacts and can import new ones as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00084.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will add our custom artifact and the updated Manage custom artifacts window
    should show the name of the artifact:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00085.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can press OKAY and continue through Axiom, adding the evidence and configuring
    our processing options. When we reach the COMPUTER ARTIFACTS selection, we want
    to confirm that the custom artifact is selected to run. It probably goes without
    saying: we should only run this artifact if the machine is running macOS or has
    a macOS partition on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'After completing the remaining configuration options, we can start processing
    the evidence. With processing complete, we run Axiom Examine to review the processed
    results. As seen in the following screenshot, we can navigate to the CUSTOM pane
    of the artifact review and see the parsed columns from the plugin! These columns
    can be sorted and exported using the standard options in Axiom, without any additional
    code on our part:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00087.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Scanning for indicators with YARA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: As a bonus section, we will leverage the powerful **Yet Another Recursive Algorithm**
    (**YARA**) regular-expression engine to scan for files of interest and indicators
    of compromise. YARA is a pattern-matching utility designed for use in malware
    identification and incident response. Many tools use this engine as the backbone
    for identification of likely malicious files. Through this recipe, we learn how
    to take YARA rules, compile them, and match them across one or more folders or
    files. While we will not cover the steps required to form a YARA rule, one can
    learn more about the process from their documentation at [http://yara.readthedocs.io/en/latest/writingrules.html](http://yara.readthedocs.io/en/latest/writingrules.html).
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe requires the installation of the third-party library `yara`. All
    other libraries used in this script are present in Python''s standard library.
    This library can be installed with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the `yara-python` library, visit [https://yara.readthedocs.io/en/latest/](https://yara.readthedocs.io/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: We can also use projects such as YaraRules ([http://yararules.com](http://yararules.com))
    and use pre-built rules from the industry and VirusShare ([http://virusshare.com](http://virusshare.com))
    to use real malware samples for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script has four main developmental steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up and compile YARA rules.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scan a single file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate through directories to process individual files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Export results to CSV.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This script imports the required libraries to handle argument parsing, file
    and folder iteration, writing CSV spreadsheets, and the `yara` library to compile
    and scan for the YARA rules.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler accepts two positional arguments, `yara_rules`
    and `path_to_scan`, which represent the path to the YARA rules and the file or
    folder to scan, respectively. This recipe also accepts one optional argument,
    `output`, which, if supplied, writes the results of the scan to a spreadsheet
    as opposed to the console. Lastly, we pass these values to the `main()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: In the `main()` function, we accept the path to the `yara` rules, the files
    or folders to scan, and the output file (if any). Since the `yara` rules can be
    a file or directory, we use the `ios.isdir()` method to determine if we use the
    `compile()` method on a whole directory or, if the input is a file, pass it to
    the method using the `filepath` keyword. The `compile()` method reads the rule
    file or files and creates an object that we can match against objects we scan.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Once the rules are compiled, we perform a similar `if-else` statement to process
    the path to scan. If the input to scan is a directory, we pass it to the `process_directory()`
    function and, otherwise, we use the `process_file()` method. Both take the compiled
    YARA rules and the path to scan and return a list of dictionaries containing any
    matches.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: As you may guess, we will ultimately convert this list of dictionaries to a
    CSV report if the output path was specified, using the columns we define in the
    `columns` list. However, if the output argument is `None`, we write this data
    to the console in a different format instead.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The `process_directory()` function essentially iterates through a directory
    and passes each file to the `process_file()` function. This decreases the amount
    of redundant code in the script. Each processed entry that is returned is added
    to the `match_info` list, as the returned object is a list. Once we have processed
    each file, we return the complete list of results to the parent function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The `process_file()` method uses with the `match()` method of the `yrules` object.
    The returned match object is an iterable containing one or more hits against the
    rules. From the hit, we can extract the rule name, any tags, the offset in the
    file, the string value of the rule, and the string value of the hit. This information,
    plus the file path, will form an entry in the report. Collectively, this information
    is useful in identifying whether the hit is a false positive or is of significance.
    It can also be helpful when fine-tuning YARA rules to ensure only relevant results
    are presented for review.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: To `write_stdout()` function reports match information to the console if the
    user does not specify an output file. We iterate through each entry in the `match_info`
    list and print each column name and its value from the `match_info` dictionary
    in a colon-delimited, newline-separated format. After each entry, we print `30`
    equals signs to visually separate the entries from each other.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The `write_csv()` method follows the standard convention, using the `DictWriter`
    class to write the headers and all of the data into the sheet. Notice how this
    function is adjusted to handle CSV writing in Python 3, using the `'w'` mode and
    `newline` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this code, we can provide the appropriate arguments at the command-line
    and generate a report of any matches. The following screenshot shows the custom
    rules for detecting Python files and keyloggers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00088.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'These rules are shown in the output CSV report, or console if a report is not
    specified, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00089.jpeg)'
  prefs: []
  type: TYPE_IMG
