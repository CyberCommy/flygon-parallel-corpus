- en: Searching, Mining and Visualizing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Geocoding an IP address
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting IP addresses of Wikipedia edits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing contributor location frequency on Wikipedia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a word cloud from a StackOverflow job listing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crawling links on Wikipedia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing page relationships on Wikipedia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating degrees of separation between Wikipedia pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we will examine how to search web content, derive analytical
    results, and also visualize those results. We will learn how to locate posters
    of content an visualize the distribution of their locations.  Then we will examine
    how to scrape, model, and visualize the relationships between pages on Wikipedia.
  prefs: []
  type: TYPE_NORMAL
- en: Geocoding an IP address
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Geocoding is the process of converting an address into geographic coordinates.
    These addresses can be actual street addresses, which can be geocoded with various
    tools such as the Google maps geocoding API ([https://developers.google.com/maps/documentation/geocoding/intro](https://developers.google.com/maps/documentation/geocoding/intro)).
    IP addresses can be, and often are, geocoded by various applications to determine
    where computers, and their users, are located. A very common and valuable use
    is analyzing web server logs to determine the source of users of your website.
  prefs: []
  type: TYPE_NORMAL
- en: This is possible because an IP address does not only represent an address of
    the computer in terms of being able to communicate with that computer, but often
    can also be converted into an approximate physical location by looking it up in
    IP address / location databases. There are many of these databases available,
    all of which are maintained by various registrars (such as ICANN). There are also
    other tools that can report geographic locations for public IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of free services for IP geolocation. We will examine one
    that is quite easy to use, freegeoip.net.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Freegeoip.net is a free geocoding service. If you go to [http://www.freegeoip.net](http://www.freegeoip.net)
    in your browser, you will be presented with a page similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9f56e0c3-c452-4ab1-b951-aa839cfdc831.png)The freegeoip.net home
    page'
  prefs: []
  type: TYPE_NORMAL
- en: The default page reports your public IP address, and also gives you the geolocation
    of the IP address according to their database. This isn't accurate to the actual
    address of my house, and is actually quite a few miles off, but the general location
    in the world is fairly accurate. We can do important things with data that is
    at this resolution and even lower. Often just knowing the country origin for web
    requests is enough for many purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Freegeoip lets you make 15000 calls per hour. Each page load counts as one call,
    and as we will see, each API call also counts as one.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We could scrape this page to get this information but fortunately, freegeoip.net
    gives us a convenient REST API to use. Scrolling further down the page, we can
    see the API documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/89c10af1-bbcf-4f28-ba1f-5d13fb0f706f.png)The freegeoio.net API documentation'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simply use the requests library to make a GET request using the properly
    formatted URL. As an example, just entering the following URL in the browser returns
    a JSON representation of the geocoded data for the given IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f87e77ef-910e-4011-b99f-c7942469aa52.png)Sample JSON for an IP address'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Python script to demonstrate this is available in `08/01_geocode_address.py`.
    The is simple and consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This has the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that your output for this IP address may vary, and surely will with different
    IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: How to collect IP addresses of Wikipedia edits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Processing aggregate results of geocoded IP addresses can provide valuable insights.
    This is very common for server logs and can also be used in many other situations.
    Many websites include the IP address of contributors of content. Wikipedia provides
    a history of changes on all of their pages. Edits created by someone that is not
    a registered user of Wikipedia have their IP address published in the history.
    We will examine how to create a scraper that will navigate the history of a given
    Wikipedia topic and collect the IP addresses of unregistered edits.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will examine the edits made to the Web scraping page in Wikipedia. This
    page is available at:  [https://en.wikipedia.org/wiki/Web_scraping](https://en.wikipedia.org/wiki/Web_scraping).
    The following shows a small part of this page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/533c554f-85a6-4f21-bea7-c382ae64db39.png)The view history tab'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note View history in the upper-right. Clicking on that link gives you access
    to the history for the edits:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/11286403-7354-45f0-9786-49148c1f52dc.png)Inspecting an IP address'
  prefs: []
  type: TYPE_NORMAL
- en: I've scrolled this down a little bit to highlight an anonymous edit.  Note that
    we can identify these anonymous edit entries using the `mw-userling mw-anonuserlink`
    class in the source.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice also that you can specify the number of edits per page to be listed,
    which can be specified by adding a parameter to the URL. The following URL will
    give us the 500 most recent edits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/w/index.php?title=Web_scraping&offset=&limit=500&action=history](https://en.wikipedia.org/w/index.php?title=Web_scraping&offset=&limit=500&action=history)'
  prefs: []
  type: TYPE_NORMAL
- en: So instead of crawling a number of different pages, walking through them 50
    at a time, we'll just do one page of 500.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to perform the scraping is in the script file, `08/02_geocode_wikipedia_edits.py`.
    Running the script produces the following output (truncated to the first few geo
    IPs):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The script also writes the geo IPs to the `geo_ips.json` file. The next recipe
    will use this file instead of making all the page requests again.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The explanation is as follows. The script begins by executing the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A call is made to `collect_geo_ips` which will request the page with the specified
    topic and up to 500 edits. These geo IPs are then printed to the console, and
    also written to the `geo_ips.json` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for `collect_geo_ips` is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This function first makes a call to `get_history_ips`, reports the quantity
    found, and then makes repeated requests to `get_geo_ips` for each IP address.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for `get_history_ips` is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This formulates the URL for the history page, retrieves the page, and then pulls
    out all distinct IP addresses with the `mw-anonuserlink` class.
  prefs: []
  type: TYPE_NORMAL
- en: '`get_geo_ips` then takes this set of IP addresses and calls `freegeoip.net`
    on each for the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While this data is useful, in our next recipe we will read in the data written
    to `geo_ips.json` (using pandas) and visualize the distribution of the users by
    country using a bar chart.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing contributor location frequency on Wikipedia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use the collected data to determine the frequency of edits of Wikipedia
    articles from countries around the world. This can be done by grouping the captured
    data by country and counting the number of edits related to each country. Then
    we will sort the data and create a bar chart to see the results.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a very simple task to perform with pandas. The code of the example is
    in `08/03_visualize_wikipedia_edits.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code begins by importing pandas and `matplotlib.pyplot`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The data file we created in the previous recipe is already in a format that
    can be read directly by pandas. This is one of the benefits of using JSON as a
    data format; pandas has built-in support for reading and writing data from JSON.
    The following reads in the data using the `pd.read_json()` function and displays
    the first five rows on the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For our immediate purpose we only require the `country_code` column, which
    we can extract with the following (and shows the first five rows in that result):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can group the rows in this series using `.groupby(''country_code'')`,
    and on the result, `call .count()` will return the number of items in each of
    those groups. The code also sorts the results from the largest to lowest values
    by `calling .sort_values()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can see from just these results that the US definitely leads in edits, with
    India being the second most popular.
  prefs: []
  type: TYPE_NORMAL
- en: 'This data can easily be visualized as a bar graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following bar graph showing overall distribution for all
    of the countries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3c71176f-4d78-4576-9b9d-63f6df781f6a.png)Histogram of the edit frequencies'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a word cloud from a StackOverflow job listing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now lets look at creating a word cloud.  Word clouds are an image that demonstrate
    the frequency of key words within a set of text.  The larger the word in the image,
    the more apparent significance it has in the body of text.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the Word Cloud  library to create our word cloud. The source for
    the library is available at [https://github.com/amueller/word_cloud](https://github.com/amueller/word_cloud). 
    This library can be installed into your Python environment using  `pip install
    wordcloud`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The script to create the word cloud is in the `08/04_so_word_cloud.py` file. 
    This recipe continues on from the stack overflow recipes from chapter 7 to provide
    a visualization of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the word cloud and the frequency distribution function from
    NLTK:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The word cloud is then generated from the probability distribution of the words
    we collected from the job listing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we just need to display the word cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'And the resulting word cloud is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/02a14763-b72f-4386-ae29-ebed2d7d7219.png)The word cloud for the
    job listing'
  prefs: []
  type: TYPE_NORMAL
- en: The positioning and size have some built-in randomness, so the result you get
    may differ.
  prefs: []
  type: TYPE_NORMAL
- en: Crawling links on Wikipedia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe we will write a small program to utilize the crawl the links
    on a Wikipedia page through several levels of depth. During this crawl we will
    gather the relationships between the pages and those referenced from each page.
    During this we will build a relationship amongst these pages the we will ultimately
    visualize in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this example is in the `08/05_wikipedia_scrapy.py`. It references
    code in a module in the `modules`/`wikipedia` folder of the code samples, so make
    sure that is in your Python path.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can the sample Python script.  It will crawl a single Wikipedia page using
    Scrapy.  The page it will crawl is the Python page at [https://en.wikipedia.org/wiki/Python_(programming_language)](https://en.wikipedia.org/wiki/Python_(programming_language)),
    and collect relevant links on that page.
  prefs: []
  type: TYPE_NORMAL
- en: 'When run you will see the similar output to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The first part of output is from the Scrapy crawler and shows the pages that
    are passed to the parse method.  These are pages that start with our initial page
    and through the first five most common links from that page.
  prefs: []
  type: TYPE_NORMAL
- en: The second part of this output is a representation of the page that is crawled
    and the links found on that page that are considered for future processing.  The
    first number is the level of the crawl the crawl that the relationship was found,
    followed by the parent page and the link found on that page.  For every page /
    link found, there is a separate entry.  Since this is a one depth crawl, we just
    show pages found from the initial page.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lets start with the code in them main script file, `08/05_wikipedia_scrapy.py`. 
    This starts with creating a `WikipediaSpider` object and running the crawl:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This tells Scrapy that we want to run it for one level of depth, and we get
    an instance of the crawler as we want to inspect its properties which are the
    result of the crawl.  The results are then printed with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Each result from the crawler is stored in the `linked_pages` property.  Each
    of those objects is represented by several properties including the title of the
    page (the last portion of the wikipedia URL) and the title of each page found
    within the content of the HTML of that page.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s walk through how the crawler functions.  The code for the spider
    is in `modules/wikipedia/spiders.py`. The crawler starts off by defining a sub-class
    of a Scrapy `Spider`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We are starting on the Python page in Wikipedia.  Next are the definition of
    a few class level variable to define how the crawl operates and for the results
    to be retrieved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Each page of this crawl will the processed by the parse method of the spider. 
    Let''s walk through it.  It starts with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In each Wikipedia page we look for links what start with `/wiki`.  There are
    other links in the page but these are the ones this this crawl will consider important.
  prefs: []
  type: TYPE_NORMAL
- en: This crawler implements an algorithm where all found links on the page are counted
    for similarity.  There are quite a few repeat links.  Some of these are spurious.
    Others represent a true importance of linking multiple times to other pages.
  prefs: []
  type: TYPE_NORMAL
- en: The `max_items_per_page` defines how many links on the current page we will
    further investigate.  There will be quite a few links on each page, and this algorithm
    counts all the similar links and puts them into buckets.  It then follows the
    `max_items_per_page`  most popular links.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process is managed though the use of the `links_counter` variable.  This
    is a dictionary of mappings between the current page and all links found on the
    page. For each link we decide to follow We count the number of times it is referenced
    on the page.  This variable is a map between that URL and and instance of the
    following object that counts the number of references:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The code then walks through all the identified links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This examines every link and only considers them for further crawling based
    upon the stated rules (no ':' in the link, nor 'International' as it is quite
    popular so we exclude it, and finally we don't include the start URL).  If the
    link passes this, then a new `LinkReferenceCounter` object is created (if this
    link as not been seen before), or its reference count is incremented.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since there are likely repeat links on each page, we want to consider only
    the `max_items_per_page` most common links.  The code does this by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Out of the `link_counter` dictionary we pull all of the `LinkReferenceCounter`
    objects and sort them by the count, and then select the top `max_items_per_page`
    items.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is for each of these qualifying items we recored them in the
    `linked_pages` field of the class.  Each object in this list of the the type `PageToPageMap`. 
    This class has the following definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Fundamentally this object represents a source page URL to a linked page URL,
    and it also tracks the current level of the crawl.  The title properties are the
    URL decoded forms of the last part of the Wikipedia URL, and represent a more
    human-friendly version of the URL.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the code yields to Scrapy new pages to crawl to.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Theres more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This crawler / algorithm also keeps track of the current level of **depth** in
    the crawl.  If a new link is considered to be beyond the maximum depth of the
    crawl. While this can be controlled to a point by Scrapy, this code still needs
    to not include links beyond the maximum depth.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is controlled by using the depth field of the `PageToPageMap` object. 
    For each page of the crawl, we check to see if the response has meta-data, a property
    which represents the `"parent"` PageToPageMap object for an given page.  We find
    this with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This code in the page parser looks to see if there is a parent object.  Only
    the first page of the crawl does not have a parent page. If there is an instance,
    the depth of this crawl is considered one higher.  When the new `PageToPageMap`
    object is created, this value is passed to it and stored.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code passes this object to the next level of the crawl by using the meta
    property of the request object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In this way we can pass data from one level of a crawl in a Scrapy spider to
    the next.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing page relationships on Wikipedia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe we take the data we collected in the previous recipe and create
    a force-directed network visualization of the page relationships using the NetworkX
    Python library.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NetworkX is software for modeling, visualizing, and analyzing complex network
    relationships. You can find more information about it at: [https://networkx.github.io](https://networkx.github.io/).
    It can be installed in your Python environment using `pip install networkx`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The script for this example is in the `08/06_visualizze_wikipedia_links.py`
    file. When run it produces a graph of the links found on the initial Python page
    in Wikipedia:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/54f24aea-344a-42b8-b02b-a68c1a8287c4.png)Graph of the links'
  prefs: []
  type: TYPE_NORMAL
- en: Now we can see the relationships between the pages!
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The crawl starts with defining a one level of depth crawl:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This information is similar to the previous recipe, and new we need to convert
    it into a model that NetworkX can use for a graph. This starts with creating a
    NetworkX graph model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'A NetworkX graph consists of nodes and edges.  From the data collected we must
    crate a set of unique nodes (the pages) and the edges (the fact that a page references
    another page).  This performed with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This iterates through all the results from out crawl and identifies all the
    unique nodes (the distinct pages), and then all of the links between any pages
    and other pages.  For each node and edge, we register those with NetworkX.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we create the plot with Matplotlib and tell NetworkX how to create the
    visuals in the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The important parts of this are first the use of NetworkX to form a spring layout
    on the nodes.  That calculates the actual positions of the nodes but does not
    render them or the edges. That is the purpose of the next two lines which give
    NetworkX the instructions on how to render both the nodes and edges. and finally,
    we need to put labels on the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This crawl only did a single depth crawl.  The crawl can be increased with
    the following change to the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Fundamentally the only change is to increase the depth one level. This then
    results in the following graph (there is randomization in any spring graph so
    the actual results have a different layout):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6ddc2576-434d-411d-9d3f-15e9482e48f6.png)Spider graph of the links'
  prefs: []
  type: TYPE_NORMAL
- en: This begins to be interesting as we now start to see inter-relationships and
    cyclic relationships between pages.
  prefs: []
  type: TYPE_NORMAL
- en: I dare you to further increase the depth and number of links per page.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating degrees of separation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's calculate the degrees of separation between any two pages.  This answers
    the question of how many pages we need to go through from a source page to find
    another page.  This could be a non-trivial graph traversal problem as there can
    be multiple paths between the two pages.  Fortunately for us, NetworkX, using
    the exact same graph model, has built in function to solve this with the exact
    same model from the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The script for this example is in the `08/07_degrees_of_separation.py`. The
    code is identical to the previous recipe, with a 2-depth crawl, except that it
    omits the graph and asks NetworkX to solve the degrees of separation between `Python_(programming_language)`
    and `Dennis_Ritchie`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This tells us that to go from `Python_(programming_language)` to `Dennis_Ritchie`
    we have to go through one other page: `C_(programming_language)`.  Hence, one
    degree of separation. If we went directly to `C_(programming_language), ` it would
    be 0 degrees of separation.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The solution of this problem is solved by an algorithm known as **A***. The
    **A*** algorithm determines the shortest path between two nodes in a graph.  Note
    that this path can be multiple paths of different lengths and that the correct
    result is the shortest path.  A good thing for us is that NetworkX has a built
    in function to do this for us.  It is done with one simple statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'From this we report on the actual path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For more information on the **A*** algorithm check out this page at[ ](https://en.wikipedia.org/wiki/A*_search_algorithm)[https://en.wikipedia.org/A*_search_algorithm](https://en.wikipedia.org/A*_search_algorithm).
  prefs: []
  type: TYPE_NORMAL
