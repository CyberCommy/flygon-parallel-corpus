- en: Data Acquisition and Extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: How to parse websites and navigate the DOM using BeautifulSoup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching the DOM with Beautiful Soup's find methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying the DOM with XPath and lxml
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying data with XPath and CSS Selectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Scrapy selectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading data in Unicode / UTF-8 format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key aspects for effective scraping are understanding how content and data
    are stored on web servers, identifying the data you want to retrieve, and understanding
    how the tools support this extraction. In this chapter, we will discuss website
    structures and the DOM, introduce techniques to parse, and query websites with
    lxml, XPath, and CSS. We will also look at how to work with websites developed
    in other languages and different encoding types such as Unicode.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, understanding how to find and extract data within an HTML document
    comes down to understanding the structure of the HTML page, its representation
    in the DOM, the process of querying the DOM for specific elements, and how to
    specify which elements you want to retrieve based upon how the data is represented.
  prefs: []
  type: TYPE_NORMAL
- en: How to parse websites and navigate the DOM using BeautifulSoup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the browser displays a web page it builds a model of the content of the
    page in a representation known as the **document object model** (**DOM**). The
    DOM is a hierarchical representation of the page's entire content, as well as
    structural information, style information, scripts, and links to other content.
  prefs: []
  type: TYPE_NORMAL
- en: It is critical to understand this structure to be able to effectively scrape
    data from web pages. We will look at an example web page, its DOM, and examine
    how to navigate the DOM with Beautiful Soup.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use a small web site that is included in the `www` folder of the sample
    code.  To follow along, start a web server from within the `www` folder.  This
    can be done with Python 3 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The DOM of a web page can be examined in Chrome by right-clicking the page and
    selecting Inspect. This opens the Chrome Developer Tools. Open a browser page
    to `http://localhost:8080/planets.html`. Within chrome you can right click and
    select 'inspect' to open developer tools (other browsers have similar tools).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/414227f7-dd30-4c7e-8bab-7fc02e136fcd.png) Selecting Inspect on the
    Page'
  prefs: []
  type: TYPE_NORMAL
- en: This opens the developer tools and the inspector. The DOM can be examined in
    the Elements tab.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows the selection of the first row in the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f3dd4285-7e9b-4b96-a3c5-3f31e318b983.png)Inspecting the First Row'
  prefs: []
  type: TYPE_NORMAL
- en: Each row of planets is within a `<tr>` element.  There are several characteristics
    of this element and its neighboring elements that we will examine because they
    are designed to model common web pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, this element has three attributes: `id`, `planet`, and `name`. Attributes
    are often important in scraping as they are commonly used to identify and locate
    data embedded in the HTML.'
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the `<tr>` element has children, and in this case, five `<td>` elements.
    We will often need to look into the children of a specific element to find the
    actual data that is desired.
  prefs: []
  type: TYPE_NORMAL
- en: This element also has a parent element, `<tbody>`. There are also sibling elements,
    and the a set of `<tr>`  child elements.  From any planet, we can go up to the
    parent and find the other planets. And as we will see, we can use various constructs
    in the various tools, such as the **find** family of functions in Beautiful Soup,
    and also  `XPath` queries, to easily navigate these relationships.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe, and most of the others in this chapter, will be presented with
    iPython in an interactive manner.  But all of the code for each is available in
    a script file.  The code for this recipe is in `02/01_parsing_html_wtih_bs.py`.
    You can type the following in, or cut and paste from the script file.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's walk through parsing HTML with Beautiful Soup. We start by loading
    this page into a `BeautifulSoup` object using the following code, which creates
    a BeautifulSoup object, loads the content of the page using with requests.get,
    and loads it into a variable named soup.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The HTML in the `soup` object can be retrieved by converting it to a string
    (most BeautifulSoup objects have this characteristic).  This following shows the
    first 1000 characters of the HTML in the document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can navigate the elements in the DOM using properties of `soup`. `soup`
    represents the overall document and we can drill into the document by chaining
    the tag names. The following navigates to the `<table>` containing the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following retrieves the the first child `<tr>` of the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note this type of notation retrieves only the first child of that type.  Finding
    more requires iterations of all the children, which we will do next, or using
    the find methods (the next recipe).
  prefs: []
  type: TYPE_NORMAL
- en: 'Each node has both children and descendants. Descendants are all the nodes
    underneath a given node (event at further levels than the immediate children),
    while children are those that are a first level descendant. The following retrieves
    the children of the table, which is actually a `list_iterator` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can examine each child element in the iterator using a `for` loop or a Python
    generator. The following uses a generator to get all the children of the and return
    the first few characters of their constituent HTML as a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Last but not least, the parent of a node can be found using the `.parent` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beautiful Soup converts the HTML from the page into its own internal representation.
    This model has an identical representation to the DOM that would be created by
    a browser. But Beautiful Soup also provides many powerful capabilities for navigating
    the elements in the DOM, such as what we have seen when using the tag names as
    properties.  These are great for finding things when we know a fixed path through
    the HTML with the tag names.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This manner of navigating the DOM is relatively inflexible and is highly dependent
    upon the structure. It is possible that this structure can change over time as
    web pages are updated by their creator(s). The pages could even look identical,
    but have a completely different structure that breaks your scraping code.
  prefs: []
  type: TYPE_NORMAL
- en: So how can we deal with this? As we will see, there are several ways of searching
    for elements that are much better than defining explicit paths. In general, we
    can do this using XPath and by using the find methods of beautiful soup. We will
    examine both in recipes later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Searching the DOM with Beautiful Soup's find methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can perform simple searches of the DOM using Beautiful Soup's find methods.
    These methods give us a much more flexible and powerful construct for finding
    elements that are not dependent upon the hierarchy of those elements.  In this
    recipe we will examine  several common uses of these functions to locate various
    elements in the DOM.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ff you want to cut and paste the following into ipython, you can find the samples
    in `02/02_bs4_find.py`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start with a fresh iPython session and start by loading the planets
    page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the previous recipe, to access all of the `<tr>` in the table, we used a
    chained property syntax to get the table, and then needed to get the children
    and iterator over them.  This does have a problem as the children could be elements
    other than `<tr>`.  A more preferred method of getting just the `<tr>` child elements
    is to use `findAll`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lets start by first finding the `<table>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This tells the soup object to find the first `<table>` element in the document. 
    From this element we can find all of the `<tr>` elements that are descendants
    of the table with `findAll`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that these are the descendants and not immediate children.  Change the
    query to `"td"` to see the difference.  The are no direct children that are `<td>`,
    but each row has multiple <td> elements.  In all, there would be 54 `<td>` elements
    found.
  prefs: []
  type: TYPE_NORMAL
- en: There is a small issue here if we want only rows that contain data for planets.
    The table header is also included.  We can fix this by utilizing the `id` attribute
    of the target rows.  The following finds the row where the value of `id` is `"planet3"`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Awesome! We used the fact that this page uses this attribute to represent table
    rows with actual data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s go one step further and collect the masses for each planet and put
    the name and mass in a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: And just like that we have made a nice data structure from the content embedded
    within the page.
  prefs: []
  type: TYPE_NORMAL
- en: Querying the DOM with XPath and lxml
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'XPath is a query language for selecting nodes from an XML document and is a
    must-learn query language for anyone performing web scraping. XPath offers a number
    of benefits to its user over other model-based tools:'
  prefs: []
  type: TYPE_NORMAL
- en: Can easily navigate through the DOM tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More sophisticated and powerful than other selectors like CSS selectors and
    regular expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a great set (200+) of built-in functions and is extensible with custom
    functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is widely supported by parsing libraries and scraping platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'XPath contains seven data models (we have seen some of them previously):'
  prefs: []
  type: TYPE_NORMAL
- en: root node (top level parent node)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: element nodes (`<a>`..`</a>`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: attribute nodes (`href="example.html"`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: text nodes (`"this is a text"`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: comment nodes (`<!-- a comment -->`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: namespace nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: processing instruction nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'XPath expressions can return different data types:'
  prefs: []
  type: TYPE_NORMAL
- en: strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: booleans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: node-sets (probably the most common case)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An (XPath) **axis** defines a node-set relative to the current node. A total
    of 13 axes are defined in XPath to enable easy searching for different node parts,
    from the current context node, or the root node.
  prefs: []
  type: TYPE_NORMAL
- en: '**lxml** is a Python wrapper on top of the libxml2 XML parsing library, which
    is written in C.  The implementation in C helps make it faster than Beautiful
    Soup, but also harder to install on some computers. The latest installation instructions
    are available at: [http://lxml.de/installation.html](http://lxml.de/installation.html).'
  prefs: []
  type: TYPE_NORMAL
- en: lxml supports XPath, which makes it considerably easy to manage complex XML
    and HTML documents. We will examine several techniques of using lxml and XPath
    together, and how to use lxml and XPath to navigate the DOM and access data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for these snippets is in `02/03_lxml_and_xpath.py` in case you want
    to save some typing.  We will start by importing `html` from `lxml`, as well as
    `requests`, and then load the page.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: By this point, lxml should be installed as a dependency of other installs. 
    If you get errors, install it with `pip install lxml`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing that we do is to load the HTML into an lxml "etree".  This is
    lxml's representation of the DOM.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `tree` variable is now an lxml representation of the DOM which models the
    HTML content. Let's now examine how to use it and XPath to select various elements
    from the document.
  prefs: []
  type: TYPE_NORMAL
- en: Out first XPath example will be to find all the the `<tr>` elements below the
    `<table>` element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This XPath navigates by tag name from the root of the document down to the `<tr>`
    element.  This example looks similar to the property notation from Beautiful Soup,
    but ultimately it is significantly more expressive.  And notice one difference
    in the result.  All the the `<tr>` elements were returned and not just the first. 
    As a matter of fact, the tags at each level of this path with return multiple
    items if they are available.  If there was multiple `<div>` elements just below
    `<body>`, then the search for `table/tr` would be executed on all of those `<div>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual result was an `lxml` element object.  The following gets the HTML
    associated with the elements but using `etree.tostring()` (albeit they have encoding
    applied):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now let's look at using XPath to select only the `<tr>` elements that are planets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The use of the `[]` next to a tag states that we want to do a selection based
    on some criteria upon the current element.  The `@` states that we want to examine
    an attribute of the tag, and in this cast we want to select tags where the attribute
    is equal to `"planet"`.
  prefs: []
  type: TYPE_NORMAL
- en: There is also another point to be made out of the query that had 11 `<tr>` rows. 
    As stated earlier, the XPath runs the navigation on all the nodes found at each
    level.  There are two tables in this document, both children of a different `<div>`
    that are both a child or the `<body>` element.  The row with `id="planetHeader"`
    came from our desired target table, the other, with `id="footerRow"`, came from
    the second table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously we solved this by selecting `<tr>` with `class="row"`, but there
    are also other ways worth a brief mention.  The first is that we can also use
    `[]` to specify a specific element at each section of the XPath like they are
    arrays.  Take the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Arrays in XPath start at 1 instead of 0 (a common source of error).  This selected
    the first `<div>`.  A change to `[2]` selects the second `<div>` and hence only
    the second `<table>`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The first `<div>` in this document also has an id attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be used to select this `<div>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Earlier we selected the planet rows based upon the value of the class attribute. 
    We can also exclude rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose that the planet rows did not have attributes (nor the header row),
    then we could do this by position, skipping the first row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'It is possible to navigate to the parent of a node using `parent::*`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This returned two parents as, remember, this XPath returns the rows from two
    tables, so the parents of all those rows are found. The `*` is a wild card that
    represents any parent tags with any name. In this case, the two parents are both
    tables, but in general the result can be any number of HTML element types.  The
    following has the same result, but if the two parents where different HTML tags
    then it would only return the `<table>` elements.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to specify a specific parent by position or attribute.
    The following selects the parent with `id="footerTable"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'A shortcut for parent is `..` (and `.` also represents the current node):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'And the last example finds the mass of Earth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The trailing portion of this XPath,`/td[3]/text()[1]`, selects the third `<td>`
    element in the row, then the text of that element (which is an array of all the
    text in the element), and the first of those which is the mass.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XPath is a element of the **XSLT** (**eXtensible Stylesheet Language Transformation**)
    standard and provides the ability to select nodes in an XML document. HTML is
    a variant of XML, and hence XPath can work on on HTML document (although HTML
    can be improperly formed and mess up XPath parsing in those cases).
  prefs: []
  type: TYPE_NORMAL
- en: XPath itself is designed to model the structure of XML nodes, attributes, and
    properties. The syntax provides means of finding items in the XML that match the
    expression. This can include matching or logical comparison of any of the nodes,
    attributes, values, or text in the XML document.
  prefs: []
  type: TYPE_NORMAL
- en: XPath expressions can be combined to form very complex paths within the document.
    It is also possible to navigate the document based upon relative positions, which
    helps greatly in finding data based upon relative positions instead of absolute
    positions within the DOM.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding XPath is essential for knowing how to parse HTML and perform web
    scraping. And as we will see, it underlies, and provides an implementation for,
    many of the higher level libraries such as lxml.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XPath is actually an amazing tool for working with XML and HTML documents. It
    is quite rich in its capabilities, and we have barely touched the surface of its
    capabilities for demonstrating a few examples that are common to scraping data
    in HTML documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn much more, please visit the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.w3schools.com/xml/xml_xpath.asp](https://www.w3schools.com/xml/xml_xpath.asp)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.w3.org/TR/xpath/](https://www.w3.org/TR/xpath/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying data with XPath and CSS selectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CSS selectors are patterns used for selecting elements and are often used to
    define the elements that styles should be applied to. They can also be used with
    lxml to select nodes in the DOM. CSS selectors are commonly used as they are more
    compact than XPath and generally can be more reusable in code. Examples of common
    selectors which may be used are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **What you are looking for** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| All tags | `*` |'
  prefs: []
  type: TYPE_TB
- en: '| A specific tag (that is, `tr`) | `.planet` |'
  prefs: []
  type: TYPE_TB
- en: '| A class name (that is, `"planet"`) | `tr.planet` |'
  prefs: []
  type: TYPE_TB
- en: '| A tag with an `ID "planet3"` | `tr#planet3` |'
  prefs: []
  type: TYPE_TB
- en: '| A child `tr` of a table | `table tr` |'
  prefs: []
  type: TYPE_TB
- en: '| A descendant `tr` of a table | `table tr` |'
  prefs: []
  type: TYPE_TB
- en: '| A tag with an attribute (that is, `tr` with `id="planet4"`) | `a[id=Mars]`
    |'
  prefs: []
  type: TYPE_TB
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start examining CSS selectors using the same start up code we used in
    the last recipe.  These code snippets are also in the `02/04_css_selectors.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s start playing with XPath and CSS selectors.  The following selects
    all `<tr>` elements with a class equal to `"planet"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Data for the Earth can be found in several ways. The following gets the row
    based on `id`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following uses an attribute with a specific value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Note that unlike XPath, the `@` symbol need not be used to specify an attribute.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: lxml converts the CSS selector you provide to XPath, and then performs that
    XPath expression against the underlying document. In essence, CSS selectors in
    lxml provide a shorthand to XPath, which makes finding nodes that fit certain
    patterns simpler than with XPath.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because CSS selectors utilize XPath under the covers, there is overhead to its
    use as compared to using XPath directly. This difference is, however, almost a
    non-issue, and hence in certain scenarios it is easier to just use cssselect.
  prefs: []
  type: TYPE_NORMAL
- en: A full description of CSS selectors can be found at: [https://www.w3.org/TR/2011/REC-css3-selectors-20110929/](https://www.w3.org/TR/2011/REC-css3-selectors-20110929/)
  prefs: []
  type: TYPE_NORMAL
- en: Using Scrapy selectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scrapy is a Python web spider framework that is used to extract data from websites.
    It provides many powerful features for navigating entire websites, such as the
    ability to follow links. One feature it provides is the ability to find data within
    a document using the DOM, and using the now, quite familiar, XPath.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe we will load the list of current questions on StackOverflow,
    and then parse this using a scrapy selector. Using that selector, we will extract
    the text of each question.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this recipe is in `02/05_scrapy_selectors.py`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by importing `Selector` from `scrapy`, and also `requests` so that
    we can retrieve the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we load the page.  For this example we are going to retrieve the most
    recent questions on StackOverflow and extract their titles.  We can make this
    query with the the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now create a `Selector` and pass it the response object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Examining the content of this page we can see that questions have the following
    structure to their HTML:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d72e8df6-61f1-4395-a003-009279e30ddb.png)The HTML of a StackOverflow
    Question'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the selector we can find these using XPath:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: And now we drill a little further into each to get the title of the question.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Underneath the covers, Scrapy builds its selectors on top of lxml. It offers
    a smaller and slightly simpler API, which is similar in performance to lxml.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about Scrapy Selectors see: [https://doc.scrapy.org/en/latest/topics/selectors.html](https://doc.scrapy.org/en/latest/topics/selectors.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Loading data in unicode / UTF-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A document's encoding tells an application how the characters in the document
    are represented as bytes in the file. Essentially, the encoding specifies how
    many bits there are per character. In a standard ASCII document, all characters
    are 8 bits. HTML files are often encoded as 8 bits per character, but with the
    globalization of the internet, this is not always the case. Many HTML documents
    are encoded as 16-bit characters, or use a combination of 8- and 16-bit characters.
  prefs: []
  type: TYPE_NORMAL
- en: A particularly common form HTML document encoding is referred to as UTF-8\.
    This is the encoding form that we will examine.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will read a file named `unicode.html` from our local web server, located
    at `http://localhost:8080/unicode.html`.  This file is UTF-8 encoded and contains
    several sets of characters in different parts of the encoding space. For example,
    the page looks as follows in your browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/89c7b066-5d99-4dff-a318-3d97e1d6be0a.png)The Page in the Browser'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using an editor that supports UTF-8, we can see how the Cyrillic characters
    are rendered in the editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/afdf6e7f-3bbb-4226-bc69-356d01a27d5a.png)The HTML in an Editor'
  prefs: []
  type: TYPE_NORMAL
- en: Code for the sample is in `02/06_unicode.py`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will look at using `urlopen` and `requests` to handle HTML in UTF-8\. These
    two libraries handle this differently, so let's examine this.  Let's start importing
    `urllib`, loading the page, and examining some of the content.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Note how the Cyrillic characters were read in as multi-byte codes using \ notation,
    such as `\xd0\x89`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To rectify this, we can convert the content to UTF-8 format using the Python
    `str` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note that the output now has the characters encoded properly.
  prefs: []
  type: TYPE_NORMAL
- en: We can exclude this extra step by using `requests`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of using `urlopen`, the conversion was explicitly performed by
    using the str statement and specifying that the content should be converted to
    UTF-8\. For `requests`, the library was able to determine from the content within
    the HTML that it was in UTF-8 format by seeing the following tag in the document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of resources available on the internet for learning about
    Unicode and UTF-8 encoding techniques. Perhaps the best is the following Wikipedia
    article, which has an excellent summary and a great table describing the encoding
    technique: [https://en.wikipedia.org/wiki/UTF-8](https://en.wikipedia.org/wiki/UTF-8)
  prefs: []
  type: TYPE_NORMAL
