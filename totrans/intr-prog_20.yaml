- en: Reactive Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the final chapter of this book, we'll break the flow of the connected narrative
    and jump closer to real-life professional programming. As more data gets processed
    and services become more sophisticated, the need for more adaptive, highly scalable,
    and distributed applications grows exponentially. That is what we are going to
    address in this chapter—how such a software system may look in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to process a lot of data quickly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reactive systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercise – Creating `io.reactivex.Observable`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to process a lot of data quickly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many measurable performance characteristics that can be applied to
    an application. Which ones to use depends on the purpose of the application. They
    are usually listed as non-functional requirements. The most typical set includes
    the following three:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throughput**: The number of requests processed per a unit of time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latency**: The time elapsed between the moment a request was submitted and
    the moment the *first* byte of the response is received. It is measured in seconds,
    milliseconds, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory footprint**: The amount of memory—min, max, or average— that the application
    consumes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, latency is often calculated as the inverse of the throughput. These
    characteristics vary as a load grows, so the non-functional requirements typically
    include the maximum value for each of them under the average and maximum load.
  prefs: []
  type: TYPE_NORMAL
- en: Often, the improvements in throughput and latency are only at the expense of
    the memory, unless adding a faster CPU can improve all three characteristics.
    But that depends on the nature of the processing. For example, an input/output
    (or another interaction) with a low-performance device can impose a limit, and
    no change in the code can improve the application performance.
  prefs: []
  type: TYPE_NORMAL
- en: There are also subtle nuances in the measuring of each of the characteristics.
    For example, instead of measuring latency as the average of all requests, we can
    use the maximum latency among 99% of the fastest (least latency) requests. Otherwise,
    it looks like an average wealth number obtained by dividing the wealth of a billionaire
    and a person at the bottom of the income pyramid by two.
  prefs: []
  type: TYPE_NORMAL
- en: 'When evaluating application performance, one has to answer the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Can the requested upper limit of the latency ever be exceeded? If yes, how often,
    and by how much?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How long can the period of poor latency be, and how often it is allowed to happen?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who/what measures the latency in production?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the expected peak load, and how long is it expected to last?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only after all of these (and similar questions) have been answered, and the
    non-functional requirements have been established, can we start designing the
    system, testing it, tweaking, and testing again. There are many programming techniques
    that prove to be effective in achieving the required throughput with an acceptable
    memory consumption.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, the terms *asynchronous*, *non-blocking*, *distributed*, *scalable*, *reactive*,
    *responsive*, *resilient*, *elastic*, and *message-driven* became ubiquitous,
    just synonyms of *high performance*. We are going to discuss each of these terms so
    that the reader can understand the motivation that brought *microservices* and
    *reactive systems* to life, which we are going to present in the next two sections
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Asynchronous** means that the requestor gets the response *immediately*,
    but the result is not there. Instead, the requestor receives an object with methods
    that allow us to check whether the result is ready. The requestor calls this method
    periodically, and, when the result is ready, retrieves it using another method
    on the same object.'
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of such a solution is that the requestor can do other things while
    waiting. For example, in [Chapter 11](e8c37d86-291d-4500-84ea-719683172477.xhtml), *JVM
    Processes and Garbage Collection*, we demonstrated how a child thread can be created.
    So, the main thread can create a child thread that sends a non-asynchronous (also
    called blocking) request, and waits for its return, doing nothing. The main thread,
    meanwhile, can continue executing something else, periodically calling the child
    thread object to see whether the result is ready.
  prefs: []
  type: TYPE_NORMAL
- en: That is the most basic of asynchronous call implementations. In fact, we we
    already used it when we processed a parallel stream. The parallel stream operations
    create child threads behind the scenes, break the stream into segments, and assign
    each segment to a dedicated thread, then aggregate the results from each segment
    in the final one. In the previous chapter, we wrote functions that did the aggregating
    job. As a reminder, these functions are called combiners.
  prefs: []
  type: TYPE_NORMAL
- en: Let's compare the performance of the same functionality when processing sequential
    and parallel streams.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential versus parallel streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate the difference between sequential and parallel processing, let''s
    imagine a system that collects data from 10 physical devices (sensors) and calculates
    an average. The interface of such a system could look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: interface MeasuringSystem {
  prefs: []
  type: TYPE_NORMAL
- en: double get(String id);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'It has only one method, `get()`, which receives an ID of a sensor and returns
    the result of the measurement. Using this interface, we can implement many different
    systems, which are able to call different devices. For demonstration purposes,
    we are not going to write a lot of code. All we need is to put a delay of 100
    milliseconds (to imitate the time it takes to collect the measurement from the
    sensor) and return some number. We can implement the delay as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: void pauseMs(int ms) {
  prefs: []
  type: TYPE_NORMAL
- en: try{
  prefs: []
  type: TYPE_NORMAL
- en: TimeUnit.MILLISECONDS.sleep(ms);
  prefs: []
  type: TYPE_NORMAL
- en: '} catch(InterruptedException ex){'
  prefs: []
  type: TYPE_NORMAL
- en: ex.printStackTrace();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the resulting number, we will use `Math.random()` to simulate the difference
    of the measurements received from different sensors (that is why we need to find
    an average—to offset the errors and other idiosyncrasies of an individual device).
    So, our demo implementation may look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: class MeasuringSystemImpl implements MeasuringSystem {
  prefs: []
  type: TYPE_NORMAL
- en: public double get(String id){
  prefs: []
  type: TYPE_NORMAL
- en: demo.pauseMs(100);
  prefs: []
  type: TYPE_NORMAL
- en: return 10\. * Math.random();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we realize that our `MeasuringInterface` is a functional interface, because
    it has only one method. This means we can use one of the standard functional interfaces
    from the `java.util.function` package; namely, `Function<String, Double>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: Function<String, Double> mSys = id -> {
  prefs: []
  type: TYPE_NORMAL
- en: demo.pauseMs(100);
  prefs: []
  type: TYPE_NORMAL
- en: return 10\. + Math.random();
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we can discard our `MeasuringSystem` interface and `MeasuringSystemImpl` class.
    But we can leave the `mSys` (*Measuring System*) identificator that reflects the
    idea behind this function: it represents a measuring system that provides access
    to its sensors and allows us to collect data from them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a list of sensor IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: List<String> ids = IntStream.range(1, 11)
  prefs: []
  type: TYPE_NORMAL
- en: .mapToObj(i -> "id" + i).collect(Collectors.toList());
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: Again, in real life, we would need to collect the IDs of real devices, but for
    demo purposes, we just generate them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will create the `collectData()` method, which calls all the sensors
    and calculates an average across all the received data:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: Stream<Double> collectData(Stream<String> stream,
  prefs: []
  type: TYPE_NORMAL
- en: Function<String, Double> mSys){
  prefs: []
  type: TYPE_NORMAL
- en: return  stream.map(id -> mSys.apply(id));
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the method receives a stream that provides IDs and a function
    that uses each ID to get a measurement from a sensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how we are going to call this method from the `averageDemo()` method,
    using the `getAverage()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: void averageDemo() {
  prefs: []
  type: TYPE_NORMAL
- en: Function<String, Double> mSys = id -> {
  prefs: []
  type: TYPE_NORMAL
- en: pauseMs(100);
  prefs: []
  type: TYPE_NORMAL
- en: return 10\. + Math.random();
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: getAverage(() -> collectData(ids.stream(), mSys));
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: void getAverage(Supplier<Stream<Double>> collectData) {
  prefs: []
  type: TYPE_NORMAL
- en: LocalTime start = LocalTime.now();
  prefs: []
  type: TYPE_NORMAL
- en: double a = collectData.get()
  prefs: []
  type: TYPE_NORMAL
- en: .mapToDouble(Double::valueOf).average().orElse(0);
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println((Math.round(a * 100.) / 100.) + " in " +
  prefs: []
  type: TYPE_NORMAL
- en: Duration.between(start, LocalTime.now()).toMillis() + " ms");
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we create the function that represents the measuring system
    and pass it into the `collectData()` method, along with the stream of IDs. Then,
    we create the `SupplierStream<Double>>` function as the `() -> collectData(ids.stream(),
    mSys)` lambda expression, and pass it as the `collectData` parameter into the `getAverage()` method.
    Inside of the `getAverage()` method, we call the `get()` of the supplier, and
    thus invoke `collectData(ids.stream(), mSys)`, which returns `Stream<Double>`.
    We then convert it to `DoubleStream` with the `mapToDouble()` operation, so we
    can apply the `average()` operation. The `average()` operation returns an `Optional<Double>` object,
    and we call its `orElse(0)` method, which returns either the calculated value
    or zero (if, for example, the measuring system could not connect to any of its
    sensors and returned an empty stream). The last line of the `getAverage()` method
    prints the result and the time it took to calculate it. In real code, we would
    return the result and use it for other calculations. But for our demonstration,
    we just print it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can compare the performance of sequential stream processing to parallel
    stream processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: List<String> ids = IntStream.range(1, 11)
  prefs: []
  type: TYPE_NORMAL
- en: .mapToObj(i -> "id" + i).collect(Collectors.toList());
  prefs: []
  type: TYPE_NORMAL
- en: Function<String, Double> mSys = id -> {
  prefs: []
  type: TYPE_NORMAL
- en: pauseMs(100);
  prefs: []
  type: TYPE_NORMAL
- en: return 10\. + Math.random();
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: getAverage(() -> collectData(ids.stream(), mSys));
  prefs: []
  type: TYPE_NORMAL
- en: '//prints: 10.46 in 1031 ms'
  prefs: []
  type: TYPE_NORMAL
- en: getAverage(() -> collectData(ids.parallelStream(), mSys));
  prefs: []
  type: TYPE_NORMAL
- en: '//prints: 10.49 in 212 ms'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, processing a parallel stream is five times faster than processing
    a sequential stream.
  prefs: []
  type: TYPE_NORMAL
- en: Although behind the scenes, the parallel stream uses `\` asynchronous processing,
    this is not what programmers have in mind when talking about processing requests asynchronously.
    From the application perspective, it is just parallel (also called concurrent)
    processing. It is faster than sequential processing, but the main thread has to
    wait until all of the calls are made and all of the data retrieved. If each call
    takes at least 100 ms (as it does in our case), then the processing of all the
    calls cannot be completed in less time.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we can create a child thread and let it place all the calls, and
    wait until they complete, while the main thread does something else. We can even
    create a service that does it, so the application can just tell such a service
    what to do (pass the sensor IDs, in our case) and continue doing something else.
    Later, the main thread can call the service again, and get the result or pick
    it up in some agreed-upon place. That would be the truly asynchronous processing
    that the programmers are talking about.
  prefs: []
  type: TYPE_NORMAL
- en: But before writing such a code, let's look at the `CompletableFuture` class
    located in the `java.util.concurrent` package. It does everything we described,
    and even more.
  prefs: []
  type: TYPE_NORMAL
- en: Using the CompletableFuture class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the `CompletableFuture` object, we can separate sending the request for
    data to the measuring system (and creating the `CompletableFuture` object) from
    getting the result from the `CompletableFuture` object. This is exactly the scenario
    we described when explaining what asynchronous processing is. Let''s demonstrate
    it in the code. Similar to the way we submitted the requests to a measuring system,
    we can do it using the `CompletableFuture.supplyAsync()` static method:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: List<CompletableFuture<Double>> list = ids.stream()
  prefs: []
  type: TYPE_NORMAL
- en: .map(id -> CompletableFuture.supplyAsync(() -> mSys.apply(id)))
  prefs: []
  type: TYPE_NORMAL
- en: .collect(Collectors.toList());
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: The difference is that the `supplyAsync()` method does not wait for the call
    to the measuring system to return. Instead, it immediately creates a `CompletableFuture`
    object and returns it, so that a client can use this object to retrieve the value
    returned by the measuring system at any time. There are also methods that allow
    us to check whether the value was returned at all, but that is not the point of
    this demonstration, which is to show how the `CompletableFuture` class can be
    used to organize asynchronous processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The created list of `CompletableFuture` objects can be stored anywhere. We
    have chosen to store it in a `Map`. In fact, we have created a `sendRequests()` method
    that can send any number of requests to any number of measuring systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: Map<Integer, List<CompletableFuture<Double>>>
  prefs: []
  type: TYPE_NORMAL
- en: sendRequests(List<List<String>> idLists,
  prefs: []
  type: TYPE_NORMAL
- en: List<Function<String, Double>> mSystems){
  prefs: []
  type: TYPE_NORMAL
- en: LocalTime start = LocalTime.now();
  prefs: []
  type: TYPE_NORMAL
- en: Map<Integer, List<CompletableFuture<Double>>> requests
  prefs: []
  type: TYPE_NORMAL
- en: = new HashMap<>();
  prefs: []
  type: TYPE_NORMAL
- en: for(int i = 0; i < idLists.size(); i++){
  prefs: []
  type: TYPE_NORMAL
- en: 'for(Function<String, Double> mSys: mSystems){'
  prefs: []
  type: TYPE_NORMAL
- en: List<String> ids = idLists.get(i);
  prefs: []
  type: TYPE_NORMAL
- en: List<CompletableFuture<Double>> list = ids.stream()
  prefs: []
  type: TYPE_NORMAL
- en: .map(id -> CompletableFuture.supplyAsync(() -> mSys.apply(id)))
  prefs: []
  type: TYPE_NORMAL
- en: .collect(Collectors.toList());
  prefs: []
  type: TYPE_NORMAL
- en: requests.put(i, list);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: long dur = Duration.between(start, LocalTime.now()).toMillis();
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println("Submitted in " + dur + " ms");
  prefs: []
  type: TYPE_NORMAL
- en: return requests;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the preceding method accepts two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`List<List<String>> idLists`: A collection (list) of lists of sensor IDs, each
    list specific to a particular measuring system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`List<Function<String, Double>> mSystems`: List of measuring systems, each
    represented as `Function<String, Double>`, that has a single `apply()` method
    that accepts a sensor ID and returns a double value (measurement result). The
    systems in this list are in the same order as the sensor ID lists in the first
    parameter, so we can match IDs to the system by their positions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, we create the `Map<Integer, List<CompletableFuture<Double>>>` object
    to store lists of the `CompletableFuture` objects. We generate them in a `for`-loop,
    and then store them in a `Map` with a key that is just some sequential number.
    The `Map` is returned to the client and can be stored anywhere, for any period
    of time (well, there are some limitations that can be modified, but we are not
    going to discuss them here). Later, when the client has decided to get the results
    of the requests, the `getAverage()` method can be used to retrieve them:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: void getAverage(Map<Integer, List<CompletableFuture<Double>>> requests){
  prefs: []
  type: TYPE_NORMAL
- en: 'for(List<CompletableFuture<Double>> list: requests.values()){'
  prefs: []
  type: TYPE_NORMAL
- en: getAverage(() -> list.stream().map(CompletableFuture::join));
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding method accepts the `Map` object created by the `sendRequests()` method
    and iterates over all the values (lists of the `ComputableFuture` objects) stored
    in `Map`. For each list, it creates a stream that maps each element (object of `ComputableFuture`)
    to the result of the `join()` method called on the element. This method retrieves
    the value returned from the corresponding call to a measuring system. If the value
    is not available, the method waits for some time (a configurable value), and either
    quits (and returns `null`), or, eventually, receives the value from the call to
    the measuring system, if available. Again, we are not going to discuss all the
    guards placed around failures, to keep the focus on the main functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `() -> list.stream().map(CompletableFuture::join)` function is actually
    passed into the `getAverage()` method (which should be familiar to you), which
    we used while processing the streams in the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: void getAverage(Supplier<Stream<Double>> collectData) {
  prefs: []
  type: TYPE_NORMAL
- en: LocalTime start = LocalTime.now();
  prefs: []
  type: TYPE_NORMAL
- en: double a = collectData.get()
  prefs: []
  type: TYPE_NORMAL
- en: .mapToDouble(Double::valueOf).average().orElse(0);
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println((Math.round(a * 100.) / 100.) + " in " +
  prefs: []
  type: TYPE_NORMAL
- en: Duration.between(start, LocalTime.now()).toMillis() + " ms");
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: This method calculates the average of all the values emitted by the passed-in
    stream, prints it, and also captures the time it took to process the stream (and
    calculate the average).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s use the new methods and see how performance is improved:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: Function<String, Double> mSys = id -> {
  prefs: []
  type: TYPE_NORMAL
- en: pauseMs(100);
  prefs: []
  type: TYPE_NORMAL
- en: return 10\. + Math.random();
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: List<Function<String, Double>> mSystems = List.of(mSys, mSys, mSys);
  prefs: []
  type: TYPE_NORMAL
- en: List<List<String>> idLists = List.of(ids, ids, ids);
  prefs: []
  type: TYPE_NORMAL
- en: Map<Integer, List<CompletableFuture<Double>>> requestLists =
  prefs: []
  type: TYPE_NORMAL
- en: 'sendRequests(idLists, mSystems);  //prints: Submitted in 13 ms'
  prefs: []
  type: TYPE_NORMAL
- en: pauseMs(2000);  //The main thread can continue doing something else
  prefs: []
  type: TYPE_NORMAL
- en: //for any period of time
  prefs: []
  type: TYPE_NORMAL
- en: 'getAverage(requestLists);               //prints: 10.49 in 5 ms'
  prefs: []
  type: TYPE_NORMAL
- en: //        10.61 in 0 ms
  prefs: []
  type: TYPE_NORMAL
- en: //        10.51 in 0 ms
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, we reused the same measuring system (and its IDs) to imitate
    working with three measuring systems. You can see that the requests for all three
    systems are submitted in 13 ms. The `sendRequests()` method exists, and the main
    thread is free to do something else for at least two seconds. That is how much
    time it takes to actually send all the requests and receive the response, because
    of `pauseMs(100)`, used for each call to a measuring system. Then, we calculate
    an average for each system, and it takes almost no time. That is what programmers
    mean when they talk about processing requests asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `CompletableFuture` class has many methods, and has support from several
    other classes and interfaces. For example, the pause period of two seconds to
    collect all of the data can be decreased by using a thread pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: Map<Integer, List<CompletableFuture<Double>>>
  prefs: []
  type: TYPE_NORMAL
- en: sendRequests(List<List<String>> idLists,
  prefs: []
  type: TYPE_NORMAL
- en: List<Function<String, Double>> mSystems){
  prefs: []
  type: TYPE_NORMAL
- en: ExecutorService pool = Executors.newCachedThreadPool();
  prefs: []
  type: TYPE_NORMAL
- en: LocalTime start = LocalTime.now();
  prefs: []
  type: TYPE_NORMAL
- en: Map<Integer, List<CompletableFuture<Double>>> requests
  prefs: []
  type: TYPE_NORMAL
- en: = new HashMap<>();
  prefs: []
  type: TYPE_NORMAL
- en: for(int i = 0; i < idLists.size(); i++){
  prefs: []
  type: TYPE_NORMAL
- en: 'for(Function<String, Double> mSys: mSystems){'
  prefs: []
  type: TYPE_NORMAL
- en: List<String> ids = idLists.get(i);
  prefs: []
  type: TYPE_NORMAL
- en: List<CompletableFuture<Double>> list = ids.stream()
  prefs: []
  type: TYPE_NORMAL
- en: .map(id -> CompletableFuture.supplyAsync(() -> mSys.apply(id),
  prefs: []
  type: TYPE_NORMAL
- en: pool))
  prefs: []
  type: TYPE_NORMAL
- en: .collect(Collectors.toList());
  prefs: []
  type: TYPE_NORMAL
- en: requests.put(i, list);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: pool.shutdown();
  prefs: []
  type: TYPE_NORMAL
- en: long dur = Duration.between(start, LocalTime.now()).toMillis();
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println("Submitted in " + dur + " ms");
  prefs: []
  type: TYPE_NORMAL
- en: return requests;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: There is a variety of such pools, for different purposes and different performances.
    But all of that does not change the overall system design, so we will omit such
    details.
  prefs: []
  type: TYPE_NORMAL
- en: So, the power of asynchronous processing is great. But who benefits from it?
  prefs: []
  type: TYPE_NORMAL
- en: If you have created an application that collects data and calculates an average
    for each measuring system on demand, then from the client's point of view, it
    still takes a lot of time, because the pause (for two seconds, or less if we use
    a thread pool) is still included in the client's wait time. So, the advantage
    of asynchronous processing is lost for the client, unless you have designed your
    API so that the client can submit the request and walk away to do something else,
    then pick up the results later.
  prefs: []
  type: TYPE_NORMAL
- en: That is the difference between a *synchronous* (or *blocking)* API, when a client
    waits (blocked) until the result is returned, and an *asynchronous* API, when
    a client submits a request and walks away to do something else, then gets the
    results later.
  prefs: []
  type: TYPE_NORMAL
- en: The possibility of an asynchronous API enhances our understanding of latency. Usually,
    by latency, programmers mean the time between the moment the request was submitted
    and the time the first byte of the response has been received *during the same
    call to the API*. But if the API is asynchronous, the definition of latency changes
    to, "The moment the request was submitted and the time the result is available
    for the client to collect." The latency during each call, in such cases, is assumed
    to be much smaller than the time between the call to place the request and the
    call to collect the result.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a notion of the *non-blocking* API, which we are going to discuss
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Non-blocking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the client of an application, the notion of a non-blocking API only tells
    us that the application is probably scalable, reactive, responsive, resilient,
    elastic, and message-driven. In the following sections, we are going to discuss
    all of these terms, but for now, we hope you can derive a sense of what each of
    them means from the names themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such a statement means two things:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Being non-blocking does not affect the protocol of communication between the
    client and the application: it may be synchronous (blocking) or asynchronous.
    Non-blocking is an implementation detail; it is a view of the API from inside
    the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-blocking is the implementation that helps the application to be all of the
    following: scalable, reactive, responsive, resilient, elastic, and message-driven.
    This means it is a very important design concept that resides at the foundation
    of many modern applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It''s know that blocking APIs and non-blocking APIs are not opposites. They
    describe different aspects of the application. The blocking API describes how
    a client interacts with it: the client calls, and stays connected until the response
    is provided. The non-blocking API describes how the application is implemented:
    it does not dedicate an execution thread to each of the requests, but provides
    several lightweight worker threads that do the processing asynchronously and concurrently.'
  prefs: []
  type: TYPE_NORMAL
- en: The term non-blocking came into use with the `java.nio` (NIO stands for non-blocking
    input/output) package that provides support for intensive input/output (I/O) operations.
  prefs: []
  type: TYPE_NORMAL
- en: The java.io versus java.nio package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Writing and reading data to and from external memory (a hard drive, for example)
    is a much slower operation than other processes that happen in the memory. The
    already existing classes and interfaces of the `java.io` package worked fine,
    but once in a while, the performance would bottleneck. The new `java.nio` package
    was created to provide more effective I/O support.
  prefs: []
  type: TYPE_NORMAL
- en: The `java.io` implementation was based on stream processing, which, as we saw
    in the previous section, is basically a blocking operation, even if some kind
    of concurrency is happening behind the scenes. To increase the speed, the `java.nio`
    implementation was based on reading/writing to/from a buffer in the memory. Such
    a design allowed us to separate the slow process of filling/emptying the buffer
    and the fast reading/writing from/to it. In a way, it is similar to what we have
    done in our example of the `CompletableFuture` class usage. The additional advantage
    of having data in a buffer is that it is possible to inspect it, going there and
    back along the buffer, which is impossible when reading sequentially from the
    stream. It has allowed for more flexibility during data processing.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the `java.nio` implementation introduced another middleman process,
    called a channel, which provided bulk data transfers to and from buffers. The
    reading thread gets data from a channel, and only receives what is currently available,
    or nothing at all (if there is no data is in the channel). If data is not available,
    the thread, instead of remaining blocked, can do something else—reading/writing
    to/from other channels, for example. The same way the main thread in our `CompletableFuture` example
    was free to do whatever had to be done while the measuring system was reading
    data from its sensors. This way, instead of dedicating a thread to one I/O process,
    a few worker-threads can serve many I/O processes.
  prefs: []
  type: TYPE_NORMAL
- en: Such a solution was called a non-blocking I/O, and was later applied to other
    processes, the most prominent being the events processing in an *event loop*,
    also called a *run loop*.
  prefs: []
  type: TYPE_NORMAL
- en: Event loop, or run loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many non-blocking systems of processing are based on the event (or run) loop—a
    thread that is continually executed, receives events (requests, messages), and
    then dispatches them to the corresponding *event handlers*. There is nothing special
    about the event handlers. They are just methods (functions) dedicated, by the
    programmer, for processing a particular event type.
  prefs: []
  type: TYPE_NORMAL
- en: This design is called a *reactor design pattern*, defined as *an event handling
    pattern for handling service requests delivered concurrently to a service handler.* It
    also provided the name for the *reactive programming* and *reactive systems* that
    *react* to some events, and process them accordingly. We will talk about reactive
    systems later, in a dedicated section.
  prefs: []
  type: TYPE_NORMAL
- en: Event loop-based design is widely used in operating systems and graphical user
    interfaces. It is available in Spring WebFlux in Spring 5, and implemented in
    JavaScript and its popular executing environment, Node.js. The last one uses an
    event loop as its processing backbone. The Vert.x toolkit is built around the
    event loop, too. We will show some examples of the latter in the *Microservices*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Before the adoption of the event loop, a dedicated thread was assigned to each
    incoming request—much like in our demonstration of stream processing. Each of
    the threads requires the allocation of a certain amount of resources that is not
    request-specific, so some of the resources—mostly memory allocation—are wasted.
    Then, as the number of requests grows, the CPU needs to switch its context from
    one thread to another more often, to allow more or less concurrent processing
    of all the requests. Under the load, the overhead of switching the context becomes
    substantial enough to affect the performance of an application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing an event loop has addressed these two issues:'
  prefs: []
  type: TYPE_NORMAL
- en: It eliminated the waste of resources by avoiding the creation of a thread dedicated
    to each request, and keeping it around until the request was processed. With an
    event loop in place, a much smaller memory allocation is needed for each request
    to capture its specifics. It made it possible to keep many more requests in memory,
    so that they can be processed concurrently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overhead of the CPU context-switching became much smaller, too, because
    of the diminishing context size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The non-blocking API is how the processing of the requests is implemented. With
    it, the systems are able to handle a much bigger load (to be more scalable and
    elastic), while remaining highly responsive and resilient.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The notion of what is distributed has changed over time. It used to mean an
    application running on several computers, connected via a network. It even had
    the synonym name of  parallel computing, because each instance of the application
    did the same thing. Such an application improved system resilience. The failure
    of one computer did not affect the system as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, another meaning was added: an application spread across several computers,
    so each of its components contributed to the result produced by the application
    as a whole. Such a design was usually used for the calculation- or data-heavy
    tasks that required a lot of CPU power, or required lots of data from many different
    sources.'
  prefs: []
  type: TYPE_NORMAL
- en: When a single CPU became powerful enough to handle the computation load of thousands
    of older computers and cloud computing, especially systems such as AWS Lambda
    serverless computing platforms, it removed the notion of an individual computer
    from consideration at all; *distributed* may mean any combination of one application,
    or its components, running on one, or many, computers.
  prefs: []
  type: TYPE_NORMAL
- en: The examples of a distributed system include big data processing systems, distributed
    file or data storage systems, and ledger systems, such as blockchain or Bitcoin,
    that can also be included in the group of data storage systems under the subcategory
    of *smart* data storage systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'When programmers call a system *distributed* today, they typically mean the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: The system can tolerate a failure of one, or even several, of its constituting
    components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each system component has only a limited, incomplete view of the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The structure of the system is dynamic, and may change during the execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system is scalable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scalability is the ability to sustain a growing load without significant degradation
    of the latency/throughput. Traditionally, it was achieved by breaking the software
    system into tiers: a front tier, middle tier, and backend tier, for example. Each
    tier was composed of multiple deployments of the copies of the same group of components
    responsible for the particular type of processing.'
  prefs: []
  type: TYPE_NORMAL
- en: The front tier components were responsible for the presentation, based on the
    request and the data they received from the middle tier. The middle tier components
    were responsible for computations and decision-making, based on the data coming
    from the front tier and the data they could read from the backend tier. They also
    sent data to the backend for storage. The backend tier stored the data, and provided
    it to the middle tier.
  prefs: []
  type: TYPE_NORMAL
- en: Adding copies of components, each tier allowed us to stay abreast with the increasing
    load. In the past, it was only possible by adding more computers to each tier.
    Otherwise, there would be no resources available for the newly deployed components'
    copies.
  prefs: []
  type: TYPE_NORMAL
- en: But, with the introduction of cloud computing, and especially the AWS Lambda
    services, scalability is achieved by adding only new copies of the software components.
    The fact that more computers are added to the tier (or not) is hidden from the
    deployer.
  prefs: []
  type: TYPE_NORMAL
- en: Another recent trend in distributed-system architecture allowed us to fine-tune
    the scalability by scaling up not only by a tier, but also by a particular small,
    functional part of a tier, and providing one, or several, particular kinds of
    services, called microservices. We will discuss this and show some examples of
    microservices in the *Microservices* section.
  prefs: []
  type: TYPE_NORMAL
- en: With such an architecture, the software system becomes a composition of many
    microservices; each may be duplicated as many times as needed, to support the
    required increase in the processing power. In this sense, we can talk about scalability
    on the level of one microservice only.
  prefs: []
  type: TYPE_NORMAL
- en: Reactive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term *reactive* is usually used in the context of reactive programming and
    reactive systems. Reactive programming (also called Rx programming) is based on
    programming with asynchronous data streams (also called reactive streams). It
    was introduced to Java in the `java.util.concurrent` package, with Java 9\. It
    allows a `Publisher` to generate a stream of data, to which a `Subscriber` can
    asynchronously subscribe.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you have seen, we were able to process data asynchronously even without
    this new API, by using `CompletableFuture`. But, after writing such a code a few
    times, one notices that most of it is just plumbing, so one gets the feeling there
    has to be an even simpler and more convenient solution. That''s how the Reactive
    Streams initiative ([http://www.reactive-streams.org](http://www.reactive-streams.org))
    was born. The scope of the effort is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The scope of Reactive Streams is to find a minimal set of interfaces, methods and protocols
    that will describe the necessary operations and entities to achieve the goal—asynchronous streams
    of data with non-blocking back pressure.
  prefs: []
  type: TYPE_NORMAL
- en: The term *non-blocking back pressure* refers to one of the problems of asynchronous
    processing—a coordination of the speed rate of the incoming data with the ability
    of the system to process it, without the need to stop (block) the data input.
    The solution is to inform the source that the consumer has a difficulty in keeping
    up with the input, but the processing should react to the change of the rate of
    incoming data in a more flexible manner than just blocking the flow (thus, the
    name reactive).
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to standard Java libraries, several other libraries already exist
    that implemented the Reactive Streams API: RxJava, Reactor, Akka Streams, and
    Vert.x are among the most known. We will use RxJava 2.1.13 in our examples. You
    can find the RxJava 2.x API at [http://reactivex.io](http://reactivex.io), under
    the name ReactiveX, which stands for Reactive Extension.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first compare two implementations of the same functionality, using the `java.util.stream`
    package and the `io.reactivex` package of RxJava 2.1.13, which can be added to
    the project with the following dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: <dependency>
  prefs: []
  type: TYPE_NORMAL
- en: <groupId>io.reactivex.rxjava2</groupId>
  prefs: []
  type: TYPE_NORMAL
- en: <artifactId>rxjava</artifactId>
  prefs: []
  type: TYPE_NORMAL
- en: <version>2.1.13</version>
  prefs: []
  type: TYPE_NORMAL
- en: </dependency>
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sample program is going to be very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a stream of integers: 1, 2, 3, 4, 5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter only even numbers (2 and 4).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the square root of each of the filtered numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the sum of all the square roots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is how it can be implemented using the `java.util.stream` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: double a = IntStream.rangeClosed(1, 5)
  prefs: []
  type: TYPE_NORMAL
- en: .filter(i -> i % 2 == 0)
  prefs: []
  type: TYPE_NORMAL
- en: .mapToDouble(Double::valueOf)
  prefs: []
  type: TYPE_NORMAL
- en: .map(Math::sqrt)
  prefs: []
  type: TYPE_NORMAL
- en: .sum();
  prefs: []
  type: TYPE_NORMAL
- en: 'System.out.println(a); //prints: 3.414213562373095'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'And the same functionality implemented with RxJava looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: Observable.range(1, 5)
  prefs: []
  type: TYPE_NORMAL
- en: .filter(i -> i % 2 == 0)
  prefs: []
  type: TYPE_NORMAL
- en: .map(Math::sqrt)
  prefs: []
  type: TYPE_NORMAL
- en: .reduce((r, d) -> r + d)
  prefs: []
  type: TYPE_NORMAL
- en: '.subscribe(System.out::println); //prints: 3.414213562373095'
  prefs: []
  type: TYPE_NORMAL
- en: RxJava is based on the Observable object (which plays the role of Publisher)
    and Observer that subscribes to the Observable and waits for data to be emitted.
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the `Stream` functionality, `Observable` has significantly different
    capabilities. For example, a stream, once closed, cannot be reopened, while an
    `Observable` object can be used again. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: Observable<Double> observable = Observable.range(1, 5)
  prefs: []
  type: TYPE_NORMAL
- en: .filter(i -> i % 2 == 0)
  prefs: []
  type: TYPE_NORMAL
- en: .doOnNext(System.out::println)    //prints 2 and 4 twice
  prefs: []
  type: TYPE_NORMAL
- en: .map(Math::sqrt);
  prefs: []
  type: TYPE_NORMAL
- en: observable
  prefs: []
  type: TYPE_NORMAL
- en: .reduce((r, d) -> r + d)
  prefs: []
  type: TYPE_NORMAL
- en: '.subscribe(System.out::println);  //prints: 3.414213562373095'
  prefs: []
  type: TYPE_NORMAL
- en: observable
  prefs: []
  type: TYPE_NORMAL
- en: .reduce((r, d) -> r + d)a
  prefs: []
  type: TYPE_NORMAL
- en: .map(r -> r / 2)
  prefs: []
  type: TYPE_NORMAL
- en: '.subscribe(System.out::println);  //prints: 1.7071067811865475'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example, as you can see from the comments, the `doOnNext()`
    operation was called twice, which means the `observable` object emitted values
    twice. But if we do not want `Observable` running twice, we can cache its data
    by adding the `cache()` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: Observable<Double> observable = Observable.range(1,5)
  prefs: []
  type: TYPE_NORMAL
- en: .filter(i -> i % 2 == 0)
  prefs: []
  type: TYPE_NORMAL
- en: .doOnNext(System.out::println)  //prints 2 and 4 only once
  prefs: []
  type: TYPE_NORMAL
- en: .map(Math::sqrt)
  prefs: []
  type: TYPE_NORMAL
- en: .cache();
  prefs: []
  type: TYPE_NORMAL
- en: observable
  prefs: []
  type: TYPE_NORMAL
- en: .reduce((r, d) -> r + d)
  prefs: []
  type: TYPE_NORMAL
- en: '.subscribe(System.out::println); //prints: 3.414213562373095'
  prefs: []
  type: TYPE_NORMAL
- en: observable
  prefs: []
  type: TYPE_NORMAL
- en: .reduce((r, d) -> r + d)
  prefs: []
  type: TYPE_NORMAL
- en: .map(r -> r / 2)
  prefs: []
  type: TYPE_NORMAL
- en: '.subscribe(System.out::println);  //prints: 1.7071067811865475'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the second usage of the same `Observable` took advantage of
    the cached data, allowing for better performance. There is more functionality
    available in the `Observable` interface and RxJava, which the format of this book
    does not allow us to describe. But we hope you get the idea.
  prefs: []
  type: TYPE_NORMAL
- en: Writing code using RxJava, or another asynchronous-streams library, constitutes
    reactive programming. It realizes the goal declared in the Reactive Manifesto
    ([https://www.reactivemanifesto.org](https://www.reactivemanifesto.org)) as building
    reactive systems that are responsive, resilient, elastic, and message-driven.
  prefs: []
  type: TYPE_NORMAL
- en: Responsive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It seems that this term is self-explanatory. The ability to respond in a timely
    manner is one of the primary qualities every client demands from any system. It
    is possible to achieve this using many different approaches. Even traditional
    blocking APIs can be supported by enough servers and other infrastructure, to
    provide the expected responsiveness under a very big load. Reactive programming
    just helps to do it using less hardware.
  prefs: []
  type: TYPE_NORMAL
- en: It comes with a price, as reactive code requires changing the way we used to
    do it, even five years ago. But after some time, this new way of thinking becomes
    as natural as any other already-familiar skill. We will see a few more examples
    of reactive programming in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Resilient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Failures are inevitable. The hardware crashes, the software has defects, unexpected
    data is received, or an unexpected and poorly-tested execution path was taken—any
    of these events, or a combination of them, can happen at any time. Resilience
    is the ability of the system to withstand such a situation and continue to deliver
    the expected results.
  prefs: []
  type: TYPE_NORMAL
- en: It can be achieved using redundancy of the deployable components and hardware,
    using isolation of parts of the system from each other (so the domino effect becomes
    less probable), designing the system so that the lost piece can be replaced automatically
    or an appropriate alarm raised so that qualified personnel can interfere, and
    through other measures.
  prefs: []
  type: TYPE_NORMAL
- en: We have talked about distributed systems already. Such an architecture makes
    the system more resilient by eliminating a single point of failure. Also, breaking
    the system into many specialized components that talk to each other using messages
    allows for a better tuning of the duplication of the most critical parts, and
    creates more opportunities for their isolation and potential failure containment.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ability to sustain the biggest possible load is usually associated with
    scalability. But the ability to preserve the same performance characteristics
    under varying loads is called elasticity. A client of an elastic system should
    not notice any difference between the idle periods and the periods of the peak
    load.
  prefs: []
  type: TYPE_NORMAL
- en: The non-blocking reactive style of implementation facilitates this quality.
    Also, breaking the program into smaller parts and converting them into services
    that can be deployed and managed independently allows for fine-tuning the resource
    allocation. Such small services are called microservices, and many of them can together
    comprise a reactive system that can be both scalable and elastic. We will discuss
    these solutions in more detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Message-driven
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already established that the components' isolation and system distribution
    are two aspects that help to keep the system responsive, resilient, and elastic.
    Loose and flexible connections is an important condition that supports these qualities,
    too. And the asynchronous nature of the reactive system simply does not leave
    the designer any other choice but to build communication between components on
    messages.
  prefs: []
  type: TYPE_NORMAL
- en: It creates a *breathing space* around each component, without which the system
    would be a tightly coupled monolith, susceptible to all kinds of problems, not
    to mention a maintenance nightmare.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we are going to look at the architectural style that can be used
    to build an application as a collection of loosely coupled services that provide
    the required business functionality—microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order for a deployable unit of code to be qualified as a microservice, it
    has to possess the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the source code of one microservice should be smaller than the size
    of a traditional application. Another size criteria is that one programmer's team
    should be able to write and support several of them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has to be deployed independently. Naturally, one microservice typically cooperates
    and expects cooperation from other systems, but that should not prevent our ability
    to deploy it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a microservice uses a database to store data, it has to have its own schema,
    or a set of tables. This statement is still under debate, especially in cases
    when several services modify the same data set or interdependent datasets. If
    the same team owns all of the related services, it is easier to accomplish. Otherwise,
    there are several possible strategies to ensure independent microservice development
    and deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has to be stateless, in the sense that its state should not be kept in memory,
    unless the memory is shared. If one instance of the service has failed, another
    should be able to accomplish what was expected from the service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should provide a way to check its *health*—that the service is up and running
    and ready to do the job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That said, let's look over the field of toolkits for microservice implementation.
    One can definitely write microservices from scratch, but before doing that, it
    is always worth looking at what is out there already, even if you find that nothing
    fits your particular needs.
  prefs: []
  type: TYPE_NORMAL
- en: The two most popular toolkits are Spring Boot ([https://projects.spring.io/spring-boot](https://projects.spring.io/spring-boot))
    and raw J2EE. The J2EE community founded the MicroProfile ([https://microprofile.io](https://microprofile.io))
    initiative, with a declared goal of optimizing Enterprise Java for a microservices
    architecture. KumuluzEE ([https://ee.kumuluz.com](https://ee.kumuluz.com)) is
    a lightweight open source microservice framework, compliant with MicroProfile.
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of some other frameworks, libraries, and toolkits includes the following
    (in alphabetical order):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Akka**: A toolkit for building highly concurrent, distributed, and resilient
    message-driven applications for Java and Scala ([https://akka.io/](https://akka.io/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bootique**: A minimally opinionated framework for runnable Java applications
    ([https://bootique.io/](https://bootique.io/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropwizard**: A Java framework for developing ops-friendly, high-performance,
    RESTful web services ([https://www.dropwizard.io/](https://www.dropwizard.io/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jodd**: A set of Java microframeworks, tools, and utilities, under 1.7 MB
    ([https://jodd.org/](https://jodd.org/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lightbend Lagom**: An opinionated microservice framework built on Akka and
    Play ([https://www.lightbend.com/](https://www.lightbend.com/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ninja**: A fullstack web framework for Java ([http://www.ninjaframework.org/](http://www.ninjaframework.org/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spotify Apollo**: A set of Java libraries used by Spotify for writing microservices
    ([http://spotify.github.io/apollo/](http://spotify.github.io/apollo/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vert.x**: A toolkit for building reactive applications on the JVM ([https://vertx.io/](https://vertx.io/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the frameworks, libraries, and toolkits listed support HTTP/JSON communication
    between microservices. Some of them also have an additional way of sending messages.
    If they do not, any lightweight messaging system can be used. We mention it here
    because, as you may recall, message-driven asynchronous processing is a foundation
    for the elasticity, responsiveness, and resilience of a reactive system composed
    of microservices.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the process of microservice building, we will use Vert.x, an
    event-driven non-blocking lightweight polyglot toolkit (components can be written
    in Java, JavaScript, Groovy, Ruby, Scala, Kotlin, or Ceylon). It supports an asynchronous
    programming model and a distributed event bus that reaches into in-browser JavaScript,
    allowing for the creation of real-time web applications.
  prefs: []
  type: TYPE_NORMAL
- en: Vert.x basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The building block in Vert.x world is a class that implements the `io.vertx.core.Verticle`
    interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: package io.vertx.core;
  prefs: []
  type: TYPE_NORMAL
- en: public interface Verticle {
  prefs: []
  type: TYPE_NORMAL
- en: Vertx getVertx();
  prefs: []
  type: TYPE_NORMAL
- en: void init(Vertx vertx, Context context);
  prefs: []
  type: TYPE_NORMAL
- en: void start(Future<Void> future) throws Exception;
  prefs: []
  type: TYPE_NORMAL
- en: void stop(Future<Void> future) throws Exception;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of the preceding interface is called a verticle. Most of
    the method names of the preceding interface are self-explanatory. The `getVertex()` method
    provides access to the `Vertx` object—the entry point into the Vert.x Core API
    that has methods that allow us to build the following functionalities necessary
    for microservice building:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating DNS clients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating periodic services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating Datagram sockets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying and undeploying verticles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing access to the shared data API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating TCP and HTTP clients and servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing access to the event bus and filesystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the deployed verticles can talk to each other via either standard HTTP
    protocol or using `io.vertx.core.eventbus.EventBus`, forming a system of microservices.
    We will show how one can build a reactive system of microservices using verticles
    and RxJava implementations from the `io.vertx.rxjava` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `Verticle` interface implementation can be created easily, by extending the
    `io.vertx.rxjava.core.AbstractVerticle` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: package io.vertx.rxjava.core;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.core.Vertx;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.core.Context;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.core.AbstractVerticle
  prefs: []
  type: TYPE_NORMAL
- en: public class AbstractVerticle extends AbstractVerticle {
  prefs: []
  type: TYPE_NORMAL
- en: protected io.vertx.rxjava.core.Vertx vertx;
  prefs: []
  type: TYPE_NORMAL
- en: public void init(Vertx vertx, Context context) {
  prefs: []
  type: TYPE_NORMAL
- en: super.init(vertx, context);
  prefs: []
  type: TYPE_NORMAL
- en: this.vertx = new io.vertx.rxjava.core.Vertx(vertx);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the preceding class extends the `io.vertx.core.AbstractVerticle`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: package io.vertx.core;
  prefs: []
  type: TYPE_NORMAL
- en: import java.util.List;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.core.Verticle;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.core.json.JsonObject;
  prefs: []
  type: TYPE_NORMAL
- en: public abstract class AbstractVerticle implements Verticle {
  prefs: []
  type: TYPE_NORMAL
- en: protected Vertx vertx;
  prefs: []
  type: TYPE_NORMAL
- en: protected Context context;
  prefs: []
  type: TYPE_NORMAL
- en: public void init(Vertx vertx, Context context) {
  prefs: []
  type: TYPE_NORMAL
- en: this.vertx = vertx;
  prefs: []
  type: TYPE_NORMAL
- en: this.context = context;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: public Vertx getVertx() { return vertx; }
  prefs: []
  type: TYPE_NORMAL
- en: public JsonObject config() { return context.config(); }
  prefs: []
  type: TYPE_NORMAL
- en: public String deploymentID() { return context.deploymentID(); }
  prefs: []
  type: TYPE_NORMAL
- en: public List<String> processArgs() { return context.processArgs(); }
  prefs: []
  type: TYPE_NORMAL
- en: public void start(Future<Void> startFuture) throws Exception {
  prefs: []
  type: TYPE_NORMAL
- en: start();
  prefs: []
  type: TYPE_NORMAL
- en: startFuture.complete();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: public void stop(Future<Void> stopFuture) throws Exception {
  prefs: []
  type: TYPE_NORMAL
- en: stop();
  prefs: []
  type: TYPE_NORMAL
- en: stopFuture.complete();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: public void start() throws Exception {}
  prefs: []
  type: TYPE_NORMAL
- en: public void stop() throws Exception {}
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, all you need to do is extend the `io.vertx.rxjava.core.AbstractVerticle`
    class and implement the `start()` method. The new verticle will be deployable,
    even without implementing the `start()` method,  but it will do nothing useful.
    The code in the `start()` method is the entry point into the functionality of
    your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Vert.x and execute the examples, the following dependencies have to
    be added to the project:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: <dependency>
  prefs: []
  type: TYPE_NORMAL
- en: <groupId>io.vertx</groupId>
  prefs: []
  type: TYPE_NORMAL
- en: <artifactId>vertx-web</artifactId>
  prefs: []
  type: TYPE_NORMAL
- en: <version>${vertx.version}</version>
  prefs: []
  type: TYPE_NORMAL
- en: </dependency>
  prefs: []
  type: TYPE_NORMAL
- en: <dependency>
  prefs: []
  type: TYPE_NORMAL
- en: <groupId>io.vertx</groupId>
  prefs: []
  type: TYPE_NORMAL
- en: <artifactId>vertx-rx-java</artifactId>
  prefs: []
  type: TYPE_NORMAL
- en: <version>${vertx.version}</version>
  prefs: []
  type: TYPE_NORMAL
- en: </dependency>
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `vertx.version` property can be set in the `properties` section of the `pom.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: <properties>
  prefs: []
  type: TYPE_NORMAL
- en: <vertx.version>3.5.1</vertx.version>
  prefs: []
  type: TYPE_NORMAL
- en: </properties>
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: What makes a verticle reactive is the underlying implementation of an event
    loop (a thread) that receives an event (request) and delivers it to a handler—a
    method in a verticle, or another dedicated class, that is processing this type
    of the event. Programmers typically describe them as functions associated with
    each event type. When a handler returns, the event loop invokes the callback,
    implementing the reactor pattern we talked about in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: For certain types of procedures that are blocking by nature (JDBC calls or long
    computations, for example), a worker verticle can be executed asynchronously,
    not through the event loop (so, not to block it), but by a separate thread, using
    the `vertx.executeBlocking()` method. The golden rule of an event loop-based application
    design is, *Don't Block the Event Loop!* Violating this rule stops the application
    in its tracks.
  prefs: []
  type: TYPE_NORMAL
- en: The HTTP server as a microservice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As an example, here is a verticle that acts as an HTTP server:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: package com.packt.javapath.ch18demo.microservices;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.rxjava.core.AbstractVerticle;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.rxjava.core.http.HttpServer;
  prefs: []
  type: TYPE_NORMAL
- en: public class HttpServer1 extends AbstractVerticle{
  prefs: []
  type: TYPE_NORMAL
- en: private int port;
  prefs: []
  type: TYPE_NORMAL
- en: public HttpServer1(int port) {
  prefs: []
  type: TYPE_NORMAL
- en: this.port = port;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: public void start() throws Exception {
  prefs: []
  type: TYPE_NORMAL
- en: HttpServer server = vertx.createHttpServer();
  prefs: []
  type: TYPE_NORMAL
- en: server.requestStream().toObservable()
  prefs: []
  type: TYPE_NORMAL
- en: .subscribe(request -> request.response()
  prefs: []
  type: TYPE_NORMAL
- en: .end("Hello from " + Thread.currentThread().getName() +
  prefs: []
  type: TYPE_NORMAL
- en: '" on port " + port + "!\n\n"));'
  prefs: []
  type: TYPE_NORMAL
- en: server.rxListen(port).subscribe();
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(Thread.currentThread().getName() +
  prefs: []
  type: TYPE_NORMAL
- en: '" is waiting on port " + port + "...");'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, the server is created, and the stream of data from a
    possible request is wrapped into an `Observable`. The data emitted by the `Observable`
    is passed to the function (a request handler) that processes the request and generates
    a necessary response. We also told the server which port to listen to, and can now
    deploy several instances of this verticle, to listen on different ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: vertx().getDelegate().deployVerticle(new HttpServer1(8082));
  prefs: []
  type: TYPE_NORMAL
- en: vertx().getDelegate().deployVerticle(new HttpServer1(8083));
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also an `io.vertx.rxjava.core.RxHelper` helper class that can be used
    for deployment. It takes care of some of the details that are not important for
    the current discussion:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: RxHelper.deployVerticle(vertx(), new HttpServer1(8082));
  prefs: []
  type: TYPE_NORMAL
- en: RxHelper.deployVerticle(vertx(), new HttpServer1(8083));
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whichever method is used, you will see the following messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: vert.x-eventloop-thread-0 is waiting on port 8082...
  prefs: []
  type: TYPE_NORMAL
- en: vert.x-eventloop-thread-0 is waiting on port 8083...
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'These messages confirm what we had expected: the same event loop thread is
    listening on both ports. We can now place a request to any of the running servers,
    using the standard `curl` command, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: curl localhost:8082
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'The response is going to be the one we hardcoded:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: Hello from vert.x-eventloop-thread-0 on port 8082!
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: Periodic service as a microservice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vert.x also allows us to create a periodic service, which does something at
    a regular interval. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: package com.packt.javapath.ch18demo.microservices;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.rxjava.core.AbstractVerticle;
  prefs: []
  type: TYPE_NORMAL
- en: import java.time.LocalTime;
  prefs: []
  type: TYPE_NORMAL
- en: import java.time.temporal.ChronoUnit;
  prefs: []
  type: TYPE_NORMAL
- en: public class PeriodicService1 extends AbstractVerticle {
  prefs: []
  type: TYPE_NORMAL
- en: public void start() throws Exception {
  prefs: []
  type: TYPE_NORMAL
- en: LocalTime start = LocalTime.now();
  prefs: []
  type: TYPE_NORMAL
- en: vertx.setPeriodic(1000, v-> {
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println("Beep!");
  prefs: []
  type: TYPE_NORMAL
- en: if(ChronoUnit.SECONDS.between(start, LocalTime.now()) > 3 ){
  prefs: []
  type: TYPE_NORMAL
- en: vertx.undeploy(deploymentID());
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '});'
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println("Vertical PeriodicService1 is deployed");
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: public void stop() throws Exception {
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println("Vertical PeriodicService1 is un-deployed");
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, this vertical, once deployed, prints the `Beep!` message every
    second, and after three seconds, is automatically un-deployed. If we deploy this
    vertical, we will see:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: Vertical PeriodicService1 is deployed
  prefs: []
  type: TYPE_NORMAL
- en: Beep!
  prefs: []
  type: TYPE_NORMAL
- en: Beep!
  prefs: []
  type: TYPE_NORMAL
- en: Beep!
  prefs: []
  type: TYPE_NORMAL
- en: Beep!
  prefs: []
  type: TYPE_NORMAL
- en: Vertical PeriodicService1 is un-deployed
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: The first `Beep!` comes out when the vertical starts, then there are three more
    messages every second, and the vertical is un-deployed, as was expected.
  prefs: []
  type: TYPE_NORMAL
- en: The HTTP client as a microservice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use periodic service verticals to send messages to the server vertical
    using the HTTP protocol. In order to do it, we need a new dependency, so we can
    use the `WebClient` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: <dependency>
  prefs: []
  type: TYPE_NORMAL
- en: <groupId>io.vertx</groupId>
  prefs: []
  type: TYPE_NORMAL
- en: <artifactId>vertx-web-client</artifactId>
  prefs: []
  type: TYPE_NORMAL
- en: <version>${vertx.version}</version>
  prefs: []
  type: TYPE_NORMAL
- en: </dependency>
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, the periodic service that sends messages to the HTTP server vertical
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: package com.packt.javapath.ch18demo.microservices;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.rxjava.core.AbstractVerticle;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.rxjava.core.buffer.Buffer;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.rxjava.ext.web.client.HttpResponse;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.rxjava.ext.web.client.WebClient;
  prefs: []
  type: TYPE_NORMAL
- en: import rx.Single;
  prefs: []
  type: TYPE_NORMAL
- en: import java.time.LocalTime;
  prefs: []
  type: TYPE_NORMAL
- en: import java.time.temporal.ChronoUnit;
  prefs: []
  type: TYPE_NORMAL
- en: public class PeriodicService2 extends AbstractVerticle {
  prefs: []
  type: TYPE_NORMAL
- en: private int port;
  prefs: []
  type: TYPE_NORMAL
- en: public PeriodicService2(int port) {
  prefs: []
  type: TYPE_NORMAL
- en: this.port = port;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: public void start() throws Exception {
  prefs: []
  type: TYPE_NORMAL
- en: WebClient client = WebClient.create(vertx);
  prefs: []
  type: TYPE_NORMAL
- en: Single<HttpResponse<Buffer>> single = client
  prefs: []
  type: TYPE_NORMAL
- en: .get(port, "localhost", "?name=Nick")
  prefs: []
  type: TYPE_NORMAL
- en: .rxSend();
  prefs: []
  type: TYPE_NORMAL
- en: LocalTime start = LocalTime.now();
  prefs: []
  type: TYPE_NORMAL
- en: vertx.setPeriodic(1000, v-> {
  prefs: []
  type: TYPE_NORMAL
- en: single.subscribe(r-> System.out.println(r.bodyAsString()),
  prefs: []
  type: TYPE_NORMAL
- en: Throwable::printStackTrace);
  prefs: []
  type: TYPE_NORMAL
- en: if(ChronoUnit.SECONDS.between(start, LocalTime.now()) >= 3 ){
  prefs: []
  type: TYPE_NORMAL
- en: client.close();
  prefs: []
  type: TYPE_NORMAL
- en: vertx.undeploy(deploymentID());
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println("Vertical PeriodicService2 undeployed");
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '});'
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println("Vertical PeriodicService2 deployed");
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this periodic service accepts the port number as a parameter
    of its constructor, then sends a message to this port on the localhost every second,
    and un-deploys itself after three seconds. The message is the value of the `name` parameter.
    By default, it is the GET request.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also modify our server vertical to read the value of the `name` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: public void start() throws Exception {
  prefs: []
  type: TYPE_NORMAL
- en: HttpServer server = vertx.createHttpServer();
  prefs: []
  type: TYPE_NORMAL
- en: server.requestStream().toObservable()
  prefs: []
  type: TYPE_NORMAL
- en: .subscribe(request -> request.response()
  prefs: []
  type: TYPE_NORMAL
- en: .end("Hi, " + request.getParam("name") + "! Hello from " +
  prefs: []
  type: TYPE_NORMAL
- en: Thread.currentThread().getName() + " on port " + port + "!"));
  prefs: []
  type: TYPE_NORMAL
- en: server.rxListen(port).subscribe();
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(Thread.currentThread().getName()
  prefs: []
  type: TYPE_NORMAL
- en: + " is waiting on port " + port + "...");
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can deploy both verticals:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: RxHelper.deployVerticle(vertx(), new HttpServer2(8082));
  prefs: []
  type: TYPE_NORMAL
- en: RxHelper.deployVerticle(vertx(), new PeriodicService2(8082));
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: Vertical PeriodicService2 deployed
  prefs: []
  type: TYPE_NORMAL
- en: vert.x-eventloop-thread-0 is waiting on port 8082...
  prefs: []
  type: TYPE_NORMAL
- en: Hi, Nick! Hello from vert.x-eventloop-thread-0 on port 8082!
  prefs: []
  type: TYPE_NORMAL
- en: Hi, Nick! Hello from vert.x-eventloop-thread-0 on port 8082!
  prefs: []
  type: TYPE_NORMAL
- en: Vertical PeriodicService2 undeployed
  prefs: []
  type: TYPE_NORMAL
- en: Hi, Nick! Hello from vert.x-eventloop-thread-0 on port 8082!
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: Other microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In principle, the whole system of microservices can be built based on the messages
    sent using the HTTP protocol, with each microservice implemented as an HTTP server
    or having an HTTP server as the front for the message exchange. Alternatively,
    any other messaging system can be used for the communication.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of Vert.x, it has its own messaging system-based on the event bus.
    In the next section, we will demonstrate it and use it as an illustration of how
    a reactive system may look.
  prefs: []
  type: TYPE_NORMAL
- en: The size of our sample microservices may leave the impression that microservices
    have to be as fine-grained as object methods. In some cases, it is worth considering
    whether a particular method needs to be scaled, for example. The truth is, this
    architectural style is novel enough to allow for definite size recommendations,
    and the existing frameworks, libraries, and toolkits are flexible enough to support
    practically any size of independently deployable services. Well, if a deployable,
    independent service is as big as a traditional application, then it probably won't
    be called a microservice, but *an external system*, or something similar.
  prefs: []
  type: TYPE_NORMAL
- en: Reactive systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Those familiar with the concept of **event-driven architecture** (**EDA**) may
    have noticed that it closely resembles the idea of a reactive system. Their descriptions
    use very similar language and diagrams. The difference is that EDA deals with
    only one aspect of a software system—the architecture. The idea of a reactive
    system, on the other hand, is more about the code style and execution flow, including
    an emphasis on using asynchronous data streams, for example. So, a reactive system
    can have EDA, and EDA can be implemented as a reactive system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at another set of examples that provides a glimpse into how a reactive
    system may look, if implemented using Vert.x. Notice that the Vert.x API has two
    source trees: one starts with `io.vertx.core`, and the other with `io.vertx.rxjava`.
    Since we are discussing reactive programming, we are going to use packages under `io.vertx.rxjava`,
    called the rx-fied Vert.x API.'
  prefs: []
  type: TYPE_NORMAL
- en: Message-driven system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vert.x has a feature that directly supports both the message-driven architecture
    and EDA. It is called an event bus. Any verticle has access to the event bus, and
    can send any message to any address (which is just a string) using the `io.vertx.core.eventbus.EventBus` class,
    or its cousin, `io.vertx.rxjava.core.eventbus.EventBus`. We are only going to
    use the latter, but similar (not rx-fied) functionality is available in `io.vertx.core.eventbus.EventBus`,
    too. One, or several, verticles can register themselves as a message consumer
    for a certain address. If several verticles are consumers for the same address,
    then the `rxSend()` method of `EventBus` delivers the message only to one of these
    consumers, using a round-robin algorithm to pick the receiver of the next message.
    Alternatively, the `publish()` method, as you would expect, delivers the message
    to all consumers with the same address. Here is the code that sends the message
    to the specified address:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: vertx.eventBus().rxSend(address, msg).subscribe(reply ->
  prefs: []
  type: TYPE_NORMAL
- en: 'System.out.println("Got reply: " + reply.body()),'
  prefs: []
  type: TYPE_NORMAL
- en: Throwable::printStackTrace );
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `rxSend()` method returns the `Single<Message>` object that represents
    a message that can be received, and the `subscribe()` method to ... well... subscribe
    to it. The `Single<Message>` class implements the reactive pattern for a single
    value response. The `subscribe()` method accepts two `Consumer` functions: the
    first processes the reply,  the second processes the error. In the preceding code,
    the first function just prints the reply:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: 'reply -> System.out.println("Got reply: " + reply.body())'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second action prints the stack trace of the exception, if it happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: Throwable::printStackTrace
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you already know, the preceding construct is called a method reference.
    The same function, as a lambda expression, would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: e -> e.printStackTrace()
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'A call to the `publish()` method looks similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: vertx.eventBus().publish(address, msg)
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: It publishes the message to many consumers potentially, so the method does not
    return a `Single` object, or any other object that can be used to get a reply.
    Instead, it just returns an `EventBus` object; if needed, more event bus methods
    can be called.
  prefs: []
  type: TYPE_NORMAL
- en: Message consumer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The message consumer in Vert.x is a verticle that registers with the event
    bus as a potential receiver of the messages sent or published to the specified
    address:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: package com.packt.javapath.ch18demo.reactivesystem;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.rxjava.core.AbstractVerticle;
  prefs: []
  type: TYPE_NORMAL
- en: public class MsgConsumer extends AbstractVerticle {
  prefs: []
  type: TYPE_NORMAL
- en: private String address, name;
  prefs: []
  type: TYPE_NORMAL
- en: public MsgConsumer(String id, String address) {
  prefs: []
  type: TYPE_NORMAL
- en: this.address = address;
  prefs: []
  type: TYPE_NORMAL
- en: this.name = this.getClass().getSimpleName() +
  prefs: []
  type: TYPE_NORMAL
- en: '"(" + id + "," + address + ")";'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: public void start() throws Exception {
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(name + " starts...");
  prefs: []
  type: TYPE_NORMAL
- en: vertx.eventBus().consumer(address).toObservable()
  prefs: []
  type: TYPE_NORMAL
- en: .subscribe(msg -> {
  prefs: []
  type: TYPE_NORMAL
- en: 'String reply = name + " got message: " + msg.body();'
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(reply);
  prefs: []
  type: TYPE_NORMAL
- en: if ("undeploy".equals(msg.body())) {
  prefs: []
  type: TYPE_NORMAL
- en: vertx.undeploy(deploymentID());
  prefs: []
  type: TYPE_NORMAL
- en: reply = name + " undeployed.";
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(reply);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: msg.reply(reply);
  prefs: []
  type: TYPE_NORMAL
- en: '}, Throwable::printStackTrace );'
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(Thread.currentThread().getName()
  prefs: []
  type: TYPE_NORMAL
- en: + " is waiting on address " + address + "...");
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `consumer(address)` method returns an object of `io.vertx.rxjava.core.eventbus.MessageConsumer<T>`,
    which represents a stream of messages to the provided address. This means that
    one can convert the stream into an `Observable` and subscribe to it to receive
    all of the messages sent to this address. The `subscribe()` method of the `Observable`
    object accepts two `Consumer` functions: the first processes the received message,
    and the second is executed when an error happens. In the first function, we have
    included the `msg.reply(reply)` method, which sends the message back to the source
    of the message. And you probably remember that the sender is able to get this
    reply if the original message was sent by the `rxSend()` method. If the `publish()` method
    was used instead, then the reply sent by the `msg.reply(reply)` method goes nowhere.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, notice that when the `undeploy` message is received, the message consumer
    undeploys itself. This method is usually only used during automatic deployment,
    when the old version is replaced by the newer one without shutting down the system.
  prefs: []
  type: TYPE_NORMAL
- en: Because we are going to deploy several message consumers with the same address
    for the demonstration, we have added the `id` parameter and included it in the `name` value.
    This value serves as a prefix in all of the messages, so we can trace how the
    message propagates across the system.
  prefs: []
  type: TYPE_NORMAL
- en: You have probably realized that the preceding implementation is just a shell
    that can be used to invoke some useful functionalities. The received message can
    be a command to do something, data to be processed, data to be stored in the database,
    or anything else. And the reply can be an acknowledgment that the message was
    received, or some other expected result. If the latter is the case, the processing
    should be very fast, to avoid blocking the event loop (remember the golden rule).
    If the processing cannot be done quickly, the replay can also be a callback token,
    used by the sender later to retrieve the result.
  prefs: []
  type: TYPE_NORMAL
- en: Message sender
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The message sender we will demonstrate is based on the HTTP server implementation
    we demonstrated in the *Microservices* section. It is not necessary to do it this
    way. In real-life code, a vertical typically sends messages automatically, either
    to get data it needs, to provide data some other vertical wants, to notify another
    vertical, to store data in the database, or for any other reason. But for demonstration
    purposes, we have decided that the sender will listen to some port for messages,
    and we will send it messages manually (using the `curl` command) or automatically,
    via some periodic service described in the *Microservices* section. That is why
    the message sender looks a bit more complex than the message consumer:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: package com.packt.javapath.ch18demo.reactivesystem;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.rxjava.core.AbstractVerticle;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.rxjava.core.http.HttpServer;
  prefs: []
  type: TYPE_NORMAL
- en: public class EventBusSend extends AbstractVerticle {
  prefs: []
  type: TYPE_NORMAL
- en: private int port;
  prefs: []
  type: TYPE_NORMAL
- en: private String address, name;
  prefs: []
  type: TYPE_NORMAL
- en: public EventBusSend(int port, String address) {
  prefs: []
  type: TYPE_NORMAL
- en: this.port = port;
  prefs: []
  type: TYPE_NORMAL
- en: this.address = address;
  prefs: []
  type: TYPE_NORMAL
- en: this.name = this.getClass().getSimpleName() +
  prefs: []
  type: TYPE_NORMAL
- en: '"(port " + port + ", send to " + address + ")";'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: public void start() throws Exception {
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(name + " starts...");
  prefs: []
  type: TYPE_NORMAL
- en: HttpServer server = vertx.createHttpServer();
  prefs: []
  type: TYPE_NORMAL
- en: server.requestStream().toObservable().subscribe(request -> {
  prefs: []
  type: TYPE_NORMAL
- en: String msg = request.getParam("msg");
  prefs: []
  type: TYPE_NORMAL
- en: request.response().setStatusCode(200).end();
  prefs: []
  type: TYPE_NORMAL
- en: vertx.eventBus().rxSend(address, msg).subscribe(reply -> {
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(name + " got reply:\n  " + reply.body());
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: e -> {
  prefs: []
  type: TYPE_NORMAL
- en: if(StringUtils.contains(e.toString(), "NO_HANDLERS")){
  prefs: []
  type: TYPE_NORMAL
- en: vertx.undeploy(deploymentID());
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(name + " undeployed.");
  prefs: []
  type: TYPE_NORMAL
- en: '} else {'
  prefs: []
  type: TYPE_NORMAL
- en: e.printStackTrace();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}); });'
  prefs: []
  type: TYPE_NORMAL
- en: server.rxListen(port).subscribe();
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(Thread.currentThread().getName()
  prefs: []
  type: TYPE_NORMAL
- en: + " is waiting on port " + port + "...");
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the preceding code is related to the HTTP server functionality. The
    few lines that send the message (received by the HTTP server) are these ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: vertx.eventBus().rxSend(address, msg).subscribe(reply -> {
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(name + " got reply:\n  " + reply.body());
  prefs: []
  type: TYPE_NORMAL
- en: '}, e -> {'
  prefs: []
  type: TYPE_NORMAL
- en: if(StringUtils.contains(e.toString(), "NO_HANDLERS")){
  prefs: []
  type: TYPE_NORMAL
- en: vertx.undeploy(deploymentID());
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(name + " undeployed.");
  prefs: []
  type: TYPE_NORMAL
- en: '} else {'
  prefs: []
  type: TYPE_NORMAL
- en: e.printStackTrace();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '});'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: After the message is sent, the sender subscribes to the possible reply and prints
    it (if the reply was received). If an error happens (an exception is thrown during
    message sending), we can check whether the exception (converted to the `String`
    value) contains literal `NO_HANDLERS`, and un-deploy the sender if so. It took
    us a while to figure out how to identify the case when there are no consumers
    assigned to the address, for which this sender is dedicated to sending messages.
    If there are no consumers (all are un-deployed, most likely), then there is no
    need for the sender, so we un-deploy it.
  prefs: []
  type: TYPE_NORMAL
- en: It is a good practice to clean up and un-deploy all of the verticles that are
    not needed anymore. But if you run the verticles in IDE, chances are, all the
    verticles are stopped as soon as you stop the main process (that has created the
    verticles) in your IDE. If not, run the `jcmd` command and see whether there are
    still Vert.x verticles running. The first number for each of the listed processes
    is the process ID. Identify the verticles that you do not need anymore and use
    the `kill -9  <process ID>` command to stop them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s deploy two message consumers, and send them messages via our message
    sender:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: String address = "One";
  prefs: []
  type: TYPE_NORMAL
- en: Vertx vertx = vertx();
  prefs: []
  type: TYPE_NORMAL
- en: RxHelper.deployVerticle(vertx, new MsgConsumer("1",address));
  prefs: []
  type: TYPE_NORMAL
- en: RxHelper.deployVerticle(vertx, new MsgConsumer("2",address));
  prefs: []
  type: TYPE_NORMAL
- en: RxHelper.deployVerticle(vertx, new EventBusSend(8082, address));
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'After you run the preceding code, the Terminal shows the following messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: MsgConsumer(1,One) starts...
  prefs: []
  type: TYPE_NORMAL
- en: MsgConsumer(2,One) starts...
  prefs: []
  type: TYPE_NORMAL
- en: EventBusSend(port 8082, send to One) starts...
  prefs: []
  type: TYPE_NORMAL
- en: vert.x-eventloop-thread-1 is waiting on address One...
  prefs: []
  type: TYPE_NORMAL
- en: vert.x-eventloop-thread-0 is waiting on address One...
  prefs: []
  type: TYPE_NORMAL
- en: vert.x-eventloop-thread-2 is waiting on port 8082...
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: Notice the different event loops running to support each verticle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s send a few messages, using the following commands from a terminal
    window:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: curl localhost:8082?msg=Hello!
  prefs: []
  type: TYPE_NORMAL
- en: curl localhost:8082?msg=Hi!
  prefs: []
  type: TYPE_NORMAL
- en: curl localhost:8082?msg=How+are+you?
  prefs: []
  type: TYPE_NORMAL
- en: curl localhost:8082?msg=Just+saying...
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'The plus sign (`+`) is necessary because a URL cannot contain spaces and has
    to be *encoded*, which means, among other things, replacing spaces with plus sign
    `+` or `%20`. In response to the preceding commands, we will see the following
    messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: 'MsgConsumer(2,One) got message: Hello!'
  prefs: []
  type: TYPE_NORMAL
- en: 'EventBusSend(port 8082, send to One) got reply:'
  prefs: []
  type: TYPE_NORMAL
- en: 'MsgConsumer(2,One) got message: Hello!'
  prefs: []
  type: TYPE_NORMAL
- en: 'MsgConsumer(1,One) got message: Hi!'
  prefs: []
  type: TYPE_NORMAL
- en: 'EventBusSend(port 8082, send to One) got reply:'
  prefs: []
  type: TYPE_NORMAL
- en: 'MsgConsumer(1,One) got message: Hi!'
  prefs: []
  type: TYPE_NORMAL
- en: 'MsgConsumer(2,One) got message: How are you?'
  prefs: []
  type: TYPE_NORMAL
- en: 'EventBusSend(port 8082, send to One) got reply:'
  prefs: []
  type: TYPE_NORMAL
- en: 'MsgConsumer(2,One) got message: How are you?'
  prefs: []
  type: TYPE_NORMAL
- en: 'MsgConsumer(1,One) got message: Just saying...'
  prefs: []
  type: TYPE_NORMAL
- en: 'EventBusSend(port 8082, send to One) got reply:'
  prefs: []
  type: TYPE_NORMAL
- en: 'MsgConsumer(1,One) got message: Just saying...'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, the consumers received messages by turns, according to the round-robin
    algorithm. Now, let''s deploy all of the verticles:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: curl localhost:8082?msg=undeploy
  prefs: []
  type: TYPE_NORMAL
- en: curl localhost:8082?msg=undeploy
  prefs: []
  type: TYPE_NORMAL
- en: curl localhost:8082?msg=undeploy
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the messages displayed in response to the preceding commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: 'MsgConsumer(1,One) got message: undeploy'
  prefs: []
  type: TYPE_NORMAL
- en: MsgConsumer(1,One) undeployed.
  prefs: []
  type: TYPE_NORMAL
- en: 'EventBusSend(port 8082, send to One) got reply:'
  prefs: []
  type: TYPE_NORMAL
- en: MsgConsumer(1,One) undeployed.
  prefs: []
  type: TYPE_NORMAL
- en: 'MsgConsumer(2,One) got message: undeploy'
  prefs: []
  type: TYPE_NORMAL
- en: MsgConsumer(2,One) undeployed.
  prefs: []
  type: TYPE_NORMAL
- en: 'EventBusSend(port 8082, send to One) got reply:'
  prefs: []
  type: TYPE_NORMAL
- en: MsgConsumer(2,One) undeployed.
  prefs: []
  type: TYPE_NORMAL
- en: EventBusSend(port 8082, send to One) undeployed.
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the preceding messages, all of our verticals are un-deployed.
    If we submit the `undeploy` message again, we will see:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: curl localhost:8082?msg=undeploy
  prefs: []
  type: TYPE_NORMAL
- en: 'curl: (7) Failed to connect to localhost port 8082: Connection refused'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: That is because the sender is undeployed, and there is no HTTP server that listens
    to port `8082` of the localhost.
  prefs: []
  type: TYPE_NORMAL
- en: Message publisher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We implemented the message publisher very similarly to the message sender:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: package com.packt.javapath.ch18demo.reactivesystem;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.rxjava.core.AbstractVerticle;
  prefs: []
  type: TYPE_NORMAL
- en: import io.vertx.rxjava.core.http.HttpServer;
  prefs: []
  type: TYPE_NORMAL
- en: public class EventBusPublish extends AbstractVerticle {
  prefs: []
  type: TYPE_NORMAL
- en: private int port;
  prefs: []
  type: TYPE_NORMAL
- en: private String address, name;
  prefs: []
  type: TYPE_NORMAL
- en: public EventBusPublish(int port, String address) {
  prefs: []
  type: TYPE_NORMAL
- en: this.port = port;
  prefs: []
  type: TYPE_NORMAL
- en: this.address = address;
  prefs: []
  type: TYPE_NORMAL
- en: this.name = this.getClass().getSimpleName() +
  prefs: []
  type: TYPE_NORMAL
- en: '"(port " + port + ", publish to " + address + ")";'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: public void start() throws Exception {
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(name + " starts...");
  prefs: []
  type: TYPE_NORMAL
- en: HttpServer server = vertx.createHttpServer();
  prefs: []
  type: TYPE_NORMAL
- en: server.requestStream().toObservable()
  prefs: []
  type: TYPE_NORMAL
- en: .subscribe(request -> {
  prefs: []
  type: TYPE_NORMAL
- en: String msg = request.getParam("msg");
  prefs: []
  type: TYPE_NORMAL
- en: request.response().setStatusCode(200).end();
  prefs: []
  type: TYPE_NORMAL
- en: vertx.eventBus().publish(address, msg);
  prefs: []
  type: TYPE_NORMAL
- en: if ("undeploy".equals(msg)) {
  prefs: []
  type: TYPE_NORMAL
- en: vertx.undeploy(deploymentID());
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(name + " undeployed.");
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '});'
  prefs: []
  type: TYPE_NORMAL
- en: server.rxListen(port).subscribe();
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(Thread.currentThread().getName()
  prefs: []
  type: TYPE_NORMAL
- en: + " is waiting on port " + port + "...");
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'The publisher differs from the sender only by this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: vertx.eventBus().publish(address, msg);
  prefs: []
  type: TYPE_NORMAL
- en: if ("undeploy".equals(msg)) {
  prefs: []
  type: TYPE_NORMAL
- en: vertx.undeploy(deploymentID());
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println(name + " undeployed.");
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since one cannot get a reply while publishing, the preceding code is much simpler
    than the message-sending code. Also, since all of the consumers recieve the `undeploy` message
    at the same time, we can assume that they are all going to be un-deployed, and
    the publisher can un-deploy itself. Let''s test it by running the following program:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: String address = "One";
  prefs: []
  type: TYPE_NORMAL
- en: Vertx vertx = vertx();
  prefs: []
  type: TYPE_NORMAL
- en: RxHelper.deployVerticle(vertx, new MsgConsumer("1",address));
  prefs: []
  type: TYPE_NORMAL
- en: RxHelper.deployVerticle(vertx, new MsgConsumer("2",address));
  prefs: []
  type: TYPE_NORMAL
- en: RxHelper.deployVerticle(vertx, new EventBusPublish(8082, address));
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'In response to the preceding code execution, we get the following messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: MsgConsumer(1,One) starts...
  prefs: []
  type: TYPE_NORMAL
- en: MsgConsumer(2,One) starts...
  prefs: []
  type: TYPE_NORMAL
- en: EventBusPublish(port 8082, publish to One) starts...
  prefs: []
  type: TYPE_NORMAL
- en: vert.x-eventloop-thread-2 is waiting on port 8082...
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we issue the following command in another terminal window:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: curl localhost:8082?msg=Hello!
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'The messages in the terminal window where the verticals are running are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: 'MsgConsumer(1,One) got message: Hello!'
  prefs: []
  type: TYPE_NORMAL
- en: 'MsgConsumer(2,One) got message: Hello!'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, both consumers with the same address receive the same message.
    Now, let''s un-deploy them:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: curl localhost:8082?msg=undeploy
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'The verticals respond with these messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: 'MsgConsumer(1,One) got message: undeploy'
  prefs: []
  type: TYPE_NORMAL
- en: 'MsgConsumer(2,One) got message: undeploy'
  prefs: []
  type: TYPE_NORMAL
- en: EventBusPublish(port 8082, publish to One) undeployed.
  prefs: []
  type: TYPE_NORMAL
- en: MsgConsumer(1,One) undeployed.
  prefs: []
  type: TYPE_NORMAL
- en: MsgConsumer(2,One) undeployed.
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we submit the `undeploy` message again, we will see:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: curl localhost:8082?msg=undeploy
  prefs: []
  type: TYPE_NORMAL
- en: 'curl: (7) Failed to connect to localhost port 8082: Connection refused'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have completed our demonstration of a reactive system composed
    of microservices. Adding methods and classes that do something useful will make
    it closer to the real-life system. But we will leave that as an exercise for the
    reader.
  prefs: []
  type: TYPE_NORMAL
- en: Reality check
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have run all of the previous examples in one JVM process. If necessary, Vert.x
    instances can be deployed in different JVM processes and clustered by adding the
    `-cluster` option to the `run` command, when the verticals are deployed not from
    IDE, but from a command line. Clustered verticals share the event bus, and the
    addresses are visible to all Vert.x instances. This way, more message consumers
    can be deployed if the consumers of certain addresses cannot process the requests
    (messages) in a timely manner.
  prefs: []
  type: TYPE_NORMAL
- en: Other frameworks that we mentioned earlier have similar capabilities. They make
    microservice creation easy, and may encourage breaking the application into tiny,
    single-method operations, with the expectation of assembling a very resilient
    and responsive system. However, these are not the only criteria of good software.
    System decomposition increases the complexity of its deployment. Also, if one
    development team is responsible for many microservices, the complexity of versioning
    so many pieces in different stages (development, test, integration test, certification,
    staging, and production) may lead to confusion. The deployment process may become
    so complex that slowing down the rate of changes is necessary to keep the system
    in sync with the market requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the developing of microservices, many other aspects have to
    be addressed to support a reactive system:'
  prefs: []
  type: TYPE_NORMAL
- en: A monitoring system has to be set up to provide insight into the state of the
    application, but its development should not be so complex as to pull the development
    resources away from the main application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerts have to be installed to warn the team about possible and actual issues
    in a timely manner, so they can be addressed before affecting the business.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If possible, self-correcting automated processes have to be implemented. For
    example, the retry logic has to be implemented, with a reasonable upper limit
    of attempts before declaring a failure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A layer of circuit breakers has to protect the system from the domino effect
    when the failure of one component deprives other components of necessary resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An embedded testing system should be able to introduce disruptions and simulate
    load increases to ensure that the application's resilience and responsiveness
    do not degrade over time. For example, the Netflix team has introduced a *chaos
    monkey*—a system that is able to shut down various parts of the production system
    and test its ability to recover. They use it even in production, because a production
    environment has a specific configuration, and no test in another environment can
    guarantee that all possible issues are found.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you have probably realized by now, before committing to a reactive system,
    the team has to weigh all the pros and cons to understand exactly why they need
    a reactive system, and the price of its development. It is an old adage that *no
    value can be added for free*. The awesome power of reactive systems comes with
    a corresponding growth of complexity, not only during development, but also during
    system tuning and maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, if the traditional systems cannot solve the processing problem that you
    face, or if you're passionate about everything reactive and love the concept,
    by all means, go for it. The ride will be challenging, but the reward will be
    worth it. As another old adage states, *what is easily achievable is not worth
    the effort*.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise – Creating io.reactivex.Observable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Write code that demonstrates several ways to create `io.reactivex.Observable`.
    In each example, subscribe to the created `Observable` object and print the emitted
    values.
  prefs: []
  type: TYPE_NORMAL
- en: We did not discuss this so you will need to study the RxJava2 API and look up
    examples on the internet.
  prefs: []
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are six of the methods that allow you to create `io.reactivex.Observable`:'
  prefs: []
  type: TYPE_NORMAL
- en: '```java'
  prefs: []
  type: TYPE_NORMAL
- en: //1
  prefs: []
  type: TYPE_NORMAL
- en: 'Observable.just("Hi!").subscribe(System.out::println); //prints: Hi!'
  prefs: []
  type: TYPE_NORMAL
- en: //2
  prefs: []
  type: TYPE_NORMAL
- en: Observable.fromIterable(List.of("1","2","3"))
  prefs: []
  type: TYPE_NORMAL
- en: '.subscribe(System.out::print); //prints: 123'
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println();
  prefs: []
  type: TYPE_NORMAL
- en: //3
  prefs: []
  type: TYPE_NORMAL
- en: String[] arr = {"1","2","3"};
  prefs: []
  type: TYPE_NORMAL
- en: 'Observable.fromArray(arr).subscribe(System.out::print); //prints: 123'
  prefs: []
  type: TYPE_NORMAL
- en: System.out.println();
  prefs: []
  type: TYPE_NORMAL
- en: //4
  prefs: []
  type: TYPE_NORMAL
- en: Observable.fromCallable(()->123)
  prefs: []
  type: TYPE_NORMAL
- en: '.subscribe(System.out::println); //prints: 123'
  prefs: []
  type: TYPE_NORMAL
- en: //5
  prefs: []
  type: TYPE_NORMAL
- en: ExecutorService pool = Executors.newSingleThreadExecutor();
  prefs: []
  type: TYPE_NORMAL
- en: Future<String> future = pool
  prefs: []
  type: TYPE_NORMAL
- en: .submit(() -> {
  prefs: []
  type: TYPE_NORMAL
- en: Thread.sleep(100);
  prefs: []
  type: TYPE_NORMAL
- en: return "Hi!";
  prefs: []
  type: TYPE_NORMAL
- en: '});'
  prefs: []
  type: TYPE_NORMAL
- en: Observable.fromFuture(future)
  prefs: []
  type: TYPE_NORMAL
- en: '.subscribe(System.out::println); //prints: Hi!'
  prefs: []
  type: TYPE_NORMAL
- en: pool.shutdown();
  prefs: []
  type: TYPE_NORMAL
- en: //6
  prefs: []
  type: TYPE_NORMAL
- en: Observable.interval(100, TimeUnit.MILLISECONDS)
  prefs: []
  type: TYPE_NORMAL
- en: .subscribe(v->System.out.println("100 ms is over"));
  prefs: []
  type: TYPE_NORMAL
- en: //prints twice "100 ms is over"
  prefs: []
  type: TYPE_NORMAL
- en: try { //this pause gives the above method a chance to print the message
  prefs: []
  type: TYPE_NORMAL
- en: TimeUnit.MILLISECONDS.sleep(200);
  prefs: []
  type: TYPE_NORMAL
- en: '} catch (InterruptedException e) {'
  prefs: []
  type: TYPE_NORMAL
- en: e.printStackTrace();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this last chapter of the book, we have provided the reader with a glimpse
    into real-life professional programming and a short overview of the challenges
    of the trade. We have revisited many of the modern terms used in relation to big
    data processing using highly-scalable responsive and resilient reactive systems
    that are able to solve the challenging processing, problems of the modern age.
    We have even provided code examples of such systems, which may serve as the first
    step for your real-life project.
  prefs: []
  type: TYPE_NORMAL
- en: We hope that you remain curious, that you will continue studying and experimenting,
    and that you will eventually build a system that will solve a real problem and
    bring more happiness to the world.
  prefs: []
  type: TYPE_NORMAL
