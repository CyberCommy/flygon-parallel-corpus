- en: CUDA Thread Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CUDA has a hierarchical thread architecture so that we can control CUDA threads
    in groups. Understanding how they work in parallel on a GPU helps you to write
    parallel programming code and achieve better performance. In this chapter, we
    will cover CUDA thread operations and their relationship with GPU resources. As
    a practical experience, we will investigate the parallel reduction algorithm and
    see how we can optimize CUDA code by using optimization strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how CUDA threads operate in a GPU: parallel
    and concurrent thread execution, warp execution, memory bandwidth issues, control
    overheads, SIMD operation, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical CUDA thread operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding CUDA occupancy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sharing across multiple CUDA threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying an application's performance limiter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimizing the CUDA warp divergence effect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing memory utilization and grid-stride loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cooperative Groups for flexible thread handling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warp synchronous programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-/mixed-precision operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter recommends using an NVIDIA GPU card later than Pascal architecture.
    In other words, your GPU's compute capability should be equal to or greater than
    60\. If you are unsure of your GPU's architecture, please visit NVIDIA's GPU site
    at [https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus),
    and confirm your GPU's compute capability.
  prefs: []
  type: TYPE_NORMAL
- en: Sample code was developed and tested with 10.1 when we wrote this book. In general,
    it is recommended to use the latest CUDA version if applicable.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll perform CUDA programming by profiling the code. If your
    GPU architecture is Turing, it is recommended to install Nsight Compute to profile
    the code. It is free, and you can download it from [https://developer.nvidia.com/nsight-compute](https://developer.nvidia.com/nsight-compute).
    When we wrote this book, it was a transition moment of the profiler. You can learn
    about its basic usage in the *Profiling Kernel with Nsight Compute* section in
    [Chapter 5](ea24897f-252a-4e76-81e3-b5d5ff645bb6.xhtml), *CUDA Application Profiling
    and Debugging*.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA threads, blocks, and the GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic working unit in CUDA programming is the CUDA thread. The basic CUDA
    thread execution model is **Single Instruction and Multiple Thread** (**SIMT**).
    In other words, the body of the kernel function is working descriptions of a single
    CUDA thread. But, CUDA architecture executes multiple CUDA threads having the
    same actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conceptually, multiple CUDA threads work in parallel in a group. CUDA thread
    blocks are collections of multiple CUDA threads. Multiple thread blocks operate
    concurrently with each other. We call a group of thread blocks a grid. The following
    diagram shows their relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60928263-1c45-4083-8d5a-0b549796024d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These hierarchical CUDA thread operations match the hierarchical CUDA architecture.
    When we launch a CUDA kernel, one or multiple CUDA thread blocks execute on each
    streaming multiprocessor in the GPU. Also, a streaming multiprocessor can run
    multiple thread blocks depending on resource availability. The number of threads
    in a thread block varies, and the number of blocks in a grid does too:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54d3c5aa-a4c2-4418-83e2-1ae060c8df0f.png)'
  prefs: []
  type: TYPE_IMG
- en: The streaming multiprocessors executes thread blocks arbitrarily and concurrently,
    executing as many as the GPU resources can afford. Therefore, the number of thread
    blocks executable in parallel varies depending on how much of the GPU's resources
    the block requires and the amount of GPU resources available. We will cover this
    in the following section. The number of streaming multiprocessors varies depending
    on the GPU specification. For instance, it is 80 for a Tesla V100, and it is 48
    for an RTX 2080 (Ti).
  prefs: []
  type: TYPE_NORMAL
- en: 'The CUDA streaming multiprocessor controls CUDA threads in groups of 32\. A
    group is called a **warp**. In this manner, one or multiple warps configures a
    CUDA thread block. The following figure shows the relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d49ee70-c8f0-4ce4-b663-d6f2b49cd288.png)'
  prefs: []
  type: TYPE_IMG
- en: The small green boxes are CUDA threads and they are grouped by a warp. The warp
    is a basic control unit of GPU architecture. Therefore, its size impacts CUDA
    programming implicitly or explicitly. For instance, the optimal thread block size
    is determined among multiple warp sizes that can fully utilize the block's warp
    scheduling and operations. We call this as occupancy, which will be covered in
    detail in the next section. Also, CUDA threads in a warp work in parallel and
    have synchronous operations, inherently. We will talk about this in the *Warp-level
    primitives programming* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exploiting a CUDA block and warp
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will look at CUDA thread scheduling and their implicit synchronization
    using CUDA's `printf`. The execution of parallel CUDA threads and the blocks operation
    is concurrent. On the other hand, printing out from the device is a sequential
    task. So, we can see their execution order easily, since the output will be arbitrary
    for the concurrent tasks and consistent for the parallel tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin to write kernel code that prints a global thread index, thread
    block index, warp index, and lane index. For that purpose, the code can be written as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code will help us to understand the concurrency of the warp and CUDA thread
    scheduling. Let's make our code get arguments from the shell to test various grid
    and thread block configurations easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will write the host code that calls the kernel function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s compile the code, execute it, and see the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is an example of the output result. The actual output
    might be different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'From the result, you will see that CUDA threads are launched in warp size and
    the order is not determined. On the other hand, the lane outputs are in order.
    From the given result, we can confirm the following facts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Out-of-order block execution:** The second column shows indexes of thread
    blocks. The result shows that it does not promise in-order execution following
    the block index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Out-of-order warp index with a thread block:** The third column shows the
    index of a warp in a block. The warp''s order varies across blocks. So, we can
    infer that there is no guarantee of warp execution order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grouped threads executed in a warp:** The fourth column shows the lane in
    a warp. To reduce the number of outputs, the application limits it to printing
    only two indices. From the in-order output within each warp, we can make an analogy
    that the `printf` function''s output order is fixed so that there is no inversion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To summarize, CUDA threads are grouped into 32 threads, and their output and
    the warp's execution have no order. Therefore, programmers have to keep this in
    mind for CUDA kernel development.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CUDA occupancy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CUDA occupancy is the ratio of active CUDA warps to the maximum warps that each
    streaming multiprocessor can execute concurrently. In general, higher occupancy
    leads to more effective GPU utilization because more warps are available to hide
    the latency of stalled warps. However, it might also degrade performance due to
    the increased resource contention between the CUDA threads. Thus, it is crucial
    for developers to understand this trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of finding optimal CUDA occupancy is to make the GPU application
    issue warps instructions efficiently with the GPU resources. The GPU schedules
    multiple warps using multiple warp schedulers on a streaming multiprocessor. When
    multiple warps are scheduled effectively, the GPU can hide latencies between the
    GPU instructions or memory latencies. Then, the CUDA cores can execute instructions
    continuously issued from the multiple warps, while the unscheduled warps have
    to wait until they can issue the next instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Developers can determine CUDA occupancy using two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Theoretical occupancy** determined by the CUDA Occupancy Calculator: This
    calculator is an Excel sheet provided with the CUDA Toolkit. We can determine
    each kernel''s occupancy theoretically from the kernel resource usages and the
    GPU''s streaming multiprocessor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Achieved occupancy** determined by the GPU: The achieved occupancy reflects
    the true number of concurrent executed warps on a streaming multiprocessor and the
    maximum available warps. This occupancy can be measured by the NVIDIA profiler
    with metric analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theoretical occupancy can be regarded as the maximum upper-bound occupancy because
    the occupancy number does not consider instructional dependencies or memory bandwidth
    limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how this occupancy and CUDA C/C++ are related.
  prefs: []
  type: TYPE_NORMAL
- en: Setting NVCC to report GPU resource usages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To begin with, we will use **simple matrix multiplication** (**SGEMM**) kernel
    code, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'And, we will call the kernel function using the following kernel code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You may want to provide appropriate GPU memory and its size information. We
    will use 2048 for `N`, `M`, and `K`. The memory size is the square of that number.
    We will set `BLOCK_DIM` as `16`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how to make the `nvcc` compiler report the GPU resource usage
    of the kernel functions.
  prefs: []
  type: TYPE_NORMAL
- en: The settings for Linux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a Linux environment, we should provide two compiler options, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--resource-usage` (`--res-usage`): Setting a verbose option for GPU resource
    usage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-gencode`: Specifying the target architecture to compile and generate opcodes
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Turing: `compute_75,sm_75`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Volta: `compute_70,sm_70`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pascal: `compute_60,sm_60`, `compute_61,sm_61`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are not sure which architecture you are using, you can find out from
    the CUDA GPU website ([https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus)). For
    example, the `nvcc` compile command can have the compile option as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also compile the code to target multiple GPU architectures as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to enable your code to be compatible with the new GPU architecture
    (Turing), you need to provide an additional option as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you want to learn more about these option, you can find the related information
    in this document: [https://docs.nvidia.com/cuda/turing-compatibility-guide/index.html#building-turing-compatible-apps-using-cuda-10-0](https://docs.nvidia.com/cuda/turing-compatibility-guide/index.html#building-turing-compatible-apps-using-cuda-10-0).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s compile the source. We can find a resource usage report from NVCC''s
    output. The following result is generated using the preceding commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48ba5f5b-a5b2-4fae-a022-e2eaeba65b62.png)'
  prefs: []
  type: TYPE_IMG
- en: NVCC reports CUDA kernels resource usage information for each compute capability.
    In the preceding output screenshot, we can see the number of registers per thread
    and constant memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Settings for Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we are developing a Windows application, we can set these settings on
    the project''s properties dialog of Visual Studio. The following the screenshot
    shows that dialog:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/056e4da9-6936-49c0-957b-ef911ecb8bd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To open this dialog, we should open debug_vs Property Pages, then traverse
    to the CUDA C/C++ | Device tab on the left-hand panel. Then, we should set the
    following options as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Verbose PTXAS Output: No | Yes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code Generation: Update the option to specify your target architecture as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Turing: `compute_75,sm_75`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Volta: `compute_70,sm_70`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pascal: `compute_60,sm_60;compute_61,sm_61`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can specify multiple target architectures using a semi-colon (`;`) for each
    target.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s build the source code and we will see NVCC''s report on the output
    panel of Visual Studio. Then, you will see output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f073fc9-3503-4296-a33e-60416c95573e.png)'
  prefs: []
  type: TYPE_IMG
- en: It is the same as the NVCC output in Linux.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's use the resource usage report to analyze a kernel's occupancy.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the optimal occupancy using the Occupancy Calculator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In practice, we can use the CUDA Occupancy Calculator, which is provided with
    the CUDA Toolkit. Using this, we can obtain theoretical occupancy by providing
    some kernel information. The calculator is an Excel file, and you can find it
    in the following, based on the OS you use:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Windows:** `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\<cuda-version>\tools`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linux:** `/usr/local/cuda/tools`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**macOS:** `/Developer/NVIDIA/<cuda-version>/tools`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a screenshot of the calculator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7845ff21-4805-4c93-beb1-9e788919b60e.png)'
  prefs: []
  type: TYPE_IMG
- en: CUDA Occupancy Calculator
  prefs: []
  type: TYPE_NORMAL
- en: 'This calculator has two parts: kernel information inputs and occupancy information
    outputs. As input, it requires two kinds of information, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The GPU's compute capability (green)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thread block resource information (yellow):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threads per CUDA thread block
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registers per CUDA thread
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared memory per block
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The calculator shows the GPU''s occupancy information here:'
  prefs: []
  type: TYPE_NORMAL
- en: GPU occupancy data (blue)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GPU's physical limitation for GPU compute capability (gray)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allocated resources per block (yellow)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum thread blocks per stream multiprocessor (yellow, orange, and red)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Occupancy limit graph following three key occupancy resources, which are threads,
    registers, and shared memory per block
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Red triangles on graphs, which show the current occupancy data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s put the obtained information into the calculator. We can edit the
    green-and orange-colored areas in the Excel sheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8220238-38e8-4301-a0af-6e667621ff39.png)'
  prefs: []
  type: TYPE_IMG
- en: Enter your acquired kernel resource information, and see how the sheet changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on compute capability and input data, the occupancy changes, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2ddc32f-0596-4d41-8b1b-0f8c4fe7fde8.png)'
  prefs: []
  type: TYPE_IMG
- en: Changes in occupancy depending on compute capability and input data
  prefs: []
  type: TYPE_NORMAL
- en: 'The blue-colored area shows the kernel function''s achieved occupancy. In this
    screenshot, it shows 100% occupancy achievements. The right-hand side of the sheet
    presents the occupancy utilization graphs for GPU resources: CUDA threads, shared
    memory, and registers.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, kernel code cannot have 100% theoretical occupancy due to many reasons.
    However, setting the pick occupancy is the start of utilizing GPU resources efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Occupancy tuning – bounding register usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CUDA register usage can increase when the kernel's algorithm is complicated,
    or the handling datatype is double precision. In that case, the occupancy drops
    due to the limited active warp size. In that situation, we can increase the theoretical
    occupancy by limiting the register usage and see whether the performance is enhanced.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way of resource tuning GPU resource usage is to use the `__launch_bound__`
    qualifier with the kernel function. This informs NVCC to guarantee the minimum
    thread blocks per stream multiprocessed with the maximum block size. Then, NVCC
    finds the optimal register size to achieve the given condition. You can use this
    if you have an idea of the size that makes your algorithm run efficiently at compile
    time. The identifier can be used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Then, the compiler checks the upper-bound resources and reduces the limiting
    resource usage per block. If its resource usage does not exceed the upper limit,
    the compiler adjusts the register usage if CUDA can schedule an extra thread block
    per multiprocessor, if the second parameter is not given. Alternatively, the compiler
    increases the register usage to hide single-thread instruction latency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we can simply limit the number of occupied register usages at the application
    level. The `--maxrregcount` flag to `NVCC` will specify the number, and the compiler
    will reorder the register usages. The following compile command shows how to use
    that flag in the Linux Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: But, keep in mind that limiting register usage in this way can introduce thread
    performance drawn by register throttling. Even the compiler can split the registers
    into local memory if it cannot set them under the limit, and the local variables
    are placed in the global memory.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the achieved occupancy from the profiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we can obtain the achieved occupancy from the profiled metric data using
    the Visual Profiler. Click the target kernel timeline bar. Then, we can see the
    theoretical and achieved occupancy in the Properties panel. We can also obtain
    more details from the Kernel Latency menu. The following screenshot shows the
    achieved performance of the example code we used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aea2d647-f88b-4f6f-9a00-922010fb039f.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance showing achieved and theoretical occupancy
  prefs: []
  type: TYPE_NORMAL
- en: With this occupancy tuning, we can design the CUDA block size to fully utilize
    warp scheduling in the streaming multiprocessor. However, this does not resolve
    the 54.75% memory throttling issue, which we found in the previous section. This
    implies that multiprocessors can stall and cannot conceal memory access latency
    due to hampered memory requests. We will discuss how to optimize this in this
    chapter, and in [Chapter 7](71d77c43-0064-491e-9b43-307a05bd6555.xhtml), *Parallel
    Programming Patterns in CUDA*, we'll discuss matrix-matrix multiplication optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding parallel reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reduction is a simple but useful algorithm to obtain a common parameter across
    many parameters. This task can be done in sequence or in parallel. When it comes
    to parallel processing to a parallel architecture, parallel reduction is the fastest
    way of getting a histogram, mean, or any other statistical values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the difference between sequential reduction and
    parallel reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cff3f592-ee81-4d90-b567-de34f12202ea.png)'
  prefs: []
  type: TYPE_IMG
- en: By having the reduction tasks in parallel, the parallel reduction algorithm
    can reduce the total steps at a log scale. Now, let's begin to implement this
    parallel reduction algorithm on the GPU. Firstly, we will implement this with
    a simple design using global memory. Then, we will implement another reduction
    version using the shared memory. By comparing the two implementations, we will
    discuss what brings a performance difference.
  prefs: []
  type: TYPE_NORMAL
- en: Naive parallel reduction using global memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first basic approach for reduction is to use parallel CUDA threads and
    share the reduction output using global memory. For every iteration, the CUDA
    kernel obtains cumulated values from global memory by reducing its size by two. The
    reduction works as shown in the following diagram, which displays naive parallel
    reduction with global memory data sharing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/873af97a-c719-4f57-94b1-2732193a2873.png)'
  prefs: []
  type: TYPE_IMG
- en: This approach is slow in CUDA because it wastes the global memory's bandwidth
    and does not utilize any faster on-chip memory. For better performance, it is
    recommended to use shared memory to save global memory bandwidth and reduce memory-fetch
    latency. We will discuss how this approach wastes bandwidth later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s implement this reduction. Firstly, we will write the reduction
    kernel function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will call the kernel function while reducing the stride size by half iteratively,
    until the `stride` size is one, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this implementation, the kernel code fetches the device memory with stridden
    addressing and outputs one reduction result. The host code triggers reduction
    kernels for each step, and the parameter size reduces by half. We cannot have
    an internal kernel loop since CUDA does not guarantee synchronized operations
    across thread blocks and streaming multiprocessors.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing kernels using shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this reduction, each CUDA thread block reduces input values, and the CUDA
    threads share data using shared memory. For a proper data update, they use the
    block-level intrinsic synchronization function, `__syncthreads()`. Then, the next
    iteration operates on the previous reduction result. Its design is shown in the
    following diagram, which displays parallel reduction using shared memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/606f158b-fc9c-4723-8a8c-a9c8b42bdacf.png)'
  prefs: []
  type: TYPE_IMG
- en: The yellow-dotted boxes represent a CUDA thread block's operation coverage.
    In this design, each CUDA thread block outputs one reduction result.
  prefs: []
  type: TYPE_NORMAL
- en: Block-level reduction lets each CUDA thread block conduct reduction and outputs
    a single reduction output. Since it does not require us to save the intermediate
    result in the global memory, the CUDA kernel can store the transitional value
    in the shared memory. This design helps to save global memory bandwidth and reduce
    memory latency.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did for global reduction, we will implement the operation. Firstly, we
    will write the kernel function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will call the kernel function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this code, we provide `n_threads * sizeof (float)` bytes, because each CUDA
    thread will share a single variable for each byte.
  prefs: []
  type: TYPE_NORMAL
- en: Writing performance measurement code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To measure each version''s performance, we will use the CUDA sample `timer`
    helper function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This function set helps to measure execution time at the microsecond level.
    Also, it is recommended to call the kernel function ahead of performance measurement
    to eliminate the device initialization overhead. For a more detailed implementation,
    visit the implemented code in the `global_reduction.cu` and `reduction.cu` files.
    These code sets are used across this chapter to evaluate the optimization effect
    along with the profiler.
  prefs: []
  type: TYPE_NORMAL
- en: Performance comparison for the two reductions – global and shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we can compare the two parallel reduction operations'' execution time.
    Performance can vary depending on GPUs and the implementation environments.Run
    the following commands for global reduction and reduction using shared memory
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Using my Tesla V100 PCIe card, the estimated performance of both reductions
    is as follows. The number of elements was *2^(24)* items:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operation** | **Estimated time (ms)** | **Speed-up** |'
  prefs: []
  type: TYPE_TB
- en: '| Original approach (reduction with global memory) | 4.609 | 1.0x |'
  prefs: []
  type: TYPE_TB
- en: '| Reduction using shared memory | 0.624 | 7.4x |'
  prefs: []
  type: TYPE_TB
- en: From this result, we can see how sharing data using shared memory in reduction
    returns the output quickly. The first implemented version is in `global_reduction.cu`,
    and the second version is in `shared_reduction.cu`, so you can compare the implementations
    for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: By dividing the reduction along with the shared memory, we could enhance the
    performance significantly. However, we cannot determine that it is the maximum
    performance we could get and do not know what bottleneck our application has.
    To analyze this, we will cover the performance limiter in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the application's performance limiter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we saw how saving global memory benefits the CUDA kernel's performance.
    In general, using an on-chip cache is better than using off-chip memory. But,
    we cannot determine whether much optimization room remains with this simple analogy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The performance limiter shows the bounding factor, which limits the performance
    of an application most significantly. Based on its profiling information, it analyzes
    performance-limiting factors among computing and memory bandwidth. Based on these
    resources'' utilization, an application can be categorized into four types: **Compute
    Bound**, **Bandwidth Bound**, **Latency Bound**, and **Compute and Latency Bound**.
    The following graph shows these categories related to compute and memory utilization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2689638e-eff4-4f99-bb64-c3664f8d08c2.png)'
  prefs: []
  type: TYPE_IMG
- en: After we identify the limiter, we can use the next optimization strategy. If
    either resource's utilization is high, we can focus on the optimization of the
    resource. If both are under-utilized, we can apply latency optimization from I/O
    aspects of the system. If both are high, we can investigate whether there is a
    memory operation stalling issue and computing-related issue.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see how we can obtain that utilization information.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the performance limiter and optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s apply this analysis to both reduction implementations. We will
    compare them and discuss how shared memory contributes to the performance limiter
    analysis with improved performance. First, let''s profile the global memory-based
    reduction application with the metric analysis using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will obtain the following chart from NVIDIA profiler, which shows
    the first global memory-based reduction''s performance limiter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2238b4cf-3f68-4583-a824-bf5c47467f6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On this chart, we need to review performance execution ratio to see if it is
    balanced by checking the kernel latency analysis. Because, as you can see in the
    preceding chart, the utilization gap between **Compute** and **Memory** is large
    and this could mean there will be a lot of latency in compute due to memory bottleneck.
    The following graph shows the result of the sampling-based analysis, and we can
    determine that CUDA cores are starved due to the memory dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cca26c03-d198-426d-ad68-fee55109fcec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the kernel execution is delayed due to memory waiting. Now,
    let''s profile the reduction based on shared memory. We can do this with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will obtain the following chart, which shows the second shared memory-based
    reduction''s performance limiter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed221ef5-237f-4cb7-aa13-38e164a8d15f.png)'
  prefs: []
  type: TYPE_IMG
- en: We can determine that it is compute-bounded and memory does not starve the CUDA
    cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s review our kernel operation to optimize computing operations. The
    following code shows the parallel reduction part in the kernel function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As an arithmetic operation, modular is heavy operation. Since the `stride`
    variable is an exponential number of `2`, it can be replaced with a bitwise operation,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command to see the optimized output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the new estimated time is **0.399 ms**, and we could achieve a more optimized
    performance, as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operation** | **Estimated time (ms)** | **Speed-up** |'
  prefs: []
  type: TYPE_TB
- en: '| Original approach (reduction with global memory) | 4.609 | 1.0x |'
  prefs: []
  type: TYPE_TB
- en: '| Reduction using shared memory | 0.624 | 7.4x |'
  prefs: []
  type: TYPE_TB
- en: '| Changing conditional operation from `%` to `&` | 0.399 | 11.55x |'
  prefs: []
  type: TYPE_TB
- en: 'The following graph shows the updated performance limiter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15e9f92a-af6d-4b94-ab56-1cd041292870.png)'
  prefs: []
  type: TYPE_IMG
- en: We can identify that its operation is **compute and latency bounded**. So, we
    can determine that we could increase memory utilization by optimizing computing
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing the CUDA warp divergence effect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a **single instruction, multiple thread** (**SIMT**) execution model, threads
    are grouped into sets of 32 threads and each group is called a **warp**. If a
    warp encounters a conditional statement or branch, its threads can be diverged
    and serialized to execute each condition. This is called **branch divergence**,
    which impacts performance significantly.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA warp divergence refers to such CUDA threads' divergent operation in a warp.
    If the conditional branch has an `if`-`else` structure and a warp has this warp
    divergence, all CUDA threads have an active and inactive operation part for the
    branched code block.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows a warp divergence effect in a CUDA warp. CUDA threads
    that are not in the idle condition and reduce the efficient use of GPU threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0513741-a1a7-41e0-9598-534517e18798.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As more of the branched part becomes significant, the GPU scheduling throughput
    becomes inefficient. Therefore, we need to avoid or minimize this warp divergence
    effect. There are several options you can choose:'
  prefs: []
  type: TYPE_NORMAL
- en: Divergence avoidance by handling different warps to execute the branched part
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coalescing the branched part to reduce branches in a warp
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shortening the branched part; only critical parts to be branched
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rearranging the data (that is, transposing, coalescing, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitioning the group using `tiled_partition` in Cooperative Group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining divergence as a performance bottleneck
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the previous reduction optimization, you might find a warning about an
    inefficient kernel due to divergent branches in the computing analysis, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e6569d1-6b97-4720-a49a-2b3883487a3a.png)'
  prefs: []
  type: TYPE_IMG
- en: '73.4 % divergence means that we have an inefficient operation path. We can
    determine that the reduction addressing is the issue, highlighted next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'When it comes to reduction addressing, we can select one of these CUDA thread
    indexing strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: Interleaved addressing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequential addressing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's review what they are and compare their performance by implementing these
    strategies. Since we will just modify the reduction kernel, we can reuse the host
    code for the next two implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Interleaved addressing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this strategy, the consecutive CUDA threads to fetch input data using the
    interleaved addressing strategy. Compared to the previous version, CUDA threads
    access input data by increasing the stride value. The following diagram shows
    how CUDA threads are interleaved with reduction items:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4d697d2-a75f-4b7c-8bd0-712132017416.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This interleaving addressing can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command to compile the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The measured kernel execution time is **0.446 ms** on the Tesla V100\. It is
    slower than the previous version because each thread block is not fully utilized
    in this approach. We would be able to get more detail by profiling its metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will try another addressing approach, which is designed so that each
    thread block computes more data.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential addressing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Compared to previous versions, this has highly coalesced indexing and addressing.
    This design is more efficient because there is no divergence when the stride size
    is greater than the warp size. The following diagram shows a coalesced thread
    operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/316964ae-b4ed-460f-9285-5043c3782407.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s write a kernel function to use sequential addressing on the reduction
    items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command to compile the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Its measured execution time is **0.378 ms** on a Tesla V100 GPU, which is slightly
    faster than the previous strategy (0.399 ms).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to the warp divergence avoiding, we could obtain a **12.2x** performance
    gain on the original compute. The following graph shows the updated performance
    limiter analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1598561f-8e61-41ea-a891-d6744ba53a19.png)'
  prefs: []
  type: TYPE_IMG
- en: Compared to the previous performance limiter, we can see the reduced control-flow
    operation and increased memory utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Performance modeling and balancing the limiter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following the performance limiter analysis, our current reduction performance
    is bounded by the compute latency due to the memory bandwidth, although the limiter
    analysis shows the full utilization of each resource. Let's cover why this is
    an issue and how we can resolve this by following the Roofline performance model.
  prefs: []
  type: TYPE_NORMAL
- en: The Roofline model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Roofline model** is an intuitive visual performance analysis model used
    to provide estimated performance for a given computing kernel on a parallel processing
    unit. Based on this model, developers in parallel programming can identify what
    the algorithm should be bounded to and determine which should be optimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows an example of the Roofline model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d377d5f-92e2-43bd-84f5-bf446cea1edb.png)'
  prefs: []
  type: TYPE_IMG
- en: The slanted part means memory-bound, and the flat part means arithmetic-bound.
    Each parallel algorithm and implementation has its own Roofline model since they
    have different computing power and memory bandwidth. With this model, algorithms
    can be placed depending on their operational intensity (flops/bytes). If an implementation
    does not meet the expected performance of this model, we can determine that this
    version is bounded by latency.
  prefs: []
  type: TYPE_NORMAL
- en: Considering our parallel reduction's complexity, it must be memory-bound. In
    other words, it has low operational intensity, so our strategy should maximize
    memory bandwidth as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we need to confirm how our reduction kernel function consumes memory
    bandwidth using the performance analysis in the profiler. The following diagram
    shows the global memory''s bandwidth usages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b17a6cf-98a6-49ac-9f48-4392a4459f5a.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see in this diagram, we didn't achieve full utilization of the memory
    bandwidth. The total bandwidth is 343.376 GB/s on a Tesla V100 GPU, which utilizes
    about one-third of the bandwidth since this GPU has 900 GB/s bandwidth HBM2 memory.
    Therefore, the next step is to increase bandwidth usage by letting each CUDA thread
    digest more data. This will resolve the latency-bound situation and make our application
    be bounded to memory bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's cover how to increase memory bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: Maximizing memory bandwidth with grid-strided loops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can achieve this with a simple idea. The reduction problem allows us to
    accumulate input data with CUDA threads and start a reduction operation. Previously,
    our reduction implementation started with the input data size. But now, we will
    iterate to the input data with a group of CUDA threads, and that size will be
    the grid size of our kernel function. This style of iteration is called grid-strided
    loops. This technique has many benefits to control multiple CUDA cores, and they
    are introduced in this document: [https://devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops](https://devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the updated reduction kernel function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: You will find that this kernel function focuses on accumulating input data first,
    and then it reduces the loaded data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to determine the grid size. To make our GPU code run on various
    GPU targets, we have to determine their size at runtime. Also, we need to utilize
    all the multiprocessors in the GPU. CUDA C provides related functions. We can
    obtain the occupancy-aware maximum active blocks per multiprocessor using the `cudaOccpancyMaxActiveBlocksPerMultiprocessor()`
    function. Also, we can obtain the multiprocessor numbers on the target GPU using
    the `cudaDeviceGetAttribte()` function. The following code shows how we can use
    those functions and call the kernel function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'There is one additional modification in this function. To save the occupancy
    calculation overhead, it launches the `reduction_kernel()` function once more
    with a single block. Run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The updated reduction performance is **0.278** **ms** on a Tesla V100, which
    is about 100 ms faster than the previous method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s review how we could utilize the memory bandwidth. The following
    diagram shows the memory utilization analysis in the Visual Profiler, and shows
    how we increased the memory bandwidth twice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44e2f316-82e4-494e-a957-3e859fdcba39.png)'
  prefs: []
  type: TYPE_IMG
- en: Although it shows an increased bandwidth, we still have room to increase it
    further. Let's cover how we can achieve more bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing the I/O throughput
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the result we got from the profiler, the local variable input has a substantial
    amount of load/store requests. Such massive I/O impacts the thread block''s scheduling
    due to the operational dependency. The worst thing in the current data accumulation
    is that it has a dependency on device memory. So, we will use extra registers
    to issue more load instructions to ease the dependency. The following code shows
    how we can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This code uses three more registers to collect global memory data. The value
    of `NUM_LOAD` can vary depending on the GPU because it is affected by the GPU''s
    memory bandwidth and the number of CUDA cores in a GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e7d7521-757d-4041-8999-c04e3cdfc52c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On running the following command, the achieved performance using Tesla V100
    card is **0.264** ms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Warp-level primitive programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CUDA 9.0 introduces new warp synchronous programming. This major change aims
    to avoid CUDA programming relying on implicit warp synchronize operations and
    handling synchronous targets explicitly. This helps to prevent inattentive race
    conditions and deadlocks in warp-wise synchronous operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Historically, CUDA provided only one explicit synchronization API, `__syncthreads()`
    for the CUDA threads in a thread block and it relied on the implicit synchronization
    of a warp. The following figure shows two levels of synchronization of a CUDA
    thread block''s operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9d730cb-7dbe-4aa1-a5ba-f727ac229f21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, the latest GPU architectures (Volta and Turing) have an enhanced thread
    control model, where each thread can execute a different instruction, while they
    keep its SIMT programming model. The following diagram shows how it has changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19f1bde5-e031-4aac-bb90-b3162e1d8069.png)'
  prefs: []
  type: TYPE_IMG
- en: Until the Pascal architecture (left), threads were scheduled at warp level,
    and they were synchronized implicitly within a warp. Therefore, CUDA threads in
    a warp synchronized implicitly. However, this had unintended deadlock potential.
  prefs: []
  type: TYPE_NORMAL
- en: The Volta architecture renovated this and introduced **independent thread scheduling**.
    This control model enables each CUDA thread to have its program counter and allows
    sets of participating threads in a warp. In this model, we have to use an explicit
    synchronous API to specify each CUDA thread's operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, CUDA 9 introduced explicit warp-level primitive functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Warp-level primitive functions** |'
  prefs: []
  type: TYPE_TB
- en: '| **Identifying active threads** | `__activemask()` |'
  prefs: []
  type: TYPE_TB
- en: '| **Masking active threads** | `__all_sync()`, `__any_sync()`, `__uni_sync()`, `__ballot_sync()``__match_any_sync()`, `__match_all_sync()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Synchronized data exchange** | `__shfl_sync()`, `__shfl_up_sync()`, `__shfl_down_sync()`, `__shfl_xor_sync()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Threads synchronization** | `__syncwarp()` |'
  prefs: []
  type: TYPE_TB
- en: There are three categories of warp-wise primitive functions, which are warp
    identification, warp operations, and synchronization. All these functions implicitly
    specify synchronization targets to avoid unintended race conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel reduction with warp primitives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how this can benefit our parallel reduction implementation. This
    recipe will use the `shfl_down()` function in Cooperative Groups, and `shfl_down_sync()`
    in warp primitive functions. The following figure shows how shift down operation
    works with `shfl_down_sync()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f62b8889-8529-494e-8b4f-61ec7ec2ca8e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this collective operation, CUDA threads in a warp can shift a specified
    register value to another thread in the same warp and synchronize with it. To
    be specific, the collective operation has two steps (the third one is optional):'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying, masking, or ballot sourcing CUDA threads in a warp that will have
    an operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Letting CUDA thread shift data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the CUDA threads in a warp are synchronization (optional).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the parallel reduction problem, we can use warp-level reduction using `__shfl_down_sync()`.
    Now, we can enhance our thread block-level reduction with the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10831534-803f-4d34-ba5f-b944331803d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Each warp's reduction result is stored to shared memory to share with other
    warp. Then, the final block-wise reduction can be obtained by doing warp-wise
    collection again.
  prefs: []
  type: TYPE_NORMAL
- en: We use `__shfl_down_sync()` since we need only one thread to have warp-level
    reduction. If you need to make all the CUDA threads have warp-level reduction,
    you can use `__shfl_xor_sync()` instead.
  prefs: []
  type: TYPE_NORMAL
- en: The number of the first block-level reductions is the dimension of the grid,
    and the outputs are stored in global memory. By calling once again, we can build
    a parallel reduction kernel using a warp-level synchronous function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s implement warp-level reduction using warp-level primitive functions.
    Firstly, we will write a function that uses warp-shifting functions to make warp-level
    reductions. The following code shows how this can be implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: For the warp-shifting, we need to let the CUDA scheduler identify the active
    threads and let the warp-shifting function do the reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step is to write a block-level reduction function using the previous
    warp-level reduction. We will collect the previous result in the shared memory
    and make the second reduction from the result. The following code shows how this
    can be implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will implement the reduction kernel function that cumulates input data,
    and do the reduction from the block-level reduction that we have implemented.
    Since we just focused on optimizing warp-level optimization, the overall design
    is the same as the previous version. The following code shows the kernel function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s compile the code using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the reduction in execution time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71029119-ad15-4383-9eb8-ccc6b3850aa7.png)'
  prefs: []
  type: TYPE_IMG
- en: No host code modification was available to switch from the warp-primitive to
    Cooperative Groups. So, we could use the same host code for the two reduction
    implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have covered warp synchronous programming in CUDA. Its application is not
    only limited to reduction but can be used for other parallel algorithms: scan,
    bitonic sort, and transpose. If you need to learn more, you can checkout the following
    articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf](http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://devblogs.nvidia.com/using-cuda-warp-level-primitives/](https://devblogs.nvidia.com/using-cuda-warp-level-primitives/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://devblogs.nvidia.com/faster-parallel-reductions-kepler/](https://devblogs.nvidia.com/faster-parallel-reductions-kepler/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://on-demand.gputechconf.com/gtc/2013/presentations/S3174-Kepler-Shuffle-Tips-Tricks.pdf](http://on-demand.gputechconf.com/gtc/2013/presentations/S3174-Kepler-Shuffle-Tips-Tricks.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cooperative Groups for flexible thread handling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CUDA 9.0 introduces a new CUDA programming feature named **Cooperative Groups**.
    This introduces a new CUDA programming design pattern for CUDA collective operations
    by specifying group-wise operations. Using this, programmers can write CUDA code
    that controls CUDA threads explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, let's see what Cooperative Groups is and its programming advantages.
  prefs: []
  type: TYPE_NORMAL
- en: Cooperative Groups in a CUDA thread block
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cooperative Groups provides explicit CUDA thread-grouping objects, which help
    programmers to write collective operations more clearly and conveniently. For
    instance, we need to obtain a mask to control the active CUDA threads in a warp
    to have warp-shifting operations. Cooperative Group objects, on the other hand,
    bind the available threads as a tile, and we control them as an object. This brings
    C++ language benefits to the CUDA C programming.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fundamental type of Cooperative Group is `thread_group`. This enables a
    C++ class-style type, `thread_group`, which can provide its configuration information
    with the `is_valid()`, `size()`, and `thread_rank()` functions. Also, this provides
    collective functions that can apply to all the CUDA threads in a group. These
    functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | thread_group collective functions |'
  prefs: []
  type: TYPE_TB
- en: '| **Identifying active threads** | `tiled_partition()`, `coalesced_threads()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Masking active threads** | `any()`, `all()`, `ballot()``match_any()`, `match_all()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Synchronized data exchange** | `shfl()`, `shfl_up()`, `shfl_down()`, `shfl_xor()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Thread synchronization** | `sync()` |'
  prefs: []
  type: TYPE_TB
- en: These function lists are similar to warp-level primitive functions. So, warp-level
    primitive operations can be replaced with Cooperative Groups. `thread_group` can
    be split by a smaller `thread_group`, `thread_block_tile`, or `coalesced_group`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cooperative Groups also provides flexibility in thread block programming. Using
    the following line of code, we can handle a thread block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '`thread_block` provides CUDA built-in keyword wrapping functions, which we
    use to obtain a block index and thread index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can obtain a thread block object using `this_thread_block()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's see what the benefits of Cooperative Groups are compared to traditional
    CUDA built-in variables.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of Cooperative Groups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Cooperative Groups provides more C++ programmability, rather than using
    traditional CUDA built-in variables. Using a `thread_block` group, you can switch
    your kernel code from using built-in variables to Cooperative Group's indexing.
    But, the real power of Cooperative Groups is more than that. Let's cover its benefits
    in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Modularity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With Cooperative Groups, programmers can modularize their collective operation
    kernel codes corresponding to the barrier target. This helps to avoid oversights,
    causing deadlock and race conditions by assuming all threads are running concurrently.
    The following is an example of a deadlock and normal operations by CUDA thread
    synchronization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50c46410-4362-4702-8911-d95b431f6817.png)'
  prefs: []
  type: TYPE_IMG
- en: For the left-hand side example, the kernel code intends to synchronize a part
    of the thread in a CUDA thread block. This code minimizes synchronization overhead
    by specifying barrier targets. However, it introduces a deadlock situation because `__syncthreads()` invokes
    a barrier, which waits for all CUDA threads to reach the barrier. However, `__synchthroead()`
    cannot meet the others and waits. The right-handed side example shows sound operation
    since it does not have any deadlock point because all the threads in the thread
    block can meet `__syncthreads()`.
  prefs: []
  type: TYPE_NORMAL
- en: In the Cooperative Groups API, on the other hand, the CUDA programmers specify
    thread groups to synchronize. The Cooperative Groups enable explicit synchronization
    targets so that the programmers can let CUDA threads synchronize explicitly. This
    item can also be treated as an instance so that we can pass the instance to the
    device functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how Cooperative Groups provide explicit synchronization
    objects and let them be handled as an instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding example code, the kernel code can specify synchronization
    groups and pass them as a parameter as a `thread_group`. This helps us to specify
    the synchronize targets in the subroutines. Therefore, programmers can prevent
    inadvertent deadlock by using Cooperative Groups. Also, we can set different types
    of groups as a `thread_group` type and reuse synchronization code.
  prefs: []
  type: TYPE_NORMAL
- en: Explicit grouped threads' operation and race condition avoidance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cooperative Groups support warp-level cooperative operations by tiling threads
    in a warp. If the tile size matches the warp size, CUDA can omit warps' implicit
    synchronization, ensuring correct memory operation to avoid race conditions. By
    eliminating implicit synchronizations, the GPU's performance can be enhanced.
    Historically, experienced CUDA programmers used separated warps for warp-level
    synchronizations. This meant the cooperative operations in a warp did not have
    to sync with other warp operations. This unleashed GPU performance. However, it
    was risky because it introduces race conditions between cooperative operations.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic active thread selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another benefit of CUDA Cooperative Groups is that the programmers can pick
    active threads in a warp to avoid a branch divergence effect. Since CUDA is a
    SIMT architecture, an instruction unit issues a group of threads, and there is
    no way to disable divergence if they meet a branch. But, from CUDA 9.0 onward,
    programmer can select active threads that will be active in the branched block
    using `coalesced_threads()`. This returns coalesced threads by disabling threads
    that do not take branches. Then, SM's instruction unit issues the next threads
    that are active in the active thread group.
  prefs: []
  type: TYPE_NORMAL
- en: Applying to the parallel reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will update the previous reduction kernel code to use the Cooperative Group. From
    the previous kernel code, you can easily apply Cooperative Groups'' `thread_block`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We don''t have to update the data input accumulation part, so let''s update
    the reduction parts for each thread block. The following code shows an example
    of block-sized reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The estimated operation performance is 0.264 ms using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command shows the same performance as in the previous version.
  prefs: []
  type: TYPE_NORMAL
- en: Cooperative Groups to avoid deadlock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cooperative Groups can support independent CUDA threads scheduling. So, we
    can control CUDA threads individually with a group, and synchronize them explicitly.
    The target group can be a predefined tile, but it can also be determined following
    the conditional branch, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This code has four-thread block synchronization options. Options `(1)` and `(2)`
    are equivalent operations with different APIs. On the other hand, options `(3)`
    and `(4)` are not. Option `(3)` introduces a deadlock of CUDA threads, and the
    host cannot have the return of the CUDA kernel, because active CUDA threads cannot
    synchronize with the non-activated CUDA threads. On the other hand, option `(4)`
    works thanks to Cooperative Groups' automatic active thread identification. This
    helps us to avoid unintended errors and develop sophisticated algorithms easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'NVIDIA provides detailed descriptions of Cooperative Groups in the following
    documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://devblogs.nvidia.com/cuda-9-features-revealed](https://devblogs.nvidia.com/cuda-9-features-revealed)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf](http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also learn about its architecture and full API lists from `cooperative_groups.h`
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: Loop unrolling in the CUDA kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CUDA can also reap the benefits of loop unrolling like other programming languages.
    With this technique, CUDA threads can reduce or remove loop control overheads
    such as *the end of loop* tests on each iteration, branch penalties, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Compiler unrolls small loops automatically if it can identify the number
    of iterations for the loops. Programmers can also place the `#pragma unroll` directive
    to give a hint to the compiler, or just rewrite the loop code as a group of independent
    statements. Applying loop unrolling is simple, so you can easily apply this to
    your current working code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s apply this to our parallel reduction implementation. Like the normal
    loop unrolling directive in C/C++, we can place the `#pragma` loop unrolling directive on
    top of `for` loops. The NVCC compiler can unroll the loop since the compiler can
    obtain the exact size of `group.size()` by itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The estimated operation performance is 0.263 ms using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'If you prefer to use the warp primitive function, you can write `warp_reduce_sum`
    like the following. Loop code can be reused by replacing `group.size()` with `warpSize`,
    but this was slightly faster in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command to compile the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Its result is 0.263 ms, the same as the previous result.
  prefs: []
  type: TYPE_NORMAL
- en: There is a pitfall of using loop unrolling. The unrolled code execution may
    result in lower occupancy by the increased register usage. Also, there can be
    a higher instruction cache miss penalty by the increased code execution size.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In CUDA programming, programmers can use atomic APIs to update shared resources
    from multiple CUDA threads. These atomic API guarantee to eliminate race conditions
    to the shared resource, so we can expect consistent outputs from the parallel
    execution. This operation is especially useful for getting statistical parameters
    such as a histogram, mean, sum, and so on. We can also simplify the code implementation.
    For example, the reduction operation can be written using the `atomicAdd()` function
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the atomic function simplifies the required operation. However,
    its performance is slow because the atomic operation serializes all the requests
    to the shared resource. Run the following command to see the execution time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This kernel function shown took 39 ms on my Tesla V100, which is far slower
    than the original version (4.609 ms). Therefore, the recommended atomic operation
    usage is to limit the request only if it is necessary. For the parallel reduction
    problem, for instance, we can reduce items in parallel at a certain level and
    use the atomic operation to output the final result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows another possible approach. This replaces block-wise
    reduction as `atomicAdd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/654607c4-860e-4c99-a464-97de231fd946.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding diagram, we can see that there are two reduction points: a
    **warp** and a **thread block**, and, the block-wise reduction result is accumulated
    by the single global memory variable atomically. As a result, we can eliminate
    the second reduction iteration. The following screenshot shows the Kernel Optimization
    Priorities (on the left) and the Performance Limiter Analysis (on the right) of
    the second reduction iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c35e7bf-577c-4903-be51-7f79d9a57267.png)'
  prefs: []
  type: TYPE_IMG
- en: Kernel Optimization Priorities with Performance Limiter Analysis (2nd iteration)
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the second iteration's performance is bounded by the latency
    due to its small grid size. So, we would be able to reduce the execution time
    by removing this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s implement that design and see how the performance can be changed.
    We just need to update the last part of the reduction kernel function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will remove the second iterative function call. As a result, we can
    remove kernel call latency and achieve better performance if the atomic operation''s
    latency is shorter than that. Run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Fortunately, the estimated execution time is 0.259 ms on a Tesla V100, so we
    could achieve a slightly enhanced result.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to learn more about atomic operations in CUDA C, please checkout
    the programming guide at this link: [https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions).'
  prefs: []
  type: TYPE_NORMAL
- en: Low/mixed precision operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mixed precision is a technique for exploring low-precision, and obtains a high
    accuracy result. This technique computes core operations with low precision and
    generates output with high-precision operations. Low precision operation computation
    has the benefits of reduced memory bandwidth and higher computing throughput compared
    with high-precision computing. If low precision suffices to get target accuracy
    from an application with high precision, this technique can benefit performance
    with this trade-off. NVIDIA Developer Blog introduces this programmability: [https://devblogs.nvidia.com/mixed-precision-programming-cuda-8](https://devblogs.nvidia.com/mixed-precision-programming-cuda-8).'
  prefs: []
  type: TYPE_NORMAL
- en: In these circumstances, CUDA extends its supports to low-precision tools lower
    than 32-bit data types, such as 8/16-bit integers (INT8/INT16) and 16-bit floating
    points (FP16). For those low-precision data types, a GPU can use **single instruction, multiple data** (**SIMD**)
    operations with some specific APIs. In this section, we will look at these two
    kinds of instructions for low-precision operations for a mixed-precision purpose.
  prefs: []
  type: TYPE_NORMAL
- en: To get benefits from this, you need to confirm that your GPU can support low
    mixed-precision operations and supporting data types. Supporting low-precision
    computing is possible in specific GPUs, and the precision varies depending on
    the GPU chipsets. To be specific, GP102 (Tesla P40 and Titan X), GP104 (Tesla
    P4), and GP106 support INT8; and GP100 (Tesla P100) and GV100 (Tesla V100) support
    FP16 (half-precision) operations. The Tesla GV100 is compatible with INT8 operation
    and has no performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA has some special intrinsic functions that enable SIMD operations for low-precision
    data types.
  prefs: []
  type: TYPE_NORMAL
- en: Half-precision operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CUDA provides intrinsic functions for the half-sized float data type (FP16)
    and developers can choose whether CUDA computes one or two values for each instruction.
    CUDA also provides type conversion functions between single-precision and half-precision.
    Due to the accuracy limitation of FP16, you must use the conversion intrinsic
    to work with single-precision values.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's implement and test the GPU's FP16 operation. GPUs can support the
    native computing with this type higher than computing capability 5.3\. But some
    GPUs do not support this, so please double-check whether your GPU supports this
    half-precision operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The half-precision datatype in CUDA C is `half`, but you can use the `__half` type
    too. For the API, CUDA provides relevant intrinsic functions with this datatype such
    as `__hfma()`, `__hmul()`, and `__hadd()`. These intrinsic functions also provide
    native operations with two data at one time using `__hfma2()`, `__hmul2()`, and
    `__hadd2()`. With these functions, we can write mixed-precision operation kernel
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: For those GPUs that do not support native half-precision operations, our code
    checks CUDA's compute capability at compile time and determines which operation
    it should take.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code calls the kernel function with the half-sized grid size
    since each CUDA thread will operate two data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Other initialization code and benchmark code is implemented in the sample recipe
    code, so please review it.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered FMA operations in FP16 precision operations. CUDA C provides
    various half-precision operations ([https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__HALF.html](https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__HALF.html)).
    Please check that for the other operations.
  prefs: []
  type: TYPE_NORMAL
- en: Dot product operations and accumulation for 8-bit integers and 16-bit data (DP4A
    and DP2A)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For 8-bit/16-bit integers, CUDA provides vectorized dot product operations.
    These are DP4A (a four element dot product with accumulation) and DP2A (a two
    element dot product with accumulation). Using these functions, CUDA developers
    can make faster operations. CUDA 8.0 Development Blog introduces these functions
    with intuitive figures  ([https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/](https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/)). The
    following shows how the GPU''s dot product and accumulation operations work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c84a489-8c06-421c-baf2-0930fb1b51c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Using this, you can write 8-bit only or 8-bit/16-bit mixed operations with 32-bit
    integer accumulation. Other operations such as sum, add, and compare are also
    available with SIMD intrinsic functions.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed previously, there are specific GPUs that can support INT8/INT16
    operations with special functions (`dp4a` and `dp2a`). The supporting GPUs' compute
    capability must be higher than 6.1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s implement a kernel function that uses the `dp4a` API, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: In this function, `__dp4a` fetches two arrays of characters coalescing four
    items and outputs its dot product outputs. This API is supported since Pascal
    with CUDA compute capability (version 6.1). But old GPU architectures, lower than
    version 6.1, need to use the original operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how we will call the implemented kernel function.
    Its grid size is reduced by four since each CUDA thread will operate on the four
    items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Other initialization code and benchmark code is implemented in samples code such
    as the previous example code.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered the dot operation of INT8, but CUDA C also provides other INT8-type
    SIMD intrinsic functions ([https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__SIMD.html](https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__SIMD.html)).
    Please check this document for the other operations.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The sample code has three versions of mixed precision operations: single-precision,
    half-precision, and INT8\. As the precision drops, we can add more operations
    for each CUDA thread.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands for single-precision, half-precision, and INT8 operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table shows the estimated performance for each precision operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Precision | Measured performance |'
  prefs: []
  type: TYPE_TB
- en: '| FP32 | 59.441 GFlops |'
  prefs: []
  type: TYPE_TB
- en: '| FP16 | 86.037 GFlops |'
  prefs: []
  type: TYPE_TB
- en: '| INT8 | 196.225 Gops |'
  prefs: []
  type: TYPE_TB
- en: Since our implementations are not optimized, the measured performance is quite
    lower than the theoretical performance of the Tesla V100\. When you profile them,
    they will report that they are highly memory-bounded. In other words, we need
    to optimize them to be arithmetically bounded to achieve close to the theoretical
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered how to configure CUDA parallel operations and optimize
    them. To do this, we have to understand the relationship between CUDA's hierarchical
    architecture thread block and streaming multiprocessors. With some performance
    models—occupancy, performance limiter analysis, and the Roofline model—we could
    optimize more performance. Then, we covered some new CUDA thread programmability,
    Cooperative Groups, and learned how this simplifies parallel programming. We optimized
    parallel reduction problems and achieved 0.259 ms with ![](img/204bf10b-1a7d-4b62-ad48-17dbbb31f177.png) elements,
    which is a 17.8 increase in speed with the same GPU. Finally, we learned about
    CUDA's SIMD operations with half-precision (FP16) and INT8 precision.
  prefs: []
  type: TYPE_NORMAL
- en: Our experience from this chapter focuses on the GPU's parallel processing level
    programming. However, CUDA programming includes system-level programming. Basically,
    the GPU is an extra computing resource and works independently from the host.
    This introduces extra computing power, but can also introduce latency on the other
    hand. CUDA provides API functions that can utilize this and conceal the latency
    and enables the full performance of the GPU. We will cover this in the next chapter.
  prefs: []
  type: TYPE_NORMAL
