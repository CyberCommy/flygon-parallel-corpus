- en: Chapter 3. Garbage Collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When writing applications, managing the available memory is boring and difficult.
    When the application gets complex, it's easy to start leaking memory. Many programming
    languages have automatic memory management, helping the developer to forget about
    this management by means of a **Garbage** **Collector** (**GC**). The GC is only
    a part of this memory management, but it's the most important one and is responsible
    for reclaiming memory that is no longer in use (garbage), by periodically looking
    at disposed referenced objects and freeing the memory associated with them.
  prefs: []
  type: TYPE_NORMAL
- en: The most common technique used by GC is monitoring reference counting. This
    means that GC, for each object, holds the number (count) of other objects that
    reference it. When an object has no references to it, it can be collected, which
    means that it can be disposed and its memory freed.
  prefs: []
  type: TYPE_NORMAL
- en: In V8, the Node.js engine, this reference counting is not constantly checked.
    Instead, it's periodically scanned, and this task is called a cycle. Usually,
    this cycle is not atomic which means that the program will pause execution while
    this cycle is running. Also, just to keep this reference counting, GC needs memory.
    This means a memory overhead on your program besides the memory used by the program.
    Also, because the language is dynamic and objects can change type, memory sometimes
    is not used in the most efficient way. Recall the previous chapter about development
    patterns for a more efficient memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic memory management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GC tremendously simplifies language usage, giving developers more time to focus
    on other aspects of the application. Also, it can reduce, although not completely
    remove, a type of error called memory leaks, which haunt long-lived applications
    and services. However, there's a performance penalty associated with its periodic
    task. It can be noticed, or not, depending on how much memory is used and disposed
    in short periods of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'By moving memory management out of the developer, Node.js removes or substantially
    reduces a few types of bugs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dangling pointer bugs**: These occur when memory is freed but still there
    are one or more pointers referencing that the memory blocks. If the memory is
    reassigned, these pointers can cause unpredictable behavior if used to change
    blocks from other parts of the program. You would have, in this case, more than
    two places in the application changing the same memory block. This is a particularly
    difficult bug to find.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Double free bugs**: These occur when memory is freed once and then freed
    again. In between, it might have been reallocated and used by another part of
    the application, destroying access to a reused block. This is similar to the previous
    memory, where two places manage the same block, but in this case, one is trying
    to use it and the other will just wipe the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory leaks**: These occur when objects are dereferenced before being freed.
    This happens when a program allocates memory, uses it, and then disposes the reference
    to that memory before explicitly freeing it. This type of bug can leave to memory
    exhaustion if this behavior occurs repeatedly, especially on long-lived services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Buffer overflows**: These occur when trying to write more information than
    the space allocated to the task. This is quite common when, for example, a program
    allocates a memory block somewhere after it needs more space than the memory it
    allocated, and fails to detect and reallocate the required space. This bug can
    halt the application or service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, moving memory management away from the developer removes
    a great deal of control over memory usage and how it's managed. GC will consume
    resources when looking at the memory being used and deciding what and when to
    free unreferenced objects, creating unpredictable pauses during your application
    execution. Also, the time at which the GC starts doing its job may be unpredictable
    and out of your control, which can introduce unpredictable performance penalties
    over times when your program is in need of resources.
  prefs: []
  type: TYPE_NORMAL
- en: This is the case of Node.js, but since it uses V8, which exposes a `gc()` method
    under the `--expose_gc` flag, you can manually force its use. You cannot decide
    when it will run, but you can force it to run more often if you think it's best.
    You can also tweak some of GC's behavior. To find out more run the `--v8-options`
    node.
  prefs: []
  type: TYPE_NORMAL
- en: There's no way of blocking its use, so you can just make it run more often,
    perhaps reducing its footprint. The GC's cost is proportional to the number of
    referencing objects, so if you use this method after substantially reducing referenced
    objects, you can keep your application lean and reduce the GC penalty later.
  prefs: []
  type: TYPE_NORMAL
- en: '![Automatic memory management](img/4183_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: GC memory graph'
  prefs: []
  type: TYPE_NORMAL
- en: Memory organization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Think of memory as a mesh of elements, usually primitives (numbers and strings)
    and objects (hash tables). It can be represented as a graph of interconnected
    points. Memory can be used to hold object information or to reference other objects.
    You can look at this interconnection as a graph where leafs are elements that
    hold information and the other nodes are references to other nodes (in Figure
    1, nodes **1**, **3**, **6**, and **9** are leafs).
  prefs: []
  type: TYPE_NORMAL
- en: When working with V8, there's some terminology you may find useful to better
    understand V8 Inspector or Chrome Developer Tools. The memory used by the object
    itself is called **shallow size**. It's used to store its immediate value, and
    usually, only strings and arrays can have a significant size.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also the distance column, which is the smallest graph distance from
    a root node to the node itself. A root note is a node from where references start
    pointing other nodes. In Figure 2 it would be node **2** as there''s no arrow
    pointing to **2** and everything on the graph starts on node **2**. In Inspectors,
    you''ll see another term in **Profiles** called **Retained size**. This is the
    size that will be freed once the object is deleted. It is at least the size of
    the object plus the size of the referenced objects, which will also be freed immediately
    since they will also get unreferenced. Confusing? Let''s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory organization](img/4183_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: GC-marked nodes before sweep'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram you see that node **2** is the root node in the graph,
    as there's no node referencing (pointing to) it. This node references node **5**
    and node **11**. If the reference to node **11** is removed, then there's no path
    from node **2** (and the rest of the left part of the graph) to get to nodes **8**
    and **1**. These nodes are part of the retained size of node **11** as they're
    useless without it. When node **11** is removed, they'll be removed too.
  prefs: []
  type: TYPE_NORMAL
- en: Memory leaks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A memory leak is a continuous loss of available memory, and it occurs when a
    program repeatedly fails to release the memory that it's no longer using. Node.js
    applications can suffer from this issue indirectly because of the GC. It's usually
    not the GC's fault, but is caused by some object destruction that is not taking
    place when it should, and this is not that difficult when you're using an event-driven
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Leaks haunt every developer as soon as their application hits medium size. As
    soon as your program starts having more interactions with external elements like
    other programs or clients, or when your program complexity grows, you start leaking
    memory. This happens when, for some reason, you're not dereferencing a no-longer-useful
    object of your application. If the GC finds that the object is still referenced
    by other objects, even if it's no longer useful to your application, it will remain
    in the heap and will be moved to a place called **old space**.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, objects live for a very long period of time (since the beginning of
    the application) or for a very short period of time (serving a specific client).
    The V8 GC is designed to take advantage of these two most common types of objects.
    GC cycles usually clean these short-lived objects, and if it thinks that these
    objects are still useful (that is, when it survives more than one or two GC cycles),
    it will move them to a bigger zone, where it will start to accumulate garbage.
    When this zone gets bigger, the GC cycle duration gets bigger too and you'll start
    noticing some stalls (complete breaks in the application) for a second, or even
    a few seconds. If this happens, it means you're already late at analyzing your
    application.
  prefs: []
  type: TYPE_NORMAL
- en: For a large memory limit, such as the default 1 GB limit of V8, if you're not
    monitoring your application, you'll probably notice leaks when your application
    starts stalling for a second, and after that, it's a few more seconds before it
    just stops because of that memory limit. GC cycles become very CPU intensive for
    large object collections, so you should really monitor GC memory management and,
    if possible, avoid greater memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Event emitters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since Node.js uses event emitters, there''s a question that should be in your
    head right now. Since GC can only sweep objects that are unreferenced, this means
    that event emitters will not be collected after you attach event listeners to
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is just an example of an echo server. In this example, GC
    will never collect `server`, which is good in this case since that's the main
    object of the program. In other cases, you might have such situations where your
    emitters won't get swept because of references to listeners. Most importantly,
    event callbacks are functions—extended objects in JavaScript—and won't get swept
    either.
  prefs: []
  type: TYPE_NORMAL
- en: Take a closer look at the previous example. Imagine that for each client (socket),
    you had more complex code with some private protocol. To simplify it, you use
    the Adapter pattern and create an abstraction to access each client. This abstraction
    could be an event emitter as a means to decouple it from other parts of the application.
    While your client keeps connected, any event listeners that don't explicitly unlisten
    events will not get garbage collected even if they are not supposed to exist anymore
    (this is true even if you null them). And if your connection gets stuck and doesn't
    time out (for example, a mobile connection), you'll collect a good pack of zombie
    connections for a while.
  prefs: []
  type: TYPE_NORMAL
- en: Referencing objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main goal of GC is to identify trashed memory. This refers to the memory
    blocks that you''re your application no longer uses, usually because your code
    no longer references them. Once identified, this memory can be reused or freed
    to the operating system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, although both `bar` and `baz` are local variables
    for the function (because of JavaScript function scoping), `baz` will be deference
    after `return` but `bar` won''t, and it will not be freed until you completely
    deference it. This might look obvious, but if your application grows and you start
    using external modules that you don''t know how they work internally, you might
    get more dangling references than you expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now imagine you call the `foo` function and ignore the returned object. You
    might think that it''ll get unreferenced, but there''s no guarantee of that because
    of what `doSomething` might have done. It might have held a reference to `bar`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now imagine you don't need to return the bar variable, and so you null it after
    you no longer need it, destroying the reference. That's better, right? No! If
    the `doSomething` function holds a reference to `bar`, there's nothing you can
    do outside `doSomething` to dereference it completely.
  prefs: []
  type: TYPE_NORMAL
- en: Even worse than this is the fact that the function can create a circular reference
    by creating a property that references itself in `bar`. But GC is clever enough
    to figure out when the rest of your application no longer uses an object. It depends
    on how complex your code is. Remember that if there is a doubt (that is, it's
    still referenced somewhere and can still be used), GC will not sweep the object.
  prefs: []
  type: TYPE_NORMAL
- en: In each cycle of its job, GC pauses V8 execution in what is called stop-the-world,
    knowing exactly where all objects are in the memory and what references exist.
    If there are too many references, GC will process only part of the object heap,
    minimizing the impact of the pause. The following image shows how V8 scans the
    memory objects, marks unreferenced ones (first row, in red), sweeps them from
    list (second row) and then compacts the list by removing empty spaces between
    objects (third row)
  prefs: []
  type: TYPE_NORMAL
- en: '![Referencing objects](img/4183_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous V8 GC generation had two algorithms for cleaning the old space:
    mark-sweep and mark-compact. In both the algorithms, GC went through the stack
    and marked reachable (referenced) objects. After that, it could use mark-sweep
    to just sweep the objects that weren''t reachable by freeing their memory, or
    use mark-compact to reallocate and compress the memory used. Both the algorithms
    worked at the page level. The problem with these two algorithms was that they
    introduced significant pauses in medium sized applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Referencing objects](img/4183_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In 2012, Google introduced an improvement that significantly reduced pauses
    in garbage collection cycles. It introduced incremental marking to avoid traversing
    a possibly huge zone. Instead, GC just goes through part of the zone to do the
    marking, making the pause smaller. Instead of a big pause, GC makes more pauses
    but small ones. But the improvement does not end here. After marking, GC does
    what is called a **lazy sweep**. Since GC knows exactly which objects are referenced
    and which are not (because of the previous mark step), it can now free the memory
    of the unreferenced ones (sweep). But it doesn't need to do that right away. Instead,
    it just sweeps on an as-needed basis. After sweeping them all, GC starts a new
    mark cycle again.
  prefs: []
  type: TYPE_NORMAL
- en: GC is fast as long as your program is kept lean and simple. Don't create a monolithic
    monster and then look for a way of raising the memory limit of V8\. On a 64-bit
    machine, you can almost double the 1 GB limit, but that's not the solution. You
    should really split your application. Even so, if you're thinking about changing
    the limit, the option in the node executable you're looking for is `--max-stack-size`
    (in bytes).
  prefs: []
  type: TYPE_NORMAL
- en: Object representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In V8, there are three primitive types: numbers, booleans and strings. Numbers
    have two forms: SMall Integers (SMI), which are 31-bit signed integers, or normal
    objects in situations such as doubles (big numbers) or numbers with extended properties.
    Strings also have two forms: one is inside the heap, and the other is outside
    the heap, with a wrapper object on the heap as a pointer to it.'
  prefs: []
  type: TYPE_NORMAL
- en: There are also other objects such as arrays, which are objects with a magic
    length property, and native objects, which are not in the heap itself (they're
    wrapped like some strings) and are therefore not managed or swept by GC.
  prefs: []
  type: TYPE_NORMAL
- en: Object heaps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GC stores objects in an object heap. The heap is divided into two main zones:
    new space and old space for—you guessed it—new objects and old objects respectively.
    New space is where objects are created and old space is where objects are moved
    to if they survive one or more GC cycles. Since GC is not constantly working,
    between cycles, objects can be created and they can be destroyed (and dereferenced)
    a few moments later. This is the most common object behavior, so GC usually sweeps
    them efficiently. Other objects live longer, and so they will survive cycles since
    they keep being referenced and used. This is where memory leaks can show up.'
  prefs: []
  type: TYPE_NORMAL
- en: These two spaces are designed with different goals in mind. The new space is
    smaller than the old one and is designed to be fast, meaningful, and analyzed
    by the GC very quickly. The old space is larger and contains objects moved there
    after a cycle. This old space can grow to a very large size, from a couple of
    megabytes to a gigabyte. This design takes advantage of the common behavior that
    most objects have a short lifetime and so live only on the new space, which is
    smaller and faster to manage.
  prefs: []
  type: TYPE_NORMAL
- en: Each space is composed of pages, contiguous blocks of memory that hold objects.
    Each page has a couple of headers on top and a bitmap telling GC what parts of
    the page the objects use.
  prefs: []
  type: TYPE_NORMAL
- en: This separation of objects and movement from one space to the other introduces
    some problems. One is, obviously, reallocation. Another is the need to know whether
    the references to an object in the new space are only in the old space. This is
    a possible situation and should prevent the object from being cleaned, but this
    would force GC to scan the old space to figure it out, breaking the speed of this
    architecture. To avoid this, GC maintains a list of references from the old space
    to the new space. This is another memory overhead but it's faster to scan this
    list. It's usually small since it's relatively rare to have this kind of reference.
  prefs: []
  type: TYPE_NORMAL
- en: The new space is small, and it's cheap to create new objects since it's just
    a matter of incrementing a pointer in the already reserved memory. When this new
    space gets full, a minor cycle is triggered to collect any dead objects and reclaim
    the space, avoiding the use of more space. If an object survives two minor cycles,
    it is moved to the old space.
  prefs: []
  type: TYPE_NORMAL
- en: In the old space, objects are swept in a major cycle that is less frequent than
    the minor one in the new space. This major cycle can get triggered when a certain
    amount of memory is reached in this space or after a more prolonged period of
    time. This cycle is less frequent and can stall the application for a little longer.
  prefs: []
  type: TYPE_NORMAL
- en: Heap snapshots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: V8 allows you to get a heap snapshot to analyze memory distribution across objects.
    It allows you to see what objects your code uses, how many of each are used, and
    how the application uses them if you request heap snapshot dumps over time. There
    are several ways of collecting a heap snapshot, and we'll look at some of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a small leaking program and analyze it with the `node-inspector`
    module. Open a terminal and install node inspector globally (`-g`) so that you
    can use it anywhere in your machine. In the following example, we''re using `sudo`
    since global modules usually reside in a restricted area:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The inspector needs to compile some modules, so you''ll need a compiler. If
    it installs correctly, you''ll see a list of installed dependencies and you can
    now start it. Once it''s running, there''s no need to restart it while you change
    and restart your program. Just start it now with no parameters and leave it in
    a terminal tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something similar to the following console output. You can see
    that I''m using version `0.10.0`, but you might get a different version. For the
    purpose of the example, it''s not actually critical that you use the same version.
    Depending on the version you use, the output may vary. In this case, it is something
    similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Open your web browser and head to the page indicated in the output. Now let''s
    create a program called `leaky`. The purpose of this program will be to leak memory
    intentionally. Create a folder and inside install the V8 profiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Be aware that this module can also need a compiler. Now, in the same folder,
    create a file called `leaky.js` with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The program can be confusing, but the idea is to blind GC from seeing that
    we''re forcing it not to garbage-collect objects, and so, leak memory. If you
    look more closely, you will see that `leakObject` gets redefined with a function
    that outputs it if called, but the way it references it makes GC unaware of our
    awful goal. Be aware that when running this program, you''ll starve the memory
    quite quickly, perhaps in the order of 100 megabytes per second. Run this with
    debug turned on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now head over to the web page you just opened, click on **Refresh**, go to
    the **Profiles** tab on the page, choose **Take Heap Snapshot**, and click on
    the **Take Snapshot** button, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Heap snapshots](img/4183_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Wait a minute and hit that button again. You'll see snapshots appearing on the
    left sidebar and you'll notice that they don't have the same size. They're growing
    and it's GC leaking our nonsense program. You can easily notice this if you select
    the last snapshot and choose to compare it with the first one.
  prefs: []
  type: TYPE_NORMAL
- en: You'll see that there's a delta change in both size and the new objects. A positive
    delta means that more objects were created than destroyed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Heap snapshots](img/4183_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can see in the preceding screenshot what the inspector looks like when showing
    a snapshot. There's a list of constructors or base objects. In this case, since
    we're comparing **Snapshot 3** with **Snapshot 1**, there are columns that show
    how many objects were created and deleted as well as how much memory was allocated
    and freed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful method for detecting memory leaks is recording object allocations
    over time. Using this very inspector, restart the program, head to **Profiles**,
    choose **Record Heap Allocations** and hit **Start**, as shown in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Heap snapshots](img/4183_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The inspector will start recording. It will stop when you click on the red circle
    in the top-left corner. You'll see a growing timeline and a bar chart for allocations
    for every minor cycle. If you wait a bit, you'll see major cycles and object reallocations
    (from new zones to old zones).
  prefs: []
  type: TYPE_NORMAL
- en: '![Heap snapshots](img/4183_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: After stopping, you can select a period of time by clicking on a start point
    and dragging it to the end point. You'll see only the allocations in that period,
    not all the objects. You can save the snapshot for later analysis or comparison.
    In this specific example, you can see how memory is quickly being consumed every
    second.
  prefs: []
  type: TYPE_NORMAL
- en: '![Heap snapshots](img/4183_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can click and expand the objects list to look at every object. If you're
    looking for a particular object, you can use the filter at the top. In this example,
    you can open the (string) group and you'll see there are several instances like
    `********…` that we created in our program.
  prefs: []
  type: TYPE_NORMAL
- en: '![Heap snapshots](img/4183_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using `v8-profiler` allows you to do more than just debug with `node-inspector`.
    You can, for example, take snapshots of your code and analyze it—maybe compare
    it with previous snapshots—or serialize and save it for later analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, taking the previous program example into consideration, we can
    periodically check how many nodes are there in our stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this new version, you might get an output similar to the following.
    This is a proof that objects are surviving GC cycles and leaking memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This is just an example. If you monitor your application and the memory keeps
    growing over time while it is idle (not doing anything), it is a reason to analyze
    further. The first-class citizens (so-called classes, for people coming from other
    object-oriented languages) will appear in the constructor list of the snapshots
    of your application.
  prefs: []
  type: TYPE_NORMAL
- en: There are other modules you can use to analyze and monitor your Node.js program
    memory and garbage collector. The `heapdump` module is another simple module that
    can help you just dump a heap snapshot every now and then to disk. Keep in mind
    that these snapshots are synchronous, so your program will pause for a moment
    if the heap is large.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use it, just install it like the other modules previously installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then change your program to use it. Here''s an example of a program that takes
    a snapshot to disk every minute. This is not a real or good use case, but perhaps
    a hourly snapshot with some kind of disposable script to avoid filling your disk
    might not be a bad idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The name of the file is the Unix date in milliseconds, so you will always know
    when it was taken. Run it and wait for at least one snapshot to be written to
    disk. In this case, you don't need to enable `debug` in the node (`--debug`).
  prefs: []
  type: TYPE_NORMAL
- en: You kept `node-inspector` running on the terminal, right? If not, please do
    it. Then go to its web page, as you did before, and refresh the page.
  prefs: []
  type: TYPE_NORMAL
- en: '![Heap snapshots](img/4183_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, instead of choosing **Take Snapshot**, just click on the **Load** button
    and choose the snapshots from your disk. This is another approach—an offline one—and
    it is usually more useful since you're usually not running your code in debug
    mode and looking at it live in v8-inspector. Also, `node-inspector` will restart
    the interface when your program stops, so you need to save your snapshots before
    restarting node-inspector.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have a memory leak you know of and you are able to reproduce it by just
    stressing it, you can use this approach and perhaps add a little twist to the
    execution of the program by activating GC trace lines for every action. You can
    then see when GC is sweeping or marking. The following is an example of what you''ll
    see if you monitor the GC actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Part of the previous output was truncated for clarity. The number *26503* is
    the process ID of the program in this example. You can see when the action took
    place and how long it took at the end of each trace line. You can also see the
    actions (`Scavenge` and `Mark-sweep`) and the memory evolution for each cycle.
  prefs: []
  type: TYPE_NORMAL
- en: For a running application, It's not feasible to have —trace-gc enabled (as in
    the previous command), and you should think of an approach that works for your
    architecture. One of the options is using `heapdump`, scheduling a snapshot every
    hour or so, and saving the last 10 or 20 snapshot. When using this approach, you
    should at least look at the last snapshot and compare it with the previous one
    to see how your application evolves over time. You might find slow memory leaks
    or very fast memory leaks. For the fast ones, you should be able to record heap
    allocations and rapidly stop leaks. For slow ones, it's harder to spot it, and
    only over very long periods are you able to compare changes and find the ghosts.
  prefs: []
  type: TYPE_NORMAL
- en: There's also another useful module that can help you spot leaks, which is called
    `memwatch`. This module will look for heap size changes, and when it finds that
    the heap size is constantly growing, it will emit a leak event (the irony). It
    also has a nice stats event with information on GC cycles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s change our initial program to use this module instead of any profilers
    or inspectors. Yes, it doesn''t need them, and it doesn''t even need you to enable
    node debug. First, let''s install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s change our program to something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now simply run the program. Let it run for a few seconds and you''ll see something
    similar to this example output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You will notice GC cycles occurring very often. This is because of our program
    behavior. GC adapts to rapid heap changes and triggers cycles more often. If you
    change the memory leak call period to 5 seconds or more, you will have to wait
    much longer to see cycles and leaks.
  prefs: []
  type: TYPE_NORMAL
- en: The `memwatch` module works by checking heap changes after GC sweeps and compacts
    it, so it won't trigger a leak just because your application is using memory,
    but because you're using it and not disposing it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another very useful feature of this module is the ability to help you compare
    heap snapshots. You do this by explicitly telling the module that you want a `heapdiff`.
    At this moment, the module snapshots heap, waits for your call to snapshot again,
    and compares it. After that, it will give you an object showing the totals before
    and after and the changes to each snapshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the program. After that, you''ll get an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If you look at the `change.details` array, you'll notice that you have a list
    of constructors that have changed between heaps. If you have a leak occurring
    between the snapshots, it will be in one of those items. In our case, it's the
    string constructor since we're leaking string variables.
  prefs: []
  type: TYPE_NORMAL
- en: With or without any of these modules, you should definitely monitor memory usage
    and growth. Rapid memory leaks will starve your resources and leave your clients
    unhappy. For high-load applications, you should create stress tests to be able
    to detect leaks before the application goes into production.
  prefs: []
  type: TYPE_NORMAL
- en: Third-party management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the spirit of dividing your application into smaller components, sometimes
    it might be a better idea to move some objects and manipulations to external services,
    which are sometimes optimized for specific workloads and object formats. Explore
    some of these servers before starting to manipulate large object structures:'
  prefs: []
  type: TYPE_NORMAL
- en: Memcached for key/values and Redis for lists, sets, and hash tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MongoDB if you want to run JavaScript on the data, and ElasticSearch for interesting
    features, such as data timeout or hierarchical elements (documents inside documents)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HBase if you need some complex map/reduce code, and Hypertable if you need a
    lightweight version of that code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OrientDB if you need a graph database, and Riak to store large binary data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your application is usually running on memory, so if it fails and stops, the
    memory used is lost and your precious data can be lost too. Using an external
    service to handle the data (and sometimes manipulate it) can greatly reduce your
    memory footprint. Moreover, these services usually allow you to access concurrently,
    enabling you to split the data manipulation effort for several instances of your
    application or tool.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You now see that the garbage collector task is not all that easy, but it certainly
    does a very good job managing memory automatically. You can help it a lot, especially
    if you are writing applications with performance in mind. Preventing the GC old
    space from growing is necessary to avoid long GC cycles. Otherwise, it can pause
    your application and sometimes force your services to restart. Every time you
    create a new variable, you allocate memory and inch closer to a new GC cycle.
    Even after understanding how memory is managed, you sometimes need to inspect
    your memory usage behavior. The cleanest way is by collecting snapshot heaps of
    the memory stack and analyzing using the V8 inspector or other similar pieces
    of software. The interface is self-explanatory, and leaks will show up simply
    if you sort the object list by shallow size, retained size, or reference counting.
    But before creating an application with a huge memory footprint, take a look at
    databases, whether relational or not, as this will help you store and manipulate
    the data, avoiding the need to do it yourself using the language. Remember that
    JavaScript was not designed to create computationally intensive tasks. If you
    still need to perform more intensive tasks, you might want to instrument the code
    to analyze and improve it so that you can achieve optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see what profiling is, what the benefits of doing
    it are, some available analysis tools, and how to understand results and upgrade
    your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepared for Bentham Chang, Safari ID bentham@gmail.com User number: 2843974
    © 2015 Safari Books Online, LLC. This download file is made available for personal
    use only and is subject to the Terms of Service. Any other use requires prior
    written consent from the copyright owner. Unauthorized use, reproduction and/or
    distribution are strictly prohibited and violate applicable laws. All rights reserved.'
  prefs: []
  type: TYPE_NORMAL
