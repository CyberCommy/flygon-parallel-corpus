- en: Statistics and Probability Refresher, and Python Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to go through a few concepts of statistics and
    probability, which might be a refresher for some of you. These concepts are important
    to go through if you want to be a data scientist. We will see examples to understand
    these concepts better. We will also look at how to implement those examples using
    actual Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Types of data you may encounter and how to treat them accordingly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical concepts of mean, median, mode, standard deviation, and variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probability density functions and probability mass functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of data distributions and how to plot them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding percentiles and moments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Alright, if you want to be a data scientist, we need to talk about the types
    of data that you might encounter, how to categorize them, and how you might treat
    them differently. Let''s dive into the different flavors of data you might encounter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1797a775-a439-4828-aa86-614c5c957af3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This will seem pretty basic, but we''ve got to start with the simple stuff
    and we''ll work our way up to the more complicated data mining and machine learning
    things. It is important to know what kind of data you''re dealing with because
    different techniques might have different nuances depending on what kind of data
    you''re handling. So, there are several flavors of data, if you will, and there
    are three specific types of data that we will primarily focus on. They are:'
  prefs: []
  type: TYPE_NORMAL
- en: Numerical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ordinal data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, there are different variations of techniques that you might use for different
    types of data, so you always need to keep in mind what kind of data you're dealing
    with when you're analyzing it.
  prefs: []
  type: TYPE_NORMAL
- en: Numerical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with numerical data. It's probably the most common data type. Basically,
    it represents some quantifiable thing that you can measure. Some examples are
    heights of people, page load times, stock prices, and so on. Things that vary,
    things that you can measure, things that have a wide range of possibilities. Now
    there are basically two kinds of numerical data, so a flavor of a flavor if you
    will.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's discrete data, which is integer-based and, for example, can be counts
    of some sort of event. Some examples are how many purchases did a customer make
    in a year. Well, that can only be discrete values. They bought one thing, or they
    bought two things, or they bought three things. They couldn't have bought, 2.25
    things or three and three-quarters things. It's a discrete value that has an integer
    restriction to it.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The other type of numerical data is continuous data, and this is stuff that
    has an infinite range of possibilities where you can go into fractions. So, for
    example, going back to the height of people, there is an infinite number of possible
    heights for people. You could be five feet and 10.37625 inches tall, or the time
    it takes to do something like check out on a website could be any huge range of
    possibilities, 10.7625 seconds for all you know, or how much rainfall in a given
    day. Again, there's an infinite amount of precision there. So that's an example
    of continuous data.
  prefs: []
  type: TYPE_NORMAL
- en: To recap, numerical data is something you can measure quantitatively with a
    number, and it can be either discrete, where it's integer-based like an event
    count, or continuous, where you can have an infinite range of precision available
    to that data.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second type of data that we're going to talk about is categorical data,
    and this is data that has no inherent numeric meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, you can't really compare one category to another directly.
    Things like gender, yes/no questions, race, state of residence, product category,
    political party; you can assign numbers to these categories, and often you will,
    but those numbers have no inherent meaning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c0d03fd-5e54-4a21-a114-327a65405380.png)'
  prefs: []
  type: TYPE_IMG
- en: So, for example, I can say that the area of Texas is greater than the area of
    Florida, but I can't just say Texas is greater than Florida, they're just categories.
    There's no real numerical quantifiable meaning to them, it's just ways that we
    categorize different things.
  prefs: []
  type: TYPE_NORMAL
- en: Now again, I might have some sort of numerical assignation to each state. I
    mean, I could say that Florida is state number 3 and Texas state number 4, but
    there's no real relationship between 3 and 4 there, right, it's just a shorthand
    to more compactly represent these categories. So again, categorical data does
    not have any intrinsic numerical meaning; it's just a way that you're choosing
    to split up a set of data based on categories.
  prefs: []
  type: TYPE_NORMAL
- en: Ordinal data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last category that you tend to hear about with types of data is ordinal
    data, and it's sort of a mixture of numerical and categorical data. A common example
    is star ratings for a movie or music, or what have you.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e6a7491-49ae-4dc7-aa89-ed143bc3c5e4.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, we have categorical data in that could be 1 through 5 stars, where
    1 might represent poor and 5 might represent excellent, but they do have mathematical
    meaning. We do know that 5 means it's better than a 1, so this is a case where
    we have data where the different categories have a numerical relationship to each
    other. So, I can say that 1 star is less than 5 stars, I can say that 2 stars
    is less than 3 stars, I can say that 4 stars is greater than 2 stars in terms
    of a measure of quality. Now you could also think of the actual number of stars
    as discrete numerical data. So, it's definitely a fine line between these categories,
    and in a lot of cases you can actually treat them interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: So, there you have it, the three different types. There is numerical, categorical,
    and ordinal data. Let's see if it's sunk in. Don't worry, I'm not going to make
    you hand in your work or anything.
  prefs: []
  type: TYPE_NORMAL
- en: '**Quick quiz:** For each of these examples, is the data numerical, categorical,
    or ordinal?'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with how much gas is in your gas tank. What do you think? Well,
    the right answer is numerical. It's a continuous numerical value because you can
    have an infinite range of possibilities of gas in your tank. I mean, yeah, there's
    probably some upper bound of how much gas you can fit in it, but there is no end
    to the number of possible values of how much gas you have. It could be three quarters
    of a tank, it could be seven sixteenths of the tank, it could be *1/pi* of a tank,
    I mean who knows, right?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How about if you're reading your overall health on a scale of 1 to 4, where
    those choices correspond to the categories poor, moderate, good, and excellent?
    What do you think? That's a good example of ordinal data. That's very much like
    our movie ratings data, and again, depending on how you model that, you could
    probably treat it as discrete numerical data as well, but technically we're going
    to call that ordinal data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What about the races of your classmates? This is a pretty clear example of categorical
    data. You can't really compare purple people to green people, right, they're just
    purple and green, but they are categories that you might want to study and understand
    the differences between on some other dimension.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How about the ages of your classmates in years? A little bit of a trick question
    there; if I said it had to be in an integer value of years, like 40, 50, or 55
    years old, then that would be discrete numerical data, but if I had more precision,
    like 40 years three months and 2.67 days, that would be continuous numerical data,
    but either way, it's a numerical data type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And finally, money spent in a store. Again, that could be an example of continuous
    numerical data. So again, this is only important because you might apply different
    techniques to different types of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There might be some concepts where we do one type of implementation for categorical
    data and a different type of implementation for numerical data, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'So that''s all you need to know about the different types of data that you''ll
    commonly find, and that we''ll focus on in this book. They''re all pretty simple
    concepts: you''ve got numeric, categorical, and ordinal data, and numerical data
    can be continuous or discrete. There might be different techniques you apply to
    the data depending on what kind of data you''re dealing with, and we''ll see that
    throughout the book. Let''s move on.'
  prefs: []
  type: TYPE_NORMAL
- en: Mean, median, and mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s do a little refresher of statistics 101\. This is like elementary school
    stuff, but good to go through it again and see how these different techniques
    are used: Mean, median, and mode. I''m sure you''ve heard those terms before,
    but it''s good to see how they''re used differently, so let''s dive in.'
  prefs: []
  type: TYPE_NORMAL
- en: This should be a review for most of you, a quick refresher, now that we're starting
    to actually dive into some real statistics. Let's look at some actual data and
    figure out how to measure these things.
  prefs: []
  type: TYPE_NORMAL
- en: Mean
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The mean, as you probably know, is just another name for the average. To calculate
    the mean of a dataset, all you have to do is sum up all the values and divide
    it by the number of values that you have.
  prefs: []
  type: TYPE_NORMAL
- en: '*Sum of samples/Number of samples*'
  prefs: []
  type: TYPE_NORMAL
- en: Let's take this example, which calculates the mean (average) number of children
    per house in my neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say I went door-to-door in my neighborhood and asked everyone, how many
    children live in their household. (That, by the way, is a good example of discrete
    numerical data; remember from the previous section?) Let's say I go around and
    I found out that the first house has no kids in it, and the second house has two
    children, and the third household has three children, and so on and so forth.
    I amassed this little dataset of discrete numerical data, and to figure out the
    mean, all I do is add them all together and divide it by the number of houses
    that I went to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of children in each house on my street:'
  prefs: []
  type: TYPE_NORMAL
- en: '*0, 2, 3, 2, 1, 0, 0, 2, 0*'
  prefs: []
  type: TYPE_NORMAL
- en: The mean is *(0+2+3+2+1+0+0+2+0)/9 = 1.11*
  prefs: []
  type: TYPE_NORMAL
- en: It comes out as 0 plus 2 plus 3 plus all the rest of these numbers divided by
    the total number of houses that I looked at, which is 9, and the mean number of
    children per house in my sample is 1.11\. So, there you have it, mean.
  prefs: []
  type: TYPE_NORMAL
- en: Median
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Median is a little bit different. The way you compute the median of the dataset
    is by sorting all the values (in either ascending or descending order), and taking
    the one that ends up in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: So, for example, let's use the same dataset of children in my neighborhood
  prefs: []
  type: TYPE_NORMAL
- en: '*0, 2, 3, 2, 1, 0, 0, 2, 0*'
  prefs: []
  type: TYPE_NORMAL
- en: I would sort it numerically, and I can take the number that's slap dab in the
    middle of the data, which turns out to be 1.
  prefs: []
  type: TYPE_NORMAL
- en: '*0, 0, 0, 0, 1, 2, 2, 2, 3*'
  prefs: []
  type: TYPE_NORMAL
- en: Again, all I do is take the data, sort it numerically, and take the center point.
  prefs: []
  type: TYPE_NORMAL
- en: If you have an even number of data points, then the median might actually fall
    in between two data points. It wouldn't be clear which one is actually the middle.
    In that case, all you do is, take the average of the two that do fall in the middle
    and consider that number as the median.
  prefs: []
  type: TYPE_NORMAL
- en: The factor of outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now in the preceding example of the number of kids in each household, the median
    and the mean were pretty close to each other because there weren't a lot of outliers.
    We had 0, 1, 2, or 3 kids, but we didn't have some wacky family that had 100 kids.
    That would have really skewed the mean, but it might not have changed the median
    too much. That's why the median is often a very useful thing to look at and often
    overlooked.
  prefs: []
  type: TYPE_NORMAL
- en: Median is less susceptible to outliers than the mean.
  prefs: []
  type: TYPE_NORMAL
- en: People have a tendency to mislead people with statistics sometimes. I'm going
    to keep pointing this out throughout the book wherever I can.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can talk about the mean or average household income in the
    United States, and that actual number from last year when I looked it up was $72,000
    or so, but that doesn't really provide an accurate picture of what the typical
    American makes. That is because, if you look at the median income, it's much lower
    at $51,939\. Why is that? Well, because of income inequality. There are a few
    very rich people in America, and the same is true in a lot of countries as well.
    America's not even the worst, but you know those billionaires, those super-rich
    people that live on Wall Street or Silicon Valley or some other super-rich place,
    they skew the mean. But there's so few of them that they don't really affect the
    median so much.
  prefs: []
  type: TYPE_NORMAL
- en: This is a great example of where the median tells a much better story about
    the typical person or data point in this example than the mean does. Whenever
    someone talks about the mean, you have to think about what does the data distribution
    looks like. Are there outliers that might be skewing that mean? And if the answer
    is potentially yes, you should also ask for the median, because often, that provides
    more insight than the mean or the average.
  prefs: []
  type: TYPE_NORMAL
- en: Mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we'll talk about mode. This doesn't really come up too often in practice,
    but you can't talk about mean and median without talking about mode. All mode
    means, is the most common value in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go back to my example of the number of kids in each house.
  prefs: []
  type: TYPE_NORMAL
- en: '*0, 2, 3, 2, 1, 0, 0, 2, 0*'
  prefs: []
  type: TYPE_NORMAL
- en: 'How many of each value are there:'
  prefs: []
  type: TYPE_NORMAL
- en: '*0: 4, 1: 1, 2: 3, 3: 1*'
  prefs: []
  type: TYPE_NORMAL
- en: The MODE is 0
  prefs: []
  type: TYPE_NORMAL
- en: If I just look at what number occurs most frequently, it turns out to be 0,
    and the mode therefore of this data is 0\. The most common number of children
    in a given house in this neighborhood is no kids, and that's all that means.
  prefs: []
  type: TYPE_NORMAL
- en: Now this is actually a pretty good example of continuous versus discrete data,
    because this only really works with discrete data. If I have a continuous range
    of data then I can't really talk about the most common value that occurs, unless
    I quantize that somehow into discrete values. So we've already run into one example
    here where the data type matters.
  prefs: []
  type: TYPE_NORMAL
- en: Mode is usually only relevant to discrete numerical data, and not to continuous
    data.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of real-world data tends to be continuous, so maybe that's why I don't
    hear too much about mode, but we see it here for completeness.
  prefs: []
  type: TYPE_NORMAL
- en: 'There you have it: mean, median, and mode in a nutshell. Kind of the most basic
    statistics stuff you can possibly do, but I hope you gained a little refresher
    there in the importance of choosing between median and mean. They can tell very
    different stories, and yet people tend to equate them in their heads, so make
    sure you''re being a responsible data scientist and representing data in a way
    that conveys the meaning you''re trying to represent. If you''re trying to display
    a typical value, often the median is a better choice than the mean because of
    outliers, so remember that. Let''s move on.'
  prefs: []
  type: TYPE_NORMAL
- en: Using mean, median, and mode in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start doing some real coding in Python and see how you compute the mean,
    median, and mode using Python in an IPython Notebook file.
  prefs: []
  type: TYPE_NORMAL
- en: So go ahead and open up the `MeanMedianMode.ipynb` file from the data files
    for this section if you'd like to follow along, which I definitely encourage you
    to do. If you need to go back to that earlier section on where to download these
    materials from, please go do that, because you will need these files for the section.
    Let's dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Calculating mean using the NumPy package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What we're going to do is create some fake income data, getting back to our
    example from the previous section. We're going to create some fake data where
    the typical American makes around $27,000 a year in this example, we're going
    to say that's distributed with a normal distribution and a standard deviation
    of 15,000\. All numbers are completely made up, and if you don't know what normal
    distribution and standard deviation means yet, don't worry. I'm going to cover
    that a little later in the chapter, but I just want you to know what these different
    parameters represent in this example. It will make sense later on.
  prefs: []
  type: TYPE_NORMAL
- en: In our Python notebook, remember to import the NumPy package into Python, which
    makes computing mean, median, and mode really easy. We're going to use the `import
    numpy as np` directive, which means we can use `np` as a shorthand to call `numpy`
    from now on.
  prefs: []
  type: TYPE_NORMAL
- en: Then we're going to create a list of numbers called `incomes` using the `np.random.normal`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The three parameters of the `np.random.normal` function mean I want the data
    centered around `27000`, with a standard deviation of `15000`, and I want python
    to make `10000` data points in this list.
  prefs: []
  type: TYPE_NORMAL
- en: Once I do that, I compute the average of those data points, or the mean by just
    calling `np.mean` on `incomes` which is my list of data. It's just that simple.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go ahead and run that. Make sure you selected that code block and then
    you can hit the play button to actually execute it, and since there is a random
    component to these income numbers, every time I run it, I'm going to get a slightly
    different result, but it should always be pretty close to `27000`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: OK, so that's all there is to computing the mean in Python, just using NumPy
    (`np.mean`) makes it super easy. You don't have to write a bunch of code or actually
    add up everything and count up how many items you had and do the division. NumPy
    mean, does all that for you.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing data using matplotlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's visualize this data to make it make a little more sense. So there's another
    package called `matplotlib`, and again we're going to talk about that a lot more
    in the future as well, but it's a package that lets me make pretty graphs in IPython
    Notebooks, so it's an easy way to visualize your data and see what's going on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we are using `matplotlib` to create a histogram of our income
    data broken up into `50` different buckets. So basically, we''re taking our continuous
    data and discretizing it, and then we can call show on `matplotlib.pyplot` to
    actually display this histogram in line. Refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Go ahead and select the code block and hit play. It will actually create a
    new graph for us as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33c9aad3-bf0f-4e80-bfad-616593e5b38d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you're not familiar with histograms or you need a refresher, the way to interpret
    this is that for each one of these buckets that we've discretized our data into
    is showing the frequency of that data.
  prefs: []
  type: TYPE_NORMAL
- en: So, for example, around 27,000-ish we see there's about **600** data points
    in that neighborhood for each given range of values. There's a lot of people around
    the 27,000 mark, but when you get over to outliers like **80,000**, there is not
    a whole lot, and apparently there's some poor souls that are even in debt at **-40,000**,
    but again, they're very rare and not probable because we defined a normal distribution,
    and this is what a normal probability curve looks like. Again, we're going to
    talk about that more in detail later, but I just want to get that idea in your
    head if you don't already know it.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating median using the NumPy package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alright, so computing the median is just as simple as computing the mean. Just
    like we had NumPy `mean`, we have a NumPy `median` function as well.
  prefs: []
  type: TYPE_NORMAL
- en: We can just use the `median` function on `incomes`, which is our list of data,
    and that will give us the median. In this case, that came up to $26,911, which
    isn't very different from the mean of $26988\. Again, the initial data was random,
    so your values will be slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We don't expect to see a lot of outliers because this is a nice normal distribution.
    Median and mean will be comparable when you don't have a lot of weird outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the effect of outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just to prove a point, let's add in an outlier. We'll take Donald Trump; I think
    he qualifies as an outlier. Let's go ahead and add his income in. So I'm going
    to manually add this to the data using `np.append`, and let's say add a billion
    dollars (which is obviously not the actual income of Donald Trump) into the incomes
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'What we''re going to see is that this outlier doesn''t really change the median
    a whole lot, you know, that''s still going to be around the same value $26,911,
    because we didn''t actually change where the middle point is, with that one value,
    as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives a new output of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Aha, so there you have it! It is a great example of how median and mean, although
    people tend to equate them in commonplace language, can be very different, and
    tell a very different story. So that one outlier caused the average income in
    this dataset to be over $127160 a year, but the more accurate picture is closer
    to 27,000 dollars a year for the typical person in this dataset. We just had the
    mean skewed by one big outlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The moral of the story is: take anyone who talks about means or averages with
    a grain of salt if you suspect there might be outliers involved, and income distribution
    is definitely a case of that.'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating mode using the SciPy package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, let's look at mode. We will just generate a bunch of random integers,
    500 of them to be precise, that range between `18` and `90`. We're going to create
    a bunch of fake ages for people.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Your output will be random, but should look something like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85aa43b4-d2c8-480e-a310-720ff89ec93e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, SciPy, kind of like NumPy, is a bunch of like handy-dandy statistics functions,
    so we can import `stats` from SciPy using the following syntax. It's a little
    bit different than what we saw before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The code means, from the `scipy` package import `stats`, and I''m just going
    to refer to the package as `stats`, Tha means that I don''t need to have an alias
    like I did before with NumPy, just different way of doing it. Both ways work.
    Then, I used the `stats.mode` function on `ages`, which is our list of random
    ages. When we execute the above code, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: So in this case, the actual mode is `39` that turned out to be the most common
    value in that array. It actually occurred `12` times.
  prefs: []
  type: TYPE_NORMAL
- en: Now if I actually create a new distribution, I would expect a completely different
    answer because this data really is completely random what these numbers are. Let's
    execute the above code blocks again to create a new distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for randomizing the equation is as distribution is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4335f346-1b95-4644-a403-da39a552f22a.png)'
  prefs: []
  type: TYPE_IMG
- en: Make sure you selected that code block and then you can hit the play button
    to actually execute it.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the mode ended up being the number `29`, which occurred `14` times.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: So, it's a very simple concept. You can do it a few more times just for fun.
    It's kind of like rolling the roulette wheel. We'll create a new distribution
    again.
  prefs: []
  type: TYPE_NORMAL
- en: There you have it, mean, median, and mode in a nutshell. It's very simple to
    do using the SciPy and NumPy packages.
  prefs: []
  type: TYPE_NORMAL
- en: Some exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I'm going to give you a little assignment in this section. If you open up `MeanMedianExercise.ipynb`
    file, there's some stuff you can play with. I want you to roll up your sleeves
    and actually try to do this.
  prefs: []
  type: TYPE_NORMAL
- en: In the file, we have some random e-commerce data. What this data represents
    is the total amount spent per transaction, and again, just like with our previous
    example, it's just a normal distribution of data. We can run that, and your homework
    is to go ahead and find the mean and median of this data using the NumPy package.
    Pretty much the easiest assignment you could possibly imagine. All the techniques
    you need are in the `MeanMedianMode.ipynb` file that we used earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The point here is not really to challenge you, it''s just to make you actually
    write some Python code and convince yourself that you can actually get a result
    and make something happen here. So, go ahead and play with that. If you want to
    play with it some more, feel free to play around with the data distribution here
    and see what effect you can have on the numbers. Try adding some outliers, kind
    of like we did with the income data. This is the way to learn this stuff: master
    the basics and the advance stuff will follow. Have at it, have fun.'
  prefs: []
  type: TYPE_NORMAL
- en: Once your're ready, let's move forward to our next concept, standard deviation
    and variance.
  prefs: []
  type: TYPE_NORMAL
- en: Standard deviation and variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's talk about standard deviation and variance. The concepts and terms you've
    probably heard before, but let's go into a little bit more depth about what they
    really mean and how you compute them. It's a measure of the spread of a data distribution,
    and that will make a little bit more sense in a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Standard deviation and variance are two fundamental quantities for a data distribution
    that you'll see over and over again in this book. So, let's see what they are,
    if you need a refresher.
  prefs: []
  type: TYPE_NORMAL
- en: Variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at a histogram, because variance and standard deviation are all
    about the spread of the data, the shape of the distribution of a dataset. Take
    a look at the following histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cacd8ae5-44dc-4b10-8a09-de01df532676.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's say that we have some data on the arrival frequency of airplanes at an
    airport, for example, and this histogram indicates that we have around 4 arrivals
    per minute and that happened on around 12 days that we looked at for this data.
    However, we also have these outliers. We had one really slow day that only had
    one arrival per minute, we only had one really fast day where we had almost 12
    arrivals per minute. So, the way to read a histogram is look up the bucket of
    a given value, and that tells you how frequently that value occurred in your data,
    and the shape of the histogram could tell you a lot about the probability distribution
    of a given set of data.
  prefs: []
  type: TYPE_NORMAL
- en: We know from this data that our airport is very likely to have around 4 arrivals
    per minute, but it's very unlikely to have 1 or 12, and we can also talk specifically
    about the probabilities of all the numbers in between. So not only is it unlikely
    to have 12 arrivals per minute, it's also very unlikely to have 9 arrivals per
    minute, but once we start getting around 8 or so, things start to pick up a little
    bit. A lot of information can be had from a histogram.
  prefs: []
  type: TYPE_NORMAL
- en: Variance measures how *spread-out* the data is.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We usually refer to variance as sigma squared, and you'll find out why momentarily,
    but for now, just know that variance is the average of the squared differences
    from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: To compute the variance of a dataset, you first figure out the mean of it. Let's
    say I have some data that could represent anything. Let's say maximum number of
    people that were standing in line for a given hour. In the first hour, I observed
    1 person standing in line, then 4, then 5, then 4, then 8.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first step in computing the variance is just to find the mean, or the average,
    of that data. I add them all, divide the sum by the number of data points, and
    that comes out to 4.4 which is the average number of people standing in line *(1+4+5+4+8)/5
    = 4.4*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now the next step is to find the differences from the mean for each data point.
    I know that the mean is 4.4\. So for my first data point, I have 1, so *1 - 4.4
    = -3.4*, The next data point is 4, so 4 - 4.4 = -0.4 *4 - 4.4 = -0.4*, and so
    on and so forth. OK, so I end up with these both positive and negative numbers
    that represent the variance from the mean for each data point *(-3.4, -0.4, 0.6,
    -0.4, 3.6)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now what I need is a single number that represents the variance of this entire
    dataset. So, the next thing I''m going to do is find the square of these differences.
    I''m just going to go through each one of those raw differences from the mean
    and square them. This is for a couple of different reasons:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, I want to make sure that negative variances. Count just as much as positive
    variances. Otherwise, they will cancel each other out. That'd be bad.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, I also want to give more weight to the outliers, so this amplifies the
    effect of things that are very different from the mean while still, making sure
    that the negatives and positives are comparable *(11.56, 0.16, 0.36, 0.16, 12.96)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at what happens there, so (-3.4)² is a positive 11.56 and (-0.4)²
    ends up being a much smaller number, that is 0.16, because that''s much closer
    to the mean of 4.4\. Also (0.6)² turned out to be close to the mean, only 0.36\.
    But as we get up to the positive outlier, (3.6)² ends up being 12.96\. That gives
    us: *(11.56, 0.16, 0.36, 0.16, 12.96).*'
  prefs: []
  type: TYPE_NORMAL
- en: To find the actual variance value, we just take the average of all those squared
    differences. So we add up all these squared variances, divide the sum by 5, that
    is number of values that we have, and we end up with a variance of 5.04.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c0450ae-7518-4fd5-913d-834fa7690b3b.png)'
  prefs: []
  type: TYPE_IMG
- en: OK, that's all variance is.
  prefs: []
  type: TYPE_NORMAL
- en: Standard deviation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now typically, we talk about standard deviation more than variance, and it turns
    out standard deviation is just the square root of the variance. It's just that
    simple.
  prefs: []
  type: TYPE_NORMAL
- en: So, if I had this variance of *5.04*, the standard deviation is *2.24*. So you
    see now why we said that the variance = (σ)². It's because σ itself represents
    the standard deviation. So, if I take the square root of (σ)², I get sigma. That
    ends up in this example to be 2.24.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5593f79-70de-45c1-95f6-a481fcd4932e.png)'
  prefs: []
  type: TYPE_IMG
- en: Identifying outliers with standard deviation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here's a histogram of the actual data we were looking at in the preceding example
    for calculating variance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f44b86df-acba-4f0d-8118-d77696204ddb.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we see that the number **4** occurred twice in our dataset, and then we
    had one **1**, one **5**, and one **8**.
  prefs: []
  type: TYPE_NORMAL
- en: The standard deviation is usually used as a way to think about how to identify
    outliers in your dataset. If I say if I'm within one standard deviation of the
    mean of 4.4, that's considered to be kind of a typical value in a normal distribution.
    However, you can see in the preceding diagram, that the numbers **1** and **8**
    actually lie outside of that range. So if I take 4.4 plus or minus 2.24, we end
    up around **7** and **2**, and **1** and **8** both fall outside of that range
    of a standard deviation. So we can say mathematically, that 1 and 8 are outliers.
    We don't have to guess and eyeball it. Now there is still a judgment call as to
    what you consider an outlier in terms of how many standard deviations a data point
    is from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: You can generally talk about how much of an outlier a data point is by how many
    standard deviations (or sometimes how many-sigmas) from the mean it is.
  prefs: []
  type: TYPE_NORMAL
- en: So that's something you'll see standard deviation used for in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Population variance versus sample variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a little nuance to standard deviation and variance, and that's when
    you're talking about population versus sample variance. If you're working with
    a complete set of data, a complete set of observations, then you do exactly what
    I told you. You just take the average of all the squared variances from the mean
    and that's your variance.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you're sampling your data, that is, if you're taking a subset of
    the data just to make computing easier, you have to do something a little bit
    different. Instead of dividing by the number of samples, you divide by the number
    of samples minus 1\. Let's look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use the sample data we were just studying for people standing in a line.
    We took the sum of the squared variances and divided by 5, that is the number
    of data points that we had, to get 5.04.
  prefs: []
  type: TYPE_NORMAL
- en: '*σ² = (11.56 + 0.16 + 0.36 + 0.16 + 12.96) / 5 = 5.04*'
  prefs: []
  type: TYPE_NORMAL
- en: If we were to look at the sample variance, which is designated by S², it is
    found by the sum of the squared variances divided by 4, that is *(n - 1)*. This
    gives us the sample variance, which comes out to 6.3.
  prefs: []
  type: TYPE_NORMAL
- en: '*S² = (11.56 + 0.16 + 0.36 + 0.16 + 12.96) / 4 = 6.3*'
  prefs: []
  type: TYPE_NORMAL
- en: So again, if this was some sort of sample that we took from a larger dataset,
    that's what you would do. If it was a complete dataset, you divide by the actual
    number. Okay, that's how we calculate population and sample variance, but what's
    the actual logic behind it?
  prefs: []
  type: TYPE_NORMAL
- en: The Mathematical explanation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As for why there is difference between population and sample variance, it gets
    into really weird things about probability that you probably don't want to think
    about too much, and it requires some fancy mathematical notation, I try to avoid
    notation in this book as much as possible because I think the concepts are more
    important, but this is basic enough stuff and that you will see it over and over
    again.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we''ve seen, population variance is usually designated as sigma squared
    (σ²), with sigma (σ) as standard deviation, and we can say that is the summation
    of each data point X minus the mean, mu, squared, that''s the variance of each
    sample squared over N, the number of data points , and we can express it with
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d3eb6fb-41e3-4d56-ab18-1d5ccd2a2bb4.png)'
  prefs: []
  type: TYPE_IMG
- en: X denotes each data point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: µ denotes the mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: N denotes the number of data points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sample variance similarly is designated as S², with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76eac09f-5b97-4c46-92a6-620407eaba69.png)'
  prefs: []
  type: TYPE_IMG
- en: X denotes each data point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: M denotes the mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: N-1 denotes the number of data points minus 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That's all there is to it.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing standard deviation and variance on a histogram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s write some code here and play with some standard deviation and variances.
    So If you pull up the `StdDevVariance.ipynb` file IPython Notebook, and follow
    along with me here. Please do, because there''s an activity at the end that I
    want you to try. What we''re going to do here is just like the previous example,
    so begin with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We use `matplotlib` to plot a histogram of some normally distributed random
    data, and we call it `incomes`. We're saying it's going to be centered around
    `100` (hopefully that's an hourly rate or something and not annual, or some weird
    denomination), with a standard deviation of `20` and `10,000` data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and generate that by executing that above code block and plotting
    it as shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff6cfb24-e6a4-4884-b7b4-0008d321351f.png)'
  prefs: []
  type: TYPE_IMG
- en: We have 10,000 data points centered around 100\. With a normal distribution
    and a standard deviation of 20, a measure of the spread of this data, you can
    see that the most common occurrence is around 100, and as we get further and further
    from that, things become less and less likely. The standard deviation point of
    20 that we specified is around 80 and around 120\. You can see in the histogram
    that this is the point where things start to fall off sharply, so we can say that
    things beyond that standard deviation boundary are unusual.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python to compute standard deviation and variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, NumPy also makes it incredibly easy to compute the standard deviation
    and variance. If you want to compute the actual standard deviation of this dataset
    that we generated, you just call the `std` function right on the dataset itself.
    So, when NumPy creates the list, it''s not just a normal Python list, it actually
    has some extra stuff tacked onto it so you can call functions on it, like `std`
    for standard deviation. Let''s do that now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us something like the following output (remember that we used random
    data, so your figures won''t be exactly the same as mine):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: When we execute that, we get a number pretty close to 20, because that's what
    we specified when we created our random data. We wanted a standard deviation of
    20\. Sure enough, 20.02, pretty close.
  prefs: []
  type: TYPE_NORMAL
- en: The variance is just a matter of calling `var`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives me the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: It comes out to pretty close to 400, which is 20². Right, so the world makes
    sense! Standard deviation is just the square root of the variance, or you could
    say that the variance is the standard deviation squared. Sure enough, that works
    out, so the world works the way it should.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I want you to dive in here and actually play around with it, make it real, so
    try out different parameters on generating that normal data. Remember, this is
    a measure of the shape of the distribution of the data, so what happens if I change
    that center point? Does it matter? Does it actually affect the shape? Why don't
    you try it out and find out?
  prefs: []
  type: TYPE_NORMAL
- en: Try messing with the actual standard deviation, that we've specified, to see
    what impact that has on the shape of the graph. Maybe try a standard deviation
    of 30, and you know, you can see how that actually affects things. Let's make
    it even more dramatic, like 50\. Just play around with 50\. You'll see the graph
    starting to get a little bit fatter. Play around with different values, just get
    a feel of how these values work. This is the only way to really get an intuitive
    sense of standard deviation and variance. Mess around with some different examples
    and see the effect that it has.
  prefs: []
  type: TYPE_NORMAL
- en: So that's standard deviation and variance in practice. You got hands on with
    some of it there, and I hope you played around a little bit to get some familiarity
    with it. These are very important concepts and we'll talk about standard deviations
    a lot throughout the book and no doubt throughout your career in data science,
    so make sure you've got that under your belt. Let's move on.
  prefs: []
  type: TYPE_NORMAL
- en: Probability density function and probability mass function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So we've already seen some examples of a normal distribution function for some
    of the examples in this book. That's an example of a probability density function,
    and there are other types of probability density functions out there. So let's
    dive in and see what it really means and what some other examples of them are.
  prefs: []
  type: TYPE_NORMAL
- en: The probability density function and probability mass functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've already seen some examples of a normal distribution function for some
    of the code we've looked at in this book. That's an example of a probability density
    function, and there are other types of probability density functions out there.
    Let's dive in and see what that really means and what some other examples of them
    there are.
  prefs: []
  type: TYPE_NORMAL
- en: Probability density functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's talk about probability density functions, and we've used one of these
    already in the book. We just didn't call it that. Let's formalize some of the
    stuff that we've talked about. For example, we've seen the normal distribution
    a few times, and that is an example of a probability density function. The following
    figure is an example of a normal distribution curve
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dae2b0a5-75ab-4fe9-bc80-358cdbc8a670.png)'
  prefs: []
  type: TYPE_IMG
- en: It's conceptually easy to try to think of this graph as the probability of a
    given value occurring, but that's a little bit misleading when you're talking
    about continuous data. Because there's an infinite number of actual possible data
    points in a continuous data distribution. There could be 0 or 0.001 or 0.00001
    so the actual probability of a very specific value happening is very, very small,
    infinitely small. The probability density function really tells the probability
    of a given range of values occurring. So that's the way you've got to think about
    it.
  prefs: []
  type: TYPE_NORMAL
- en: So, for example, in the normal distribution shown in the above graph, between
    the mean (**0**) and one standard deviation from the mean (**1σ**) there's a **34.1%**
    chance of a value falling in that range. You can tighten this up or spread it
    out as much as you want, figure out the actual values, but that's the way to think
    about a probability density function. For a given range of values it gives you
    a way of finding out the probability of that range occurring.
  prefs: []
  type: TYPE_NORMAL
- en: You can see in the graph, as you get close to the mean (**0**), within one standard
    deviation (**-1σ** and **1σ**), you're pretty likely to land there. I mean, if
    you add up 34.1 and 34.1, which equals to 68.2%, you get the probability of landing
    within one standard deviation of the mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, as you get between two and three standard deviations (**-3σ** to **-2σ**
    and **2σ** to **3σ**), we're down to just a little bit over 4% (4.2%, to be precise).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you get out beyond three standard deviations (**-3σ** and **3σ**) then we're
    much less than 1%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, the graph is just a way to visualize and talk about the probabilities of
    the given data point happening. Again, a probability distribution function gives
    you the probability of a data point falling within some given range of a given
    value, and a normal function is just one example of a probability density function.
    We'll look at some more in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: Probability mass functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now when you''re dealing with discrete data, that little nuance about having
    infinite numbers of possible values goes away, and we call that something different.
    So that is a probability mass function. If you''re dealing with discrete data,
    you can talk about probability mass functions. Here''s a graph to help visualize
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18a1b7a7-3456-4c58-8d7d-30cd73c7c1d0.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, you can plot a normal probability density function of continuous
    data on the black curve shown in the graph, but if we were to quantize that into
    a discrete dataset like we would do with a histogram, we can say the number 3
    occurs some set number of times, and you can actually say the number 3 has a little
    over 30% chance of occurring. So a probability mass function is the way that we
    visualize the probability of discrete data occurring, and it looks a lot like
    a histogram because it basically is a histogram.
  prefs: []
  type: TYPE_NORMAL
- en: 'Terminology difference: A probability density function is a solid curve that
    describes the probability of a range of values happening with continuous data.
    A probability mass function is the probabilities of given discrete values occurring
    in a dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Types of data distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look at some real examples of probability distribution functions and data
    distributions in general and wrap your head a little bit more around data distributions
    and how to visualize them and use them in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and open up the `Distributions.ipynb` from the book materials, and
    you can follow along with me here if you'd like.
  prefs: []
  type: TYPE_NORMAL
- en: Uniform distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start off with a really simple example: uniform distribution. A uniform
    distribution just means there''s a flat constant probability of a value occurring
    within a given range.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: So we can create a uniform distribution by using the NumPy `random.uniform`
    function. The preceding code says, I want a uniformly distributed random set of
    values that ranges between `-10` and `10`, and I want `100000` of them. If I then
    create a histogram of those values, you can see it looks like the following.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b90c1711-c37a-4144-9ece-b930a56e0e71.png)'
  prefs: []
  type: TYPE_IMG
- en: There's pretty much an equal chance of any given value or range of values occurring
    within that data. So, unlike the normal distribution, where we saw a concentration
    of values near the mean, a uniform distribution has equal probability across any
    given value within the range that you define.
  prefs: []
  type: TYPE_NORMAL
- en: So what would the probability distribution function of this look like? Well,
    I'd expect to see basically nothing outside of the range of **-10** or beyond
    **10**. But when I'm between **-10** and **10**, I would see a flat line because
    there's a constant probability of any one of those ranges of values occurring.
    So in a uniform distribution you would see a flat line on the probability distribution
    function because there is basically a constant probability. Every value, every
    range of values has an equal chance of appearing as any other value.
  prefs: []
  type: TYPE_NORMAL
- en: Normal or Gaussian distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we've seen normal, also known as Gaussian, distribution functions already
    in this book. You can actually visualize those in Python. There is a function
    called `pdf` (probability density function) in the `scipy.stats.norm` package
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, we''re creating a list of x values for plotting that
    range between -3 and 3 with an increment of 0.001 in between them by using the
    `arange` function. So those are the x values on the graph and we''re going to
    plot the *x*-axis with using those values. The *y*-axis is going to be the normal
    function, `norm.pdf`, that the probability density function for a normal distribution,
    on those x values. We end up with the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7cd81e2-a685-4e43-bc34-1d79beccfcea.png)'
  prefs: []
  type: TYPE_IMG
- en: The pdf function with a normal distribution looks just like it did in our previous
    section, that is, a normal distribution for the given numbers that we provided,
    where 0 represents the mean, and the numbers **-3**, **-2**, **-1**, **1**, **2**,
    and **3** are standard deviations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will generate random numbers with a normal distribution. We''ve done
    this a few times already; consider this a refresher. Refer to the following block
    of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In the above code, we use the `random.normal` function of the NumPy package,
    and the first parameter `mu`, represents the mean that you want to center the
    data around. `sigma` is the standard deviation of that data, which is basically
    the spread of it. Then, we specify the number of data points that we want using
    a normal probability distribution function, which is `10000` here. So that''s
    a way to use a probability distribution function, in this case the normal distribution
    function, to generate a set of random data. We can then plot that, using a histogram
    broken into `50` buckets and show it. The following output is what we end up with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00edc207-6532-4388-ae55-fad682c9caf0.png)'
  prefs: []
  type: TYPE_IMG
- en: It does look more or less like a normal distribution, but since there is a random
    element, it's not going to be a perfect curve. We're talking about probabilities;
    there are some odds of things not quite being what they should be.
  prefs: []
  type: TYPE_NORMAL
- en: The exponential probability distribution or Power law
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another distribution function you see pretty often is the exponential probability
    distribution function, where things fall off in an exponential manner.
  prefs: []
  type: TYPE_NORMAL
- en: When you talk about an exponential fall off, you expect to see a curve, where
    it's very likely for something to happen, near zero, but then, as you get farther
    away from it, it drops off very quickly. There's a lot of things in nature that
    behave in this manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that in Python, just like we had a function in `scipy.stats` for `norm.pdf`,
    we also have an `expon.pdf`, or an exponential probability distribution function
    to do that in Python, we can do the same syntax that we did for the normal distribution
    with an exponential distribution here as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'So again, in the above code, we just create our x values using the NumPy `arange`
    function to create a bunch of values between `0` and `10` with a step size of
    `0.001`. Then, we plot those x values against the y-axis, which is defined as
    the function `expon.pdf(x)`. The output looks like an exponential fall off. As
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b99bbd72-f38d-4d68-9ef0-d5cf60bd8ae1.png)'
  prefs: []
  type: TYPE_IMG
- en: Binomial probability mass function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also visualize probability mass functions. This is called the binomial
    probability mass function. Again, we are going to use the same syntax as before,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'So instead of `expon` or `norm`, we just use `binom`. A reminder: The probability
    mass function deals with discrete data. We have been all along, really, it''s
    just how you think about it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming back to our code, we''re creating some discrete `x` values between `0`
    and `10` at a spacing of `0.01`, and we''re saying I want to plot a binomial probability
    mass function using that data. With the `binom.pmf` function, I can actually specify
    the shape of that data using two shape parameters, `n` and `p`. In this case,
    they''re `10` and `0.5` respectively. output is shown on the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9ba7a94-b664-4088-b2da-bbecf2418455.png)'
  prefs: []
  type: TYPE_IMG
- en: If you want to go and play around with different values to see what effects
    it has, that's a good way to get an intuitive sense of how those shape parameters
    work on the probability mass function.
  prefs: []
  type: TYPE_NORMAL
- en: Poisson probability mass function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lastly, the other distribution function you might hear about is a Poisson probability
    mass function, and this has a very specific application. It looks a lot like a
    normal distribution, but it's a little bit different.
  prefs: []
  type: TYPE_NORMAL
- en: The idea here is, if you have some information about the average number of things
    that happen in a given time period, this probability mass function can give you
    a way to predict the odds of getting another value instead, on a given future
    day.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s say I have a website, and on average I get 500 visitors
    per day. I can use the Poisson probability mass function to estimate the probability
    of seeing some other value on a specific day. For example, with my average of
    500 visitors per day, what''s the odds of seeing 550 visitors on a given day?
    That''s what a Poisson probability mass function can give you take a look at the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code example, I''m saying my average is 500 mu. I''m going to set up
    some x values to look at between `400` and `600` with a spacing of `0.5`. I''m
    going to plot that using the `poisson.pmf` function. I can use that graph to look
    up the odds of getting any specific value that''s not `500`, assuming a normal
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e402208-e7bc-46f3-aa27-c4a2ef1c7f1a.png)'
  prefs: []
  type: TYPE_IMG
- en: The odds of seeing **550** visitors on a given day, it turns out, comes out
    to about **0.002** or 0.2% probability. Very interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Alright, so those are some common data distributions you might run into in the
    real world.
  prefs: []
  type: TYPE_NORMAL
- en: Remember we used a probability distribution function with continuous data, but
    when we're dealing with discrete data, we use a probability mass function.
  prefs: []
  type: TYPE_NORMAL
- en: So that's probability density functions, and probability mass functions. Basically,
    a way to visualize and measure the actual chance of a given range of values occurring
    in a dataset. Very important information and a very important thing to understand.
    We're going to keep using that concept over and over again. Alright, let's move
    on.
  prefs: []
  type: TYPE_NORMAL
- en: Percentiles and moments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we''ll talk about percentiles and moments. You hear about percentiles
    in the news all the time. People that are in the top 1% of income: that''s an
    example of percentile. We''ll explain that and have some examples. Then, we''ll
    talk about moments, a very fancy mathematical concept, but it turns out it''s
    very simple to understand conceptually. Let''s dive in and talk about percentiles
    and moments, a couple of a pretty basic concepts in statistics, but again, we''re
    working our way up to the hard stuff, so bear with me as we go through some of
    this review.'
  prefs: []
  type: TYPE_NORMAL
- en: Percentiles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see what percentiles mean. Basically, if you were to sort all of the data
    in a dataset, a given percentile is the point at which that percent of the data
    is less than the point you're at.
  prefs: []
  type: TYPE_NORMAL
- en: A common example you see talked about a lot, is income distribution. When we
    talk about the 99th percentile, or the one-percenters, imagine that you were to
    take all the incomes of everybody in the country, in this case the United States,
    and sort them by income. The 99th percentile will be the income amount at which
    99% of the rest of the country was making less than that amount. It's a very easy
    way to comprehend it.
  prefs: []
  type: TYPE_NORMAL
- en: In a dataset, a percentile is the point at which *x%* of the values are less
    than the value at that point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph is an example for income distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9aeb8fcc-4e0d-45fa-b2de-f6f7a831a257.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding image shows an example of income distribution data. For example,
    at the 99th percentile we can say that 99% of the data points, which represent
    people in America, make less than $506,553 a year, and one percent make more than
    that. Conversely, if you're a one-percenter, you're making more than $506,553
    a year. Congratulations! But if you're a more typical median person, the 50th
    percentile defines the point at which half of the people are making less and half
    are making more than you are, which is the definition of median. The 50th percentile
    is the same thing as median, and that would be at $42,327 given this dataset.
    So, if you're making $42,327 a year in the US, you are making exactly the median
    amount of income for the country.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the problem of income distribution in the graph above. Things tend
    to be very concentrated toward the high end of the graph, which is a very big
    political problem right now in the country. We'll see what happens with that,
    but that's beyond the scope of this book. That's percentiles in a nutshell.
  prefs: []
  type: TYPE_NORMAL
- en: Quartiles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Percentiles are also used in the context of talking about the quartiles in a
    distribution. Let's look at a normal distribution to understand this better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example illustrating Percentile in normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/638aba41-52be-4e4b-a75a-003a40569792.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the normal distribution in the preceding image, we can talk about
    quartiles. Quartile 1 (Q1) and quartile 3 (Q3) in the middle are just the points
    that contain together 50% of the data, so 25% are on left side of the median and
    25% are on the right side of the median.
  prefs: []
  type: TYPE_NORMAL
- en: The median in this example happens to be near the mean. For example, the **interquartile
    range** (**IQR**), when we talk about a distribution, is the area in the middle
    of the distribution that contains 50% of the values.
  prefs: []
  type: TYPE_NORMAL
- en: The topmost part of the image is an example of what we call a box-and-whisker
    diagram. Don't concern yourself yet about the stuff out on the edges of the box.
    That gets a little bit confusing, and we'll cover that later. Even though they
    are called quartile 1 (Q1) and quartile 3 (Q1), they don't really represent 25%
    of the data, but don't get hung up on that yet. Focus on the point that the quartiles
    in the middle represent 25% of the data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Computing percentiles in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look at some more examples of percentiles using Python and kind of get
    our hands on it and conceptualize this a little bit more. Go ahead and open the
    `Percentiles.ipynb` file if you'd like to follow along, and again I encourage
    you to do so because I want you to play around with this a little bit later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start off by generating some randomly distributed normal data, or normally
    distributed random data, rather, refer to the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this example, what we're going to do is generate some data centered around
    zero, that is with a mean of zero, with a standard deviation of `0.5`, and I'm
    going to make `10000` data points with that distribution. Then, we're going to
    plot a histogram and see what we come up with.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33390974-0859-4cc0-8a6d-6d78be980298.png)'
  prefs: []
  type: TYPE_IMG
- en: The generated histogram looks very much like a normal distribution, but because
    there is a random component we have a little outlier near the deviation of -2
    in this example here. Things are tipped a little bit at the mean, a little bit
    of random variation there to make things interesting.
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy provides a very handy percentile function that will compute the percentile
    values of this distribution for you. So, we created our `vals` list of data using
    `np.random.normal`, and I can just call the `np.percentile` function to figure
    out the 50th percentile value in using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The output turns out to be 0.005\. So remember, the 50th percentile is just
    another name for the median, and it turns out the median is very close to zero
    in this data. You can see in the graph that we're tipped a little bit to the right,
    so that's not too surprising.
  prefs: []
  type: TYPE_NORMAL
- en: 'I want to compute the 90th percentile, which gives me the point at which 90%
    of the data is less than that value. We can easily do that with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of that code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The 90th percentile of this data turns out to be 0.64, so it's around here,
    and basically, at that point less than 90% of the data is less than that value.
    I can believe that. 10% of the data is greater than 0.64, 90% of it, less than
    0.65.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compute the 20th percentile value, that would give me the point at which
    20% of the values are less than that number that I come up with. Again, we just
    need a very simple alteration to the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The 20th percentile point works out to be -0.4, roughly, and again I believe
    that. It's saying that 20% of the data lies to the left of -0.4, and conversely,
    80% is greater.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to get a feel as to where those breaking points are in a dataset,
    the percentile function is an easy way to compute them. If this were a dataset
    representing income distribution, we could just call `np.percentile(vals, 99)`
    and figure out what the 99th percentile is. You could figure out who those one-percenters
    people keep talking about really are, and if you're one of them.
  prefs: []
  type: TYPE_NORMAL
- en: Alright, now to get your hands dirty. I want you to play around with this data.
    This is an IPython Notebook for a reason, so you can mess with it and mess with
    the code, try different standard deviation values, see what effect it has on the
    shape of the data and where those percentiles end up lying, for example. Try using
    smaller dataset sizes and add a little bit more random variation in the thing.
    Just get comfortable with it, play around with it, and find you can actually do
    this stuff and write some real code that works.
  prefs: []
  type: TYPE_NORMAL
- en: Moments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, let's talk about moments. Moments are a fancy mathematical phrase, but
    you don't actually need a math degree to understand it, though. Intuitively, it's
    a lot simpler than it sounds.
  prefs: []
  type: TYPE_NORMAL
- en: It's one of those examples where people in statistics and data science like
    to use big fancy terms to make themselves sound really smart, but the concepts
    are actually very easy to grasp, and that's the theme you're going to hear again
    and again in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, moments are ways to measure the shape of a data distribution, of
    a probability density function, or of anything, really. Mathematically, we''ve
    got some really fancy notation to define them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f5319b6-97bd-41c2-b66e-aa3d787011af.png)'
  prefs: []
  type: TYPE_IMG
- en: If you do know calculus, it's actually not that complicated of a concept. We're
    taking the difference between each value from some value raised to the nth power,
    where n is the moment number and integrating across the entire function from negative
    infinity to infinity. But intuitively, it's a lot easier than calculus.
  prefs: []
  type: TYPE_NORMAL
- en: Moments can be defined as quantitative measures of the shape of a probability
    density function.
  prefs: []
  type: TYPE_NORMAL
- en: Ready? Here we go!
  prefs: []
  type: TYPE_NORMAL
- en: The first moment works out to just be the mean of the data that you're looking
    at. That's it. The first moment is the mean, the average. It's that simple.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second moment is the variance. That's it. The second moment of the dataset
    is the same thing as the variance value. It might seem a little bit creepy that
    these things kind of fall out of the math naturally, but think about it. The variance
    is really based on the square of the differences from the mean, so coming up with
    a mathematical way of saying that variance is related to mean isn't really that
    much of a stretch, right. It's just that simple.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now when we get to the third and fourth moments, things get a little bit trickier,
    but they're still concepts that are easy to grasp. The third moment is called
    skew, and it is basically a measure of how lopsided a distribution is.![](img/94f6ca96-30ca-4930-b926-a7a488f8e6b3.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can see in these two examples above that, if I have a longer tail on the
    left, now then that is a negative skew, and if I have a longer tail on the right
    then, that's a positive skew. The dotted lines show what the shape of a normal
    distribution would look like without skew. The dotted line out on the left side
    then I end up with a negative skew, or on the other side, a positive skew in that
    example. OK, so that's all skew is. It's basically stretching out the tail on
    one side or the other, and it is a measure of how lopsided, or how skewed a distribution
    is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The fourth moment is called kurtosis. Wow, that''s a fancy word! All that really
    is, is how thick is the tail and how sharp is the peak. So again, it''s a measure
    of the shape of the data distribution. Here''s an example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/912d4b01-0a9f-4c55-9f5e-ae25596d2a18.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the higher peak values have a higher kurtosis value. The topmost
    curve has a higher kurtosis than the bottommost curve. It's a very subtle difference,
    but a difference nonetheless. It basically measures how peaked your data is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s review all that: the first moment is mean, the second moment is variance,
    the third moment is skew, and the fourth moment is kurtosis. We already know what
    mean and variance are. Skew is how lopsided the data is, how stretched out one
    of the tails might be. Kurtosis is how peaked, how squished together the data
    distribution is.'
  prefs: []
  type: TYPE_NORMAL
- en: Computing moments in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's play around in Python and actually compute these moments and see how you
    do that. To play around with this, go ahead and open up the `Moments.ipynb`, and
    you can follow along with me here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s again create that same normal distribution of random data. Again, we''re
    going to make it centered around zero, with a 0.5 standard deviation and 10,000
    data points, and plot that out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: So again, we get a randomly generated set of data with a normal distribution
    around zero.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca45799c-e3d1-4fc7-b144-646ab4c6cde0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we find the mean and variance. We''ve done this before; NumPy just gives
    you a `mean` and `var` function to compute that. So, we just call `np.mean` to
    find the first moment, which is just a fancy word for the mean, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The output turns out to be very close to zero, just like we would expect for
    normally distributed data centered around zero. So, the world makes sense so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we find the second moment, which is just another name for variance. We
    can do that with the following code, as we''ve seen before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Providing the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: That output turns out to be about 0.25, and again, that works out with a nice
    sanity check. Remember that standard deviation is the square root of variance.
    If you take the square root of 0.25, it comes out to 0.5, which is the standard
    deviation we specified while creating this data, so again, that checks out too.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third moment is skew, and to do that we''re going to need to use the SciPy
    package instead of NumPy. But that again is built into any scientific computing
    package like Enthought Canopy or Anaconda. Once we have SciPy, the function call
    is as simple as our earlier two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This displays the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We can just call `sp.skew` on the `vals` list, and that will give us the skew
    value. Since this is centered around zero, it should be almost a zero skew. It
    turns out that with random variation it does skew a little bit left, and actually
    that does jive with the shape that we're seeing in the graph. It looks like we
    did kind of pull it a little bit negative.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth moment is kurtosis, which describes the shape of the tail. Again,
    for a normal distribution that should be about `zero.SciPy` provides us with another
    simple function call
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'And here''s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Indeed, it does turn out to be zero. Kurtosis reveals our data distribution
    in two linked ways: the shape of the tail, or the how sharp the peak If I just
    squish the tail down it kind of pushes up that peak to be pointier, and likewise,
    if I were to push down that distribution, you can imagine that''s kind of spreading
    things out a little bit, making the tails a little bit fatter, and the peak of
    it a little bit lower. So that''s what kurtosis means, and in this example, kurtosis
    is near zero because it is just a plain old normal distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to play around with it, go ahead and, again, try to modify the distribution.
    Make it centered around something besides 0, and see if that actually changes
    anything. Should it? Well, it really shouldn't because these are all measures
    of the shape of the distribution, and it doesn't really say a whole lot about
    where that distribution is exactly. It's a measure of the shape. That's what the
    moments are all about. Go ahead and play around with that, try different center
    values, try different standard deviation values, and see what effect it has on
    these values, and it doesn't change at all. Of course, you'd expect things like
    the mean to change because you're changing the mean value, but variance, skew,
    maybe not. Play around, find out.
  prefs: []
  type: TYPE_NORMAL
- en: There you have percentiles and moments. Percentiles are a pretty simple concept.
    Moments sound hard, but it's actually pretty easy to understand how to do it,
    and it's easy in Python too. Now you have that under your belt. It's time to move
    on.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw the types of data (numeric, categorical, and ordinal
    data) that you might encounter and how to categorize them and how you treat them
    differently depending on what kind of data you're dealing with. We also walked
    through the statistical concepts of mean, median and mode, and we also saw the
    importance of choosing between median and mean, and that often the median is a
    better choice than the mean because of outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we analyzed how to compute mean, median, and mode using Python in an IPython
    Notebook file. We learned the concepts of standard deviation and variance in depth
    and how to compute them in Python. We saw that they’re a measure of the spread
    of a data distribution. We also saw a way to visualize and measure the actual
    chance of a given range of values occurring in a dataset using probability density
    functions and probability mass functions.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at the types of data distributions (Uniform distribution, Normal or
    Gaussian distribution, Exponential probability distribution, Binomial probability
    mass function, Poisson probability mass function) in general and how to visualize
    them using Python. We analyzed the concepts of percentiles and moments and saw
    how to compute them using Python.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll look at using the `matplotlib` library more extensively,
    and also dive into the more advanced topics of covariance and correlation.
  prefs: []
  type: TYPE_NORMAL
