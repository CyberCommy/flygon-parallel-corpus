- en: Architecting a Kubernetes Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand the basics of what composes a Kubernetes cluster, we
    still need to understand how to place all the Kubernetes components together,
    and how to suit their requirements to provision a production-ready Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will examine how to determine these requirements and how
    they will help us maintain steady workloads and achieve a successful deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will explore the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Kube-sizing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining storage considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining network requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing kube objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kube-sizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When designing a Kubernetes cluster, we don't just need to worry about how we
    are going to configure our deployment objects to host our applications, or how
    we are going to configure our service objects to provide communication across
    our pods—where all this is hosted is also important. Therefore, we also need to
    take into account the resources that are required to bring balance to our application
    workloads and our control plane.
  prefs: []
  type: TYPE_NORMAL
- en: etcd considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will require at least a three-node `etcd` cluster in order for it to be able
    to support itself in case one node fails. Because `etcd` uses a distributed census
    algorithm called **Raft**, odd-numbered clusters are recommended. This is because,
    in order for an action to be allowed, more than 50% of the members of the cluster
    have to agree on it. In a scenario with a two-node cluster, for example, if one
    of the nodes fails, the other node's vote is only 50% of the cluster, and therefore,
    the cluster loses quorum. Now, when we have a three-node cluster, a single node
    failure represents only a 33.33% vote loss and the two remaining nodes' votes
    still 66.66% for the action to be allowed.
  prefs: []
  type: TYPE_NORMAL
- en: The following link is for a great website where you can learn exactly how the
    Raft algorithm works: [http://thesecretlivesofdata.com/raft/](http://thesecretlivesofdata.com/raft/).
  prefs: []
  type: TYPE_NORMAL
- en: For `etcd`, we can choose from two deployment models for our cluster. We can
    either run it on the same node as our kube-apiserver, or we can have a separate
    set of clusters running our key-value store. Either way, this will not change
    how `etcd` reaches quorum, so you will still have to install `etcd` in odd numbers
    across your control-plane manager nodes.
  prefs: []
  type: TYPE_NORMAL
- en: For a Kubernetes use case, `etcd` won't consume lots of compute resources such
    as CPU or memory. Although `etcd` does aggressively cache key-value data and uses
    most of its memory-tracking watchers, two cores and 8 GB of RAM will be more than
    enough.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to the disk, this is where you need to be more critical. The `etcd` cluster
    relies heavily on disk latency, because of the way the consensus protocol persistently
    stores metadata in the log. Every member of the `etcd` cluster has to store every
    request, and any major spike in latency can trigger a cluster leader election,
    which will cause instability for the cluster. A **hard disk drive** (**HDD**) for
    `etcd` is out of the question unless you are running 15k RPM disks in a Raid 0
    disk to squeeze the highest performance possible out of a magnetic drive. A **solid
    state drive** (**SSD**) is the way to go and, with extremely low latency and higher
    **input/output operations per second** (**IOPS**), they are the perfect candidate
    to host your key-value store. Thankfully, all major cloud providers offer SSD
    solutions to satisfy this need.
  prefs: []
  type: TYPE_NORMAL
- en: kube-apiserver sizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The remaining resources that are required for the control-plane components will
    depend on the number of nodes that they will be managing and which add-ons you
    will be running on them. One additional thing to take into account is the fact
    that you can put these master nodes behind a load balancer to ease the load and
    provide high availability. In addition to this, you can always horizontally scale
    your master nodes in periods of contention.
  prefs: []
  type: TYPE_NORMAL
- en: Taking all this into consideration, and taking into account that `etcd` will
    be hosted alongside our master nodes, we can say that a three-master node cluster
    with **virtual machines** (**VMs**) with 2 to 4 vCPUs, and between 8 and 16 GB
    of RAM will be more than enough to handle greater than or equal to 100 worker
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Worker nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The worker nodes, on the other hand, are the ones that'll be doing the heavy
    lifting—these guys are the ones that will be running our application workloads.
    Standardizing the size of these nodes will be impossible as they fall into a *What
    if?* scenario. We are required to know exactly what type of applications we will
    be running on our nodes, and what their resource requirements are, for us to be
    able to size them correctly. Nodes will not only be sized on the application resource
    requirements, but we will also have to consider periods where we will have more
    than our planned pods running on them. For instance, you can perform a rolling
    update on a deployment to use a newer image depending on how you have configured
    your `maxSurge`; this node will have to handle 10% to 25% more load.
  prefs: []
  type: TYPE_NORMAL
- en: Containers are really lightweight, but when orchestrators come into play, you
    can have 30, 40, or even 100 containers running on a single node! This exponentially
    increases your resource consumption per host. While pods come with resource-limiting
    functionalities and specifications to limit the container's resource consumption,
    you still need to account for the required resources of those containers.
  prefs: []
  type: TYPE_NORMAL
- en: Nodes can always be scaled horizontally during periods of contention and high-resource
    demand. However, it's always good to have those extra resources available to avoid
    any undesirable **out of memory** (**OOMs**)killers. So, plan for the future and
    for the *What if?* scenario by having a pool of extra resources.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancer considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our nodes still need to talk to our API server and, as we mentioned before,
    having several master nodes requires a load balancer. When it comes to load balancing
    requests from our nodes to the masters, we have several options to pick from,
    depending on where you are running your cluster. If you are running Kubernetes
    in a public cloud, you can go ahead with your cloud provider's load balancer option,
    as they are usually elastic. This means that they autoscale as needed and offer
    more features than you actually require. Essentially, load balancing requests
    to the API server will be the only task that your load balancer will perform.
    This leads us to the on-premises scenario—as we are sticking to open source solutions
    here, then you can configure a Linux box running either HAProxy or NGINX to satisfy
    your load balancing needs. There is no wrong answer in choosing between HAProxy
    and NGINX, as they provide you with exactly what you need.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, the basic architecture will look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae11e5e7-863f-4eb1-aaab-b5be6cdd8b81.png)'
  prefs: []
  type: TYPE_IMG
- en: Storage considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Storage needs are not as straightforward as they are for a regular host or hypervisor.
    There are several types of storage that our nodes and pods will be consuming,
    and we need to tier them properly. Because you are running Linux, tiering the
    storage into different filesystems and storage backends will be extremely easy—nothing
    that **logical volume manager** (**LVM**) or different mount points can't solve.
  prefs: []
  type: TYPE_NORMAL
- en: The basic Kubernetes binaries, such as `kubelet` and `kube-proxy`, can run on
    basic storage alongside the OS files; nothing very high-end is required, as any
    SSD will be enough to satisfy their needs.
  prefs: []
  type: TYPE_NORMAL
- en: Now, on the other hand, we have the storage in which our container images will
    be stored and run from. Going back to the [Chapter 6](6da53f60-978c-43a4-9dc9-f16b14405709.xhtml),
    *Creating a Highly Available Self-Healing Architecture*, we learned that containers
    are composed of read-only layers. This means that when the disks are running tens
    or even hundreds of containers in a single node, they will be hit very hard on
    read requests. The storage backend for this will have to serve read requests with
    very low latency. Specific numbers in terms of IOPS and latency will vary across
    each environment, but the basis will be the same. This is because of the nature
    of containers—disks that provide a higher read performance over writes will be
    preferable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Storage performance is not the only factor to take into account. Storage space
    is also very important. Calculating how much space you require will depend on
    the following two things:'
  prefs: []
  type: TYPE_NORMAL
- en: How big are the images that you are going to be running?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How many different images will you be running and what are their sizes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will directly consume the space in `/var/lib/docker` or `/var/lib/containerd`.
    With this in mind, a separate mount point for `/var/lib/docker` or `containerd/`
    with enough space to store all the images that you are going to be running on
    the pods will be a good option. Take into account that these images are ephemeral
    and will not live on your node forever. Kubernetes does have garbage collection
    strategies embedded in kubelet, which will delete old images that are no longer
    in use if you reach a specified threshold for disk usage. These options are `HighThresholdPercent`
    and `LowThresholdPercent.` You can set them with a kubelet flag: `--eviction-hard=imagefs.available`
    or `--eviction-soft=imagefs.available`. These flags are already configured by
    default to garbage collect when free storage reaches less than 15%, however, you
    can adjust them to your needs. `eviction-hard` is the threshold that it needs
    to reach in order to start deleting images, while `eviction-soft` is the percentage
    or amount that it needs to reach to stop deleting images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some containers will still require some sort of read/write volume for persistent
    data. As discussed in [Chapter 7](d89f650b-f4ea-4cda-9111-a6e6fa6c2256.xhtml), *Understanding
    the Core Components of a Kubernetes Cluster*, there are several storage provisioners,
    and all of them will suit different scenarios. All you need to know is that you
    have a series of options available to you, thanks to the Kubernetes storage classes.
    Some of the open source software-defined storage solutions that are worth mentioning
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Ceph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GlusterFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenStack Cinder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network File System** (**NFS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each storage provisioner will have its benefits and downsides, but it is beyond
    the scope of this book to go through each one in detail. We have offered a good
    overview of Gluster in previous chapters, as it is what we are going to use in
    later chapters for our example deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Network requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to understand the network requirements of our cluster, we first need
    to understand the Kubernetes networking model and what problems it aims to solve.
    Container networking can be very hard to grasp; however, it has three essential
    problems:'
  prefs: []
  type: TYPE_NORMAL
- en: How do containers talk to each other (on the same host and on different hosts)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do containers talk to the outside world, and how does the outside world
    talk to the containers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Who allocates and configures each container's unique IP address?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Containers on the same host can talk to each other through a virtual bridge
    that you can see with the `brctl` utility from the `bridge-utils` package. This
    is handled by the Docker engine and it's called the Docker networking model. Containers
    are attached to the virtual bridge named `docker0` through a `veth` virtual interface that
    is allocated an IP from a private subnet address. In this way, all containers
    can talk to each other through their `veth` virtual interface. The problem with
    the Docker model arises when containers are allocated on different hosts, or when
    external services want to communicate with them. To solve this, Docker provides
    a method where containers are exposed to the outside world through the host's
    ports. Requests come into a certain port in the host's IP address and are then
    proxied to the container behind that port.
  prefs: []
  type: TYPE_NORMAL
- en: This method is useful but not ideal. You can't configure services to specific
    ports or in a dynamic port allocation scenario—our services will require flags
    to connect to the correct ports each time we deploy them. This can get really
    messy very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid this, Kubernetes have implemented their own networking model that
    has to comply with the following rules:'
  prefs: []
  type: TYPE_NORMAL
- en: All pods can communicate with all other pods without **network address translation**
    (**NAT**)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All nodes can communicate with all pods without NAT
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The IP that the pod sees itself as is the same IP that others see it as
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are several open source projects out there that can help us to reach
    this goal, and the one that suits you best will depend on your circumstances. Here
    are some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: Project Calico
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weave Net
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flannel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kube-router
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning IPs to pods and making them talk between them is not the only issue
    to be aware of. Kubernetes also provides DNS-based service discovery, because
    applications that talk through DNS records rather than IPs are far more efficient
    and scalable.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes DNS-based service discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes has a deployment in its kube-system namespace and we will revisit
    namespaces later in this chapter. The deployment is composed of a pod with a set
    of containers that forms a DNS server that is in charge of creating all DNS records
    in the cluster and serving DNS requests for service discovery.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes will also create a service pointing to the mentioned deployment,
    and will tell the kubelet to configure each pod''s container to use the service''s
    IP as the DNS resolver by default. This is the default behavior, but you can overwrite
    this by setting a DNS policy on your pod''s specification. You can choose from
    the following specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Default**: This one is counter-intuitive as it is not the default one in
    reality. With this policy, pods will inherit the name resolution from the node
    that runs that pod. For example, if a node is configured to use `8.8.8.8` as its
    DNS server, the `resolv.conf` pods will also be configured to use that same DNS
    server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ClusterFirst**: This is actually the default policy and, as we mentioned
    before, any pod running with ClusterFirst will have `resolv.conf` configured with
    the IP of the `kube-dns` service. Any requests that are not local to the cluster
    will be forwarded to the node''s configured DNS server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Not all Kubernetes objects have DNS records. Only services and, in some specific
    cases, pods have records created for them. There are two types of records in the
    DNS server: **A records** and **service records** (**SRVs**). A records are created
    depending on the type of service created; and we are not referring to `spec.type` here.
    There are two types of services: **normal services**, which we revised in [Chapter
    7](d89f650b-f4ea-4cda-9111-a6e6fa6c2256.xhtml), *Understanding the Core Components
    of a Kubernetes Cluster*, and correspond to the ones under the `type` specification;
    and **headless services**. Before explaining headless services, let''s explore
    how normal services behave.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each normal service, an A record that points to the service''s cluster
    IP address is created; these records are in the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Any pod that is running on the same namespace as the service can resolve the
    service through only its `shortname: <service-name>` field. This is because any
    other pod outside of the namespace has to specify the namespace after the shortname
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For headless services, records work a little bit different. First of all, a
    headless service is a service with no cluster IP assigned to it. Therefore, an
    A record that points to the service's IP is impossible to create. To create a
    headless service, you define the `.spec.clusterIP` namespace with `none` in this
    way, so that no IP is assigned to it. Kubernetes will then create A records based
    on the endpoints of this service. Essentially, the pods are selected through the
    `selector` field, although this is not the only requirement. Because of the format
    in which the A record is created, pods require several new fields in order for
    the DNS server to create records for them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pods will require two new specification fields: `hostname` and `subdomain`.
    The `hostname` field will be the `hostname` field of the pod, while `subdomain`
    will be the name of the headless service that you are creating for these pods.
    The A records for this will point to each pod''s IP in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, another record will be created with only the headless service,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This record will return all the IP addresses of the pods behind the service.
  prefs: []
  type: TYPE_NORMAL
- en: We now have what's necessary to start building our cluster. However, there are
    still some design features that do not only include the Kubernetes binaries and
    their configuration, Kubernetes API objects can also be tuned. We will go through
    some of the adjustments that you can perform in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing kube objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to Kubernetes objects, everything will depend on the type of workload
    or application that you are trying to build the infrastructure for. Therefore,
    rather than designing or architecting any particular customization, we will go
    through how to configure the most commonly used and helpful specifications on
    each object.
  prefs: []
  type: TYPE_NORMAL
- en: Namespacing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes offers namespaces as a way of segmenting your cluster into multiple
    **virtual clusters**. Think of it as a way of segmenting your cluster's resources
    and objects and putting them in logical isolation from each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Namespaces will only be used in very specific scenarios, but Kubernetes comes
    with some predefined namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Default**: This is the default namespace that all objects without a namespace
    definition will be placed into.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-system**: Any object that is created by and for the Kubernetes cluster
    will be placed on this namespace. Objects that are required for the basic functionality
    of the cluster will be placed here. For example, you will find `kube-dns`, `kubernetes-dashboard`, `kube-proxy`,
    or any additional component or agent for external applications, such as `fluentd`,
    `logstash`, `traefik`, and ingress controllers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-public**: A namespace that is reserved for objects that can be visible
    to anyone, including non-authenticated users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Creating a namespace is very simple and straightforward; you can do so by running
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it—you now have your own namespace. To place objects in this namespace,
    you will be using the `metadata` field and adding the `namespace` key-value pair;
    for example, consider this excerpt from a YAML pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You will find yourself creating custom namespaces for clusters that are usually
    very large and have a considerable number of users or different teams that are
    consuming their resources. For these types of scenarios, namespaces are perfect.
    Namespaces will let you segregate all the objects of a team from the rest. Names
    can even be repeated on the same class objects as long as they are on different
    namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: Namespaces will not only provide isolation for objects, but you can also set
    resource quotas for each namespace. Let's say that you have a couple of development
    teams working on your cluster—one team is developing a very lightweight application,
    and the other one is developing a very resource-intensive app. In this scenario,
    you don't want the first development team consuming any additional compute resources
    from the resource-intensive app team—this is where resource quotas come into play.
  prefs: []
  type: TYPE_NORMAL
- en: Limiting namespace resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Resource quotas are also Kubernetes API objects; however, they are designed
    to work specifically on namespaces by creating limits on compute resources and
    even limiting the number of objects on each assigned space.
  prefs: []
  type: TYPE_NORMAL
- en: The `ResourceQuota` API object is declared like any other object in Kubernetes,
    through a `YAML` file passed to the `kubectl` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'A basic resource quota definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two types of basic quotas that we can set: compute resource quotas
    and object resource quotas. As seen in the previous example, `pods` is an object
    quota and the rest are compute quotas.'
  prefs: []
  type: TYPE_NORMAL
- en: In each of these fields, you will specify the total sum of the provided resource,
    which the namespace cannot exceed. For example, in this namespace, the total number
    of running `pods` cannot exceed `4`, and the sum of their resources can't exceed
    `1` CPU and `2Gi` of RAM memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The maximum number of objects per namespace can be assigned to any kube API
    object that can be put in a namespace; here is a list of the objects that can
    be limited with namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Persistent Volume Claims** (**PVCs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secrets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ConfigMaps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replication controllers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReplicaSets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StatefulSets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cron jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When it comes to compute resources, it is not only memory and CPU that can be
    limited, but you can also assign quotas to storage space—these quotas will apply
    only to PVCs, however.
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand compute quotas better, we need to dive deeper and explore
    how these resources are managed and assigned on a pod basis. This will also be
    a good time to understand how to architect pods better.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pods without resource limitations on non-limited namespaces can consume all
    of a node's resources without warning; however, you have a set of tools in the
    pod's specification to handle their compute allocation better.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you allocate resources to a pod, you are not actually allocating them
    to the pod. Instead, you are doing it on a container basis. Therefore, a pod with
    multiple containers will have multiple resource constraints for each of its containers;
    let''s consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this pod declaration, under the `containers` definition, we have two new
    fields that we haven''t covered: `env` and `resources`. The `resources` field
    contains the compute resource limitations and requirements for our `containers`.
    By setting `limits`, you are telling the container the maximum number of resources
    that it can ask of that resource type. If a container exceeds the limit, it will
    be restarted or terminated.'
  prefs: []
  type: TYPE_NORMAL
- en: The `request` field refers to how much of that resource Kubernetes will guarantee
    to that container. In order for the container to be able to run, the host node
    must have enough free resources to satisfy the request.
  prefs: []
  type: TYPE_NORMAL
- en: 'CPU and memory are measured in different ways. For instance, when we assign
    or limit CPU, we talk in CPU units. There are several ways of setting the CPU units;
    first, you can either specify round or fractional numbers such as 1, 2, 3, 0.1,
    and 1.5, which will correspond to the number of virtual cores that you want to
    assign to that container. Another way of assigning is to use the **milicore**
    expression. One milicore (1m), which is the minimum CPU quantity that you can
    assign, is equivalent to 0.001 CPU cores; for example, you could do the following
    assignment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'That would be the same as writing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preferred way of assigning CPU is through Millicores, as the API will convert
    whole numbers into Millicores either way.
  prefs: []
  type: TYPE_NORMAL
- en: For memory allocation, you can use normal memory units such as kilobytes or
    kibibytes; the same goes for any other memory unit, such as E, P, T, G, and M.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to resource quotas, we can see how individual container resource
    management will play together with resource quotas on namespaces. This is because
    the resource quotas will tell us how many limits and requests we can set per namespace
    in our containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second field that we haven''t revised is the `env` field. With `env`, we
    configure the environmental variables for our containers. With variable declarations,
    we can pass settings, parameters, passwords, and more configurations to our containers.
    The simplest way to declare a variable in a pod is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now the container will have access to the `VAR` variable content in its shell,
    referred to as `$VAR`. As we mentioned previously, this is the easiest way to
    declare a variable and provide a value to it. However, this is not the most efficient
    one—when you declare a value in this way, the value will only live in the pod
    declaration.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we need to edit the value or pass this value to multiple pods, it becomes
    a hassle as you need to type the same value on every pod that requires it. This
    is where we will introduce two more Kubernetes API objects: `Secrets` and `ConfigMaps`.'
  prefs: []
  type: TYPE_NORMAL
- en: With `ConfigMaps` and `Secrets`, we can store values for our variables in a
    persistent and more modular form. In essence, `ConfigMaps` and `Secrets` are the
    same, but secrets contain their values encoded in `base64`. Secrets are used to
    store sensitive information such as passwords or private keys—essentially, any
    type of confidential data. All the rest of the data that you don't need to be
    hidden can be passed through `ConfigMap`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way that you create these two types of objects is the same way as with
    any other object in Kubernetes—through `YAML`. You can create a `ConfigMap` object
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The only difference on this definition, form all the other definitions in this
    chapter is that we are missing the specification field. Instead, we have data
    where we will be placing the key-value pairs that contain the data that we want
    to store.
  prefs: []
  type: TYPE_NORMAL
- en: 'With `Secrets`, this works a little bit differently. This is because the value
    for the key that we need to store has to be encoded. In order to store a value
    in a secret''s key, we pass the value to `base64`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When we have the `base64` hash of our string, we are ready to create our secret.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block shows a `YAML` file configured with the secret''s
    value in `base64`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To use our `ConfigMaps` and `Secrets` objects in pods, we use the `valueFrom`
    field in the `env` array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here, the name under `secretKeyRef` corresponds to the `Secret` API object name,
    and the `key` is the `key` from the `data` field in `Secret`.
  prefs: []
  type: TYPE_NORMAL
- en: With `ConfigMaps`, it will look almost the same; however, in the `valueFrom`
    field, we will use `configMapKeyRef` instead of `secretKeyRef`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ConfigMap` declaration is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now that you understand the basics of customizing pods, you can take a look
    at a real-life example at [https://kubernetes.io/docs/tutorials/configuration/configure-redis-using-configmap/](https://kubernetes.io/docs/tutorials/configuration/configure-redis-using-configmap/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to determine the compute and network requirements
    of a Kubernetes cluster. We also touched upon the software requirements that come
    along with it, such as `etcd`, and how odd-numbered clusters are preferred (due
    to the census algorithm) as the cluster needs to achieve more than 50% of votes
    for consensus.
  prefs: []
  type: TYPE_NORMAL
- en: The `etcd` cluster can either run on the kube-apiserver or have a separate set
    of clusters dedicated just for `etcd`. When it comes to resources, 2 CPUs and
    8 GB of RAM should be enough. When deciding on the storage system for `etcd`,
    opt for lower latency and higher IOPS storage such as SSD. We then jumped into
    sizing the kube-apiserver, which can be run alongside `etcd`. Given that both
    components can coexist, resources should be bumped to anything between 8 and 16
    GB of RAM and between 2 and 4 CPUs per node.
  prefs: []
  type: TYPE_NORMAL
- en: In order to properly size the worker nodes, we have to keep in mind that this
    is where the actual application workloads will be running. These nodes should
    be sized for application requirements, and additional resources should be considered
    for periods where more than the planned number of pods could be running, such
    as during rolling updates. Continuing with the requirements for the cluster, we
    touched on how a load balancer can help with the master node's communication by
    balancing requests among the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Storage needs for Kubernetes can be quite overwhelming as many factors can
    affect the overall setup, and leaning toward a storage system that benefits reads
    over writes is preferable. Additionally, some of the most common storage providers
    for Kubernetes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Ceph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GlusterFS (covered in [Chapter 2](7a05974e-0554-45d0-a505-921d484942ef.xhtml), *Defining
    GlusterFS Storage* to [Chapter 5](b140a44b-3594-4c0d-ad7c-03de29a31815.xhtml),
    *Analyzing Performance in a Gluster System*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenStack Cinder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then moved on to the networking side of things and learned how Kubernetes
    provides services such as DNS-based service discovery, which is in charge of creating
    all DNS records in the cluster and serving DNS requests for service discovery. Objects
    in Kubernetes can be customized to accommodate the different needs of each workload,
    and things such as namespaces are used as a way of segmenting your cluster into
    multiple virtual clusters. Resource limits can be done through resource quotas.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, pods can be customized to allow an absolute maximum of resources to
    be allocated and to avoid a single pod from consuming all of the worker node's
    resources. We discussed the various storage considerations and requirements in
    detail, including how to customize kube objects and pods.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll jump into deploying a Kubernetes cluster and learn
    how to configure it.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why are odd-numbered `etcd` clusters preferred?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can `etcd` run alongside kube-apiserver?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is lower latency recommended for `etcd`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the worker nodes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What should be considered when sizing worker nodes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the storage providers for Kubernetes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is a load balancer needed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can namespaces be used?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Mastering Kubernetes* by Gigi Sayfan: [https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes](https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kubernetes for Developers* by Joseph Heck: [https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers](https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hands-On Microservices with Kubernetes* by Gigi Sayfan: [https://www.packtpub.com/virtualization-and-cloud/hands-microservices-kubernetes](https://www.packtpub.com/virtualization-and-cloud/hands-microservices-kubernetes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Getting Started with Kubernetes – Third Edition* by Jonathan Baier, Jesse
    White: [https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition](https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mastering Docker – Second Edition* by Russ McKendrick, Scott Gallagher: [https://www.packtpub.com/virtualization-and-cloud/mastering-docker-second-edition](https://www.packtpub.com/virtualization-and-cloud/mastering-docker-second-edition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Docker Bootcamp* by Russ McKendrick et al.: [https://www.packtpub.com/virtualization-and-cloud/docker-bootcamp](https://www.packtpub.com/virtualization-and-cloud/docker-bootcamp)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
