- en: From Requirements To Test Cases
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从需求到测试用例
- en: '*Program testing can be used to show the presence of bugs, but never to show
    their absence!* *- Edsger Dijkstra*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*程序测试可以用来显示错误的存在，但永远不能用来显示错误的不存在！- Edsger Dijkstra*'
- en: 'This chapter provides a base of knowledge aimed to help software engineers
    to write meaningful test cases. The starting point for this process is the understanding
    of the requirements of the system being tested. Without that information, it is
    not feasible to design nor implement valuable tests. After that, several actions
    might be executed before the actual coding of the tests, namely, test planning
    and test design. Once we start the test coding process, we need to have in mind
    a set of principles to write code right, and also a set of anti-patterns and bad
    smells to be avoided. All this information is provided in this chapter in form
    of the following sections:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了一些知识基础，旨在帮助软件工程师编写有意义的测试用例。这个过程的起点是理解正在测试的系统的要求。没有这些信息，设计和实施有价值的测试是不可行的。在实际编写测试之前，可能会执行几个动作，即测试计划和测试设计。一旦我们开始测试编码过程，我们需要牢记一套编写正确代码的原则，以及一套要避免的反模式和坏味道。所有这些信息都以以下部分的形式在本章中提供：
- en: '**The importance of requirements**: This section provides a general overview
    of the software development process, started by the statement of some needs to
    be covered by a software system, and followed by several stages, typically including
    analysis, design, implementation, and tests.'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**要求的重要性**：本节概述了软件开发过程，从提出需要由软件系统满足的一些需求开始，然后经过几个阶段，通常包括分析、设计、实施和测试。'
- en: '**Test planning**: A document called *test plan* can be generated at the beginning
    of a software project. This section reviews the structure of a test plan according
    to the IEEE 829 Standard for Test Documentation. As we will discover, the complete
    statement of a test plan is a very fine-grained process, especially recommended
    for large projects in which the communication among the team is a key aspect for
    the success of the project.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试计划**：在软件项目开始时可以生成一个名为*测试计划*的文档。本节根据IEEE 829测试文档标准审查了测试计划的结构。正如我们将发现的那样，测试计划的完整陈述是一个非常细粒度的过程，特别适用于团队之间的沟通对项目成功至关重要的大型项目。'
- en: '**Test design:** Before starting the coding of the tests, it is always a good
    practice to think about the blueprint of these tests. In this section, we review
    the major aspects to be taken into consideration to design properly our tests.
    We put the accent on the test data (expected outcome), which feed the test assertions.
    In this regard, we review some black-box data generation techniques (equivalence
    partitioning and boundary analysis) and white-box (test coverage).'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试设计**：在开始编写测试代码之前，考虑这些测试的蓝图总是一个好的做法。在本节中，我们回顾了设计我们的测试时需要考虑的主要方面。我们强调测试数据（预期结果），这些数据为测试断言提供支持。在这方面，我们回顾了一些黑盒数据生成技术（等价分区和边界分析）和白盒（测试覆盖）。'
- en: '**Software testing principles**: This section provides a set of best-practices
    which can help us write our tests.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软件测试原则**：本节提供了一组可以帮助我们编写测试的最佳实践。'
- en: '**Test anti-patterns**: Finally, the opposite side is also reviewed: what are
    the patterns and code smells to be avoided when writing our test cases.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试反模式**：最后，还审查了相反的一面：在编写我们的测试用例时要避免的模式和代码坏味道。'
- en: The importance of requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要求的重要性
- en: 'Software systems are built to satisfy some kind of need to a group of consumers
    (final users or customer). Understanding those needs is one of most challenging
    problems in software engineering due to the fact that it is quite common that
    consumer needs are nebulous (especially in the early stages of the project). Moreover,
    it is also common that these needs can be deeply changed throughout the project
    lifetime. Fred Brooks, a well-known software engineer, and computer scientist,
    defines this problem in his seminal book *The Mythical Man-Month (*1975*)*:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 软件系统是为满足一组消费者（最终用户或客户）的某种需求而构建的。理解这些需求是软件工程中最具挑战性的问题之一，因为消费者的需求通常是模糊的（特别是在项目的早期阶段）。此外，这些需求在项目的整个生命周期中也经常发生深刻的变化。弗雷德·布鲁克斯（Fred
    Brooks），一位著名的软件工程师和计算机科学家，在他的开创性著作《神话般的程序员月度（1975）》中定义了这个问题：
- en: '*The hardest single part of building a software system is deciding precisely
    what to build. No other part of the conceptual work is as difficult as establishing
    the detailed technical requirements … No other part of the work so cripples the
    resulting system if done wrong. No other part is as difficult to rectify later.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*构建软件系统中最困难的部分是准确决定要构建什么。概念工作中没有其他部分像确立详细的技术要求那样困难……如果做错了，没有其他部分会像这样严重地瘫痪最终的系统。后来纠正这一点也是最困难的。*'
- en: In any case, consumer's needs are the touchstone for any software project. From
    these needs, a list of features can emerge. We define a feature as a high-level
    description of a software system functionality. From each feature, one or more
    requirements (functional and non-functional) should be derived. A requirement
    is everything that be true about the software, in order to meet the consumer's
    expectations. Scenarios (real-life examples rather than abstract descriptions)
    can be useful for adding details to the requirements description. The group of
    requirements and/or list of features of the software system are often known as
    specification.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，消费者的需求都是任何软件项目的试金石。从这些需求中，可以得出一系列功能。我们将功能定义为软件系统功能的高级描述。从每个功能中，应该派生出一个或多个要求（功能性和非功能性）。要求是关于软件的一切，以满足消费者的期望。场景（真实生活的例子而不是抽象描述）对于为要求描述添加细节是有用的。软件系统的要求组和/或功能列表通常被称为规范。
- en: In software engineering, the stage of defining the requirements is called **requirements
    elicitation**. In this stage, software engineers need to clarify *what* problem
    they are trying to solve. At the end of this phase, it is a common practice to
    start the modeling of the system. To that aim, a modeling language (typically
    UML) is employed to create a group of diagrams. The UML diagrams, which typically
    fits in the elicitation stage is the use case diagram (model of the functionality
    of the system and its relationship with the involved actors).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Modeling is not always carried out in all software projects. For example, agile
    methodologies are more based on the principle of sketching rather than in a formal
    modeling strategy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: After elicitation, requirements should be refined in the **analysis** stage.
    In this phase, the stated requirements are analysed in order to resolve incomplete,
    ambiguous of contradictory issues. As a result, in this stage it is likely to
    continue modeling, using for example high-level class diagrams not linked to any
    specific technology yet. Once the analysis is clear (that is, the *what* of the
    system), we need to find out *how* to implement it. This stage is known as **design**.
    In the design phase, the guidelines of the project should be established. To that
    aim, an architecture of the software system is typically derived from the requirements.
    Again, the modeling techniques are broadly employed to carry out different aspects
    of the design. There is a bunch of UML diagrams that can be used at this point,
    including structural diagrams (component, deployment, object, package, and profile
    diagram) and behavioral diagrams (activity, communication, sequence, or state
    diagram). From the design, the actual **implementation** (that is, coding) can
    start.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The amount of modeling carried out in the design stage varies significantly
    depending on different factors, including the type and size of the company producing
    the software (multinationals, SMEs, governmental, and so on), the development
    process (waterfall, spiral, prototyping, agile, and so on), the type of project
    (enterprise, open source, and so on), the type of software (custom made software,
    commercial off-the-shelf, and so on), and even the background of the people involved
    (experience, career, and so on). All in all, the designs need to be understood
    as a way of communication between the different roles of software engineers participating
    in the project. Typically, the bigger the project, the more necessary a fine-grained
    design based on different modeling diagrams is.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Concerning **tests**, in order to make a proper test plan (see next section
    for further details), again we need to use the requirements elicitation data,
    that is, the list of requirements and/or features. In other words, in order to
    verify our system, we need to to know beforehand what we expect from it. Using
    the classic definition proposed by Barry Boehm (see [chapter 1](part0021.html#K0RQ0-ef8404ed083f459d860f84cc8198f8bb), *Retrospective
    On Software Quality And Java Testing*), verification is used to answer the question
    *Are we building the product right?* To that, we need to know the requirements,
    or at least, the desired features. In addition to verification, it would be desirable
    to carry out some validation (according to Boehm: *Are we building the right product?*).
    This is necessary since sometimes there is a gap between what has been specified
    (the features and requirements) and the real needs of the consumer. Therefore,
    validation is a high-level assessment method, and to carry out it, the final consumer
    can be involved (validating the software system once it is deployed). All these
    ideas are depicted in the following picture:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00136.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Software engineering generic development process
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: There is no universal workflow for the terms presented so far (communication,
    requirement elicitation, analysis, design, implementation/test, and deployment).
    In the preceding diagram, it follows a linear process flows, nevertheless, in
    practice, it can follow an iterative, evolutionary, or parallel workflow.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the potential problems involved in the different phases in software
    engineer (analysis, design, implementation, and so on), it is worth to review
    the classical cartoon *How project really works?* The original source of this
    picture is unknown (there are versions dating back to the 1960s). In 2007, a site
    called *Project Cartoon* emerged ([http://www.projectcartoon.com/](http://www.projectcartoon.com/)),
    allowing to customize the original cartoon with new scenes. The following chart
    is the version 1.5 of the cartoon provided on that site:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00137.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: How projects really work, version 1.5 (illustrated created by [www.projectcartoon.com](http://www.projectcartoon.com))
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: If we think about this picture, we discover that the root of the problems comes
    from the requirements, badly explained by the customer at the beginning, and worst
    understood by the project leader. From that point, the whole software engineering
    process turns into the *Chinese whispers* children game. To solve all these problems
    is out of the scope of this book, but as a good start, we need to take special
    care in the requirements, which guide the whole process, including, of course,
    the tests.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Test planning
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A first step in the testing path can be the generation of a document called
    *test plan*, which is the blueprint to conduct software testing. This document
    describes the objective, scope, approach, focus, and distribution of the testing
    efforts. The process of preparing such document is a useful way to think about
    the needs to verify of a software system. Again, this document is especially useful
    when the size of the SUT and the involved team is large, due to the fact that
    the separation of work in different roles makes the communication a potential
    deterrent for the success of the project.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'A way to create a test plan is to follow the IEEE 829 Standard for Test Documentation.
    Although this standard might be too much formal for the most of software projects,
    it might be worth to review the guidelines proposed in this standard, and use
    the parts needed (if any) in our software projects. The steps proposed in IEEE
    829 are the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '**Analyze the product**: This part reinforces the idea of extracting the understanding
    the requirements of the system from the consumer needs. As already explained,
    it is not possible to test a software if no information about it is available.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Design the test strategy**: This part of the plan contains several parts,
    including:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define scope of testing, that is, the system components to be tested (in scope)
    and those parts which do not (out of scope). As explained later, exhaustive testing
    is not feasible, and we need to choose carefully what is going to be tested. This
    is not a simple choice, and it can be determined by different factors, such as
    precise customer requests, project budget and timing, and skills of the involved
    software engineers.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify testing type, that is, which levels of tests should be conducted (unit,
    integration, system, acceptance) and which test strategy (black box, white box,
    non-functional).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document risks, that is, potential problems which might cause different issues
    in the project.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define the test objectives**: In this part of the plan, the list of features
    to be tested are listed together with the target of testing each one.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Define the test criteria**: These criteria are typically made up by two parts,
    namely:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suspension criteria, for instance the percentage of failed tests in which the
    development of new features is suspended until the team solves all the failures.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exit criteria, for example, the percentage of critical tests that should be
    passed to proceed to next phase of development.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 退出标准，例如应通过的关键测试的百分比，以便继续进行下一阶段的开发。
- en: '**Resource planning**: This part of the plan is devoted to summarize the resources
    required to carry out the testing activities. It could be personnel, equipment,
    or infrastructure.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**资源规划**：计划的这一部分致力于总结进行测试活动所需的资源。这可能是人员、设备或基础设施。'
- en: '**Plan test environment**: It consists of the software and hardware setup on
    which test are going to be executed.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计划测试环境**：它由将要执行测试的软件和硬件设置组成。'
- en: '**Schedule and estimation**: In this phase, managers are supposed to break
    out the whole project into small tasks estimating the efforts (person-month).'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**日程安排和估算**：在这个阶段，经理们应该将整个项目分解为小任务，估算工作量（人月）。'
- en: '**Determine test deliverables**: Determine all the documents that has to be
    maintained to support the testing activities.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**确定测试交付物**：确定必须维护以支持测试活动的所有文档。'
- en: As can be seen, test planning is a complex task, typically carried out in large
    projects by managers. In the rest of this chapter we continue discovering how
    to write test cases, but hereinafter from a point of view closest to the actual
    test coding.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看出，测试计划是一个复杂的任务，通常由经理在大型项目中执行。在本章的其余部分，我们将继续探讨如何编写测试用例，但从最接近实际测试编码的角度来看。
- en: Test design
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试设计
- en: 'In order to design properly a test, we need to define specifically what needs
    to be implemented. To that aim, it is important to remember what is the generic
    structure of a test, already explained in [chapter 1](part0021.html#K0RQ0-ef8404ed083f459d860f84cc8198f8bb),
    *Retrospective On Software Quality And Java Testing*. Therefore, for each test
    we need to define:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确设计测试，我们需要具体定义需要实现的内容。为此，重要的是要记住测试的通用结构，已在[第1章](part0021.html#K0RQ0-ef8404ed083f459d860f84cc8198f8bb)中解释过，*关于软件质量和Java测试的回顾*。因此，对于每个测试，我们需要定义：
- en: What is test fixture, that is, the required state in the SUT to carry out the
    test? This is done at the beginning of the test in the stage called setup. At
    the end of the test, the test fixture might be released in the stage called teardown.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试装置是什么，也就是SUT中进行测试所需的状态？这是在测试的开始阶段称为设置。在测试结束时，测试装置可能在拆卸阶段被释放。
- en: What is the SUT, and if we are doing unit tests, which are its DOC(s)? Unit
    test should be in isolation and therefore we need to define test doubles (typically
    mocks or spies) for the DOC(s).
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SUT是什么，如果我们正在进行单元测试，它的DOC(s)是什么？单元测试应该是独立的，因此我们需要为DOC(s)定义测试替身（通常是模拟对象或间谍）。
- en: 'What are the assertions? This a key part of tests. Without assertions, we cannot
    claim that a test is actually made. In order to design assertion, it is worth
    to recall which is its generic structure. In short, an assertion consists in the
    comparison of some expected value (test data) and the actual outcome obtained
    from the SUT. If any of the assertions is negative, the test will be declared
    as failed (test verdict):'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 断言是什么？这是测试的关键部分。没有断言，我们无法声称测试实际上已经完成。为了设计断言，值得回顾一下它的通用结构。简而言之，断言包括比较一些预期值（测试数据）和从SUT获得的实际结果。如果任何一个断言是负面的，测试将被宣布为失败（测试判决）：
- en: '![](img/00138.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00138.jpeg)'
- en: Test cases and assertions general schema
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 测试用例和断言的一般模式
- en: 'Test data plays a crucial role in the testing process. The source of test data
    is often called test oracles, and typically can be extracted from the requirements.
    Nevertheless, there are some others commonly used sources for tests oracles, for
    example:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据在测试过程中起着至关重要的作用。测试数据的来源通常被称为测试预言，通常可以从需求中提取。然而，还有一些其他常用的测试预言来源，例如：
- en: A different program, which produces the expected output (inverse relationship).
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产生预期输出的不同程序（反向关系）。
- en: A heuristic or statistical oracle that provides approximate results.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供近似结果的启发式或统计预言。
- en: Values based on the experience of human experts.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于人类专家经验的价值观。
- en: Moreover, test data can be derived, depending on the underlying testing technique.
    When using black-box testing, that is, exercise some specific requirement based
    using some input and expecting some output, different techniques can be employed,
    such as equivalence partitioning or boundary analysis. On the other side, if we
    are using white-box testing, the structure is the basis for our test and therefore
    the test coverage will be key to select the test input which maximizes these coverage
    rates. In the following sections, these techniques are reviewed.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，测试数据可以根据底层测试技术进行推导。当使用黑盒测试时，也就是说，使用一些输入来执行特定的基于需求的测试，并期望得到一些输出时，可以采用不同的技术，例如等价分区或边界分析。另一方面，如果我们使用白盒测试，结构将是我们测试的基础，因此测试覆盖率将是选择最大化这些覆盖率的测试输入的关键。在接下来的章节中，将对这些技术进行审查。
- en: Equivalence partitioning
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 等价分区
- en: 'Equivalence partitioning (also known as equivalence class partitioning) is
    a black-box technique (that is, it relies in the requirements of the system) aimed
    to reduce the number of tests that should be executed against a SUT. This technique was
    first defined by Glenford Myers in 1978 as:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 等价分区（也称为等价类分区）是一种黑盒技术（即，它依赖于系统的需求），旨在减少应该针对SUT执行的测试数量。这项技术最早由Glenford Myers于1978年定义为：
- en: “*A technique that partitions the input domain of a program into a finite number
    of classes [sets], it then identifies a minimal set of well-selected test cases
    to represent these classes.*”
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: “*将程序的输入域划分为有限数量的类[集合]的技术，然后确定一组精心选择的最小测试用例来代表这些类。*”
- en: In other words, equivalence partitioning provides a criteria to answer the question *How
    many tests do we need**?* The idea is to divide all possible input test data (which
    often is a enormous number of combinations) in a set of values for which we assume
    to be processed in the same way by the SUT. We call equivalence classes to these
    sets of values. The idea is that testing one representative value within the equivalence
    class is consider sufficient because it is assumed that all the values are processed
    in the same way by the SUT.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, the equivalence classes for a given SUT can be grouped in two types:
    valid and invalid inputs. The equivalence partitioning testing theory ensures
    that only one test case of each partition is needed to evaluate the behavior of
    the program for the related partition (both the valid and the invalid classes).
    The following process describes how to systematically carry out the equivalence
    partitioning for a given SUT:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: First, we determine the domain of all possible valid inputs for a SUT. To find
    out these values, we rely on the specification (features or functional requirements).
    Our SUT is supposed to process these values (valid equivalence class) correctly.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If our specification establishes that some elements of the equivalence class
    are processed differently, they should assigne to another equivalence class.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The values outside this domain can be seen as another equivalence class, this
    time for invalid inputs.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For every single equivalence class, a representative value is chosen. This decision
    is an heuristic process typically based on the tester experience.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For every test input, the proper test output is also selected, and with these
    values we will be able to complete our test case (test exercise and assertions).
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Boundary analysis
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As any programmer knows, faults often appear at the boundary of a equivalence
    class (for example, the initial value of an array, the maximum value for a given
    range, and so on). Boundary value analysis is a method, which complements equivalence
    partitioning by looking at the boundaries of the test input. It was defined by
    the National Institute of Standards and Technology (NIST) in 1981 as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: “*A selection technique in which test data are chosen to lie along ‘boundaries’
    of the input domain [or output range] classes, data structures, and procedure
    parameters.*”
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'All in all, to apply boundary value analysis in our tests, we need to evaluate
    our SUT exactly in the borders of our equivalence class. Therefore, typically
    two tests cases are derived using this approach: the upper and the lower boundary
    of the equivalence class.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Test coverage
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Test coverage is the rate of code in SUT that is exercised for any of their
    tests. Test coverage is very useful to finding untested parts of our SUT. Therefore,
    it can be the perfect white box technique (structural) to complement the black
    box (functional). As a general rule, a test coverage rate of 80% or above is considered
    reasonable.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different Java libraries, which allows to make test coverage in a
    simple manner, for instance:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Cobertura ([http://cobertura.github.io/cobertura/](http://cobertura.github.io/cobertura/)):
    It is an open source reporting tool, which can be executed using Ant, Maven, or
    directly using the command line.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'EclEmma ([http://www.eclemma.org/](http://www.eclemma.org/)): It is an open
    source code coverage tool for Eclipse. As of Eclipse 4.7 (Oxygen), EclEmma is
    integrated out of the box in the IDE. The following screenshot shows an example
    on how EclEmma highlights the code coverage on a Java class in Eclipse:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00139.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Test coverage with EclEmma in Eclipse 4.7 (Oxygen)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'JaCoCo ([http://www.jacoco.org/jacoco/](http://www.jacoco.org/jacoco/)): It
    is an open source code coverage library created by the EclEmma team based on other
    old coverage library called EMMA ([http://emma.sourceforge.net/](http://emma.sourceforge.net/)).
    JaCoCo is available as a Maven dependency.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Codecov ([https://codecov.io/](https://codecov.io/)): It is a cloud solution
    offering a friendly code coverage web dashboard. It is free for open source projects.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software testing principles
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exhaustive testing is the name given to a test approach, which uses all possible
    combinations of test inputs to verify a software system. This approach is only
    applicable to tiny software systems or components with a close finite number of
    possible of operations and allowed data. In the majority of software systems,
    it is not feasible to verify every possible permutation and input combination,
    and therefore exhaustive testing is just a theoretical approach.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'For that reason, it is said that the absence of defects in a software system
    cannot be proved. This was stated by the computer science pioneer Edsger W. Dijkstra
    (see quote at beginning of this chapter). Thus, testing is, at best, sampling,
    and it must be carried out in any software project to reduce the risk of system
    failures (see [chapter 1](part0021.html#K0RQ0-ef8404ed083f459d860f84cc8198f8bb), *Retrospective
    On Software Quality And Java Testing,* to recall the software defect taxonomy).
    Since we cannot test everything, we need to test properly. In this section, we
    review a set of best practices to write effective and efficient test cases, namely:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '**Tests should be simple**: The software engineer writing the test (call him
    or her tester, programmer, developer, or whatever) should avoid attempting to
    test his or her program. In regards to testing, the right answer to the question
    *Who watches the watchmen?* Should be nobody. Our test logic should be simple
    enough to avoid any kind of meta-testing, since this would lead to a recursive
    problem out of any logic. Indirectly, if we keep tests simple, we also obtain
    another desirable feature: tests will be easy to maintain.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Do not implement simple tests**: One thing is make simple tests, and another
    very different stuff is to implement dummy code, such as getter or setters. As
    introduced before, test is at best sampling, and we cannot waste precious time
    in assessing such kind of part of our codebase.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Easy to read**: The first step is to provide a meaningful name for our test
    method. In addition, thanks to the JUnit 5 `@DisplayName` annotation, we can provide
    a rich textual description, which defines without Java naming constraints the
    goal of the test.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single responsibility principle**: This is a general principle of computer
    programming that states that every class should have responsibility of a single
    functionality. It is closely related to the metric of cohesion. This principle
    is very important to be accomplished when coding tests: a single test should be
    only referred to a given system requirement.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test data is key**: As described in the section before, the expected outcome
    from the SUT is a central part of the tests. The correct management of these data
    is critical to create effective tests. Fortunately, JUnit 5 provides a rich toolbox
    to handle test data (see section *Parameterized tests* in [chapter 4](part0100.html#2VBO80-ef8404ed083f459d860f84cc8198f8bb),
    *Simplifying Testing With Advanced JUnit Features*).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unit test should be executed very fast**: A commonly accepted rule of thumb
    for the duration of unit test is that a unit test should last a second at the
    most. To accomplish that goal, it is also required that unit test isolates properly
    the SUT, doubling properly its DOCs.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test must be repeatable**: Defects should be reproduced as many times as
    required for developers to find the cause of the bug. This is the theory, but
    unfortunately this is not always applicable. For example, in multi-threaded SUT
    ( a real-time or server-side software systems), race conditions are likely to
    occur. In those situations, non-deterministic defects (often called *heisenbugs*)
    might be experienced.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**We should test positive and the negative scenarios**: This mean that we need
    to write tests with for input condition that assess the expected outcome, but
    we also need to verify what the program is not supposed to do. In addition to
    meet its requirements, programs must be tested to avoid unwanted side effects.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing cannot be done only for the sake of coverage**: Just because all
    parts of the code have been touched by some tests, we cannot assure that those
    parts have been thoroughly tested. For that to be true, tests have to analyzed
    in terms of reduction of risks.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The psychology of testing
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From a psychological point of view, the objective of testing should be executing
    a software system with the intent of finding defects. Understanding the motivation
    of that claim can make the difference in the success of our tests.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Human beings tend to be goal oriented. If we carry out tests to demonstrate
    that a program has no errors, we will tend to implement tests selecting test data
    with a low probability of causing program failures. On the other hand, if the
    objective is to demonstrate that a program has errors, we will increase the probability
    of finding them, adding more value to the program than the former approach. For
    that reason, testing is often considered as a destructive process, since testers
    are supposed to prove that the SUT has errors.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, trying to demonstrate that errors are present in the software is a
    goal feasible, while trying to demonstrate their absence, as explained before,
    it is impossible. Again, psychology studies tell us that people perform poorly
    when they know that a task is infeasible.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Test anti-patterns
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In software design, a pattern is a reusable solution to solve recurring problems.
    There are a bunch of them, including for example singleton, factory, builder,
    facade, proxy, decorator, or adapter, to name a few. Anti-patterns are also patterns,
    but undesirable ones. Concerning to testing, it is worth to know some of these
    anti-patterns to avoid them in our tests:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '**Second class citizens**: Test code containing a lot of duplicated code, making
    it hard to maintain.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The free ride** (also known as *Piggyback*): Instead of writing a new method
    to verify another feature/requirement, a new assertion is added to an existing
    test.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Happy path**: It only verifies expected results without testing for boundaries
    and exceptions.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The local hero**: A test dependent to some specific local environment. This
    anti-pattern can be summarized in the phrase *It works in my machine.*'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The hidden dependency**: A test that requires some existing data populated
    somewhere before the test runs.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chain gang**: Tests that must be run in a certain order, for example, changing
    the SUT to a state expected by the next one.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The mockery**: A unit test that contains too much test doubles that the SUT
    is not even tested at all, instead of returning data from test doubles.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The silent catcher**: A test that passes even if an unintended exception
    actually occurs.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The inspector**: A test that violates encapsulation that any refactor in
    the SUT requires reflecting those changes in the test.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Excessive setup**: A test that requires a huge setup in order to start the
    exercise stage.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anal probe**: A test which has to use unhealthy ways to perform its task,
    such as reading private fields using reflection.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The test with no name**: Test methods name with no clear indicator about
    what it is being tested (for example, identifier in a bug tracking tool).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The slowpoke**: A unit test which lasts over few seconds.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The flickering test**: A test which contains race conditions within the proper
    test, making it to fail from time to time.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wait and see**: A test that needs to wait a specific amount of time (for
    example, `Thread.sleep()`) before it can verify some expected behavior.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inappropriately shared fixture**: Tests that use a test fixture without even
    need the setup/teardown.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The giant**: A test class that contains a huge number of tests methods (God
    Object).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wet floor**: A test that creates persisted data but it is not clean up at
    when finished.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The cuckoo**: A unit test which establishes some kind of fixture before the
    actual test, but then the test discards somehow the fixture.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The secret catcher**: A test that is not making any assertion, relying on
    an exception to be thrown and reporting by the testing framework as a failure.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The environmental vandal**: A test which requires the use of given environment
    variables (for instance, a free port number to allows simultaneous executions).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Doppelganger**: Copying parts of the code under test into a new class to
    make visible for the test.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The mother hen**: A fixture which does more than the test needs.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The test it all**: Tests that should not break the Single Responsibility
    Principle.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line hitter**: A test without any kind of real verification of the SUT.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The conjoined twins**: Tests that are called *unit tests* but are really
    integration tests since there is no isolation between the SUT and the DOC(s).'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The liar**: A test that does not test what was supposed to test.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code smells
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Code smells (also known as *bad smell* when referred to software) are undesirable
    symptoms within the source code. Code smells are not problematic per se, but they
    can evidence some kind of issue nearby.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'As described in previous sections, tests should be simple and easy to read.
    With that promises, code smells should be present in our tests under no circumstances.
    All in all, generic code smells might be avoided in our tests. Some of the most
    common code smells are the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '**Duplicated code**: Cloned code is always a bad idea in software, since it
    breaks the principle **Don’t Repeat Yourself** (**DRY**). This problem is even
    worst in tests, since test logic must be crystal clear.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High complexity**: Too many branches or loops may be potentially simplified
    into smaller pieces.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long method**: A method that has grown too large is always problematic, and
    it is a very bad symptom when this method is a test.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unappropriated naming convention**: Variables, class, and method names should
    be concise. It is considered a bad smell to use very long identifiers, but also
    use excessive short (or meaningless) ones.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The starting point for the test design should be the list of requirements. If
    these requirements have not been formally elicited, at least we need to know the
    SUT features, which reflects the software needs. From this point, several strategies
    can be carried out. As usual, there is no unique path to reach our goal, which
    in the end should be reducing the risks of the project.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: This chapter reviewed a process aimed to create effective and efficient tests
    cases. This process involves the analysis of requirements, definition of a test
    plan, design of test cases, and finally writing the test cases. We should be aware
    that, even though software testing is technical task, it involves some important
    considerations of human psychology. These factors should be known by software
    engineers and testers in order to follow know best practices and also avoiding
    common mistakes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: In [chapter 7](part0160.html#4OIQ00-ef8404ed083f459d860f84cc8198f8bb), *Testing
    management*, we are going to understand how software testing activities are managed
    in a living software project. To that, first we review when and how to carry out
    testing in the common software development processes, such as waterfall, spiral,
    iterative, spiral, agile, or test-driven development. Then, the server-side infrastructure
    (such as Jenkins or Travis) aimed to automate the software development process
    in the context of JUnit 5 is reviewed. Finally, we learn how to keep track of
    the defects found with the Jupiter tests using the so-called issue tracking systems
    and test reporting libraries.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
