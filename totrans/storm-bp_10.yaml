- en: Chapter 10. Storm in the Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce you to deploying and running Storm in a hosted
    environment of a cloud provider.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html "Chapter 2. Configuring Storm Clusters"), *Configuring
    Storm Clusters*, you were introduced to the steps necessary to set up Storm in
    a clustered environment, and the subsequent chapters covered the installation
    and configuration of complementary technologies such as Kafka, Hadoop, and Cassandra.
    While most installations are relatively straightforward, the cost to maintain
    even a modestly sized cluster—in terms of the physical asset requirements as well
    as the time necessary to configure and maintain the environment—can easily become
    a burden, if not an outright blocker to the adoption of distributed computing
    technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, today there are a number of cloud hosting providers that offer
    services for on-demand dynamic provisioning of multimachine computing environments.
    Most cloud hosting providers offer a wide range of services and options to fit
    most users' needs, ranging from a single small footprint server to a large-scale
    infrastructure consisting of hundreds or even thousands of machines. In fact,
    a common trend among high-profile Internet content providers is to choose a cloud
    hosting provider over an in-house data center.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key benefits of using a cloud provider is the ability to deploy and
    undeploy computing resources as necessary, and on demand. An online retailer,
    for example, might provision additional servers and resources during the lead
    up to the holiday season in order to meet demand, scaling back later when the
    rush subsides. Also, as we'll see, cloud providers offer a cost-effective method
    for testing and prototyping distributed applications.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start by provisioning a Storm cluster with a cloud provider. Later in
    the chapter, we'll show you how to provision and manage local, virtualized Storm
    instances for testing Storm applications in a fully clustered environment on your
    workstation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning virtual machines using the **Amazon Web Services** (**AWS**) **Elastic
    Compute Cloud** (**EC2**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Apache Whirr to automate the provisioning and deployment of Storm clusters
    to EC2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Vagrant to launch and provision virtualized Storm clusters in a local
    environment for development and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Amazon Elastic Compute Cloud (EC2)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon EC2 is the central part of many remote compute services offered by Amazon.
    EC2 allows users to rent virtual compute resources hosted on Amazon's network
    infrastructure on demand.
  prefs: []
  type: TYPE_NORMAL
- en: We'll begin by setting up an EC2 account and manually launching a virtual machine
    on Amazon's EC2 infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an AWS account
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Establishing an AWS account is easy but requires an Amazon account. If you don't
    already have an Amazon account, sign up for one at [http://www.amazon.com/](http://www.amazon.com/).
  prefs: []
  type: TYPE_NORMAL
- en: With your Amazon account established, you can set up an AWS account at [http://aws.amazon.com/](http://aws.amazon.com/).
  prefs: []
  type: TYPE_NORMAL
- en: The AWS Management Console
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **AWS Management Console** acts as the main administrative interface to
    all the cloud services that Amazon offers. We''re primarily interested in the
    EC2 service, so let''s begin by logging in to the EC2 Management Console as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The AWS Management Console](img/8294OS_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Creating an SSH key pair
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before you can launch any EC2 instances, you will need a key pair. To create
    a new key pair, click on the **Key Pairs** link to open the key pair manager,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating an SSH key pair](img/8294OS_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You will be prompted to give the key pair a name. Enter a name and click on
    the **Yes** button. At this point, depending on which browser you are using, you
    will be prompted to download your private certificate file or the file will be
    downloaded automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s very important that you keep this file safe since the key will give you
    full administrator access to any EC2 images launched with that key. Immediately
    after downloading your private key, you should change its file permissions so
    it is not publicly readable; for example, with UNIX, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Many SSH clients will look at the permissions of the key file and issue a warning
    or refuse to use a key file that is publicly readable.
  prefs: []
  type: TYPE_NORMAL
- en: Launching an EC2 instance manually
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have created a key pair, you are ready to launch an EC2 instance.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in launching an EC2 machine is to select an **Amazon Machine
    Image** (**AMI**). An AMI is a virtual appliance template that can be run as a
    virtual machine on Amazon EC2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon provides a number of AMIs for popular operating system distributions
    such as Red Hat, Ubuntu, and SUSE. For our purposes, we will be using an Ubuntu
    Server instance as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Launching an EC2 instance manually](img/8294OS_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Once you've selected an AMI, you will be prompted to select an **instance type**.
    Instance types represent virtual hardware profiles with varying memory (RAM),
    CPU cores, storage, and I/O performance. Amazon charges by the hour for running
    instances, with prices ranging from a few cents per hour for its weakest instance
    type (**t1.micro**) to several dollars per hour for its most powerful instance
    type (**hs1.8xlarge**). The type you select will depend on your use case and budget.
    For example, a **t1.micro** instance (one CPU, 0.6 GB RAM, and low I/O performance)
    can be useful for testing purposes but is clearly not suited for heavy production
    loads.
  prefs: []
  type: TYPE_NORMAL
- en: 'After selecting an instance type, you are ready to launch the virtual machine
    by clicking on the **Review and Launch** button, reviewing the instance details,
    and then clicking on **Launch**. You will then be prompted to select a key pair
    for remote login and management of the instance. After a few minutes, your instance
    will be up and running as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Launching an EC2 instance manually](img/8294OS_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Logging in to the EC2 instance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you launch an instance, EC2 will preconfigure SSH with the key pair you
    selected during the setup, allowing you to remotely log in to the machine. To
    log in to the instance remotely, you will need the private key file you downloaded
    earlier as well as the public DNS name (or public IP address) assigned to the
    instance. You can find this information in the EC2 Management Console by clicking
    on the instance and viewing the details.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now connect to the instance with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, to connect as the "ubuntu" user using the `my-keypair.pem` private
    key file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The Ubuntu user has administrator permissions on the remote host, giving you
    the ability to configure the machine the way you like.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you could install Storm or any other services you like. However,
    manually configuring instances for anything larger than a trivially sized cluster
    will quickly become time-consuming and unmanageable. In the next section, we'll
    introduce a way to automate this process as part of a more scalable workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Apache Whirr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Apache Whirr project ([http://whirr.apache.org](http://whirr.apache.org))
    provides a Java API and set of shell scripts for installing and running various
    services on cloud providers such as Amazon EC2 and Rackspace. Whirr allows you
    to define the layout of a cluster in terms of the number of nodes as well as control
    which services run on each node. Whirr also comes with a set of scripts for performing
    management operations such as launching new clusters, starting and stopping clusters,
    and terminating clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Whirr began as a set of shell scripts for running Hadoop on Amazon EC2, and
    later matured to include a Java API based on the Apache jclouds ([http://jclouds.apache.org](http://jclouds.apache.org))
    project, which allowed it to support multiple cloud providers. Whirr has also
    expanded beyond Hadoop to support many additional distributed computing services
    such as Cassandra, Elastic Search, HBase, Pig, and others.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Whirr
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Begin by downloading a recent release and unpacking it on the computer you
    will use to launch and manage your clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For convenience, add Whirr''s `bin` directory to your system''s `PATH` environment
    variable so you can run the Whirr command from any directory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Whirr uses SSH to communicate with cloud instances, so we will create a dedicated
    key pair for using it with Whirr. Whirr requires that the key has an empty passphrase
    as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In order for Whirr to interact with your cloud provider account, it needs to
    know your credentials. For EC2, this consists of your EC2 Access Key ID and your
    EC2 Secret Access Key. If your AWS account is new, you will need to generate new
    credentials; otherwise, you should already have downloaded your credentials to
    a safe location. To generate a new set of EC2 credentials, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the **AWS Management Console**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the name in the top-right section of the navigation bar and select
    **Security Credentials**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Expand the section titled **Access Keys (Access Key ID and Secret Access Key)**
    and click on the **Create New Access Key** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Download Key File** to download your credentials to a safe location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The key file you downloaded will contain your Access Key ID and Secret Access
    Key in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Whirr gives you three options for specifying your cloud credentials: command-line
    parameters, cluster configuration file, or a local credentials file (`~/.whirr/credentials`).
    We''ll use the last option as it is the most convenient as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Configuring a Storm cluster with Whirr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have Whirr installed, let's turn our attention toward cluster configuration.
    Whirr's configuration files, or recipes, are just Java property files that contain
    Whirr properties which define the layout of nodes and services within a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by looking at the minimum configuration necessary to launch a
    3-node ZooKeeper cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `whirr.cluster-name` property simply assigns a unique identifier to the
    cluster and is used when running management commands such as listing the hosts
    in a cluster or destroying a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The `whirr.instance-template` property defines the number of nodes in a cluster
    and the services that run on each node. In the preceding example, we've defined
    a cluster of three nodes, with each node assigned with the ZooKeeper role.
  prefs: []
  type: TYPE_NORMAL
- en: 'With just these two properties defined, we have enough to tell Whirr how to
    launch and manage a ZooKeeper cluster. Whirr will use default values for everything
    else. However, there are a few options that you will typically want to override.
    For example, we''ll want Whirr to use the dedicated key pair we created earlier
    as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll configure Whirr with the hardware specification we want and the
    region in which our cluster should be hosted, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `whirr.image-id` property is provider specific and specifies which machine
    image to use. Here, we've specified an Ubuntu 10.04 64-bit AMI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we''re just testing Whirr, we''ve chosen the smallest (and least expensive)
    instance type: `t1.micro`. Finally, we''ve specified that we want our cluster
    deployed in the `us-east-1` region.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a complete list of public AMIs, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: From the EC2 Management Console, select a region from the drop-down menu in
    the upper-right corner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the left navigation pane, click on **AMIs**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the **Filter** drop-down menu at the top of the page, select **Public images**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Whirr is most thoroughly tested with Ubuntu Linux images. While other operating
    systems may work, if you run into problems, try again with an Ubuntu image.
  prefs: []
  type: TYPE_NORMAL
- en: Launching the cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our configuration file for a ZooKeeper cluster now looks like the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If we save those properties to a file named `zookeeper.properties`, we can
    then launch the cluster with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When the command completes, Whirr will output the list of instances created
    as well as the SSH command that can be used to connect to each instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can log in to instances using the following SSH commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To destroy a cluster, run `whirr destroy-cluster` with the same options used
    to launch it.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you are finished with the cluster, you can terminate all instances with
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Introducing Whirr Storm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Whirr Storm project ([https://github.com/ptgoetz/whirr-storm](https://github.com/ptgoetz/whirr-storm))
    is a Whirr service implementation for configuring Storm clusters. Whirr Storm
    supports the configuration of all Storm daemons as well as full control over Storm's
    `storm.yaml` configuration file.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Whirr Storm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To install the Whirr Storm service, simply place the JAR file in the `$WHIRR_HOME/lib`
    directory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, verify the installation by running the `Whirr` command without arguments
    to print a list of instance roles available to Whirr. The list should now include
    the roles provided by Whirr Storm as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Cluster configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our previous Whirr example, we created a cluster of three nodes where each
    node had only the ZooKeeper role. Whirr allows you to assign multiple roles to
    a node, which we''ll need to do for a Storm cluster. Before we get into the details
    of configuring Whirr for Storm, let''s take a look at the different roles Whirr
    Storm defines as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Role | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `storm-nimbus` | This is the role for running the Nimbus daemon. Only one
    node per cluster should be assigned with this role. |'
  prefs: []
  type: TYPE_TB
- en: '| `storm-supervisor` | This is the role for running the supervisor daemon.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `storm-ui` | This is the role for running the Storm UI web service. |'
  prefs: []
  type: TYPE_TB
- en: '| `storm-logviewer` | This is the role for running the Storm logviewer service.
    This role should only be assigned to nodes that also have the `storm-supervisor`
    role. |'
  prefs: []
  type: TYPE_TB
- en: '| `storm-drpc` | This is the role for running the Storm DRPC service. |'
  prefs: []
  type: TYPE_TB
- en: '| `zookeeper` | This role is provided by Whirr. Nodes with this role will be
    part of a ZooKeeper cluster. You must have at least one ZooKeeper node in a Storm
    cluster, and for multi-node ZooKeeper clusters, the number of nodes should be
    odd. |'
  prefs: []
  type: TYPE_TB
- en: 'To use these roles in a Whirr configuration, we specify them in the `whirr.instance-template`
    property in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, to create a single-node pseudocluster, where all Storm''s daemons
    are run on one machine, we would use the following value for `whirr.instance-template`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If we wanted to create a multinode cluster with one node running Nimbus and
    Storm UI, three nodes running the supervisor and logviewer daemons, and a 3-node
    ZooKeeper cluster, we would use the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Customizing Storm's configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whirr Storm will generate a `storm.yaml` configuration file with values for
    `nimbus.host`, `storm.zookeeper.servers`, and `drpc.servers` that are automatically
    calculated based on the hostnames of nodes in the cluster and which roles they
    have been assigned with. All other Storm configuration parameters will inherit
    default values unless specifically overridden. Note that if you attempt to override
    values for `nimbus.host`, `storm.zookeeper.servers`, or `drpc.servers`, Whirr
    Storm will ignore it and log a warning message.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although Whirr Storm will automatically calculate and configure the `nimbus.host`
    value for the cluster, you will still need to tell the Storm executable the host
    name of the Nimbus host when running the command locally. The easiest way to do
    this, and the most convenient if you have multiple clusters, is to specify a hostname
    for nimbus with the `–c` flag as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Other Storm configuration parameters can be specified in the Whirr configuration
    file by adding a property with a key prefixed with `whirr-storm`. For example,
    to set a value for the `topology.message.timeout.secs` parameter, we would add
    it to the Whirr configuration file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code would result in the following line in `storm.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Configuration parameters that accept a list of values can be expressed in the
    Whirr configuration file as a comma-separated list, such as the following configuration
    for `supervisor.slots.ports`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code would produce the following YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Customizing firewall rules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a new machine instance is launched on EC2, most of its network ports are
    blocked by a firewall by default. To enable network communication between instances,
    you must explicitly configure firewall rules to allow ingress and egress on specific
    ports between hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Whirr Storm will automatically create the security groups and firewall
    rules necessary for Storm components to communicate, such as opening the Nimbus
    Thrift port for topology submission and opening port 2181 between **Nimbus** and
    **Supervisor** nodes, and ZooKeeper nodes as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customizing firewall rules](img/8294OS_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: However, in many cases, Storm's worker processes will need to communicate with
    other services on arbitrary ports. For example, if you have a spout that consumes
    data from an external queue or a bolt that writes to a database, you will need
    additional firewall rules to enable that interaction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a scenario where we have a spout reading data from a Kafka queue and
    streaming data to a bolt that writes to a Cassandra database. In such scenarios,
    we would set up our cluster with the following `whirr.instance-template` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'With this setup, we need a firewall configuration that allows each of the Supervisor/worker
    nodes to connect to each of the Kafka nodes on port 9092 and each of the Cassandra
    nodes on port 9126, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customizing firewall rules](img/8294OS_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For this situation, Whirr Storm has the configuration property `whirr.storm.supervisor.firewall-rules`
    that allows you to open arbitrary ports on other nodes in the cluster. The property
    value is a comma-delimited list of role-port pairs as shown in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, to set up the rules for our scenario, we would use the following
    setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This configuration will instruct Whirr Storm to create firewall rules that allow
    each Supervisor node to connect to each Cassandra node on port 9160 and each Supervisor
    node to connect to each Kafka node on port 9092.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Vagrant
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vagrant ([http://www.vagrantup.com](http://www.vagrantup.com)) is a tool similar
    to Apache Whirr in that it's designed to help provision virtual machine instances
    in an easy and repeatable manner. However, Whirr and Vagrant differ in a key way.
    While Whirr's primary purpose is to enable cloud-based provisioning, Vagrant focuses
    more on local virtualization with virtualization software such as VirtualBox and
    VMWare.
  prefs: []
  type: TYPE_NORMAL
- en: Vagrant supports several virtual machine providers, including VirtualBox ([https://www.virtualbox.org](https://www.virtualbox.org))
    and VMWare ([http://www.vmware.com](http://www.vmware.com)). In this chapter,
    we'll cover the use of Vagrant with VirtualBox since it is free and well supported
    by Vagrant.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to using Vagrant, you must install a 4.x Version of VirtualBox (Vagrant
    does not yet support Version 5.x). We covered the VirtualBox installation in [Chapter
    2](ch02.html "Chapter 2. Configuring Storm Clusters"), *Configuring Storm Clusters*,
    and will not repeat those instructions here. Installing VirtualBox is largely
    just a matter of running an installer, but if you run into issues, please refer
    to the instructions in [Chapter 2](ch02.html "Chapter 2. Configuring Storm Clusters"),
    *Configuring Storm Clusters*.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Vagrant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Linux packages and Vagrant installers for OS X and Windows are available on
    the Vagrant website ([http://www.vagrantup.com/downloads.html](http://www.vagrantup.com/downloads.html)).
    Be sure to install the latest version of Vagrant as it will include the most recent
    updates and bug fixes. The installation process will update your system''s `PATH`
    variable to include the Vagrant executable. You can verify the installation by
    opening a terminal and typing `vagrant --version` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: If the command fails for any reason, consult the Vagrant website for solutions
    to common problems.
  prefs: []
  type: TYPE_NORMAL
- en: Launching your first virtual machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Launching a virtual machine with Vagrant involves two steps. First, you initialize
    a new Vagrant project with the `vagrant init` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The two arguments to the `vagrant init` command are `name` and `URL` for a Vagrant
    *box*. A Vagrant box is a virtual machine image that is specially packaged for
    use with Vagrant. Since Vagrant boxes can be quite large (over 300 MB), Vagrant
    will store them locally on the disk rather than download them every time. The
    `name` parameter simply provides an identifier for the box, so it can be reused
    in other Vagrant configurations, while the `URL` parameter tells Vagrant about
    the download location for the box.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to launch the virtual machine as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: If the Vagrant box specified in the `vagrant init` command is not found on the
    local disk, Vagrant will download it. Vagrant will then clone the virtual machine,
    boot it, and configure networking so it is easily accessible from the host machine.
    When the command completes, a VirtualBox virtual machine running Ubuntu 12.04
    LTS 64-bit will be running in the background.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can then log in to the machine using SSH commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The Vagrant user has administrative privileges so you are free to do anything
    you like with the virtual machine, such as install software packages and modify
    files. When you are finished with the virtual machine, you can shut it down and
    remove all traces of it with the `vagrant destroy` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Vagrant provides additional management commands for operations such as suspending,
    resuming, and halting the virtual machine. For an overview of the commands Vagrant
    provides, run the `vagrant --help` command.
  prefs: []
  type: TYPE_NORMAL
- en: The Vagrantfile and shared filesystem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When we ran the `vagrant init` command, Vagrant created a file named `Vagrantfile`
    in the directory where we ran the command. This file describes the type of machine(s)
    a project requires and how to provision and set up the machines. Vagrantfiles
    are written using a Ruby syntax that is easy to learn even if you are not a Ruby
    developer. The initial content of the `Vagrantfile` will be minimal and largely
    made up of documentation comments. With the comments removed, our Vagrant file
    looks like the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the file simply contains the box name and URL that we passed
    to the `vagrant init` command. We will expand on this later as we build out a
    Vagrant project to provision a virtualized Storm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: When you launch a machine with `vagrant up`, by default Vagrant will create
    a shared folder on the virtual machine (`/vagrant`) that will be synchronized
    with the contents of the project directory (the directory containing the `Vagrantfile`).
    You can verify this functionality by logging in to the virtual machine and listing
    the contents of that directory
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This is where we will store all our provisioning scripts and data files. While
    the `vagrant destroy` command removes all traces of a virtual machine, it leaves
    the contents of the project directory untouched. This allows us to store persistent
    project data that will always be available to our virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: Vagrant provisioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vagrant supports provisioning with shell scripts as well Puppet and Chef. We'll
    use the shell provisioner since it is the easiest to start with as it does not
    require any additional knowledge aside from basic shell scripting.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate how Vagrant shell provisioning works, we''ll modify our Vagrant
    project to install the Apache web server in the Vagrant virtual machine. We will
    begin by creating a simple shell script to install Apache2 using Ubuntu''s APT
    package manager. Save the following script as `install_apache.sh` in the same
    directory as the `Vagrantfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll modify our `Vagrantfile` to execute our script when Vagrant provisions
    our virtual machine by adding the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, configure port forwarding so requests to port 8080 on the host machine
    are forwarded to port 8080 on the guest (virtual) machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Our complete Vagrantfile should now look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: If your virtual machine is still running, kill it now by running `vagrant destroy`,
    then execute `vagrant up` to bring up a new virtual machine. When Vagrant completes,
    you should be able to view the default Apache page by pointing your browser to
    `http://localhost:8080` on the host machine.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring multimachine clusters with Vagrant
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to model a virtualized Storm cluster with Vagrant, we need a way to
    configure multiple machines within a single Vagrant project. Fortunately, Vagrant
    supports multiple machines with a syntax that makes it easy to convert our existing
    single-machine project into a multimachine configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our multimachine setup, we''ll define two virtual machines named `www1`
    and `www2`. To avoid port conflicts on the host machine, we''ll forward the host
    port 8080 to port 80 on `www1` and the host port 7070 to port 80 on `www2`, as
    shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'With a multimachine setup, running `vagrant up` without arguments will bring
    up every machine defined in the `Vagrantfile`. This behavior applies to Vagrant''s
    other management commands as well. To control an individual machine, add that
    machine''s name to the command. For example, if we want to launch just the `www1`
    machine, we would use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, to destroy virtual machine, we would use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Creating Storm-provisioning scripts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html "Chapter 2. Configuring Storm Clusters"), *Configuring
    Storm Clusters*, we covered the manual installation of Storm and its dependencies
    on Ubuntu Linux. We can leverage the commands we used in [Chapter 2](ch02.html
    "Chapter 2. Configuring Storm Clusters"), *Configuring Storm Clusters*, by using
    them to create Vagrant provisioning scripts to automate what would otherwise be
    a manual process. If you don't understand some of the commands used in the provisioning
    scripts, refer to [Chapter 2](ch02.html "Chapter 2. Configuring Storm Clusters"),
    *Configuring Storm Clusters*, for a more in-depth explanation.
  prefs: []
  type: TYPE_NORMAL
- en: ZooKeeper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ZooKeeper is available pre-packaged for most Linux platforms, which makes our
    installation script simple, letting the package manager do most of the work. The
    following is the command line to install ZooKeeper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'And the commands to install ZooKeeper are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Storm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Storm installation script is a little more complicated since it is not
    pre-packaged and must be installed manually. We''ll take the commands we used
    in [Chapter 2](ch02.html "Chapter 2. Configuring Storm Clusters"), *Configuring
    Storm Clusters*, assemble them into a script, and parameterize them to the script
    so it expects a Storm version string as an argument. This will allow us to easily
    switch between different Storm versions without having to modify the installation
    script as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The `install-storm.sh` script leverages the existence of the Vagrant shared
    directory (`/vagrant`). This allows us to keep the `storm.yaml` and `logback.xml`
    files in a convenient location right next to the `Vagrantfile`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `storm.yaml` file, we will use hostnames instead of IP addresses and
    let Vagrant configure the name resolution as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Supervisord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The supervisord service is installed by the `install-storm.sh` script, but
    we still need to configure it to manage the Storm daemons. Instead of creating
    separate configuration files for each service, we''ll write a script that generates
    the supervisord configuration with a service name as a parameter, as shown in
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The `configure-supervisord.sh` script expects a single argument representing
    the name of the Storm service to manage. For example, to generate a supervisord
    configuration for the Nimbus daemon, you would invoke the script using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The Storm Vagrantfile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our Storm cluster, we will create a cluster with one ZooKeeper node, one
    Nimbus node, and one or more Supervisor nodes. Because the `Vagrantfile` is written
    in Ruby, we have access to many of Ruby's language features, which will allow
    us to make the configuration file more robust. We will, for example, make the
    number of Supervisor nodes easily configurable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `storm.yaml` file, we used hostnames rather than IP addresses, which
    means our machines must be able to resolve names to IP addresses. Vagrant does
    not come with a facility for managing entries in the `/etc/hosts` file, but fortunately,
    there is a Vagrant plugin that does. Before we delve into the `Vagrantfile` for
    the Storm cluster, install the `vagrant-hostmanager` plugin ([https://github.com/smdahlen/vagrant-hostmanager](https://github.com/smdahlen/vagrant-hostmanager))
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The `vagrant-hostmanager` plugin will set up hostname resolution for all the
    machines in our cluster. It also has an option to add the name resolution between
    the host machine and virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at the complete `Vagrantfile` and walk through it line by
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The first line of the file tells the Ruby interpreter to require the `uri` module,
    which we will use for URL parsing.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we set up some variables representing the URL of the Storm distribution
    archive, the number of Supervisor nodes we want, and the name of the Vagrant box
    type for our virtual machines. These variables are intended to be changed by the
    user.
  prefs: []
  type: TYPE_NORMAL
- en: The `STORM_ARCHIVE` and `STORM_VERSION` values are set to the filename and version
    name of the Storm distribution by parsing the distribution URL using Ruby's `File`
    and `URI` classes. These values will be passed as arguments to the provisioning
    scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we enter the main Vagrant configuration section. We begin by configuring
    the `vagrant-hostmanager` plugin as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are telling the `vagrant-hostmanager` plugin to manage the hostname
    resolution between the host machine and virtual machines and that it should manage
    the `/etc/hosts` files on the virtual machines as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next block checks to see whether the Storm distribution archive has already
    been downloaded; if not, it uses the `wget` command to download it as shown in
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will download the Storm archive to the same directory as
    the `Vagrantfile`, thus making it accessible to the provisioning scripts in the
    `/vagrant` shared directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next two code blocks configure ZooKeeper and Nimbus and are relatively
    straightforward. They contain two new directives we have not seen before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The `zookeeper.vm.network` directive signals Vagrant to assign a specific IP
    address to the virtual machine using the VirtualBox host-only network adapter.
    The next line tells Vagrant to set the hostname on the virtual machine to a specific
    value. Finally, we invoke the provisioning scripts appropriate for each node.
  prefs: []
  type: TYPE_NORMAL
- en: The final block configures the Supervisor node(s). The Ruby code creates a loop
    iterating from `1` to the value of `STORM_SUPERVISOR_COUNT` and allows you to
    set the number of Supervisor nodes in the cluster. It will dynamically set the
    virtual machine name, hostname, and IP address based on the number of Supervisor
    nodes specified by the `STORM_SUPERVISOR_COUNT` variable.
  prefs: []
  type: TYPE_NORMAL
- en: Launching the Storm cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With our cluster defined in the `Vagrantfile` and our provisioning scripts in
    place, we're ready to launch the Vagrant cluster with `vagrant up`. With four
    machines and a considerable amount of software to install on each, this will take
    a while.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once Vagrant has finished launching the cluster, you should be able to view
    the Storm UI from the host machine at `http://nimbus:8080`. To submit a topology
    to the cluster, you can do so with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've just scratched the surface of deploying Storm in a cloud
    environment but hopefully introduced you to the many possibilities available,
    from deploying it to a hosted cloud environment such as Amazon EC2 to deploying
    it to a local cloud provider on your workstation or even an in-house hypervisor
    server.
  prefs: []
  type: TYPE_NORMAL
- en: We encourage you to explore both cloud hosting providers such as AWS as well
    as virtualization options such as Vagrant in more depth to better equip your Storm
    deployment options. Between the manual installation procedures introduced in [Chapter
    2](ch02.html "Chapter 2. Configuring Storm Clusters"), *Configuring Storm Clusters*,
    and the technology introduced in this chapter, you should be well equipped to
    find the development, test, and deployment solution that best fits your needs.
  prefs: []
  type: TYPE_NORMAL
