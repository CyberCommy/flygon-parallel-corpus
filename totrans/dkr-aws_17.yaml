- en: Elastic Kubernetes Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Kubernetes** is a popular open source container management platform originally
    developed by Google, who based Kubernetes on Google''s own internal **Borg** ([https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/](https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/))
    container platform. Kubernetes draws on Google''s extensive experience of running
    containers at scale, and is now supported by all of the major cloud platform providers
    with the release of the AWS Elastic Kubernetes Service (EKS). EKS provides a managed
    Kubernetes cluster to which you can deploy your container applications, without
    having to worry about day-to-day operational overheads and the complexities of
    cluster management. AWS has performed all the heavy lifting of establishing a
    robust and scalable platform, making it easier than ever to get up and running
    with Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will be introduced to the world of Kubernetes, we will
    work through how we can configure Kubernetes to ensure we are able to successfully
    deploy and operate the sample application we have used through this book, and
    then establish an EKS cluster in AWS where you will deploy the application using
    the configuration you have developed locally. This will provide practical, real-world
    insights into how, as an application owner, you can deploy your container workloads
    to Kubernetes, and how you can quickly get up and running with EKS.
  prefs: []
  type: TYPE_NORMAL
- en: We will first learn how you can work with the platform locally, using the native
    support that Docker for Mac and Docker for Windows now include for Kubernetes.
    You can spin up a local single-node cluster straight out of the box, reducing
    much of the manual configuration you would typically require to get a local environment
    up and running. You will learn how to create the various types of resources required
    to run the sample application in Kubernetes, addressing key operational challenges
    such as providing persistent storage for your application database, secrets management,
    and running one-shot tasks such as database migrations.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have established a working configuration to get the sample application
    up and running locally in Kubernetes, we will turn our attention to getting started
    with EKS, creating an EKS cluster, and establishing an EC2 auto scaling group
    where worker nodes that run your container workloads are managed. You will learn
    how to set up access to your cluster from your local environment and proceed to
    deploy the Kubernetes Dashboard, which provides a rich management user interface
    from which you can deploy and manage your applications. Finally, you will set
    up integrations with other AWS services including Elastic Block Store (EBS) and
    Elastic Load Balancing (ELB), and proceed to deploy the sample application to
    your EKS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Kubernetes using Docker Desktop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating core Kubernetes resources including pods, deployments, and services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating Persistent volumes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating Kubernetes secrets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running Kubernetes jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an EKS cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establishing access to an EKS cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying applications to EKS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Administrator access to an AWS account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A local AWS profile, configured as per the instructions in Chapter 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS CLI version 1.15.71 or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker 18.06 or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Compose 1.22 or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GNU Make 3.82 or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter assumes you have completed all of the preceding chapters in this
    book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following GitHub URL contains the code samples used in this chapter: [https://github.com/docker-in-aws/docker-in-aws/tree/master/ch17](https://github.com/docker-in-aws/docker-in-aws/tree/master/ch17).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2LyGtSY](http://bit.ly/2LyGtSY)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Kubernetes **is an open source container management platform that was open
    sourced by Google in 2014, and achieved production readiness in 2015 with its
    1.0 release. In the space of three years, it has established itself as the most
    popular container management platform, and is very popular for larger organizations
    that are looking to run their applications as container workloads. Kubernetes
    is one of the most popular open source projects ([https://github.com/cncf/velocity/blob/master/docs/top30_chart_creation.md](https://github.com/cncf/velocity/blob/master/docs/top30_chart_creation.md))
    on GitHub, and according to [Redmonk](https://redmonk.com/fryan/2017/09/10/cloud-native-technologies-in-the-fortune-100/),
    Kubernetes is used at 54% of Fortune 100 companies as of late 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Key features of Kubernetes include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Platform agnostic**: Kubernetes can run anywhere, from your local machine
    to your data centre and in cloud providers such as AWS, Azure, and Google Cloud,
    whom all now offer integrated managed Kubernetes offerings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open source**: Kubernetes'' greatest strength is its community and open source
    nature, which has seen Kubernetes become one of the leading open source projects
    on the planet. Major organizations and vendors are investing significant time
    and resources contributing to the platform, ensuring that the entire community
    benefits from these ongoing enhancements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pedigree**: Kubernetes'' roots are from Google''s internal Borg platform,
    which has been running containers at scale since the early 2000s. Google is one
    of the pioneers of container technology and is without a doubt one of, if not
    the largest, adopters of containers – back in 2014, Google indicated that they
    were running 2 billion containers every week, at a time when most enterprises
    had only just heard about containers through a new project called Docker that
    was taking the tech industry by storm. This pedigree and heritage ensures many
    of the lessons Google has learned over its many years of running containers at
    scale are encapsulated in the Kubernetes platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Production grade container management features**: Kubernetes offers all of
    the container management features that you would expect to see and will come across
    with other competing platforms. This includes cluster management, multi-host networking,
    pluggable storage, health checks, service discovery and load balancing, service
    scaling and rolling updates, desired stage configuration, role-based access control,
    and secret management to name a few. All of these features are implemented in
    a modular building-block fashion that allows you to tune the system to meet the
    specific requirements of your organization, and is one of the reasons Kubernetes
    is now considered the gold standard for enterprise-grade container management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes versus Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, I provided my own thoughts on Docker Swarm versus Kubernetes,
    and here I will continue, this time with more of a focus on why you would choose
    Kubernetes over Docker Swarm. As you work through this chapter, it should become
    apparent that Kubernetes has a more elaborate architecture that means there is
    a higher learning curve, and what I cover in this chapter only really scratches
    the surface of what is possible with Kubernetes. That said, once you get your
    head around these concepts, at least from my perspective, you should see that
    ultimately Kubernetes is more powerful with greater flexibility, and arguably
    it's probably fair to state that Kubernetes certainly feels more "enterprise-grade"
    than Docker Swarm, with many more knobs you can tune to tailor Kubernetes to your
    specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: Probably the biggest advantage Kubernetes has over Docker Swarm and other competitors
    is its community, which is significant and means that information about almost
    any configuration scenario you can think of, can be readily found across the wider
    Kubernetes community and ecosystem. There has been a lot of momentum behind the
    Kubernetes movement, and this only seems to be growing as leading vendors and
    providers such as AWS embrace Kubernetes with their own offerings and solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Architecturally, Kubernetes organizes itself in the form of a cluster, where
    master nodes form the cluster control plane, and worker nodes run your actual
    container workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/64ef6b49-8e21-40b1-a9e2-3473309890f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Within each master node, a number of components exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '**kube-apiserver**: This exposes the Kubernetes API and is the frontend component
    that you use to interact with the Kubernetes control plane.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**etcd**: This provides a distributed and highly available key/value store
    across the cluster that is used to store Kubernetes configuration and operational
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-scheduler**: This schedules pods onto worker nodes, taking into consideration
    resource requirements, constraints, data locality, and other factors. You will
    learn more about pods later on, but for now you can think of them as a collection
    of associated containers and volumes that collectively need to be create created,
    updated, and deployed together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-controller-manager**: This is responsible for managing controllers,
    which consists of a number of components that detects when your nodes go down,
    ensures that the correct number of instances or replicas of your pods are running,
    publishes service endpoints for the applications running in your pods, and manages
    service accounts and API access tokens for the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cloud-controller-manager**: This provides controllers that interact with
    an underlying cloud provider, enabling cloud providers to support features specific
    to their platform. Examples of cloud controllers include service controllers,
    which create, update, and delete cloud provider load balancers, and volume controllers,
    which create, attach, detach, and delete the various storage volume technologies
    supported by the cloud provider.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Add-ons**: A number of add-ons are available that extend the functionality
    of your cluster. These run in the form of pods and services that provide cluster
    features. One add-on that is typically deployed on most installations is the cluster
    DNS add-on, which provides automatic DNS naming and resolution for services and
    pods running on the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On all nodes, the following components exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '**kubelet**: An agent that runs on each node in the cluster and ensures all
    containers in a pod are healthy and running. The kubelet can also collect container
    metrics that can be published to monitoring systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-proxy**: This manages network communications, port mappings, and routing
    rules required on each node to support the various service abstractions that Kubernetes
    supports.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container runtime**: This provides the container engine that runs containers.
    The most popular container runtime that''s supported is Docker, however under
    container runtimes such as rkt (Rocket) or any OCI runtime-spec implementation
    can be supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pods**: A pod is the core unit of work for deploying your container applications.
    Each pod consists of one more containers and associated resources, and a single
    network interface, meaning each container in a given pod shares the same network
    stack.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that worker nodes only run the components previously listed directly, while
    master nodes run all of the components we have discussed to date, allowing master
    nodes to also run container workloads for scenarios such as a single-node cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes also provides a client component called **kubectl**, which provides
    the ability to manage your clusters via the Kubernetes API. **kubectl** is supported
    on Windows, macOS, and Linux, and allows you to easily manage and switch between
    multiple clusters, running both locally and remotely.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have been briefly introduced to Kubernetes, let's focus on getting
    up and running with Kubernetes in your local environment.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier in this book when you set up your local development environment, if
    you are using macOS or Windows, you installed the community edition (CE) versions
    of Docker Desktop (Docker for Mac or Docker for Windows, which I may refer to
    collectively as Docker Desktop in this chapter), which includes native support
    for Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a variant of Docker for Mac/Windows that does not support Kubernetes,
    or are using Linux, you can install minikube by following the instructions at [https://github.com/kubernetes/minikube](https://github.com/kubernetes/minikube).
    Most of the examples included in this section should work with minikube, although
    features such as load balancing and dynamic host path provisioning may not be
    directly supported and require some additional configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable Kubernetes, select **Kubernetes** in your local Docker Desktop settings,
    and check the **Enable Kubernetes** option. Once you click **Apply**, Kubernetes
    will be installed and will take a few minutes to get up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9c033101-8ec4-4dba-bfaf-dd99d43ed4e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Enabling Kubernetes using Docker for Mac
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Desktop also installs and configures the Kubernetes command-line utility
    `kubectl` automatically for you, which can be used to verify your installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are using Docker for Windows in conjunction with the Linux subsystem
    for Windows, you will need to install `kubectl` into the subsystem by running
    the following commands (see [https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-via-native-package-management](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-via-native-package-management) for
    more details):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After installing `kubectl` , if you previously changed your Linux subsystem
    home folder to your Windows home folder, you should be able now interact with
    your local Kubernetes cluster without further configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your home folder is different from the Windows home folder (this is the
    case by default), then you will need to set up a symbolic link that points to
    the `kubectl` config file in your Windows home folder, after which you should
    be able to use `kubectl` to interact with your local Kubernetes installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The Linux subsytem for Windows also allows you to run Windows command-line programs,
    so alternatively you can run `kubectl.exe` to invoke the Windows kubectl component.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Kubernetes, you deploy your applications as *pods*, which refer to one or
    more containers and other resources that are closely related to each other and
    collectively represent your application. A **pod** is the core unit of work in
    Kubernetes and is conceptually similar to an ECS task definition, although under
    the hood they work in a completely different way.
  prefs: []
  type: TYPE_NORMAL
- en: A common shorthand code for Kubernetes is k8s, where the "ubernete" portion
    of the name Kubernetes is replaced with the digit 8, representing the number of
    characters in "ubernete".
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we create our first pod, let''s establish a folder called `k8s` in the
    todobackend repository that will hold all Kubernetes configuration for the todobackend
    application, and then create a folder called `app`, which will store all resource
    definitions that relate to the core todobackend applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code demonstrates a basic pod definition for the todobackend
    application, which we will save to the `k8s/app/deployment.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The format of the pod configuration file is easy to follow, and in general most
    of the parameters that you see map to parameters of the same name if you are used
    to defining your containers using Docker Compose. One important difference that
    does tend to cause confusion is the `command` parameter – in Kubernetes, this
    parameter is the equivalent of the `ENTRYPOINT` Dockerfile directive and `entrypoint`
    parameter in a Docker Compose service specification, while the `args` parameter
    in Kubernetes is equivalent to the CMD directive (Dockerfile) and `command` service
    parameter (Docker Compose). This means that in the preceding configuration, the
    default entrypoint script in our container is bypassed, and instead the uwsgi
    web server will be run directly.
  prefs: []
  type: TYPE_NORMAL
- en: The `imagePullPolicy` property value of `IfNotPresent` configures Kubernetes
    to only pull an image if one is not available in the local Docker Engine registry,
    which means you must ensure the existing todobackend Docker Compose workflow has
    been run to build and tag the todobackend image locally, before attempting to
    create the pod. This is required as Kubernetes only includes native support for
    ECR when you are running Kubernetes on AWS EC2 instances, and does not natively
    support ECR when you are running Kubernetes outside of AWS.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of third-party plugins available that allow you to manage
    AWS credentials and pull ECR images. A popular example can be found at [https://github.com/upmc-enterprises/registry-creds](https://github.com/upmc-enterprises/registry-creds)
  prefs: []
  type: TYPE_NORMAL
- en: 'To create our pod and verify that it is running, you can run the `kubectl apply`
    command, with the `-f` flag referencing the deployment file you just created,
    followed by the `kubectl get pods` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the status of the pod is `Running` and that a container has
    been deployed to the single-node Kubernetes cluster running in your local Docker
    Desktop environment. One important point to note is that the todobackend container
    that has been deployed has no means of communicating with the outside world, as
    there are no networks ports that have been published from the pod and its associated
    container.
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting aspect of Kubernetes is that you can use the Kubernetes API
    to interact with your pods. To demonstrate this, you first run the `kubectl proxy` command,
    which sets up a local HTTP proxy that exposes the API via a plain old HTTP interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now access the container port 8000 on your pod via the URL `http://localhost:8001/api/v1/namespaces/default/pods/todobackend:8000/proxy/`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/30a20671-faac-4e12-af03-adfd67e9629a.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the kubectl proxy
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the todobackend application is running, although it is missing
    static content as we haven't generated this yet. Notice also that the todos link
    at the bottom of the page (`http://localhost:8001/todos`) is invalid, as the todobackend
    application has no knowledge of the API path that is called to access the application
    via the proxy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another interesting feature of Kubernetes is the ability to expose a port from
    your Kubernetes client to the application by running the `kubectl port-forward`
    command, which publishes a local port on the client and connects it to a specified
    pod using the Kubernetes API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If you now attempt to access `http://localhost:8000`, you should see the todobackend
    home page, and the todos link at the bottom of the page should now be accessible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6ccde81a-5c0f-4f36-bdb8-79dfb0de4d8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Accessing a port forwarded pod
  prefs: []
  type: TYPE_NORMAL
- en: You can see that, once again, our application is not in a fully functional state,
    given we haven't configured any database settings as yet.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we have been able to publish our todobackend application, the mechanism
    that we have used to do this is not suitable for real-world production use, and
    is only really useful for limited local development scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: One key requirement for running our application in the real-world is the ability
    to scale up or down the number of instances or *replicas* of the application container.
    To achieve this, Kubernetes supports a class of resources referred to as *controllers*,
    which are responsible for coordinating, orchestrating, and managing multiple replicas
    of a given pod. One popular type of controller is the *deployment* resource, which
    as the name suggests includes support for creating and updating new versions of
    your pods along with features such as rolling upgrades and support for rollbacks
    should a deployment fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates updating the `k8s/app/deployment.yaml` file
    in the `todobackend` repository to define a deployment resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We update the previous pod resource to now be a deployment resource, with the
    `template` property of the top-level `spec` property (i.e. `spec.template`) defining
    inline the pod that should be deployed. A key concept of deployments and Kubernetes
    in general is the use of set-based label selector matching ([https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors))
    to determine the resources or pods that the deployment applies to. In the preceding
    example, the deployment resource `spec` specifies two `replicas` and uses `selectors.matchLabels`
    to match the deployment to a pod that includes the label `app` with a value of
    `todobackend`. This is a simple yet powerful paradigm that allows you to create
    your own structures and relationships between resources in a flexible and loosely
    coupled manner. Notice that we also add the `readinessProbe` and `livenessProbe`
    properties to the container definition, which create a readiness probe and liveness
    probe, respectively. A readiness probe defines an action that should be performed
    by Kubernetes to determine if a container is ready, while a liveness probe is
    used to determine if a container is still healthy. In the preceding example, the
    readiness probe uses HTTP GET requests to port 8000 to determine when the deployment
    controller should permit connections to be forwarded to the container, while the
    liveness probe is used to restart the container in the event it no longer responds
    to the liveness probes. Refer to [https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/)
    for further information on the different types of probes and how they can be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the new deployment resource, we can first remove the existing pod
    and then apply the `k8s/app/deployment.yaml` file in the `todobackend` repository
    using `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: After creating the deployment, you can see that the configured number of replicas
    are deployed in the form of two pods, each with a unique name.  The state for
    each pod will transition to ready as soon as the readiness probe you configured
    is successful.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we have defined a pod for our application and deployed multiple
    replicas of our application using a deployment resource, and we now need to ensure
    external clients can connect to our application. Given that we have multiple replicas
    of our application running, we require a component that is able to provide a stable
    service endpoint, track the location of each replica, and load balance incoming
    connections across all replicas.
  prefs: []
  type: TYPE_NORMAL
- en: '*Services* are the Kubernetes resources that provide such features, where each
    service is assigned a virtual IP address that can be used to access a given set
    of pods, and incoming connections to the virtual IP address are load balanced
    to each pod replica, based upon iptables rules that are managed and updated via
    a standard Kubernetes system resource called the kube-proxy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/869fe4cb-aa5f-4772-935c-4f14ca899e43.png)'
  prefs: []
  type: TYPE_IMG
- en: Services and endpoints in Kubernetes
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, a client pod is attempting to communicate with the
    application pod using a virtual IP address of `10.1.1.1` on port `80` (`10.1.1.1:80`).
    Note that the service virtual IP address is published on every node in the cluster,
    with the **kube-proxy** component responsible for updating iptables rules that
    select an appropriate endpoint that client connections should be routed to in
    a round robin fashion. Because the virtual IP address is published on every node
    in the cluster, any client on any node can communicate with the service, and traffic
    is distributed across the cluster in an even manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have a high level understanding of how a service works, let''s
    actually define a new service in the `k8s/app/deployment.yaml` file that''s located
    within the `todobackend` repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Notice that you can define multiple resources in a single YAML file by using
    the `---` separator to separate each resource, and that we can create a service
    called todobackend that uses label matching to bind the service to any pods with
    a label of `app=todobackend`. In the `spec.ports` section, we configure port 80
    as the incoming or listener port on the service, which load balances connections
    to a `targetPort` of 8000 on each pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the definition of our service in place, you can now deploy the service
    using the `kubectl apply` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You can use the `kubectl get svc` command to view current services, and notice
    that each service includes a unique cluster IP address, which is the virtual IP
    address that other resources in the cluster can use to communicate with the pods
    associated with the service. The `kubectl get endpoints` command shows the actual
    endpoints associated with each service, and you can see that connections to the
    `todobackend` service virtual IP address of `10.103.210.17:80` will be load balanced
    to `10.1.0.27:8000` and `10.1.0.30:8000`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each service is also allocated a unique DNS name in the form of `<service-name>.<namespace>.svc.cluster.local`.
    The default namespace in Kubernetes is called `default`, so for our todobackend
    application, it will be assigned a name of `todobackend.default.svc.cluster.local`,
    which you can verify is reachable within the cluster by using the `kubectl run`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, you can simply query for todobackend, as Kubernetes
    sends the DNS search domain to `<namespace>.svc.cluster.local` (`default.svc.cluster.local`,
    in our use case), and you can see that this resolves to the cluster IP address
    of the todobackend service.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that the cluster IP address is only reachable within
    the Kubernetes cluster – we can't reach this service externally without further
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Exposing a service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To allow external clients and systems to communicate with Kubernetes services,
    you must expose the service to the outside world. In true Kubernetes style, there
    are a variety of options available to achieve this, which are controlled by Kubernetes
    `ServiceTypes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Node ports**: This service type maps an external port on each Kubernetes
    node to the internal cluster IP and port configured for the service. This creates
    several external connection points for your service that may change as nodes come
    and go, making it difficult to create a stable external service endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load balancer**: Represents an external dedicated Layer 4 (TCP or UDP) load
    balancer that is mapped exclusively to your service. The actual load balancer
    that is deployed depends on your target platform – for example, with AWS, a classic
    Elastic Load Balancer is created. This is a very popular option, however one significant
    limitation is that a load balancer is created per service, meaning that this option
    can become quite expensive if you have a lot of services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ingress**: This is a shared Layer 7 (HTTP) load balancer resource that works
    in a similar fashion to the AWS Application Load Balancer, where connections to
    a single HTTP/HTTPS endpoint can be routed to multiple services based upon host
    header or URL path patterns. This is considered the best option for HTTP-based
    services, given you can share one load balancer across multiple services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The most popular method to publish your services externally is to use the load
    balancer method, which works as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/95505204-2d0d-4d38-9188-5741bbd5bfc6.png)'
  prefs: []
  type: TYPE_IMG
- en: Load balancing in Kubernetes
  prefs: []
  type: TYPE_NORMAL
- en: The external load balancer publishes the external service endpoint that clients
    will connect to, which in the preceding example is `192.0.2.43:80`. The load balancer
    service endpoint will be associated with the nodes in your cluster that have active
    pods associated with the service, who each have a node port mapping set up via
    the **kube-proxy** component. The node port mapping is then mapped to each of
    the local endpoints on the node, allowing traffic to be load balanced efficiently
    and evenly across the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: For communications from internal clients within the cluster, communications
    still take place using the service cluster IP address, as described earlier in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Later on in this chapter, we will see how to integrate AWS load balancers with
    EKS, however for now your local Docker Desktop environment includes support for
    its own load balancer resource, which publishes an external endpoint on your host
    for your service. Adding an external load balancer to a service is very simple,
    as demonstrated in the following example, where we modify the configuration of
    the `k8s/app/deployments.yaml` file in the todobackend repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'All that is required to deploy the appropriate load balancer for your environment
    is to set the `spec.type` property to `LoadBalancer`, and Kubernetes will automatically
    create an external load balancer. You can test this by applying your updated configuration
    and running the `kubectl get svc` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the `kubectl get svc` output now shows that the external IP address
    of the todobackend service is localhost (localhost is always the external interface
    that's reachable by your Docker client when using Docker Desktop) and that it
    is published externally on port 80, which you can verify is true by running the
    `curl localhost` command. The external port maps to port 31417 on a single node
    cluster, which is the port that the **kube-proxy** component listens on in order
    to support the load balancer architecture we described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Adding volumes to your pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have an understanding of how to publish our application both internally
    within the Kubernetes cluster and externally to the outside world, we can focus
    on making the todobackend application fully functional by adding support for the
    various deployment activities and dependencies of the todobackend application.
  prefs: []
  type: TYPE_NORMAL
- en: We will first tackle the issue of serving static content for the todobackend
    application – as you know from previous chapters, we need to run a **collectstatic**
    task that ensures static content is available for the **todobackend** application,
    and this should be run any time the **todobackend** application is deployed. The
    **collectstatic** task needs to write static content to a volume that is then
    mounted by the main application container, so let's discuss how we can add volumes
    to Kubernetes pods.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has a powerful storage subsystem that supports a variety of volume
    types, which you can read more about at [https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes](https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes).
    For the **collectstatic** use case, the [emptyDir](https://kubernetes.io/docs/concepts/storage/volumes/#emptydir)
    volume type is suitable, which is a volume that follows the lifecycle of each
    pod – it is created and destroyed dynamically with the pod – hence it is suitable
    as an ephemeral storage type for use cases such as caching and serving static
    content that can be easily regenerated on pod creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates adding a public `emptyDir` volume to the
    `k8s/app/deployment.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We define a volume called `public` in the `spec.Volumes` property of the pod
    template, and then use the `volumeMounts` property in the todobackend container
    definition to mount the `public` volume to `/public`. One important configuration
    requirement for our use case is setting the `spec.securityContext.fsGroup` property,
    which defines the group ID that will be configured as the group owner for any
    filesystem mounts in the pod. We set this value to `1000`; recall from earlier
    chapters that the todobackend image runs as the `app` user, which has a user/group
    ID of 1000\. This configuration ensures that the todobackend container is able
    to read and write static content to the `public` volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you now deploy your configuration changes, you should be able to use the
    `kubectl exec` command to inspect the todobackend container filesystem and verify
    that we can read and write to the `/public` mount:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `kubectl exec` command is similar to the `docker exec` command, in that
    it allows you to execute a command within a currently running pod container. This
    command must reference the name of the pod, and we use the `kubectl get pods`
    command along with a JSON path query to extract this name. As you can see, the
    `app` user within the **todobackend** container is able to read and write to the
    `/public` mount.
  prefs: []
  type: TYPE_NORMAL
- en: Adding init containers to your pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With an ephemeral volume in place for static content, we can now focus on scheduling
    the **collectstatic** task to generate static content for our application. Kubernetes
    has support for [init containers](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/),
    which are a special type of container within a pod that are executed before your
    main application container(s) are started. Kubernetes will ensure that your init
    containers run to completion and complete successfully before starting your application,
    and if you specify multiple init containers, Kubernetes will execute them in order,
    one by one, until all init containers have completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code demonstrates adding an init container to the `k8s/app/deployment.yaml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now deploy your changes and use the `kubectl logs` command to verify
    that the collectstatic init container executed successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If you now browse to `http://localhost` in your browser, you should be able
    to verify that the static content is now rendering correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6dce9b13-5f6f-4d61-a6a3-6dc3ac1017b6.png)'
  prefs: []
  type: TYPE_IMG
- en: The todobackend application with correct static content
  prefs: []
  type: TYPE_NORMAL
- en: Adding a database service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step in getting the **todobackend** application fully functional is
    to add a database service that will host the **todobackend** application database.
    We will run this service within our Kubernetes cluster, however in a real-world
    production use case in AWS, I would typically recommend using the Relational Database
    Service (RDS).
  prefs: []
  type: TYPE_NORMAL
- en: 'Defining the database service requires two main configuration tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating persistent storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a database service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating persistent storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key requirement of our database service is persistent storage, and for our
    single-node local Kubernetes development environment, the [**hostPath**](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath) volume
    type represents the standard option for providing simple persistent storage requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Although you can create a **hostPath** volume very easily by specifying a path
    directly in your volume definition (see the example pod definition at [https://kubernetes.io/docs/concepts/storage/volumes/#hostpath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath)),
    one problem with such an approach is that it creates a hard dependency on the
    underlying volume type, and also requires manual cleanup if you ever want to delete
    the pod and the data associated with the volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'A very useful feature of the Docker Desktop Kubernetes support is the inclusion
    of a dynamic volume provisioner called `docker.io/hostpath` that automatically
    creates volumes of type **hostPath** for you, which is available via the default
    *storage class* that you can view by running the `kubectl get sc` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: A storage class provides an abstraction over the underlying volume type, meaning
    your pods can request storage from a specific class. This includes generic requirements
    such as volume size, without needing to worry about the underlying volume type.
    In the case of Docker Desktop, a default storage class is included out of the
    box, which provisions storage requests using the **hostPath** volume type.
  prefs: []
  type: TYPE_NORMAL
- en: However, later on when we set up a Kubernetes cluster in AWS using EKS, we will
    configure a default storage class that uses the AWS Elastic Block Store (EBS)
    as the underlying volume type. Taking this approach means that we don't need to
    change our pod definitions, as we will be referring to the same storage class
    in each environment.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using minikube, a dynamic provisioner called `k8s.io/minikube-hostpath`
    provides similar functionality to the Docker hostpath provisioner, but mounts
    volumes under `/tmp/hostpath-provisioner`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use storage classes rather than specify your volume types directly with
    your pod definitions, you need to create a *persistent volume claim*, which provides
    a logical definition of storage requirements such as volume size and access mode.
    Let''s define a persistent volume claim, but before we do this we need to establish
    a new folder called `k8s/db` in the todobackend repository that will store our
    database service configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Within this folder, we will create a file called `k8s/db/storage.yaml`, in
    which we will define a persistent volume claim:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We create the claim (called `todobackend-data`) in a dedicated file, as this
    will allow us to independently manage the life cycle of the claim. One property
    that is not included in the preceding example is the `spec.storageClassName` property
    – if this is omitted, the default storage class is used, however bear in mind
    that you can create and reference your own storage classes. The `spec.accessModes`
    property specifies how the storage should be mounted – in the case of both local
    storage and EBS storage in AWS, we only want a single container at a time to be
    able to read and write to the volume, which is encompassed by the `ReadWriteOnce`
    access mode.
  prefs: []
  type: TYPE_NORMAL
- en: The `spec.resources.requests.storage` property specifies the size of the persistent
    volume, which in this case we configure as 8 GB.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using Docker for Windows, you will be prompted to share your C:\
    with Docker the first time you attempt to use the Docker hostPath provisioner.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you now deploy the persistent volume claim using `kubectl`, you can use
    the `kubectl get pvc` command to view your newly created claim:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that when you create a persistent volume claim, a persistent volume
    is dynamically created. When using Docker Desktop, this is actually created in
    the path `~/.docker/Volumes/<persistent-volume-claim>/<volume>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are using Docker for Windows and you are using the Windows Subsystem
    for Linux, you can create a symbolic link to the `.docker` folder on your Windows
    host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note that if you followed the instructions in [Chapter 1](3d8c99a9-f463-4891-ad1a-bc2450d91251.xhtml),
    *Container and Docker Fundamentals*, for setting up the Windows Subsystem for
    Linux, you already configured `/mnt/c/Users/<user-name>/` as your home directory
    so you don't need to perform the configuration above.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a database service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have created a persistent volume claim, we can define the database
    service. We will define the database service within a new file called `k8s/db/deployment.yaml` in
    the `todobackend` repository, where we create a service and deployment definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We first define a service called `todobackend-db`, which publishes the default
    MySQL TCP port `3306`. Notice that we specify a `spec.clusterIP` value of `None`,
    which creates a headless service. Headless services are useful for single instance
    services and allow the IP address of the pod to be used as the service endpoint,
    rather than using the **kube-proxy** component with a virtual IP address that
    would only ever load balance to a single endpoint. Defining a headless service
    will still publish a DNS record for the service but will associate the record
    with the pod IP address, ensuring that the **todobackend** application can connect
    to the `todobackend-db` service by name. We then create a deployment for the `todobackend-db`
    service, and define a volume called `data` which is mapped to the persistent volume
    claim we created earlier and mounted to the database data directory (`/var/lib/mysql`)
    in the MySQL container. Notice that we specify the `args` property (the equivalent
    of the CMD/command directive in Docker/Docker Compose), which configures MySQL
    to ignore the `lost+found` directory if it is present. Although this won't be
    a problem when using Docker Desktop, it will be a problem in AWS for the same
    reasons discussed in the previous Docker Swarm chapter. Finally, we create a liveness
    probe of type `exec` that executes the `mysqlshow` command to check that a connection
    to the MySQL database can be made locally within the MySQL container. Because
    the MySQL secret is located in a file, we wrap the MySQL command in a shell process
    (`/bin/sh`), which allows us to use the `$(cat /tmp/secrets/MYSQL_PASSWORD)` command
    substitution.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes allows you resolve environment variables at execution time by using
    the syntax `$(<environment variable>)`. For example, the `$(MYSQL_USER)` value
    included in the preceding liveness probe will be resolved to the environment variable
    `MYSQL_USER` when the probe is executed. See [https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#use-environment-variables-to-define-arguments](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#use-environment-variables-to-define-arguments)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you now deploy the database service and deployment resources, you can use
    the `kubectl get svc` and `kubectl get endpoints` commands to verify your headless
    service configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the `todobackend-db` service is deployed with a cluster IP of none,
    which means that the published endpoint of the service is the IP address of the
    `todobackend-db` pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also verify that the data volume was created correctly by listing the
    contents of the physical volume directory in `~/.docker/Volumes/todobackend-data` on
    your local host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If you now delete just the database service and deployment, you should be able
    to verify that the persistent volume is not removed and persists, meaning that
    you can then recreate the database service and reattach to the `data` volume with
    no data loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is a good example of why we separated out the persistent
    volume claim into its own file – doing this means that we can easily manage the
    life cycle of the database service without any data loss. In the event that you
    do want to destroy the database service and its data, you can choose to remove
    the persistent volume claim, in which case the Docker Desktop **hostPath** provisioner
    will automatically remove the persistent volume and any stored data.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes also supports a controller called a StatefulSet, which is specifically
    designed for stateful applications such as databases. You can read more about
    StatefulSets at [https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/).
  prefs: []
  type: TYPE_NORMAL
- en: Creating and consuming secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes supports *secret* objects, which allow sensitive data such as a password
    or token to be stored securely in an encrypted format, and then exposed privately
    as required to your containers. Kubernetes secrets are stored in a key/value map
    or dictionary format, which is different from Docker secrets, which as you saw
    in the previous chapter typically just store the secret value.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create secrets manually using literal values, or by including your
    secret values in a file and applying the file. I recommend creating your secrets
    using literal values to avoid storing your secrets in your configuration files,
    which may be inadvertently committed and pushed to your source code repositories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, you use the `kubectl create secret generic` command
    to create a secret called `todobackend-secret` that stores three secret values.
    Notice that each value is stored with a key that has the same name as the expected
    environment variable, which will make configuration of these values easy to consume.
  prefs: []
  type: TYPE_NORMAL
- en: With the secret now created, you can configure the `todobackend` and `db` deployments
    to consume the secret. Kubernetes includes a special volume type called secret
    that allows you to mount your secrets at a configurable location in your containers,
    which your applications can then read securely and privately.
  prefs: []
  type: TYPE_NORMAL
- en: Consuming secrets for the database service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s first update the database deployment resource that is defined in the
    `k8s/db/deployment.yaml` file to consume the `todobackend-secret`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: You first create a volume called `secrets` that is of type `secret` which references
    the `todobackend-secret` we created earlier. By default, all secret items will
    be available, however you can control which items are published to the volume
    via the optional `items` property. Because the `todobackend-secret` includes the
    `SECRET_KEY` secret that is specific to the todobackend application, we configure
    the `items` list to exclude this item and only present the `MYSQL_PASSWORD` and
    `MYSQL_ROOT_PASSWORD` keys. Note that the specified `path` is required and is
    expressed as a relative path based from where the secret volume is mounted in
    each of your containers.
  prefs: []
  type: TYPE_NORMAL
- en: You then mount the `secrets` volume as read only to `/tmp/secrets` in the `db`
    container, and update the password-related environment variables to reference
    the secret files rather than using values directly from the environment. Notice
    that each secret value will be created in a file that is named based upon the
    key within the folder that the secret volume is mounted to.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy our new configuration, you first need to delete the database service
    and its associated persistent volume as this includes the previous credentials,
    and then redeploy the database service. You can do this very easily by referencing
    the entire `k8s/db` directory when you execute the delete and apply actions, rather
    than specifying each file individually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have recreated the `db` service, you can use the `kubectl exec` command
    to verify that the `MYSQL_PASSWORD` and `MYSQL_ROOT_PASSWORD` secret items have
    been written to `/tmp/secrets`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Consuming secrets for the application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now need to update the todobackend service to consume our secrets by modifying
    the `k8s/app/deployment.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: You must define the `secrets` volume and ensure that only the `MYSQL_PASSWORD`
    and `SECRET_KEY` items are exposed to the **todobackend** container. After mounting
    the volume as read only in the **todobackend** application container, you must
    then configure the `SECRETS_ROOT` environment variable with the path to the `secrets`
    mount. Recall in the last chapter that we added support to the **todobackend**
    application for consuming Docker secrets, which by default expects your secrets
    to be located at `/run/secrets`. However, because `/run` is a special tmpfs filesystem,
    you cannot mount your secrets using a regular file system mount at this location,
    hence we need to configure the `SECRETS_ROOT` environment variable, which reconfigure
    the secrets location the application will look in. We must also configure the
    `MYSQL_HOST` and `MYSQL_USER` environment variables, so that along with the `MYSQL_PASSWORD`
    secret, the **todobackend** application has the required information to connect
    to the database service.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you now deploy the changes, you should be able to verify that the correct
    secret items are mounted in the **todobackend** container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: If you browse to `http://localhost/todos`, you should receive an error indicating
    that a database table does not exist, which means that the application is now
    successfully connecting and authenticating to the database, but is missing the
    required schema and tables that the application expects.
  prefs: []
  type: TYPE_NORMAL
- en: Running jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our **todobackend** application is almost fully functional, however there is
    one key deployment task that we need to perform, which is to run database migrations
    to ensure that the correct schema and tables are present in the **todobackend**
    database. As you have seen throughout this book, database migrations should only
    be executed once per deployment, regardless of the number of instances running
    of our application. Kubernetes supports tasks of this nature via a special controller
    of type *job*, which as the name suggests, runs a task or process (in the form
    of a pod) until the job completes successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a job for the required database migrations task, we will create a
    new file called `k8s/app/migrations.yaml` that''s located in the `todobackend`
    repository, which allows you to run the job independently of the other application
    resources that are defined in the co-located `deployment.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: You must specify a kind of `Job` to configure this resource as a job, and for
    the most part, the configuration is very similar to the pod/deployment template
    we created earlier, except for the `spec.backoffLimit` property, which defines
    how many times Kubernetes should attempt to re-run the job should it fail, and
    the template `spec.restartPolicy` property, which should always be set to `Never`
    for a job.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you now run the job, you should be to verify that database migrations ran
    successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: At this point, you have successfully deployed the todobackend application in
    a fully functional state, and you should be able to connect to the todobackend
    application and create, update, and delete todo items.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an EKS cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have a solid understanding of Kubernetes and have defined the core
    resources required to deploy and run the todobackend application locally, it is
    time to shift our attention to the Elastic Kubernetes Service (EKS).
  prefs: []
  type: TYPE_NORMAL
- en: The core resource supported by EKS is the EKS cluster, which represents a fully
    managed, highly available cluster of Kubernetes managers that take care of the
    Kubernetes control plane for you. In this section, we will focus on creating an
    EKS cluster in AWS, establishing authentication and access to the cluster, and
    deploying the Kubernetes dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating an EKS cluster consists of the following primary tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Install client components**: In order to manager your EKS cluster, you need
    to install various client components, including `kubectl` (which you have already
    installed) and the AWS IAM authenticator for Kubernetes tool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create cluster resources**: This establishes the control plane component
    of Kubernetes, which consists of Kubernetes masters. When using EKS, the masters
    are provided as a fully managed service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configure kubectl for EKS**: This allows you to manage EKS using your local
    kubectl client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create worker nodes**: This consists of Kubernetes nodes that are intended
    to run your container workloads. When using EKS, you are responsible for creating
    your own worker nodes, which you will typically deploy in the form of EC2 auto
    scaling groups. Just like for the ECS service, AWS provides an EKS-optimized AMI
    ([https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html))
    that includes all of the necessary software components for a worker node to join
    your EKS clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deploy the Kubernetes dashboard**: The Kubernetes dashboard provides you
    with a web-based management interface to manage and monitor your cluster and container
    applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of writing, EKS clusters are not part of the AWS free tier and cost
    $0.20 USD per minute to run, so bear this in mind before you continue (see [https://aws.amazon.com/eks/pricing/](https://aws.amazon.com/eks/pricing/)
    for latest pricing information). We will be using CloudFormation templates to
    deploy both the EKS cluster and EKS worker nodes, so you can easily tear down
    and recreate your EKS cluster and worker nodes as required to reduce costs.
  prefs: []
  type: TYPE_NORMAL
- en: Installing client components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To manage your EKS cluster, you must have `kubectl` installed, as well as the
    AWS IAM authenticator for Kubernetes components, which allows `kubectl` to authenticate
    to your EKS cluster using your IAM credentials.
  prefs: []
  type: TYPE_NORMAL
- en: 'You already have `kubectl` installed, so to install the AWS IAM authenticator
    for Kubernetes, you need to install a binary called `aws-iam-authenticator` that
    is published by AWS as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Creating cluster resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before creating your an EKS cluster, you need to ensure that your AWS account
    meets the following prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: '**VPC resources**: EKS resources must be deployed into a VPC with a minimum
    of three subnets. AWS recommends that you create your own dedicated VPC and subnets
    per EKS cluster, however in this chapter we will use the default VPC and subnets
    that are automatically created in your AWS account. Note that when you create
    a VPC and define the subnets that your cluster will use, you must specify *all*
    subnets where you expect your worker nodes *and *load balancers will be deployed.
    A recommended pattern is to deploy your worker nodes in private subnets, and ensure
    that you have also included public subnets so that EKS can create public facing
    load balancers as required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EKS service role**: When creating an EKS cluster, you must specify an IAM
    role that grants access to the EKS service to manage your clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Control plane security group**: You must provide a security group that is
    used for control plane communications between your EKS cluster managers and worker
    nodes. The security group rules will be modified by the EKS service, so you should
    create a new, empty security group for this requirement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The AWS documentation includes a Getting Started ([https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html](https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html))
    section, which provides details on how to create EKS clusters using the AWS console.
    Given that EKS is supported by CloudFormation and the infrastructure as code approach
    we have used throughout this book, we need to create a folder called `eks` in
    the `todobackend-aws` repository and define our EKS cluster and the associated
    EKS service role in a new CloudFormation template file called `todobackend-aws/eks/stack.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The template requires two input parameters – the target VPC ID and target Subnet
    IDs. The `EksServiceRole` resource creates an IAM role that grants the `eks.awsamazon.com`
    service the ability to manage EKS clusters on your behalf, as specified by the
    managed policies referenced in the `ManagedPolicyArns` property. You must then
    define an empty security group for control plane communications, and finally define
    the EKS cluster resource, referencing the `EksServiceRole` resource for the `RoleArn`
    property, and defining a VPC configuration that targets the input `ApplicationSubnets`
    and uses the `EksClusterSecurityGroup` resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now deploy this template using the `aws cloudformation deploy` command,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The cluster will take approximately 10 minutes to create and, once created,
    you can use the AWS CLI to obtain further information about the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The cluster endpoint and certificate authority data are both required for later
    on in this chapter, so take note of these values.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring kubectl for EKS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With your EKS cluster created, you now need to add the new cluster to your local
    `kubectl` configuration. All clusters that `kubectl` is aware of are defined in
    a file called `~/.kube/config` by default, which at the moment will include a
    single cluster called `docker-for-desktop-cluster` if you are using Docker for
    Mac or Docker for Windows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code demonstrates adding your EKS cluster and associated configuration
    to the `~/.kube/config` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: You must first add a new cluster called `eks-cluster` to the `clusters` property,
    specifying the certificate authority data and server endpoint you captured earlier
    after you created the EKS cluster. You must then add a context called `eks`, which
    will allow you to switch between your local Kubernetes server and your EKS cluster,
    and finally add a new user called `aws` to the users section which is used by
    the `eks` context to authenticate to your EKS cluster. The `aws` user configuration
    configures kubectl to execute the `aws-iam-authenticator` component you previously
    installed, passing the argument `token -i eks-cluster` and using your local `docker-in-aws`
    profile to authenticate access. Executing this command will automatically return
    an authentication token to `kubectl` that can then be used to authenticate to
    your EKS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the preceding configuration in place, you should now be able to access
    a new context called `eks` and verify connectivity to your EKS cluster, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Note that if you are using the **multi-factor authentication** (**MFA**) configuration
    we set up in earlier chapters, you will be prompted to enter an MFA token every
    single time you run a `kubectl` command against your EKS cluster, which will quickly
    become tiresome.
  prefs: []
  type: TYPE_NORMAL
- en: 'To disable MFA temporarily, you can remove your user account from the Users
    group using the `aws iam remove-user-from-group` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'And then comment the `mfa_serial` line for your local AWS profile in the `~/.aws/config`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Creating worker nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next step in setting up EKS is creating worker nodes that will join your
    EKS cluster. Unlike the Kubernetes master nodes that are fully managed by AWS,
    you are responsible for creating and managing your worker nodes. AWS provide an
    EKS-optimized AMI that includes all of the software required to join an EKS cluster
    and operate as an EKS worker. You can browse to [https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html)
    to obtain the latest AMI ID for your region:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/75c0a503-604b-4852-bf2b-bc5234c54df1.png)Amazon EKS-Optimized AMI'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing this book, the EKS-Optimized AMI requires extensive
    configuration using the **cfn-init** framework that we learned about in earlier
    chapters. The recommended approach to create your worker nodes is to use a predefined
    CloudFormation template which is published by AWS that already includes the required
    configuration specified at [https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html](https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/86d253f0-e18a-494d-9ca2-56ee21b93408.png)Worker CloudFormation template
    URL'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now create a new CloudFormation stack for your worker nodes by selecting
    **Services** | **CloudFormation** in the AWS console, clicking the **Create Stack**
    button, and pasting the worker template URL you obtained previously in the **Choose
    a template** section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cafa3d9c-5393-4aa9-9be5-7aaeec57b1ec.png)Creating a worker node
    CloudFormation stack'
  prefs: []
  type: TYPE_NORMAL
- en: 'After clicking **Next**, you will be prompted to enter a stack name (you can
    specify a name of `eks-cluster-workers` or similar) and provide the following
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ClusterName**: Specifies the name of your EKS cluster (`eks-cluster`, in
    our example).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ClusterControlPlaneSecurityGroup**: The name of the control plane security
    group. In our example, we previously created a security group called `eks-cluster-control-plane-sg`
    when we created our EKS cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NodeGroupName**: This defines part of the name of the EC2 auto scaling group
    that will be created for your workers. For our scenario, you can specify a name
    of `eks-cluster-workers` or similar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NodeAutoScalingGroupMinSize** **and** **NodeAutoScalingGroupMaxSize**: By,
    default these are set to 1 and 3, respectively. Note that the CloudFormation template
    sets the desired size of the auto scaling group to the value of the `NodeAutoScalingGroupMaxSize`
    parameter, so you may want to lower this value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NodeInstanceType**: The smallest instance type you can specify using the
    predefined worker CloudFormation template is `t2.small`. For EKS, the node instance
    type is not just important in terms of CPU and memory resources, but also has
    implications on pod capacity in terms on networking requirements. The EKS networking
    model ([https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html](https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html))
    exposes each pod in your EKS cluster as an IP address that''s reachable within
    your VPC, using a combination of elastic network interfaces (ENI) and secondary
    IP addresses running on each ENI. You can refer to [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI),
    which describes the maximum number of ENIs and secondary IP addresses per interface
    for the various EC2 instance types and ultimately determines the maximum number
    of pods you can run per node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NodeImageId**: Specifies the ID of the EKS-Optimized AMI for your region
    (see the previous screenshots).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KeyName**: Specifies an existing EC2 key pair in your account (for example,
    the admin keypair you created earlier in this book).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VpcId**: Specifies the VPC ID where your EKS cluster is located.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subnets**: Specifies the subnets where you would like to place your workers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once you have configured the various parameters that are required, click on
    the **Next** button twice and finally acknowledge that CloudFormation may create
    IAM resources before clicking the **Create** button to deploy your worker nodes.
    When your stack has been created successfully, open the **Outputs** tab for the
    stack and take a note of the `NodeInstanceRole` output, which is required for
    the next configuration step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b95ee581-3246-49f3-89ed-40a29347f66e.png)Obtaining the NodeInstanceRole
    output'
  prefs: []
  type: TYPE_NORMAL
- en: Joining worker nodes to your EKS cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the CloudFormation stack has deployed successfully, your worker nodes
    will attempt to join your cluster, however before they can do this you need to
    grant access to the EC2 instance role of your worker nodes by applying an AWS
    authenticator `ConfigMap` resource called `aws-auth` to your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: A ConfigMap is simply a key/value data structure used to store configuration
    data that can be used by different resources in your cluster. The `aws-auth` ConfigMap
    is used by EKS to grant AWS users the ability to interact with your cluster, which
    you can read more about at [https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html](https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html).
    You can also download a sample `aws-auth` ConfigMap from [https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-06-05/aws-auth-cm.yaml](https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-06-05/aws-auth-cm.yaml).
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the `aws-auth` ConfigMap, create a file called `aws-auth-cm.yaml`
    in the `todobackend-aws/eks` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, you need to paste the value of the `NodeInstanceRole`
    output you obtained when you created your workers CloudFormation stack. Once you
    have created this file, you can now apply it to your EKS cluster using the `kubectl
    apply` command, and then wait for your worker nodes to join the cluster by running `kubectl
    get nodes --watch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Once all of your workers have a status of `Ready`, you have successfully joined
    your worker nodes to your EKS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the Kubernetes dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final step in setting up your EKS cluster is to deploy the Kubernetes dashboard
    to your cluster. The Kubernetes dashboard is a powerful and comprehensive web-based
    management interface for managing and monitoring your cluster and container applications,
    and is deployed as a container-based application within the `kube-system` namespace
    of your Kubernetes cluster. The dashboard consists of a number of components that
    I won't go into detail about here, but you can read more about the dashboard at [https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard).
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy the dashboard, we will first create a folder called `todobackend-aws/eks/dashboard` and
    proceed to download and apply the various components that comprise the dashboard
    to this folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'You then need to create a file called `eks-admin.yaml` that creates a service
    account and cluster role binding with full cluster-admin privileges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating this file, you need to apply it to your EKS cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'With the `eks-admin` service account in place, you can retrieve an authentication
    token for this account by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The key piece of information in the preceding example is the token value, which
    you need to copy and paste when you connect to the dashboard. To connect to the
    dashboard, you need to start the kubectl proxy, which provides HTTP access to
    the Kubernetes API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'If you now browse to `http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/`,
    you will be prompted to sign in to the dashboard, where you need to paste the
    token that you retrieved previously for the `eks-admin` service account:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8315bcd4-56a5-44a0-b7a5-52bee1895ec2.png)'
  prefs: []
  type: TYPE_IMG
- en: Signing in to the Kubernetes dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have signed in, if you change the Namespace to **kube-system** and
    select **Workloads** | **Deployments**, it is possible that you may be shown an
    error indicating that the image for the **monitoring-influxdb** deployment could
    not be found:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c029f6d5-044b-49cc-99ad-afb86d5b0fbd.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes dashboard deployment failure
  prefs: []
  type: TYPE_NORMAL
- en: 'If this is the case, you will need to update the `todobackend-aws/eks/dashboard/influxdb.yml`
    file that you downloaded earlier to reference `k8s.gcr.io/heapster-influxdb-amd64:v1.3.3`
    (this is a known issue (`https://github.com/kubernetes/heapster/issues/2059`))
    that may or may not exist when you are reading this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: If you now re-apply the file by running `kubectl apply -f influxdb.yml`, the
    dashboard should now show all services running as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the sample application to EKS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that our EKS cluster and worker nodes are in place and we have confirmed
    that we can deploy to the cluster, it's time to deploy the todobackend application
    to EKS. You have already performed the majority of the hard work earlier when
    you defined the various resources required to run your application locally in
    Kubernetes, and all that is required is to adapt some of the external resources
    such as the load balancer and persistent volume for the database service to use
    AWS native services.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are now required to perform the following configuration tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring support for persistent volumes using the AWS Elastic Block Store
    (EBS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring support for AWS Elastic Load Balancers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the sample application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring support for persistent volumes using AWS EBS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in this chapter, we discussed the concepts of persistent volume claims
    and storage classes, which allow you to abstract the details of your storage infrastructure
    away from your applications. We learned that when using Docker Desktop, a default
    storage class is provided that will automatically create persistent volumes of
    type hostPath that are accessible from your local operating system at `~/.docker/Volumes`,
    which makes it easy to provision, manage, and maintain persistent volumes when
    using Docker Desktop with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: When using EKS, it is important to understand that unlike Docker Desktop, by
    default, no storage classes are created for you. This requires you to create at
    least one storage class if you want to support persistent volume claims, and in
    most use cases, you would typically define a default storage class that provides
    a standard default storage medium and volume type for your cluster. When using
    EKS, a good candidate for these storage classes is the Elastic Block Store (EBS),
    which provides a standard integrated mechanism to support block-based volume storage
    for the EC2 instances that run as worker nodes in your cluster. Kubernetes supports
    a volume type called `AWSElasticBlockStore`, which allows you to access and mount
    EBS volumes from your worker nodes, and also includes support for a storage provisioner
    called `aws-ebs`, which provides dynamic provisioning and management of EBS volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this native support for AWS EBS included out of the box, it is very easy
    to create a default storage class that will automatically provision EBS storage,
    which we will define in a file called `todobackend-aws/eks/gp2-storage-class.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We will create a storage class called `gp2`, which, as the name suggests, will
    provision EBS storage of type `gp2`, or SSD, from AWS, using the `kubernetes.io/aws-ebs`
    storage provisioner. The `parameters` section controls this storage selection,
    and depending on the type of storage, there may be other configuration options
    available, which you can read more about at [https://kubernetes.io/docs/concepts/storage/storage-classes/#aws](https://kubernetes.io/docs/concepts/storage/storage-classes/#aws).
    The value for `reclaimPolicy` can be either `Retain` or `Delete`, which controls
    whether or not the storage provisioner should retain or delete the associated
    EBS volume whenever a persistent volume claim associated with the storage class
    is deleted from Kubernetes. For production use cases, you would typically set
    this to `Retain`, but for non-production environments, you may want to set this
    to the default reclaim policy of `Delete` to save you from having to manually
    clean up EBS volumes that are no longer used by your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create this storage class in our EKS cluster, after which we can
    configure the new storage class to be the default storage class for the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: After creating the storage class, you use the `kubectl patch` command to add
    an annotation to the storage class, which configures the class as the default
    class. You can see that when you run the `kubectl describe sc/gp2` command to
    view details about the storage class, the `IsDefaultClass` attribute is set to
    `Yes`, confirming that the newly created class is the default storage class for
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: With this in place, the Kubernetes configuration for the **todobackend** application
    now has a default storage class that can be applied for the `todobackend-data`
    persistent volume claim, which will provision an EBS volume of type `gp2` based
    upon the storage class parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The `eksServiceRole` IAM role that you created earlier in this chapter includes
    the `AmazonEKSClusterPolicy` managed policy, which grants your EKS cluster the
    ability to manage EBS volumes. If you choose to implement your own custom IAM
    policies for the EKS service role, you must ensure that you include the various
    EC2 IAM permissions for managing volumes, such as `ec2:AttachVolume`, `ec2:DetachVolume`,
    `ec2:CreateVolumes`, `ec2:DeleteVolumes`, `ec2:DescribeVolumes`, and `ec2:ModifyVolumes`
    (this is not an exhaustive list). See [https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html](https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html)
    for details on the full list of IAM permissions that are granted by AWS-defined
    EKS service roles and managed policies.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring support for AWS Elastic Load Balancers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in this chapter, when you defined your Kubernetes configuration for
    the todobackend application, you created a service for the todobackend application
    of type `LoadBalancer`. We discussed that the implementation details of the load
    balancer are specific to the platform that your Kubernetes cluster is deployed
    to, and in the case of Docker Desktop, Docker provides their own load balancer
    component that allows the service to be exposed to the local network interface
    on your development machine.
  prefs: []
  type: TYPE_NORMAL
- en: When using EKS, the good news is that you don't need to do anything to support
    services of type `LoadBalancer` – your EKS cluster will automatically create and
    associate an AWS Elastic Load Balancer with each service endpoint, with the `AmazonEKSClusterPolicy`
    managed policy granting the required IAM permissions for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes does allow you configure vendor-specific features of the `LoadBalancer`
    type by configuring *annotations*, which are a metadata property that will be
    understood by a given vendor on their target platform and ignored if deploying
    on a different platform, such as your local Docker Desktop environment. You can
    read more about these annotations at [https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types),
    and the following example demonstrates adding several annotations that are specific
    to the AWS Elastic Load Balancer to the service definition within the `todobackend/k8s/app/deployment.yaml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, we added the following annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`service.beta.kubernetes.io/aws-load-balancer-backend-protocol`: This configures
    the backend protocol. A value of `http` ensures that the `X-Forward-For` header
    is set on incoming requests so that your web applications can track client IP
    addresses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled`:
    This enables connection draining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout`:
    This specifies the connection draining timeout.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One important point to note is that the annotations expect every value to be
    a string value, so ensure that you quote boolean values such as `"true"` and `"false"`,
    as well as any numeric values such as `"60"`, as demonstrated in the preceding
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the sample application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You are now ready to deploy the sample application to AWS, which you can do
    by first switching over to the todobackend repository and ensuring that use are
    using the `eks` context you created earlier in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Creating secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that both the application and database services rely on secrets that
    we manually created in our local Docker Desktop context, so you first need to
    create these secrets in your EKS context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Deploying the database service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can now deploy the database service, which should create a new persistent
    volume backed by EBS as per the configuration of the default storage class you
    created earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that a persistent volume was created, and if you browse to **Services**
    | **EC2** in the AWS console and select **Volumes** from the left ELASTIC BLOCK
    STORAGE menu, you should be able to see a corresponding EBS volume for the persistent
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/30dc1bb5-7f02-4b95-9203-d31dc5359383.png)'
  prefs: []
  type: TYPE_IMG
- en: Viewing EBS volumes
  prefs: []
  type: TYPE_NORMAL
- en: Notice that Kubernetes tags the EBS volume with a number of tags that allow
    easy identification of which persistent volume and persistent volume claim a given
    EBS volume it is associated with.
  prefs: []
  type: TYPE_NORMAL
- en: In the Kubernetes dashboard, you can verify that the `todobackend-db` deployment
    is running by selecting **Workloads** | **Deployments:**
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/112c5cad-3258-4358-b836-c1fe352f41fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Viewing EBS volumes
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the application service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the database service in place, you can now proceed to deploy the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploying the application will perform the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the `todobackend-migrate` job, which runs database migrations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create the `todobackend` deployment, which runs a collectstatic initContainer
    and then runs the main todobackend application container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create the `todobackend` service, which will deploy a new service with an AWS
    ELB frontend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the Kubernetes dashboard, if you select **Discovery and Load Balancing**
    | **Services** and select the **todobackend** service, you can view each of the
    internal endpoints for the service, as well as the external load balancer endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/caeb39a5-d2a2-4864-b707-b2822a511d4f.png)'
  prefs: []
  type: TYPE_IMG
- en: Viewing the todobackend service in the Kubernetes dashboardYou can also obtain
    the external endpoint URL by running the `kubectl describe svc/todobackend` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click on the external endpoint URL, you should be able to verify that
    the todobackend application is fully functional, with all static content being
    displayed correctly and the ability to add, remove, and update Todo items in the
    application database:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/15a03619-bdd8-4854-875c-955fbad7f459.png)'
  prefs: []
  type: TYPE_IMG
- en: Verifying the todobackend application
  prefs: []
  type: TYPE_NORMAL
- en: Tearing down down the sample application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tearing down the sample application is very simple, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Once this has been completed, you should be able to verify that the Elastic
    Load Balancer resource associated with the todobackend service has been deleted,
    along with the EBS volume for the todobackend database, given that you configured
    the reclaim policy for the default storage class as Delete. Of course you should
    also delete the worker node stack and EKS cluster stack you created earlier in
    this chapter, to avoid unnecessary charges.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to deploy Docker applications using Kubernetes
    and the AWS Elastic Kubernetes Service (EKS). Kubernetes has established itself
    as one of the leading container management platforms with a strong open source
    community, and with AWS now supporting Kubernetes customers with the EKS service,
    Kubernetes is certain to grow even more in popularity.
  prefs: []
  type: TYPE_NORMAL
- en: You first learned how to leverage the native support for Kubernetes in Docker
    Desktop, which makes it very easy to get up and running with Kubernetes locally.
    You learned how to create a variety of core Kubernetes resources including pods,
    deployments, services, secrets, and jobs, which provide the fundamental building
    blocks for running your applications in Kubernetes. You also learned how to configure
    support for persistent storage, leveraging persistent volume claims to abstract
    the application's storage requirements away from the underlying storage engine.
  prefs: []
  type: TYPE_NORMAL
- en: You were then introduced to EKS and learned how to create an EKS cluster and
    associated supporting resources, including an EC2 auto scaling group that runs
    your worker nodes. You established access to the EKS cluster, and tested that
    the cluster was working correctly by deploying the Kubernetes dashboard, which
    provides a rich and powerful management user interface for your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you proceeded to deploy the todobackend application to EKS, which included
    integration with the AWS Elastic Load Balancer (ELB) service for external connectivity
    and the Elastic Block Store (EBS) for providing persist storage. An important
    consideration here is that we did not need to modify the Kubernetes configuration
    we created earlier when deploying locally to our Docker Desktop environment, other
    than adding a few annotations to control the configuration of todobackend service
    load balancer (these annotations are ignored when using Docker Desktop, so are
    considered "safe" vendor-specific configuration elements). You should always strive
    for this goal, as it ensures that your applications will have maximum portability
    across different Kubernetes environments, and can be easily deployed independently
    of the underlying Kubernetes platform, whether it be a local development environment,
    AWS EKS, or the Google Kubernetes Engine (GKE).
  prefs: []
  type: TYPE_NORMAL
- en: Well, all good things must come to an end, and it's time for me to say congratulations
    and thank you for completing this book! It was a great pleasure to write this
    book and I hope that you have learned how to leverage the power of Docker and
    AWS to test, build, deploy and operate your own container applications.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'True/false: Kubernetes is a native feature of Docker Desktop CE.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You define a custom command string to run in a pod definition using the commands
    property, and notice that the entrypoint script container is no longer executed.
    How can you resolve this?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True/false: Kubernetes includes three node types – manager, worker, and agent.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True/false: Kubernetes provides integration with AWS application load balancers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True/false: Kubernetes supports relocating EBS volumes to other nodes in your
    cluster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What component can you use to expose the Kubernetes API to web applications?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True/false: Kubernetes supports integration with the Elastic Container Registry.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What Kubernetes resource provides a virtual IP address that can be used to connect
    to multiple instances of a given application?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What Kubernetes resource is suitable for running database migrations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True/false: EKS manages both Kubernetes manager nodes and worker nodes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What type of EBS storage does the default storage class provision when using
    EKS?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You want to run a task every time you deploy a pod that needs to run before
    starting the main application in the pod. How would you achieve this?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can check the following links for more information about the topics covered
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Kubernetes?: [https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes Tutorials: [https://kubernetes.io/docs/tutorials/](https://kubernetes.io/docs/tutorials/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes Pods: [https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes Deployments: [https://kubernetes.io/docs/concepts/workloads/controllers/deployment/](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes Jobs: [https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes Services: [https://kubernetes.io/docs/concepts/services-networking/service/](https://kubernetes.io/docs/concepts/services-networking/service/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNS for Services and Pods: [https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes Secrets: [https://kubernetes.io/docs/concepts/configuration/secret/](https://kubernetes.io/docs/concepts/configuration/secret/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes Volumes: [https://kubernetes.io/docs/concepts/storage/volumes/](https://kubernetes.io/docs/concepts/storage/volumes/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes Persistent Volumes: [https://kubernetes.io/docs/concepts/storage/persistent-volumes/](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes Storage Classes: [https://kubernetes.io/docs/concepts/storage/storage-classes/](https://kubernetes.io/docs/concepts/storage/storage-classes/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic Volume Provisioning: [https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/](https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubectl Command Reference: [https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon EKS User Guide: [https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html](https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EKS Optimized AMI: [https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EKS Cluster CloudFormation Resource Reference: [https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-eks-cluster.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-eks-cluster.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
