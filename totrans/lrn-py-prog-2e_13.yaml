- en: Data Science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"If we have data, let''s look at data. If all we have are opinions, let''s
    go with mine."– Jim Barksdale, former Netscape CEO'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data science** is a very broad term and can assume several different meanings
    based on context, understanding, tools, and so on. There are countless books on
    this subject, which is not suitable for the faint-hearted.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to do proper data science, you need to, at the very least, know mathematics
    and statistics. Then, you may want to dig into other subjects, such as pattern
    recognition and machine learning and, of course, there is a plethora of languages
    and tools you can choose from.
  prefs: []
  type: TYPE_NORMAL
- en: I won't be able to talk about everything here. Therefore, in order to render
    this chapter meaningful, we're going to work on a cool project together instead.
  prefs: []
  type: TYPE_NORMAL
- en: Around the year 2012/2013, I was working for a top-tier social media company
    in London. I stayed there for two years, and I was privileged to work with several
    people whose brilliance I can only start to describe. We were the first in the
    world to have access to the Twitter Ads API, and we were partners with Facebook
    as well. That means a lot of data.
  prefs: []
  type: TYPE_NORMAL
- en: Our analysts were dealing with a huge number of campaigns and they were struggling
    with the amount of work they had to do, so the development team I was a part of
    tried to help by introducing them to Python and to the tools Python gives you
    to deal with data. It was a very interesting journey that led me to mentor several
    people in the company and eventually took me to Manila where, for two weeks, I
    gave intensive training in Python and data science to the analysts over there.
  prefs: []
  type: TYPE_NORMAL
- en: The project we're going to do in this chapter is a lightweight version of the
    final example I presented to my students in Manila. I have rewritten it to a size
    that will fit this chapter, and made a few adjustments here and there for teaching
    purposes, but all the main concepts are there, so it should be fun and instructional
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we are going to explore the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The Jupyter Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pandas and NumPy: main libraries for data science in Python'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A few concepts around Pandas's `DataFrame` class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and manipulating a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start by talking about Roman gods.
  prefs: []
  type: TYPE_NORMAL
- en: IPython and Jupyter Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2001, Fernando Perez was a graduate student in physics at CU Boulder, and
    was trying to improve the Python shell so that he could have the niceties he was
    used to when he was working with tools such as Mathematica and Maple. The result
    of that effort took the name **IPython**.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, that small script began as an enhanced version of the Python
    shell and, through the effort of other coders and eventually with proper funding
    from several different companies, it became the wonderful and successful project
    it is today. Some 10 years after its birth, a Notebook environment was created,
    powered by technologies such as WebSockets, the Tornado web server, jQuery, CodeMirror,
    and MathJax. The ZeroMQ library was also used to handle the messages between the
    Notebook interface and the Python core that lies behind it.
  prefs: []
  type: TYPE_NORMAL
- en: The IPython Notebook has become so popular and widely used that, over time,
    all sorts of goodies have been added to it. It can handle widgets, parallel computing,
    all sorts of media formats, and much, much more. Moreover, at some point, it became
    possible to code in languages other than Python from within the Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'This has led to a huge project that at some stage has been split into two:
    IPython has been stripped down to focus more on the kernel part and the shell,
    while the Notebook has become a brand new project called **Jupyter**. Jupyter
    allows interactive scientific computations to be made in more than 40 languages.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter's project will all be coded and run in a Jupyter Notebook, so let
    me explain in a few words what a Notebook is.
  prefs: []
  type: TYPE_NORMAL
- en: A Notebook environment is a web page that exposes a simple menu and the cells
    in which you can run Python code. Even though the cells are separate entities
    that you can run individually, they all share the same Python kernel. This means
    that all the names that you define in a cell (the variables, functions, and so
    on) will be available in any other cell.
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, a Python kernel is a process in which Python is running. The Notebook
    web page is, therefore, an interface exposed to the user for driving this kernel.
    The web page communicates to it using a very fast messaging system.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from all the graphical advantages, the beauty of having such an environment
    lies in the ability to run a Python script in chunks, and this can be a tremendous
    advantage. Take a script that is connecting to a database to fetch data and then
    manipulate that data. If you do it in the conventional way, with a Python script,
    you have to fetch the data every time you want to experiment with it. Within a
    Notebook environment, you can fetch the data in a cell and then manipulate and
    experiment with it in other cells, so fetching it every time is not necessary.
  prefs: []
  type: TYPE_NORMAL
- en: The Notebook environment is also extremely helpful for data science because
    it allows for step-by-step introspection. You do one chunk of work and then verify
    it. You then do another chunk and verify again, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: It's also invaluable for prototyping because the results are there, right in
    front of your eyes, immediately available.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to know more about these tools, please check out [ipython.org](https://ipython.org/) and
    [jupyter.org](http://jupyter.org/).
  prefs: []
  type: TYPE_NORMAL
- en: 'I have created a very simple example Notebook with a `fibonacci` function that
    gives you the list of all the Fibonacci numbers smaller than a given `N`. In my
    browser, it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Every cell has an In [] label. If there's nothing between the brackets, it means
    that a cell has never been executed. If there is a number, it means that the cell
    has been executed, and the number represents the order in which the cell was executed.
    Finally, a * means that the cell is currently being executed.
  prefs: []
  type: TYPE_NORMAL
- en: You can see in the picture that in the first cell I have defined the `fibonacci`
    function, and I have executed it. This has the effect of placing the `fibonacci`
    name in the global frame associated with the Notebook, therefore the `fibonacci`
    function is now available to the other cells as well. In fact, in the second cell,
    I can run `fibonacci(100)` and see the results in Out [2]. In the third cell,
    I have shown you one of the several magic functions you can find in a Notebook
    in the second cell. %timeit runs the code several times and provides you with
    a nice benchmark for it. All the measurements for the list comprehensions and
    generators I did in [Chapter 5](part0142.html#47DFS0-2ddb708647cc4530a187c2c6c0e9acfe),
    *Saving Time and Memory*, were carried out with this nice feature.
  prefs: []
  type: TYPE_NORMAL
- en: You can execute a cell as many times as you want, and change the order in which
    you run them. Cells are very malleable, you can also put in markdown text or render
    them as headers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Markdown** is a lightweight markup language with plain text formatting syntax
    designed so that it can be converted to HTML and many other formats.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, whatever you place in the last row of a cell will be automatically printed
    for you. This is very handy because you're not forced to write `print(...)` explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to explore the Notebook environment; once you're friends with it,
    it's a long-lasting relationship, I promise.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the required libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to run the Notebook, you have to install a handful of libraries, each
    of which collaborates with the others to make the whole thing work. Alternatively,
    you can just install Jupyter and it will take care of everything for you. For
    this chapter, there are a few other dependencies that we need to install. You
    can find them listed in `requirements/requirements.data.science.in`. To install
    them, please take a look at `README.rst` in the root folder of the project, and
    you will find instructions specifically for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Using Anaconda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes installing data science libraries can be extremely painful. If you
    are struggling to install the libraries for this chapter in your virtual environment,
    an alternative choice you have is to install Anaconda. Anaconda is a free and
    open source distribution of the Python and R programming languages for data science
    and machine-learning-related applications that aims to simplify package management
    and deployment. You can download it from the [anaconda.org](https://anaconda.org/)
    website. Once you have installed it in your system, take a peek at the various
    requirements for this chapter and install them through Anaconda.
  prefs: []
  type: TYPE_NORMAL
- en: Starting a Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you have all the required libraries installed, you can either start a
    Notebook with the following command or by using the Anaconda interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You will have an open page in your browser at this address (the port might
    be different): `http://localhost:8888/`. Go to that page and create a new Notebook
    using the menu. When you feel comfortable with it, you''re ready to go. I strongly
    encourage you to try and get a Jupyter environment running, before you proceed
    reading on. It is an excellent exercise sometimes to have to deal with difficult
    dependencies.'
  prefs: []
  type: TYPE_NORMAL
- en: Our project will take place in a Notebook, therefore I will tag each code snippet
    with the cell number it belongs to, so that you can easily reproduce the code
    and follow along.
  prefs: []
  type: TYPE_NORMAL
- en: If you familiarize yourself with the keyboard shortcuts (look in the Notebook's
    Help section), you will be able to move between cells and handle their content
    without having to reach for the mouse. This will make you more proficient and
    way faster when you work in a Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now move on and talk about the most interesting part of this chapter:
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Typically, when you deal with data, this is the path you go through: you fetch
    it, you clean and manipulate it, and then you inspect it, and present results
    as values, spreadsheets, graphs, and so on. I want you to be in charge of all
    three steps of the process without having any external dependency on a data provider,
    so we''re going to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We're going to create the data, simulating the fact that it comes in a format
    that is not perfect or ready to be worked on
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We're going to clean it and feed it to the main tool we'll use in the project
    such as `DataFrame` from the `pandas` library
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We're going to manipulate the data in `DataFrame`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We're going to save `DataFrame` to a file in different formats
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We're going to inspect the data and get some results out of it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting up the Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First things first, let''s produce the data. We start from the `ch13-dataprep` Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Cell `#1` takes care of the imports. We have already encountered them, apart
    from `faker`. You can use this module to prepare fake data. It's very useful in
    tests, when you prepare your fixtures, to get all sorts of things such as names,
    email addresses, phone numbers, and credit card details. It is all fake, of course.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We want to achieve the following data structure: we''re going to have a list
    of user objects. Each user object will be linked to a number of campaign objects.
    In Python, everything is an object, so I''m using this term in a generic way.
    The user object may be a string, a dictionary, or something else.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A **campaign** in the social media world is a promotional campaign that a media
    agency runs on social media networks on behalf of a client. Remember that we''re
    going to prepare this data so that it''s not in perfect shape (but it won''t be
    that bad either...):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Firstly, we instantiate the `Faker` that we''ll use to create the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we need usernames. I want 1,000 unique usernames, so I loop over the length
    of the `usernames` set until it has 1,000 elements. A `set` method doesn''t allow
    duplicated elements, therefore uniqueness is guaranteed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, we create a list of `users`. Each `username` has now been augmented to
    a full-blown `user` dictionary, with other details such as `name`, `gender`, and
    `email`. Each `user` dictionary is then dumped to JSON and added to the list.
    This data structure is not optimal, of course, but we're simulating a scenario
    where users come to us like that.
  prefs: []
  type: TYPE_NORMAL
- en: Note the skewed use of `random.random()` to make 60% of users female. The rest
    of the logic should be very easy for you to understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note also the last line. Each cell automatically prints what''s on the last
    line; therefore, the output of `#4` is a list with the first three `users`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: I hope you're following along with your own Notebook. If you are, please note
    that all data is generated using random functions and values; therefore, you will
    see different results. They will change every time you execute the Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code `#5` is the logic to generate a campaign name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Analysts use spreadsheets all the time, and they come up with all sorts of coding
    techniques to compress as much information as possible into the campaign names.
    The format I chose is a simple example of that technique—there is a code that
    tells us the campaign type, then the start and end dates, then the target `age`
    and `gender`, and finally the currency. All values are separated by an underscore.
  prefs: []
  type: TYPE_NORMAL
- en: In the `get_type` function, I use `random.choice()` to get one value randomly
    out of a collection. Probably more interesting is `get_start_end_dates`. First,
    I get the duration for the campaign, which goes from one day to two years (randomly),
    then I get a random offset in time which I subtract from today's date in order
    to get the start date. Given that an offset is a random number between -365 and
    365, would anything be different if I added it to today's date instead of subtracting
    it?
  prefs: []
  type: TYPE_NORMAL
- en: When I have both the start and end dates, I return a stringified version of
    them, joined by an underscore.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we have a bit of modular trickery going on with the age calculation. I
    hope you remember the modulo operator (`%`) from [Chapter 2](part0056.html#1LCVG0-2ddb708647cc4530a187c2c6c0e9acfe),
    *Built-in Data Types*.
  prefs: []
  type: TYPE_NORMAL
- en: What happens here is that I want a date range that has multiples of five as
    extremes. So, there are many ways to do it, but what I do is to get a random number
    between `20` and `45` for the left extreme, and remove the remainder of the division
    by `5`. So, if, for example, I get *28*, I will remove *28 % 5 = 3* from it, getting
    *25*. I could have just used `random.randrange()`, but it's hard to resist modular
    division.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the functions are just some other applications of `random.choice()`
    and the last one, `get_campaign_name`, is nothing more than a collector for all
    these puzzle pieces that returns the final campaign name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In `#6`, we write a function that creates a complete campaign object. I used
    a few different functions from the `random` module. `random.randint()` gives you
    an integer between two extremes. The problem with it is that it follows a uniform
    probability distribution, which means that any number in the interval has the
    same probability of coming up.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, when dealing with a lot of data, if you distribute your fixtures
    using a uniform distribution, the results you get will all look similar. For this
    reason, I chose to use `triangular` and `gauss`, for `clicks` and `impressions`.
    They use different probability distributions so that we'll have something more
    interesting to see in the end.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to make sure we''re on the same page with the terminology: `clicks` represents
    the number of clicks on a campaign advertisement, `budget` is the total amount
    of money allocated for the campaign, `spent` is how much of that money has already
    been spent, and `impressions` is the number of times the campaign has been fetched,
    as a resource, from its source, regardless of the number of clicks that were performed
    on the campaign. Normally, the amount of `impressions` is greater than the number
    of `clicks`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the data, it''s time to put it all together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, each item in `data` is a dictionary with a `user` and a list
    of campaigns that are associated with that `user`.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start cleaning the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We simulate fetching the data from a source and then inspect it. The Notebook
    is the perfect tool for inspecting your steps. You can vary the granularity to
    your needs. The first item in `rough_data` looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we now start working on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing we need to do in order to be able to feed `DataFrame` with
    this `data` is to denormalize it. This means transforming `data` into a list whose
    items are campaign dictionaries, augmented with their relative `user` dictionary.
    Users will be duplicated in each campaign they belong to. The first item in `data`
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the `user` object has been brought into the campaign dictionary,
    which was repeated for each campaign.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, I would like to help you and offer a deterministic second part of the
    chapter, so I''m going to save the data I generated here so that I (and you, too)
    will be able to load it from the next Notebook, and we should then have the same
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You should find the `data.json` file in the source code for the book. Now we
    are done with `ch13-dataprep`, so we can close it, and open up `ch13`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we have another round of imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `json` and `calendar` libraries come from the standard library. `numpy`
    is the NumPy library, the fundamental package for scientific computing with Python. NumPy
    stands for Numeric Python, and it is one of the most widely-used libraries in
    the data science environment. I'll say a few words about it later on in this chapter.
    `pandas` is the very core upon which the whole project is based. **Pandas** stands
    for **Python Data Analysis Library**. Among many other things, it provides `DataFrame`,
    a matrix-like data structure with advanced processing capabilities. It's customary
    to import `DataFrame` separately and then to `import pandas as pd`.
  prefs: []
  type: TYPE_NORMAL
- en: '`arrow` is a nice third-party library that speeds up dealing with dates dramatically.
    Technically, we could do it with the standard library, but I see no reason not
    to expand the range of the example and show you something different.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the imports, we load the `data` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, it''s time to create `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can inspect the first five rows using the `head` method of `DataFrame`.
    You should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Jupyter renders the output of the `df.head()` call as HTML automatically. In
    order to have a text-based output, simply wrap `df.head()` in a `print` call.
  prefs: []
  type: TYPE_NORMAL
- en: The `DataFrame` structure is very powerful. It allows us to manipulate a lot
    of its contents. You can filter by rows, columns, aggregate on data, and many
    other operations. You can operate with rows or columns without suffering the time
    penalty you would have to pay if you were working on data with pure Python. This
    happens because, under the covers, `pandas` is harnessing the power of the NumPy
    library, which itself draws its incredible speed from the low-level implementation
    of its core.
  prefs: []
  type: TYPE_NORMAL
- en: Using `DataFrame` allows us to couple the power of NumPy with spreadsheet-like
    capabilities so that we'll be able to work on our data in a fashion that is similar
    to what an analyst could do. Only, we do it with code.
  prefs: []
  type: TYPE_NORMAL
- en: 'But let''s go back to our project. Let''s see two ways to quickly get a bird''s
    eye view of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`count` yields a count of all the non-empty cells in each column. This is good
    to help you understand how sparse your data can be. In our case, we have no missing
    values, so the output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Nice! We have 5,037 rows, and the data type is integers (`dtype: int64` means
    long integers because they take 64 bits each). Given that we have 1,000 users
    and the amount of campaigns per user is a random number between 2 and 8, we''re
    exactly in line with what I was expecting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `describe` method is a nice, quick way to introspect a bit further:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it gives us several measures, such as `count`, `mean`, `std`
    (standard deviation), `min`, and `max`, and shows how data is distributed in the
    various quadrants. Thanks to this method, we already have a rough idea of how
    our data is structured.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see which are the three campaigns with the highest and lowest budgets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And a call to tail shows us the ones with the lowest budgets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Unpacking the campaign name
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now it's time to increase the complexity. First of all, we want to get rid of
    that horrible campaign name (`cmp_name`). We need to explode it into parts and
    put each part in one dedicated column. In order to do this, we'll use the `apply`
    method of the `Series` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `pandas.core.series.Series` class is basically a powerful wrapper around
    an array (think of it as a list with augmented capabilities). We can extrapolate
    a `Series` object from `DataFrame` by accessing it in the same way we do with
    a key in a dictionary, and we can call `apply` on that `Series` object, which
    will run a function feeding each item in the `Series` to it. We compose the result
    into a new `DataFrame`, and then join that `DataFrame` with `df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Within `unpack_campaign_name`, we split the campaign `name` in parts. We use
    `arrow.get()` to get a proper `date` object out of those strings (`arrow` makes
    it really easy to do it, doesn''t it?), and then we return the objects. A quick
    peek at the last line reveals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Nice! One important thing: even if the dates appear as strings, they are just
    the representation of the real `date` objects that are hosted in `DataFrame`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another very important thing: when joining two `DataFrame` instances, it''s
    imperative that they have the same `index`, otherwise `pandas` won''t be able
    to know which rows go with which. Therefore, when we create `campaign_df`, we
    set its `index` to the one from `df`. This enables us to join them. When creating
    this `DataFrame`, we also pass the column''s names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'And after `join`, we take a peek, hoping to see matching data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The truncated output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `join` was successful; the campaign name and the separate columns
    expose the same data. Did you see what we did there? We're accessing `DataFrame` using
    the square brackets syntax, and we pass a list of column names. This will produce
    a brand new `DataFrame`, with those columns (in the same order), on which we then
    call the `head()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Unpacking the user data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now do the exact same thing for each piece of `user` JSON data. We call
    `apply` on the `user` series, running the `unpack_user_json` function, which takes
    a JSON `user` object and transforms it into a list of its fields, which we can
    then inject into a brand new `DataFrame`, `user_df`. After that, we''ll join `user_df`
    back with `df`, like we did with `campaign_df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Very similar to the previous operation, isn''t it? We should also note here
    that, when creating `user_df`, we need to instruct `DataFrame` about the column
    names and the `index`. Let''s join and take a quick peek:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows us that everything went well. We''re good, but we''re not
    done yet. If you call `df.columns` in a cell, you''ll see that we still have ugly
    names for our columns. Let''s change that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Good! Now, with the exception of `'cmp_name'` and `'user'`, we only have nice
    names.
  prefs: []
  type: TYPE_NORMAL
- en: 'Completing the `datasetNext` step will be to add some extra columns. For each
    campaign, we have the numbers of clicks and impressions, and we have the amounts
    spent. This allows us to introduce three measurement ratios: **CTR**, **CPC**,
    and **CPI**. They stand for **Click Through Rate**, **Cost Per Click**, and **Cost
    Per Impression**, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last two are straightforward, but CTR is not. Suffice it to say that it
    is the ratio between clicks and impressions. It gives you a measure of how many
    clicks were performed on a campaign advertisement per impression—the higher this
    number, the more successful the advertisement is in attracting users to click
    on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: I wrote this as a function, but I could have just written the code in the cell.
    It's not important. What I want you to notice here is that we're adding those
    three columns with one line of code each, but `DataFrame` applies the operation
    automatically (the division, in this case) to each pair of cells from the appropriate
    columns. So, even if they are masked as three divisions, these are actually *5037
    * 3* divisions, because they are performed for each row. Pandas does a lot of
    work for us, and also does a very good job of hiding the complexity of it.
  prefs: []
  type: TYPE_NORMAL
- en: The function, `calculate_extra_columns`, takes `DataFrame`, and works directly
    on it. This mode of operation is called **in-place**. Do you remember how `list.sort()`
    was sorting the list? It is the same deal. You could also say that this function
    is not pure, which means it has side effects, as it modifies the mutable object
    it is passed as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can take a look at the results by filtering on the relevant columns and
    calling `head`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us that the calculations were performed correctly on each row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, I want to verify the accuracy of the results manually for the first row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This is exactly what we saw in the previous output. Of course, I wouldn't normally
    need to do this, but I wanted to show you how can you perform calculations this
    way. You can access `Series` (a column) by passing its name to `DataFrame`, in
    square brackets, and then you access each row by its position, exactly as you
    would with a regular list or tuple.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re almost done with our `DataFrame`. All we are missing now is a column
    that tells us the duration of the campaign and a column that tells us which `day`
    of the week corresponds to the start date of each campaign. This allows me to
    expand on how to play with `date` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We used two different techniques here but first, the code.
  prefs: []
  type: TYPE_NORMAL
- en: '`get_day_of_the_week` takes a `date` object. If you cannot understand what
    it does, please take a few moments to try to understand for yourself before reading
    the explanation. Use the inside-out technique like we''ve done a few times before.'
  prefs: []
  type: TYPE_NORMAL
- en: So, as I'm sure you know by now, if you put `calendar.day_name` in a `list`
    call, you get `['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',
    'Sunday']`. This means that, if we enumerate `calendar.day_name` starting from
    `1`, we get pairs such as `(1, 'Monday')`, `(2, 'Tuesday')`, and so on. If we
    feed these pairs to a dictionary, we get a mapping between the days of the week
    as numbers (1, 2, 3, ...) and their names. When the mapping is created, in order
    to get the name of a day, we just need to know its number. To get it, we call
    `date.isoweekday()`, which tells us which day of the week that date is (as a number).
    You feed that into the mapping and, boom! You have the name of the day.
  prefs: []
  type: TYPE_NORMAL
- en: '`get_duration` is interesting as well. First, notice it takes an entire row,
    not just a single value. What happens in its body is that we perform a subtraction
    between a campaign''s end and start dates. When you subtract `date` objects, the
    result is a `timedelta` object, which represents a given amount of time. We take
    the value of its `.days` property. It is as simple as that.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can introduce the fun part, the application of those two functions.
  prefs: []
  type: TYPE_NORMAL
- en: The first application is performed on a `Series` object, like we did before
    for `'user'` and `'cmp_name'`; there is nothing new here.
  prefs: []
  type: TYPE_NORMAL
- en: The second one is applied to the whole `DataFrame` and, in order to instruct
    `pandas` to perform that operation on the rows, we pass `axis=1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify the results very easily, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: So, we now know that between the 24^(th) of March, 2019 and the 6^(th) of November,
    2020 there are 593 days, and that the 24^(th) of March, 2019 is a Sunday.
  prefs: []
  type: TYPE_NORMAL
- en: If you're wondering what the purpose of this is, I'll provide an example. Imagine
    that you have a campaign that is tied to a sports event that usually takes place
    on a Sunday. You may want to inspect your data according to the days so that you
    can correlate them to the various measurements you have. We're not going to do
    it in this project, but it was useful to see, if only for the different way of
    calling `apply()` on `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning everything up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have everything we want, it''s time to do the final cleaning; remember
    we still have the `''cmp_name''` and `''user''` columns. Those are useless now,
    so they have to go. Also, I want to reorder the columns in `DataFrame` so that
    it is more relevant to the data it now contains. In order to do this, we just
    need to filter `df` on the column list we want. We''ll get back a brand new `DataFrame`
    that we can reassign to `df` itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: I have grouped the campaign information at the beginning, then the measurements,
    and finally the user data at the end. Now our `DataFrame` is clean and ready for
    us to inspect.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start going crazy with graphs, what about taking a snapshot of `DataFrame`
    so that we can easily reconstruct it from a file, rather than having to redo all
    the steps we did to get here. Some analysts may want to have it in spreadsheet
    form, to do a different kind of analysis than the one we want to do, so let's
    see how to save `DataFrame` to a file. It's easier done than said.
  prefs: []
  type: TYPE_NORMAL
- en: Saving the DataFrame to a file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can save `DataFrame` in many different ways. You can type `df.to_` and then
    press *Tab* to make autocompletion pop up, to see all the possible options.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to save `DataFrame` in three different formats, just for fun.
    First, CSV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, in an Excel spreadsheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The CSV file looks like this (output truncated):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'And the JSON one looks like this (again, output truncated):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'So, it''s extremely easy to save `DataFrame` in many different formats, and
    the good news is that the reverse is also true: it''s very easy to load a spreadsheet
    into `DataFrame`. The programmers behind `pandas` went a long way to ease our
    tasks, something to be grateful for.'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, the juicy bits. In this section, we're going to visualize some results.
    From a data science perspective, I'm not very interested in going deep into analysis,
    especially because the data is completely random, but still, this code will get
    you started with graphs and other features.
  prefs: []
  type: TYPE_NORMAL
- en: Something I learned in my life, and this may come as a surprise to you, is that—*looks
    also count*, so it's very important that when you present your results, you do
    your best to *make them pretty*.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we tell `pandas` to render graphs in the cell output frame, which is
    convenient. We do it with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we proceed with some styling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Its purpose is to make the graphs we will look at in this section a little bit
    prettier. You can also instruct the Notebook to do this when you start it from
    the console by passing a parameter, but I wanted to show you this way too since
    it can be annoying to have to restart the Notebook just because you want to plot
    something. In this way, you can do it on the fly and then keep working.
  prefs: []
  type: TYPE_NORMAL
- en: We also use `pylab` to set the `font.family` to `serif`. This might not be necessary
    on your system. Try to comment it out and execute the Notebook, and see whether
    anything changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that `DataFrame` is complete, let''s run `df.describe()` (`#26`) again.
    The results should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This kind of quick result is perfect for satisfying those managers who have
    20 seconds to dedicate to you and just want rough numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, please keep in mind that our campaigns have different currencies,
    so these numbers are actually meaningless. The point here is to demonstrate the
    `DataFrame` capabilities, not to get to a correct or detailed analysis of real
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, a graph is usually much better than a table with numbers because
    it''s much easier to read it and it gives you immediate feedback. So, let''s graph
    out the four pieces of information we have on each campaign—`''Budget''`, `''Spent''`,
    `''Clicks''`, and `''Impressions''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We extrapolate those four columns (this will give us another `DataFrame` made
    with only those columns) and call the histogram `hist()` method on it. We give
    some measurements on the bins and figure sizes, but basically, everything is done
    automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'One important thing: since this instruction is the only one in this cell (which
    also means, it''s the last one), the Notebook will print its result before drawing
    the graph. To suppress this behavior and have only the graph drawn with no printing,
    just add a semicolon at the end (you thought I was reminiscing about Java, didn''t
    you?). Here are the graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'They are beautiful, aren''t they? Did you notice the serif font? How about
    the meaning of those figures? If you go back and take a look at the way we generate
    the data, you will see that all these graphs make perfect sense:'
  prefs: []
  type: TYPE_NORMAL
- en: Budget is simply a random integer in an interval, therefore we were expecting
    a uniform distribution, and there we have it; it's practically a constant line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spent is a uniform distribution as well, but the high end of its interval is
    the budget, which is moving. This means we should expect something such as a quadratic
    hyperbole that decreases to the right. And there it is as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clicks was generated with a triangular distribution with a mean roughly 20%
    of the interval size, and you can see that the peak is right there, at about 20%
    to the left.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Impressions was a Gaussian distribution, which is the one that assumes the famous
    bell shape. The mean was exactly in the middle and we had a standard deviation
    of 2\. You can see that the graph matches those parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Good! Let''s plot out the measures we calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the plot representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the CPC is highly skewed to the left, meaning that most of the
    CPC values are very low. The CPI has a similar shape, but is less extreme.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, all this is nice, but if you wanted to analyze only a particular segment
    of the data, how would you do it? We can apply a mask to `DataFrame` so that we
    get another one with only the rows that satisfy the mask condition. It''s like
    applying a global, row-wise `if` clause:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: In this case, I prepared `mask` to filter out all the rows for which the amount
    spent is less than or equal to 75% of the budget. In other words, we'll include
    only those campaigns for which we have spent at least three-quarters of the budget.
    Notice that in `mask`, I am showing you an alternative way of asking for a `DataFrame`
    column, by using direct property access (`object.property_name`), instead of dictionary-like
    access (`object['property_name']`). If `property_name` is a valid Python name,
    you can use both ways interchangeably (JavaScript works like this as well).
  prefs: []
  type: TYPE_NORMAL
- en: '`mask` is applied in the same way that we access a dictionary with a key. When
    you apply `mask` to `DataFrame`, you get back another one and we select only the
    relevant columns on this and call `hist()` again. This time, just for fun, we
    want the results to be green:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the shapes of the graphs haven't changed much, apart from the Spent
    graph,  which is quite different. The reason for this is that we've asked only
    for the rows where the amount spent is at least 75% of the budget. This means
    that we're including only the rows where the amount spent is close to the budget.
    The budget numbers come from a uniform distribution. Therefore, it is quite obvious
    that the Spent graph is now assuming that kind of shape. If you make the boundary
    even tighter and ask for 85% or more, you'll see the Spent graph become more and
    more like the Budget one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now ask for something different. How about the measure of `''Spent''`,
    `''Clicks''`, and `''Impressions''` grouped by day of the week:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The first line creates a new `DataFrame`, `df_weekday`, by asking for a grouping
    by `'Day of Week'` on `df`. The function used to aggregate the data is an addition.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second line gets a slice of `df_weekday` using a list of column names,
    something we''re accustomed to by now. On the result, we call `plot()`, which
    is a bit different to `hist()`. The `subplots=True` option makes `plot` draw three
    independent graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Interestingly enough, we can see that most of the action happens on Sundays
    and Wednesdays. If this were meaningful data, this would potentially be important
    information to give to our clients, which is why I'm showing you this example.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the days are sorted alphabetically, which scrambles them up a bit.
    Can you think of a quick solution that would fix the issue? I'll leave it to you
    as an exercise to come up with something.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s finish this presentation section with a couple more things. First, a
    simple aggregation. We want to aggregate on `''Target Gender''` and `''Target
    Age''`, and show `''Impressions''` and `''Spent''`. For both, we want to see `''mean''`
    and the standard deviation (`''std''`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s very easy to do. We will prepare a dictionary that we''ll use as a configuration.
    Then, we perform a grouping on the `''Target Gender''` and `''Target Age''` columns,
    and we pass our configuration dictionary to the `agg()` method. The result is
    truncated and rearranged a little bit to make it fit, and shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: This is the textual representation, of course, but you can also have the HTML
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do one more thing before we wrap this chapter up. I want to show you
    something called a **pivot table**. It''s kind of a buzzword in the data environment,
    so an example such as this one, albeit very simple, is a must:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We create a pivot table that shows us the correlation between `'Target Age'`
    and `'Impressions'`, `'Clicks'`, and `'Spent'`. These last three will be subdivided
    according to `'Target Gender'`. The aggregation function (`aggfunc`) used to calculate
    the results is the `numpy.sum` function (`numpy.mean` would be the default, had
    I not specified anything).
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating the pivot table, we simply print it with the last line in the
    cell, and here''s a crop of the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It's pretty clear and provides very useful information when the data is meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: That's it! I'll leave you to discover more about the wonderful world of IPython,
    Jupyter, and data science. I strongly encourage you to get comfortable with the
    Notebook environment. It's much better than a console, it's extremely practical
    and fun to use, and you can even create slides and documents with it.
  prefs: []
  type: TYPE_NORMAL
- en: Where do we go from here?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data science is indeed a fascinating subject. As I said in the introduction,
    those who want to delve into its meanders need to be well-trained in mathematics
    and statistics. Working with data that has been interpolated incorrectly renders
    any result about it useless. The same goes for data that has been extrapolated
    incorrectly or sampled with the wrong frequency. To give you an example, imagine
    a population of individuals that are aligned in a queue. If for some reason, the
    gender of that population alternated between male and female, the queue would
    be something like this: F-M-F-M-F-M-F-M-F...'
  prefs: []
  type: TYPE_NORMAL
- en: If you sampled it taking only the even elements, you would draw the conclusion
    that the population was made up only of males, while sampling the odd ones would
    tell you exactly the opposite.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this was just a silly example, I know, but it's very easy to make
    mistakes in this field, especially when dealing with big data where sampling is
    mandatory and therefore, the quality of the introspection you make depends, first
    and foremost, on the quality of the sampling itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to data science and Python, these are the main tools you want
    to look at:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NumPy** ([http://www.numpy.org/](http://www.numpy.org/)): This is the main
    package for scientific computing with Python. It contains a powerful N-dimensional
    array object, sophisticated (broadcasting) functions, tools for integrating C/C++
    and Fortran code, useful linear algebra, the Fourier transform, random number
    capabilities, and much more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scikit-Learn** ([http://scikit-learn.org/](http://scikit-learn.org/)): This
    is probably the most popular machine learning library in Python. It has simple
    and efficient tools for data mining and data analysis, accessible to everybody,
    and reusable in various contexts. It''s built on NumPy, SciPy, and Matplotlib.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pandas** ([http://pandas.pydata.org/](http://pandas.pydata.org/)): This is
    an open source, BSD-licensed library providing high-performance, easy-to-use data
    structures, and data analysis tools. We''ve used it throughout this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IPython** ([http://ipython.org/](http://ipython.org/))/**Jupyter** ([http://jupyter.org/](http://jupyter.org/)):
    These provide a rich architecture for interactive computing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matplotlib** ([http://matplotlib.org/](http://matplotlib.org/)): This is
    a Python 2-D plotting library that produces publication-quality figures in a variety
    of hard-copy formats and interactive environments across platforms. Matplotlib
    can be used in Python scripts, the Python and IPython shell, Jupyter Notebook,
    web application servers, and four graphical user interface toolkits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Numba** ([http://numba.pydata.org/](http://numba.pydata.org/)): This gives
    you the power to speed up your applications with high-performance functions written
    directly in Python. With a few annotations, array-oriented and math-heavy Python
    code can be just-in-time compiled to native machine instructions, similar in performance
    to C, C++, and Fortran, without having to switch languages or Python interpreters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bokeh** ([http://bokeh.pydata.org/](https://bokeh.pydata.org/)): This is
    a Python-interactive visualization library that targets modern web browsers for
    presentation. Its goal is to provide elegant, concise construction of novel graphics
    in the style of D3.js, but also deliver this capability with high-performance
    interactivity over very large or streaming datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other than these single libraries, you can also find ecosystems, such as **SciPy**
    ([http://scipy.org/](http://scipy.org/)) and the aforementioned **Anaconda** ([https://anaconda.org/](https://anaconda.org/)),
    that bundle several different packages in order to give you something that just
    works in an "out-of-the-box" fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Installing all these tools and their several dependencies is hard on some systems,
    so I suggest that you try out ecosystems as well to see whether you are comfortable
    with them. It may be worth it.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about data science. Rather than attempting to explain
    anything about this extremely wide subject, we delved into a project. We familiarized
    ourselves with the Jupyter Notebook, and with different libraries, such as Pandas,
    Matplotlib, and NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, having to compress all this information into one single chapter means
    I could only touch briefly on the subjects I presented. I hope the project we've
    gone through together has been comprehensive enough to give you an idea of what
    could potentially be the workflow you might follow when working in this field.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is dedicated to web development. So, make sure you have a browser
    ready and let's go!
  prefs: []
  type: TYPE_NORMAL
