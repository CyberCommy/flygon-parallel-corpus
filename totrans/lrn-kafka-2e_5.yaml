- en: Chapter 5. Writing Consumers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consumers are the applications that consume the messages published by Kafka
    producers and process the data extracted from them. Like producers, consumers
    can also be different in nature, such as applications doing real-time or near
    real-time analysis, applications with NoSQL or data warehousing solutions, backend
    services, consumers for Hadoop, or other subscriber-based solutions. These consumers
    can also be implemented in different languages such as Java, C, and Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus on the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka Consumer API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java-based Kafka consumers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java-based Kafka consumers consuming partitioned messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the chapter, we will explore some of the important properties
    that can be set for a Kafka consumer. So, let's start.
  prefs: []
  type: TYPE_NORMAL
- en: '![Writing Consumers](img/3090OS_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram explains the high-level working of the Kafka consumer
    when consuming the messages. The consumer subscribes to the message consumption
    from a specific topic on the Kafka broker. The consumer then issues a fetch request
    to the lead broker to consume the message partition by specifying the message
    offset (the beginning position of the message offset). Therefore, the Kafka consumer
    works in the pull model and always pulls all available messages after its current
    position in the Kafka log (the Kafka internal data representation).
  prefs: []
  type: TYPE_NORMAL
- en: While subscribing, the consumer connects to any of the live nodes and requests
    metadata about the leaders for the partitions of a topic. This allows the consumer
    to communicate directly with the lead broker receiving the messages. Kafka topics
    are divided into a set of ordered partitions and each partition is consumed by
    one consumer only. Once a partition is consumed, the consumer changes the message
    offset to the next partition to be consumed. This represents the states about
    what has been consumed and also provides the flexibility of deliberately rewinding
    back to an old offset and re-consuming the partition. In the next few sections,
    we will discuss the API provided by Kafka for writing Java-based custom consumers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All the Kafka classes referred to in this book are actually written in Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka consumer APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kafka provides two types of API for Java consumers:'
  prefs: []
  type: TYPE_NORMAL
- en: High-level API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-level API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The high-level consumer API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The high-level consumer API is used when only data is needed and the handling
    of message offsets is not required. This API hides broker details from the consumer
    and allows effortless communication with the Kafka cluster by providing an abstraction
    over the low-level implementation. The high-level consumer stores the last offset
    (the position within the message partition where the consumer left off consuming
    the message), read from a specific partition in Zookeeper. This offset is stored
    based on the consumer group name provided to Kafka at the beginning of the process.
  prefs: []
  type: TYPE_NORMAL
- en: The consumer group name is unique and global across the Kafka cluster and any
    new consumers with an in-use consumer group name may cause ambiguous behavior
    in the system. When a new process is started with the existing consumer group
    name, Kafka triggers a rebalance between the new and existing process threads
    for the consumer group. After the rebalance, some messages that are intended for
    a new process may go to an old process, causing unexpected results. To avoid this
    ambiguous behavior, any existing consumers should be shut down before starting
    new consumers for an existing consumer group name.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the classes that are imported to write Java-based basic consumers
    using the high-level consumer API for a Kafka cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ConsumerConnector`: Kafka provides the `ConsumerConnector` interface (`interface
    ConsumerConnector`) that is further implemented by the `ZookeeperConsumerConnector`
    class (`kafka.javaapi.consumer.ZookeeperConsumerConnector`). This class is responsible
    for all the interaction a consumer has with ZooKeeper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the class diagram for the `ConsumerConnector` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The high-level consumer API](img/3090OS_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`KafkaStream`: Objects of the `kafka.consumer.KafkaStream` class are returned
    by the `createMessageStreams` call from the `ConsumerConnector` implementation.
    This list of the `KafkaStream` objects is returned for each topic, which can further
    create an iterator over messages in the stream. The following is the Scala-based
    class declaration:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, the parameters `K` and `V` specify the type for the partition key and
    message value, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In the create call from the `ConsumerConnector` class, clients can specify the
    number of desired streams, where each stream object is used for single-threaded
    processing. These stream objects may represent the merging of multiple unique
    partitions.
  prefs: []
  type: TYPE_NORMAL
- en: '`ConsumerConfig`: The `kafka.consumer.ConsumerConfig` class encapsulates the
    property values required for establishing the connection with ZooKeeper, such
    as ZooKeeper URL, ZooKeeper session timeout, and ZooKeeper sink time. It also
    contains the property values required by the consumer such as group ID and so
    on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A high-level API-based working consumer example is discussed after the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: The low-level consumer API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The high-level API does not allow consumers to control interactions with brokers.
    Also known as "simple consumer API", the low-level consumer API is stateless and
    provides fine grained control over the communication between Kafka broker and
    the consumer. It allows consumers to set the message offset with every request
    raised to the broker and maintains the metadata at the consumer's end. This API
    can be used by both online as well as offline consumers such as Hadoop. These
    types of consumers can also perform multiple reads for the same message or manage
    transactions to ensure the message is consumed only once.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the high-level consumer API, developers need to put in extra effort
    to gain low-level control within consumers by keeping track of offsets, figuring
    out the lead broker for the topic and partition, handling lead broker changes,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the low-level consumer API, consumers first query the live broker to find
    out the details about the lead broker. Information about the live broker can be
    passed on to the consumers either using a properties file or from the command
    line. The `topicsMetadata()` method of the `kafka.javaapi.TopicMetadataResponse`
    class is used to find out metadata about the topic of interest from the lead broker.
    For message partition reading, the `kafka.api.OffsetRequest` class defines two
    constants: `EarliestTime` and `LatestTime`, to find the beginning of the data
    in the logs and the new messages stream. These constants also help consumers to
    track which messages are already read.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main class used within the low-level consumer API is the `SimpleConsumer`
    (`kafka.javaapi.consumer.SimpleConsumer`) class. The following is the class diagram
    for the `SimpleConsumer` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The low-level consumer API](img/3090OS_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A simple consumer class provides a connection to the lead broker for fetching
    the messages from the topic and methods to get the topic metadata and the list
    of offsets.
  prefs: []
  type: TYPE_NORMAL
- en: A few more important classes for building different request objects are `FetchRequest`
    (`kafka.api.FetchRequest`), `OffsetRequest` (`kafka.javaapi.OffsetRequest`), `OffsetFetchRequest`
    (`kafka.javaapi.OffsetFetchRequest`), `OffsetCommitRequest` (`kafka.javaapi.OffsetCommitRequest`),
    and `TopicMetadataRequest` (`kafka.javaapi.TopicMetadataRequest`).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All the examples in this chapter are based on the high-level consumer API. For
    examples based on the low-level consumer API, refer to [https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example](https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example).
  prefs: []
  type: TYPE_NORMAL
- en: Simple Java consumers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will start writing a single-threaded simple Java consumer developed using
    the high-level consumer API for consuming the messages from a topic. This `SimpleHLConsumer`
    class is used to fetch a message from a specific topic and consume it, assuming
    that there is a single partition within the topic.
  prefs: []
  type: TYPE_NORMAL
- en: Importing classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a first step, we need to import the following classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Defining properties
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a next step, we need to define properties for making a connection with Zookeeper
    and pass these properties to the Kafka consumer using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us see the major properties mentioned in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`zookeeper.connect`: This property specifies the ZooKeeper `<node:port>` connection
    detail that is used to find the Zookeeper running instance in the cluster. In
    the Kafka cluster, Zookeeper is used to store offsets of messages consumed for
    a specific topic and partition by this consumer group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`group.id`: This property specifies the name for the consumer group shared
    by all the consumers within the group. This is also the process name used by Zookeeper
    to store offsets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zookeeper.session.timeout.ms`: This property specifies the Zookeeper session
    timeout in milliseconds and represents the amount of time Kafka will wait for
    Zookeeper to respond to a request before giving up and continuing to consume messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zookeeper.sync.time.ms`: This property specifies the ZooKeeper sync time in
    milliseconds between the ZooKeeper leader and the followers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`auto.commit.interval.ms`: This property defines the frequency in milliseconds
    at which consumer offsets get committed to Zookeeper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading messages from a topic and printing them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a final step, we need to read the message using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'So the complete program will look like the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Before running this, make sure you have created the topic `kafkatopic` from
    the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before compiling and running a Java-based Kafka program in the console, make
    sure you download the `slf4j-1.7.7.tar.gz` file from [http://www.slf4j.org/download.html](http://www.slf4j.org/download.html)
    and copy `slf4j-log4j12-1.7.7.jar` contained within `slf4j-1.7.7.tar.gz` to the
    `/opt/kafka_2.9.2-0.8.1.1/libs` directory. Also add all the libraries available
    in `/opt/kafka_2.9.2-0.8.1.1/libs` to the classpath using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Also run the `SimpleProducer` class developed in [Chapter 4](ch04.html "Chapter 4. Writing
    Producers"), *Writing Producers*, which takes two arguments: first, the topic
    name and second, the number of messages to be published as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the preceding `SimpleHLConsumer` class using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the simple high-level consumer using the following command in a separate
    console window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'For successful execution, the `SimpleHLConsumer` class takes three arguments:
    first, the Zookeeper connection string `<host:port>`; second, the unique group
    ID; and third, the Kafka topic name.'
  prefs: []
  type: TYPE_NORMAL
- en: Multithreaded Java consumers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous example is a very basic example of a consumer that consumes messages
    from a single broker with no explicit partitioning of messages within the topic.
    Let's jump to the next level and write another program that consumes messages
    from multiple partitions connecting to single/multiple topics.
  prefs: []
  type: TYPE_NORMAL
- en: A multithreaded, high-level, consumer-API-based design is usually based on the
    number of partitions in the topic and follows a one-to-one mapping approach between
    the thread and the partitions within the topic. For example, if four partitions
    are defined for any topic, as a best practice, only four threads should be initiated
    with the consumer application to read the data; otherwise, some conflicting behavior,
    such as threads never receiving a message or a thread receiving messages from
    multiple partitions, may occur. Also, receiving multiple messages will not guarantee
    that the messages will be placed in order. For example, a thread may receive two
    messages from the first partition and three from the second partition, then three
    more from the first partition, followed by some more from the first partition,
    even if the second partition has data available.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move further on.
  prefs: []
  type: TYPE_NORMAL
- en: Importing classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a first step, we need to import the following classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Defining properties
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the next step, we need to define properties for making a connection with
    Zookeeper and pass these properties to the Kafka consumer using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The preceding properties have already been discussed in the previous example.
    For more details on Kafka consumer properties, refer to the last section of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the message from threads and printing it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The only difference in this section from the previous section is that we first
    create a thread pool and get the Kafka streams associated with each thread within
    the thread pool, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete program listing for the multithread Kafka consumer based on the
    Kafka high-level consumer API is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Compile the preceding program, and before running it, read the following tip.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we run this program, we need to make sure our cluster is running as a
    multi-broker cluster (comprising either single or multiple nodes). For more information
    on how to set up single node—multiple broker cluster, refer to [Chapter 2](ch02.html
    "Chapter 2. Setting Up a Kafka Cluster"), *Setting Up a Kafka Cluster*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once your multi-broker cluster is up, create a topic with four partitions and
    set the replication factor to `2` before running this program using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, run the `SimpleProducer` class developed in [Chapter 4](ch04.html "Chapter 4. Writing
    Producers"), *Writing Producers*, which takes two arguments: first, the topic
    name and second, the number of messages to be published, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the preceding `MultiThreadHLConsumer` class using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now run the multithreaded high-level consumer using the following command in
    a separate console window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'For successful execution, the `SimpleHLConsumer` class takes four arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: The Zookeeper connection string `<host:port>`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The unique group ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kafka topic name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The thread count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This program will print all partitions of messages associated with each thread.
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka consumer property list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following lists of a few important properties that can be configured for
    high-level, consumer-API-based Kafka consumers. The Scala class `kafka.consumer.ConsumerConfig`
    provides implementation-level details for consumer configurations. For a complete
    list, visit [http://kafka.apache.org/documentation.html#consumerconfigs](http://kafka.apache.org/documentation.html#consumerconfigs).
  prefs: []
  type: TYPE_NORMAL
- en: '| Property name | Description | Default value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `group.id` | This property defines a unique identity for the set of consumers
    within the same consumer group. |   |'
  prefs: []
  type: TYPE_TB
- en: '| `consumer.id` | This property is specified for the Kafka consumer and generated
    automatically if not defined. | `null` |'
  prefs: []
  type: TYPE_TB
- en: '| `zookeeper.connect` | This property specifies the Zookeeper connection string,
    `< hostname:port/chroot/path>`. Kafka uses Zookeeper to store offsets of messages
    consumed for a specific topic and partition by the consumer group. `/chroot/path`
    defines the data location in a global zookeeper namespace. |   |'
  prefs: []
  type: TYPE_TB
- en: '| `client.id` | The `client.id` value is specified by the Kafka client with
    each request and is used to identify the client making the requests. | `${group.id}`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `zookeeper.session.timeout.ms` | This property defines the time (in milliseconds)
    for a Kafka consumer to wait for a Zookeeper pulse before it is declared dead
    and rebalance is initiated. | `6000` |'
  prefs: []
  type: TYPE_TB
- en: '| `zookeeper.connection.timeout.ms` | This value defines the maximum waiting
    time (in milliseconds) for the client to establish a connection with ZooKeeper.
    | `6000` |'
  prefs: []
  type: TYPE_TB
- en: '| `zookeeper.sync.time.ms` | This property defines the time it takes to sync
    a Zookeeper follower with the Zookeeper leader (in milliseconds). | `2000` |'
  prefs: []
  type: TYPE_TB
- en: '| `auto.commit.enable` | This property enables a periodical commit of message
    offsets to the Zookeeper that are already fetched by the consumer. In the event
    of consumer failures, these committed offsets are used as a starting position
    by the new consumers. | `true` |'
  prefs: []
  type: TYPE_TB
- en: '| `auto.commit.interval.ms` | This property defines the frequency (in milliseconds)
    for the consumed offsets to get committed to ZooKeeper. | `60 * 1000` |'
  prefs: []
  type: TYPE_TB
- en: '| `auto.offset.reset` | This property defines the offset value if an initial
    offset is available in Zookeeper or the offset is out of range. Possible values
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`largest`: reset to largest offset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`smallest`: reset to smallest offset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'anything else: throw an exception'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| `largest` |'
  prefs: []
  type: TYPE_TB
- en: '| `consumer.timeout.ms` | This property throws an exception to the consumer
    if no message is available for consumption after the specified interval. | `-1`
    |'
  prefs: []
  type: TYPE_TB
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to write basic consumers and learned about
    some advanced levels of Java consumers that consume messages from partitions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to integrate Kafka with Storm and Hadoop.
  prefs: []
  type: TYPE_NORMAL
