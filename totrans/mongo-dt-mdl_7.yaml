- en: Chapter 7. Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scalability has been a much-discussed subject over the years. Even though many
    things have already been said about it, this topic is very important and here,
    in this book, it will surely find its place too.
  prefs: []
  type: TYPE_NORMAL
- en: It is not in our interest to deal with all the concepts that involve database
    scalability, especially in NoSQL databases, but to show the possibilities that
    MongoDB offers when working with scalability in our collections and also how the
    flexibility of MongoDB's data model can influence our choices.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to horizontally scale MongoDB based on a simple infrastructure
    and low-cost sharding requests. Sharding is the technique of distributing data
    through multiple physical partitions called **shards**. Even though the database
    is physically partitioned, to our clients the database itself is a single instance.
    The technique of sharding is completely transparent for the database's clients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dear reader, get ready! In this chapter, you will see some crucial topics for
    database maintenance, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling out with sharding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the shard key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling a social inbox schema design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling out MongoDB with sharding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we talk about database scalability, we have two reference methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scale up or vertical scale**: In this method, we add more resources to a
    machine. For example, a CPU, disk, and memory in order to increase the system''s
    capacity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scale out or horizontal scale**: In this method, we add more nodes to the
    systems and distribute the work among the available nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice between one or the other does not depend on our desire. It depends
    on the system that we want to scale. It is necessary to know whether it is possible
    to scale that system in the way that we want to. We must also keep in mind that
    there is a difference and trade-off between the two techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the storage capacity, CPU, or memory can be very expensive and sometimes
    impossible due to our service provider's limitations. On the other hand, increasing
    the number of nodes in a system can also increase complexity both conceptually
    and operationally.
  prefs: []
  type: TYPE_NORMAL
- en: However, considering the advances in virtualization technology and the facilities
    offered by cloud providers, scaling horizontally is becoming the more practical
    solution for some applications.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB is prepared to scale horizontally. This is done with the help of a technique
    of sharding. This technique consists of partitioning our data set and distributing
    the data among many servers. The main purpose of sharding is to support bigger
    databases that are able to deal with a high-throughput operation by distributing
    the operation's load between each shard.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have a 1-terabyte database and four configured shards, each
    shard should have 256 GB of data. But, this does not mean that each shard will
    manage 25 percent of throughput operation. This will only depend on the way that
    we decided to construct our shard. This is a big challenge and the main target
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates how a shard works on MongoDB:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling out MongoDB with sharding](img/B04075_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'At the time that this book was written, MongoDB, in its 3.0 version, offers
    multiple sharding policies: **range-based**, **hash-based**, and **location-based**
    sharding.'
  prefs: []
  type: TYPE_NORMAL
- en: In the range-based policy, MongoDB will partition the data based on the value
    for the shard key. The documents that the shard key values close to each other
    will be allocated in the same shard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the hash-based policy, the documents are distributed considering the MD5
    value for the shard key.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the location-based policy, the documents will be distributed in shards based
    on a configuration that will associate shard range values with a specific shard.
    This configuration uses tags to do this, which is very similar to what you saw
    in [Chapter 6](ch06.html "Chapter 6. Managing the Data"), *Managing the Data*,
    where we discussed operation segregation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharding works in MongoDB at the collections level, which means we can have
    collections with sharding and without sharding enabled in the same database. To
    set sharding in a collection, we must configure a sharded cluster. The elements
    for a sharded cluster are shards, query routers, and configuration servers:'
  prefs: []
  type: TYPE_NORMAL
- en: A **shard** is where a part of our data set will be allocated. A shard can be
    a MongoDB instance or a replica set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **query router** is the interface offered for the database clients that
    will be responsible for directing the operations to the correct shard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **config server** is a MongoDB instance that is responsible for keeping
    the sharded cluster configurations or, in other words, the cluster's metadata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows a shared cluster and its components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling out MongoDB with sharding](img/B04075_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will not go any deeper into the creation and maintenance of a sharded cluster,
    as this is not our objective in this chapter. However, it is important to know
    that the sharded cluster's setup depends on the scenario.
  prefs: []
  type: TYPE_NORMAL
- en: In a production environment, the minimum recommended setup is at least three
    configuration servers, two or more replica sets, which will be our shards, and
    one or more query routers. By doing this, we can ensure the minimum redundancy
    and high availability for our environment.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the shard key
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we've decided that we have the need for a sharded cluster, the next step
    is to choose the shard key. The shard key is responsible for determining the distribution
    of documents among the cluster's shards. These will also be a key factor in determining
    the success or the failure of our database.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each write operation, MongoDB will allocate a new document based on the
    range value for the shard key. A shard key''s range is also known as a **chunk**.
    A chunk has a default length of 64 MB, but if you want this value to be customized
    to your need, it can be configured. In the following diagram, you can see how
    documents are distributed on chunks given an numeric shard key from infinity negative
    to infinity positive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Choosing the shard key](img/B04075_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Before starting a discussion about the things that can affect our shard key's
    construction, there are some limitations in MongoDB that must be respected. These
    limitations are significant and, in some ways, they help us to eliminate the possibilities
    of some errors in our choices.
  prefs: []
  type: TYPE_NORMAL
- en: A shard key cannot exceed a length of 512 bytes. A shard key is an indexed field
    in the document. This index can be a simple field or a composed field, but it
    will never be a multikey field. It is also possible to use indexes of simple hash
    fields since the 2.4 version of MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: The following information must be read quietly, like a mantra, so you will not
    make any mistakes from the very beginning.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You have to keep one thing in your mind: the shard key is unchangeable.'
  prefs: []
  type: TYPE_NORMAL
- en: To repeat, the shard key is unchangeable. That means, dear reader, that once
    a shard key is created, you can never change it. Never!
  prefs: []
  type: TYPE_NORMAL
- en: You can find detailed information about MongoDB sharded cluster limitations
    in the MongoDB manual reference at [http://docs.mongodb.org/manual/reference/limits/#sharded-clusters](http://docs.mongodb.org/manual/reference/limits/#sharded-clusters).
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if I created a shard key and I want to change it? What should I do?
    Instead of trying to change it, we should do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Execute a dump of the database in a disk file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop the collection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure a new collection using the new shard key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute a pre-split of the chunks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recover the dump file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you can see, we do not change the shard key. We recreated almost everything
    from scratch. Therefore, be careful when executing the command for shard key's
    creation or you will get a headache if you need to change it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next piece of information you need to remember is that you cannot update
    the value of one or more fields that are a part of the shard key. In other words,
    the value for a shard key is also unchangeable.
  prefs: []
  type: TYPE_NORMAL
- en: There is no use in trying to execute the `update()` method in a field that is
    part of a shard key. It will not work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed, let''s see in practice what we discussed until this point.
    Let''s create a sharded cluster for testing. The following shard configuration
    is very useful for testing and developing. Never use this configuration in a production
    environment. The commands given will create:'
  prefs: []
  type: TYPE_NORMAL
- en: Two shards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One configuration server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One query router
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As a first step, let''s start a configuration server instance. The configuration
    server is nothing more than a `mongod` instance with the initialization parameter
    `--configsvr.` If we do not set a value for the parameter `--port <port number>`,
    it will start on port 27019 by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to start the query router. The query router is a `mongos`
    MongoDB instance, which route queries and write operations to shards, using the
    parameter `--configdb <configdb hostname or ip:port>`, which indicates the configuration
    server. By default, MongoDB starts it on port 27017:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s start the shards. The shards in this example will be just two
    simple instances of `mongod`. Similar to `mongos`, a `mongod` instance starts
    on port 27017 by default. As we already started the `mongos` instance on this
    port, let''s set a different port for the `mongod` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Done! Now we have the basic infrastructure for our test sharded cluster. But,
    wait! We do not have a sharded cluster yet. The next step is to add the shards
    to the cluster. To do this, we must connect the `mongos` instance that we already
    started to the query router:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once on the `mongos` shell, we have to execute the `addShard` method in the
    following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to check the result of the preceding operations, we can execute
    the `status()`command and see some information about the created shard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the returned document, we can only see basic information, such as which the
    hosts are for our sharded cluster and the databases that we have. For now, we
    do not have any collection using the sharding enabled. For that reason, the information
    is greatly simplified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the shards, the configuration server, and the query router,
    let''s enable sharding in the database. It is necessary first to enable sharding
    in a database before doing the same for a collection. The following command enables
    sharding in a database called `ecommerce`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'By consulting the sharded cluster''s status, we can notice that we have information
    about our `ecommerce` database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider that in the `ecommerce` database, we have a `customers` collection
    with the following documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We must execute the `shardCollection` command to enable sharding in this collection,
    using the collection name and a document that will represent our shard key as
    a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s enable the shard in the `customers` collection by executing the following
    command in the `mongos` shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, something went wrong during the command''s execution. MongoDB
    is warning us that we must have an index and the shard key must be a prefix. So,
    we must execute the following sequence on the `mongos` shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Well done! Now we have the `customers` collection of the `ecommerce` database
    with the shard enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are sharding an empty collection, the `shardCollection` command will
    create the index of the shard key.
  prefs: []
  type: TYPE_NORMAL
- en: But what was the factor that determined the choice of `address.zip` and `registered`
    as the shard key? In this case, as I said before, I chose a random field just
    to illustrate. From now on, let's think about what factors can establish the creation
    of a good shard key.
  prefs: []
  type: TYPE_NORMAL
- en: Basic concerns when choosing a shard key
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The choice of which shard key is not an easy task and there is no recipe for
    it. Most of the time, knowing our domain and its use in advance is fundamental.
    It is essential to be very careful when doing this. A not-so-appropriate shard
    key can bring us a series of problems in our database and consequently affect
    its performance.
  prefs: []
  type: TYPE_NORMAL
- en: First of all is divisibility. We must think of a shard key that allows us to
    visualize the documents' division among the shards. A shard key with a limited
    number of values may result in "unsplittable" chunks.
  prefs: []
  type: TYPE_NORMAL
- en: We can state that this field must have a high cardinality, such as fields with
    a high variety of values and also unique fields. Identifications fields such as
    e-mail addresses, usernames, phone numbers, social security numbers, and zip codes
    are a good example of fields with high cardinality.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, each one of them can be unique if we take into account a certain situation.
    In an ecommerce system, if we have a document that is related to shipment, we
    will have more than one document with the same zip code. But, consider another
    example, a catalogue system for beauty salons in a city. Then, if a document represents
    a beauty salon, the zip code will be a more unique number than it was in the previous
    example.
  prefs: []
  type: TYPE_NORMAL
- en: The third is maybe the most polemical point until now because it contradicts
    the last one in a certain way. We have seen that a shard key with a high randomness
    degree is good practice in trying to increase the performance in write operations.
    Now, we will consider a shard key's creation to target a single shard. When we
    think about performance on read operations, it is a good idea to read from a single
    shard. As you already know, in a sharded cluster, the database complexity is abstracted
    on the query router. In other words, it is **mongos'** responsibility to discover
    which shards it should search for the information requested in a query. If our
    shard key is distributed across multiple shards, then `mongos` will search for
    the information on the shards, collect and merge them all, and then deliver it.
    But, if the shard key was planned to target a single shard, then the mongos task
    will search for the information in this unique shard and, in sequence, deliver
    it.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth and last point is about cases when we do not have any field in the
    document that would be a good choice for our shard key. In this situation, we
    must think about a composed shard key. In the previous example, we use a composed
    shard key with the fields `address.zip` and `registered`. A composed shard key
    will also help us to have a more divisible key due the fact that if the first
    value from the shard key does not have a high cardinality, adding a second value
    will increase the cardinality.
  prefs: []
  type: TYPE_NORMAL
- en: So, these basic concerns show us that depending on what we want to search for,
    we should choose different approaches for the shard key's document. If we need
    query insulation, then a shard key that can focus on one shard is a good choice.
    But, when we need to escalate the write operation, the more random our shard key,
    the better it will be for performance.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling a social inbox schema design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On October 31, 2014, MongoDB Inc. introduced on their community blog three different
    approaches to solve a very common problem, social inboxes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to see the blog post, refer to [http://blog.mongodb.org/post/65612078649/schema-design-for-social-inboxes-in-mongodb](http://blog.mongodb.org/post/65612078649/schema-design-for-social-inboxes-in-mongodb).
  prefs: []
  type: TYPE_NORMAL
- en: From the three presented schema designs, it is possible to see the application
    of all the scaling concepts we have seen until now in an easy and efficient way.
    In all of the cases, the concept of a fan out applies, in which the workload is
    distributed among the shards in parallel. Each approach has its own application
    according to the needs of the database client.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three schema designs are:'
  prefs: []
  type: TYPE_NORMAL
- en: Fan out on read
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan out on write
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan out on write with buckets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan out on read
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fan out on read design bears this name due to the query router's behavior
    when a client reads an inbox. It is considered to be the design with the simplest
    mechanics compared to the others. It is also the easiest to implement.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the fan out on read design, we will have one `inbox` collection, where we
    will insert every new message. The document that will reside on this collection
    has four fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from`: A string that represents the message sender'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`to`: An array with all message recipients'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sent`: A date field that represents when the message was sent to the recipients'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`message`: A string field that represents the message itself'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following document, we can see an example of a message sent from John
    to Mike and Billie:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The operations on this collection will be the most straightforward of all. To
    send a message is to make an insert operation in the `inbox` collection, while
    to read is to find all the messages with a specific recipient.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to be done is to enable sharding on the database. Our `inbox`
    collection is in a database called `social`. To do this, and all other things
    that we will do in this chapter, we will use the `mongos` shell. So, let''s start
    out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will have to create the collection''s shard key. To implement this
    design, we will create a shard key using the `from` field of the `inbox` collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In case our collection already has documents, we should create an index for
    the shard key field.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step is to create a compound index on the `to` and `sent` fields,
    seeking a better performance on read operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to send and read messages in our `inbox` collection. On the
    `mongos` shell, let''s create a message and send it to the recipients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to read Mike''s inbox, we should use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The write operation in this design may be considered as efficient. Depending
    on the number of active users, we will have an even distribution of data across
    the shards.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, viewing an inbox is not so efficient. Every inbox read issues
    a `find` operation using the `to` field sorted by the `sent` field. Because our
    collection has the `from` field as a shard key, which means that the messages
    are grouped by sender on the shards, every query that does not use the shard key
    will be routed to all shards.
  prefs: []
  type: TYPE_NORMAL
- en: This design applies well if our application is targeted to sending messages.
    As we need a social application in which you send and read messages, let's take
    a look at the next design approach, fan out on write.
  prefs: []
  type: TYPE_NORMAL
- en: Fan out on write
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the fan out on write design, we can say that we will have an opposite effect
    compared to the previous one. While in fan out on read, we reached every shard
    on the cluster to view an inbox, in fan out on write, we will have the write operations
    distributed between all the shards.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement fan out on write instead of sharding on the sender, we will shard
    on the recipient of the message. The following command creates the shard key in
    the `inbox` collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the same document we used on the fan out on read design. So, to
    send a message from John to Mike and Billie, we will execute the following commands
    in the `mongos` shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To better understand what is happening, let''s do a little code breakdown:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we should do is to create a `msg` variable and store a
    message in JSON there:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To send a message to every recipient, we must iterate over the value in the
    `to` field, create a new field on the message JSON, `msg.recipient`, and store
    the message''s recipient:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we insert the message in the `inbox` collection:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'For every recipient of the message, we will insert a new document in the `inbox`
    collection. The following command, executed on the `mongos` shell, shows Mike''s
    inbox:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As the message has both Mike and Billie as recipients, we can also read Billie''s
    inbox:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: By doing so, when we are reading a user's inbox, we are targeting a single shard,
    since we are using the shard key as criteria for the find query.
  prefs: []
  type: TYPE_NORMAL
- en: But, even though we reach only one shard to view an inbox, we will have many
    random reads when the number of users grows. To deal with this problem, we are
    going to meet the concept of bucketing.
  prefs: []
  type: TYPE_NORMAL
- en: Fan out on write with buckets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fan out on write design is a very interesting approach to the social inboxes
    problem. Every time we need to, we could add more shards to our cluster, and the
    inboxes data will be evenly distributed between then. However, as we stated before,
    the random reads we made as our database grows are a bottleneck we must deal with.
    Although we target a single shard on a read operation by using the shard key as
    criteria for our find query, we will always have a random read on viewing an inbox.
    Suppose we have an average of 50 messages by each user, then for each inbox view
    it will produce 50 random reads. So, when we multiply these random reads by users
    simultaneously accessing their inboxes, we can imagine how fast we will saturate
    our database.
  prefs: []
  type: TYPE_NORMAL
- en: In an attempt to reduce this bottleneck, the fan out on write with buckets approach
    emerges. Fan out with buckets is a refined fan out on write, by bucketing messages
    together in documents of messages sorted by time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of this design is quite different compared to the previous
    ones. In fan out on write with buckets, we will have two collections:'
  prefs: []
  type: TYPE_NORMAL
- en: One `users` collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One `inbox` collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `users` collection will have documents with user data. In this document
    besides the basic user information, we also have a field that stores the total
    number of inbox messages that the user has.
  prefs: []
  type: TYPE_NORMAL
- en: The `inbox` collection will store documents with a set of user messages. We
    will have an `owner` field that will identify the user in this collection and
    a `sequence` field that identifies the bucket. These are the fields that we will
    shard the `inbox` collection with.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, each bucket will have 50 messages. The following commands will
    enable sharding on the social database and create the shard key in the `inbox`
    collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As it was previously mentioned, we also have a `users` collection. The following
    command creates a shard key in the `user` collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have created our shard keys, let''s send a message from John to
    Mike and Billie. The message document will be very similar to the previous one.
    The difference between them is the `owner` and `sequence` fields. The following
    code, executed on the `mongos` shell, will send a message from John to Mike and
    Billie:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As we did before, to understand to send the message, let''s do a code breakdown:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we create a `msg` variable and store message JSON there
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We iterate over the recipients in the `to` field, and execute a `findAndModify`
    method, where we look for a document in the `users` collection and who the owner
    of the message recipient is. As we use the `upsert` option with the value `true`,
    if we did not find the user, then we create a new one. The `update` field has
    a `$inc` operator, which means that we will increment one to the `msg_count` field.
    The method also uses a `new` option with the value `true`, and we will have executed
    the saved document as a result of this command.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the returned document, we get the value of the `msg_count` field, which
    represents the total messages to the user, and store the value on a `count` variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To discover the bucket where the message will be saved, we will use the function
    `floor` of the `Math` class that is available on the `mongos` shell. As we said
    before, we will have 50 messages in each bucket, so we will divide the value of
    the `count` variable by 50, and get the `floor` function of the result. So, for
    example, if we are sending a third user message, then the bucket to save this
    message is the result of `Math.floor(3/50)`, which is 0\. When we reach the 50th
    message, the bucket value becomes 1, which means that the next message will be
    in a new bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will update the document in the `inbox` collection that has the `owner` and
    `sequence` value we calculated. As we use the `upsert` option with the value `true`
    on the `update` command, it will create the document if it does not exist.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It this way, we will guarantee that a user's inbox is entirely on a single shard.
    In contrast to fan on write, where we have many random reads when we view a inbox,
    in fan out on write with buckets, we do one document read for every 50 user messages.
  prefs: []
  type: TYPE_NORMAL
- en: Fan out on write with buckets is without a doubt the best option to a social
    inbox schema design, when our requirements are to send and read messages efficiently.
    However, the document size of the `inbox` collection can become a problem. Depending
    on the messages' sizes, we will have to be careful with our storage.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Schema design is a better scalability strategy. No matter how many techniques
    and tools we have at our disposal, to know how our data will be used and dedicate
    time to our design is the cheaper and long-lasting approach to use.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will use everything you've learned until now and create
    a schema design from scratch for a real-life example.
  prefs: []
  type: TYPE_NORMAL
