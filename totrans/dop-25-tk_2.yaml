- en: Auto-scaling Nodes of a Kubernetes Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: May I say that I have not thoroughly enjoyed serving with humans? I find their
    illogic and foolish emotions a constant irritant.
  prefs: []
  type: TYPE_NORMAL
- en: '- *Spock*'
  prefs: []
  type: TYPE_NORMAL
- en: Usage of **HorizontalPodAutoscaler** (**HPA**) is one of the most critical aspects
    of making a resilient, fault-tolerant, and highly-available system. However, it
    is of no use if there are no nodes with available resources. When Kubernetes cannot
    schedule new Pods because there's not enough available memory or CPU, new Pods
    will be unschedulable and in the pending status. If we do not increase the capacity
    of our cluster, pending Pods might stay in that state indefinitely. To make things
    more complicated, Kubernetes might start removing other Pods to make room for
    those that are in the pending state. That, as you might have guessed, might lead
    to worse problems than the issue of our applications not having enough replicas
    to serve the demand.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes solves the problem of scaling nodes through Cluster Autoscaler.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Autoscaler has a single purpose to adjust the size of the cluster by
    adding or removing worker nodes. It adds new nodes when Pods cannot be scheduled
    due to insufficient resources. Similarly, it eliminates nodes when they are underutilized
    for a period of time and when Pods running on one such node can be rescheduled
    somewhere else.
  prefs: []
  type: TYPE_NORMAL
- en: The logic behind Cluster Autoscaler is simple to grasp. We are yet to see whether
    it is simple to use as well.
  prefs: []
  type: TYPE_NORMAL
- en: Let's create a cluster (unless you already have one) and prepare it for autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll continue using definitions from the `vfarcic/k8s-specs` ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    repository. To be on the safe side, we'll pull the latest version first.
  prefs: []
  type: TYPE_NORMAL
- en: All the commands from this chapter are available in the `02-ca.sh` ([https://gist.github.com/vfarcic/a6b2a5132aad6ca05b8ff5033c61a88f](https://gist.github.com/vfarcic/a6b2a5132aad6ca05b8ff5033c61a88f))
    Gist.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need a cluster. Please use the Gists below as inspiration to create
    a new cluster or to validate that the one you already fulfills all the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: A note to AKS users At the time of this writing (October 2018), Cluster Autoscaler
    does not (always) work in **Azure Kubernetes Service** (**AKS**). Please jump
    to *Setting up Cluster Autoscaler in AKS* section for more info and the link to
    instructions how to set it up.
  prefs: []
  type: TYPE_NORMAL
- en: '`gke-scale.sh`: **GKE** with 3 n1-standard-1 worker nodes, with **tiller**,
    and with the `--enable-autoscaling` argument ([https://gist.github.com/vfarcic/9c777487f7ebee6c09027d3a1df8663c](https://gist.github.com/vfarcic/9c777487f7ebee6c09027d3a1df8663c)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eks-ca.sh`: **EKS** with 3 t2.small worker nodes, with **tiller**, and with
    **Metrics Server** ([https://gist.github.com/vfarcic/3dfc71dc687de3ed98e8f804d7abba0b](https://gist.github.com/vfarcic/3dfc71dc687de3ed98e8f804d7abba0b)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aks-scale.sh`: **AKS** with 3 Standard_B2s worker nodes and with **tiller**
    ([https://gist.github.com/vfarcic/f1b05d33cc8a98e4ceab3d3770c2fe0b](https://gist.github.com/vfarcic/f1b05d33cc8a98e4ceab3d3770c2fe0b)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When examining the Gists, you'll notice a few things. First of all, Docker for
    Desktop and minikube are not there. Both are single-node clusters that cannot
    be scaled. We need to run a cluster in a place where we can add and remove the
    nodes on demand. We'll have to use one of the cloud vendors (for example, AWS,
    Azure, GCP). That does not mean that we cannot scale on-prem clusters.
  prefs: []
  type: TYPE_NORMAL
- en: We can, but that depends on the vendor we're using. Some do have a solution,
    while others don't.Â For simplicity, we'll stick with one of the big three. Please
    choose between **Google Kuberentes Engine** (**GKE**), Amazon **Elastic Container
    Service** for Kubernetes (**EKS**), or **Azure Kubernetes Service** (**AKS**).
    If you're not sure which one to pick, I suggest GKE, since it's the most stable
    and feature-rich managed Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: You'll also notice that GKE and AKS Gists are the same as in the previous chapter,
    while EKS changed. As you already know, the former already have the Metrics Server
    baked in. EKS doesn't, so I copied the Gist we used before and added the instructions
    to install Metrics Server. We might not need it in this chapter but we will use
    it heavily later on, and I want you to get used to having it at all times.
  prefs: []
  type: TYPE_NORMAL
- en: If you prefer running the examples locally, you might be devastated by the news
    that we won't use a local cluster in this chapter. Don't despair. The costs will
    be kept to a minimum (probably a few dollars in total), and we'll be back to local
    clusters in the next chapter (unless you choose to stay in clouds).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a cluster in GKE, EKS, or AKS, our next step is to enable cluster
    auto-scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Cluster Autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We might need to install Cluster Autoscaler before we start using it. I said
    that we *might*, instead of saying that we *have to* because some Kubernetes flavors
    do come with Cluster Autoscaler baked in, while others don't. We'll go through
    each of the "big three" managed Kubernetes clusters. You might choose to explore
    all three of them, or to jump to the one you prefer. As a learning experience,
    I believe that it is beneficial to experience running Kubernetes in all three
    providers. Nevertheless, that might not be your view and you might prefer using
    only one. The choice is yours.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Cluster Autoscaler in GKE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This will be the shortest section ever written. There's nothing to do in GKE
    if you specified the `--enable-autoscaling` argument when creating the cluster.
    It already comes with Cluster Autoscaler pre-configured and ready.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Cluster Autoscaler in EKS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike GKE, EKS does not come with Cluster Autoscaler. We'll have to configure
    it ourselves. We'll need to add a few tags to the Autoscaling Group dedicated
    to worker nodes, to put additional permissions to the Role we're using, and to
    install Cluster Autoscaler.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get going.
  prefs: []
  type: TYPE_NORMAL
- en: We'll add a few tags to the Autoscaling Group dedicated to worker nodes. To
    do that, we need to discover the name of the group. Since we created the cluster
    using **eksctl**, names follow a pattern which we can use to filter the results.
    If, on the other hand, you created your EKS cluster without eksctl, the logic
    should still be the same as the one that follows, even though the commands might
    differ slightly.
  prefs: []
  type: TYPE_NORMAL
- en: First, we'll retrieve the list of the AWS Autoscaling Groups, and filter the
    result with `jq` so that only the name of the matching group is returned.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The output of the latter command should be similar to the one that follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We stored the name of the cluster in the environment variable `NAME`. Further
    on, we retrieved the list of all the groups and filtered the output with `jq`
    so that only those with names that start with `eksctl-$NAME-nodegroup` are returned.
    Finally, that same `jq` command retrieved the `AutoScalingGroupName` field and
    we stored it in the environment variable `ASG_NAME`. The last command output the
    group name so that we can confirm (visually) that it looks correct.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll add a few tags to the group. Kubernetes Cluster Autoscaler will
    work with the one that has the `k8s.io/cluster-autoscaler/enabled` and `kubernetes.io/cluster/[NAME_OF_THE_CLUSTER]`
    tags. So, all we have to do to let Kubernetes know which group to use is to add
    those tags.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The last change we'll have to do in AWS is to add a few additional permissions
    to the role created through eksctl. Just as with the Autoscaling Group, we do
    not know the name of the role, but we do know the pattern used to create it. Therefore,
    we'll retrieve the name of the role, before we add a new policy to it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The output of the latter command should be similar to the one that follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We listed all the roles, and we used `jq` to filter the output so that only
    the one with the name that starts with `eksctl-$NAME-nodegroup-0-NodeInstanceRole`
    is returned. Once we filtered the roles, we retrieved the `RoleName` and stored
    it in the environment variable `IAM_ROLE`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need JSON that describes the new policy. I already prepared one, so
    let's take a quick look at it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If you're familiar with AWS (I hope you are), that policy should be straightforward.
    It allows a few additional actions related to `autoscaling`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can `put` the new policy to the role.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now that we added the required tags to the Autoscaling Group and that we created
    the additional permissions that will allow Kubernetes to interact with the group,
    we can install Cluster Autoscaler Helm Chart.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Once the Deployment is rolled out, the autoscaler should be fully operational.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Cluster Autoscaler in AKS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of this writing (October 2018), Cluster Autoscaler does not work
    in AKS. At least, not always. It is still in beta stage, and I cannot recommend
    it just yet. Hopefully, it will be fully operational and stable soon. When that
    happens, I will update this chapter with AKS-specific instructions. If you feel
    adventurous or you are committed to Azure, please follow the instructions from
    the *Cluster Autoscaler on Azure Kubernetes Service (AKS) - Preview* ([https://docs.microsoft.com/en-in/azure/aks/cluster-autoscaler](https://docs.microsoft.com/en-in/azure/aks/cluster-autoscaler))
    article. If it works, you should be able to follow the rest of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective is to scale the nodes of our cluster to meet the demand of our
    Pods. We want not only to increase the number of worker nodes when we need additional
    capacity, but also to remove them when they are underused. For now, we'll focus
    on the former, and explore the latter afterward.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by taking a look at how many nodes we have in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The output, from GKE, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In your case, the number of nodes might differ. That's not important. What matters
    is to remember how many you have right now since that number will change soon.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the definition of the `go-demo-5` application before we
    roll it out.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the relevant parts, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this context, the only important part of the definition we are about to apply
    is the HPA connected to the `api` Deployment. Its minimum number of replicas is
    `15`. Given that each `api` container requests 500 MB RAM, fifteen replicas (7.5
    GB RAM) should be more than our cluster can sustain, assuming that it was created
    using one of the Gists. Otherwise, you might need to increase the minimum number
    of replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Let's apply the definition and take a look at the HPAs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The output of the latter command is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: It doesn't matter if the targets are still `unknown`. They will be calculated
    soon, but we do not care for them right now. What matters is that the `api` HPA
    will scale the Deployment to at least `15` replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to wait for a few seconds before we take a look at the Pods in
    the `go-demo-5` Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can see that some of the `api` Pods are being created, while others are pending.
    There can be quite a few reasons why a Pod would enter into the pending state.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, there are not enough available resources to host all the Pods.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a1c839db-c439-4994-a113-8b0a27c59e84.png)Figure 2-1: Unschedulable
    (pending) Pods waiting for the cluster capacity to increase'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see whether Cluster Autoscaler did anything to help with our lack of capacity.
    We'll explore the ConfigMap that contains Cluster Autoscaler status.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The output is too big to be presented in its entirety, so we'll focus on the
    parts that matter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The status is split into two sections; `Cluster-wide` and `NodeGroups`. The
    `ScaleUp` section of the cluster-wide status shows that scaling is `InProgress`.
    At the moment, there are `3` ready nodes.
  prefs: []
  type: TYPE_NORMAL
- en: If we move down to the `NodeGroups`, we'll notice that there is one for each
    group that hosts our nodes. In AWS those groups map to Autoscaling Groups, in
    case of Google to Instance Groups, and in Azure to Autoscale. One of the `NodeGroups`
    in the config has the `ScaleUp` section `InProgress`. Inside that group, `1` node
    is `ready`. The `cloudProviderTarget` value should be set to a number higher than
    the number of `ready` nodes, and we can conclude that Cluster Autoscaler already
    increased the desired amount of nodes in that group.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the provider, you might see three groups (GKE) or one (EKS) node
    group. That depends on how each provider organizes its node groups internally.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know that Cluster Autoscaler is in progress of scaling up the nodes,
    we might explore what triggered that action.
  prefs: []
  type: TYPE_NORMAL
- en: Let's describe the `api` Pods and retrieve their events. Since we want only
    those related to `cluster-autoscaler`, we'll limit the output using `grep`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The output, on GKE, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We can see that several Pods triggered `scale-up` event. Those are the Pods
    that were in the pending state. That does not mean that each trigger created a
    new node. Cluster Autoscaler is intelligent enough to know that it should not
    create new nodes for each trigger, but that, in this case, one or two nodes (depending
    on the missing capacity) should be enough. If that proves to be false, it will
    scale up again a while later.
  prefs: []
  type: TYPE_NORMAL
- en: Let's retrieve the nodes that constitute the cluster and see whether there are
    any changes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We can see that a new worker node was added to the cluster. It is not yet ready,
    so we'll need to wait for a few moments until it becomes fully operational.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the number of new nodes depends on the required capacity to
    host all the Pods. You might see one, two, or more new nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/27fff93a-5bb4-452a-bc2f-e480ec2c725a.png)Figure 2-2: The Cluster
    Autoscaler process of scaling up nodes'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see what happened to out Pods. Remember, the last time we checked
    them, there were quite a few in the pending state.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Cluster Autoscaler increased the desired number of nodes in the node group (for
    example, Autoscaling Group in AWS) which, in turn, created a new node. Once the
    scheduler noticed the increase in cluster's capacity, it scheduled the pending
    Pods into the new node. Within a few minutes, our cluster expanded and all the
    scaled Pods are running.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3126993e-31ae-4184-bee7-068f2752fa4c.png)Figure 2-3: Creation of
    the new node through node groups and rescheduling of the pending Pods'
  prefs: []
  type: TYPE_NORMAL
- en: So, what are the rules Cluster Autoscaler uses to decide when to scale up the
    nodes?
  prefs: []
  type: TYPE_NORMAL
- en: The rules governing nodes scale-up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster Autoscaler monitors Pods through a watch on Kube API. It checks every
    10 seconds whether there are any unschedulable Pods (configurable through the
    `--scan-interval` flag). In that context, a Pod is unschedulable when the Kubernetes
    Scheduler is unable to find a node that can accommodate it. For example, a Pod
    can request more memory than what is available on any of the worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Autoscaler assumes that the cluster is running on top of some kind of
    node groups. As an example, in the case of AWS, those groups are **Autoscaling
    Groups** (**ASGs**). When there is a need for additional nodes, Cluster Autoscaler
    creating a new node by increasing the size of a node group.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Autoscaler assumes that requested nodes will appear within 15 minutes
    (configurable through the `--max-node-provision-time` flag). If that period expires
    and a new node was not registered, it will attempt to scale up a different group
    if the Pods are still in pending state. It will also remove unregistered nodes
    after 15 minutes (configurable through the `--unregistered-node-removal-time`
    flag).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll explore how to scale down the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling down the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling up the cluster to meet the demand is essential since it allows us to
    host all the replicas we need to fulfill (some of) our SLAs. When the demand drops
    and our nodes become underutilized, we should scale down. That is not essential
    given that our users will not experience problems caused by having too much hardware
    in our cluster. Nevertheless, we shouldn't have underutilized nodes if we are
    to reduce expenses. Unused nodes result in wasted money. That is true in all situations,
    especially when running in Cloud and paying only for the resources we used. Even
    on-prem, where we already purchased hardware, it is essential to scale down and
    release resources so that they can be used by other clusters.
  prefs: []
  type: TYPE_NORMAL
- en: We'll simulate a decrease in demand by applying a new definition that will redefine
    the HPAs threshold to `2` (min) and `5` (max).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The output of the latter command is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the min and max values of the `api` HPA changed to `2` and `5`.
    The current number of replicas is still `15`, but that will drop to `5` soon.
    The HPA already changed the replicas of the Deployment, so let's wait until it
    rolls out and take another look at the Pods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The output of the latter command is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Let's see what happened to the `nodes`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that we still have four nodes (or whatever was your number
    before we de-scaled the Deployment).
  prefs: []
  type: TYPE_NORMAL
- en: Given that we haven't yet reached the desired state of only three nodes, we
    might want to take another look at the `cluster-autoscaler-status` ConfigMap.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the relevant parts, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'If your output does not contain `ScaleDown: CandidatesPresent`, you might need
    to wait a bit and repeat the previous command.'
  prefs: []
  type: TYPE_NORMAL
- en: If we focus on the `Health` section of the cluster-wide status, all four nodes
    are still ready.
  prefs: []
  type: TYPE_NORMAL
- en: Judging by the cluster-wide section of the status, we can see that there is
    one candidate to `ScaleDown` (it might be more in your case). If we move to the
    `NodeGroups`, we can observe that one of them has `CandidatesPresent` set to `1`
    in the `ScaleDown` section (or whatever was your initial value before scaling
    up).
  prefs: []
  type: TYPE_NORMAL
- en: In other words, one of the nodes is the candidate for removal. If it remains
    so for ten minutes, the node will be drained first to allow graceful shutdown
    of the Pods running inside it. After that, it will be physically removed through
    manipulation of the scaling group.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ede07cff-dc03-427e-9205-4facebd4711f.png)Figure 2-4: Cluster Autoscaler
    processes of scaling-down'
  prefs: []
  type: TYPE_NORMAL
- en: We should wait for ten minutes before we proceed, so this is an excellent opportunity
    to grab some coffee (or tea).
  prefs: []
  type: TYPE_NORMAL
- en: Now that enough time passed, we'll take another look at the `cluster-autoscaler-status`
    ConfigMap.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the relevant parts, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: From the cluster-wide section, we can see that now there are `3` ready nodes,
    but that there are still `4` (or more) registered. That means that one of the
    nodes was drained, but it was still not destroyed. Similarly, one of the node
    groups shows that there is `1` ready node, even though `2` are registered (your
    numbers might vary).
  prefs: []
  type: TYPE_NORMAL
- en: From Kubernetes perspective, we are back to three operational worker nodes,
    even though the fourth is still physically present.
  prefs: []
  type: TYPE_NORMAL
- en: Now we need to wait a bit more before we retrieve the nodes and confirm that
    only three are available.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The output, from GKE, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the node was removed and we already know from past experience
    that Kube Scheduler moved the Pods that were in that node to those that are still
    operational.Â Now that you experienced scaling down of your nodes, we'll explore
    the rule that governs the process.
  prefs: []
  type: TYPE_NORMAL
- en: The rules governing nodes scale-down
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster Autoscaler iterates every 10 seconds (configurable through the `--scan-interval`
    flag). If the conditions for scaling up are not met, it checks whether there are
    unneeded nodes.
  prefs: []
  type: TYPE_NORMAL
- en: It will consider a node eligible for removal when all of the following conditions
    are met.
  prefs: []
  type: TYPE_NORMAL
- en: The sum of CPU and memory requests of all Pods running on a node is less than
    50% of the node's allocatable resources (configurable through the `--scale-down-utilization-threshold`
    flag).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All Pods running on the node can be moved to other nodes. The exceptions are
    those that run on all the nodes like those created through DaemonSets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether a Pod might not be eligible for rescheduling to a different node when
    one of the following conditions are met.
  prefs: []
  type: TYPE_NORMAL
- en: A Pod with affinity or anti-affinity rules that tie it to a specific node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Pod that uses local storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Pod created directly instead of through controllers like Deployment, StatefulSet,
    Job, or ReplicaSet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All those rules boil down to a simple one. If a node contains a Pod that cannot
    be safely evicted, it is not eligible for removal.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we should speak about cluster scaling boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Can we scale up too much or de-scale to zero nodes?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we let Cluster Autoscaler do its "magic" without defining any thresholds,
    our cluster or our wallet might be at risk.
  prefs: []
  type: TYPE_NORMAL
- en: We might, for example, misconfigure HPA and end up scaling Deployments or StatefulSets
    to a huge number of replicas. As a result, Cluster Autoscaler might add too many
    nodes to the cluster. As a result, we could end up paying for hundreds of nodes,
    even though we need much less. Luckily, AWS, Azure, and GCP limit how many nodes
    we can have so we cannot scale to infinity. Nevertheless, we should not allow
    Cluster Autoscaler to go over some limits.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, there is a danger that Cluster Autoscaler will scale down to too
    few nodes. Having zero nodes is almost impossible since that would mean that we
    have no Pods in the cluster. Still, we should maintain a healthy minimum of nodes,
    even if that means sometimes being underutilized.
  prefs: []
  type: TYPE_NORMAL
- en: A reasonable minimum of nodes is three. That way, we have a worker node in each
    zone (datacenter) of the region. As you already know, Kubernetes requires three
    zones with master nodes to maintain quorum. In some cases, especially on-prem,
    we might have only one geographically collocated datacenter with low latency.
    In that case, one zone (datacenter) is better than none. But, in the case of Cloud
    providers, three zones is the recommended distribution, and having a minimum of
    one worker node in each makes sense. That is especially true if we use block storage.
  prefs: []
  type: TYPE_NORMAL
- en: By its nature, block storage (for example, EBS in AWS, Persistent Disk in GCP,
    and Block Blob in Azure) cannot move from one zone to another. That means that
    we have to have a worker node in each zone so that there is (most likely) always
    a place for it in the same zone as the storage. Of course, we might not use block
    storage in which case this argument is unfounded.
  prefs: []
  type: TYPE_NORMAL
- en: How about the maximum number of worker nodes? Well, that differs from one use
    case to another. You do not have to stick with the same maximum for all eternity.
    It can change over time.
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb, I'd recommend having a maximum double from the actual number
    of nodes. However, don't take that rule seriously. It truly depends on the size
    of your cluster. If you have only three worker nodes, your maximum size might
    be nine (three times bigger). On the other hand, if you have hundreds or even
    thousands of nodes, it wouldn't make sense to double that number as the maximum.
    That would be too much. Just make sure that the maximum number of nodes reflects
    the potential increase in demand.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, I'm sure that you'll figure out what should be your minimum and
    your maximum number of worker nodes. If you make a mistake, you can correct it
    later. What matters more is how to define those thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, setting up min and max values is easy in EKS, GKE, and AKS. For EKS,
    if you're using `eksctl` to create the cluster, all we have to do is add `--nodes-min`
    and `--nodes-max` arguments to the `eksctl create cluster` command. GKE is following
    a similar logic with `--min-nodes` and `--max-nodes` arguments of the `gcloud
    container clusters create` command. If one of the two is your preference, you
    already used those arguments if you followed the Gists. Even if you forget to
    specify them, you can always modify Autoscaling Groups (AWS) or Instance Groups
    (GCP) since that's where the limits are actually applied.
  prefs: []
  type: TYPE_NORMAL
- en: Azure takes a bit different approach. We define its limits directly in the `cluster-autoscaler`
    Deployment, and we can change them just by applying a new definition.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Autoscaler compared in GKE, EKS, and AKS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster Autoscaler is a prime example of the differences between different managed
    Kubernetes offerings. We'll use it to compare the three major Kubernetes-as-a-Service
    providers.
  prefs: []
  type: TYPE_NORMAL
- en: I'll limit the comparison between the vendors only to the topics related to
    Cluster Autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: GKE is a no-brainer for those who can use Google to host their cluster. It is
    the most mature and feature-rich platform. They started **Google Kubernetes Engine**
    (**GKE**) long before anyone else. When we combine their head start with the fact
    that they are the major contributor to Kubernetes and hence have the most experience,
    it comes as no surprise that their offering is way above others.
  prefs: []
  type: TYPE_NORMAL
- en: When using GKE, everything is baked into the cluster. That includes Cluster
    Autoscaler. We do not have to execute any additional commands. It simply works
    out of the box. Our cluster scales up and down without the need for our involvement,
    as long as we specify the `--enable-autoscaling` argument when creating the cluster.
    On top of that, GKE brings up new nodes and joins them to the cluster faster than
    the other providers. If there is a need to expand the cluster, new nodes are added
    within a minute.
  prefs: []
  type: TYPE_NORMAL
- en: There are many other reasons I would recommend GKE, but that's not the subject
    right now. Still, Cluster Autoscaling alone should be the proof that GKE is the
    solution others are trying to follow.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon's **Elastic Container Service** for Kubernetes (**EKS**) is somewhere
    in the middle. Cluster Autoscaling works, but it's not baked in. It's as if Amazon
    did not think that scaling clusters is important and left it as an optional add-on.
  prefs: []
  type: TYPE_NORMAL
- en: EKS installation is too complicated (when compared to GKE and AKS) but thanks
    to eksctl ([https://eksctl.io/](https://eksctl.io/)) from the folks from Weaveworks,
    we have that, more or less, solved. Still, there is a lot left to be desired from
    eksctl. For example, we cannot use it to upgrade our clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The reason I'm mentioning eksctl in the context of auto-scaling lies in the
    Cluster Autoscaler setup.
  prefs: []
  type: TYPE_NORMAL
- en: I cannot say that setting up Cluster Autoscaler in EKS is hard. It's not. And
    yet, it's not as simple as it should be. We have to tag the Autoscaling Group,
    put additional privileges to the role, and install Cluster Autoscaler. That's
    not much. Still, those steps are much more complicated than they should be. We
    can compare it with GKE. Google understands that auto-scaling Kuberentes clusters
    is a must and it provides that with a single argument (or a checkbox if you prefer
    UIs). AWS, on the other hand, did not deem auto-scaling important enough to give
    us that much simplicity. On top of the unnecessary setup in EKS, the fact is that
    AWS added the internal pieces required for scaling only recently. Metrics Server
    can be used only since September 2018.
  prefs: []
  type: TYPE_NORMAL
- en: My suspicion is that AWS does not have the interest to make EKS great by itself
    and that they are saving the improvements for Fargate. If that's the case (we'll
    find that out soon), I'd characterize it as "sneaky business." Kubernetes has
    all the tools required for scaling Pod and nodes and they are designed to be extensible.
    The choice not to include Cluster Autoscaler as an integral part of their managed
    Kubernetes service is a big minus.
  prefs: []
  type: TYPE_NORMAL
- en: What can I say about AKS? I admire the improvements Microsoft made in Azure
    as well as their contributions to Kubernetes. They do recognize the need for a
    good managed Kubernetes offering. Yet, Cluster Autoscaler is still in beta. Sometimes
    it works, more often than not it doesn't. Even when it does work as it should,
    it is slow. Waiting for a new node to join the cluster is an exercise in patience.
  prefs: []
  type: TYPE_NORMAL
- en: The steps required to install Cluster Autoscaler in AKS are sort of ridiculous.
    We are required to define a myriad of arguments that were supposed to be already
    available inside the cluster. It should know what is the name of the cluster,
    what is the resource group, and so on and so forth. And yet, it doesn't. At least,
    that's the case at the time of this writing (October 2018). I hope that both the
    process and the experience will improve over time. For now, from the perspective
    of auto-scaling, AKS is at the tail of the pack.
  prefs: []
  type: TYPE_NORMAL
- en: You might argue that the complexity of the setup does not really matter. You'd
    be right. What matters is how reliable Cluster Autoscaling is and how fast it
    adds new nodes to the cluster. Still, the situation is the same. GKE leads in
    reliability and the speed. EKS is the close second, while AKS is trailing behind.
  prefs: []
  type: TYPE_NORMAL
- en: What now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's not much left to say about Cluster Autoscaler.
  prefs: []
  type: TYPE_NORMAL
- en: We finished exploring fundamental ways to auto-scale Pods and nodes. Soon we'll
    dive into more complicated subjects and explore things that are not "baked" into
    a Kubernetes cluster. We'll go beyond the core project and introduce a few new
    tools and processes.
  prefs: []
  type: TYPE_NORMAL
- en: This is the moment when you should destroy your cluster if you're not planning
    to move into the next chapter right away and if your cluster is disposable (for
    example, not on bare-metal). Otherwise, please delete the `go-demo-5` Namespace
    to remove the resources we created in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Before you leave, you might want to go over the main points of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Autoscaler has a single purpose to adjust the size of the cluster by
    adding or removing worker nodes. It adds new nodes when Pods cannot be scheduled
    due to insufficient resources. Similarly, it eliminates nodes when they are underutilized
    for a period of time and when Pods running on one such node can be rescheduled
    somewhere else.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster Autoscaler assumes that the cluster is running on top of some kind of
    node groups. As an example, in the case of AWS, those groups are Autoscaling Groups
    (ASGs). When there is a need for additional nodes, Cluster Autoscaler creating
    a new node by increasing the size of a node group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cluster will be scaled down when the sum of CPU and memory requests of all
    Pods running on a node is less than 50% of the node's allocatable resources and
    when all Pods running on the node can be moved to other nodes (DamonSets are the
    exception).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
