- en: Extending HorizontalPodAutoscaler with Custom Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computers make excellent and efficient servants, but I have no wish to serve
    under them.
  prefs: []
  type: TYPE_NORMAL
- en: '- *Spock*'
  prefs: []
  type: TYPE_NORMAL
- en: Adoption of **HorizontalPodAutoscaler** (**HPA**) usually goes through three
    phases.
  prefs: []
  type: TYPE_NORMAL
- en: The first phase is *discovery*. The first time we find out what it does, we
    usually end up utterly amazed. "Look at this. It scales our applications automatically.
    I don't need to worry about the number of replicas anymore."
  prefs: []
  type: TYPE_NORMAL
- en: The second phase is the *usage*. Once we start using HPA, we quickly realize
    that scaling applications based memory and CPU is not enough. Some apps do increase
    their memory and CPU usage with the increase in load, while many others don't.
    Or, to be more precise, not proportionally. HPA, for some applications, works
    well. For many others, it does not work at all, or it is not enough. Sooner or
    later, we'll need to extend HPA thresholds beyond those based on memory and CPU.
    This phase is characterized by *disappointment*. "It seemed like a good idea,
    but we cannot use it with most of our applications. We need to fall back to alerts
    based on metrics and manual changes to the number of replicas."
  prefs: []
  type: TYPE_NORMAL
- en: The third phase is *re-discovery*. Once we go through the HPA v2 documentation
    (still in beta at the time of this writing) we can see that it allows us to extend
    it to almost any type of metrics and expressions. We can hook HPAs to Prometheus,
    or nearly any other tool, though adapters. Once we master that, there is almost
    no limit to the conditions we can set as triggers of automatic scaling of our
    applications. The only restriction is our ability to transform data into Kubernetes'
    custom metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Our next goal is to extend HorizontalPodAutoscaler definitions to include conditions
    based on data stored in Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `vfarcic/k8s-specs` ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    repository will continue to serve as our source of Kubernetes definitions. We'll
    make sure that it is up-to-date by pulling the latest version.
  prefs: []
  type: TYPE_NORMAL
- en: All the commands from this chapter are available in the `05-hpa-custom-metrics.sh`
    ([https://gist.github.com/vfarcic/cc546f81e060e4f5fc5661e4fa003af7](https://gist.github.com/vfarcic/cc546f81e060e4f5fc5661e4fa003af7))
    Gist.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The requirements are the same as those we had in the previous chapter. The only
    exception is **EKS**. We'll continue using the same Gists as before for all other
    Kuberentes flavors.
  prefs: []
  type: TYPE_NORMAL
- en: A note to EKS users Even though three t2.small nodes we used so far have more
    than enough memory and CPU, they might not be able to host all the Pods we'll
    create. EKS (by default) uses AWS networking. A t2.small instance can have a maximum
    of three network interfaces, with four IPv4 address per each. That means that
    we can have up to twelve IPv4 addresses on each t2.small node. Given that each
    Pod needs to have its own address, that means that we can have a maximum of twelve
    Pods per node. In this chapter, we might need more than thirty-six Pods across
    the cluster. Instead of creating a cluster with more than three nodes, we'll add
    Cluster Autoscaler (CA) to the mix, and let the cluster expand if needed. We already
    explored CA in one of the previous chapters and the setup instructions are now
    added to the Gist `eks-hpa-custom.sh` ([https://gist.github.com/vfarcic/868bf70ac2946458f5485edea1f6fc4c)](https://gist.github.com/vfarcic/868bf70ac2946458f5485edea1f6fc4c)).
  prefs: []
  type: TYPE_NORMAL
- en: Please use one of the Gists that follow to create a new cluster. If you already
    have a cluster you'd like to use for the exercises, please use the Gists to validate
    that it fulfills all the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '`gke-instrument.sh`: **GKE** with 3 n1-standard-1 worker nodes, **nginx Ingress**,
    **tiller**, **Prometheus** Chart, and environment variables **LB_IP**, **PROM_ADDR**,
    and **AM_ADDR** ([https://gist.github.com/vfarcic/675f4b3ee2c55ee718cf132e71e04c6e](https://gist.github.com/vfarcic/675f4b3ee2c55ee718cf132e71e04c6e)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eks-hpa-custom.sh`: **EKS** with 3 t2.small worker nodes, **nginx Ingress**,
    **tiller**, **Metrics Server**, **Prometheus** Chart, environment variables **LB_IP**,
    **PROM_ADDR**, and **AM_ADDR**, and **Cluster Autoscaler** ([https://gist.github.com/vfarcic/868bf70ac2946458f5485edea1f6fc4c](https://gist.github.com/vfarcic/868bf70ac2946458f5485edea1f6fc4c)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aks-instrument.sh`: **AKS** with 3 Standard_B2s worker nodes, **nginx Ingress**,
    and **tiller**, **Prometheus** Chart, and environment variables **LB_IP**, **PROM_ADDR**,
    and **AM_ADDR** ([https://gist.github.com/vfarcic/65a0d5834c9e20ebf1b99225fba0d339](https://gist.github.com/vfarcic/65a0d5834c9e20ebf1b99225fba0d339)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker-instrument.sh`: **Docker for Desktop** with **2 CPUs**, **3 GB RAM**,
    **nginx Ingress**, **tiller**, **Metrics Server**, **Prometheus** Chart, and environment
    variables **LB_IP**, **PROM_ADDR**, and **AM_ADDR** ([https://gist.github.com/vfarcic/1dddcae847e97219ab75f936d93451c2](https://gist.github.com/vfarcic/1dddcae847e97219ab75f936d93451c2)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minikube-instrument.sh`: **Minikube** with **2 CPUs**, **3 GB RAM**, **ingress**,
    **storage-provisioner**, **default-storageclass**, and **metrics-server** addons
    enabled, **tiller**, **Prometheus** Chart, and environment variables **LB_IP**,
    **PROM_ADDR**, and **AM_ADDR** ([https://gist.github.com/vfarcic/779fae2ae374cf91a5929070e47bddc8](https://gist.github.com/vfarcic/779fae2ae374cf91a5929070e47bddc8)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we're ready to extend our usage of HPA. But, before we do that, let's briefly
    explore (again) how HPA works out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Using HorizontalPodAutoscaler without metrics adapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we do not create a metrics adapter, Metrics Aggregator only knows about CPU
    and memory usage related to containers and nodes. To make things more complicated,
    that information is limited only to the last few minutes. Since HPA is just concerned
    about Pods and containers inside them, we are limited to only two metrics. When
    we create an HPA, it will scale or de-scale our Pods if memory or CPU consumption
    of the containers that constitute those Pods is above or below predefined thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics Server periodically fetches information (CPU and memory) from Kubelets
    running inside worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Those metrics are passed to Metrics Aggregator which, in this scenario, does
    not add any additional value. From there on, HPAs periodically consult the data
    in the Metrics Aggregator (through its API endpoint). When there is a discrepancy
    between target values defined in an HPA and the actual values, an HPA will manipulate
    the number of replicas of a Deployment or a StatefulSet. As we already know, any
    change to those controllers results in rolling updates executed through creation
    and manipulation of ReplicaSets, which create and delete Pods, which are converted
    into containers by a Kubelet running on a node where a Pod is scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d9f95b8d-7de9-4e19-86e5-702c61885999.png)Figure 5-1: HPA with out
    of the box setup (arrows show the flow of data)'
  prefs: []
  type: TYPE_IMG
- en: Functionally, the flow we just described works well. The only problem is the
    data available in the Metrics Aggregator. It is limited to memory and CPU. More
    often than not, that is not enough. So, there's no need for us to change the process,
    but to extend the data available to HPA. We can do that through a metrics adapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Prometheus Adapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given that we want to extend the metrics available through the Metrics API,
    and that Kubernetes allows us that through it's *Custom Metrics API* ([https://github.com/kubernetes/metrics/tree/master/pkg/apis/custom_metrics](https://github.com/kubernetes/metrics/tree/master/pkg/apis/custom_metrics)),
    one option to accomplish our goals could be to create our own adapter.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the application (DB) where we store metrics, that might be a good
    option. But, given that it is pointless to reinvent the wheel, our first step
    should be to search for a solution. If someone already created an adapter that
    suits our needs, it would make sense to adopt it, instead of creating a new one
    by ourselves. Even if we do choose something that provides only part of the features
    we're looking for, it's easier to build on top of it (and contribute back to the
    project), than to start from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Given that our metrics are stored in Prometheus, we need a metrics adapter that
    will be capable of fetching data from it. Since Prometheus is very popular and
    adopted by the community, there is already a project waiting for us to use it.
    It's called *Kubernetes Custom Metrics Adapter for Prometheus*. It is an implementation
    of the Kubernetes Custom Metrics API that uses Prometheus as the data source.
  prefs: []
  type: TYPE_NORMAL
- en: Since we adopted Helm for all our installations, we'll use it to install the
    adapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We installed `prometheus-adapter` Helm Chart from the `stable` repository. The
    resources were created in the `metrics` Namespace, and the `image.tag` is set
    to `v0.3.0`.
  prefs: []
  type: TYPE_NORMAL
- en: We changed `metricsRelistInterval` from the default value of `30s` to `90s`.
    That is the interval the adapter will use to fetch metrics from Prometheus. Since
    our Prometheus setup is fetching metrics from its targets every sixty seconds,
    we had to set the adapter's interval to a value higher than that. Otherwise, the
    adapter's frequency would be higher than pulling frequency of Prometheus, and
    we'd have iterations that would be without new data.
  prefs: []
  type: TYPE_NORMAL
- en: The last two arguments specified the URL and the port through which the adapter
    can access Prometheus API. In our case, the URL is set to go through Prometheus'
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Please visit *Prometheus Adapter Chart README* ([https://github.com/helm/charts/tree/master/stable/prometheus-adapter](https://github.com/helm/charts/tree/master/stable/prometheus-adapter))
    for more information about all the values you can set to customize the installation.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we waited until the `prometheus-adapter` rolls out.
  prefs: []
  type: TYPE_NORMAL
- en: If everything is working as expected, we should be able to query Kubernetes'
    custom metrics API and retrieve some of the Prometheus data provided through the
    adapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Given the promise that each chapter will feature a different Kubernetes' flavor,
    and that AWS did not have its turn yet, all the outputs are taken from EKS. Depending
    on the platform you're using, your outputs might be slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: The first entries of the output from querying Custom Metrics is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The list of the custom metrics available through the adapter is big, and we
    might be compelled to think that it contains all those stored in Prometheus. We'll
    find out whether that's true later. For now, we'll focus on the metrics we might
    need with HPA tied to `go-demo-5` Deployment. After all, providing metrics for
    auto-scaling is adapter's primary, if not the only function.
  prefs: []
  type: TYPE_NORMAL
- en: From now on, Metrics Aggregator contains not only the data from the metrics
    server but also those from the Prometheus Adapter which, in turn, is fetching
    metrics from Prometheus Server. We are yet to confirm whether the data we're getting
    through the adapter is enough and whether HPA works with custom metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2caa17d4-543c-4aef-bc51-67282a29270b.png)Figure 5-2: Custom metrics
    with Prometheus Adapter (arrows show the flow of data)'
  prefs: []
  type: TYPE_IMG
- en: Before we go deeper into the adapter, we'll define our goals for the `go-demo-5`
    application.
  prefs: []
  type: TYPE_NORMAL
- en: We should be able to scale the Pods not only based on memory and CPU usage but
    also by the number of requests entering through Ingress or observed through instrumented
    metrics. There can be many other criteria we could add but, as a learning experience,
    those should be enough. We already know how to configure HPA to scale based on
    CPU and memory, so our mission is to extend it with a request counter. That way,
    we'll be able to set the rules that will increase the number of replicas when
    the application is receiving too many requests, as well as to descale it if traffic
    decreases.
  prefs: []
  type: TYPE_NORMAL
- en: Since we want to extend the HPA connected to `go-demo-5`, our next step is to
    install the application.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We defined the address of the application, we installed the Chart, and we waited
    until the Deployment rolled out.
  prefs: []
  type: TYPE_NORMAL
- en: 'A note to EKS users If you received `error: deployment "go-demo-5" exceeded
    its progress deadline` message, the cluster is likely auto-scaling to accommodate
    all the Pods and zones of the PersistentVolumes. That might take more time than
    the `progress deadline`. In that case, wait for a few moments and repeat the `rollout`
    command.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll generate a bit of traffic by sending a hundred requests to the application
    through its Ingress resource.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now that we generated some traffic, we can try to find a metric that will help
    us calculate how many requests passed through Ingress. Since we already know (from
    the previous chapters) that `nginx_ingress_controller_requests` provides the number
    of requests that enter through Ingress, we should check whether it is now available
    as a custom metric.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We sent a request to `/apis/custom.metrics.k8s.io/v1beta1`. But, as you already
    saw, that alone would return all the metrics, and we are interested in only one.
    That's why we piped the output to `jq` and used its filter to retrieve only the
    entries that contain `nginx_ingress_controller_requests` as the `name`.
  prefs: []
  type: TYPE_NORMAL
- en: If you received an empty output, please wait for a few moments until the adapter
    pulls the metrics from Prometheus (it does that every ninety seconds) and re-execute
    the command.
  prefs: []
  type: TYPE_NORMAL
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We got three results. The name of each consists of the resource type and the
    name of the metric. We'll discard those related to `jobs.batch` and `namespaces`,
    and concentrate on the metric related to `ingresses.extensions` since it provides
    the information we need. We can see that it is `namespaced`, meaning that the
    metrics are, among other things, separated by namespaces of their origin. The
    `kind` and the `verbs` are (almost) always the same, and there's not much value
    going through them.
  prefs: []
  type: TYPE_NORMAL
- en: The major problem with `ingresses.extensions/nginx_ingress_controller_requests`
    is that it provides the number of requests for an Ingress resource. We couldn't
    use it in its current form as an HPA criterion. Instead, we should divide the
    number of requests with the number of replicas. That would give us the average
    number of requests per replica which should be a better HPA threshold. We'll explore
    how to use expressions instead of simple metrics later. Knowing the number of
    requests entering through Ingress is useful, but it might not be enough.
  prefs: []
  type: TYPE_NORMAL
- en: Since `go-demo-5` already provides instrumented metrics, it would be helpful
    to see whether we can retrieve `http_server_resp_time_count`. As a reminder, that's
    the same metric we used in the [Chapter 4](31a0e783-db26-40cf-be37-f1a60fc1bc9e.xhtml),
    *Debugging Issues Discovered Through Metrics and Alerts*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We used `jq` to filter the result so that only `http_server_resp_time_count`
    is retrieved. Don't be surprised by empty output. That's normal since Prometheus
    Adapter is not configured to process all the metrics from Prometheus, but only
    those that match its own internal rules. So, this might be a good moment to take
    a look at the `prometheus-adapter` ConfigMap that contains its configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output is too big to be presented in a book, so we'll go only through the
    first rule. It is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The first rule retrieves only the metrics with the name that starts with `container`
    (`__name__=~"^container_.*"`), with the label `container_name` being anything
    but `POD`, and with `namespace` and `pod_name` not empty.
  prefs: []
  type: TYPE_NORMAL
- en: Each rule has to specify a few resource overrides. In this case, the `namespace`
    label contains the `namespace` resource. Similarly, the `pod` resource is retrieved
    from the label `pod_name`. Further on, we can see the `name` section uses regular
    expressions to name the new metric. Finally, `metricsQuery` tells the adapter
    which Prometheus query it should execute when retrieving data.
  prefs: []
  type: TYPE_NORMAL
- en: If that setup looks confusing, you should know that you're not the only one
    bewildered at first sight. Prometheus Adapter, just as Prometheus Server configs
    are complicated to grasp at first. Nevertheless, they are very powerful allowing
    us to define rules for service discovery, instead of specifying individual metrics
    (in the case of the adapter) or targets (in the case of Prometheus Server). Soon
    we'll go into more details with the setup of the adapter rules. For now, the important
    note is that the default configuration tells the adapter to fetch all the metrics
    that match a few rules.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we saw that `nginx_ingress_controller_requests` metric is available
    through the adapter, but that it is of no use since we need to divide the number
    of requests with the number of replicas. We also saw that the `http_server_resp_time_count`
    metric originating in `go-demo-5` Pods is not available. All in all, we do not
    have all the metrics we need, while most of the metrics currently fetched by the
    adapter are of no use. It is wasting time and resources with pointless queries.
  prefs: []
  type: TYPE_NORMAL
- en: Our next mission is to reconfigure the adapter so that only the metrics we need
    are retrieved from Prometheus. We'll try to write our own expressions that will
    fetch only the data we need. If we manage to do that, we should be able to create
    HPA as well.
  prefs: []
  type: TYPE_NORMAL
- en: Creating HorizontalPodAutoscaler with custom metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you already saw, Prometheus Adapter comes with a set of default rules that
    provide many metrics that we do not need, and not all those that we do. It's wasting
    CPU and memory by doing too much, but not enough. We'll explore how we can customize
    the adapter with our own rules. Our next goal is to make the adapter retrieve
    only the `nginx_ingress_controller_requests` metric since that's the only one
    we need. On top of that, it should provide that metric in two forms. First, it
    should retrieve the rate, grouped by the resource.
  prefs: []
  type: TYPE_NORMAL
- en: The second form should be the same as the first but divided with the number
    of replicas of the Deployment that hosts the Pods where Ingress forwards the resources.
  prefs: []
  type: TYPE_NORMAL
- en: That one should give us an average number of requests per replica and will be
    a good candidate for our first HPA definition based on custom metrics.
  prefs: []
  type: TYPE_NORMAL
- en: I already prepared a file with Chart values that might accomplish our current
    objectives, so let's take a look at it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The first few entries in that definition are the same values as the ones we
    used previously through `--set` arguments. We'll skip those and jump into the
    `rules` section.
  prefs: []
  type: TYPE_NORMAL
- en: Within the `rules` section, we're setting the `default` entry to `false`. That
    will get rid of the default rules we explored previously, and allow us to start
    with a clean slate. Further on, there are two `custom` rules.
  prefs: []
  type: TYPE_NORMAL
- en: The first rule is based on the `seriesQuery` with `nginx_ingress_controller_requests`
    as the value. The `overrides` entry inside the `resources` section helps the adapter
    find out which Kubernetes resources are associated with the metric. We're setting
    the value of the `namespace` label to the `namespace` resource. There's a similar
    entry for `ingress`. In other words, we're associating Prometheus labels with
    Kubernetes resources `namespace` and `ingress`.
  prefs: []
  type: TYPE_NORMAL
- en: As you will see soon, the metric itself will be a part of a full query that
    will be treated as a single metric by HPA. Since we are creating something new,
    we need a name. So, we specified the `name` section with a single `as` entry set
    to `http_req_per_second`. That will be the reference in our HPA definitions.
  prefs: []
  type: TYPE_NORMAL
- en: You already know that `nginx_ingress_controller_requests` is not very useful
    by itself. When we used it in Prometheus, we had to put it inside a `rate` function,
    we had to `sum` everything, and we had to group the results by a resource. We're
    doing something similar through the `metricsQuery` entry. Think of it as an equivalent
    of expressions we're writing in Prometheus. The only difference is that we are
    using "special" syntax like `<<.Series>>`. That's adapter's templating mechanism.
    Instead of hard-coding the name of the metric, the labels, and the group by statements,
    we have `<<.Series>>`, `<<.LabelMatchers>>`, and `<<.GroupBy>>` clauses that will
    be populated with the correct values depending on what we put in API calls.
  prefs: []
  type: TYPE_NORMAL
- en: The second rule is almost the same as the first. The difference is in the name
    (now it's `http_req_per_second_per_replica`) and in the `metricsQuery`. The latter
    is now dividing the result with the number of replicas of the associated Deployment,
    just as we practiced in the [Chapter 3](13a73b25-73b2-4775-bd32-7945c7a25e46.xhtml),
    *Collecting and Querying Metrics and Sending Alerts*.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll update the Chart with the new values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now that the Deployment rolled out successfully, we can double-check that the
    configuration stored in the ConfigMap is indeed correct.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the `Data` section, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the default `rules` we explored earlier are now replaced with
    the two rules we defined in the `rules.custom` section of our Chart values file.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that the configuration looks correct does not necessarily mean that
    the adapter now provides data as Kubernetes custom metrics. Let's check that as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can see that there are four metrics available, two of `http_req_per_second`
    and two of `http_req_per_second_per_replica`. Each of the two metrics we defined
    is available as both `namespaces` and `ingresses`. Right now, we do not care about
    `namespaces`, and we'll concentrate on `ingresses`.
  prefs: []
  type: TYPE_NORMAL
- en: I'll assume that at least five minutes (or more) passed since we sent a hundred
    requests. If it didn't, you are a speedy reader, and you'll have to wait for a
    while before we send another hundred requests. We are about to create our first
    HPA based on custom metrics, and I want to make sure that you see how it behaves
    both before and after it is activated.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at an HPA definition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The first half of the definition should be familiar since it does not differ
    from what we used before. It will maintain between `3` and `10` replicas of the
    `go-demo-5` Deployment. The new stuff is in the `metrics` section.
  prefs: []
  type: TYPE_NORMAL
- en: In the past, we used `spec.metrics.type` set to `Resource`. Through that type,
    we defined CPU and memory targets. This time, however, our type is `Object`. It
    refers to a metric describing a single Kubernetes object which, in our case, happens
    to be a custom metric coming from Prometheus Adapter.
  prefs: []
  type: TYPE_NORMAL
- en: If we go through the *ObjectMetricSource v2beta1 autoscaling* ([https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#objectmetricsource-v2beta1-autoscaling](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#objectmetricsource-v2beta1-autoscaling))
    documentation, we can see that the fields of the `Object` type are different than
    those we used before when our type was `Resources`. We set the `metricName` to
    the metric we defined in Prometheus Adapter (`http_req_per_second_per_replica`).
  prefs: []
  type: TYPE_NORMAL
- en: Remember that it is not a metric, but that we defined an expression that the
    adapter uses to fetch data from Prometheus and convert it into a custom metric.
    In this case, we are getting the number of requests entering an Ingress resource
    and divided by the number of replicas of a Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `targetValue` is set to `50m` or 0.05 requests per second. I intentionally
    set it to a very low value so that we can easily reach the target and observe
    what happens.
  prefs: []
  type: TYPE_NORMAL
- en: Let's `apply` the definition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Next, we'll describe the newly created HPA, and see whether we can observe anything
    interesting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the relevant parts, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can see that there is only one entry in the `Metrics` section. The HPA is
    using the custom metric `http_req_per_second_per_replica` based on `Namespace/go-demo-5`.
    At the moment, the current value is `0`, and the `target` is set to `50m` (0.05
    requests per second). If, in your case, the `current` value is `unknown`, please
    wait for a few moments, and re-run the command.
  prefs: []
  type: TYPE_NORMAL
- en: Further down, we can see that both the `current` and the `desired` number of
    Deployment Pods is set to `3`.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, the target is not reached (there are `0` requests) so there's no
    need for the HPA to do anything. It maintains the minimum number of replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Let's spice it up a bit by generating some traffic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We sent a hundred requests to the `go-demo-5` Ingress.
  prefs: []
  type: TYPE_NORMAL
- en: Let's `describe` the HPA again, and see whether there are some changes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the relevant parts, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the `current` value of the metric increased. In my case, it
    is `138m` (0.138 requests per second). If your output still shows `0`, you'll
    have to wait until the metrics are pulled by Prometheus, until the adapter fetches
    them, and until the HPA refreshes its status. In other words, wait for a few moments,
    and re-run the previous command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that the `current` value is higher than the `target`, in my case, the
    HPA changed the `desired` number of `Deployment pods` to `6` (your number might
    differ depending on the value of the metric). As a result, HPA modified the Deployment
    by changing its number of replicas, and we should see additional Pods running.
    That becomes more evident through the `Events` section. There should be a new
    message stating `New size: 6; reason: Ingress metric http_req_per_second_per_replica
    above target`.'
  prefs: []
  type: TYPE_NORMAL
- en: To be on the safe side, we'll list the Pods in the `go-demo-5` Namespace and
    confirm that the new ones are indeed running.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We can see that there are now six `go-demo-5-*` Pods, with three of them much
    younger than the rest.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll explore what happens when traffic drops below the HPAs `target`.
    We'll accomplish that by not doing anything for a while. Since we are the only
    ones sending requests to the application, all we have to do is to stand still
    for five minutes or, even better, use this time to fetch coffee.
  prefs: []
  type: TYPE_NORMAL
- en: The reason we need to wait for at least five minutes lies in the frequencies
    HPA uses to scale up and down. By default, an HPA will scale up every three minutes,
    as long as the `current` value is above the `target`. Scaling down requires five
    minutes. An HPA will de-scale only if the `current` value is below the target
    for at least three minutes since the last time it scaled up.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, we need to wait for five minutes, or more, before we see the scaling
    effect in the opposite direction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the relevant parts, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The most interesting part of the output is the events section. We'll focus on
    the `Age` and `Message` fields. Remember, scale-up events are executed every three
    minutes if the current value is above the target, while scale-down iterations
    are every five minutes.
  prefs: []
  type: TYPE_NORMAL
- en: In my case, the HPA scaled the Deployment again, three minutes later. The number
    of replicas jumped from six to nine. Since the expression used by the adapter
    uses five minutes rate, some of the requests entered the second HPA iteration.
    Scaling up even after we stopped sending requests might not seem like a good idea
    (it isn't) but in the "real world" scenarios that shouldn't occur, since there's
    much more traffic than what we generated and we wouldn't put `50m` (0.2 requests
    per second) as a target.
  prefs: []
  type: TYPE_NORMAL
- en: Five minutes after the last scale-up event, the `current` value was at `0`,
    and the HPA scaled the Deployment down to the minimum number of replicas (`3`).
    There's no traffic anymore, and we're back to where we started.
  prefs: []
  type: TYPE_NORMAL
- en: We confirmed that Prometheus' metrics, fetched by Prometheus Adapter, and converted
    into Kuberentes' custom metrics, can be used in HPAs. So far, we used metrics
    pulled by Prometheus through exporters (`nginx_ingress_controller_requests`).
    Given that the adapter fetches metrics from Prometheus, it shouldn't matter how
    they got there. Nevertheless, we'll confirm that instrumented metrics can be used
    as well. That will give us an opportunity to cement what we learned so far and,
    at the same time, maybe learn a few new tricks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The output is yet another set of Prometheus Adapter Chart values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This time, we're combining rules containing different metric series. The first
    rule is based on the `http_server_resp_time_count` instrumented metric that origins
    in `go-demo-5`. We used it in the [Chapter 4](31a0e783-db26-40cf-be37-f1a60fc1bc9e.xhtml),
    *Debugging Issues Discovered Through Metrics and Alerts* and there's nothing truly
    extraordinary in its definition. It follows the same logic as the rules we used
    before. The second rule is the copy of one of the rules we used before.
  prefs: []
  type: TYPE_NORMAL
- en: What is interesting about those rules is that there are two completely different
    queries producing different results. However, the name is the same (`http_req_per_second_per_replica`)
    in both cases.
  prefs: []
  type: TYPE_NORMAL
- en: '"Wait a minute", you might say. The names are not the same. One is called `${1}req_per_second_per_replica`
    while the other is `http_req_per_second_per_replica`. While that is true, the
    final name, excluding resource type, is indeed the same. I wanted to show you
    that you can use regular expressions to form a name. In the first rule, the name
    consists of `matches` and `as` entries. The `(.*)` part of the `matches` entry
    becomes the first variable (there can be others) which is later on used as part
    of the `as` value (`${1}`). Since the metric is `http_server_resp_time_count`,
    it will extract `http_` from `^(.*)server_resp_time_count` which, in the next
    line, is used instead of `${1}`. The final result is `http_req_per_second_per_replica`,
    which is the same as the name of the second rule.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we established that both rules will provide custom metrics with the
    same name, we might think that will result in a conflict. How will HPA know which
    metric to use, if both are called the same? Will the adapter have to discard one
    and keep the other? The answer lies in the `resources` sections.
  prefs: []
  type: TYPE_NORMAL
- en: A true identifier of a metric is a combination of its name and the resource
    it ties with. The first rule produces two custom metrics, one for Services, and
    the other for Namespaces. The second also generates custom metrics for Namespaces,
    but for Ingresses as well.
  prefs: []
  type: TYPE_NORMAL
- en: How many metrics is that in total? I'll let you think about the answer before
    we check the result. To do that, we'll have to `upgrade` the Chart for the new
    values to take effect.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We upgraded the Chart with the new values and waited until the Deployment rolls
    out.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can go back to our pending question "how many custom metrics we've got?"
    Let's see...
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the relevant parts, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now we have three custom metrics, not four. I already explained that the unique
    identifier is the name of the metric combined with the Kubernetes resource it's
    tied to. All the metrics are called `http_req_per_second_per_replica`. But, since
    both rules override two resources, and `namespace` is set in both, one had to
    be discarded. We don't know which one was removed and which stayed. Or, maybe,
    they were merged. It does not matter since we shouldn't override the same resource
    with the metrics with the same name. There was no practical reason for me to include
    `namespace` in adapter's rule, other than to show you that there can be multiple
    overrides, and what happens when they are the same.
  prefs: []
  type: TYPE_NORMAL
- en: Other than that silly reason, you can mentally ignore the `namespaces/http_req_per_second_per_replica`
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: We used two different Prometheus' expressions to create two different custom
    metrics, with the same name but related to other resources. One (based on `nginx_ingress_controller_requests`
    expression) comes from Ingress resources, while the other (based on `http_server_resp_time_count`)
    comes from Services. Even though the latter originates in `go-demo-5` Pods, Prometheus
    discovered it through Services (as discussed in the previous chapter).
  prefs: []
  type: TYPE_NORMAL
- en: We can use the `/apis/custom.metrics.k8s.io` endpoint not only to discover which
    custom metrics we have but also to inspect details, including values. For example,
    we can retrieve `services/http_req_per_second_per_replica` metric through the
    command that follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The `describedObject` section shows us the details of the items. Right now,
    we have only one Service with that metric.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the Service resides in the `go-demo-5` Namespace, that its name
    is `go-demo-5`, and that it is using `v1` API version.
  prefs: []
  type: TYPE_NORMAL
- en: Further down, we can see the current value of the metric. In my case, it is
    `1130m`, or slightly above one request per second. Since nobody is sending requests
    to the `go-demo-5` Service, that value is as expected, considering that a health
    check is executed once a second.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll explore updated HPA definition that will use the Service-based metric.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: When compared with the previous definition, the only change is in the `target`
    and `targetValue` fields. Remember, the full identifier is a combination of the
    `metricName` and `target`. Therefore, this time we changed the `kind` to `Service`.
    We also had to change the `targetValue` since our application is receiving not
    only external requests through Ingress but also internal ones. They could be originating
    in other applications that might communicate with `go-demo-5` or, as in our case,
    in Kubernetes' health checks. Since their frequency is one second, we set the
    `targetValue` to `1500m`, or 1.5 requests per second. That way, scaling will not
    be triggered if we do not send any requests to the application. Normally, you'd
    set a much bigger value. But, for now, we're only trying to observe how it behaves
    before and after scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll apply the changes to the HPA, and describe it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The output of the latter command, limited to the relevant parts, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: For now, there's no reason for the HPA to scale up the Deployment. The current
    value is below the threshold. In my case, it's `1100m`.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can test whether autoscaling based on custom metrics originating from
    instrumentation works as expected. Sending requests through Ingress might be slow,
    especially if our cluster runs in Cloud. The round-trip from out laptop all the
    way to the service might be too slow. So, we'll send requests from inside the
    cluster, by spinning up a Pod and executing a request loop from inside it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Normally, I prefer `alpine` images since they are much small and efficient.
    However, `for` loops do not work from `alpine` (or I don't know how to write them),
    so we switched to `debian` instead. It doesn't have `curl` though, so we'll have
    to install it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now we can send requests that will generate enough traffic for HPA to trigger
    the scale-up process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We sent five-hundred requests to `/demo/hello` endpoint, and we exited the container.
    Since we used the `--rm` argument when we created the Pod, it will be removed
    automatically from the system, so we do not need to execute any cleanup operation.
  prefs: []
  type: TYPE_NORMAL
- en: Let's `describe` the HPA and see what happened.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the relevant parts, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: HPA detected that the `current` value is above the target (in my case it's `1794m`)
    and changed the desired number of replicas from `3` to `4`. We can observe that
    from the last event as well. If, in your case, the `desired` number of replicas
    is still `3`, please wait for a few moments for the next iteration of HPA evaluations,
    and repeat the `describe` command.
  prefs: []
  type: TYPE_NORMAL
- en: If we need an additional confirmation that scaling indeed worked as expected,
    we can retrieve the Pods in the `go-demo-5` Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: There's probably no need to confirm that the HPA will soon scale down the `go-demo-5`
    Deployment after we stopped sending requests. Instead, we'll jump into the next
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: Combining Metric Server data with custom metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, the few HPA examples used a single custom metric to decide whether to
    scale the Deployment. You already know from the [Chapter 1](c24be85c-1380-457c-84cb-7f29cbac1c62.xhtml),
    *Autoscaling Deployments and StatefulSets Based on Resource Usage*, that we can
    combine multiple metrics in an HPA. However, all the examples in that chapter
    used data from the Metrics Server. We learned that in many cases memory and CPU
    metrics from the Metrics Server are not enough, so we introduced Prometheus Adapter
    that feeds custom metrics to the Metrics Aggregator. We successfully configured
    an HPA to use those custom metrics. Still, more often than not, we'll need a combination
    of both types of metrics in our HPA definitions. While memory and CPU metrics
    are not enough by themselves, they are still essential. Can we combine both?
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at yet another HPA definition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the relevant parts, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This time, HPA has three entries in the `metrics` section. The first two are
    the "standard" `cpu` and `memory` entries based on `Resource` type. The last entry
    is one of the `Object` types we used earlier. With those combined, we're telling
    HPA to scale up if any of the three criteria are met. Similarly, it will scale
    down as well but for that to happen all three criteria need to be below the targets.
  prefs: []
  type: TYPE_NORMAL
- en: Let's `apply` the definition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Next, we'll describe the HPA. But, before we do that, we'll have to wait for
    a bit until the updated HPA goes through its next iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the relevant parts, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the memory-based metric is above the threshold from the start.
    In my case, it is `110%`, while the target is `80%`. As a result, HPA scaled up
    the Deployment. In my case, it set the new size to `5` replicas.
  prefs: []
  type: TYPE_NORMAL
- en: There's no need to confirm that the new Pods are running. By now, we should
    trust HPA to do the right thing. Instead, we'll comment briefly on the whole flow.
  prefs: []
  type: TYPE_NORMAL
- en: The complete HorizontalPodAutoscaler flow of events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metrics Server is fetching memory and CPU data from Kubelets running on the
    worker nodes. In parallel, Prometheus Adapter is fetching data from Prometheus
    Server which, as you already know, pulls data from different sources. Data from
    both Metrics Server and Prometheus Adapter is combined in Metrics Aggregator.
  prefs: []
  type: TYPE_NORMAL
- en: HPA is periodically evaluating metrics defined as scaling criteria. It's fetching
    data from Metrics Aggregator, and it does not really care whether they're coming
    from Metrics Server, Prometheus Adapter, or any other tool we could have used.
  prefs: []
  type: TYPE_NORMAL
- en: Once scaling criteria is met, HPA manipulates Deployments and StatefulSets by
    changing their number of replicas.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, rolling updates are performed by creating and updating ReplicaSets
    which, in turn, create or remove Pods.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3a9e1aa2-f19e-47a8-ba94-b5c03eccf23a.png)Figure 5-3: HPA using a
    combination of metrics from Metrics Server and those provided by Prometheus Adapter
    (arrows show the flow of data)'
  prefs: []
  type: TYPE_IMG
- en: Reaching nirvana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to add almost any metric to HPAs, they are much more useful
    than what it seemed in the [Chapter 1](c24be85c-1380-457c-84cb-7f29cbac1c62.xhtml),
    *Autoscaling Deployments and StatefulSets Based on Resource Usage*. Initially,
    HPAs weren't very practical since memory and CPU are, in many cases, insufficient
    for making decisions on whether to scale our Pods. We had to learn how to collect
    metrics (we used Prometheus Server for that), and how to instrument our applications
    to gain more detailed visibility. Custom Metrics was the missing piece of the
    puzzle. If we extend the "standard" metrics (CPU and memory) with the additional
    metrics we need (for example, Prometheus Adapter), we gain a potent process that
    will keep the number of replicas of our applications in sync with internal and
    external demands. Assuming that our applications are scalable, we can guarantee
    that they will (almost) always be as performant as needed. There is no need for
    manual interventions any more, at least when scaling is in question. HPA with
    "standard" and custom metrics will guarantee that the number of Pods meets the
    demand, and Cluster Autoscaler (when applicable) will ensure that we have the
    sufficient capacity to run those Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Our system is one step closer to being self-sufficient. It will self-adapt to
    changed conditions, and we (humans) can turn our attention towards more creative
    and less repetitive tasks than those required to maintain the system in the state
    that meets the demand. We are one step closer to nirvana.
  prefs: []
  type: TYPE_NORMAL
- en: What now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please note that we used `autoscaling/v2beta1` version of HorizontalPodAutoscaler.
    At the time of this writing (November 2018), only `v1` is stable and production-ready.
    However, `v1` is so limited (it can use only CPU metrics) that it's almost useless.
    Kubernetes community worked on new (`v2`) HPA for a while and, in my experience,
    it works reasonably well. The main problem is not stability but potential changes
    in the API that might not be backward compatible. A short while ago, `autoscaling/v2beta2`
    was released, and it uses a different API. I did not include it in the book because
    (at the time of this writing) most Kubernetes clusters do not yet support it.
    If you're running Kubernetes 1.11+, you might want to switch to `v2beta2`. If
    you do so, remember that you'll need to make a few changes to the HPA definitions
    we explored. The logic is still the same, and it behaves in the same way. The
    only visible difference is in the API.
  prefs: []
  type: TYPE_NORMAL
- en: Please consult *HorizontalPodAutoscaler v2beta2 autoscaling* ([https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#horizontalpodautoscaler-v2beta2-autoscaling](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#horizontalpodautoscaler-v2beta2-autoscaling))
    for the changes from `v2beta1` we used to `v2beta2` available in Kubernetes 1.11+.
  prefs: []
  type: TYPE_NORMAL
- en: That's it. Destroy the cluster if its dedicated to this book, or keep it if
    it's not or if you're planning to jump to the next chapter right away. If you're
    keeping it, please delete the `go-demo-5` resources by executing the commands
    that follow.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Before you leave, you might want to go over the main points of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: HPA is periodically evaluating metrics defined as scaling criteria.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HPA fetching data from Metrics Aggregator, and it does not really care whether
    they're coming from Metrics Server, Prometheus Adapter, or any other tool we could
    have used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
