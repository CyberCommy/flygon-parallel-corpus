- en: '*Chapter 12*: Scaling Out KVM with OpenStack'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第12章*：使用OpenStack扩展KVM'
- en: 'Being able to virtualize a machine is a big thing, but sometimes, just virtualization
    is not enough. The problem is how to give individual users tools so that they
    can virtualize whatever they need, when they need it. If we combine that user-centric
    approach with virtualization, we are going to end up with a system that needs
    to be able to do two things: it should be able to connect to KVM as a virtualization
    mechanism (and not only KVM) and enable users to get their virtual machines running
    and automatically configured in a self-provisioning environment that''s available
    through a web browser. OpenStack adds one more thing to this since it is completely
    free and based entirely on open source technologies. Provisioning such a system
    is a big problem due to its complexity, and in this chapter, we are going to show
    you – or to be more precise, point you – in the right direction regarding whether
    you need a system like this.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 能够虚拟化一台机器是一件大事，但有时，仅仅虚拟化是不够的。问题在于如何给予个人用户工具，使他们可以在需要时虚拟化他们需要的任何东西。如果我们将以用户为中心的方法与虚拟化相结合，我们将得到一个需要能够连接到KVM作为虚拟化机制（不仅仅是KVM）并使用户能够在自助环境中通过Web浏览器获取其虚拟机并自动配置的系统。OpenStack增加了一件事情，因为它完全免费并且完全基于开源技术。由于其复杂性，配置这样的系统是一个大问题，在本章中，我们将向您展示
    - 或者更准确地说，指向您 - 关于是否需要这样的系统的正确方向。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduction to OpenStack
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍OpenStack
- en: Software-defined networking
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件定义网络
- en: OpenStack components
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack组件
- en: Additional OpenStack use cases
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外的OpenStack用例
- en: Provisioning the OpenStack environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置OpenStack环境
- en: Integrating OpenStack with Ansible
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将OpenStack与Ansible集成
- en: Let's get started!
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Introduction to OpenStack
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍OpenStack
- en: In its own words, **OpenStack** is a cloud operating system that is used to
    control a large number of different resources in order to provide all the essential
    services for **Infrastructure-as-a-Service** (**IaaS**) and **orchestration**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其自己的说法，**OpenStack**是一个云操作系统，用于控制大量不同的资源，以提供**基础设施即服务**（**IaaS**）和**编排**的所有基本服务。
- en: But what does this mean? OpenStack is designed to completely control all the
    resources that are in the data center, and to provide both central management
    and direct control over anything that can be used to deploy both its own and third-party
    services. Basically, for every service that we mention in this book, there is
    a place in the whole OpenStack landscape where that service is or can be used.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但这意味着什么？OpenStack旨在完全控制数据中心中的所有资源，并提供对可以用于部署其自身和第三方服务的任何资源的集中管理和直接控制。基本上，对于我们在本书中提到的每项服务，在整个OpenStack景观中都有一个可以使用该服务的地方。
- en: OpenStack itself consists of several different interconnected services or service
    parts, each with its own set of functionalities, and each with its own API that
    enables full control of the service. In this part of this book, we will try to
    explain what different parts of OpenStack do, how they interconnect, what services
    they provide, and how to use those services to our advantage.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack本身由几个不同的相互连接的服务或服务部分组成，每个都有自己的功能集，每个都有自己的API，可以完全控制该服务。在本书的这一部分，我们将尝试解释OpenStack的不同部分的功能，它们如何相互连接，它们提供的服务以及如何利用这些服务来获得优势。
- en: The reason OpenStack exists is because there was the need for an open source
    cloud computing platform that would enable creating public and private clouds
    that are independent of any commercial cloud platform. All parts of OpenStack
    are open source and were released under the Apache License 2.0\. The software
    was created by a large, mixed group of individuals and large cloud providers.
    Interestingly, the first major release was the result of NASA (a US government
    agency) and Rackspace Technology (a large US hosting company) joining their internal
    storage and computing infrastructure solutions. These releases were later designated
    with the names Nova and Swift, and we will cover them in more detail later.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack存在的原因是因为需要一个开源的云计算平台，可以创建独立于任何商业云平台的公共和私有云。OpenStack的所有部分都是开源的，并且根据Apache许可证2.0发布。该软件是由大型混合个人和大型云提供商创建的。有趣的是，第一个主要版本的发布是NASA（美国政府机构）和Rackspace
    Technology（美国大型托管公司）合并其内部存储和计算基础设施解决方案的结果。这些发布后来被指定为Nova和Swift，并且我们将在后面更详细地介绍它们。
- en: The first thing you will notice about OpenStack is its services since there
    is no single *OpenStack* service but an actual stack of services. The name *OpenStack*
    comes directly from this concept because it correctly identifies OpenStack as
    an open source component that acts as services that are, in turn, grouped into
    functional sets.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你将注意到关于OpenStack的第一件事是它的服务，因为没有单一的*OpenStack*服务，而是一整套服务。名称*OpenStack*直接来自这个概念，因为它正确地将OpenStack识别为一个作为服务的开源组件，这些服务又被分组成功能集。
- en: Once we understand that we are talking about autonomous services, we also need
    to understand that services in OpenStack are grouped by their function, and that
    some functions have more than one specialized service under them. We will try
    to cover as much as possible about different services in this chapter, but there
    are simply too many of them to even mention all of them here. All the documentation
    and all the whitepapers can be found at [http://openstack.org](http://openstack.org),
    and we strongly suggest that you consult it for anything not mentioned here, and
    even for things that we mention but that could have changed by the time you read
    this.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们了解到我们正在谈论自主服务，我们还需要了解OpenStack中的服务是按其功能分组的，并且某些功能下有不止一个专门的服务。我们将尽量在本章中涵盖尽可能多的不同服务，但是其中有太多的服务，甚至无法在这里提及所有。所有文档和白皮书都可以在[http://openstack.org](http://openstack.org)找到，我们强烈建议您查阅其中任何未在此处提及的内容，甚至对我们提到但在您阅读时可能已经发生变化的内容也要查阅。
- en: The last thing we need to clarify is the naming – every service in OpenStack
    has its project name and is referred to by that name in the documentation. This
    might, at first glance, look confusing since some of the names are completely
    unrelated to the specific function a particular service has in the whole project,
    but using names instead of official designators for a function is far easier once
    you start using OpenStack. Take, for example, Swift. Swift's full name is *OpenStack
    Object Store*, but this is rarely mentioned in the documentation or its implementation.
    The same goes for other services or *projects* under OpenStack, such as Nova,
    Ironic, Neutron, Keystone, and over 20 other different services.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要澄清的最后一件事是命名——OpenStack中的每个服务都有其项目名称，并且在文档中以该名称来引用。乍一看，这可能看起来令人困惑，因为其中一些名称与特定服务在整个项目中的具体功能完全无关，但一旦你开始使用OpenStack，使用名称而不是官方指示符来表示功能会更容易。例如，Swift。Swift的全名是*OpenStack对象存储*，但在文档或其实施中很少提到。其他服务或在OpenStack下的*项目*，如Nova、Ironic、Neutron、Keystone以及其他20多个不同的服务也是如此。
- en: If you step away from OpenStack for a second, then you need to consider what
    cloud services are all about. The cloud is all about scaling – in terms of compute
    resources, storage, network, APIs – whatever. But, as always in life, as you scale
    things, you're going to run into problems. And these problems have their own *names*
    and *solutions*. So, let's discuss these problems for a minute.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你暂时离开OpenStack，那么你需要考虑云服务的本质。云服务的本质就是扩展——无论是计算资源、存储、网络、API等。但是，就像生活中的一切一样，随着事物的扩展，你会遇到问题。这些问题有它们自己的*名称*和*解决方案*。所以，让我们讨论一下这些问题。
- en: 'The basic problems for cloud provider scalability can be divided into three
    groups of problems that need to be solved at scale:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商可扩展性的基本问题可以分为三组需要大规模解决的问题：
- en: '**Compute problems** (Compute = CPU + memory power): These problems are pretty
    straightforward to solve – if you need more CPU and memory power, you buy more
    servers, which, by design, means more CPU and memory. If you need a quality of
    service/**service-level agreement** (**SLA**) type of concept, we can introduce
    a concept such as compute resource pools so that we can slice the compute *pie*
    according to our needs and divide those resources between our clients. It doesn''t
    matter whether our client is just a private person or a company buying into cloud
    services. In cloud technologies, we call our clients *tenants*.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算问题**（计算=CPU+内存能力）：这些问题解决起来相当简单——如果你需要更多的CPU和内存能力，你就购买更多的服务器，这意味着更多的CPU和内存。如果你需要服务质量/**服务级别协议**（**SLA**）类型的概念，我们可以引入计算资源池的概念，这样我们可以根据需要切割计算*饼*并在我们的客户之间分配这些资源。无论我们的客户是私人还是购买云服务的公司，这都无关紧要。在云技术中，我们称我们的客户为*租户*。'
- en: '**Storage problems**: As you scale your cloud environments, things become really
    messy in terms of storage capacity, management, monitoring and – especially –
    performance. The performance side of that problem has a couple of most commonly
    used variables – read and write throughput and read and write IOPS. When you grow
    your environment from 100 hosts to 1,000 hosts or more, performance bottlenecks
    are going to become a major issue that will be difficult to tackle without proper
    concepts. So, the storage problem can be solved by adding additional storage devices
    and capacity, but it''s much more involved than the compute problem as it needs
    much more configuration and money. Remember, every virtual machine has a statistical
    influence on other virtual machines'' performance, and the more virtual machines
    you have, the greater this entropy is. This is the most difficult process to manage
    in storage infrastructure.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储问题**：随着云环境的扩展，存储容量、管理、监控以及性能方面变得非常混乱。性能问题有一些最常用的变量——读写吞吐量和读写IOPS。当你将环境从100个主机扩展到1,000个或更多时，性能瓶颈将成为一个难以解决的主要问题。因此，存储问题可以通过增加额外的存储设备和容量来解决，但它比计算问题更复杂，需要更多的配置和资金。记住，每个虚拟机对其他虚拟机的性能有统计上的影响，虚拟机越多，这种熵就越大。这是存储基础设施中最难管理的过程。'
- en: '`A` can''t communicate with the network traffic of tenant `B`. At the same
    time, you still need to offer a capability where you can have multiple networks
    (usually implemented via VLANs in non-cloud infrastructures) per tenant and routing
    between these networks, if that''s what the tenant needs.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`A`无法与租户`B`的网络流量通信。与此同时，你仍然需要提供一个能够让租户拥有多个网络（通常在非云基础设施中通过VLAN实现）并在这些网络之间进行路由的能力，如果这是租户需要的话。'
- en: This network problem is a scalability problem based on technology, as the technology
    behind VLAN was standardized years before the number of VLANs could become a scalability
    problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络问题是一个基于技术的可扩展性问题，因为VLAN背后的技术在VLAN数量成为可扩展性问题之前已经标准化了多年。
- en: Let's continue our journey through OpenStack by explaining the most fundamental
    subject of cloud environments, which is scaling cloud networking via **software-defined
    networking** (**SDN**). The reason for this is really simple – without SDN concepts,
    the cloud wouldn't really be scalable enough for customers to be happy, and that
    would be a complete showstopper. So, buckle up your seatbelts and let's do an
    SDN primer.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续通过解释云环境的最基本主题来了解OpenStack，即通过**软件定义网络**（**SDN**）扩展云网络。这样做的原因非常简单——没有SDN概念，云对于客户来说就不够可扩展，这将是一个完全的停滞。所以，系好你的安全带，让我们进行SDN入门。
- en: Software-defined networking
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软件定义网络
- en: One of the straightforward stories about the cloud – at least on the face of
    it – should have been the story about cloud networking. In order to understand
    how simple this story should've been, we only need to look at one number, and
    that number is the `VLAN 1`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 云的一个直接的故事——至少表面上是这样——应该是关于云网络的故事。为了理解这个故事应该有多简单，我们只需要看一个数字，那个数字就是`VLAN 1`。
- en: So, basically, we're left with 4,093 separate logical networks in a real-life
    scenario, which is probably more than enough for the internal infrastructure of
    any given company. However, this is nowhere near enough for public cloud providers.
    The same problem applies to public cloud providers that use hybrid-cloud types
    of services to – for example – extend their compute power to the cloud.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，基本上，在现实场景中，我们剩下了4,093个单独的逻辑网络，这可能已经足够用于任何公司的内部基础设施。然而，对于公共云提供商来说，这远远不够。同样的问题也适用于使用混合云类型服务的公共云提供商，例如将他们的计算能力扩展到云端。
- en: So, let's focus on this network problem for a bit. Realistically, if we look
    at this problem from the cloud user perspective, data privacy is of utmost importance
    to us. If we look at this problem from the cloud provider perspective, then we
    want our network isolation problem to be a non-issue for our tenants. This is
    what cloud services are all about at a more basic level – no matter what the background
    complexity in terms of technology is, users have to be able to access all of the
    necessary services in as user-friendly a way as possible. Let's explain this by
    using an example.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们稍微关注一下这个网络问题。从云用户的角度来看，数据隐私对我们来说至关重要。如果从云提供商的角度来看这个问题，那么我们希望我们的网络隔离问题对我们的租户来说不成问题。这就是云服务在更基本层面上的全部意义——无论技术背景的复杂性如何，用户都必须能够以尽可能用户友好的方式访问所有必要的服务。让我们通过一个例子来解释这一点。
- en: What happens if we have 5,000 different clients (tenants) in our public cloud
    environment? What happens if every tenant needs to have five or more logical networks?
    We quickly realize that we have a big problem as cloud environments need to be
    separated, isolated, and fenced. They need to be separated from one another at
    a network level for security and privacy reasons. However, they also need to be
    routable, if a tenant needs that kind of service. On top of that, we need the
    ability to scale so that situations in which we need more than 5,000 or 50,000
    isolated networks don't bother us. And, going back to our previous point – roughly
    4,000 VLANs just isn't going to cut it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的公共云环境中有5,000个不同的客户（租户）会发生什么？如果每个租户都需要拥有五个或更多的逻辑网络会发生什么？我们很快意识到我们有一个大问题，因为云环境需要被分隔、隔离和围栏起来。出于安全和隐私原因，它们需要在网络层面相互分隔。然而，如果租户需要那种服务，它们也需要可路由。除此之外，我们需要能够扩展，以便在需要超过5,000或50,000个隔离网络的情况下不会困扰我们。再次回到我们之前的观点——大约4,000个VLAN根本不够用。
- en: There's a reason why we said that this should have been a straightforward story.
    The engineers among us see these situations in black and white – we focus on a
    problem and try to come to a solution. And the solution seems rather simple –
    we need to extend the 12-bit VLAN ID field so that we can have more available
    logical networks. How difficult can that be?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以说这应该是一个简单的故事，是有原因的。我们中的工程师们将这些情况看得很黑白分明——我们专注于问题并试图找到解决方案。解决方案似乎相当简单——我们需要扩展12位VLAN
    ID字段，以便我们可以拥有更多可用的逻辑网络。这有多难呢？
- en: As it turns out, very difficult. If history teaches us anything, it's that various
    different interests, companies, and technologies compete for years for that *top
    dog* status in anything in terms of IT technology. Just think of the good old
    days of DVD+R, DVD-R, DVD+RW, DVD-RW, DVD-RAM, and so on. To simplify things a
    bit, the same thing happened here when the initial standards for cloud networking
    were introduced. We usually call these network technologies cloud overlay network
    technologies. These technologies are the basis for SDN, the principle that describes
    the way cloud networking works at a global, centralized management level. There
    are multiple standards on the market to solve this problem – VXLAN, GRE, STT,
    NVGRE, NVO3, and more.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，这是非常困难的。如果历史教会我们任何东西，那就是各种不同的利益、公司和技术会在IT技术方面争夺多年的*顶级*地位。只需想想DVD+R、DVD-R、DVD+RW、DVD-RW、DVD-RAM等旧日的情形。简化一下，当云网络的初始标准被引入时，同样的情况也发生了。我们通常将这些网络技术称为云覆盖网络技术。这些技术是SDN的基础，它描述了云网络在全球、集中管理层面上的工作方式。市场上有多种标准来解决这个问题——VXLAN、GRE、STT、NVGRE、NVO3等等。
- en: Realistically, there's no need to break them all down one by one. We are going
    to take a simpler route – we're going to describe one of them that's the most
    valuable for us in the context of today (**VXLAN**) and then move on to something
    that's considered to be a *unified* standard of tomorrow (**GENEVE**).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，没有必要一一解释它们。我们将采取更简单的方式——我们将描述其中一个在今天环境中最有价值的标准（**VXLAN**），然后转向被认为是明天*统一*标准的东西（**GENEVE**）。
- en: First, let's define what an overlay network is. When we're talking about overlay
    networks, we're talking about networks that are built on top of another network
    in the same infrastructure. The idea behind an overlay network is simple – we
    need to disentangle the physical part of the network from the logical part of
    the network. If we want to do that in absolute terms (configure everything without
    spending massive amounts of time in the CLI to configure physical switches, routers,
    and so on), we can do that as well. If we don't want to do it that way and we
    still want to work directly with our physical network environment, we need to
    add a layer of programmability to the overall scheme. Then, if we want to, we
    can interact with our physical devices and push network configuration to them
    for a more top-to-bottom approach. If we do things this way, we'll need a bit
    more support from our hardware devices in terms of capability and compatibility.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've described what network overlay is, let's talk about VXLAN, one
    of the most prominent overlay network standards. It also serves as a basis for
    developing some other network overlay standards (such as GENEVE), so – as you
    might imagine – it's very important to understand how it works.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Understanding VXLAN
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start with the confusing part. VXLAN (IETF RFC 7348) is an extensible
    overlay network standard that enables us to aggregate and tunnel multiple Layer
    2 networks across Layer 3 networks. How does it do that? By encapsulating a Layer
    2 packet inside a Layer 3 packet. In terms of transport protocol, it uses UDP,
    by default on port `4789` (more about that in just a bit). In terms of special
    requests for VXLAN implementation – as long as your physical network supports
    MTU 1600, you can implement VXLAN as a cloud overlay solution easily. Almost all
    the switches you can buy (except for the cheap home switches, but we're talking
    about enterprises here) support jumbo frames, which means that we can use MTU
    9000 and be done with it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'From the standpoint of encapsulation, let''s see what it looks like:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – VXLAN frame encapsulation'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_01.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 – VXLAN frame encapsulation
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: In more simplistic terms, VXLANs use tunneling between two VXLAN endpoints (called
    VTEPs; that is, VXLAN tunneling endpoints) that check **VXLAN network identifiers**
    (**VNIs**) so that they can decide which packets go where.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: If this seems complicated, then don't worry – we can simplify this. From the
    perspective of VXLAN, a VNI is the same thing as a VLAN ID is to VLAN. It's a
    unique network identifier. The difference is just the size – the VNI field has
    24 bits, compared to VLAN's 12\. That means that we have 2^24 VNIs compared to
    VLAN's 2^12\. So, VXLANs – in terms of network isolation – are VLANs squared.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Why does VXLAN use UDP?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: When designing overlay networks, what you usually want to do is reduce latency
    as much as possible. Also, you don't want to introduce any kind of overhead. When
    you consider these two basic design principles and couple that with the fact that
    VXLAN tunnels Layer 2 traffic inside Layer 3 (whatever the traffic is – unicast,
    multicast, broadcast), that literally means we should use UDP. There's no way
    around the fact that TCP's two methods – three-way handshakes and retransmissions
    – would get in the way of these basic design principles. In the simplest of terms,
    TCP would be too complicated for VXLAN as it would mean too much overhead and
    latency at scale.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of VTEPs, just imagine them as two interfaces (implemented in software
    or hardware) that can encapsulate and decapsulate traffic based on VNIs. From
    a technology standpoint, VTEPs map various tenant''s virtual machines and devices
    to VXLAN segments (VXLAN-backed isolated networks), perform package inspection,
    and encapsulate/decapsulate network traffic based on VNIs. Let''s describe this
    communication with the help of the following diagram:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14834_12_02.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – VTEPs in unicast mode
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: In our open source-based cloud infrastructure, we're going to implement cloud
    overlay networks by using OpenStack Neutron or Open vSwitch, a free, open source
    distributed switch that supports almost all network protocols that you could possibly
    think of, including the already mentioned VXLAN, STT, GENEVE, and GRE overlay
    networks.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Also, there's a kind of gentleman's agreement in place in cloud networking regarding
    not using VXLANs from `1-4999` in most use cases. The reason for this is simple
    – because we still want to have our VLANs with their reserved range of `0-4095`
    in a way that is simple and not error-prone. In other words, by design, we leave
    network IDs `0-4095` for VLANs and start VXLANs with VNI 5000 so that it's really
    easy to differentiate between the two. Not using 5,000 VXLAN-backed networks out
    of 16.7 million VXLAN-backed networks isn't that much of a sacrifice for good
    engineering practices.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplicity, scalability, and extensibility of VXLAN also means more really
    useful usage models, such as the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '**Stretching Layer 2 across sites**: This is one of the most common problems
    regarding cloud networking, as we will describe shortly.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer 2 bridging**: Bridging a VLAN to a cloud overlay network (such as VXLAN)
    is *very* useful when onboarding our users to our cloud services as they can then
    just connect to our cloud network directly. Also, this usage model is heavily
    used when we want to physically insert a hardware device (for example, a physical
    database server or a physical appliance) into a VXLAN. If we didn''t have Layer
    2 bridging, imagine all the pain that we would have. All our customers running
    the Oracle Database Appliance would have no way to connect their physical servers
    to our cloud-based infrastructure.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Various offloading technologies**: These include load balancing, antivirus,
    vulnerability and antimalware scanning, firewall, IDS, IPS integration, and so
    on. All of these technologies enable us to have useful, secure environments with
    simple management concepts.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We mentioned that stretching Layer 2 across sites is a fundamental problem,
    so it's obvious that we need to discuss it. We'll do that next. Without a solution
    to this problem, you'd have very little chance of creating multiple data center
    cloud infrastructures efficiently.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Stretching Layer 2 across sites
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most common sets of problems that cloud providers face is how to
    stretch their environment across sites or continents. In the past, when we didn't
    have concepts such as VXLAN, we were forced to use some kind of Layer 2 VPN or
    MPLS-based technologies. These types of services are really expensive, and sometimes,
    our service providers aren't exactly happy with our *give me MPLS* or *give me
    Layer 2 access* requests. They would be even less happy if we mentioned the word
    *multicast* in the same sentence, and this was a set of technical criteria that
    was *often* used in the past. So, having the capability to deliver Layer 2 over
    Layer 3 fundamentally changes that conversation. Basically, if you have the capability
    to create a Layer 3-based VPN between sites (which you can almost always do),
    you don't have to be bothered with that discussion at all. Also, that significantly
    reduces the price of these types of infrastructure connections.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following multicast-based example:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Extending VXLAN segments across sites in multicast mode'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_03.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.3 – Extending VXLAN segments across sites in multicast mode
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that the left-hand side of this diagram is the first site and that
    the right-hand side of this diagram is the second site. From the perspective of
    `VM1`, it doesn't really matter that `VM4` is in some other remote site as its
    segment (VXLAN 5001) *spans* across those sites. How? As long as the underlying
    hosts can communicate with each other over the VXLAN transport network (usually
    via the management network as well), the VTEPs from the first site can *talk*
    to the VTEPs from the second site. This means that virtual machines that are backed
    by VXLAN segments in one site can talk to the same VXLAN segments in the other
    site by using the aforementioned Layer 2-to-Layer 3 encapsulation. This is a really
    simple and elegant way to solve a complex and costly problem.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设这个图表的左侧是第一个站点，右侧是第二个站点。从`VM1`的角度来看，`VM4`在其他远程站点并不重要，因为它的段（VXLAN 5001）*跨越*这些站点。怎么做？只要底层主机可以通过VXLAN传输网络相互通信（通常也通过管理网络），第一个站点的VTEP可以与第二个站点的VTEP进行*通信*。这意味着在一个站点由VXLAN段支持的虚拟机可以通过上述的第2层到第3层封装与另一个站点中相同的VXLAN段进行通信。这是解决一个复杂且昂贵问题的一个非常简单而优雅的方法。
- en: We mentioned that VXLAN, as a technology, served as a basis for developing some
    other standards, with the most important being GENEVE. As most manufacturers work
    toward GENEVE compatibility, VXLAN will slowly but surely disappear. Let's discuss
    what the purpose of the GENEVE protocol is and how it aims to become *the standard*
    for cloud overlay networking.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到，作为一种技术，VXLAN作为开发其他标准的基础，其中最重要的是GENEVE。随着大多数制造商朝着GENEVE兼容性迈进，VXLAN将慢慢消失。让我们讨论一下GENEVE协议的目的，以及它如何成为云覆盖网络的*标准*。
- en: Understanding GENEVE
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解GENEVE
- en: The basic problem that we touched upon earlier is the fact that history kind
    of repeated itself in cloud overlay networks, as it did many times before. Different
    standards, different firmwares, and different manufacturers supporting one standard
    over another, where all of the standards are incredibly similar but still not
    compatible with each other. That's why VMware, Microsoft, Red Hat, and Intel proposed
    GENEVE, a new cloud overlay standard that only defines the encapsulation data
    format, without interfering with the control planes of these technologies, which
    are fundamentally different. For example, VXLAN uses a 24-bit field width for
    VNI, while STT uses 64-bit. So, the GENEVE standard proposes no fixed field size
    as you can't possibly know what the future brings. Also, taking a look at the
    existing user base, we can still happily use our VXLANs as we don't believe that
    they will be influenced by future GENEVE deployments.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到的基本问题是，云覆盖网络中的历史重演了很多次。不同的标准，不同的固件，不同的制造商支持一种标准而不是另一种，所有这些标准都非常相似，但仍然不兼容。这就是为什么VMware、微软、红帽和英特尔提出了GENEVE，这是一个新的云覆盖标准，只定义封装数据格式，而不干涉这些技术的控制平面，这些技术在根本上是不同的。例如，VXLAN使用24位字段宽度的VNI，而STT使用64位。因此，GENEVE标准不提出固定的字段大小，因为你不可能知道未来会发生什么。此外，从现有用户群的角度来看，我们仍然可以愉快地使用我们的VXLAN，因为我们不认为它们会受到未来GENEVE部署的影响。
- en: 'Let''s see what the GENEVE header looks like:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看GENEVE标头是什么样的：
- en: '![Figure 12.4 – GENEVE cloud overlay network header'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.4-GENEVE云覆盖网络标头'
- en: '](img/B14834_12_04.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_04.jpg)'
- en: Figure 12.4 – GENEVE cloud overlay network header
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4-GENEVE云覆盖网络标头
- en: The authors of GENEVE learned from some other standards (BGP, IS-IS, and LLDP)
    and decided that the key to doing things right is extensibility. This is why it
    was embraced by the Linux community in Open vSwitch and VMware in NSX-T. VXLAN
    is supported as the network overlay technology for **Hyper-V Network Virtualization**
    (**HNV**) since Windows Server 2016 as well. Overall, GENEVE and VXLAN seem to
    be two technologies that are surely here to stay – and both are supported nicely
    from the perspective of OpenStack.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GENEVE的作者从其他一些标准（BGP、IS-IS和LLDP）中学到了一些东西，并认为做正确的事情的关键是可扩展性。这就是为什么它被Linux社区在Open
    vSwitch和VMware在NSX-T中采用。自Windows Server 2016以来，VXLAN也被支持为**Hyper-V网络虚拟化**（**HNV**）的网络覆盖技术。总的来说，GENEVE和VXLAN似乎是两种肯定会留下来的技术-从OpenStack的角度来看，两者都得到了很好的支持。
- en: Now that we've covered the most basic problem regarding the cloud – cloud networking
    – we can go back and discuss OpenStack. Specifically, our next subject is related
    to OpenStack components – from Nova through to Glance and then to Swift, and others.
    So, let's get started.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解决了云的最基本问题-云网络-我们可以回过头来讨论OpenStack。具体来说，我们下一个主题与OpenStack组件有关-从Nova到Glance，然后到Swift，以及其他组件。所以，让我们开始吧。
- en: OpenStack components
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack组件
- en: 'When OpenStack was first formed as a project, it was designed from two different
    services:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当OpenStack最初作为一个项目形成时，它是从两种不同的服务设计的：
- en: A computing service that was designed to manage and run virtual machines themselves
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个计算服务，旨在管理和运行虚拟机本身
- en: A storage service that was designed for large-scale object storage
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个旨在进行大规模对象存储的存储服务
- en: These services are now called OpenStack Compute or *Nova*, and OpenStack Object
    Store or *Swift*. These services were later joined by *Glance* or the OpenStack
    Image service, which was designed to simplify working with disk images. Also,
    after our SDN primer, we need to discuss OpenStack Neutron, the **Network-as-a-Service**
    (**NaaS**) component of OpenStack.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '这些服务现在被称为OpenStack Compute或*Nova*，以及OpenStack Object Store或*Swift*。这些服务后来又加入了*Glance*或OpenStack镜像服务，旨在简化与磁盘映像的工作。此外，在我们的SDN入门之后，我们需要讨论OpenStack
    Neutron，OpenStack的**网络即服务**（**NaaS**）组件。 '
- en: 'The following diagram shows the components of OpenStack:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了OpenStack的组件：
- en: '![Figure 12.5 – Conceptual architecture of OpenStack (source: https://docs.openstack.org/)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.5-OpenStack的概念架构（来源：https://docs.openstack.org/）'
- en: '](img/B14834_12_05.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_05.jpg)'
- en: 'Figure 12.5 – Conceptual architecture of OpenStack (source: https://docs.openstack.org/)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5-OpenStack的概念架构（来源：https://docs.openstack.org/）
- en: We'll go through these in no particular order and will include additional services
    that are important. Let's start with **Swift**.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照没有特定顺序进行介绍，并包括其他重要的服务。让我们从**Swift**开始。
- en: Swift
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Swift
- en: 'The first service we need to talk about is Swift. For that purpose, we are
    going to grab the project''s own definition from the OpenStack official documentation
    and parse it to try and explain what services are fulfilled by this project, [and
    what is it used for. The Swift webs](https://docs.openstack.org/swift/latest/)ite
    ([https://docs.openstack.org/swift/latest/](https://docs.openstack.org/swift/latest/))
    states the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要谈论的第一个服务是Swift。为此，我们将从OpenStack官方文档中获取项目的定义，并解析它，以尝试解释这个项目实现了哪些服务，[以及它的用途。Swift网站](https://docs.openstack.org/swift/latest/)
    ([https://docs.openstack.org/swift/latest/](https://docs.openstack.org/swift/latest/))中陈述了以下内容：
- en: '*"Swift is a highly available, distributed, eventually consistent object/blob
    store. Organizations can use Swift to store lots of data efficiently, safely,
    and cheaply. It''s built for scale and optimized for durability, availability,
    and concurrency across the entire dataset. Swift is ideal for storing unstructured
    data that can grow without bounds."*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*"Swift是一个高度可用的、分布式的、最终一致的对象/大块存储。组织可以使用Swift高效、安全、廉价地存储大量数据。它专为规模而构建，并针对整个数据集的耐用性、可用性和并发性进行了优化。Swift非常适合存储可以无限增长的非结构化数据。"*'
- en: Having read that, we need to point out quite a few things that may be completely
    new to you. First and foremost, we are talking about storing data in a particular
    way that is not common in computing unless you have used unstructured data stores.
    Unstructured does not mean that this way of storing data is lacking structure;
    in this context, it means that we are the ones that are defining the structure
    of the data, but the service itself does not care about our structure, instead
    relying on the concept of objects to store our data. One result of this is something
    that may also sound unusual at first, and that is that the data we store in Swift
    is not directly accessible through any filesystem, or any other way we are used
    to manipulating files through our machines. Instead, we are manipulating data
    as objects and we must use the API that is provided as part of Swift to get the
    data objects. Our data is stored in *blobs*, or objects, that the system itself
    just labels and stores to take care of availability and access speed. We are supposed
    to know what the internal structure of our data is and how to parse it. On the
    other hand, because of this approach, Swift can be amazingly fast with any amount
    of data and scales horizontally in a way that is almost impossible to achieve
    using normal, classic databases.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 读完这段话后，我们需要指出一些可能对你来说完全新的事情。首先，我们谈论的是以一种特定的方式存储数据，这在计算中并不常见，除非你使用过非结构化数据存储。非结构化并不意味着这种存储数据的方式缺乏结构；在这个上下文中，它意味着我们定义数据的结构，但服务本身不关心我们的结构，而是依赖于对象的概念来存储我们的数据。这种方式的一个结果是，我们存储在Swift中的数据不能直接通过任何文件系统或我们习惯通过机器操作文件的其他方式直接访问。相反，我们是以对象的方式操作数据，必须使用Swift提供的API来获取数据对象。我们的数据存储在*大块*或对象中，系统本身只是标记和存储以确保可用性和访问速度。我们应该知道我们的数据的内部结构以及如何解析它。另一方面，由于这种方法，Swift可以以惊人的速度处理任意数量的数据，并且以一种几乎不可能使用普通的经典数据库实现的方式进行水平扩展。
- en: 'Another thing worth mentioning is that this service offers highly available,
    distributed, and *eventually consistent* storage. This means that, first and foremost,
    the priority is for the data to be distributed and highly available, which are
    two things that are important in the cloud. Consistency comes after that but is
    eventually achieved. Once you come to use this service, you will understand what
    that means. In almost all usual scenarios where data is read and rarely written,
    it is nothing to even think about, but there are some cases where this can change
    the way we need to think about the way we go about delivering the service. The
    documentation states the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，这项服务提供了高度可用、分布式和*最终一致*的存储。这意味着，首先，优先级是数据分布和高可用性，这两点在云中非常重要。一致性在此之后，但最终会实现。一旦你开始使用这项服务，你就会明白这意味着什么。在几乎所有通常的情况下，数据被读取而很少被写入，这根本不值得考虑，但也有一些情况下，这可能改变我们需要思考如何提供服务的方式。文档中陈述了以下内容：
- en: '*"Because each replica in Object Storage functions independently and clients
    generally require only a simple majority of nodes to respond to consider an operation
    successful, transient failures such as network partitions can quickly cause replicas
    to diverge. These differences are eventually reconciled by asynchronous, peer-to-peer
    replicator processes. The replicator processes traverse their local filesystems
    and concurrently perform operations in a manner that balances load across physical
    disks."*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*"因为对象存储中的每个副本都是独立运行的，客户端通常只需要大多数节点简单响应即可认为操作成功，瞬时故障如网络分区可能会迅速导致副本发散。这些差异最终由异步的点对点复制进程协调一致。复制进程遍历其本地文件系统，并以一种平衡负载的方式同时执行操作。"*'
- en: We can roughly translate this. Let's say that you have a three-node Swift cluster.
    In such a scenario, a Swift object will become available to clients after the
    `PUT` operation has been confirmed to have been completed on at least two nodes.
    So, if your goal is to create a low-latency, synchronous storage replication with
    Swift, there are other solutions available for that.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以粗略地翻译一下。假设你有一个三节点的Swift集群。在这种情况下，Swift对象在`PUT`操作至少在两个节点上确认已完成后，才会对客户端可用。因此，如果你的目标是创建一个低延迟、同步的Swift存储复制，那么还有其他解决方案可供选择。
- en: Having put aside all the abstract promises regarding what Swift offers, let's
    go into more details. High availability and distribution are the direct result
    of using a concept of *zones* and having multiple copies of the same data written
    onto multiple storage servers. Zones are nothing but a simple way of logically
    dividing the storage resources we have at our disposal and deciding on what kind
    of isolation we are ready to provide, as well as what kind of redundancy we need.
    We can group servers by the server itself, by the rack, by sets of servers across
    a Datacenter, in groups across different Datacenters, and in any combination of
    those. Everything really depends on the amount of available resources and the
    data redundancy and availability we need and want, as well as, of course, the
    cost that will accompany our configuration.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在搁置了有关Swift提供的所有抽象承诺之后，让我们进一步详细讨论一下。高可用性和分布是使用“区域”概念以及将相同数据写入多个存储服务器的直接结果。区域只是一种简单的逻辑划分我们可用的存储资源的方式，并决定我们愿意提供的隔离类型以及我们需要的冗余类型。我们可以按照服务器本身、机架、数据中心内的服务器集、跨不同数据中心的组以及任何这些的组合来对服务器进行分组。一切都取决于可用资源的数量以及我们需要和想要的数据冗余和可用性，当然还有伴随我们配置的成本。
- en: Based on the resources we have, we are supposed to configure our storage system
    in terms of how many copies it will hold and how many zones we are prepared to
    use. A copy of a particular data object in Swift is referred to as a *replica*,
    and currently, the best practices call for at least three replicas across no less
    than five zones.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们拥有的资源，我们应该根据它将我们的存储系统配置为它将保存多少副本以及我们准备使用多少区域。Swift中特定数据对象的副本被称为*副本*，目前，最佳实践要求至少在不少于五个区域中拥有至少三个副本。
- en: A zone can be a server or a set of servers, and if we configure everything correctly,
    losing any one zone should have no impact on the availability or distribution
    of data. Since a zone can be as small as a server and as big as any number of
    data centers, the way we structure our zones has a huge impact on the way the
    system reacts to any failures and changes. The same goes for replicas. In the
    recommended scenario, configuration has a smaller number of replicas than the
    number of zones, so only some of the zones will hold some of these replicas. This
    means the system must balance the way data is written in order to evenly distribute
    both the data and the load, including both the writing and the reading load for
    the data. At the same time, the way we structure the zones will have an enormous
    impact on the cost – redundancy has a real cost in terms of server and storage
    hardware, and multiplying replicas and zones creates additional demands in regard
    to how much storage and computing power we need to allocate for our OpenStack
    installation. Being able to do this correctly is the biggest problem that a Datacenter
    architect has to solve.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 区域可以是服务器或一组服务器，如果我们正确配置了一切，失去任何一个区域对数据的可用性或分布都不会产生影响。由于区域可以小到一个服务器，大到任意数量的数据中心，我们构建区域的方式对系统对任何故障和变化的反应有巨大影响。副本也是如此。在推荐的方案中，配置的副本数量比区域数量少，因此只有一些区域将持有这些副本。这意味着系统必须平衡数据的写入方式，以均匀分布数据和负载，包括数据的写入和读取负载。同时，我们构建区域的方式将对成本产生巨大影响-冗余在服务器和存储硬件方面具有实际成本，而增加副本和区域会对我们OpenStack安装需要分配多少存储和计算能力提出额外要求。能够正确做到这一点是数据中心架构师必须解决的最大问题。
- en: Now, we need to go back to the concept of eventual consistency. Eventual consistency
    in this context means that data is going to be written to the Swift store and
    that objects are going to get updated, but the system will not be able to do a
    completely simultaneous write of all the data into all the copies (replicas) of
    the data across all zones. Swift will try to reconcile the differences as soon
    as possible and will be aware of these changes, so it serves new versions of the
    objects to whoever tries to read them. Scenarios where data is inconsistent due
    to a failure of some part of the system exist, but they are to be considered abnormal
    states of the system and need to be repaired rather than the system being designed
    to ignore them.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要回到最终一致性的概念。在这个背景下，最终一致性意味着数据将被写入Swift存储，并且对象将被更新，但系统将无法完全同时将所有数据写入所有副本中。Swift将尽快协调差异，并意识到这些变化，因此为尝试读取它们的任何人提供对象的新版本。由于系统的某些部分失败而导致数据不一致的情况存在，但它们应被视为系统的异常状态，并需要修复，而不是系统被设计为忽略它们。
- en: Swift daemons
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Swift守护程序
- en: 'Next, we need to talk about the way Swift is designed in regard to its architecture.
    Data is managed through three separate logical daemons:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要讨论Swift在架构方面的设计。数据通过三个独立的逻辑守护程序进行管理：
- en: '**Swift-account** is used to manage a SQL database that contains all the accounts
    defined with the object storage service. Its main task is to read and write the
    data that all the other services need, primarily in order to validate and find
    appropriate authentication and other data.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Swift-account**用于管理包含所有定义的对象存储服务帐户的SQL数据库。它的主要任务是读取和写入所有其他服务需要的数据，主要是为了验证和查找适当的身份验证和其他数据。'
- en: '**Swift-container** is another database process, but it is used strictly to
    map data into containers, a logical structure similar to AWS *buckets*. This can
    include any number of objects that are grouped together.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Swift-container**是另一个数据库进程，但它严格用于将数据映射到容器中，这是一种类似于AWS *buckets*的逻辑结构。这可以包括任意数量的对象，它们被分组在一起。'
- en: '**Swift-object** manages mapping to actual objects, and it keeps track of the
    location and availability of the objects themselves.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Swift-object**管理到实际对象的映射，并跟踪对象本身的位置和可用性。'
- en: 'All these daemons are just in charge of data and make sure that everything
    is both mapped and replicated correctly. Data is used by another layer in the
    architecture: the presentation layer.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: When a user wants to use any data object, it first needs to authenticate via
    a token that can be either externally provided or created by an authentication
    system inside Swift. After that, the main process that orchestrates data retrieval
    is Swift-proxy, which handles communication with three daemons that deal with
    the data. Provided that the user presented a valid token, it gets the data object
    delivered to the user request.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: This is just the briefest of overviews regarding how Swift works. In order to
    understand this, you need to not only read the documentation but also use some
    kind of system that will perform low-level object retrieval and storage into and
    out of Swift.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Cloud services can't be scaled or used efficiently if we don't have orchestration
    services, which is why we need to discuss the next service on our list – **Nova**.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Nova
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another important service or project is Nova – an orchestration service that
    is used for providing both provisioning and management for computing instances
    at a large scale. What it basically does is allow us to use an API structure to
    directly allocate, create, reconfigure, and delete or *destroy* virtual servers.
    The following is a diagram of a logical Nova service structure:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Logical structure of the Nova service (openstack.org)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_06.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.6 – Logical structure of the Nova service (openstack.org)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Most of Nova is a very complex distributed system written almost entirely in
    Python that consists of a number of working scripts that do the orchestration
    part and a gateway service that receives and carries through API calls. The API
    is also based on Python; it's a **Web Server Gateway Interface** (**WSGI**)-compatible
    application that handles calls. WSGI, in turn, is a standard that defines how
    a web application and a server should exchange data and commands. This means that,
    in theory, any system capable of using the WSGI standard can also establish communication
    with this service.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Aside from this multifaceted orchestration solution, there are two more services
    that are at the heart of Nova – the database and messaging queue. Neither of these
    is Python-based. We'll talk about messaging and databases first.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Almost all distributed systems must rely on queues to be able to perform their
    tasks. Messages need to be forwarded to a central place that will enable all daemons
    to do their tasks, and using the right messaging and queueing system is crucial
    for system speed and reliance. Nova currently uses RabbitMQ, a highly scalable
    and available system by itself. Using a production-ready system like this means
    that not only are there tools to debug the system itself, but there are a lot
    of reporting tools available for directly querying the messaging queue.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: The main purpose of using a messaging queue is to completely decouple any clients
    from servers, and to provide asynchronous communication between different clients.
    There is a lot to be said on how the actual messaging works, but for this chapter,
    we will just refer you to the official documentation at [https://docs.openstack.org/nova/latest/](https://docs.openstack.org/nova/latest/),
    since we are not talking about a couple of functions on a server but an entirely
    independent software stack.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: The database is in charge of holding all the state data for the tasks currently
    being performed, as well as enabling the API to return information about the current
    state of different parts of Nova.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'All in all, the system consists of the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '`nova-api` is actually referring to this daemon, sometimes calling it just
    *API*, *controller*, or *cloud controller*. We need to explain a little bit more
    about Nova in order to understand that calling nova-api a controller is wrong,
    but since there exists a class inside a daemon named `CloudController`, a lot
    of users confuse this daemon for the whole distributed system.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nova-api`实际上是指这个守护程序，有时称其为API、控制器或云控制器。我们需要更详细地解释一下Nova，以便理解将nova-api称为控制器是错误的，但由于守护程序中存在一个名为CloudController的类，许多用户将这个守护程序误认为是整个分布式系统。'
- en: nova-api is a powerful system since it can, by itself, process and sort out
    some API calls, getting the data from the database and working out what needs
    to be done. In a more common case, nova-api will just initiate a task and forward
    it in the form of messages to other daemons inside Nova.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: nova-api是一个强大的系统，因为它可以自行处理和整理一些API调用，从数据库获取数据并找出需要做什么。在更常见的情况下，nova-api将只是启动一个任务，并以消息的形式转发给Nova内的其他守护程序。
- en: Another important daemon is the **scheduler**. Its main function is to go through
    the queue and determine when and where a particular request should run. This sounds
    simple enough, but given the possible complexity of the system, this *where and
    when* can lead to extreme gains or losses in performance. In order to solve this,
    we can choose how the scheduler makes decisions regarding choosing the right place
    to perform requests. Users can choose either to write their own request or to
    use one of the predetermined ones.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个重要的守护程序是调度程序。它的主要功能是浏览队列，并确定特定请求应该在何时何地运行。这听起来足够简单，但鉴于系统可能的复杂性，这个“何时何地”可能导致性能的极端增益或损失。为了解决这个问题，我们可以选择调度程序在选择执行请求的正确位置时如何做出决策。用户可以选择编写自己的请求，也可以使用预定的请求。
- en: 'If we are choosing the ones provided by Nova, we have three choices:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择Nova提供的存储卷，我们有三种选择：
- en: a) **Simple scheduler** determines where the request will be run based on the
    load on the hosts – it will monitor all the hosts and try to allocate the one
    that has the least load in a particular slice of time.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: a) 简单调度程序根据主机的负载确定请求将在哪里运行 - 它将监视所有主机，并尝试分配在特定时间片内负载最小的主机。
- en: b) **Chance** is the default way of scheduling. As its name suggests, it's the
    simplest algorithm – a host is randomly chosen from the list and given the request.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: b) **Chance**是默认的调度方式。顾名思义，这是最简单的算法 - 从列表中随机选择一个主机并给出请求。
- en: c) **Zone scheduling** will also randomly choose a host but will do so from
    within a zone.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: c) 区域调度也会随机选择主机，但是会在区域内进行选择。
- en: 'Now, we will look at *workers*, daemons that actually perform requests. There
    are three of these – network, volume, and compute:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看一下*workers*，实际执行请求的守护程序。这些守护程序有三个 - 网络、存储和计算。
- en: '**nova-network** is in charge of the network. It will perform whatever is given
    to it from the queue that is related to anything on the network and will create
    interfaces and rules as needed. It is also in charge of IP address allocation;
    it will allocate both fixed and dynamically assigned addresses and take care of
    both the external and internal networks. Instances usually use one or more fixed
    IPs to enable management and connectivity, and these are usually local addresses.
    There are also floating addresses to enable connecting from the outside. This
    service has been obsolete since the OpenStack Newton release from 2016, although
    you can still use it in some legacy configurations.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nova-network**负责网络。它将执行与网络相关的队列中的任何任务，并根据需要创建接口和规则。它还负责IP地址分配；它将分配固定和动态分配的地址，并处理外部和内部网络。实例通常使用一个或多个固定IP来实现管理和连接，这些通常是本地地址。还有浮动地址用于从外部进行连接。自2016年OpenStack
    Newton版本发布以来，这项服务已经过时，尽管在一些传统配置中仍然可以使用。'
- en: '**nova-volume** handles storage volumes or, to be more precise, all the ways
    data storage can be connected to any instance. This includes standards such as
    iSCSI and AoE, which are targeted at encapsulating known common protocols, and
    providers such as Sheepdog, LeftHand, and RBD, which cover connections to open
    source and closed source storage systems such as CEPH or HP LeftHand.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: nova-volume处理存储卷，或者更准确地说，处理数据存储与任何实例连接的所有方式。这包括诸如iSCSI和AoE之类的标准，这些标准旨在封装已知的常见协议，以及诸如Sheepdog、LeftHand和RBD之类的提供者，这些提供者涵盖了与开源和闭源存储系统（如CEPH或HP
    LeftHand）的连接。
- en: '`nova-compute` must adapt itself to using different virtualization technologies
    and to completely different platforms. It also needs to be able to dynamically
    allocate and free resources. Primarily, it uses libvirt for its VM management,
    directly supporting KVM for creating and deleting new instances. This is the reason
    this chapter exists, since nova-compute using libvirt to start KVM machines is
    by far the most common way of configuring OpenStack, but support for different
    technologies extends a lot further. The libvirt interface also supports Xen, QEMU,
    LXC, and **user mode Linux** (**UML**), and through different APIs, nova-compute
    can support Citrix, XCP, VMware ESX/ESXi vSphere, and Microsoft Hyper-V. This
    enables Nova to control all the currently used enterprise virtualization solutions
    from one central API.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nova-compute`必须适应不同的虚拟化技术和完全不同的平台。它还需要能够动态分配和释放资源。主要使用libvirt进行VM管理，直接支持KVM创建和删除新实例。这就是本章存在的原因，因为nova-compute使用libvirt启动KVM机器是配置OpenStack的最常见方式，但对不同技术的支持范围更广。libvirt接口还支持Xen、QEMU、LXC和用户模式Linux（UML），通过不同的API，nova-compute可以支持Citrix、XCP、VMware
    ESX/ESXi vSphere和Microsoft Hyper-V。这使得Nova能够从一个中央API控制所有当前使用的企业虚拟化解决方案。'
- en: As a side note, **nova-conductor** is there to process requests that require
    any conversion regarding objects, resizing, and database/proxy access.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个旁注，nova-conductor用于处理需要对对象、调整大小和数据库/代理访问进行任何转换的请求。
- en: The next service on our list is **Glance** – a service that is very important
    for virtual machine deployment as we want to do this from images. Let's discuss
    Glance now.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Glance
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At first, having a separate service for cloud disk image management makes little
    sense, but when scaling any infrastructure, image management will become a problem
    that needs an API to be solved. Glance basically has this dual identity – it can
    be used to directly manipulate VM images and store them inside blobs of data,
    but at the same time it can be used to completely automatically orchestrate a
    lot of tasks when dealing with a huge number of images.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Glance is relatively simple in terms of its internal structure as it consists
    of an image information database, an image store that uses Swift (or a similar
    service), and an API that glues everything together. Database is sometimes called
    Registry, and it basically gives information about a given image. Images themselves
    can be stored on different types of stores, either from Swift (as blobs) on HTTP
    servers or on a filesystem (such as NFS).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Glance is completely nonspecific about the type of image store it uses, so NFS
    is perfectly okay and makes implementing OpenStack a little bit easier, but when
    scaling OpenStack, both Swift and Amazon S3 can be used.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: When thinking about the place in the big OpenStack puzzle that Glance belongs
    to, we could describe it as being the service that Nova uses to find and instantiate
    images. Glance itself uses Swift (or any other storage) to store images. Since
    we are dealing with multiple architectures, we need a lot of different supported
    file formats for images, and Glance does not disappoint. Every disk format that
    is supported by different virtualization engines is supported by Glance. This
    includes both unstructured formats such as `raw` and structured formats such as
    VHD, VMDK, `qcow2`, VDI ISO, and AMI. OVF – as an example of an image container
    – is also supported.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Glance probably has the simplest API of them all, enabling it to be used even
    from the command line using curl to query the server and JSON as the format of
    the messages.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll finish this section with a small note directly from the Nova documentation:
    it explicitly states that everything in OpenStack is designed to be horizontally
    scalable but that, at any time, there should be significantly more computing nodes
    than any other type. This actually makes a lot of sense – computing nodes are
    the ones in charge of actually accepting and working on requests. The amount of
    storage nodes you''ll need will depend on your usage scenario, and Glance''s will
    inevitably depend on the capabilities and resources available to Swift.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: The next service in line is **Horizon** – a *human-readable* GUI dashboard of
    OpenStack where we *consume* a lot of OpenStack visual information.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Horizon
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having explained the core services that enable OpenStack to do what it does
    the way it does in some detail, we need to address the user interaction. In almost
    every paragraph in this chapter, we refer to APIs and scripting interfaces as
    a way to communicate and orchestrate OpenStack. While this is completely true
    and is the usual way of managing large-scale deployments, OpenStack also has a
    pretty useful interface that is available as a web service in a browser. The name
    of this project is Horizon, and its sole purpose is to provide a user with a way
    of interacting with all the services from one place, called the dashboard. Users
    can also reconfigure most, if not all, the things in the OpenStack installation,
    including security, networking, access rights, users, containers, volumes, and
    everything else that exists in the OpenStack installation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Horizon also supports plugins and *pluggable panels*. There is an active plugin
    marketplace for Horizon that aims at extending its functionality even further
    than it already has. If that's still not enough for your particular scenario,
    you can create your own plugins in Angular and get them to run in Horizon.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Horizon还支持插件和*可插拔面板*。Horizon有一个活跃的插件市场，旨在扩展其功能，甚至比它已经具有的功能更进一步。如果这对您的特定情况仍然不够，您可以使用Angular创建自己的插件，并让它们在Horizon中运行。
- en: Pluggable panels are also a nice idea – without changing any defaults, a user
    or a group of users can change the way the dashboard looks and get more (or less)
    information presented to them. All of this requires a little bit of coding; changes
    are made in the config files, but the main thing is that the Horizon system itself
    supports such a customization model. You can find out more about the interface
    itself and the functions that are available to the user when we cover installing
    OpenStack and creating OpenStack instances in the *Provisioning the OpenStack
    environment* section.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 可插拔面板也是一个不错的主意 - 在不改变任何默认设置的情况下，用户或一组用户可以改变仪表板的外观，并获得更多（或更少）呈现给他们的信息。所有这些都需要一点编码；更改是在配置文件中进行的，但主要的是Horizon系统本身支持这样的定制模型。在我们讨论安装OpenStack和创建OpenStack实例的*配置OpenStack环境*部分时，您可以了解更多关于界面本身和用户可用的功能。
- en: As you are aware, networks don't really work all that well without name resolution,
    which is why OpenStack has a service called **Designate**. We'll briefly discuss
    Designate next.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所知，没有名称解析，网络实际上无法正常工作，这就是为什么OpenStack有一个名为**Designate**的服务。我们接下来会简要讨论Designate。
- en: Designate
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指定
- en: Every system that uses any kind of network must have at least some kind of name
    resolution service in the form of a local or remote DNS or a similar mechanism.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 任何使用任何类型网络的系统都必须至少有某种形式的名称解析服务，如本地或远程DNS或类似的机制。
- en: Designate is a service that tries to integrate the *DNSaaS* concept in OpenStack
    in one place. When connected to Nova and Neutron, it will try to keep up-to-date
    records in regards to all the hosts and infrastructure details.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Designate是一项服务，试图在OpenStack中整合*DNSaaS*概念。当连接到Nova和Neutron时，它将尝试保持有关所有主机和基础设施详细信息的最新记录。
- en: Another very important aspect of the cloud is how we manage identities. For
    that specific purpose, OpenStack has a service called **Keystone**. We'll discuss
    what it does next.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 云的另一个非常重要的方面是我们如何管理身份。为此，OpenStack有一个名为**Keystone**的服务。我们接下来会讨论它的作用。
- en: Keystone
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keystone
- en: Identity management is a big thing in cloud computing, simply because when deploying
    a large-scale infrastructure, not only do you need a way to scale your resources,
    but you also need a way to scale user management. A simple list of users that
    can access a resource is not an option anymore, mainly because we are not talking
    about simple users anymore. Instead, we are talking about domains containing thousands
    of users separated by groups and by roles – we are talking about multiple ways
    of logging in and providing authentication and authorization. Of course, this
    also can span multiple standards for authentication, as well as multiple specialized
    systems.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 身份管理在云计算中非常重要，因为在部署大规模基础设施时，不仅需要一种方式来扩展资源，还需要一种方式来扩展用户管理。简单的用户访问资源的列表不再是一个选择，主要是因为我们不再谈论简单的用户。相反，我们谈论包含成千上万用户的域，由组和角色分隔
    - 我们谈论多种登录和提供身份验证和授权的方式。当然，这也可能涉及多种身份验证标准，以及多个专门的系统。
- en: For these reasons, user management is a separate project/service in OpenStack
    named Keystone.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因，用户管理是OpenStack中名为Keystone的一个独立项目/服务。
- en: Keystone supports simple user management and the creation of users, groups,
    and roles, but it also supports LDAP, Oauth, OpenID Connect, SAML, and SQL database
    authentication and has its own API that can support every possible scenario for
    user management. Keystone is in a world by itself, and in this book, we will treat
    it as a simple user provider. However, it can be much more and can require a lot
    of configuration, depending on the case. The good thing is that, once installed,
    you will rarely need to think about this part of OpenStack.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Keystone支持简单的用户管理和用户、组和角色的创建，但它也支持LDAP、Oauth、OpenID Connect、SAML和SQL数据库身份验证，并且有自己的API，可以支持用户管理的各种场景。Keystone是一个独立的世界，在本书中，我们将把它视为一个简单的用户提供者。然而，它可以是更多，可能需要根据情况进行大量配置。好消息是，一旦安装好，你很少需要考虑OpenStack的这一部分。
- en: The next service on our list is **Neutron**, the API/backend for (cloud) networking
    in OpenStack.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列表上的下一个服务是**Neutron**，这是OpenStack中（云）网络的API/后端。
- en: Neutron
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Neutron
- en: "OpenStack Neutron is an API-based service that aims to provide a simple and\
    \ extensible cloud network concept as a development of what used to be called\
    \ a *Quantum* service \Lin older releases of OpenStack. Before this service, networking\
    \ was managed by nova-network, which, as we mentioned, is a solution that's obsolete,\
    \ with Neutron being the reason for this. Neutron integrates with some of the\
    \ services that we've already discussed – Nova, Horizon, and Keystone. As a standalone\
    \ concept, we can deploy Neutron to a separate server, which will then give us\
    \ the ability to use the Neutron API. This is reminiscent of what VMware does\
    \ in NSX with the NSX Controller concept."
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack Neutron是一个基于API的服务，旨在提供一个简单且可扩展的云网络概念，作为OpenStack旧版本中称为*Quantum*服务的发展。在这项服务之前，网络是由nova-network管理的，正如我们提到的，这是一个已经过时的解决方案，而Neutron正是这一变化的原因。Neutron与我们已经讨论过的一些服务集成
    - Nova、Horizon和Keystone。作为一个独立的概念，我们可以部署Neutron到一个单独的服务器，然后就可以使用Neutron API。这让人想起了VMware在NSX中使用NSX
    Controller概念的做法。
- en: 'When we deploy neutron-server, a web-based service that hosts the API connects
    to the Neutron plugin in the background so that we can introduce networking changes
    to our Neutron-managed cloud network. In terms of architecture, it has the following
    services:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们部署neutron-server时，一个托管API的基于Web的服务会与Neutron插件后台连接，以便我们可以对我们的Neutron管理的云网络进行网络更改。在架构方面，它有以下服务：
- en: Database for persistent storage
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于持久存储的数据库
- en: neutron-server
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: neutron-server
- en: External agents (plugins) and drivers
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部代理（插件）和驱动程序
- en: 'In terms of plugins, it has a *lot* of them, but here''s a short list:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在插件方面，它有*很多*，但这里是一个简短的列表：
- en: Open vSwitch
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Open vSwitch
- en: Cisco UCS/Nexus
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cisco UCS/Nexus
- en: The Brocade Neutron plugin
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brocade Neutron插件
- en: IBM SDN-VE
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM SDN-VE
- en: VMware NSX
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware NSX
- en: Juniper OpenContrail
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Juniper OpenContrail
- en: Linux bridging
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux bridging
- en: ML2
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML2
- en: Many others
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有很多其他的
- en: Most of these plugin names are logical, so you won't have any problems understanding
    what they do. But we'd like to mention one of these plugins specifically, which
    is the **Modular Layer 2** (**ML2**) plugin.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些插件名称都是逻辑的，所以你不会有任何问题理解它们的作用。但我们想特别提到其中一个插件，即**Modular Layer 2**（**ML2**）插件。
- en: By using the ML2 plugin, OpenStack Neutron can connect to various Layer 2 backends
    – VLAN, GRE, VXLAN, and so on. It also enables Neutron to go away from the Open
    vSwitch and Linux bridge plugins as its basic plugins (which are now obsolete).
    These plugins are considered to be too monolithic for Neutron's modular architecture,
    and ML2 has replaced them completely since the release of Havana (2013). ML2 today
    has many vendor-based plugins for integration. As shown by the preceding list,
    Arista, Cisco, Avaya, HP, IBM, Mellanox, and VMware all have ML2-based plugins
    for OpenStack.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用ML2插件，OpenStack Neutron可以连接到各种第2层后端 - VLAN、GRE、VXLAN等。它还使Neutron摆脱了Open
    vSwitch和Linux桥插件作为其基本插件（现在已经过时）。这些插件被认为对于Neutron的模块化架构来说过于庞大，自2013年Havana发布以来，ML2已完全取代了它们。今天，ML2有许多基于供应商的插件用于集成。正如前面的列表所示，Arista、Cisco、Avaya、HP、IBM、Mellanox和VMware都有基于ML2的OpenStack插件。
- en: 'In terms of network categories, Neutron supports two:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 关于网络类别，Neutron支持两种：
- en: '**Provider networks**: Created by an OpenStack administrator, these are used
    for external connections on a physical level, which are usually backed by flat
    (untagged) or VLAN (802.1q tagged) concepts. These networks are shared since tenants
    use them to access their private infrastructure in hybrid cloud models or to access
    the internet. Also, these networks describe the way underlay and overlay networks
    interact, as well as their mappings.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供者网络**：由OpenStack管理员创建，用于物理级别的外部连接，通常由平面（未标记）或VLAN（802.1q标记）概念支持。这些网络是共享的，因为租户使用它们来访问他们的私有基础设施在混合云模型中或访问互联网。此外，这些网络描述了底层和覆盖网络的交互方式，以及它们的映射关系。'
- en: '**Tenant networks**, **self-service networks**, **project networks**: These
    networks are created by users/tenants and their administrators so that they can
    connect their virtual resources and networks in whatever shape or form they need.
    These networks are isolated and usually backed by a network overlay such as GRE
    or VXLAN, as that''s the whole purpose of tenant networks.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**租户网络**，**自助服务网络**，**项目网络**：这些网络是由用户/租户及其管理员创建的，以便他们可以连接他们需要的任何形状或形式的虚拟资源和网络。这些网络是隔离的，通常由GRE或VXLAN等网络覆盖层支持，因为这就是租户网络的整个目的。'
- en: Tenant networks usually use some kind of SNAT mechanism to access external networks,
    and this service is usually implemented via virtual routers. The same concept
    is used in other cloud technologies such as VMware NSX-v and NSX-t, as well as
    Microsoft Hyper-V SDN technologies backed by Network Controller.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 租户网络通常使用某种SNAT机制来访问外部网络，这项服务通常通过虚拟路由器实现。同样的概念也适用于其他云技术，如VMware NSX-v和NSX-t，以及由网络控制器支持的Microsoft
    Hyper-V SDN技术。
- en: 'In terms of network types, Neutron supports multiple types:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络类型方面，Neutron支持多种类型：
- en: '**Local**: Allows us to communicate within the same host.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Local**：允许我们在同一主机内进行通信。'
- en: '**Flat**: An untagged virtual network.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Flat**：未标记的虚拟网络。'
- en: '**VLAN**: An 802.1Q VLAN tagged virtual network.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VLAN**：一个802.1Q VLAN标记的虚拟网络。'
- en: '**GRE**, VXLAN, GENEVE: Depending on the network overlay technologies, we select
    these network backends.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GRE**，VXLAN，GENEVE：根据网络覆盖技术，我们选择这些网络后端。'
- en: Now that we've covered OpenStack's usage models, ideas, and services, let's
    discuss additional ways in which OpenStack can be used. As you might imagine,
    OpenStack – being what it is – is highly capable of being used in many non-standard
    scenarios. We'll discuss these non-obvious scenarios next.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了OpenStack的使用模型、思想和服务，让我们讨论一下OpenStack可以被用于的其他方式。正如你所想象的，OpenStack -
    作为它所是的东西 - 非常适合在许多非标准场景中使用。接下来我们将讨论这些非明显的场景。
- en: Additional OpenStack use cases
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他OpenStack使用案例
- en: OpenStack has a lot of really detailed documentation available at [https://docs.openstack.org](https://docs.openstack.org).
    One of the more useful topics is the architecture and design examples, which both
    explain the usage scenarios and the ideas behind how a particular scenario can
    be solved using the OpenStack infrastructure. We are going to talk a lot about
    two different edge cases when we deploy our test OpenStack, but some things need
    to be said about configuring and running an OpenStack installation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack在[https://docs.openstack.org](https://docs.openstack.org)上有很多非常详细的文档。其中一个更有用的主题是架构和设计示例，它们都解释了使用场景和使用OpenStack基础设施解决特定场景的思想。当我们部署我们的测试OpenStack时，我们将讨论两种不同的边缘情况，但有些事情需要说一下关于配置和运行OpenStack安装。
- en: OpenStack is a complex system that encompasses not only computing and storage
    but also a lot of networking and supporting infrastructure. You will first notice
    that when you realize that even the documentation is neatly divided into an administration,
    architecture, operations, security, and virtual machine image guide. Each of these
    subjects is practically a topic for a single book, and a lot of things that guides
    cover are part experience, part best practice advice, and part assumptions based
    on best guesses.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack是一个复杂的系统，不仅涵盖计算和存储，还涉及大量的网络和支持基础设施。当你意识到即使文档也被整齐地分成了管理、架构、运维、安全和虚拟机镜像指南时，你会首先注意到这一点。每个主题实际上都可以成为一本书的主题，指南涵盖的许多内容既是经验，又是最佳实践建议，也是基于最佳猜测的假设的一部分。
- en: There are a couple of things that are more or less common to all these use cases.
    First, when designing a cloud, you must try and get all the information about
    possible loads and your clients as soon as possible, even before a first server
    is booted. This will enable you to plan not only how many servers you need, but
    their location, the ratio of computing to storage nodes, the network topology,
    energy requirements, and all the other things that need to be thought through
    in order to create a working solution.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些用例大致上有一些共同之处。首先，在设计云时，你必须尽早获取关于可能的负载和客户的所有信息，甚至在第一台服务器启动之前。这将使你能够计划不仅需要多少服务器，还有它们的位置、计算与存储节点的比例、网络拓扑、能源需求以及所有其他需要深思熟虑的事情，以创建一个可行的解决方案。
- en: 'When deploying OpenStack, we are talking about a large-scale enterprise solution
    that is usually deployed for one of three reasons:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署OpenStack时，我们通常是出于以下三个原因之一部署大规模企业解决方案：
- en: '*Testing and learning*: Maybe we need to learn how to configure a new installation,
    or we need to test a new computing node before we even go near production systems.
    For that reason, we need a small OpenStack environment, perhaps a single server
    that we can expand if there is a need for that. In practice, this system should
    be able to support probably a single user with a couple of instances. Those instances
    will usually not be the focus of your attention; they are going to be there just
    to enable you to explore all the other functionalities of the system. Deploying
    such a system is usually done the way we described in this chapter – using a readymade
    script that installs and configures everything so that we can focus on the part
    we are actually working on.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*测试和学习*：也许我们需要学习如何配置新的安装，或者在接近生产系统之前需要测试新的计算节点。因此，我们需要一个小型的OpenStack环境，也许是一个单独的服务器，如果有需要的话可以扩展。在实践中，这个系统应该能够支持一个用户和几个实例。这些实例通常不会成为你关注的焦点；它们只是为了让你能够探索系统的所有其他功能而存在。部署这样的系统通常是使用本章描述的方式完成的——使用一个现成的脚本来安装和配置一切，这样我们就可以专注于我们实际正在处理的部分。'
- en: '*We have a need for a staging or pre-production environment*: Usually, this
    means that we need to either support the production team so they have a safe environment
    to work in, or we are trying to keep a separate test environment for storing and
    running instances before they are pushed into production.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要一个暂存或预生产环境：通常，这意味着我们需要支持生产团队，让他们有一个安全的工作环境，或者我们正在尝试保留一个单独的测试环境，用于存储和运行实例，然后再推入生产环境。
- en: Having such an environment is definitively recommended, even if you haven't
    had it yet, since it enables you and your team to experiment without fear of breaking
    the production environment. The downside is that this installation requires an
    environment that has to have some resources available for the users and their
    instances. This means we are not going to be able to get away with using a single
    server. Instead, we will have to create a cloud that will be, at least in some
    parts, as powerful as the production environment. Deploying such an installation
    is basically the same as production deployment since once it comes online, this
    environment will, from your perspective, be just another system in production.
    Even if we are calling it pre-production or test, if the system goes down, your
    users will inevitably call and complain. This is the same as what happens with
    the production environment; you will have to plan downtime, schedule upgrades,
    and try to keep it running as best as you can.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这样的环境是明确建议的，即使你还没有拥有它，因为它使你和你的团队能够在不担心破坏生产环境的情况下进行实验。缺点是，这种安装需要一个环境，必须为用户和他们的实例提供一些资源。这意味着我们不能只用一台服务器。相反，我们将不得不创建一个云，至少在某些部分，它要和生产环境一样强大。部署这样的安装基本上与生产部署相同，因为一旦它上线，从你的角度来看，这个环境将只是生产中的另一个系统。即使我们称其为预生产或测试，如果系统崩溃，你的用户必然会打电话抱怨。这与生产环境发生的情况相同；你必须计划停机时间，安排升级，并尽力使其尽可能地运行良好。
- en: '*For production*: This one is demanding in another way – maintenance. When
    creating an actual production cloud environment, you will need to design it well,
    and then carefully monitor the system to be able to respond to problems. Clouds
    are a flexible thing from the user''s perspective since they offer scaling and
    easy configuration, but being a cloud administrator means that you need to enable
    these configuration changes by having spare resources ready. At the same time,
    you need to pay attention to your equipment, servers, storage, networking, and
    everything else to be able to spot problems before the users see them. Has a switch
    failed over? Are the computing nodes all running correctly? Have the disks degraded
    in performance due to a failure? Each of these things, in a carefully configured
    system, will have minimal to no impact on the users, but if we are not proactive
    in our approach, compounding errors can quickly bring the system down.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having distinguished between a single server and a full install in two different
    scenarios, we are going to go through both. The single server will be done manually
    using scripts, while the multi-server will be done using Ansible playbooks.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've covered OpenStack in quite a bit of detail, it's time to start
    using it. Let's start with some small things (a small environment to test) in
    order to provision a regular OpenStack environment for production, and then discuss
    integrating OpenStack with Ansible. We'll revisit OpenStack in the next chapter,
    when we start discussing scaling out KVM to Amazon AWS.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Packstack demo environment for OpenStack
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you just need a **Proof of Concept** (**POC**), there''s a very easy way
    to install OpenStack. We are going to use **Packstack** as it''s the simplest
    way to do this. By using Packstack installation on CentOS 7, you''ll be able to
    configure OpenStack in 15 minutes or so. It all starts with a simple sequence
    of commands:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As the process goes through its various phases, you''ll see various messages,
    such as the following, which are quite nice as you get to see what''s happening
    in real time with a decent verbosity level:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Appreciating Packstack''s installation verbosity'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_07.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.7 – Appreciating Packstack's installation verbosity
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'After the installation is finished, you will get a report screen that looks
    similar to this:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Successful Packstack installation'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_08.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.8 – Successful Packstack installation
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'The installer has finished successfully, and it gives us a warning about `NetworkManager`
    and a kernel update, which means we need to restart our system. After the restart
    and checking the `/root/keystonerc_admin` file for our username and password,
    Packstack is alive and kicking and we can log in by using the URL mentioned in
    the previous screen''s output (`http://IP_or_hostname_where_PackStack_is_deployed/dashboard`):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Packstack UI'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_09.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.9 – Packstack UI
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: There's a bit of additional configuration that needs to be done, as noted in
    the Packstack documentation at [https://wiki.openstack.org/wiki/Packstack](https://wiki.openstack.org/wiki/Packstack).
    If you're going to use an external network, you need a static IP address without
    `NetworkManager`, and you probably want to either configure `firewalld` or stop
    it altogether. Other than that, you can start using this as your demo environment.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning the OpenStack environment
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the tasks that is going to be the simplest and, at the same time, the
    hardest when you need to create your first OpenStack configuration is going to
    be provisioning. There are basically two ways you can go with this: one is to
    install services one at a time in a carefully prepared hardware configuration,
    while the other is to just use a *single server install* guide from the OpenStack
    site and create a single machine that will serve as your test bed. In this chapter,
    everything we do is created in such an instance, but before we learn how to install
    the system, we need to understand the differences.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack is a cloud operating system, and its main idea is to enable us to
    use multiple servers and other devices to create a coherent, easily configured
    cloud that can be managed from a central point, either through an API or through
    a web server. The size and type of the OpenStack deployment can be from one server
    running everything, to thousands of servers and storage units integrated across
    several Datacenters. OpenStack does not have a problem with large-scale deployment;
    the only real limiting factor is usually the cost and other requirements for the
    environment we are trying to create.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned scalability a few times, and this is where OpenStack shines in
    both ways. The amazing thing is that not only does it scale up easily but that
    it also scales down. An installation that will work perfectly fine for a single
    user can be done on a single machine – even on a single VM inside a single machine
    – so you will be able to have your own cloud within a virtual environment on your
    laptop. This is great for testing things but nothing else.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Having a bare-metal install that will follow the guidelines and recommended
    configuration requirements for particular roles and services is the only way to
    go forward when creating a working, scalable cloud, and obviously this is the
    way to go if you need to create a production environment. Having said that, between
    a single machine and a thousand server installs, there are a lot of ways that
    your infrastructure can be shaped and redesigned to support your particular use
    case scenario.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Let's first quickly go through an installation inside another VM, a task that
    can be accomplished in under 10 minutes on a faster host machine. For our platform,
    we decided on installing Ubuntu 18.04.3 LTS in order to be able to keep the host
    system to a minimum. The entire guide for Ubuntu regarding what we are trying
    to do is available at [https://docs.openstack.org/devstack/latest/guides/single-machine.html](https://docs.openstack.org/devstack/latest/guides/single-machine.html).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: One thing that we must point out is that the OpenStack site has a guide for
    a number of different install scenarios, both on virtual and bare-metal hardware,
    and they are all extremely easy to follow, simply because the documentation is
    straight to the point. There's also a simple install script that takes care of
    everything once a few steps are done manually by you.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Be careful with hardware requirements. There are some good sources available
    to cover this subject. Start here: [https://docs.openstack.org/newton/install-guide-rdo/overview.html#figure-hwreqs](https://docs.openstack.org/newton/install-guide-rdo/overview.html#figure-hwreqs).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Installing OpenStack step by step
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing we need to do is create a user that is going to install the
    entire system. This user needs to have `sudo` privileges since a lot of things
    require system-wide permissions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a user either as root or through `sudo`:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The next thing we need to do is allow this user to use `sudo`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We also need to install `git` and switch to our newly created user:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.10 – Installing git, the first step in deploying OpenStack'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_10.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.10 – Installing git, the first step in deploying OpenStack
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Now for the fun part. We are going to clone (copy the latest version of) `devstack`,
    the installation script that will provide everything we need to be able to run
    and use OpenStack on this machine:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.11 – Cloning devstack by using git'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_11.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.11 – Cloning devstack by using git
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'A little bit of configuration is now needed. Inside the `samples` directory,
    in the directory we just cloned, there is a file called `local.conf`. Use it to
    configure all the things the installer needs. Networking is one thing that has
    to be configured manually – not just the local network, which is the one that
    connects you to the rest of the internet, but also the internal network address
    space, which is going to get used for everything OpenStack needs to do between
    instances. Different passwords for different services also need to be set. All
    of this can be read in the sample file. Directions on how to exactly configure
    this are both on the web at the address we gave you earlier, and inside the file
    itself:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.12 – Installer configuration'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_12.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.12 – Installer configuration
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'There will be some issues with this installation process, and as a result,
    installation might break twice because of the following reasons:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Ownership of `/opt/stack/.cache` is `root:root`, instead of `stack:stack`. Please
    correct this ownership before running the installer;
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An installer problem (a known one), as it fails to install a component and then
    fails. Solution is rather simple - there's a line that needs to be changed in
    a file in inc directory, called python. At time of writing, line 192 of that file
    needs to be changed from `$cmd_pip $upgrade \` to `$cmd_pip $upgrade --ignore-installed
    \`
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the end, after we collected all the data and modified the file, we settled
    on this configuration:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.13 – Example configuration'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_13.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.13 – Example configuration
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of these parameters are understandable, but let''s cover two of them first:
    `FLOATING_RANGE` and `FIXED_RANGE`. The `FLOATING_RANGE` parameter tells our OpenStack
    installation which network scope will be used for *private* networks. On the other
    hand, `FIXED_RANGE` is the network scope that will be used by OpenStack-provisioned
    virtual machines. Basically, virtual machines provisioned in OpenStack environments
    will be given internal addresses from `FIXED_RANGE`. If a virtual machine needs
    to be available from the outside world as well, we will assign a network address
    from `FLOATING_RANGE`. Be careful with `FIXED_RANGE` as it shouldn''t match an
    existing network range in your environment.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: One thing we changed from what is given in the guide is that we reduced the
    number of replicas in the Swift installation to one. This gives us no redundancy,
    but reduces the space used for storage and speeds things up a little. Do not do
    this in the production environment.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your configuration, you may also need to set the `HOST_IP` address
    variable in the file. Here, set it to your current IP address.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Then, run `./stack.sh`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''ve run the script, a really verbose installation should start and
    dump a lot of lines on your screen. Wait for it to finish – it is going to take
    a while and download a lot of files from the internet. At the end, it is going
    to give you an installation summary that looks something like this:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.14 – Installation summary'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_14.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.14 – Installation summary
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this is done, if everything is okay, you should have a complete running
    version of OpenStack on your local machine. In order to verify that, connect to
    your machine using a web browser; a welcome screen should appear:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.15 – OpenStack login screen'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_15.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.15 – OpenStack login screen
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: After logging in with the credentials that are written on your machine, after
    the installation (the default administrator name is `admin` and the password is
    the one you set in `local.conf` when installing the service), you are going to
    be welcomed by a screen showing you the stats for your cloud. The screen you are
    looking at is actually a Horizon dashboard and is the main screen that provides
    you with all you need to know about your cloud at a glance.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack administration
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Looking at the top-left corner of Horizon, we can see that there are three
    distinct sections that are configured by default. The first one – **Project**
    – covers everything about our default instance and its performance. This is where
    you can create new instances, manage images, and work on server groups. Our cloud
    is just a core installation, so we only have one server and two defined zones,
    which means that we have no server groups installed:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.16 – Basic Horizon dashboard'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_16.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.16 – Basic Horizon dashboard
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create a quick instance to show how this is done:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.17 – Creating an instance'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_17.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.17 – Creating an instance
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to create an instance:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Go to **Launch Instance** in the far-right part of the screen. A window will
    open that will enable you to give OpenStack all the information it needs to create
    a new VM instance:![Figure 12.18 – Launch Instance wizard
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B14834_12_18.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.18 – Launch Instance wizard
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: On the next screen, you need to supply the system with the image source. We
    already mentioned glances – these images are taken from the Glance store and can
    be either an image snapshot, a ready-made volume, or a volume snapshot. We can
    also create a persistent image if we want to. One thing that you'll notice is
    that there are two differences when comparing this process to almost any other
    deployment. The first is that we are using a ready-made image by default as one
    was provided for us. Another big thing is the ability to create a new persistent
    volume to store our data in, or to have it deleted when we are done with the image,
    or have it not be created at all.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Choose the one image you have allocated in the public repository; it should
    be called something similar to the one shown in the following screenshot. CirrOS
    is a test image provided with OpenStack. It''s a minimal Linux distribution that
    is designed to be as small as possible and enable easy testing of the whole cloud
    infrastructure but to be as unobtrusive as possible. CirrOS is basically an OS
    placeholder. Of course, we need to click on the **Launch Instance** button to
    go to the next step:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.19 – Selecting an instance source'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_19.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.19 – Selecting an instance source
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: The next important part of creating a new image is choosing a flavor. This is
    another one of those peculiarly named things in OpenStack. A flavor is a combination
    of certain resources that basically creates a computing, memory, and storage template
    for new instances. We can choose from instances that have as little as 64 MB of
    RAM and 1 vCPU and go as far as our infrastructure can provide:![Figure 12.20
    – Selecting an instance flavor
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B14834_12_20.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.20 – Selecting an instance flavor
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: In this particular example, we are going to choose `cirros256`, a flavor that
    is basically designed to provide our test system with as few resources as is feasible.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'The last thing we actually need to choose is the network connectivity. We need
    to set all the adapters our instance will be able to use while running. Since
    this is a simple test, we are going to use both adapters we have, both the internal
    and external one. They are called `public` and `shared`:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.21 – Instance network configuration'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_21.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.21 – Instance network configuration
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can launch our instance and it will be able to boot. Once you click
    on the **Launch Instance** button, it is going to take probably under a minute
    to create a new instance. The screen showing its current progress and instance
    status will auto update while the instance is being deployed.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this is done, our instance will be ready:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.22 – The instance is ready'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_22.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.22 – The instance is ready
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll quickly create another instance, and then create a snapshot so that
    we can show you how image management works. If you click on the **Create snapshot**
    button on the right-hand side of the instance list, Horizon will create a snapshot
    and immediately put you in the interface meant for image administration:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.23 – Images'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_23.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.23 – Images
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have two different snapshots: one that is the start image and another
    that is an actual snapshot of the image that is running. So far, everything has
    been simple. What about creating an instance out of a snapshot? It''s just a click
    away! What you need to do is just click on the **Launch Instance** button on the
    right and go through the wizard for creating new instances.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'The end result of our short example of instance creation should be something
    like this:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.24 – New instance creation finished'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_24.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.24 – New instance creation finished
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'What we can see is all the information we need on our instances, what their
    IP addresses are, their flavor (which translates into what amount of resources
    are allocated for a particular instance), the availability zone that the image
    is running in, and information on the current instance state. The next thing we
    are going to check out is the **Volumes** tab on the left. When we created our
    instances, we told OpenStack to create one permanent volume for the first instance.
    If we now click on **Volumes**, we should see the volume under a numeric name:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.25 – Volumes'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_25.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.25 – Volumes
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: From this screen, we can now snapshot the volume, reattach it to a different
    instance, and even upload it as an image to the repository.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: The third tab on the left-hand side, named **Network**, contains even more information
    about our currently configured setup.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'If we click on the **Network Topology** tab, we will get the whole network
    topology of our currently running network, shown in a simple graphical display.
    We can choose from **Topology** and **Graph**, both of which basically represent
    the same thing:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.26 – Network topology'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_26.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.26 – Network topology
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: If we need to create another network or change anything in the network matrix,
    we can do so here. We consider this to be really administrator-friendly, on top
    of being documentation-friendly. Both of these points make our next topic – day-to-day
    administration – much easier.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Day-to-day administration
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are more or less finished with the most important options that are in any
    way connected to the administration of our day-to-day tasks in the **Project**
    Datacenter. If we click on the tab named **Admin**, we will notice that the menu
    structure we''ve opened looks a lot like the one under **Project**. This is because,
    now, we are looking at administration tasks that have something to do with the
    infrastructure of the cloud, not the infrastructure of our particular logical
    Datacenter, but the same building blocks exist in both of these. However, if we
    – for example – open **Compute**, a completely different set of options exist:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.27 – Different available configuration options'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_27.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.27 – Different available configuration options
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: This part of the interface is used to completely administer parts that form
    our infrastructure and those that define different things we can use while working
    in our *Datacenter*. When logged in as a user, we can add and remove virtual machines,
    configure networks, and use resources, but to put resources online, add new hypervisors,
    define flavors, and do these kinds of tasks that completely change the infrastructure,
    we need to be assigned the administrative role. Some of the functions overlap,
    such as both the administrative part of the interface and the user-specific part,
    which have control over instances. However, the administrative part has all these
    functions, and users can have their set of commands tweaked so that they are,
    for instance, unable to delete or create new instances.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'The adminsitrative view enables us to monitor our nodes on a more direct level,
    not only through the services they provide, but also through raw data about a
    particular host and the resources utilized on it:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.28 – Available hypervisors in our Datacenter'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_28.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.28 – Available hypervisors in our Datacenter
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Our Datacenter has only one hypervisor, but we can see the amount of resources
    physically available on it, and the share of those resources the current setup
    is using at this particular moment.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Flavors are also one important part of the whole of OpenStack. We already mentioned
    them as a predefined sets of resource presets that form a platform that the instance
    is going to run on. Our test setup has a few of them defined, but we can delete
    the ones that are shipped in this setup and create new ones tailored to our needs.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Since the point of the cloud is to optimize resource management, flavors play
    a big part in this concept. Creating flavors is not an easy task in terms of planning
    and design. First and foremost, it requires deep knowledge of what is possible
    on a given hardware platform, how much and what computing resources even exist,
    and how to utilize it to the full extent possible. So, it is essential that we
    plan and design things properly. The other thing is that we actually need to understand
    what kind of load we are preparing for. Is it memory-intensive? Do we have many
    small services that require a lot of nodes with a simple configuration? Are we
    going to need a lot of computing power and/or a lot of storage? The answers to
    those questions are something that will not only enable us to create what our
    clients want, but also create flavors that will have users utilizing our infrastructure
    in full.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea is to create flavors that will give individual users just enough
    resources to get their job done in a satisfactory way. This is not obvious in
    a deployment that has 10 instances, but once we run into thousands, a flavor that
    always leaves 10 percent of the storage unused is quickly going to eat into our
    resources and limit our ability to serve more users. Striking this balance between
    what we have and what we give users to use in a particular way is probably the
    hardest task in planning and designing our environments:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.29 – Create Flavor wizard'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_29.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.29 – Create Flavor wizard
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a flavor is a simple task. We need to do the following:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Give it a name; an ID will be assigned automatically.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the number of vCPUs and RAM for our flavor.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the size of a base disk, and an ephemeral disk that doesn't get included
    in any of the snapshots and gets deleted when a virtual machine is terminated.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the amount of swap space.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the RX/TX factor so that we can create a bit of QoS on the network level.
    Some flavors will need to have more network traffic priority than others.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenStack allows a particular project to have more than one flavor, and for
    a single flavor to belong to different projects. Now that we've learned that,
    let's work with our user identities and assign them some objects.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Identity management
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last tab on the left-hand side is **Identity**, which is responsible for
    handling users, roles, and projects. This is where we are going to configure not
    only our usernames, but the user roles, groups, and projects a particular user
    can use:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.30 – Users, Groups, and Roles management'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_30.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.30 – Users, Groups, and Roles management
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'We are not going to go too much into how users are managed and installed, but
    just cover the basics of user management. As always, the original documentation
    on the OpenStack site is the place to go to learn more. Make sure that you check
    out this link: [https://docs.openstack.org/keystone/pike/admin/cli-manage-projects-users-and-roles.html](https://docs.openstack.org/keystone/pike/admin/cli-manage-projects-users-and-roles.html).'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: In short, once you create a project, you need to define which users are going
    to be able to see and work on a particular project. In order to ease administration,
    users can also be part of groups, and you can then assign whole groups to a project.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'The point of this structure is to enable the administrator to limit the users
    not only to what they can administer, but also to how many of the available resources
    are available for a particular project. Let''s use an example for this. If we
    go to **Projects** and edit an existing project (or create a new one), we''ll
    see a tab called **Quota** in the configuration menu, which looks like this:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.31 – Quotas on the default project'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_12_31.jpg)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.31 – Quotas on the default project
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Once you create a project, you can assign all the resources in the form of quotas.
    This assignment limits the maximum available resources for a particular group
    of instances. The user has no overview of the whole system; they can only *see*
    and utilize resources available through the project. If a user is part of multiple
    projects, they can create, delete, and manage instances based on their role in
    the project, and the resources available to them are specific to a project.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: We'll discuss OpenStack/Ansible integration next, as well as some potential
    use cases for these two concepts to work together. Keep in mind that the larger
    the OpenStack environment is, the more use cases we will find for them.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Integrating OpenStack with Ansible
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dealing with any large-scale application is not easy, and not having the right
    tool can make it impossible. OpenStack provides a lot of ways for us to directly
    orchestrate and manage a huge horizontal deployment, but sometimes, this is not
    enough. Luckily, in our arsenal of tools, we have another one – **Ansible**. In
    [*Chapter 11*](B14834_11_Final_ASB_ePub.xhtml#_idTextAnchor191), *Ansible for
    Orchestration and Automation*, we covered some other, smaller ways to use Ansible
    to deploy and configure individual machines, so we are not going to go back to
    that. Instead, we are going to focus on things that Ansible is good for in the
    OpenStack environment.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: One thing that we must make clear, though, is that using Ansible in an OpenStack
    environment can be based on two very distinct scenarios. One is using Ansible
    to handle deployed instances, in a way that would pretty much look the same across
    all the other cloud or bare-metal deployments. You, as an administrator of a large
    number of instances, create a management node that is nothing more than a Python-enabled
    server with added Ansible packages and playbooks. After that, you sort out the
    inventory for your deployment and are ready to manage your instances. This scenario
    is not what this part of this chapter is about.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: What we are talking about here is using Ansible to manage the cloud itself.
    This means we are not deploying instances inside the OpenStack cloud; we are deploying
    compute and storage nodes for OpenStack itself.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'The environment we are talking about is sometimes referred to as **OpenStack-Ansible**
    (**OSA**) and is common enough to have its own deployment guide, located at the
    following URL: [https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest/](https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest/).'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: The requirements for a minimal installation in OpenStack-Ansible are considerably
    greater than those in a single VM or on a single machine. The reason for this
    is not just that the system needs all the resources; it's the tools that need
    to be used and the philosophy behind it all.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s quickly go through what Ansible means in terms of OpenStack:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Once configured, it enables the quick deployment of any kind of resource, be
    it storage or computing.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It makes sure you are not forgetting to configure something in the process.
    When deploying a single server, you will have to make sure that everything works
    and that errors in configuration are easy to spot, but when deploying multiple
    nodes, errors can creep in and degrade the performance of part of the system without
    anyone noticing. The normal deployment practice to avoid this is an installation
    checklist, but Ansible is a much better solution than that.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More streamlined configuration changes. Sometimes, we need to apply a confi[guration
    change across the whole system or some part](https://docs.openstack.org/openstack-ansible/latest/)
    of it. This can be frustrating if not scripted.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, having said all that, let's quickly go through [https://docs.openstack.org/openstack-ansible/latest/](https://docs.openstack.org/openstack-ansible/latest/)
    and see what the official documentation says about how to deploy and use Ansible
    and OpenStack.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: What exactly does OpenStack offer to the administrator in regard to Ansible?
    The simplest answer is playbooks and roles.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Ansible to deploy OpenStack, you basically need to create a deployment
    host and then use Ansible to deploy the whole OpenStack system. The whole workflow
    goes something like this:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the deployment host
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the target hosts
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure Ansible for deployment
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run playbooks and let them install everything
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check whether OpenStack is correctly installed
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When we are talking about deployment and target hosts, we need to make a clear
    distinction: the deployment host is a single entity holding Ansible, scripts,
    playbooks, roles, and all the supporting bits. The target hosts are the actual
    servers that are going to be part of the OpenStack cloud.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'The requirements for installation are straightforward:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: The operating system should be a minimal installation of Debian, Ubuntu CentOS,
    or openSUSE (experimental) with the latest kernel and full updates applied.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Systems should also run Python 2.7, have SSH access enabled with public key
    authentication, and have NTP time sync enabled. This covers the deployment host.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also usual recommendations for different types of nodes. Computing
    nodes must support hardware-assisted virtualization, but that's an obvious requirement.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a requirement that should go without saying, and that is to use multicore
    processors, with as many cores as possible, to enable some services to run much
    faster.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disk requirements are really up to you. OpenStack suggests using fast disks
    if possible, recommending SSD drives in a RAID, and large pools of disks available
    for block storage.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure nodes have requirements that are different than the other types
    of nodes since they are running a few databases that grow over time and need at
    least 100 GB of space. The infrastructure also runs its services as containers,
    so it will consume resources in a particular way that will be different than the
    other compute nodes.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deployment guide also suggests running a logging host since all the services
    create logs. The recommended disk space is at least 50 GB for logs, but in production,
    this will quickly grow in orders of magnitude.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack needs a fast, stable network to work with. Since everything in OpenStack
    will depend on the network, every possible solution that will speed up network
    access is recommended, including using 10G and bonded interfaces. Installing a
    deployment server is the first step in the overall process, which is why we'll
    do that next.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Installing an Ansible deployment server
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our deployment server needs to be up to date with all the upgrades and have
    Python, `git`, `ntp`, `sudo`, and `ssh` support installed. After you've installed
    the required packages, you need to configure the `ssh` keys to be able to log
    into the target hosts. This is an Ansible requirement and is also a best practice
    that leverages security and ease of access.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: The network is simple – our deployment host must have connectivity to all the
    other hosts. The deployment host should also be installed on the L2 of the network,
    which is designed for container management.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the repository should be cloned:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, an Ansible bootstrap script needs to be run:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This concludes preparing the Ansible deployment server. Now, we need to prepare
    the target computers we are going to use for OpenStack. Target computers are currently
    supported on Ubuntu Server (18.04) LTS, CentOS 7, and openSUSE 42.x (at the time
    of writing, there still isn''t CentOS 8 support). You can use any of these systems.
    For each of them, there is a helpful guide that will get you up and running quickly:
    [https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest/deploymenthost.html](https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest/deploymenthost.html).
    We''ll just explain the general steps to ease you into installing it, but in all
    truth, just copy and paste the commands that have been published for your operating
    system from [https://www.openstack.org/](https://www.openstack.org/).'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: No matter which system you decide to run on, you have to be completely up to
    date with system updates. After that, install the `linux-image-extra` package
    (if it exists for your kernel) and install the `bridge-utils`, `debootstrap`,
    `ifenslave`, `lsof`, `lvm2`, `chrony`, `openssh-server`, `sudo`, `tcpdump`, `vlan`,
    and Python packages. Also, enable bonding and VLAN interfacing. All these things
    may or may not be available for your system, so if something is already installed
    or configured, just skip over it.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Configure the NTP time sync in `chrony.conf` to synchronize time across the
    whole deployment. You can use any time source you like, but for the system to
    work, time must be in sync.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Now, configure the `ssh` keys. Ansible is going to deploy using `ssh` and key-based
    authentication. Just copy the public keys from the appropriate user on your deployment
    machine to `/root/.ssh/authorized_keys`. Test this setup by simply logging in
    from the deployment host to the target machine. If everything is okay, you should
    be able to log in without any password or any other prompt. Also, note that the
    root user on the deployment host is the default user for managing everything and
    that they have to have their `ssh` keys generated in advance since they are used
    not only on the target hosts but also in all the containers for different services
    running across the system. These keys must exist when you start to configure the
    system.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: For storage nodes, please note that LVM volumes will be created on the local
    disks, thus overwriting any existing configuration. Network configuration is going
    to be done automatically; you just need to ensure that Ansible is able to connect
    to the target machines.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: The next step is configuring our Ansible inventory so that we can use it. Let's
    do that now.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the Ansible inventory
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we can run the Ansible playbooks, we need to finish configuring the
    Ansible inventory so that it points the system to the hosts it should install
    on. We are going to quote the verbatim, available at [https://docs.openstack.org/project-deploy-guide/openstack-ansible/queens/configure.html](https://docs.openstack.org/project-deploy-guide/openstack-ansible/queens/configure.html):'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Copy the contents of the /opt/openstack-ansible/etc/openstack_deploy directory
    to
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: the /etc/openstack_deploy directory.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Change to the /etc/openstack_deploy directory.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Copy the openstack_user_config.yml.example file to
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: /etc/openstack_deploy/openstack_user_config.yml.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Review the openstack_user_config.yml file and make changes to the deployment
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: of your OpenStack environment.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Once inside the configuration file, review all the options. `Openstack_user_config.yml`
    defines which hosts run which services and nodes. Before committing to installing,
    please review the documentation mentioned in the previous paragraph.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing that stands out on the web is `install_method`. You can choose either
    source or distro. Each has its pros and cons:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Source is the simplest installation as it's done directly from the sources on
    the OpenStack official site and contains an environment that's compatible with
    all systems.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distro method is customized for the particular distribution you are installing
    on by using specific packages known to work and known as being stable. The major
    drawback of this is that updates are going to be much slower since not only OpenStack
    needs to be deployed but also information about all the packages on distributions,
    and that setup needs to be verified. As a result, expect long waits between when
    the upgrade reaches the *source* and gets to your *distro* installation. After
    installing, you must go with your primary choice; there is no mechanism for switching
    from one to the other.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last thing you need to do is open the `user_secrets.yml` file and assign
    passwords for all the services. You can either create your own passwords or use
    a script provided just for this purpose.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Running Ansible playbooks
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we go through the deployment process, we will need to start a couple of
    Ansible playbooks. We need to use these three provided playbooks in this order:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '`setup-hosts.yml` : The initial Ansible playbook that we use to provision the
    necessary services on our OpenStack hosts.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setup-infrastructure.yml`: The Ansible playbook that deploys some more services,
    such as RabbitMQ, repository server, Memcached, and so on.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setup-openstack.yml`: The Ansible playbook that deploys the remaining services
    – Glance, Cinder, Nova, Keystone, Heat, Neutron, Horizon, and so on.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All of these Ansible playbooks need to be finished successfully so that we
    can integrate Ansible with Openstack. So, the only thing left is to run the Ansible
    playbooks. We need to start with the following command:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can find the appropriate files in `/opt/openstack-ansible/playbooks`. Now,
    run the remaining setups:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: All the playbooks should finish without unreachable or failed plays. And with
    that – congratulations! You have just installed OpenStack.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-401
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we spent a lot of time describing the architecture and inner
    workings of OpenStack. We discussed software-defined networking and its challenges,
    as well as different OpenStack services such as Nova, Swift, Glance, and so on.
    Then, we moved on to practical issues, such as deploying Packstack (let's just
    call that OpenStack for proof of concept), and full OpenStack. In the last part
    of this chapter, we discussed OpenStack-Ansible integration and what it might
    mean for us in larger environments.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've covered the *private* cloud aspect, it's time to grow our environment
    and expand it to a more *public* or *hybrid*-based approach. In KVM-based infrastructures,
    this usually means connecting to AWS to convert your workloads and transfer them
    there (public cloud). If we're discussing the hybrid type of cloud functionality,
    then we have to introduce an application called Eucalyptus. For the hows and whys,
    check out the next chapter.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the main problem with VLAN as a cloud overlay technology?
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which types of cloud overlay networks are being used on the cloud market today?
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does VXLAN work?
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the most common problems with stretching Layer 2 networks across
    multiple sites?
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is OpenStack?
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the architectural components of OpenStack?
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is OpenStack Nova?
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is OpenStack Swift?
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is OpenStack Glance?
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is OpenStack Horizon?
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are OpenStack flavors?
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is OpenStack Neutron?
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please refer to the following links for more information regarding what was
    covered in this chapter:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenStack documentation: [https://docs.openstack.org](https://docs.openstack.org)'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arista VXLAN overview: [https://www.arista.com/assets/data/pdf/Whitepapers/Arista_Networks_VXLAN_White_Paper.pdf](https://www.arista.com/assets/data/pdf/Whitepapers/Arista_Networks_VXLAN_White_Paper.pdf)'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Red Hat – What is GENEVE?: [https://www.redhat.com/en/blog/what-geneve](https://www.redhat.com/en/blog/what-geneve)'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cisco – Configuring Virtual Networks Using OpenStack: [https://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus1000/kvm/config_guide/network/5x/b_Cisco_N1KV_KVM_Virtual_Network_Config_5x/configuring_virtual_networks_using_openstack.pdf](https://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus1000/kvm/config_guide/network/5x/b_Cisco_N1KV_KVM_Virtual_Network_Config_5x/configuring_virtual_networks_using_openstack.pdf)'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Packstack: [http://rdoproject.org](http://rdoproject.org)'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
