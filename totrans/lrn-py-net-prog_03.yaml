- en: Chapter 3. APIs in Action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talk about APIs in relation to Python, we usually refer to the classes
    and the functions that a module presents to us to interact with. In this chapter,
    we'll be talking about something different, that is, web APIs.
  prefs: []
  type: TYPE_NORMAL
- en: A web API is a type of API that you interact with through the HTTP protocol.
    Nowadays, many web services provide a set of HTTP calls, which are designed to
    be used programmatically by clients, that is, they are meant to be used by machines
    rather than by humans. Through these interfaces it's possible to automate interaction
    with the services and to perform tasks such as extracting data, configuring the
    service in some way, and uploading your own content into the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll look at:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two popular data exchange formats used by web APIs: XML and JSON'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How to interact with two major web APIs: Amazon S3 and Twitter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to pull data from HTML pages when an API is not available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to make life easier for the webmasters that provide these APIs and websites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are hundreds of services that offer web APIs. A quite comprehensive and
    ever-growing list of these services can be found at [http://www.programmableweb.com](http://www.programmableweb.com).
  prefs: []
  type: TYPE_NORMAL
- en: We're going to start by introducing how XML is used in Python, and then we will
    explain an XML-based API called the Amazon S3 API.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with XML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Extensible Markup Language** (**XML**) is a way of representing hierarchical
    data in a standard text format. When working with XML-based web APIs, we'll be
    creating XML documents and sending them as the bodies of HTTP requests and receiving
    XML documents as the bodies of responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the text representation of an XML document, perhaps this represents
    the stock at a cheese shop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you've coded with HTML before, then this may look familiar. XML is a markup
    based format. It is from the same family of languages as HTML. The data is structured
    in an hierarchy formed by elements. Each element is represented by two tags, a
    start tag, for example, `<name>`, and a matching end tag, for example, `</name>`.
    Between these two tags, we can either put data, such as `Caerphilly`, or add more
    tags, which represent child elements.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike HTML, XML is designed such that we can define our own tags and create
    our own data formats. Also, unlike HTML, the XML syntax is always strictly enforced.
    Whereas in HTML small mistakes, such as tags being closed in the wrong order,
    closing tags missing altogether, or attribute values missing quotes are tolerated,
    in XML, these mistakes will result in completely unreadable XML documents. A correctly
    formatted XML document is called well formed.
  prefs: []
  type: TYPE_NORMAL
- en: The XML APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two main approaches to working with XML data:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading in a whole document and creating an object-based representation of it,
    then manipulating it by using an object-oriented API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing the document from start to end, and performing actions as specific
    tags are encountered
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For now, we're going to focus on the object-based approach by using a Python
    XML API called **ElementTree**. The second so-called pull or event-based approach
    (also often called **SAX**, as SAX is one of the most popular APIs in this category)
    is more complicated to set up, and is only needed for processing large XML files.
    We won't need this to work with Amazon S3.
  prefs: []
  type: TYPE_NORMAL
- en: The basics of ElementTree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll be using the Python standard library implementation of the `ElementTree`
    API, which is in the `xml.etree.ElementTree` module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how we may create the aforementioned example XML document by using
    `ElementTree`. Open a Python interpreter and run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We start by creating the root element, that is, the outermost element of the
    document. We create a root element `<inventory>` here, and then print its string
    representation to screen. The `<inventory />` representation is an XML shortcut
    for `<inventory></inventory>`. It's used to show an empty element, that is, an
    element with no data and no child tags.
  prefs: []
  type: TYPE_NORMAL
- en: We create the `<inventory>` element by creating a new `ElementTree.Element`
    object. You'll notice that the argument we give to `Element()` is the name of
    the tag that is created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `<inventory>` element is empty at the moment, so let''s put something in
    it. Do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have an element called `<cheese>` in our `<inventory>` element. When
    an element is directly nested inside another, then the nested element is called
    a **child** of the outer element, and the outer element is called the **parent**.
    Similarly, elements that are at the same level are called **siblings**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add another element, and this time let''s give it some content. Add
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, our document is starting to shape up. We do two new things here: first,
    we use the shortcut class method `ElementTree.SubElement()` to create the new
    `<name>` element and insert it into the tree as a child of `<cheese>` in a single
    operation. Second, we give it some content by assigning some text to the element''s
    `text` attribute.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can remove elements by using the `remove()` method on the parent element,
    as shown in the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Pretty printing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It would be useful for us to be able to produce output in a more legible format,
    such as the example shown at the beginning of this section. The ElementTree API
    doesn''t have a function for doing this, but another XML API, `minidom`, provided
    by the standard library, does, and it''s simple to use. First, import `minidom`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Second, use the following command to print some nicely formatted XML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: These are not the easiest lines of code at first glance, so let's break them
    down. The `minidom` library can't directly work with ElementTree elements, so
    we use ElementTree's `tostring()` function to create a string representation of
    our XML. We load the string into the `minidom` API by using `minidom.parseString()`,
    and then we use the `toprettyxml()` method to output our formatted XML.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be wrapped into a function so that it becomes more handy. Enter the
    command block as shown in the following into your Python shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, just do the following to pretty print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Element attributes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the example shown at the beginning of this section, you may have spotted
    something in the opening tag of the `<cheese>` element, that is, the `id="c01"`
    text. This is called an **attribute**. We can use attributes to attach extra information
    to elements, and there's no limit to the number of attributes an element can have.
    Attributes are always comprised of an attribute name, which in this case is `id`,
    and a value, which in this case is `c01`. The values can be any text, but they
    must be enclosed in quotes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, add the `id` attribute to the `<cheese>` element, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `attrib` attribute of an element is a dict-like object which holds an element's
    attribute names and values. We can manipulate the XML attributes as we would a
    regular `dict`.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should be able to fully recreate the example document shown at the
    beginning of this section. Go ahead and give it a try.
  prefs: []
  type: TYPE_NORMAL
- en: Converting to text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we have an XML tree that we''re happy with, usually we would want to convert
    it into a string to send it over the network. The `ET.dump()` function that we''ve
    been using isn''t appropriate for this. All the `dump()` function does is print
    the tag to the screen. It doesn''t return a string which we can use. We need to
    use the `ET.tostring()` function for this, as shown in the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Notice that it returns a bytes object. It encods our string for us. The default
    character set is `us-ascii` but it's better to use UTF-8 for transmitting over
    HTTP, since it can encode the full range of Unicode characters, and it is widely
    supported by web applications.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For now, this is all that we need to know about creating XML documents, so let's
    see how we can apply it to a web API.
  prefs: []
  type: TYPE_NORMAL
- en: The Amazon S3 API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon S3 is a data storage service. It underpins many of today's high-profile
    web services. Despite offering enterprise-grade resilience, performance and features,
    it's pretty easy to start with. It is affordable, and it provides a simple API
    for automated access. It's one of many cloud services in the growing **Amazon
    Web Services** (**AWS**) portfolio.
  prefs: []
  type: TYPE_NORMAL
- en: APIs change every now and then, and they are usually given a version number
    so that we can track them. We'll be working with the current version of the S3
    REST API, "2006-03-01".
  prefs: []
  type: TYPE_NORMAL
- en: You'll notice that in the S3 documentation and elsewhere, the S3 web API is
    referred to as a **REST API**. **REST** stands for **Representational State Transfer**,
    and it is a fairly academic conception of how HTTP should be used for APIs, originally
    presented by Roy Fielding in his PhD dissertation. Although the properties that
    an API should possess so as to be considered RESTful are quite specific, in practice
    pretty much any API that is based on HTTP is now slapped with the RESTful label.
    The S3 API is actually among the most RESTful high-profile APIs, because it appropriately
    uses a good range of the HTTP methods.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to read more about this topic, Roy Fielding's dissertation is available
    here [http://ics.uci.edu/~fielding/pubs/dissertation](http://ics.uci.edu/~fielding/pubs/dissertation),
    and one of the original books that promoted the concept, and is a great read,
    *RESTful Web Services* by *Leonard Richardson* and *Sam Ruby*, is now available
    for free download from this page [http://restfulwebapis.org/rws.html](http://restfulwebapis.org/rws.html).
  prefs: []
  type: TYPE_NORMAL
- en: Registering with AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we can access S3, we need to register with AWS. It is the norm for APIs
    to require registration before allowing access to their features. You can use
    either an existing Amazon account or create a new one at [http://www.amazonaws.com](http://www.amazonaws.com).
    Although S3 is ultimately a paid-for service, if you are using AWS for the first
    time, then you will get a year's free trial for low-volume use. A year is plenty
    of time for finishing this chapter! The trial provides 5GB of free S3 storage.
  prefs: []
  type: TYPE_NORMAL
- en: Authentication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we need to discuss authentication, which is an important topic of discussion
    when using many web APIs. Most web APIs we use will specify a way for supplying
    authentication credentials that allow requests to be made to them, and typically
    every HTTP request we make must include authentication information.
  prefs: []
  type: TYPE_NORMAL
- en: 'APIs require this information for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that others can't abuse your application's access permissions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To apply per-application rate limiting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To manage delegation of access rights, so that an application can act on the
    behalf of other users of a service or other services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collection of usage statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the AWS services use an HTTP request signing mechanism for authentication.
    To sign a request, we hash and sign unique data in an HTTP request using a cryptographic
    key, then add the signature to the request as a header. By recreating the signature
    on the server, AWS can ensure that the request has been sent by us, and that it
    doesn't get altered in transit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The AWS signature generation process is currently on its 4th version, and an
    involved discussion would be needed to cover it, so we''re going to employ a third-party
    library, that is, `requests-aws4auth`. This is a companion library for the `Requests`
    module that automatically handles signature generation for us. It''s available
    at PyPi. So, install it on a command line with the help of `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Setting up an AWS user
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To use authentication, we need to acquire some credentials.
  prefs: []
  type: TYPE_NORMAL
- en: We will set this up through the AWS Console. Once you've registered with AWS,
    log into the Console at [https://console.aws.amazon.com](https://console.aws.amazon.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you are logged in, you need to perform the steps shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on your name at the top-right, and then choose **Security Credentials**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Users**, which is on the list in the left-hand side of the screen,
    and then click on the **Create New Users** button at the top.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type in the **username**, and make sure that **Generate an access key for each
    user** has been checked, and then click on the **Create** button in the bottom
    right-hand corner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You'll see a new page saying that the user has been created successfully. Click
    on the **Download credentials** button at the bottom right corner to download
    a CSV file, which contains the **Access ID** and **Access Secret** for this user.
    These are important because they will help in authenticating ourselves to the
    S3 API. Make sure that you store them securely, as they will allow full access
    to your S3 files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, click on **Close** at the bottom of the screen, and click on the new
    user in the list that will appear, and then click on the **Attach Policy** button.
    A list of policy templates will appear. Scroll down this list and select the **AmazonS3FullAccess**
    policy, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up an AWS user](graphics/6008OS_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Finally, click on the **Attach Policy** button at the bottom right-hand side
    when it appears. Now, our user has full access to the S3 service.
  prefs: []
  type: TYPE_NORMAL
- en: Regions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS has datacenters around the world, so when we activate a service in AWS we
    pick the region we want it to live in. There is a list of regions for S3 at [http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region](http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region).
  prefs: []
  type: TYPE_NORMAL
- en: It's best to choose a region that is closest to the users who will be using
    the service. For now, you'll be the only user, so just decide on the region that
    is closest to you for our first S3 tests.
  prefs: []
  type: TYPE_NORMAL
- en: S3 buckets and objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'S3 organizes the data that we store in it using two concepts: buckets and objects.
    An object is the equivalent of a file, that is, a blob of data with a name, and
    a bucket is equivalent to a directory. The only difference between buckets and
    directories is that buckets cannot contain other buckets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Every bucket has its own URL of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://<bucketname>.s3-<region>.amazonaws.com`.'
  prefs: []
  type: TYPE_NORMAL
- en: In the URL, `<bucketname>` is the name of the bucket and `<region>` is the AWS
    region where the bucket is present, for example `eu-west-1`. The bucket name and
    region are set when we create the bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Bucket names are shared globally among all S3 users, and so they must be unique.
    If you own a domain, then a subdomain of that will make an appropriate bucket
    name. You could also use your email address by replacing the `@` symbol with a
    hyphen or underscore.
  prefs: []
  type: TYPE_NORMAL
- en: Objects are named when we first upload them. We access objects by adding the
    object name to the end of the bucket's URL as a path. For example, if we have
    a bucket called `mybucket.example.com` in the `eu-west-1` region containing the
    object `cheeseshop.txt`, then we can access it by using the URL [http://mybucket.example.com.s3-eu-west-1.amazonaws.com/cheeseshop.txt](http://mybucket.example.com.s3-eu-west-1.amazonaws.com/cheeseshop.txt).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create our first bucket through the AWS Console. We can perform most
    of the operations that the API exposes manually through this web interface, and
    it''s a good way of checking that our API client is performing the desired tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Log into the Console at [https://console.aws.amazon.com](https://console.aws.amazon.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the S3 service. You will see a page, which will prompt you to create a
    bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Create Bucket** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a bucket name, pick a region, and then click on **Create**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will be taken to the bucket list, and you will be able to see your bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An S3 command-line client
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Okay, enough preparation, let's get to coding. For the rest of this section
    on S3, we will be writing a small command line client that will enable us to interact
    with the service. We will create buckets, and then upload and download files.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we''ll set up our command line interpreter and initialize the authentication.
    Create a file called `s3_client.py` and save the following code block in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Downloading the example code**'
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: You'll need to replace `<ACCESS ID>` and `<ACCESS KEY>` with the values from
    the credentials CSV that we downloaded earlier, and `<REGION>` with the AWS region
    of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: So, what are we doing here? Well, first we set up our endpoint. An endpoint
    is a general term for a URL which is used to access an API. Some web APIs have
    a single endpoint, some have many endpoints, it depends on how the API is designed.
    The endpoint we generate here is actually only a part of the full endpoint which
    we'll use when we work with buckets. Our actual endpoint is the endpoint prefixed
    by a bucket name.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create our `auth` object. We'll use this in conjunction with `Requests`
    to add AWS authentication to our API requests.
  prefs: []
  type: TYPE_NORMAL
- en: The `ns` variable is a string, which we'll need for working with XML from the
    S3 API. We'll discuss this when we use it.
  prefs: []
  type: TYPE_NORMAL
- en: We've included a modified version of our `xml_pprint()` function to help with
    debugging. And, for now, the `create_bucket()` function is just a placeholder.
    We'll learn more about this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we have the command interpreter itself - it simply takes the first
    argument given to the script on the command line and tries to run a function with
    the same name, passing any remaining command-line arguments to the function. Let''s
    give this a test run. Enter the following in a command prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the script pulls `create_bucket` from the command line arguments
    and so calls the function `create_bucket()`, passing `myBucket` as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: This framework makes adding functions to expand our client's capabilities a
    straightforward process. Let's start by making `create_bucket()` do something
    useful.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a bucket with the API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whenever we write a client for an API, our main point of reference is the API
    documentation. The documentation tells us how to construct the HTTP requests for
    performing operations. The S3 documentation can be found at [http://docs.aws.amazon.com/AmazonS3/latest/API/APIRest.html](http://docs.aws.amazon.com/AmazonS3/latest/API/APIRest.html).
    The [http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html](http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html)
    URL will provide the details of bucket creation.
  prefs: []
  type: TYPE_NORMAL
- en: This documentation tells us that to create a bucket we need to make an HTTP
    request to our new bucket's endpoint by using the HTTP `PUT` method. It also tells
    us that the request body must contain some XML, which specifies the AWS region
    that we want the bucket to be created in.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, now we know what we''re aiming for, let''s discuss our function. First,
    let''s create the XML. Replace the content of `create_bucket()` with the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we create an XML tree following the format given in the S3 documentation.
    If we run our client now, then we will see the XML shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This matches the format specified in the documentation. You can see that we've
    used the `ns` variable to fill the `xmlns` attribute. This attribute pops up throughout
    the S3 XML, having the `ns` variable pre-defined makes it quicker to work with
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s add the code to make the request. Replace the `xml_pprint(data)`
    at the end of `create_bucket()` with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The first line shown here will generate the full URL from our bucket name and
    endpoint. The second line will make the request to the S3 API. Notice that we
    have used the `requests.put()` function to make this request using the HTTP `PUT`
    method, rather than by using either the `requests.get()`method or the `requests.post()`
    method. Also, note that we have supplied our `auth` object to the call. This will
    allow `Requests` to handle all the S3 authentication for us!
  prefs: []
  type: TYPE_NORMAL
- en: If all goes well , then we print out a message. In case everything does not
    go as expected, we print out the response body. S3 returns error messages as XML
    in the response body. So we use our `xml_pprint()` function to display it. We'll
    look at working with these errors in the *Handling errors* section, later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now run the client, and if everything works as expected, then we will get a
    confirmation message. Make sure that you have picked a bucket that hasn''t already
    been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: When we refresh the S3 Console in our browser, we will see that our bucket has
    been created.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading a file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we've created a bucket, we can upload some files. Writing a function
    for uploading a file is similar to creating a bucket. We check the documentation
    to see how to construct our HTTP request, figure out what information should be
    collected at the command line, and then write the function.
  prefs: []
  type: TYPE_NORMAL
- en: We need to use an HTTP `PUT` again. We need the name of the bucket that we want
    to store the file in and the name that we want the file to be stored under in
    S3\. The body of the request will contain the file data. At the command line,
    we'll collect the bucket name, the name we want the file to have in the S3 service
    and the name of the local file to upload.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following function to your `s3_client.py` file after the `create_bucket()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In creating this function, we follow a pattern similar to that for creating
    a bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the data that will go in the request body.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct our URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make the request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the outcome.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that we open the local file in binary mode. The file could contain any
    type of data, so we don't want text transforms applied. We could pull this data
    from anywhere, such as a database or another web API. Here, we just use a local
    file for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: The URL is the same endpoint that we constructed in `create_bucket()` with the
    S3 object name appended to the URL path. Later, we can use this URL to retrieve
    the object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, run the command shown here to upload a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: You'll need to replace `mybucket.example.com` with your own bucket name. Once
    the file gets uploaded, you will see it in the S3 Console.
  prefs: []
  type: TYPE_NORMAL
- en: I have used a JPEG image that was stored in my home directory as the source
    file. You can use any file, just change the last argument to an appropriate path.
    However, using a JPEG image will make the following sections easier for you to
    reproduce.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving an uploaded file through a web browser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By default, S3 applies restrictive permissions for buckets and objects. The
    account that creates them has full read-write permissions, but access is completely
    denied for anyone else. This means that the file that we've just uploaded can
    only be downloaded if the download request includes authentication for our account.
    If we try the resulting URL in a browser, then we'll get an access denied error.
    This isn't very useful if we're trying to use S3 for sharing files with other
    people.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution for this is to use one of S3''s mechanisms for changing the permissions.
    Let''s look at the simple task of making our uploaded file public. Change `upload_file()`
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We have now included a header in our HTTP request, `x-amz-acl`, which specifies
    a permission set to be applied to the object. We've also added a new argument
    to our function signature so that we can specify the permission set on the command
    line. We have used the so-called **canned** **ACLs** (**canned** **Access Control
    Lists**), which have been provided by S3, and are documented at [http://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl](http://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl).
  prefs: []
  type: TYPE_NORMAL
- en: 'The ACL that we''re interested in is called `public-read`. This will allow
    anyone to download the file without needing any kind of authentication. We can
    now re-run our upload, but this time it will apply this ACL to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now, visiting the file's S3 URL in a browser will give us the option to download
    the file.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying an uploaded file in a web browser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have uploaded an image, then you may be wondering why the browser had
    asked us to save it instead of just displaying it. The reason is that we haven't
    set the file's `Content-Type`.
  prefs: []
  type: TYPE_NORMAL
- en: If you remember from the last chapter, the `Content-Type` header in an HTTP
    response tells the client, which in this case is our browser, the type of file
    that is in the body. By default, S3 applies the content type of `binary/octet-stream`.
    Because of this `Content-Type`, the browser can't tell that it's downloading an
    image, so it just presents it as a file that can be saved. We can fix this by
    supplying a `Content-Type` header in the upload request. S3 will store the type
    that we specify, and it will use it as the `Content-Type` in the subsequent download
    responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the code block shown here to the import at the beginning of `s3_client.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then change `upload_file()` to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have used the `mimetypes` module to guess a suitable `Content-Type`
    by looking at the file extension of `local_path`. If `mimetypes` can't determine
    a `Content-Type` from `local_path`, then we don't include the `Content-Type` header,
    and let S3 apply the default `binary/octet-stream` type.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, in S3 we won't be able to overwrite the metadata for an existing
    object by using a simple `PUT` request. It's possible to do it by using a `PUT`
    copy request, but that's beyond the scope of this chapter. For now, it's better
    to just delete the file from S3 by using the AWS Console before uploading it again.
    We only need to do this once. Now, our code will automatically add the `Content-Type`
    for any new file that we upload.
  prefs: []
  type: TYPE_NORMAL
- en: Once you've deleted the file, re-run the client just as shown in the last section,
    that is, upload the file with the new `Content-Type` and try to download the file
    in a browser again. If all goes well, then the image will be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading a file with the API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Downloading a file through the S3 API is similar to uploading it. We simply
    take the bucket name, the S3 object name and the local filename again but issue
    a `GET` request instead of a `PUT request`, and then write the data received to
    disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following function to your program, underneath the `upload_file()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run the client and download a file, which you have uploaded previously,
    by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Parsing XML and handling errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you ran into any errors while running the aforementioned code, then you''ll
    notice that a clear error message will not get displayed. S3 embeds error messages
    in the XML returned in the response body, and until now we''ve just been dumping
    the raw XML to the screen. We can improve on this and pull the text out of the
    XML. First, let''s generate an error message so that we can see what the XML looks
    like. In `s3_client.py`, replace your access secret with an empty string, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, try and perform the following operation on the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The preceding XML is the S3 error information. I've truncated several of the
    fields so as to show it here. Your code block will be slightly longer than this.
    In this case, it's telling us that it can't authenticate our request, and this
    is because we have set a blank access secret.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing XML
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Printing all of the XML is too much for an error message. There's a lot of extraneous
    information which isn't useful to us. It would be better if we could just pull
    out the useful parts of the error message and display them.
  prefs: []
  type: TYPE_NORMAL
- en: Well, `ElementTree` gives us some powerful tools for extracting such information
    from XML. We're going back to XML for a while to explore these tools a little.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we need to open an interactive Python shell, and then generate the aforementioned
    error message again by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: You'll need to replace `<ID>` with your AWS access ID. Print out `r.text` to
    make sure that you get an error message, which is similar to the one that we generated
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can explore our XML. Convert the XML text into an `ElementTree` tree.
    A handy function for doing this is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We now have an ElementTree instance, with `root` as the root element.
  prefs: []
  type: TYPE_NORMAL
- en: Finding elements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The simplest way of navigating the tree is by using the elements as iterators.
    Try doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Iterating over `root` returns each of its child elements, and then we print
    out the tag of an element by using the `tag` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply a filter to the tags that we iterate over by using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have used the `findall()` method of the `root` element. This method
    will provide us with a list of all the direct children of the `root` element that
    match the specified tag, which in this case is `<Message>`.
  prefs: []
  type: TYPE_NORMAL
- en: And this will solve our problem of just extracting the text of the error message.
    Now, let's update our error handling.
  prefs: []
  type: TYPE_NORMAL
- en: Handling errors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can go back and add this to our `s3_client.py` file, but let''s include
    a little more information in the output, and structure the code to allow re-use.
    Add the following function to the file underneath the `download_file()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: You'll notice that we have used a new function here, namely, `root.find()`.
    This works in the same way as `findall()` except that it only returns the first
    matching element, as opposed to a list of all matching elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, replace each instance of `xml_pprint(r.text)` in your file with `handle_error(r)`
    and then run the client again with the incorrect access secret. Now, you will
    see a more informative error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Further enhancements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: That's as far as we're going to take our client. We've written a command line
    program that can perform essential operations, such as creating buckets and uploading
    and downloading objects on the Amazon S3 service. There are still plenty of operations
    that can be implemented, and these can be found in the S3 documentation; operations
    such as listing buckets' contents, deleting objects, and copying objects.
  prefs: []
  type: TYPE_NORMAL
- en: We could improve a few other things, especially if we are going to make this
    into a production application. The command-line parsing mechanism, although compact,
    is not satisfactory from a security perspective, since anybody with access to
    the command line can run any built-in python command. It would be better to have
    a whitelist of functions and to implement a proper command line parser by using
    one of the standard library modules like `argparse`.
  prefs: []
  type: TYPE_NORMAL
- en: Storing the access ID and the access secret in the source code is also a problem
    for security. Several serious security incidents have happened because passwords
    were stored in source code and then uploaded to cloud code repositories. It's
    much better to load the keys from an external source, such as a file or a database
    at run time.
  prefs: []
  type: TYPE_NORMAL
- en: The Boto package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've discussed working directly with the S3 REST API, and this has given us
    some useful techniques that will allow us to program against similar APIs in the
    future. In many cases, this will be the only way in which we can interact with
    a web API.
  prefs: []
  type: TYPE_NORMAL
- en: However, some APIs, including AWS, have ready-to-use packages which expose the
    functionality of the service without having to deal with the complexities of the
    HTTP API. These packages generally make the code cleaner and simpler, and they
    should be preferred for doing production work if they're available.
  prefs: []
  type: TYPE_NORMAL
- en: The AWS package is called **Boto**. We will take a very quick look at the `Boto`
    package to see how it can provide some of the functionalities that we wrote earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `boto` package is available in PyPi, so we can install it with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, fire up a Python shell and let''s try it out. We need to connect to the
    service first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll need to replace `<ACCESS ID>` and `<ACCESS SECRET>` with your access
    ID and access secret. Now, let''s create a bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates the bucket in the default standard US region. We can supply a
    different region, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The region names we need to use for this function are different to the ones
    we used when creating buckets earlier. To see a list of acceptable region names
    do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Do the following to display a list of the buckets we own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also list the contents of a bucket. To do so, first, we need to get
    a reference to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'And then to list the contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Uploading a file is a straightforward process. First, we need to get a reference
    to the bucket that we want to put it in, and then we need to create a `Key` object,
    which will represent our object in the bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have to set the `Key` name and then upload our file data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The `boto` package will automatically set the `Content-Type` when it uploads
    a file like this, and it uses the same `mimetypes` module that we used earlier
    for determining a type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Downloading also follows a similar pattern. Try the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: This downloads the `parrot.txt` S3 object in the `mybucket.example.com` bucket
    and then stores it in the `~/parrot.txt` local file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have a reference to the key, just use the following to set the ACL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: I'll leave you to further explore the `boto` package's functionality with the
    help of the tutorial, which can be found at [https://boto.readthedocs.org/en/latest/s3_tut.html](https://boto.readthedocs.org/en/latest/s3_tut.html).
  prefs: []
  type: TYPE_NORMAL
- en: It should be evident that for everyday S3 work in Python, `boto` should be your
    go to package.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping up with S3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, we've discussed some of the uses of the Amazon S3 API, and learned some
    things about working with XML in Python. These skills should give you a good start
    in working with any XML based REST API, whether or not it has a pre-built library
    like `boto`.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, XML isn''t the only data format that is used by web APIs, and the
    S3 way of working with HTTP isn''t the only model used by web APIs. So, we''re
    going to move on and take a look at the other major data format in use today,
    JSON and another API: Twitter.'
  prefs: []
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**JavaScript Object Notation (JSON)** is a standard way of representing simple
    objects, such as `lists` and `dicts`, in the form of text strings. Although, it
    was originally developed for JavaScript, JSON is language independent and most
    languages can work with it. It''s lightweight, yet flexible enough to handle a
    broad range of data. This makes it ideal for exchanging data over HTTP, and a
    large number of web APIs use this as their primary data format.'
  prefs: []
  type: TYPE_NORMAL
- en: Encoding and decoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We use the `json` module for working with JSON in Python. Let''s create a JSON
    representation of a Python list by using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `json.dumps()` function for converting an object to a JSON string.
    In this case, we can see that the JSON string appears to be identical to Python''s
    own representation of a list, but note that this is a string. Confirm this by
    doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Converting JSON to a Python object is also straightforward, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We use the `json.loads()` function, and just pass it a JSON string. As we'll
    see, this is very powerful when interacting with web APIs. Typically, we will
    receive a JSON string as the body of an HTTP response, which can simply be decoded
    using `json.loads()` to provide immediately usable Python objects.
  prefs: []
  type: TYPE_NORMAL
- en: Using dicts with JSON
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JSON natively supports a mapping-type object, which is equivalent to a Python
    `dict`. This means that we can work directly with `dicts` through JSON.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Also, it is useful to know how JSON handles nested objects.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'There is just one gotcha though: JSON dictionary keys can only be in the form
    of strings.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice, how the keys in the JSON dictionary become string representations of
    integers? To decode a JSON dictionary that uses numeric keys, we need to manually
    type-convert them if we want to work with them as numbers. Do the following to
    accomplish this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We just use a dictionary comprehension to apply `int()` to the dictionary's
    keys.
  prefs: []
  type: TYPE_NORMAL
- en: Other object types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'JSON cleanly handles only Python `lists` and `dicts`, for other object types
    `json` may attempt to cast the object type as one or the other, or fail completely.
    Try a tuple, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'JSON doesn''t have a tuple data type, so the `json` module will cast it to
    a `list`. If we convert it back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'It will still remain a `list`. The `json` module doesn''t support `sets`, so
    they also need to be recast as `lists`. Try the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: This will cause problems similar to the ones caused by tuples. If we convert
    the JSON back to a Python object, then it will be a `list` and not a `set`.
  prefs: []
  type: TYPE_NORMAL
- en: We almost never encounter web APIs that need these kinds of specialist Python
    objects, and if we do, then the API should provide some kind of convention for
    handling it. But we do need to keep track of any conversions that we would need
    to apply to the outgoing or the incoming objects, if we were storing the data
    locally in any format other than that of `lists` or `dicts`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an understanding of JSON, let's see how it works in a web API.
  prefs: []
  type: TYPE_NORMAL
- en: The Twitter API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Twitter API provides access to all the functions that we may want a Twitter
    client to perform. With the Twitter API, we can create clients that search for
    recent tweets, find out what's trending, look up user details, follow users' timelines,
    and even act on the behalf of users by posting tweets and direct messages for
    them.
  prefs: []
  type: TYPE_NORMAL
- en: We'll be looking at Twitter API version 1.1, the version current at time of
    writing this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Twitter maintains comprehensive documentation for its API, which can be found
    at [https://dev.twitter.com/overview/documentation](https://dev.twitter.com/overview/documentation).
  prefs: []
  type: TYPE_NORMAL
- en: A Twitter world clock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To illustrate some of the functionalities of the Twitter API, we're going to
    write the code for a simple Twitter world clock. Our application will periodically
    poll its Twitter account for mentions which contain a recognizable city name,
    and if it finds one, then it will reply to the Tweet with the current local time
    of that city. In Twitter speak, a mention is any Tweet which includes our account
    name prefixed by an `@`, for example, `@myaccount`.
  prefs: []
  type: TYPE_NORMAL
- en: Authentication for Twitter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to S3, we need to determine how authentication will be managed before
    we get started. We need to register, and then we need to find out how Twitter
    expects us to authenticate our requests.
  prefs: []
  type: TYPE_NORMAL
- en: Registering your application for the Twitter API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We need to create a Twitter account, register our application against the account,
    and then we will receive the authentication credentials for our app. It's also
    a good idea to set up a second account, which we can use for sending test tweets
    to the application account. This provides for a cleaner way of checking whether
    the app is working properly, rather than having the app account send tweets to
    itself. There's no limit on the number of Twitter accounts that you can create.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create an account, go to [http://www.twitter.com](http://www.twitter.com)
    and complete the signup process. Do the following for registering your application
    once you have a Twitter account:'
  prefs: []
  type: TYPE_NORMAL
- en: Log into [http://apps.twitter.com](http://apps.twitter.com) with your main Twitter
    account, and then create a new app.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fill out the new app form, note that Twitter application names need to be unique
    globally.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the app's settings and then change the app permissions to have read and
    write access. You may need to register your mobile number for enabling this. Even
    if you're unhappy about supplying this, we can create the full app; however the
    final function that sends a tweet in reply won't be active.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now we need to get our access credentials, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the **Keys and Access Tokens** section and then note the **Consumer Key**
    and the **Access Secret**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an **Access Token**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note down the **Access Token** and the **Access Secret**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Authenticating requests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now have enough information for authenticating requests. Twitter uses an
    authentication standard called **oAuth,** version 1.0a. It's described in detail
    at [http://oauth.net/core/1.0a/](http://oauth.net/core/1.0a/).
  prefs: []
  type: TYPE_NORMAL
- en: The oAuth authentication standard is a little tricky, but fortunately the `Requests`
    module has a companion library called `requests-oauthlib`, which can handle most
    of the complexity for us. This is available on PyPi, so we can download and install
    it with `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can add authentication to our requests, and then write our application.
  prefs: []
  type: TYPE_NORMAL
- en: A Twitter client
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Save the code mentioned here to a file, and save it as `twitter_worldclock.py`.
    You''ll need to replace `<CONSUMER_KEY>`, `<CONSUMER_SECRET>`, `<ACCESS_TOKEN>`,
    and `<ACCESS_SECRET>` with the values that you have taken down from the aforementioned
    Twitter app configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Remember that `consumer_secret` and `access_secret` act as the password to your
    Twitter account, so in a production app they should be loaded from a secure external
    location instead of being hard-coded into the source code.
  prefs: []
  type: TYPE_NORMAL
- en: In the aforementioned code, we create the `OAuth1` authentication instance,
    `auth_obj`, in the `init_auth()` function by using our access credentials. We
    pass this to `Requests` whenever we need to make an HTTP request, and through
    it `Requests` handles the authentication. You can see an example of this in the
    `verify_credentials()` function.
  prefs: []
  type: TYPE_NORMAL
- en: In the `verify_credentials()` function, we test whether Twitter recognizes our
    credentials. The URL that we're using here is an endpoint that Twitter provides
    purely for testing whether our credentials are valid. It returns an HTTP 200 status
    code if they are valid or a 401 status code if not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s run `twitter_worldclock.py` and if we''ve registered our application
    and filled out the tokens and secrets properly, then we should see `Validated
    credentials OK`. Now that the authentication is working, the basic flow of our
    program will be, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A Twitter client](graphics/6008OS_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Our program will be running as a daemon, polling Twitter periodically to see
    whether there are any new tweets for us to process and reply to. When we poll
    the mentions timeline, we will download any new tweets that were received in a
    single batch since our last poll, so that we can process all of them without having
    to poll again.
  prefs: []
  type: TYPE_NORMAL
- en: Polling for Tweets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s add a function for checking and retrieving new tweets from our mentions
    timeline. We''ll get this to work before we add the loop. Add the new function
    underneath `verify_credentials()`, and then add a call this function to the main
    section, as shown here; also, add `json` to the list of the imports at the beginning
    of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `get_mentions()`, we check for and download any tweets that mention our
    app account by connecting to the `statuses/mentions_timeline.json` endpoint. We
    supply a number of parameters, which `Requests` passes on as a query string. These
    parameters are specified by Twitter and they control how the tweets will be returned
    to us. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''count''`: This specifies the maximum number of tweets that will be returned.
    Twitter will allow 200 tweets to be received by a single request made to this
    endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''include_entities''`: This is used for trimming down some extraneous information
    from the tweets retrieved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''include_rts''`: This tells Twitter not to include any retweets. We don''t
    want the user to receive another time update if someone retweets our reply.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''since_id''`: This tells Twitter to only return the tweets with IDs above
    this value. Every tweet has a unique 64-bit integer ID, and later tweets have
    higher value IDs than earlier tweets. By remembering the ID of the last tweet
    we process and then passing it as this parameter, Twitter will filter out the
    tweets that we''ve already seen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before running the aforementioned, we want to generate some mentions for our
    account so that we have something to download. Log into your Twitter test account
    and then create a couple of tweets that contain `@username`, where you replace
    `username` with your app account's username. After this, when you go into the
    **Mentions** section of the **Notifications** tab of your app account, you will
    see these tweets.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we run the aforementioned code, then we will get the text of our mentions
    printed to screen.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the Tweets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step is to parse our mentions and then generate the times that we
    want to include in our replies. Parsing is a straightforward process. In this,
    we just check the ''text'' value of the tweets, but it takes a little more work
    to generate the times. In fact, for this, we''ll need a database of cities and
    their time zones. This is available in the `pytz` package, which can be found
    at PyPi. For doing this, install the following package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'And then, we can write our tweet processing function. Add this function underneath
    `get_mentions()`, and then add `datetime` and `pytz` to the list of the imports
    at the beginning of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: The bulk of `process_tweet()` is used for formatting the tweet's text and processing
    the time zone data. First we will remove any `@username` mentions and `#hashtags`
    from the tweet. Then, we prepare the remaining tweet text to be compared with
    the time zone names database. The time zone names database is held in `pytz.common_timezones`,
    but the names also contain regions, which are separated from the names with slashes
    (`/`). Also, in these names underscores are used in place of spaces.
  prefs: []
  type: TYPE_NORMAL
- en: We scan through the database checking against the formatted tweet text. If a
    match is found, then we construct a reply, which contains the local time of the
    matched time zone. For this, we use the `datetime` module along with a time zone
    object generated by `pytz.` If we don't find a match in the time zone database,
    then we compose a reply to let the user know the same. Then, we print our reply
    to screen to check if it's working as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Again, before running this, we may want to create a few tweets that contain
    just a city name and mention our world clock app account, so that the function
    has something to process. Some cities that appear in the time zone database are
    Dublin, New York, and Tokyo.
  prefs: []
  type: TYPE_NORMAL
- en: Give it a try! When you run it, you will get some tweet reply texts on the screen,
    which contain the cities and the current local times for those cities.
  prefs: []
  type: TYPE_NORMAL
- en: Rate limits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we run the aforementioned several times, then we'll find that it will stop
    working after a while. Either the credentials will temporarily fail to validate,
    or the HTTP request in `get_mentions()` will fail.
  prefs: []
  type: TYPE_NORMAL
- en: This is because Twitter applies rate limits to its API, which means that our
    application is only allowed to make a certain number of requests to an endpoint
    in a given amount of time. The limits are listed in the Twitter documentation
    and they vary according to the authentication route (as discussed later) and endpoint.
    We are using `statuses/mentions_timeline.json`, so our limit is 15 requests for
    every 15 minutes. If we exceed this, then Twitter will respond with a `429` `Too
    many requests` status code. This will force us to wait till the next 15 minute
    window starts before it lets us get any useful data back.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rate limits are a common feature of web APIs, so it''s useful to have ways
    of testing efficiently when using them. One approach to testing with data from
    rate-limited APIs is to download some data once and then store it locally. After
    this, load it from the file instead of pulling it from the API. Download some
    test data by using the Python interpreter, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: You'll need to be in the same folder as `twitter_worldclock.py` when you run
    this`.` This creates a file called `test_mentions.json,` which contains our JSONized
    mentions. Here, the `json.dump()` function writes the supplied data into a file
    rather than returning it as a string.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of calling the API, we can use this data by modifying our program''s
    main section to look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Sending a reply
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The final function that we need to perform is sending a tweet in response to
    a mention. For this, we use the `statuses/update.json` endpoint. If you''ve not
    registered your mobile number with your app account, then this won''t work. So,
    just leave your program as it is. If you have registered your mobile number, then
    add this function under `process_tweets()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'And add this below the `print()` call at the end of `process_tweet()`, at the
    same indentation level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Now, if you run this and then check your test account's Twitter notifications,
    you will see some replies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `post_reply()` function just calls the endpoint by using the following
    parameters to inform Twitter on what to post:'
  prefs: []
  type: TYPE_NORMAL
- en: '`status`: This is the text of our reply tweet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`in_reply_to_status_id`: This is the ID of the tweet that we''re replying to.
    We supply this so that Twitter can link the tweets as a conversation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When testing this, we might get some `403` status code responses. This is okay,
    it's just that Twitter refuses to let us post two tweets with identical text in
    a row, which we may find happens with this set up, depending on what test tweets
    we send.
  prefs: []
  type: TYPE_NORMAL
- en: Final touches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The building blocks are in place, and we can add our main loop to make the
    program a daemon. Add the `time` module to the imports at the top, and then change
    the main section to what is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: This will call `get_mentions()` every 60 seconds and then process any new tweets
    that have been downloaded. If we hit any HTTP errors, then it will retry the process
    15 times before exiting the program.
  prefs: []
  type: TYPE_NORMAL
- en: Now if we run our program, then it will run continuously, replying to tweets
    that mention the world clock app account. Give it a try, run the program, and
    then send some tweets from your test account. After a minute, you will see some
    replies to your notifications.
  prefs: []
  type: TYPE_NORMAL
- en: Taking it further
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we've written a basic functional Twitter API client, there are certainly
    some things that we could improve upon. Although we don't have space in this chapter
    to explore enhancements in detail, it's worth mentioning a few to inform future
    projects you may want to undertake.
  prefs: []
  type: TYPE_NORMAL
- en: Polling and the Twitter streaming APIs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may have already spotted a problem that our client will only pull a maximum
    of 200 tweets per poll. In each poll, Twitter provides the most recent tweets
    first. This means that if we get more than 200 tweets in 60 seconds, then we will
    permanently lose the tweets that come in first. In fact, there is no complete
    solution for this using the `statuses/mentions_timeline.json` endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Twitter's solution for this problem is to provide an alternative type of API,
    which is called a **streaming API**. When connecting to these APIs, the HTTP response
    connection is actually left open and the incoming tweets are continuously streamed
    through it. The `Requests` package provides neat functionality for handling this.
    The `Requests` response objects have an `iter_lines()` method, which runs indefinitely.
    It is capable of outputting a line of data whenever the server sends one, which
    can then be processed by us. If you do find that you need this, then there's an
    example that will help you in getting started in the Requests documentation, and
    it can be found at [http://docs.python-requests.org/en/latest/user/advanced/#streaming-requests](http://docs.python-requests.org/en/latest/user/advanced/#streaming-requests).
  prefs: []
  type: TYPE_NORMAL
- en: Alternative oAuth flows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our setup for having our app operate against our main account and having a second
    account for sending the test tweets is a little clunky, especially so if you use
    your app account for regular tweeting. Wouldn't it be better to have a separate
    account dedicated to handling the world clock tweets?
  prefs: []
  type: TYPE_NORMAL
- en: Well, yes it would. The ideal set up is to have a main account on which you
    register the app, and which you can also use it as a regular Twitter account,
    and have the app process tweets for a second dedicated world clock account.
  prefs: []
  type: TYPE_NORMAL
- en: oAuth makes this possible, but there are some extra steps that are needed to
    get it to work. We would need the world clock account to authorize our app to
    act on its behalf. You'll notice that the oAuth credentials mentioned earlier
    are comprised of two main elements, **consumer** and **access**. The consumer
    element identifies our application, and the access element proves that the account
    the access credentials came from authorized our app to act on its behalf. In our
    app we shortcut the full account authorization process by having the app act on
    behalf of the account through which it was registered, that is, our app account.
    When we do this, Twitter lets us acquire the access credentials directly from
    the [dev.twitter.com](http://dev.twitter.com) interface. To use a different user
    account, we would have needed to have inserted a step where the user is taken
    to Twitter, which would be opened in a web browser, where the user would have
    to log in and then explicitly authorize our application.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This process is demonstrated in the `requests-oauthlib` documentation, which
    can be found at [https://requests-oauthlib.readthedocs.org/en/latest/oauth1_workflow.html](https://requests-oauthlib.readthedocs.org/en/latest/oauth1_workflow.html).
  prefs: []
  type: TYPE_NORMAL
- en: HTML and screen scraping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although more and more services are offering their data through APIs, when a
    service doesn't do this then the only way of getting the data programmatically
    is to download its web pages and then parse the HTML source code. This technique
    is called **screen scraping**.
  prefs: []
  type: TYPE_NORMAL
- en: Though it sounds simple enough in principle, screen scraping should be approached
    as a last resort. Unlike XML, where the syntax is strictly enforced and data structures
    are usually reasonably stable and sometimes even documented, the world of web
    page source code is a messy one. It is a fluid place, where the code can change
    unexpectedly and in a way that can completely break your script and force you
    to rework the parsing logic from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Still, it is sometimes the only way to get essential data, so we're going to
    take a brief look at developing an approach toward scraping. We will discuss ways
    to reduce the impact when the HTML code does change.
  prefs: []
  type: TYPE_NORMAL
- en: You should always check a site's terms and conditions before scraping. Some
    websites explicitly disallow automated parsing and retrieval. Breaching the terms
    may result in your IP address being barred. However, in most cases, as long as
    you don't republish the data and don't make excessively frequent requests, you
    should be okay.
  prefs: []
  type: TYPE_NORMAL
- en: HTML parsers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll be parsing HTML just as we parsed XML. We again have a choice between
    pull-style APIs and object-oriented APIs. We are going to use `ElementTree` for
    the same reasons as mentioned before.
  prefs: []
  type: TYPE_NORMAL
- en: There are several HTML parsing libraries that are available. They're differentiated
    by their speed, the interfaces that they offer for navigating within HTML documents,
    and their ability at handling badly constructed HTML. The Python standard library
    doesn't include an object-oriented HTML parser. The universally recommended third-party
    package for this is `lxml`, which is primarily an XML parser. However, it does
    include a very good HTML parser. It's quick, it offers several ways of navigating
    documents, and it is tolerant of broken HTML.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `lxml` library can be installed on Debian and Ubuntu through the `python-lxml`
    package. If you need an up-to-date version or if you''re not able to install the
    system packages, then `lxml` can be installed through `pip`. Note that you''ll
    need a build environment for this. Debian usually comes with an environment that
    has already been set up but if it''s missing, then the following will install
    one for both Debian and Ubuntu:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you should be able to install `lxml`, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'If you hit compilation problems on a 64-bit system, then you can also try:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: On Windows, installer packages are available from the `lxml` website at [http://lxml.de/installation.html](http://lxml.de/installation.html).
    Check the page for links to third-party installers in case an installer for your
    version of Python isn't available.
  prefs: []
  type: TYPE_NORMAL
- en: The next best library, in case `lxml` doesn't work for you, is BeautifulSoup.
    BeautifulSoup is pure Python, so it can be installed with `pip`, and it should
    run anywhere. Although it has its own API, it's a well-respected and capable library,
    and it can, in fact, use `lxml` as a backend library.
  prefs: []
  type: TYPE_NORMAL
- en: Show me the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we start parsing HTML, we need something to parse! Let's grab the version
    and codename of the latest stable Debian release from the Debian website. Information
    about the current stable release can be found at [https://www.debian.org/releases/stable/](https://www.debian.org/releases/stable/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The information that we want is displayed in the page title and in the first
    sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Show me the data](graphics/6008OS_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, we should extract the *"jessie"* codename and the 8.0 version number.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing HTML with lxml
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's open a Python shell and get to parsing. First, we'll download the page
    with `Requests`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Next, we parse the source into an `ElementTree` tree. This is the same as it
    is for parsing XML with the standard library's `ElementTree`, except here we will
    use the `lxml` specialist `HTMLParser`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: The `HTML()` function is a shortcut that reads the HTML that is passed to it,
    and then it produces an XML tree. Notice that we're passing `response.content`
    and not `response.text`. The `lxml` library produces better results when it uses
    the raw response rather than the decoded Unicode text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `lxml` library''s `ElementTree` implementation has been designed to be
    100 percent compatible with the standard library''s, so we can start exploring
    the document in the same way as we did with XML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have printed out the text content of the document's
    `<title>` element, which is the text that appears in the tab in the preceding
    screenshot. We can already see it contains the codename that we want.
  prefs: []
  type: TYPE_NORMAL
- en: Zeroing in
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Screen scraping is the art of finding a way to unambiguously address the elements
    in the HTML that contain the information that we want, and extract the information
    from only those elements.
  prefs: []
  type: TYPE_NORMAL
- en: However, we also want the selection criteria to be as simple as possible. The
    less we rely on the contents of the document, the lesser the chance of it being
    broken if the page's HTML changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s inspect the HTML source of the page, and see what we''re dealing with.
    For this, either use `View Source` in a web browser, or save the HTML to a file
    and open it in a text editor. The page''s source code is also included in the
    source code download for this book. Search for the text `Debian 8.0`, so that
    we are taken straight to the information we want. For me, it looks like the following
    block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: I've skipped the HTML between the `<body>` and the `<div>` to show that the
    `<div>` is a direct child of the `<body>` element. From the above, we can see
    that we want the contents of the `<p>` tag child of the `<div>` element.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we navigated to this element by using the `ElementTree` functions, which
    we have used before, then we''d end up with something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: But this isn't the best approach, as it depends quite heavily on the HTML structure.
    A change, such as a `<div>` tag being inserted before the one that we needed,
    would break it. Also, in more complex documents, this can lead to horrendous chains
    of method calls, which are hard to maintain. Our use of the `<title>` tag in the
    previous section to get the codename is an example of a good technique, because
    there is always only one `<head>` and one `<title>` tag in a document. A better
    approach to finding our `<div>` would be to make use of the `id="content"` attribute
    it contains. It's a common web page design pattern to break a page into a few
    top-level `<divs>` for the major page sections like the header, the footer and
    the content, and to give the `<divs> id` attributes which identify them as such.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, if we could search for `<div>`s with an `id` attribute of `"content"`,
    then we'd have a clean way of selecting the right `<div>.` There is only one `<div>`
    in the document that is a match, and it's unlikely that another`<div>` like that
    will be added to the document. This approach doesn't depend on the document structure,
    and so it won't be affected by any changes that are made to the structure. We'll
    still need to rely on the fact that the `<p>` tag in the `<div>` is the first
    `<p>` tag that appears, but given that there is no other way to identify it, this
    is the best we can do.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we run such a search for our content `<div>`?
  prefs: []
  type: TYPE_NORMAL
- en: Searching with XPath
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to avoid exhaustive iteration and the checking of every element, we
    need to use **XPath**, which is more powerful than what we've used so far. It
    is a query language that was developed specifically for XML, and it's supported
    by `lxml`. Plus, the standard library implementation provides limited support
    for it.
  prefs: []
  type: TYPE_NORMAL
- en: We're going to take a quick look at XPath, and in the process we will find the
    answer to the question posed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, use the Python shell from the last section, and do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the simplest form of XPath expression: it searches for children of
    the current element that have tag names that match the specified tag name. The
    current element is the one we call `xpath()` on, in this case `root`. The `root`
    element is the top-level `<html>` element in the HTML document, and so the returned
    element is the `<body>` element`.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'XPath expressions can contain multiple levels of elements. The searches start
    from the node the `xpath()` call is made on and work down the tree as they match
    successive elements in the expression. We can use this to find just the `<div>`
    child elements of `<body>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: The `body/div` expression means match `<div>` children of `<body>` children
    of the current element. Elements with the same tag can appear more than once at
    the same level in an XML document, so an XPath expression can match multiple elements,
    hence the `xpath()` function always returns a list.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding queries are relative to the element that we call `xpath()` on,
    but we can force a search from the root of the tree by adding a slash to the start
    of the expression. We can also perform a search over all the descendants of an
    element, with the help of a double-slash. To do this, try the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Here, we've directly found our `<h1>` element by only specifying a single tag,
    even though it's several levels below `root`. This double-slash at the beginning
    of the expression will always search from the root, but we can prefix this with
    a dot if we want it to start searching from the context element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: This will not find anything because there are no `<h1>` descendents of `<head>`.
  prefs: []
  type: TYPE_NORMAL
- en: XPath conditions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, we can be quite specific by supplying paths, but the real power of XPath
    lies in applying additional conditions to the elements in the path. In particular,
    our aforementioned problem, which is, testing element attributes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The square brackets after `div`, `[@id="content"]`, form a condition that we
    place on the `<div>` elements that we''re matching. The `@` sign before `id` means
    that `id` refers to an attribute, so the condition means: only elements with an
    `id` attribute equal to `"content"`. This is how we can find our content `<div>`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we employ this to extract our information, let''s just touch on a couple
    of useful things that we can do with conditions. We can specify just a tag name,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns all `<div>` elements which have an `<h1>` child element. Also
    try:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Putting a number as a condition will return the element at that position in
    the matched list. In this case this is the second `<div>` child element of `<body>`.
    Note that these indexes start at `1`, unlike Python indexing which starts at `0`.
  prefs: []
  type: TYPE_NORMAL
- en: There's a lot more that XPath can do, the full specification is a **World Wide
    Web Consortium** (**W3C**) standard. The latest version can be found at [http://www.w3.org/TR/xpath-3/](http://www.w3.org/TR/xpath-3/).
  prefs: []
  type: TYPE_NORMAL
- en: Pulling it together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we''ve added XPath to our superpowers, let''s finish up by writing
    a script to get our Debian version information. Create a new file, `get_debian_version.py`,
    and save the following to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have downloaded and parsed the web page by pulling out the text that
    we want with the help of XPath. We have used a regular expression to pull out
    *jessie*, and a `split` to extract the version 8.0\. Finally we print it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, run it like it is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Magnificent. Well, darned nifty, at least. There are some third-party packages
    available which can speed up scraping and form submission, two popular ones are
    Mechanize and Scrapy. Check them out at [http://wwwsearch.sourceforge.net/mechanize/](http://wwwsearch.sourceforge.net/mechanize/),
    and [http://scrapy.org](http://scrapy.org).
  prefs: []
  type: TYPE_NORMAL
- en: With great power...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an HTTP client developer, you may have different priorities to the webmasters
    that run websites. A webmaster will typically provide a site for human users;
    possibly offering a service designed for generating revenue, and it is most likely
    that all this will need to be done with the help of very limited resources. They
    will be interested in analyzing how humans use their site, and may have areas
    of the site they would prefer that automated clients didn't explore.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP clients that automatically parse and download pages on websites are called
    various things, such as *bots*, *web crawlers*, and *spiders*. Bots have many
    legitimate uses. All the search engine providers make extensive use of bots for
    crawling the web and building their huge page indexes. Bots can be used to check
    for dead links, and to archive sites for repositories, such as the Wayback Machine.
    But, there are also many uses that might be considered as illegitimate. Automatically
    traversing an information service to extract the data on its pages and then repackaging
    that data for presentation elsewhere without permission of the site owners, downloading
    large batches of media files in one go when the spirit of the service is online
    viewing and so on could be considered as illegitimate. Some sites have terms of
    service which explicitly bar automated downloads. Although some actions such as
    copying and republishing copyrighted material are clearly illegitimate, some other
    actions are subject to interpretation. This gray area is a subject of ongoing
    debate, and it is unlikely that it will ever be resolved to everyone's satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: However, even when they do serve a legitimate purpose, in general, bots do make
    webmasters lives somewhat more difficult. They pollute the webserver logs, which
    webmasters use for calculating statistics on how their site is being used by their
    human audience. Bots also consume bandwidth and other server resources.
  prefs: []
  type: TYPE_NORMAL
- en: Using the methods that we are looking at in this chapter, it is quite straightforward
    to write a bot that performs many of the aforementioned functions. Webmasters
    provide us with services that we will be using, so in return, we should respect
    the aforementioned areas and design our bots in such a way that they impact them
    as little as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a User Agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a few things that we can do to help our webmasters out. We should
    always pick an appropriate user agent for our client. The principle way in which
    webmasters filter out bot traffic from their logfiles is by performing user agent
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: There are lists of the user agents of known bots, for example, one such list
    can be found at [http://www.useragentstring.com/pages/Crawlerlist/](http://www.useragentstring.com/pages/Crawlerlist/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Webmasters can use these in their filters. Many webmasters also simply filter
    out any user agents that contain the words *bot*, *spider*, or *crawler*. So,
    if we are writing an automated bot rather than a browser, then it will make the
    webmasters'' lives a little easier if we use a user agent that contains one of
    these words. Many bots used by the search engine providers follow this convention,
    some examples are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Mozilla/5.0` `compatible; bingbot/2.0; http://www.bing.com/bingbot.htm`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Baiduspider: http://www.baidu.com/search/spider.htm`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Mozilla/5.0 compatible; Googlebot/2.1; http://www.google.com/bot.html`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also some guidelines in section 5.5.3 of the HTTP RFC 7231.
  prefs: []
  type: TYPE_NORMAL
- en: The Robots.txt file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is an unofficial but standard mechanism to tell bots if there are any
    parts of a website that they should not crawl. This mechanism is called `robots.txt`,
    and it takes the form of a text file called, unsurprisingly, `robots.txt`. This
    file always lives in the root of a website so that bots can always find it. It
    has rules that describe the accessible parts of the website. The file format is
    described at [http://www.robotstxt.org](http://www.robotstxt.org).
  prefs: []
  type: TYPE_NORMAL
- en: The Python standard library provides the `urllib.robotparser` module for parsing
    and working with `robots.txt` files. You can create a parser object, feed it a
    `robots.txt` file and then you can simply query it to see whether a given URL
    is permitted for a given user agent. A good example can be found in the documentation
    in the standard library. If you check every URL that your client might want to
    access before you access it, and honor the webmasters wishes, then you'll be helping
    them out.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, since we may be making quite a lot of requests as we test out our fledgling
    clients, it's a good idea to make local copies of the web pages or the files that
    you want your client to parse and test it against them. In this way, we'll be
    saving bandwidth for ourselves and for the websites.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've covered a lot of ground in this chapter, but you should now be able to
    start making real use of the web APIs that you encounter.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at XML, how to construct documents, parse them and extract data from
    them by using the `ElementTree` API. We looked at both the Python `ElementTree`
    implementation and `lxml`. We also looked at how the XPath query language can
    be used efficiently for extracting information from documents.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at the Amazon S3 service and wrote a client that lets us perform basic
    operations, such as creating buckets, and uploading and downloading files through
    the S3 REST API. We learned about setting access permissions and setting content
    types, such that the files work properly in web browsers.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at the JSON data format, how to convert Python objects into the JSON
    data format and how to convert them back to Python objects.
  prefs: []
  type: TYPE_NORMAL
- en: We then explored the Twitter API and wrote an on-demand world clock service,
    through which we learned how to read and process tweets for an account, and how
    to send a tweet as a reply.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how to extract or scrape information from the HTML source of web pages.
    We saw how to work with HTML when using `ElementTree` and the `lxml` HTML parser.
    We also learned how to use XPath to help make this process more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: And finally, we looked at how we can give back to the webmasters that provide
    us with all the data. We discussed a few ways in which we can code our clients
    to make the webmasters lives a little easier and respect how they would like us
    to use their sites.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, that''s it for HTTP for now. We''ll re-visit HTTP in [Chapter 9](ch09.html
    "Chapter 9. Applications for the Web"), *Applications for the Web*, where we''ll
    be looking at using Python for constructing the server-side of web applications.
    In the next chapter, we''ll discuss the other great workhorse of the Internet:
    e-mail.'
  prefs: []
  type: TYPE_NORMAL
