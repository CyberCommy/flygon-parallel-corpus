- en: Chapter 3. User-Defined Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the Docker network configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating user-defined networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting containers to networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a user-defined bridge network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a user-defined overlay network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isolating networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier versions of Docker relied on a mostly static network model, which worked
    relatively well for most container networking needs. However, if you wanted to
    do something different, you weren't left with many options. For instance, you
    could tell Docker to deploy containers to a different bridge, but there wasn't
    a strong integration point between Docker and that network. With the introduction
    of user-defined networking in Docker 1.9, the game has changed. You can now create
    and manage bridge and multi-host networks directly through the Docker engine.
    In addition, the door has also been opened for third-party network plugins to
    integrate with Docker through libnetwork and its **Container Network Model** (**CNM**)
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CNM is Docker's model for defining a container network model. In [Chapter 7](ch07.html
    "Chapter 7. Working with Weave Net"), *Working with Weave Net*, we'll examine
    a third-party plugin (Weave) that can integrate as a Docker driver. The focus
    in this chapter will be on the default network drivers natively included with
    a Docker engine.
  prefs: []
  type: TYPE_NORMAL
- en: The move to a driver-based model symbolizes a great change in Docker networking.
    In addition to defining new networks, you're now also given the ability to connect
    and disconnect container interfaces dynamically. This inherent flexibility opens
    the door to many new possibilities to connect containers.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the Docker network configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned, defining and managing networks can now be done directly through
    Docker with the addition of the `network` subcommand. The `network` command provides
    you with all the options you need to build networks and connect containers to
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this recipe, we'll learn how to view defined Docker networks as well as inspect
    them for specific details.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `docker network` subcommand was introduced in Docker 1.9, so you'll need
    a Docker host running at least that version. In our examples, we'll be using Docker
    version 1.12\. You'll also want to have a good understanding of your current network
    layout, so you can follow along as we inspect the current configuration. It is
    assumed that each Docker host is in its native configuration.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing we want to do is figure out what networks Docker thinks are
    already defined. This can be done using the `network ls` subcommand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, Docker shows that we have three different networks already defined.
    To view more information about a network, we can use the `network inspect` subcommand
    to retrieve specifics about the network definition as well as its current state.
    Let's take a close look at each defined network.
  prefs: []
  type: TYPE_NORMAL
- en: Bridge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The bridge network represents the `docker0` bridge that the Docker engine creates
    by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the `inspect` command shows us a wealth of information about
    the defined network:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Driver`: In this case, we can see that the network bridge implements the `Driver`
    bridge. Although this may seem obvious, it''s important to call out that all network
    functionality, including native functionality, is implemented through drivers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Subnet`: In this case, the `subnet` is the default we expect from the `docker0`
    bridge, `172.17.0.1/16`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bridge.default_bridge`: A value of `true` implies that Docker will provision
    all containers to this bridge unless told otherwise. That is, if you start a container
    without specifying a network (`--net`), the container will end up on this bridge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bridge.host_binding_ipv4`: By default, this will be set to `0.0.0.0` or all
    interfaces. As we saw in [Chapter 2](ch02.html "Chapter 2. Configuring and Monitoring
    Docker Networks"), *Configuring and Monitoring Docker Networks*, we can tell Docker
    at a service level to limit this by passing the `--ip` flag as a Docker option
    to the service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bridge.name`: As we suspected, this network represents the `docker0` bridge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`driver.mtu`: By default, this will be set to `1500`. As we saw in [Chapter
    2](ch02.html "Chapter 2. Configuring and Monitoring Docker Networks"), *Configuring
    and Monitoring Docker Networks*, we can tell Docker at a service level to change
    **MTU** (**Maximum Transmission Unit**) by passing the `--mtu` flag as a Docker
    option to the service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: None
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `none` network represents just what it says, nothing. The `none` mode is
    used when you wish to define a container with absolutely no network definition.
    After inspecting the network, we can see that there isn''t much there as far as
    a network definition is concerned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `Driver` is represented by `null`, implying that this isn't
    a `Driver` for this network at all. There are a few use cases for the `none` network
    mode and we'll cover those later on when we talk about connecting and disconnecting
    containers to defined networks.
  prefs: []
  type: TYPE_NORMAL
- en: Host
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The *host* network represents the host mode we saw in [Chapter 2](ch02.html
    "Chapter 2. Configuring and Monitoring Docker Networks"), *Configuring and Monitoring
    Docker Networks*, where a container was bound directly to the Docker host''s own
    network interfaces. By taking a closer look, we can see that much like the `none`
    network, there isn''t much defined for this network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Although the host network certainly does more than the `none` mode, it wouldn't
    appear so from inspecting its definition. The key difference here is that this
    network uses the host `Driver`. As this network type uses the existing host's
    network interfaces, we don't need to define any of that as part of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using the `network ls` command, you can pass additional parameters to
    further filter or alter the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--quiet` (`-q`): This only shows the numeric network IDs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--no-trunc`: This prevents the command from automatically truncating the network
    ID in the output that allows you to see the full network ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--filter` (`-f`): This filters the output based on either network ID, network
    name, or by network definition (built-in or user-defined)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, we can show all user-defined networks with this filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Or we can show all networks with a network ID that contains `158`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Creating user-defined networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we've seen so far, there are at least two different network drivers that
    are inherently part of every Docker installation, bridge, and host. In addition
    to those two, while not defined initially because of prerequisites, there is another
    `Driver` overlay that is available out-of-the-box as well. Later recipes in this
    chapter will cover specifics regarding the bridge and overlay drivers.
  prefs: []
  type: TYPE_NORMAL
- en: Because it wouldn't make sense to create another iteration of the host network
    using the host `Driver`, the built-in user-defined networks are limited to the
    bridge and overlay drivers. In this recipe, we'll show you the basics of creating
    a user-defined network as well as options that are relevant to the `network create`
    and `network rm` Docker subcommands.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `docker network` subcommand was introduced in Docker 1.9, so you'll need
    a Docker host running at least that version. In our examples, we'll be using Docker
    version 1.12\. You'll also want to have a good understanding of your current network
    layout, so you can follow along as we inspect the current configuration. It is
    assumed that each Docker host is in its native configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Warning: Creating network interfaces on a Linux host must be done with caution.
    Docker will do its best to prevent you from shooting yourself in the foot, but
    you must have a good idea of your network topology before defining new networks
    on a Docker host. A common mistake to avoid is to define a new network that overlaps
    with other subnets in your network. In the case of remote administration, this
    can cause host and container connectivity issues.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Networks are defined by using the `network create` subcommand, which has the
    following options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s spend a little time discussing what each of these options means:'
  prefs: []
  type: TYPE_NORMAL
- en: '`aux-address`: This allows you to define IP addresses that Docker should not
    assign to containers when they are spawned. These are the equivalent of IP reservations
    in a DHCP scope.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Driver`: Which `Driver` the network implements. The built-in options include
    bridge and overlay, but you can also use third-party drivers as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gateway`: The Gateway for the network. If not specified, Docker will assume
    that it is the first available IP address in the subnet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`internal`: This option allows you to isolate networks and is covered in greater
    detail later in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ip-range`: This allows you to specify a smaller subnet of the defined network
    subnet to use for container addressing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ipam-driver`: In addition to consuming third-party network drivers, you can
    also leverage third-party IPAM drivers. For the purposes of this book, we''ll
    be focusing mostly on the default or built-in IPAM `Driver`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ipv6`: This enables IPv6 networking on the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label`: This allows you to specify additional information about the network
    that will be stored as metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ipam-opt`: This allows you to specify options to pass to the IPAM `Driver`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`opt`: This allows you to specify options that can be passed to the network
    `Driver`. Specific options for each built-in `Driver` will be discussed in the
    relevant recipes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subnet`: This defines the subnet associated with the network type you are
    creating.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You might notice some overlap here between some of the settings we can define
    at a service level for Docker networking and the user-defined options listed in
    the preceding term list. Examining the options, you may be tempted to compare
    the following configuration flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: While these settings are largely equivalent, they are not all identical. The
    only two that act in the exact same fashion are `--fixed-cidr` and `ip-range`.
    Both of those options, define a smaller subnetwork of the larger master network
    to be used for container IP addressing. The other two options are similar, but
    not identical.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the service options, `--bip` applies to the `docker0` bridge
    and `--default-gateway` applies to the containers themselves. On the user-defined
    side, the `--subnet` and the `--gateway` option, apply directly to the network
    construct being defined (in this comparison, a bridge). Recall that the `--bip`
    option expects to receive an IP address in a network, not the network itself.
    Having the bridge IP defined in this manner covers both the subnet as well as
    the gateway, which are defined separately when defining a user-defined network.
    That being said, the service definition is a little more flexible in that it allows
    you to define both the interface of the bridge as well as the gateway assigned
    to containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keeping with the theme of having sane defaults, none of these options are actually
    required to create a user-defined network. You can create your first user-defined
    network by just giving it a name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon inspection, we can see what Docker uses for defaults:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Docker assumes that if you don't specify a `Driver` that you'd like to create
    a network using the bridge `Driver`. It also automatically chooses and assigns
    a subnet for this bridge if you don't define one when you create the network.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is always advisable that you specify subnets for network, you create. As
    we'll see later on, not all network topologies rely on hiding the container networks
    behind the host interfaces. In those cases, defining a routable non-overlapping
    subnet will be a necessity.
  prefs: []
  type: TYPE_NORMAL
- en: It also automatically selects the first useable IP address for the Subnet as
    the gateway. Because we didn't define any options specific to the `Driver`, the
    network has none but again, there are defaults that are used in this case. Those
    will be discussed in the recipes related to each specific `Driver`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Networks that are empty, that is, they have no active endpoints, may be deleted
    using the `network rm` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: One other item that's worth noting here is that Docker makes user-defined networks
    persistent. In most cases, any Linux network constructs that are manually defined
    are lost when the system reboots. Docker records the network configuration and
    takes care of replaying it back when the Docker service restarts. This is a huge
    advantage to building the networks through Docker rather than on your own.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting containers to networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While having the ability to create your own networks is a huge leap forward,
    it means nothing without a means to connect containers to it. In previous versions
    of Docker, this was traditionally done during container runtime by passing the
    `--net` flag specifying which network the container should use. While this is
    certainly still the case, the `docker network` subcommand also allows you to connect
    and disconnect running containers to existing networks.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `docker network` subcommand was introduced in Docker 1.9, so you'll need
    a Docker host running at least that version. In our examples, we'll be using Docker
    version 1.12\. You'll also want to have a good understanding of your current network
    layout, so you can follow along as we inspect the current configuration. It is
    assumed that each Docker host is in its native configuration.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Connecting and disconnecting containers is done via the `network connect` and
    `network disconnect` subcommands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s review what our options are for connecting containers to networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Alias**: This allows you to define an alias for container name resolution
    in the network you are connecting the container to. We''ll talk more about this
    in [Chapter 5](ch05.html "Chapter 5. Container Linking and Docker DNS"), *Container
    Linking and Docker DNS*, where we discuss DNS and linking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IP**: This defines an IP address to be used for the container. This will
    work so long as the IP address is not currently in use. Once allocated, it will
    remain reserved as long as the container is running or paused. Stopping the container
    will remove the reservation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IP6**: This defines an IPv6 address to be used for the container. The same
    allocation and reservation requirements that applied to the IPv4 address also
    apply to the IPv6 address.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Link**: This allows you to specify a link to another container. We''ll talk
    more about this in [Chapter 5](ch05.html "Chapter 5. Container Linking and Docker
    DNS"), *Container Linking and Docker DNS*, where we discuss DNS and linking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once a `network connect` request is sent, Docker handles all the configuration
    required in order for the container to start using the new interface. Let''s take
    a look at a quick example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the above output we started a simple container without specifying any network-related
    configuration. The result is the container being mapped to the `docker0` bridge.
    Now let''s try connecting this container to the network we created in the previous
    recipe, `mynetwork`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the container now has an IP interface on the network `mynetwork`.
    If we now once again inspect the network, we should see a container association:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Networks can be disconnected just as easily. For instance, we can now remove
    the container from the `docker0` bridge by removing it from the bridge network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s interesting to point out that Docker also takes care of ensuring container
    connectivity when you connect and disconnect networks from the containers. For
    instance, before disconnecting the container from the bridge network, the default
    Gateway of the container was still out of the `docker0` bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This makes sense as we wouldn''t want to interrupt container connectivity while
    connecting the container to a new network. However, once we remove the network
    hosting the default gateway by disconnecting the interface to the bridge network,
    we see that Docker updates the default gateway to the remaining interface out
    of the `mynetwork` bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This ensures that the container has connectivity regardless of which network
    it's connected to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, I want to point out an interesting aspect of the `none` network type
    when you are connecting and disconnecting containers to networks. As I mentioned
    earlier, the `none` network type tells Docker to not assign the container to any
    networks. This however, does not mean just initially, it''s a configuration state
    telling Docker that the container should not have any networks associated with
    it. For instance, assume we start the following container with a network of `none`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the container doesn''t have any network interfaces besides
    its loopback. Now, let''s try and connect this container to a new network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Docker is telling us that this container was defined to have no networks and
    is preventing us from connecting the container to any network. If we inspect the
    `none` network, we can see that this container is in fact attached to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to connect this container to a new network, we first have to disconnect
    it from the `none` network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Once you disconnect it from the `none` network, you are free to connect it to
    any other defined network.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a user-defined bridge network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Through the use of the bridge `Driver`, users can provision custom bridges to
    connect to containers. You can create as many as you like with the only real limitation
    being that you must use unique IP addressing on each bridge. That is, you can't
    overlap with existing subnets that are already defined on other network interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll learn how to define a user-defined bridge as well as some
    of the unique options available to you during its creation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `docker network` subcommand was introduced in Docker 1.9, so you'll need
    a Docker host running at least that version. In our examples, we'll be using Docker
    version 1.12\. You'll also want to have a good understanding of your current network
    layout, so you can follow along as we inspect the current configuration. It is
    assumed that each Docker host is in its native configuration.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous recipe, we talked about the process for defining a user-defined
    network. While the options discussed there are relevant to all network types,
    we can pass other options to the `Driver` our network implements by passing the
    `--opt` flag. Let''s quickly review the options that are available with the bridge
    `Driver`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`com.docker.network.bridge.name`: This is the name you wish to give to the
    bridge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`com.docker.network.bridge.enable_ip_masquerade`: This instructs the Docker
    host to hide or masquerade all containers in this network behind the Docker host''s
    interfaces if the container attempts to route off the local host .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`com.docker.network.bridge.enable_icc`: This turns on or off **Inter-Container
    Connectivity** (**ICC**) mode for the bridge. This feature is covered in greater
    detail in [Chapter 6](ch06.html "Chapter 6. Securing Container Networks"), *Securing
    Container Networks*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`com.docker.network.bridge.host_binding_ipv4`: This defines the host interface
    that should be used for port binding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`com.docker.network.driver.mtu`: This sets MTU for containers attached to this
    bridge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These options can be directly compared with the options we define under the
    Docker service to make changes to the default `docker0` bridge.
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding table compares the service-level settings that impact the `docker0`
    bridge to the settings available to you as part of defining a user-defined bridge
    network. It also lists the default setting used if the setting is not specified
    in either case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Between the Driver-specific options and the generic options that are part of
    the `network create` subcommand, we have quite a bit of flexibility when defining
    container networks. Let''s walk through a couple of quick examples of building
    user-defined bridges:'
  prefs: []
  type: TYPE_NORMAL
- en: Example 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding `network create` statement defines a network with the following
    characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: A user-defined network of type `bridge`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `subnet` of `10.15.20.0/24`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `gateway` or bridge IP interface of `10.15.20.1`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two reserved addresses: `10.15.20.2` and `10.15.20.3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A port binding interface of `10.10.10.101` on the host
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Linux interface name of `linuxbridge1`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Docker network name of `testbridge1`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that some of these options are included for example purpose only.
    Practically, we don't need to define the `Gateway` for the network `Driver` in
    the preceding example since the defaults will cover us.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we create the earlier-mentioned network upon inspection, we should see the
    attributes we defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The options you pass to the network are not validated. That is, if you misspell
    `host_binding` as `host_bniding`, Docker will still let you create the network,
    and the option will get defined; however, it won't work.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding `network create` statement defines a network with the following
    characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: A user-defined network of type `bridge`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `subnet` of `192.168.50.0/24`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `gateway` or bridge IP interface of `192.168.50.1`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A container network range of `192.168.50.128/25`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IP masquerade on the host turned off
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Docker network named `testbridge2`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As stated in Example 1, we don''t need to define the driver type if we''re
    creating a bridge network. In addition, if we''re OK with the gateway being the
    first available IP in the container defined subnet, we can exclude that from the
    definition as well. Inspecting this network after creation should show us results
    similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Creating a user-defined overlay network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the ability to create your own bridge is certainly appealing, your scope
    is still limited to that of a single Docker host. The overlay network `Driver`
    aims to solve that by allowing you to extend one or more subnets across multiple
    Docker hosts using an overlay network. Overlay networks are a means to build isolated
    networks on top of existing networks. In this case, the existing network provides
    transport for the overlay and is often named the **underlay network**. The overlay
    `Driver` implements what Docker refers to as multi-host networking.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll learn how to configure the prerequisites for the overlay
    `Driver` as well as deploy and validate overlay-based networks.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Throughout the following examples, we''ll be using this lab topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](graphics/B05453_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The topology consists of a total of four Docker host''s two of which are in
    the `10.10.10.0/24` subnet and the other two are in the `192.168.50.0/24` subnet.
    As we walk through this recipe, the hosts shown in the diagram will play the following
    roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker1`: Docker host serving a Consul **key-value store**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker2`: Docker host participating in overlay networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker3`: Docker host participating in overlay networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker4`: Docker host participating in overlay networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As mentioned earlier, the overlay `Driver` isn't instantiated by default. This
    is because there are several prerequisites required for the overlay `Driver` to
    work.
  prefs: []
  type: TYPE_NORMAL
- en: A key-value store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since we're now dealing with a distributed system, Docker needs a place to store
    information about the overlay network. To do this, Docker uses a key-value store
    and supports Consul, etcd, and ZooKeeper for this purpose. It will store information
    that requires consistency across all the nodes such as IP address allocations,
    network IDs, and container endpoints. In our examples, we'll be deploying Consul.
  prefs: []
  type: TYPE_NORMAL
- en: 'As luck would have it, Consul can be deployed as a Docker container itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Running this image will start a single instance of the Consul key-value store.
    A single instance is all we need for basic lab testing. In our case, we'll start
    this image on the host `docker1`. All the Docker hosts that participate in the
    overlay must have reachability to the key-value store across the network.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Running Consul with a single cluster member should only be done for demonstration
    purposes. You need at least three cluster members to have any sort of failure
    tolerance. Make sure that you research the key-value store you decide to deploy
    and understand its configuration and failure tolerances.
  prefs: []
  type: TYPE_NORMAL
- en: Linux kernel version of 3.16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Your Linux kernel version needs to be 3.16 or greater. You can check your current
    kernel version with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Open ports
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Docker hosts must be able to talk to each other using the following ports:'
  prefs: []
  type: TYPE_NORMAL
- en: TCP and UDP `7946` (Serf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UDP `4789` (VXLAN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TCP `8500` (Consul key-value store)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker service configuration options
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All the hosts that are participating in the overlay need access to the key-value
    store. To tell them where it is, we define a couple of service-level options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The cluster-store variable defines where the key-value store is. In our case,
    it's a container running on the host `docker1` (`10.10.10.101`). We also need
    to enable the `cluster-advertise` feature and pass it an interface and port. This
    configuration relates more to using a Swarm cluster, but the flag is also used
    as part of enabling multi-host networking. That being said, you need to pass it
    a valid interface and a port. In this case, we use the host physical interface
    and port `0`. In our example, we'll add these options to hosts `docker2`, `docker3`,
    and `docker4` as those are the hosts we'll have participating in the overlay network.
  prefs: []
  type: TYPE_NORMAL
- en: 'After adding the option, reload the `systemd` configuration and restart the
    Docker service. You can verify that Docker has accepted the command by checking
    the output of the `docker info` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we''ve met the prerequisites for using the overlay `Driver`, we can
    deploy our first user-defined overlay network. Defining a user-defined overlay
    network follows much the same process as that of defining a user-defined bridge
    network. For instance, let''s configure our first overlay network using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Much like with user-defined bridges, we don't have to enter much information
    to create our first overlay network. In fact, the only difference here is that
    we have to specify the `Driver` as type overlay because the default `Driver` type
    is bridge. Once we enter the command, we should be able to see the network defined
    on any node participating in overlay networking.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The host `docker2` pushes the network configuration into the store when it creates
    the network. Now all the hosts can see the new network since they're all reading
    and writing data to and from the same key-value store. Once the network is created,
    any node participating in the overlay (configured with the correct service-level
    options) can view, connect containers to, and delete the overlay network.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if we go to host `docker4`, we can delete the network that we
    created on host `docker2` initially:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Let's now define a new overlay with a little more configuration. Unlike the
    user-defined bridge, the overlay `Driver` does not currently support any additional
    options being passed to it during creation with the `--opt` flag. That being said,
    the only options that we can configure on overlay type networks are those that
    are part of the `network create` subcommand.
  prefs: []
  type: TYPE_NORMAL
- en: '`aux-address`: As with the user-defined bridge, this command allows you to
    define IP addresses that Docker should not assign to containers when they are
    spawned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gateway`: Although you can define a gateway for the network, and if you don''t,
    Docker will do it for you, this isn''t actually used in overlay networks. That
    is, there is no interface that this IP address gets assigned to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`internal`: This option allows you to isolate networks and is covered in greater
    detail later in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ip-range`: Allows you to specify a smaller subnet of the defined network subnet
    to use for container addressing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ipam-driver`: In addition to consuming third-party network drivers, you can
    also leverage third-party IPAM drivers. For the purposes of this book we''ll be
    focusing mostly on the default or built-in IPAM driver.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ipam-opt`: This allows you to specify options to pass to the IPAM driver.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subnet`: This defines the subnet associated with the network type you are
    creating.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s redefine the network `myoverlay` on the host `docker4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we define the network with the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: A `subnet` of `172.16.16.0/24`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A reserved or auxiliary address of `172.16.16.2` (Recall that Docker will allocate
    a Gateway IP to be the first IP in the subnet despite the fact that it's not actually
    being used. In this case, this means that `.1` and `.2` are technically reserved
    at this point.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A container assignable IP range of `172.16.16.128/25`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A name of `myoverlay`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As before, this network is now available for consumption on all three hosts
    participating in the overlay configuration. Let''s now define our first container
    on the overlay network from host `docker2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we ask the host to start a container named `web1` and attach it to the
    network `myoverlay`. Let''s now inspect the container''s IP interface configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Surprisingly, the container has two interfaces. The `eth0` interface is attached
    to the network associated with the overlay network `myoverlay`, but `eth1` is
    associated with a new network `172.18.0.0/16`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You've likely noticed by this point that the name of the interfaces within the
    container use VETH pair naming syntax. Docker uses VETH pairs to connect containers
    to bridges and configures the container IP address directly on the container side
    interface. This will be covered extensively in [Chapter 4](ch04.html "Chapter 4. Building
    Docker Networks"), *Building Docker Networks*, where we walk through the details
    of how Docker attaches containers to the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'To figure out where it''s attached, let''s try and find the other end of the
    VETH pair that the container''s `eth1` interface attaches to. As shown in [Chapter
    1](ch01.html "Chapter 1. Linux Networking Constructs"), *Linux Networking Constructs*,
    we could use `ethtool` to look up the `interface ID` for a VETH pairs peer. However,
    there''s an easier way to do this when looking at user-defined networks. Notice
    in the preceding output that the VETH pair name has a syntax of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'As luck would have it, the number shown after `if` is the `interface ID` of
    the other side of the VETH pair. So, in the preceding output, we see that the
    `eth1` interface''s matching interface has an `interface ID` of `11`. Looking
    at the local Docker host, we can see that we have an interface `11` defined and
    that its `peer interface ID` is `10`, which matches `interface ID` in the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that this end of the VETH pair (`interface ID 11`) has a master named
    `docker_gwbridge`. That is, this end of the VETH pair is part of the bridge `docker_gwbridge`.
    Let''s look at the networks defined on the Docker host again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to our overlay network, there''s also a new user-defined bridge
    of the same name. If we inspect this bridge, we see our container is connected
    to it as expected and the network has some options defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, ICC mode for this bridge is disabled. ICC prevents containers
    on the same network bridge from communicating directly with each other. But what
    is the purpose of this bridge and why are containers spawned on the `myoverlay`
    network being attached to it?
  prefs: []
  type: TYPE_NORMAL
- en: 'The `docker_gwbridge` network is the solution to external container connectivity
    for overlay connected containers. Overlay networks can be thought of as layer
    2 network segments. You can attach multiple containers to them and anything on
    that network can talk across the local network segment. However, this doesn''t
    allow the container to talk to resources off the network. This limits Docker''s
    ability to access container resources through published ports as well as the container''s
    ability to talk to the outside network. If we examine the container''s routing
    configuration, we can see that its default gateway points to the interface of
    the `docker_gwbridge`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This coupled with the fact that the `docker_gwbridge` has IP masquerading enabled
    means that the container can still talk to the outside network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As with the default bridge network, containers will hide behind their Docker
    host IP interface if they attempt to route through to reach the outside network.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also means that since I published ports on this container using the `-P`
    flag that Docker has published those ports using `docker_gwbridge`. We can verify
    the ports were published by using the `docker port` subcommand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'And verify that port is published on the `docker_gwbridge` by checking the
    netfilter rules with `iptables`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding output, Docker is using the container's interface
    on the `docker_gwbridge` to provide port publishing to the Docker host's interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, our container topology looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Adding a container to an overlay network automatically created the bridge `docker_gwbridge`,
    which is used for container connectivity onto and off the host. The `myoverlay`
    overlay network is used only for connectivity related to the defined `subnet`,
    `172.16.16.0/24`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now start two more containers, one on the host `docker3` and another
    on the host `docker4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Notice that, when I attempt to run the same container on both hosts, Docker
    tells me that the container `web2` already exists. Docker won't allow you to run
    a container with the same name on the same overlay network. Recall that Docker
    is storing information related to each container on the overlay in the key-value
    store. Using unique names becomes important when we start talking about Docker
    name resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may notice at this point that the containers can resolve each other by name.
    This is one of the really powerful features that come along with user-defined
    networks. We'll talk about this in much more detail in [Chapter 5](ch05.html "Chapter 5. Container
    Linking and Docker DNS"), *Container Linking and Docker DNS*, where we discuss
    DNS and linking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Restart the container on `docker4` with a unique name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have three containers running, one on each host participating in the
    overlay. Let''s take a brief moment to visualize what''s going on here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: I've removed the host and underlay networking on the diagram to make this easier
    to read. As described, each container has two IP network interfaces. One IP address
    is on the shared overlay network and is in the `172.16.16.128/25` network. The
    other is on the bridge `docker_gwbridge` and is the same on each host. Since the
    `docker_gwbridge` lives on each host independently, there is no need to have unique
    addressing for this interface. The container interface on that bridge serves only
    as a means for the container to talk to the outside network. That is, every container
    on the same host, which has a network interface on an overlay type network, will
    receive an IP address on this same bridge.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering if this raises a security concern since all containers
    with overlay networks, regardless of which they are connected to, will also have
    an interface on a shared bridge (`docker_gwbridge`). Recall earlier that I pointed
    out that the `docker_gwbridge` had ICC mode disabled. This means that, while many
    containers can be deployed to the bridge, none of them can actually communicate
    directly with each other through their IP interfaces on that bridge. We'll talk
    much more about this in [Chapter 6](ch06.html "Chapter 6. Securing Container Networks"),
    *Securing Container Networks*, where we discuss container security, but for now
    know that ICC prevents ICC from occurring on the shared bridge.
  prefs: []
  type: TYPE_NORMAL
- en: Containers on the overlay network believe that they are on the same network
    segment, or are layer 2 adjacent to each other. Let's prove this by connecting
    to the web service on container `web2` from container `web1`. Recall that when
    we provisioned the container `web2`, we did not ask it to publish any ports.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with other Docker network constructs, containers connected to the same overlay
    network can talk directly to each other on any port in which they have a service
    bound to without the need to publish the port:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's important to remember that the Docker host has no direct means to connect
    to the overlay connected containers. With the bridge network type this was feasible
    because the host had an interface on the bridge, in the case of overlay type networks,
    this interface does not exist.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we can successfully access the web server running in container
    `web2` from container `web1`. Not only are these containers on totally different
    hosts, but the hosts themselves are on totally different subnets. This type of
    communication was only available previously when both containers sat on the same
    host, and were connected to the same bridge. We can prove that the containers
    believe themselves to be layer 2 adjacent by checking the ARP and MAC entries
    on each respective container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the container has an ARP entry from the remote container specifying
    its IP address as well as its MAC address. If the containers were not on the same
    network, the container `web1` would not have an ARP entry for `web2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify that we have local connectivity between all three containers
    from container `web2-2` on host `docker4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we know the overlay works, let''s talk about how it''s implemented.
    The mechanism used for overlay transport is VXLAN. We can see the container-generated
    packets as they traverse the underlay network by looking at a packet capture taken
    on the physical network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding screenshot of a packet taken from the capture, I want to call
    out a couple of items:'
  prefs: []
  type: TYPE_NORMAL
- en: The outer IP packet is sourced from the `docker2` host (`10.10.10.102`) and
    destined to the `docker3` host (`192.168.50.101`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can see that outer IP packet is UDP and is detected as being VXLAN encapsulation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **VNI** (**VXLAN Network Identifier**) or segment ID is `260`. VNI is unique
    per subnet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inner frame has a layer 2 and layer 3 header. The layer 2 header has a destination
    MAC addresses of container `web2` as shown earlier. The IP packet shows a source
    of container `web1` and a destination of container `web2`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Docker hosts encapsulate the overlay traffic using their own IP interface
    and send it across the underlay network to the destination Docker host. Information
    from the key-value store is used to determine what host a given container is on
    in order for the VXLAN encapsulation to send the traffic to the right host.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might now be wondering where all the configuration for this VXLAN overlay
    is. At this point, we haven''t seen any configuration that actually talks about
    VXLAN or tunneling. To provide VXLAN encapsulation, Docker creates what I refer
    to as an *overlay namespace* for each user-defined overlay network. As we saw
    in [Chapter 1](ch01.html "Chapter 1. Linux Networking Constructs"), *Linux Networking
    Constructs*, you can use the `ip netns` tool to interact with the network namespace.
    However, since Docker stores their network namespaces in a nondefault location,
    we won''t be able to see any of the namespaces using the `ip netns` tool. By default,
    namespaces are stored in `/var/run/netns`. The problem is that Docker stores its
    network namespaces in `/var/run/docker/netns`, which means the `ip netns` tool
    is looking in the wrong place to see network namespaces created by Docker. As
    a work around to this issue, we can create a `symlink` that links `/var/run/docker/netns/`
    to `/var/run/nents` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Notice that there are two network namespace defined. The overlay namespace will
    be identified with the following syntax `x-<id>` where `x` is a random number.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The other namespace we see displayed in the output is associated with the container
    running on the host. In the next chapter, we'll be doing a deep dive on how these
    namespaces are created and used by Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'So in our case, the overlay namespace is `2-4695c5484e`, but where did it come
    from? If we inspect the network configuration of this namespace, we''ll see that
    it has some unusual interfaces defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'These interfaces define the overlay network namespace I mentioned earlier.
    Earlier we saw that the container `web2-2` has two interfaces. The `eth1` interface
    was one end of a VETH pair with the other end placed on the `docker_gwbridge`.
    The VETH pair shown in the preceding overlay network namespace represents one
    side of the pair for the container''s `eth0` interface. We can prove this by matching
    up the sides of the VETH pair by `interface ID`. Notice that this end of the VETH
    pair shows the other end to have an `interface ID` of `12`. If we look at the
    container `web2-2`, we''ll see that its `eth0` interface has an ID of `12`. In
    turn, the container''s interface shows a pair ID of `13`, which matches the output
    we saw in the overlay namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we know how the container''s overlay interface (`eth0`) is connected,
    we need to know how traffic headed into the overlay namespace gets encapsulated
    and sent to the other Docker hosts. This is done through the overlay namespace''s
    `vxlan1` interface. This interface has specific forwarding entries that describe
    all of the other endpoints on the overlay:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we have two entries referencing a MAC address and a destination.
    The MAC address represents the MAC address of another container on the overlay,
    and the IP address is the Docker host in which the container lives. We can verify
    that by checking one of the other hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: With this information, the overlay namespace knows that in order to reach that
    destination MAC address, it needs to encapsulate traffic in VXLAN and send it
    towards `10.10.10.102` (`docker2`).
  prefs: []
  type: TYPE_NORMAL
- en: Isolating networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: User-defined networks can support what's referred to as internal mode. We saw
    this option in the earlier recipe about creating user-defined networks, but didn't
    spend much time discussing it. Using the `--internal` flag when creating a network
    prevents containers connected to the network from talking to any outside networks.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `docker network` subcommand was introduced in Docker 1.9, so you'll need
    a Docker host running at least that version. In our examples, we'll be using Docker
    version 1.12\. You'll also want to have a good understanding of your current network
    layout so that you can follow along as we inspect the current configuration. It
    is assumed that each Docker host is in its native configuration.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Making a user-defined network internal is pretty straightforward, you just add
    the option `--internal` to the `network create` subcommand. Since a user-defined
    network can be of type bridge or type overlay, we should understand how Docker
    implements isolation in either case.
  prefs: []
  type: TYPE_NORMAL
- en: Creating internal user-defined bridges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Define a user-defined bridge and pass it the `internal` flag, as well as the
    flag to give the bridge a custom name on the host. We can do this with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s take a look at the IP information that Docker assigned to the bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Taking this information, we now check and see what Docker has programmed in
    netfilter for this bridge. Let''s check the filter table and see:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this case, I'm using the `iptables-save` syntax to query the current rules.
    Sometimes, this can be more readable than looking at individual tables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that Docker has added two rules. The first says that any traffic
    that is not sourced from the bridge subnet and is leaving the bridge interface
    should be dropped. This can be hard to comprehend, so it's easiest to think of
    this in terms of an example. Say that a host on your network `192.168.127.57`
    was trying to access something on this bridge. That flows source IP address would
    not be in the bridge subnet, which fulfills the first part of the rule. It would
    also be attempting to go out of (or onto) `mybridge1` meeting the second part
    of the rule. This rule effectively prevents all inbound communication.
  prefs: []
  type: TYPE_NORMAL
- en: The second rule looks for traffic that does not have a destination in the bridge
    subnet, and that has an ingress interface of the bridge `mybridge1`. In this case,
    a container might have an IP address of 172.19.0.5/16\. If it were attempting
    to talk off of it's local network, the destination would not be in the `172.19.0.0/16`
    which would match the first part of the rule. As it attempted to leave the bridge
    towards an external network, it would match the second part of the rule as it's
    coming into the `mybridge1` interface. This rule effectively prevents all outbound
    communication.
  prefs: []
  type: TYPE_NORMAL
- en: Between these two rules, no traffic is allowed in or out of the bridge. This
    does not, however, prevent container-to-container connectivity between containers
    on the same bridge.
  prefs: []
  type: TYPE_NORMAL
- en: 'It should be noted that Docker will allow you to specify the publish (`-P`)
    flag when running containers against an internal bridge. However, no ports will
    ever get mapped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Creating internal user-defined overlays
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Creating internal overlays follows the same process. We just pass the `--internal`
    flag to the `network create` subcommand. However, in the case of overlay networks,
    the isolation model is much simpler. We can create an internal overlay network
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Once created, it''s really no different than a non-internal overlay. The difference
    comes when we run containers on the internal overlay:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Checking the container interface configuration, we can see that the container
    only has one interface, which is a member of the overlay network (`192.10.10.0/24`).
    The interface that would normally connect the container to the `docker_gwbridge`
    (`172.18.0.0/16`) network for external connectivity is missing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Overlay networks are inherently isolated, hence, need for the `docker_gwbridge`.
    Not mapping a container interface to `docker_gwbridge` means that there's no way
    to talk in or out of the overlay network that provides the isolation.
  prefs: []
  type: TYPE_NORMAL
