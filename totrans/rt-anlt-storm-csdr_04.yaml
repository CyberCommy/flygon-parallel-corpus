- en: Chapter 4. Storm in a Clustered Mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now arrived at the next step in our journey with Storm, that is, to
    understand the clustered mode setup of Storm and its associated components. We
    will go through the various configurations in Storm and Zookeeper, and understand
    the concepts behind them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that will be covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Storm cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the configuration of the cluster and its impact on the functioning
    of the system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Storm UI and understanding the UI parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provisioning and monitoring applications for production setups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of the chapter, you should be able to understand configurations of
    Storm and Zookeeper nodes. Also, you should be able to understand the Storm UI
    and set up Storm clusters and monitor them using various tools.
  prefs: []
  type: TYPE_NORMAL
- en: The Storm cluster setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depicted in the following figure is the Storm and Zookeeper reference cluster
    that we set up in [Chapter 2](part0020_split_000.html#page "Chapter 2. Getting
    Started with Your First Topology"), *Getting Started with Your First Topology*.
  prefs: []
  type: TYPE_NORMAL
- en: We have three-node Zookeeper clusters for a three-node Storm cluster (which
    has one Nimbus and two supervisors).
  prefs: []
  type: TYPE_NORMAL
- en: We are using the recommended three-node Zookeeper clusters to avoid a single
    point of failure in the Storm set up.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Zookeeper cluster should have an odd number of nodes. The reason for this
    requirement is that the Zookeeper election logic requires the leader to have an
    odd number of votes, and that combination is possible only when odd nodes are
    in the quorum, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Storm cluster setup](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Zookeeper configurations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's assume you have installed Zookeeper on all three Zookeeper nodes; now
    we will walk you through the configurations so that you have a better understanding
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'An excerpt from `zoo.cfg`, in our case, is located at `<zookeeper_installation_dir>/
    zookeeper-3.4.5/conf/`. The Zookeeper configurations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dataDir=/usr/local/zookeeper/tmp`: This is the path where Zookeeper stores
    its snapshots; these snapshots are actually the state log where the current cluster
    state is maintained for the purpose of coordination. In the event of a failure,
    these snapshots are used to restore the cluster to the last stable state. This
    directory also contains a file containing a single entry called `myID`. This value
    starts from `1` and is different for each Zookeeper node, so we will keep it as
    follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Whenever you want to start from scratch, or when you upscale or downscale Storm
    or Zookeeper clusters, it is recommended that you clean up this `local.dir` file
    so that stale data is cleared.
  prefs: []
  type: TYPE_NORMAL
- en: '`clientPort=2182`: This configuration specifies the port on which the clients
    build connections with Zookeeper:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: These three rows in the preceding code actually specify the IP or the names
    of the servers that form a part of the Zookeeper cluster. In this configuration,
    we have created the three-node Zookeeper cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '`maxClientCnxns=30l`: This number specifies the maximum number of connections
    a single client can make with this Zookeeper node. Here is how the calculation
    will go in our case:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The max number of connections one supervisor can make is 30 with one Zookeeper
    node. So, the maximum number of connections one supervisor can create with three
    Zookeeper nodes is 90 (that is, 30*3).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows a capture from the Storm UI depicting the used,
    available, and free slots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Zookeeper configurations](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The number of workers in the Storm cluster is related to the number of connections
    available in the Zookeeper cluster. If you don't have sufficient Zookeeper cluster
    connections, Storm supervisors will not be able to start.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up Zookeeper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have seen how Zookeeper stores all its coordination data in the form of
    snapshots in the path specified in the `dataDir` configuration. This requires
    periodic clean up or archival to remove old snapshots so that we don''t end up
    consuming the entire disk space. Here is a small cleanup script that needs to
    be configured on all Zookeeper nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we have the cleanup script as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`numBackUps`: Here we specify how many snapshots we want to retain after cleanup;
    the minimum is three and the maximum can vary as per requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataDir`: Here we specify the path of the data directory where snapshots need
    to be cleaned up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logDir`: This is the path where the clean up script will store its logs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.zookeeper.server.PurgeTxnLog`: This is the utility class that purges
    all snapshots except the last three, as mentioned in `n``umBackups`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storm configurations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will look at the Storm daemons and configurations around the daemons. For
    the Nimbus node, we have the following configuration settings in `storm.yaml`.
    Let''s understand these configurations as given in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The functions of the configurations used in the preceding code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`storm.zookeeper.servers`: Here we specify the names or IPs of the Zookeeper
    servers from the Zookeeper cluster; note that we are using the same host names
    as mentioned in the `zoo.cfg` configuration in the previous section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storm.zookeeper.port`: Here we specify the port on the Zookeeper node to which
    the Storm nodes connect. Again, we specified the same port that we had specified
    on `zoo.cfg` in the previous section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storm.local.dir`: Storm has its own temporary data that is stored in a local
    directory. This data is automatically cleaned up, but whenever you want to start
    from scratch, or when you upscale or downscale the Storm or Zookeeper clusters,
    it is recommended that you clean up this `local.dir` configuration so that stale
    data is cleared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nimbus.host`: This specifies the hostname or IP of the hostname that is being
    set as Nimbus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topology.message.timeout.secs`: This value specifies the duration in seconds
    after which a tuple being processed by the topology is declared as timed out and
    discarded. Thereafter, depending upon whether it''s a reliable or unreliable topology,
    it''s replayed or not. This value should be set cautiously; if set too low, all
    messages will end up being timed out. If it is set too high, one may never get
    to know the performance bottlenecks in the topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topology.debug`: This parameter denotes whether you want to run the topology
    in the debug mode or node. The debug mode is when all the debug logs are printed,
    and it is recommended in the development and staging mode, but not in the production
    mode because I/O is very high in this mode and thus hits the overall performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`supervisor.slots.ports`: This parameter specifies the ports for the supervisor
    workers. This number directly ties to the number of workers that can be spawned
    on the supervisor. When topologies are spawned, one has to specify the number
    of workers that are to be allocated, which in turn ties to actual resources allocated
    to the topology. The number of workers is really important because they actually
    identify how many topologies can run on the cluster and in turn how much parallelism
    can be achieved. For example, by default, we have four slots per supervisor, so
    in our cluster, we will have *Total number of slots/workers = 4*2 = 8*. Each worker
    takes a certain number of resources from the system in terms of CPU and RAM, so
    how many workers are spawned on the supervisor depends on the system configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storm logging configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we will have a look at the logging configurations of Storm. They use the
    `logback` implementation of Log4J and its configurations can be found and tweaked
    from `cluster.xml` located at `<storm-installation-dir>/apache-storm-0.9.2-incubating/logback`
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding snippet, a few sections are highlighted, which we will walk
    through for a closer look. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<file>`: This tag holds the log directory path and the filename on which the
    logs are generated by the Storm framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<filenamepattern>`: This is the pattern in which files are formed and rolled
    over; for example, with the preceding code pattern, we have worker log files as
    `worker-6700.log` and `worker-6700.1.log`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<minIndex> and <maxIndex>`: These are very important in order to specify how
    many files we want to retain with this rolling appender; in this case, we will
    have nine backup files, which are numbered from one to nine, and one running log
    file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxFileSize`: This parameter specifies at what size the file should rollover,
    for instance, in our case, it''s 100 MB; this means the worker log file will roll
    over to the next index when it reaches this size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`root level`: This specifies the logging level; in our case, we have specified
    it as *Info*, which means `Info` and the above logs will be printed in the log
    files, but logs from levels below the `Info` level will not be written to the
    logs. The following is the logging level hierarchy for reference:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OFF`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FATAL`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ERROR`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WARN`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`INFO`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DEBUG`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TRACE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ALL`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Storm UI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: cluster.submitTopology("word-count", conf,  builder.createTopology());
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: conf.setNumWorkers(3);
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: builder.setSpout("spout", new RandomSentenceSpout(), 5);
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: builder.setSpout("spout", new RandomSentenceSpout(),  5).setNumTasks(10);
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Section 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section holds the various actions that can be performed on the topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Activate**: The UI provides a facility to revive or reactivate a topology
    that has been suspended earlier. Once activated, it can again start consuming
    the messages from the spout and process them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deactivate**: When this action is executed, the topology immediately turns
    off the spout, that is, no new messages are read from the spout and pushed downstream
    to the DAG. Existing messages that are already being processed in various bolts
    are processed completely.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rebalance**: This action is executed when the worker allocation to a live
    topology is altered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kill**: As the name suggests, this is used to send a termination signal for
    the topology to the Storm framework. It''s always advisable to provide a reasonable
    kill time so that the topology drains completely and is able to clean the pipelined
    events before it terminates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section displays a capture of the number of messages processed on the
    timeline. It has the following key sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Window**: This field specifies the time duration in the following segments:
    last for 10 minutes, last 3 hours, the past day, or all the time. The topology''s
    progress is captured against these time sections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Emitted**: This captures the number of tuples emitted by the spout at various
    time segments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transferred**: This specifies the number of tuples sent to other components
    in the topology. Please note that the number of emitted tuples may or may not
    be equal to the number of transferred tuples as the former is the exact number
    of times the emit method of the spout is executed, while the latter is the number
    transferred based on the grouping used; for example, if we have bound a spout
    to a bolt that has the parallelism of two tuples using all grouping, then for
    every `x` tuples emitted by the spout, `2x` tuples will be transferred.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete latency**(**ms**): This is the average total time taken by a tuple
    to execute throughout the topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Acked**: This holds the number of events acked that are successfully processed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failed**: This is the number of events that failed to process successfully.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section 4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section is the same as *Section 3*, the only difference being that here,
    the terms are displayed at a component level, that is spouts and bolts, while
    in *Section 3*, it was at the topology level. There are a few more terms on the
    UI that you should be introduced to. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Capacity**: This is one of the most important metrics to look at when fine-tuning
    the topology. It indicates the percentage of time the bolt spent in the last ten
    minutes to execute the tuple. Any value close to one or above is an indication
    to increase the parallelism of this bolt. It''s calculated using the following
    formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Execute latency**: This is the average time a tuple spends in the execute
    method of the bolt during processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Process** **latency**: Process latency is the average time it takes from
    when the tuple is received by the bolt to the time when it''s acked (acknowledged
    to denote successful processing).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The visualization section
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the improvements in Storm 0.9.2 is visual depiction of the topology.
    The following figure is the depiction of a sample topology in the Storm UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The visualization section](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, you can see all the streams visually labeled by
    various bolts and spouts on the topology along with latency and other key attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Storm UI provides a very rich interface where the user can start from a
    very high level and drill down deeper in specific areas, as you can see in the
    screenshot in *The Storm cluster setup* section where we discussed the Storm cluster
    level attributes; in the second level, we moved to a specific topology. Next,
    within a topology, you can click on any bolt or worker and the component level
    details will be presented to you. One item as highlighted in the following screenshot
    is of high importance for debugging and log deciphering in cluster setup—the worker
    ID. If some component spout or bolt is giving us issues and we want to understand
    the working, the first place to look is the logs. To be able to look at logs,
    one needs to know where the troublesome bolt is executing which supervisor and
    which worker; this can be inferred by drilling on that component and looking into
    the executor section as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The visualization section](img/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The Storm UI capturing the supervisor port
  prefs: []
  type: TYPE_NORMAL
- en: Here, the host tells you which supervisor this component is running on and the
    port tells you about the worker, so if I want to look for logs of this component,
    I will look into `logdir` for `sup-flm-dev-1.mydomain.net` in the log directory
    under `worker-6711.log`.
  prefs: []
  type: TYPE_NORMAL
- en: Storm monitoring tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The clustered setup of the likes of Storm need constant monitoring, because
    they are generally developed to support real-time systems wherein downtime could
    be of concern for **Service Level Agreement** (**SLA**). A lot of tools are available
    on the market that could be used to monitor the Storm cluster and to raise an
    alert. Some of the Storm monitoring tools are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nagios**: This is a very powerful monitoring system that can be extended
    to generate e-mail alerts. It can monitor various processes and system KPIs and
    can be tweaked by writing custom scripts and plugins to restart certain components
    in the event of a failure.![Storm monitoring tools](img/00035.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagios service console
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot of a Storm cluster with Nagios monitoring, you can
    see various processes and other system level KPIs that can be monitored such as
    CPU, memory, latency, HDD usage, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ganglia**: This is another widely used open source tool that lets you set
    up a monitoring framework for Storm clusters.![Storm monitoring tools](img/00036.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As seen in the preceding screenshot, we have a lot of drill-down options; we
    can see load and CPU level details as well as other system and cluster level KPIs
    to capture and plot out the cluster health.
  prefs: []
  type: TYPE_NORMAL
- en: '**SupervisorD**: This is another open source monitoring system that is widely
    used in conjunction with Storm to capture and retain the health of the cluster.
    SupervisorD also helps in provisioning and starting the Storm services and it
    can be configured to restart them in case of any failures.![Storm monitoring tools](img/00037.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ankush**: This is another provisioning and monitoring system that could be
    used for Storm and other big data cluster setup and management. It has both paid
    and open source versions ([https://github.com/impetus-opensource/ankush](https://github.com/impetus-opensource/ankush)).
    It has the following salient features:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Provisioning** | **Environment supported by this application****Physical
    nodes****Virtual nodes on Cloud (AWS or On-Premise)** |'
  prefs: []
  type: TYPE_TB
- en: '| Single technology clusters |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-technology clusters |'
  prefs: []
  type: TYPE_TB
- en: '| Template-based cluster creation |'
  prefs: []
  type: TYPE_TB
- en: '| Redeploy an erred cluster |'
  prefs: []
  type: TYPE_TB
- en: '| Rack support |'
  prefs: []
  type: TYPE_TB
- en: '| Enhanced node validation before deployment |'
  prefs: []
  type: TYPE_TB
- en: '| **Monitoring** | Heat maps |'
  prefs: []
  type: TYPE_TB
- en: '| Service monitoring |'
  prefs: []
  type: TYPE_TB
- en: '| Technology-based monitoring |'
  prefs: []
  type: TYPE_TB
- en: '| Rich graphs |'
  prefs: []
  type: TYPE_TB
- en: '| Alerts and notifications for key events |'
  prefs: []
  type: TYPE_TB
- en: '| Centralized log view |'
  prefs: []
  type: TYPE_TB
- en: '| Audit trail |'
  prefs: []
  type: TYPE_TB
- en: '| Alerts on dashboard and e-mails |'
  prefs: []
  type: TYPE_TB
- en: The following screenshot is a dashboard screenshot of Ankush. All the system
    level KPIs such as CPU, load, network, memory, and so on are very well captured.
  prefs: []
  type: TYPE_NORMAL
- en: '![Storm monitoring tools](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Quiz time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Q.1\. State whether the following statements are true or false:'
  prefs: []
  type: TYPE_NORMAL
- en: The Storm configurations are stored in `cluster.xml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can have only four workers allocated per supervisor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Zookeeper cluster always has an odd number of nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zookeeper needs a minimum of three snapshots to recover its state from failure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A topology can continue to execute if Nimbus and the supervisor dies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Q.2\. Fill in the blanks:'
  prefs: []
  type: TYPE_NORMAL
- en: _______________ is the average time a tuple takes to get processed and acked.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: _______________ is the average time a tuple spends in the execute method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ____________ component is responsible for the recovery of the Storm cluster
    in the event of a failure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Q.3\. Execute the `WordCount` topology on a three-node Storm cluster (one Nimbus
    and two supervisors) and then perform the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Kill the Nimbus node while the topology is running—observe that the topology
    will not fail; it will continue unaffected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kill the supervisor while the topology is running—observe that the topology
    does not fail, it will continue unaffected. The workers will continue to execute
    with Zookeeper co-ordination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try various operations such as rebalance and deactivate from the Storm UI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you got a detailed understanding of the Storm and Zookeeper
    configurations. We explored and walked you through the Storm UI and its attributes.
    Having done the cluster setup, we briefly touched upon various monitoring tools
    available for operational production support in Storm.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will get you introduced to RabbitMQ and its integration
    with Storm.
  prefs: []
  type: TYPE_NORMAL
