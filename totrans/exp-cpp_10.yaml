- en: Concurrency and Multithreading
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrent programming allows the creation of more efficient programs. C++ didn't
    have built-in support for concurrency or multithreading for a long time. Now it
    has full support for concurrent programming, threads, thread synchronization objects,
    and other functionality that we will discuss in this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Before the language updated for thread support, programmers had to use third-party
    libraries. One of the most popular multithreading solutions was **POSIX** (**Portable
    Operating System Interface**) threads. C++ introduced thread support since C++11\.
    It makes the language even more robust and applicable to wider areas of software
    development. Understanding threads is somewhat crucial for C++ programmers as
    they tend to squeeze every bit of the program to make it run even faster. Threads
    introduce us to a completely different way of making programs faster by running
    functions concurrently. Learning multithreading at a fundamental level is a must
    for every C++ programmer. There are lots of programs where you can't avoid using
    multithreading, such as network applications, games, and GUI applications. This
    chapter will introduce you to concurrency and multithreading fundamentals in C++
    and best practices for concurrent code design.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Understanding concurrency and multithreading
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with threads
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing threads and sharing data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing concurrent code
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using thread pools to avoid thread creation overheads
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting familiar with coroutines in C++20
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The g++ compiler with the `-std=c++2a` option is used to compile the examples
    in this chapter. You can find the source files used in this chapter at [https://github.com/PacktPublishing/Expert-CPP](https://github.com/PacktPublishing/Expert-CPP)
    .
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Understanding concurrency and multithreading
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest form of running a program involves its instructions being executed
    one by one by the **CPU** (**Central Processing Unit**). As you already know from
    previous chapters, a program consists of several sections, one of them containing
    the instructions of the program. Each instruction is loaded into a CPU register
    for the CPU to decode and execute it. It doesn't actually matter what programming
    paradigm you use to produce an application; the result is always the same—the
    executable file contains machine code.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'We mentioned that programming languages such as Java and C# use support environments.
    However, if you cut down the support environment in the middle (usually, the virtual
    machine), the final instructions being executed should have a form and format
    familiar to that particular CPU. It''s obvious to programmers that the order of
    statements run by the CPU is not mixed in any circumstance. For example, we are
    sure and can continue to be so that the following program will output `4`, `"hello"`,
    and `5`, respectively:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We can guarantee that the value of the `a` variable will be initialized before
    we print it to the screen. The same way we can guarantee that the `"hello"` string will
    be printed before we decrement the value of `b`, and that the `(b + 1)` sum will
    be calculated before printing the result to the screen. The execution of each
    instruction might involve reading data from or writing to memory.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'As introduced in [Chapter 5](2b708ddc-255e-490e-bd4c-e783ccae5f9e.xhtml), *Memory Management
    and Smart Pointers*, the memory hierarchy is sophisticated enough to make our
    understanding of program execution a little bit harder. For example, the `int
    b{a};` line from the previous example assumes that the value of `a` is loaded
    from the memory into a register in the CPU, which then will be used to write into
    the memory location of `b`. The keyword here is the *location* because it carries
    a little bit of special interpretation for us. More specifically, we speak about
    memory location. Concurrency support depends on the memory model of the language,
    that is, a set of guarantees for concurrent access to memory. Although the byte
    is the smallest addressable memory unit, the CPU works with words in data. That
    said, the word is the smallest unit the CPU reads from or writes to memory. For
    example, we consider the following two declarations separate variables:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If those variables are allocated in the same word (considering the word size
    as bigger than the size of a `char`), reading and writing any of the variables
    involves reading the word containing both of them. Concurrent access to the variables
    might lead to unexpected behavior. That''s the issue requiring memory model guarantees. The
    C++ memory model guarantees that two threads can access and update separate memory
    locations without interfering with each other. A memory location is a scalar type.
    A scalar type is an arithmetic type, pointer, enumeration, or `nullptr_t`. The
    largest sequence of adjacent bit-fields of non-zero length is considered a memory
    location too. A classic example would be the following structure:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For the preceding example, two threads accessing the same struct's separate
    memory locations won't interfere with each other. So, what should we consider
    when speaking about concurrency or multithreading?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrency is usually confused with multithreading. They are similar in nature but
    are different concepts in detail. To make things easy, just imagine concurrency
    as two operations whose running times interleave together. Operation `A` runs
    concurrently with operation `B` if their start and end times are interleaved at
    any point, as shown in the following diagram:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a602270-5f05-4ec9-9d02-5ab13bbf4883.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: 'When two tasks run concurrently, they don''t have to run parallel. Imagine
    the following situation: you are watching TV while surfing the internet. Though
    it''s not a good practice to do so, however, let''s imagine for a moment that
    you have a favorite TV show that you can''t miss and at the same time, your friend
    asked you to do some research on bees. You can''t actually concentrate on both
    tasks; at any fixed moment, your attention is grabbed by either the show you are
    watching or the interesting facts about bees that you are reading in an article
    found on the web. Your attention goes from the show to the bees from time to time.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of concurrency, you are doing two tasks concurrently. Your brain gives
    a time portion to the show: you watch, enjoy, and then switch to the article,
    read a couple of sentences, and switch back to the show. This is a simple example
    of concurrently running tasks. Just because their start and end times interleave
    doesn''t mean they run at the same time. On the other hand, you breathe while
    doing any of the tasks mentioned earlier. Breathing happens in the background;
    your brain doesn''t switch your attention from the show or the article to your
    lungs to inhale or exhale. Breathing while watching the show is an example of
    parallel running tasks. Both examples show us the essence of concurrency.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: So, what is going on when you run more than one application on your computer?
    Are they running in parallel? It's for sure that they run concurrently, however,
    the actual parallelism depends on your computer's hardware. Most mass-market computers
    consist of a single CPU. As we know from previous chapters, the main job of the
    CPU is running an application's instructions one by one. How would a single CPU
    handle the running of two applications at the same time? To understand that, we
    should learn about processes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Processes
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A process is an image of a program running in the memory. When we start a program,
    the OS reads the content of the program from the hard disk, copies it to the memory,
    and points the CPU to the starting instruction of the program. The process has
    its private virtual address space, stack, and heap. Two processes don't interfere
    with each other in any way. That's a guarantee provided by the OS. That also makes
    a programmer's job very difficult if they aim for **Interprocess Communication**
    (**IPC**). We are not discussing low-level hardware features in this book but
    you should have a general understanding of what is going on when we run a program.
    It really depends on the underlying hardware—more specifically, the kind and structure
    of the CPU. The number of CPUs, number of CPU cores, levels of cache memory, and
    shared cache memory between CPUs or their cores—all of these affect the way the
    OS runs and executes programs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of CPUs in a computer system defines the number of processes running
    truly in parallel. This is shown in the following diagram:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/249ef5f9-c6a5-43bf-bad3-7648ec848c90.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: When we speak about multiprocessing, we consider an environment that allows
    several processes to run concurrently. And here comes the tricky part. If the
    processes actually run at the same time, then we say that they run in parallel.
    So, concurrency is not parallelism while parallelism implies concurrency.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: If the system has just one CPU, processes run concurrently but not in parallel.
    The OS manages this with a mechanism called **context switching**. Context switching
    implies freezing the work of the process for a moment, copying all of the register
    values that the process was using at the current time, and storing all of the active
    resources and values of the process. When a process is stopped, another process
    takes on the rights to run. After the specified amount of time provided for this
    second process, the OS starts the context switching for it. Again, it copies all
    of the resources used by the process. Then, the previous process gets started.
    Before starting it, the OS copies back the resources and values to the corresponding
    slots used by the first process and then resumes the execution of this process.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'The interesting thing is that the processes are not even aware of such a thing.
    The described process happens so fast that the user cannot actually notice that
    the programs running in the OS are not actually running at the same time. The
    following illustration depicts two processes run by a single CPU. When one of
    the processes is *active*, the CPU executes its instructions sequentially, storing
    any intermediary data in its registers (you should consider cache memory as in
    the game, too). The other process is *waiting* for the OS to provide its time
    portion to run:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a385b09c-4de2-4fd5-8057-8831d0675c61.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: 'Running more than one process is a sophisticated job for the OS. It manages
    states of processes, defines which process should take more CPU time than others,
    and so on. Each process gets a fixed time to run before the OS switches to another
    process. This time can be longer for one process and shorter for another. Scheduling
    processes happens using priority tables. The OS provides more time to processes
    with a higher priority, for example, a system process has higher priority than
    user processes. Another example could be that a background task monitoring network
    health has a higher priority than a calculator application. When the provided
    time slice is up, the OS initiates a context switch, that is, it stores the state
    of **Process A** to resume its execution later:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 运行多个进程对操作系统来说是一项复杂的工作。它管理进程的状态，确定哪个进程应该比其他进程占用更多的CPU时间等。每个进程在操作系统切换到另一个进程之前都有固定的运行时间。这个时间对于一个进程可能更长，对于另一个进程可能更短。使用优先级表来调度进程。操作系统为优先级更高的进程提供更多的时间，例如，系统进程的优先级高于用户进程。另一个例子可能是，监控网络健康的后台任务的优先级高于计算器应用程序。当提供的时间片用完时，操作系统会启动上下文切换，即，它会存储**进程A**的状态以便稍后恢复其执行：
- en: '![](img/1d6e1429-9787-443b-9ea6-2a591b51de4c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d6e1429-9787-443b-9ea6-2a591b51de4c.png)'
- en: 'After storing the state, as showing in the following diagram, it switches to
    the next process to execute it:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储状态之后，如下图所示，它切换到下一个进程来执行：
- en: '![](img/9ba4ecae-987c-4b5a-a51c-2790b12a3c5e.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ba4ecae-987c-4b5a-a51c-2790b12a3c5e.png)'
- en: 'Obviously, if **Process B** was running before, its state should be loaded
    back to the CPU. In the same way, when the time slice (or time quantum) is up
    for **Process B**, the OS stores its state and loads the state of **Process A**
    back to the CPU (the state it had before being paused by the OS):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，如果**进程B**之前正在运行，它的状态应该被加载回CPU。同样，当**进程B**的时间片（或时间量子）用完时，操作系统会存储它的状态，并将**进程A**的状态加载回CPU（在被操作系统暂停之前的状态）：
- en: '![](img/ed451d71-cf02-45f6-8929-484ecb4ed453.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed451d71-cf02-45f6-8929-484ecb4ed453.png)'
- en: 'Processes do not share anything in common—or at least they think so. Each running
    process behaves as if it''s alone in the system. It has all of the resources the
    OS can provide. In reality, the OS manages to keep processes unaware of each other,
    hence simulating freedom for each one. Finally, after loading the state of **Process
    A** back, the CPU continues executing its instructions like nothing happened:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 进程之间没有任何共同之处，或者至少它们认为是这样。每个运行的进程都表现得好像它是系统中唯一的。它拥有操作系统可以提供的所有资源。实际上，操作系统设法让进程彼此不知晓，因此为每个进程模拟了自由。最后，在将**进程A**的状态加载回来后，CPU继续执行它的指令，就好像什么都没有发生过：
- en: '![](img/b44e37d7-6d77-4248-ad76-e94dc8ab76f4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b44e37d7-6d77-4248-ad76-e94dc8ab76f4.png)'
- en: '**Process B** is frozen until a new time slice is available for it to run.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**进程B**被冻结，直到有新的时间片可用于运行它。'
- en: A single CPU running more than one process is similar to a teacher checking
    examination papers of students. The teacher can check only one exam paper at a
    time, though they can introduce some concurrency by checking answers one by one
    for each exam test. First, they check the answer to the first question for one
    student, then switch to the first answer of the test of the second student, and
    then switches back to the first student's second answer and so on. Whenever the
    teacher switch from one exam paper to the other, they note down the number of
    the question where they left off. This way, they will know where to start when
    getting back to the same paper.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一个单CPU运行多个进程类似于一位老师检查学生的考卷。老师一次只能检查一份考卷，尽管他们可以通过逐个检查每个考试的答案来引入一些并发性。首先，他们检查一个学生的第一个问题的答案，然后切换到第二个学生的考试的第一个答案，然后再切换回第一个学生的第二个答案，依此类推。每当老师从一份考卷切换到另一份时，他们都会记下他们停下来的问题的编号。这样，当他们回到同一份考卷时，他们就知道从哪里开始。
- en: In the same way, the OS notes down the point of execution of a process before
    pausing it to resume another process. The second process can (and most probably
    will) use the same register set used by the paused process. This forces the OS
    to store register values for the first process somewhere to be recovered later.
    When the OS pauses the second process to resume the first one, it loads already
    saved register values back into corresponding registers. The resumed process won't
    notice any difference and will continue its work like it was never paused.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，操作系统在暂停一个进程以恢复另一个进程之前记录下进程的执行点。第二个进程可以（而且很可能会）使用被暂停进程使用的相同寄存器集。这迫使操作系统将第一个进程的寄存器值存储在某个地方，以便稍后恢复。当操作系统暂停第二个进程以恢复第一个进程时，它会将已保存的寄存器值加载回相应的寄存器中。恢复的进程不会注意到任何差异，并将继续工作，就好像它从未被暂停过一样。
- en: Everything described in the preceding two paragraphs relates to single-CPU systems.
    In the case of multi-CPU systems, each CPU in the system has its own set of registers.
    Also, each CPU can execute program instructions independently of the other CPUs,
    which allows running processes in parallel without pausing and resuming them.
    In the example, a teacher with a couple of assistants is similar to a system with
    three CPUs. Each one of them can check one exam paper; all of them are checking
    three different exam papers at any point in time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 前两段描述的一切都与单CPU系统有关。在多CPU系统中，系统中的每个CPU都有自己的寄存器集。此外，每个CPU可以独立地执行程序指令，而不受其他CPU的影响，这允许进程并行运行而无需暂停和恢复它们。在这个例子中，一位老师和几个助手类似于一个有三个CPU的系统。他们每个人都可以检查一份考卷；他们在任何时候都在检查三份不同的考卷。
- en: Challenges with processes
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进程的挑战
- en: 'Difficulties arise when processes need to contact each other in some way. Let''s
    say a process should calculate something and pass the value to a completely different
    process. There are several methods to achieve IPC—one of them is using a memory
    segment shared between processes. The following diagram depicts two processes
    accessing the shared memory segment:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba6aa4ed-83da-4987-b27f-17429fa23ec4.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: One process stores the results of the calculation to a shared segment in the
    memory, and the second process reads it from the segment. In the context of our
    previous example, the teacher and their assistants share their checking results
    in a shared paper. Threads, on the other hand, share address space of the process
    because they run in the context of the process. While a process is a program,
    thread is a function rather than a program. That said, a process must have at
    least one thread , which we call the thread of execution. A thread is the container
    of instructions of a program that are run in the system, while the process encapsulates
    the thread and provides resources for it. Most of our interest lies in threads
    and their orchestration mechanisms. Let's now meet them in person.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Threads
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **thread** is a section of code in the scope of a process that can be scheduled
    by the OS scheduler. While a process is the image of the running program, managing
    multi-process projects along with IPC is much harder and sometimes useless compared
    to projects leveraging multithreading. Programs deal with data and, usually, collections
    of data. Accessing, processing, and updating data is done by functions that are
    either the methods of objects or free functions composed together to achieve an
    end result. In most projects, we deal with tens of thousands of functions and
    objects. Each function represents a bunch of instructions wrapped under a sensible
    name used to invoke it by other functions. Multithreading aims to run functions
    concurrently to achieve better performance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a program that calculates the sum of three different vectors and
    prints them calls the function calculating the sum for the first vector, then
    for the second vector, and finally, for the last one. It all happens sequentially.
    If the processing of a single vector takes A amount of time, then the program
    will run in `3A` time. The following code demonstrates the example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If there was a way to run the same function for three different vectors simultaneously,
    it would take just A amount of time for the whole program in the preceding example.
    Threads of execution, or just threads, are exact ways of running tasks concurrently.
    By tasks, we usually mean a function, although you should remember `std::packaged_task`
    as well. Again, concurrency shouldn't be confused with parallelism. When we speak
    about threads running concurrently, you should consider the context switching
    discussed previously for the process. Almost the same applies to threads.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '`std::packaged_task` is similar to `std::function`. It wraps a callable object—a
    function, lambda, function object, or bind expression. The difference with `std::packaged_task`
    is that it can be invoked asynchronously. There''s more on that later in this
    chapter.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Each process has a single thread of execution, sometimes called the **main thread**.
    A process can have more than one thread, and that's when we call it **multithreading**.
    Threads run in almost the same way the processes. They also have context switching.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Threads run separately from each other, but they share most of the resources
    of process because all of the threads belong to the process. The process occupies
    hardware and software resources such as CPU registers and memory segments, including
    its own stack and heap. While a process doesn't share its stack or heap with other
    processes, its threads have to use the same resources that are occupied by the
    process. Everything that happens in a thread's life happens within the process.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'However, threads don''t share the stack. Each thread has its portion of the
    stack. The reason behind this segregation relies on the fact that a thread is
    just a function and the function itself should have access to the stack to manage
    the life cycle of its arguments and local variables. When we run the same function
    as two (or more) separately running threads, the runtime should somehow handle
    their boundaries. Although it''s error-prone, you can pass a variable from one
    thread to another (either by value or by reference). Let''s suppose that we started
    three threads running the `process_vector()` function for the three vectors in
    the preceding example. You should imagine that starting a thread means *copying*
    the underlying function somehow (its variables but not the instructions) and running
    it separately from any other thread. In this scenario, the same function will
    be copied as three different images, and each one of them will run independently
    of the others, hence each should have its own stack. On the other hand, the heap
    is shared between threads. So, basically, we arrive at the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，线程不共享堆栈。每个线程都有自己的堆栈部分。这种隔离的原因在于，线程只是一个函数，函数本身应该可以访问堆栈来管理其参数和局部变量的生命周期。当我们将相同的函数作为两个（或更多）分别运行的线程运行时，运行时应该以某种方式处理它们的边界。虽然这很容易出错，但你可以通过值或引用将一个变量从一个线程传递到另一个线程。假设我们启动了三个线程，分别运行上面例子中的三个向量的`process_vector()`函数。你应该想象启动一个线程意味着以某种方式*复制*底层函数（它的变量但不是指令）并将其独立地运行。在这种情况下，相同的函数将被复制为三个不同的图像，并且每个图像都将独立于其他图像运行，因此每个图像都应该有自己的堆栈。另一方面，堆在线程之间是共享的。因此，基本上我们得到了以下结论：
- en: '![](img/2837ebc0-a491-4359-aa66-f0e737feff43.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2837ebc0-a491-4359-aa66-f0e737feff43.png)'
- en: As in the case of processes, threads running concurrently are not necessarily running
    in parallel. Each thread gets a small portion of CPU time to be run and, again,
    there is an overhead regarding the switching from one thread to another. Each
    paused thread's state should be stored somewhere to be recovered later when resuming
    it. The internal structure of the CPU defines whether threads could truly run
    in parallel. The number of CPU cores defines the number of threads that can truly
    run in parallel.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与进程一样，并发运行的线程不一定是并行运行的。每个线程都会获得一小部分CPU时间来运行，而且从一个线程切换到另一个线程也会有开销。每个暂停的线程状态都应该被存储在某个地方，以便在恢复时能够恢复。CPU的内部结构定义了线程是否能够真正并行运行。CPU核心的数量定义了可以真正并行运行的线程数量。
- en: The C++ thread library provides the `hardware_concurrency()` function to find
    out the number of threads that can truly run concurrently. You can refer to this
    number when designing concurrent code.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: C++线程库提供了`hardware_concurrency()`函数，用于查找可以真正并发运行的线程数量。在设计并发代码时，可以参考这个数字。
- en: 'The following diagram depicts two CPUs having four cores each. Each core can
    run a thread independently of the other:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了两个CPU，每个CPU都有四个核心。每个核心可以独立地运行一个线程：
- en: '![](img/82938768-6461-41e2-94dc-454295e6fd96.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82938768-6461-41e2-94dc-454295e6fd96.png)'
- en: 'Not only do two processes run in parallel but also their threads are run in
    parallel using the CPU cores. Now, how will the situation change if we have several
    threads but one single-core CPU? Almost the same as we have illustrated earlier
    for processes. Look at the following diagram—it depicts how the CPU executes **Thread
    1** for some time slice:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅两个进程并行运行，它们的线程也使用CPU核心并行运行。那么，如果我们有几个线程但只有一个单核CPU，情况会如何改变呢？几乎与我们之前为进程所说明的情况相同。看看下面的图表——它描述了CPU如何在某个时间片段内执行**线程1**：
- en: '![](img/43a122ae-5378-4cb0-8770-cfea0b51cad2.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43a122ae-5378-4cb0-8770-cfea0b51cad2.png)'
- en: The currently active **Process A** has two threads that run concurrently. At
    each specified point in time, only one of the threads is executed. When the time
    slice is up for **Thread 1**, **Thread 2** is executed. The difference from the
    model we discussed for processes is that threads share the resources of the process,
    which leads to unnatural behavior if we aren't concerned with concurrent code
    design issues. Let's dive into C++ threading support and find out what issues
    arise when using multithreading.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当前活动的**进程A**有两个同时运行的线程。在每个指定的时间点，只有一个线程被执行。当**线程1**的时间片用完时，**线程2**被执行。与我们讨论过的进程模型的不同之处在于，线程共享进程的资源，如果我们不关心并发代码设计问题，这会导致不自然的行为。让我们深入了解C++线程支持，并找出在使用多线程时会出现什么问题。
- en: Working with threads
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用线程
- en: 'When the C++ program starts, that is, the `main()` function starts its execution,
    you can create and launch new threads that will run concurrently to the main thread. To
    start a thread in C++, you should declare a thread object and pass it the function
    that you want to run concurrently to the main thread. The following code demonstrates
    the declaration and starting of a thread using `std::thread` defined in `<thread>`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当C++程序启动时，也就是`main()`函数开始执行时，你可以创建并启动新的线程，这些线程将与主线程并发运行。要在C++中启动一个线程，你应该声明一个线程对象，并将要与主线程并发运行的函数传递给它。以下代码演示了使用`<thread>`中定义的`std::thread`声明和启动线程：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'That''s it. We can create a better example to show how two threads work concurrently.
    Let''s say we print numbers in a loop concurrently to see which thread prints
    what:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。我们可以创建一个更好的例子来展示两个线程如何同时工作。假设我们同时在循环中打印数字，看看哪个线程打印了什么：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding example will print both outputs with the `Main:` and `Background:` prefixes mixed
    together. An excerpt from the output might look like this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的例子将打印出带有`Main:`和`Background:`前缀混合在一起的两个输出。输出的摘录可能如下所示：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Whenever the main thread finishes its work (printing to the screen one million
    times), the program wants to finish without waiting for the background thread
    to complete. It leads to program termination. Let's see how we should modify the
    previous example.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当主线程完成其工作（向屏幕打印一百万次）时，程序希望在不等待后台线程完成的情况下结束。这会导致程序终止。让我们看看如何修改之前的例子。
- en: Waiting for threads
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 等待线程
- en: 'The `thread` class provides the `join()` function if you want to wait for it
    to finish. Here is a modified version of the previous example that waits for the
    `background` thread:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要等待线程完成，`thread`类提供了`join()`函数。以下是等待`background`线程的修改版本的示例：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we already discussed previously, the `thread` function is run as a separate
    entity independently from other threads- even the one that started it. It won't
    wait for the thread it has just started, and that's why you should explicitly
    tell the caller function to wait for it to finish. It is necessary to signal that
    the calling thread (the main thread) is waiting for the thread to finish before
    itself.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，`thread`函数作为一个独立的实体运行，独立于其他线程-甚至是启动它的线程。它不会等待它刚刚启动的线程，这就是为什么您应该明确告诉调用函数在自己之前等待它完成。在它完成之前，必须发出信号表明调用线程（主线程）正在等待线程完成。
- en: 'The symmetric opposite of the `join()` function is the `detach()` function.
    The `detach()` function indicates that the caller isn''t interested in waiting
    for the thread to finish. In this case, the thread can have an independent life.
    As shown here (like it''s already 18 years old):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`join()`函数的对称相反是`detach()`函数。`detach()`函数表示调用者对等待线程完成不感兴趣。在这种情况下，线程可以有独立的生命周期。就像这里显示的（就像它已经18岁了）：'
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Although detaching a thread might seem natural, there are plenty of scenarios
    when we need to wait for the thread to finish. For example, we might pass local
    to the caller variables to the running thread. In this case, we can''t let the
    caller detach the thread as the caller might finish its work earlier than the
    thread started in it. Let''s illustrate that for the sake of clarity. **Thread
    1** declares the `loc` variable and passes it to **Thread 2**, which has been
    started from **Thread 1**:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管分离线程可能看起来很自然，但有很多情况需要等待线程完成。例如，我们可能会将局部变量传递给正在运行的线程。在这种情况下，我们不能让调用者分离线程，因为调用者可能比线程更早完成其工作。让我们为了清晰起见举个例子。**Thread
    1**声明了`loc`变量并将其传递给了从**Thread 1**启动的**Thread 2**：
- en: '![](img/8bcd907b-742d-4b19-9484-87822f730a68.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bcd907b-742d-4b19-9484-87822f730a68.png)'
- en: 'Passing the address of `loc` to **Thread 2** is error-prone if **Thread 1**
    doesn''t join it. If **Thread 1** finishes its execution before **Thread 2**,
    then accessing `loc` by its address leads to an undefined behavior:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果**Thread 1**在**Thread 2**之前完成其执行，那么通过地址访问`loc`会导致未定义的行为：
- en: '![](img/58cea33e-6366-4f28-9ac5-84e785da0452.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58cea33e-6366-4f28-9ac5-84e785da0452.png)'
- en: There is no such object anymore, so the best that we can hope for the program
    is to crash. It will lead to unexpected behavior because the running thread won't
    have access to the caller's local variables anymore. You should either join or
    detach a thread.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 不再有这样的对象，因此我们可以希望程序最好崩溃。这将导致意外行为，因为运行线程将不再访问调用者的局部变量。您应该加入或分离线程。
- en: 'We can pass any callable object to `std::thread`. The following example shows
    passing a lambda expression to the thread:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将任何可调用对象传递给`std::thread`。以下示例显示了将lambda表达式传递给线程：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Furthermore, we can use callable objects as thread arguments. Take a look at
    the following code declaring the `TestTask` class with the overridden `operator()`
    function:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以使用可调用对象作为线程参数。看一下以下代码，声明了具有重载的`operator()`函数的`TestTask`类：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: One of the advantages of a functor (the `TestTask` class with the overridden
    `operator()` function ) is its ability to store state information. Functors are
    a beautiful implementation of the command design pattern that we will discuss
    in [Chapter 11](0e28887e-1a43-4510-a8ef-b3ad7531868d.xhtml), *Designing a Strategy
    Game Using Design Patterns*. Getting back to the threads, let's move on the a
    new addition in the language that allows for better ways to join threads.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 函数对象（具有重载的`operator()`函数的`TestTask`类）的一个优点是它能够存储状态信息。函数对象是命令设计模式的一个美丽实现，我们将在[第11章](0e28887e-1a43-4510-a8ef-b3ad7531868d.xhtml)中讨论，*使用设计模式设计策略游戏*。回到线程，让我们继续讨论语言中的一个新添加，它允许更好地加入线程的方式。
- en: Using std::jthread
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用std::jthread
- en: C++20 introduces a joinable thread, `std::jthread`. It provides the same interface
    `std::thread` as provides, so we can replace all threads with jthreads in the
    code. It actually wraps `std::thread`, so basically it delegates down to the wrapped
    thread.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: C++20引入了可加入线程`std::jthread`。它提供了与`std::thread`相同的接口，因此我们可以在代码中用jthreads替换所有线程。它实际上是对`std::thread`的封装，因此基本上是将操作委托给封装的线程。
- en: 'If the version of your compiler doesn''t support `std::jthread`, you are free
    to go with the **RAII** (**Resource Acquisition Is Initialization**) idiom, which
    is perfectly applicable to threads. Take a look at the following code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的编译器版本不支持`std::jthread`，您可以选择使用**RAII**（**资源获取即初始化**）习惯用法，这对线程非常适用。看一下以下代码：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'However, the preceding code lacks an additional check because a thread passed
    to the RAII class might have been already detached. To see whether the thread
    could be joined, we use the `joinable()` function. This is how we should overwrite
    the `thread_raii` class:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，前面的代码缺少了一个额外的检查，因为传递给RAII类的线程可能已经被分离。为了查看线程是否可以加入，我们使用`joinable()`函数。这是我们应该如何重写`thread_raii`类的方式：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The destructor first tests whether the thread is joinable before calling the
    `join()` function. However, instead of dealing with idioms and being concerned
    about whether the thread has been joined already before joining it, we prefer
    using `std::jthread`. Here''s how we can do that using the `TestTask` function
    declared previously:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用`join()`函数之前，析构函数首先测试线程是否可加入。但是，与其处理习惯用法并担心线程在加入之前是否已经加入，我们更喜欢使用`std::jthread`。以下是如何使用先前声明的`TestTask`函数来做到这一点：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'That''s it—there''s no need to call `jt.join()`, and a new cooperative interruptible
    feature out of the box that we use by incorporating jthread. We say that jthread
    is cooperative interruptible because it provides the `request_stop()` function
    ,which does what its name says—requests the thread to stop. Although that request
    fulfillment is implementation-defined, it''s a nice way not to wait for the thread
    forever. Recall the example with the thread printing numbers in an infinite loop.
    We modified the main thread to wait for it, and that leads to waiting for it forever.
    Here''s how we can modify the thread using `std::jthread` to leverage the `request_stop()`
    function:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样——不需要调用`jt.join()`，并且我们使用`std::jthread`内置的新的协作可中断功能。我们说`jthread`是协作可中断的，因为它提供了`request_stop()`函数，它做了它的名字所说的事情——请求线程停止。尽管请求的实现是定义的，但这是一个不必永远等待线程的好方法。回想一下线程在无限循环中打印数字的例子。我们修改了主线程来等待它，这导致永远等待它。下面是我们如何使用`std::jthread`修改线程以利用`request_stop()`函数：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `print_numbers_in_background()` function now receives a request and can
    behave accordingly. Now, let's see how to pass arguments to the thread function.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`print_numbers_in_background()`函数现在接收到一个请求，并可以相应地行为。现在，让我们看看如何将参数传递给线程函数。'
- en: Passing arguments to the thread function
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将参数传递给线程函数
- en: 'The `std::thread` constructor takes arguments and forwards them to the underlying
    `thread` function. For example, to pass the arguments `4` and `2` to the `foo()`
    function here, we pass the arguments to the `std::thread` constructor:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`std::thread`构造函数接受参数并将它们转发给底层的`thread`函数。例如，要将参数`4`和`2`传递给`foo()`函数，我们将参数传递给`std::thread`构造函数：'
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The arguments `4` and `2` will be passed as the first and second arguments to
    the `foo()` function.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`4`和`2`参数将作为`foo()`函数的第一个和第二个参数传递。'
- en: 'The following example illustrates the passing of an argument by reference:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例说明了通过引用传递参数：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To understand why we named the function `error_prone`, we should know that the
    thread constructor copies the values passed to it and then passes them to the
    thread function with `rvalue` references. This is done to work with move-only
    types. So it will try to call the `make_changes()` function with `rvalue`, which
    will fail to compile (you can't pass `rvalue` to a function that expects a non-constant
    reference). We need to wrap the arguments that need to be a reference in `std::ref:`
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么我们将函数命名为`error_prone`，我们应该知道线程构造函数会复制传递给它的值，然后使用`rvalue`引用将它们传递给线程函数。这是为了处理仅可移动类型。因此，它将尝试使用`rvalue`调用`make_changes()`函数，这将无法编译通过（不能将`rvalue`传递给期望非常量引用的函数）。我们需要在需要引用的参数中使用`std::ref`进行包装。
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The preceding code emphasizes that the argument should be passed by reference.
    Working with threads requires being a little more attentive because there are
    many ways to get unexpected results or undefined behavior in the program. Let's
    see how we can manage threads to produce safer multithreaded applications.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码强调了参数应该通过引用传递。处理线程需要更加注意，因为程序中有许多方法可以获得意外结果或未定义的行为。让我们看看如何管理线程以生成更安全的多线程应用程序。
- en: Managing threads and sharing data
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理线程和共享数据
- en: As discussed previously, the execution of threads involves pausing and resuming
    some of them if the number of threads exceeds the number of parallel running threads
    supported by the hardware. Besides that, the creation of a thread also has its
    overhead. One of the suggested practices to deal with having many threads in a
    project is using thread pools.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前讨论的，线程的执行涉及暂停和恢复其中一些线程，如果线程数量超过硬件支持的并行运行线程数量。除此之外，线程的创建也有开销。在项目中处理有许多线程的建议做法之一是使用线程池。
- en: 'The idea of a thread pool lies in the concept of caching. We create and keep
    threads in some container to be used later. The container is called a pool. For
    example, the following vector represents a simple thread pool:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 线程池的概念在于缓存的概念。我们创建并保留线程在某个容器中以便以后使用。这个容器称为池。例如，以下向量表示一个简单的线程池：
- en: '[PRE18]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Whenever we need a new thread, instead of declaring the corresponding `std::thread`
    object, we use one already created in the pool. When we are done with the thread,
    we can push it back to the vector to use it later if necessary. This saves some
    time when working with 10 or more threads. A proper example would be a web server.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们需要一个新线程时，我们不是声明相应的`std::thread`对象，而是使用已在池中创建的线程。当我们完成线程时，我们可以将其推回向量以便以后使用。这在处理10个或更多线程时可以节省一些时间。一个合适的例子是一个Web服务器。
- en: 'A web server is a program that waits for incoming client connections and creates
    a separate connection for each client to be processed independently from others.
    A typical web server usually deals with thousands of clients at the same time.
    Each time a new connection is initiated with some client, the web server creates
    a new thread and handles the client requests. The following pseudo-code demonstrates
    a simple implementation of a web server''s incoming connection management:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Web服务器是一个等待传入客户端连接并为每个客户端创建一个独立连接以独立处理的程序。一个典型的Web服务器通常同时处理数千个客户端。每当与某个客户端启动新连接时，Web服务器都会创建一个新线程并处理客户端请求。以下伪代码演示了Web服务器传入连接管理的简单实现：
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When using a thread pool, the preceding code will avoid the creation of a thread
    each time it needs to process a client request. The creation of a new thread requires
    additional and rather expensive work from the OS. To save that time, we use a
    mechanism that omits creating new threads on each request. To make the pool even
    better, let''s replace its container with a queue. Whenever we ask for a thread,
    the pool will return a free thread, and whenever we are done with a thread, we
    push it back to the pool. A simple design of a thread pool would look like the
    following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The constructor creates and pushes threads to the queue. In the following pseudo-code,
    we replace the direct creation of a thread for client request processing with
    `ThreadPool` ,which we looked at previously:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Supposing that the `handle_request()` function pushes the thread back to the
    pool when it's done, the pool behaves as a centralized store for connection threads.
    Though shown in the preceding snippet is far from being ready for production,
    it conveys the basic idea of using thread pools in intensive applications.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Sharing data
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A race condition is something that programmers using multithreading are scared
    of and try to avoid as much as possible. Imagine two functions that work concurrently
    with the same data, as shown here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: A potential race condition is happening because threads `t1` and `t2` are modifying
    the same variable with more than one step. Any operation that is performed in
    a single thread-safe step is called an **atomic operation**. In this case, incrementing
    the value of the variable is not an atomic operation, even if we use the increment
    operator.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Protecting shared data using a mutex
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To protect shared data, objects called **mutexes** are used widely. A mutex
    is an object that controls the running of a thread. Imagine threads as humans
    making a deal about how to work with data one by one. When a thread locks a mutex,
    the other thread waits until it is done with the data and unlocks the mutex. The
    other thread then locks the mutex and starts working with data. The following
    code demonstrates how we can solve the problem of a race condition using a mutex:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: When `t1` starts executing `inc()`, it locks a mutex, which avoids any other
    thread to access the global variable unless the original thread doesn't unlock
    the next thread.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'C++17 introduced a lock guard that allows guarding the mutex in order not to
    forget unlock it:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: It's always better to use language-provided guards if possible.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding deadlocks
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: New problems arise with mutexes, such as **deadlocks**. A deadlock is a condition
    of  multithreaded code when two or more threads lock a mutex and wait for the
    other to unlock another.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: The common advice to avoid deadlock is to always lock two or more mutexes in
    the same order. C++ provides the `std::lock()` function, which serves the same
    purpose.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates the `swap` function, which takes two arguments
    of type `X`. We suppose that `X` has a member, `mt` , which is a mutex. The implementation
    of the `swap` function locks the mutex of the left object first, then it locks
    the mutex of the right object:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: To avoid deadlocks in general, avoid nested locks. That said, don't acquire
    a lock if you are already holding one. If this is not the case, then acquire locks
    in a fixed order. The fixed order will allow you to avoid deadlocks.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Designing concurrent code
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Project complexity rises drastically when concurrency is introduced. It's much
    easier to deal with sequentially executing synchronous code compared to concurrent
    counterparts. Many systems avoid using multithreading at all by introducing event-driven
    development concepts, such as the event loop. The point of using an event loop
    is to introduce a manageable approach to asynchronous programming. To take the
    concept further, imagine any application providing a **Graphical User Interface**
    (**GUI**). Whenever the user clicks on any GUI component, such as buttons; types
    in fields; or even moves the mouse, the application receives so-called events
    regarding the user action. Whether it's `button_press`, `button_release`, `mouse_move`,
    or any other event, it represents a piece of information to the application to
    react properly. A popular approach is to incorporate an event loop to queue any
    event that occurred during user interaction.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: While the application is busy with its current task, the events produced by
    user actions are queued to be processed at some time in the future. The processing
    involves calling handler functions attached to each event. They are called in
    the order they were put into the queue.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Introducing multithreading to the project brings additional complexity with
    it. You should now take care of race conditions and proper thread handling, maybe
    even using a thread pool to reuse thread objects. In sequentially executed code,
    you care for the code and only the code. Using multithreading, you now care a
    little bit more about the ways of execution of the very same code. For example,
    a simple design pattern such as the singleton behaves differently in a multithreading
    environment. The classic implementation of a singleton looks like the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following code starts two threads, both using the `MySingleton` class:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Threads `t1` and `t2` both call the `get_instance()` static member function
    of the `MySingleton` class. It''s possible that `t1` and `t2` both pass the check
    for the empty instance and both execute the new operator. Clearly, we have a race
    condition here. The resource, in this case, the class instance, should be protected
    from such a scenario. Here''s an obvious solution using a mutex:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Using a mutex will solve the problem, but will make the function work more
    slowly because each time a thread requests an instance, a mutex will be locked
    instead (which involves additional operations by the OS kernel). The proper solution
    would be using the Double-Checked Locking pattern. Its basic idea is this:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Lock the mutex after the `instance_` check.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the `instance_` again after the mutex has been locked because another
    thread might have passed the first check and wait for the mutex to unlock.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'See the code for details:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Several threads may pass the first check and one of them will lock the mutex.
    Only one thread makes it to the new operator call. However, after unlocking the
    mutex, threads that have passed the first check will try to lock it and create
    the instance. The second check is there to prevent this. The preceding code allows
    us to reduce the performance overhead of the synchronized code. The approach we
    provided here is one of the ways to prepare yourself for concurrent code design.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent code design is very much based on the capabilities of the language
    itself. The evolution of C++ is marvelous. In its earliest versions, it didn't
    have built-in support for multithreading. Now, it has a solid thread library and
    the new C++20 standard provides us with even more powerful tools, such as coroutines.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Introducing coroutines
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed an example of asynchronous code execution when speaking about GUI
    applications. GUI components react to user actions by firing corresponding events,
    which are pushed in the event queue. This queue are then processed one by one
    by invoking attached handler functions. The described process happens in a loop;
    that's why we usually refer to the concept as the event loop.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Asynchronous systems are really useful in I/O operations because any input
    or output operation blocks the execution at the point of I/O call. For example,
    the following pseudo-code reads a file from a directory and then prints a welcome
    message to the screen:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Attached to the synchronous execution pattern, we know that the message Welcome
    to the app! will be printed only after the `read_file()` function finishes executing.
    `process_file_contents()` will be invoked only after `cout` completes. When dealing
    with asynchronous code, all we know about code execution starts to behave like
    something unrecognizable. The following modified version of the preceding example
    uses the `read_file_async()` function to read the file contents asynchronously:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Considering `read_file_async()` is an asynchronous function, the message Welcome
    to the app! will be printed sooner than the file contents. The very nature of
    asynchronous execution allows us to invoke functions to be executed in the background,
    which provides us with non-blocking input/output.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is a slight change in the way we treat the return value of the
    function. If we deal with an asynchronous function, its return value is considered
    as something called a **promise** or a **promise object**. It''s the way the system
    notifies us when the asynchronous function has completed. The promise object has
    three states:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Pending
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rejected
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fulfilled
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A promise object is said to be fulfilled if the function is done and the result
    is ready to be processed. In the event of an error, the promise object will be
    in the rejected state. If the promise is not rejected nor fulfilled, it is in
    the pending state.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'C++20 introduced coroutines as an addition to the classic asynchronous functions.
    Coroutines move the background execution of the code to next level; they allow
    a function to be paused and resumed when necessary. Imagine a function that reads
    file contents and stops in the middle, passes the execution context to another
    function, and then resumes the reading of the file to its end. So, before diving
    deeper, consider a coroutine as a function that can be as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Started
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paused
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resumed
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finished
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To make a function a coroutine, you would use one of the keywords `co_await`,
    `co_yield`, or `co_return`. `co_await` is a construct telling the code to wait
    for asynchronously executing code. It means the function can be suspended at that
    point and resume its execution when a result is ready. For example, the following
    code requests an image from the network using a socket:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As the network request operation is also considered as an **input/output**
    operation, it might block the execution of the code. To prevent blocking, we use
    asynchronous calls. The line using `co_await` in the preceding example is a point
    where the function execution could be suspended. In simpler words, when the execution
    reaches the line with `co_await`, the following happens:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: It quits the function for a while (until there isn't ready data).
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It continues executing from where it was before `process_image()` was called.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then comes back again to continue executing `process_image()` at the point
    where it left it.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To achieve this, a coroutine (the `process_image()` function is a coroutine)
    is not handled the way regular functions are handled in C++. One of the interesting
    or even surprising features of coroutines is that they are **stackless.** We know
    that functions can't live without the stack. That's where the function pushes
    its arguments and local variables before even executing its instructions. Coroutines,
    on the other hand, instead of pushing anything to the stack, save their state
    in the heap and recover it when resumed.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: This is tricky because there are also stackful coroutines. Stackful coroutines,
    also referred to as **fibers**, have a separate stack.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines are connected to callers. In the preceding example, the function
    that call `sprocess_image()` transfers execution to the coroutine and the pause
    by the coroutine (also known as **yielding**) transfers the execution back to
    the caller. As we stated, the heap is used to store the state of the coroutine,
    but the actual function-specific data (arguments, and local variables) are stored
    on the caller's stack. That's it—the coroutine is associated with an object that
    is stored on the caller function's stack. Obviously, the coroutine lives as long
    as its object.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines might give a wrong impression of redundant complexity added to the
    language, but their use cases are great in improving applications that use asynchronous
    I/O code (as in the preceding example) or lazy computations. That said, when we
    have to invent new patterns or introduce complexities into projects to handle,
    for instance, lazy computations, we now can improve our experience by using coroutines
    in C++. Please note that asynchronous I/O or lazy computations are just two examples
    of coroutine applications. There are more out there.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've discussed the concept of concurrency and showed the difference
    between parallelism. We learned the difference between a process and a thread,
    the latter being of interest. Multithreading allows us to manage a program to
    be more efficient, though it also brings additional complexity with it. To handle
    data races, we use synchronization primitives such as a mutex. A mutex is a way
    to lock the data used by one thread to avoid invalid behavior produced by simultaneously
    accessing the same data from several threads.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: We also covered the idea that an input/output operation is considered blocking
    and asynchronous functions are one of the ways to make it non-blocking. Coroutines
    as a part of the asynchronous execution of code were introduced in C++20.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: We learned how to create and start a thread. More importantly, we learned how
    to manage data between threads. In the next chapter, we will dive into data structures
    that are used in concurrent environments.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is concurrency?
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between concurrency and parallelism?
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a process?
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the difference between a process and a thread?
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write code to start a thread.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could you make the singleton pattern thread-safe?
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rewrite the `MySingleton` class to use `std::shared_ptr` for the returned instance.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are coroutines and what is the `co_await` keyword used for?
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Anthony Williams, C++ Concurrency in Action*, [https://www.amazon.com/C-Concurrency-Action-Anthony-Williams/dp/1617294691/](https://www.amazon.com/C-Concurrency-Action-Anthony-Williams/dp/1617294691/)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
