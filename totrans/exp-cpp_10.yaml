- en: Concurrency and Multithreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrent programming allows the creation of more efficient programs. C++ didn't
    have built-in support for concurrency or multithreading for a long time. Now it
    has full support for concurrent programming, threads, thread synchronization objects,
    and other functionality that we will discuss in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Before the language updated for thread support, programmers had to use third-party
    libraries. One of the most popular multithreading solutions was **POSIX** (**Portable
    Operating System Interface**) threads. C++ introduced thread support since C++11\.
    It makes the language even more robust and applicable to wider areas of software
    development. Understanding threads is somewhat crucial for C++ programmers as
    they tend to squeeze every bit of the program to make it run even faster. Threads
    introduce us to a completely different way of making programs faster by running
    functions concurrently. Learning multithreading at a fundamental level is a must
    for every C++ programmer. There are lots of programs where you can't avoid using
    multithreading, such as network applications, games, and GUI applications. This
    chapter will introduce you to concurrency and multithreading fundamentals in C++
    and best practices for concurrent code design.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding concurrency and multithreading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing threads and sharing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing concurrent code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using thread pools to avoid thread creation overheads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting familiar with coroutines in C++20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The g++ compiler with the `-std=c++2a` option is used to compile the examples
    in this chapter. You can find the source files used in this chapter at [https://github.com/PacktPublishing/Expert-CPP](https://github.com/PacktPublishing/Expert-CPP)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Understanding concurrency and multithreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest form of running a program involves its instructions being executed
    one by one by the **CPU** (**Central Processing Unit**). As you already know from
    previous chapters, a program consists of several sections, one of them containing
    the instructions of the program. Each instruction is loaded into a CPU register
    for the CPU to decode and execute it. It doesn't actually matter what programming
    paradigm you use to produce an application; the result is always the same—the
    executable file contains machine code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We mentioned that programming languages such as Java and C# use support environments.
    However, if you cut down the support environment in the middle (usually, the virtual
    machine), the final instructions being executed should have a form and format
    familiar to that particular CPU. It''s obvious to programmers that the order of
    statements run by the CPU is not mixed in any circumstance. For example, we are
    sure and can continue to be so that the following program will output `4`, `"hello"`,
    and `5`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We can guarantee that the value of the `a` variable will be initialized before
    we print it to the screen. The same way we can guarantee that the `"hello"` string will
    be printed before we decrement the value of `b`, and that the `(b + 1)` sum will
    be calculated before printing the result to the screen. The execution of each
    instruction might involve reading data from or writing to memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'As introduced in [Chapter 5](2b708ddc-255e-490e-bd4c-e783ccae5f9e.xhtml), *Memory Management
    and Smart Pointers*, the memory hierarchy is sophisticated enough to make our
    understanding of program execution a little bit harder. For example, the `int
    b{a};` line from the previous example assumes that the value of `a` is loaded
    from the memory into a register in the CPU, which then will be used to write into
    the memory location of `b`. The keyword here is the *location* because it carries
    a little bit of special interpretation for us. More specifically, we speak about
    memory location. Concurrency support depends on the memory model of the language,
    that is, a set of guarantees for concurrent access to memory. Although the byte
    is the smallest addressable memory unit, the CPU works with words in data. That
    said, the word is the smallest unit the CPU reads from or writes to memory. For
    example, we consider the following two declarations separate variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If those variables are allocated in the same word (considering the word size
    as bigger than the size of a `char`), reading and writing any of the variables
    involves reading the word containing both of them. Concurrent access to the variables
    might lead to unexpected behavior. That''s the issue requiring memory model guarantees. The
    C++ memory model guarantees that two threads can access and update separate memory
    locations without interfering with each other. A memory location is a scalar type.
    A scalar type is an arithmetic type, pointer, enumeration, or `nullptr_t`. The
    largest sequence of adjacent bit-fields of non-zero length is considered a memory
    location too. A classic example would be the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For the preceding example, two threads accessing the same struct's separate
    memory locations won't interfere with each other. So, what should we consider
    when speaking about concurrency or multithreading?
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrency is usually confused with multithreading. They are similar in nature but
    are different concepts in detail. To make things easy, just imagine concurrency
    as two operations whose running times interleave together. Operation `A` runs
    concurrently with operation `B` if their start and end times are interleaved at
    any point, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a602270-5f05-4ec9-9d02-5ab13bbf4883.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When two tasks run concurrently, they don''t have to run parallel. Imagine
    the following situation: you are watching TV while surfing the internet. Though
    it''s not a good practice to do so, however, let''s imagine for a moment that
    you have a favorite TV show that you can''t miss and at the same time, your friend
    asked you to do some research on bees. You can''t actually concentrate on both
    tasks; at any fixed moment, your attention is grabbed by either the show you are
    watching or the interesting facts about bees that you are reading in an article
    found on the web. Your attention goes from the show to the bees from time to time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of concurrency, you are doing two tasks concurrently. Your brain gives
    a time portion to the show: you watch, enjoy, and then switch to the article,
    read a couple of sentences, and switch back to the show. This is a simple example
    of concurrently running tasks. Just because their start and end times interleave
    doesn''t mean they run at the same time. On the other hand, you breathe while
    doing any of the tasks mentioned earlier. Breathing happens in the background;
    your brain doesn''t switch your attention from the show or the article to your
    lungs to inhale or exhale. Breathing while watching the show is an example of
    parallel running tasks. Both examples show us the essence of concurrency.'
  prefs: []
  type: TYPE_NORMAL
- en: So, what is going on when you run more than one application on your computer?
    Are they running in parallel? It's for sure that they run concurrently, however,
    the actual parallelism depends on your computer's hardware. Most mass-market computers
    consist of a single CPU. As we know from previous chapters, the main job of the
    CPU is running an application's instructions one by one. How would a single CPU
    handle the running of two applications at the same time? To understand that, we
    should learn about processes.
  prefs: []
  type: TYPE_NORMAL
- en: Processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A process is an image of a program running in the memory. When we start a program,
    the OS reads the content of the program from the hard disk, copies it to the memory,
    and points the CPU to the starting instruction of the program. The process has
    its private virtual address space, stack, and heap. Two processes don't interfere
    with each other in any way. That's a guarantee provided by the OS. That also makes
    a programmer's job very difficult if they aim for **Interprocess Communication**
    (**IPC**). We are not discussing low-level hardware features in this book but
    you should have a general understanding of what is going on when we run a program.
    It really depends on the underlying hardware—more specifically, the kind and structure
    of the CPU. The number of CPUs, number of CPU cores, levels of cache memory, and
    shared cache memory between CPUs or their cores—all of these affect the way the
    OS runs and executes programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of CPUs in a computer system defines the number of processes running
    truly in parallel. This is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/249ef5f9-c6a5-43bf-bad3-7648ec848c90.png)'
  prefs: []
  type: TYPE_IMG
- en: When we speak about multiprocessing, we consider an environment that allows
    several processes to run concurrently. And here comes the tricky part. If the
    processes actually run at the same time, then we say that they run in parallel.
    So, concurrency is not parallelism while parallelism implies concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: If the system has just one CPU, processes run concurrently but not in parallel.
    The OS manages this with a mechanism called **context switching**. Context switching
    implies freezing the work of the process for a moment, copying all of the register
    values that the process was using at the current time, and storing all of the active
    resources and values of the process. When a process is stopped, another process
    takes on the rights to run. After the specified amount of time provided for this
    second process, the OS starts the context switching for it. Again, it copies all
    of the resources used by the process. Then, the previous process gets started.
    Before starting it, the OS copies back the resources and values to the corresponding
    slots used by the first process and then resumes the execution of this process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The interesting thing is that the processes are not even aware of such a thing.
    The described process happens so fast that the user cannot actually notice that
    the programs running in the OS are not actually running at the same time. The
    following illustration depicts two processes run by a single CPU. When one of
    the processes is *active*, the CPU executes its instructions sequentially, storing
    any intermediary data in its registers (you should consider cache memory as in
    the game, too). The other process is *waiting* for the OS to provide its time
    portion to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a385b09c-4de2-4fd5-8057-8831d0675c61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Running more than one process is a sophisticated job for the OS. It manages
    states of processes, defines which process should take more CPU time than others,
    and so on. Each process gets a fixed time to run before the OS switches to another
    process. This time can be longer for one process and shorter for another. Scheduling
    processes happens using priority tables. The OS provides more time to processes
    with a higher priority, for example, a system process has higher priority than
    user processes. Another example could be that a background task monitoring network
    health has a higher priority than a calculator application. When the provided
    time slice is up, the OS initiates a context switch, that is, it stores the state
    of **Process A** to resume its execution later:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d6e1429-9787-443b-9ea6-2a591b51de4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After storing the state, as showing in the following diagram, it switches to
    the next process to execute it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ba4ecae-987c-4b5a-a51c-2790b12a3c5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Obviously, if **Process B** was running before, its state should be loaded
    back to the CPU. In the same way, when the time slice (or time quantum) is up
    for **Process B**, the OS stores its state and loads the state of **Process A**
    back to the CPU (the state it had before being paused by the OS):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed451d71-cf02-45f6-8929-484ecb4ed453.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Processes do not share anything in common—or at least they think so. Each running
    process behaves as if it''s alone in the system. It has all of the resources the
    OS can provide. In reality, the OS manages to keep processes unaware of each other,
    hence simulating freedom for each one. Finally, after loading the state of **Process
    A** back, the CPU continues executing its instructions like nothing happened:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b44e37d7-6d77-4248-ad76-e94dc8ab76f4.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Process B** is frozen until a new time slice is available for it to run.'
  prefs: []
  type: TYPE_NORMAL
- en: A single CPU running more than one process is similar to a teacher checking
    examination papers of students. The teacher can check only one exam paper at a
    time, though they can introduce some concurrency by checking answers one by one
    for each exam test. First, they check the answer to the first question for one
    student, then switch to the first answer of the test of the second student, and
    then switches back to the first student's second answer and so on. Whenever the
    teacher switch from one exam paper to the other, they note down the number of
    the question where they left off. This way, they will know where to start when
    getting back to the same paper.
  prefs: []
  type: TYPE_NORMAL
- en: In the same way, the OS notes down the point of execution of a process before
    pausing it to resume another process. The second process can (and most probably
    will) use the same register set used by the paused process. This forces the OS
    to store register values for the first process somewhere to be recovered later.
    When the OS pauses the second process to resume the first one, it loads already
    saved register values back into corresponding registers. The resumed process won't
    notice any difference and will continue its work like it was never paused.
  prefs: []
  type: TYPE_NORMAL
- en: Everything described in the preceding two paragraphs relates to single-CPU systems.
    In the case of multi-CPU systems, each CPU in the system has its own set of registers.
    Also, each CPU can execute program instructions independently of the other CPUs,
    which allows running processes in parallel without pausing and resuming them.
    In the example, a teacher with a couple of assistants is similar to a system with
    three CPUs. Each one of them can check one exam paper; all of them are checking
    three different exam papers at any point in time.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Difficulties arise when processes need to contact each other in some way. Let''s
    say a process should calculate something and pass the value to a completely different
    process. There are several methods to achieve IPC—one of them is using a memory
    segment shared between processes. The following diagram depicts two processes
    accessing the shared memory segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba6aa4ed-83da-4987-b27f-17429fa23ec4.png)'
  prefs: []
  type: TYPE_IMG
- en: One process stores the results of the calculation to a shared segment in the
    memory, and the second process reads it from the segment. In the context of our
    previous example, the teacher and their assistants share their checking results
    in a shared paper. Threads, on the other hand, share address space of the process
    because they run in the context of the process. While a process is a program,
    thread is a function rather than a program. That said, a process must have at
    least one thread , which we call the thread of execution. A thread is the container
    of instructions of a program that are run in the system, while the process encapsulates
    the thread and provides resources for it. Most of our interest lies in threads
    and their orchestration mechanisms. Let's now meet them in person.
  prefs: []
  type: TYPE_NORMAL
- en: Threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **thread** is a section of code in the scope of a process that can be scheduled
    by the OS scheduler. While a process is the image of the running program, managing
    multi-process projects along with IPC is much harder and sometimes useless compared
    to projects leveraging multithreading. Programs deal with data and, usually, collections
    of data. Accessing, processing, and updating data is done by functions that are
    either the methods of objects or free functions composed together to achieve an
    end result. In most projects, we deal with tens of thousands of functions and
    objects. Each function represents a bunch of instructions wrapped under a sensible
    name used to invoke it by other functions. Multithreading aims to run functions
    concurrently to achieve better performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a program that calculates the sum of three different vectors and
    prints them calls the function calculating the sum for the first vector, then
    for the second vector, and finally, for the last one. It all happens sequentially.
    If the processing of a single vector takes A amount of time, then the program
    will run in `3A` time. The following code demonstrates the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If there was a way to run the same function for three different vectors simultaneously,
    it would take just A amount of time for the whole program in the preceding example.
    Threads of execution, or just threads, are exact ways of running tasks concurrently.
    By tasks, we usually mean a function, although you should remember `std::packaged_task`
    as well. Again, concurrency shouldn't be confused with parallelism. When we speak
    about threads running concurrently, you should consider the context switching
    discussed previously for the process. Almost the same applies to threads.
  prefs: []
  type: TYPE_NORMAL
- en: '`std::packaged_task` is similar to `std::function`. It wraps a callable object—a
    function, lambda, function object, or bind expression. The difference with `std::packaged_task`
    is that it can be invoked asynchronously. There''s more on that later in this
    chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Each process has a single thread of execution, sometimes called the **main thread**.
    A process can have more than one thread, and that's when we call it **multithreading**.
    Threads run in almost the same way the processes. They also have context switching.
  prefs: []
  type: TYPE_NORMAL
- en: Threads run separately from each other, but they share most of the resources
    of process because all of the threads belong to the process. The process occupies
    hardware and software resources such as CPU registers and memory segments, including
    its own stack and heap. While a process doesn't share its stack or heap with other
    processes, its threads have to use the same resources that are occupied by the
    process. Everything that happens in a thread's life happens within the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, threads don''t share the stack. Each thread has its portion of the
    stack. The reason behind this segregation relies on the fact that a thread is
    just a function and the function itself should have access to the stack to manage
    the life cycle of its arguments and local variables. When we run the same function
    as two (or more) separately running threads, the runtime should somehow handle
    their boundaries. Although it''s error-prone, you can pass a variable from one
    thread to another (either by value or by reference). Let''s suppose that we started
    three threads running the `process_vector()` function for the three vectors in
    the preceding example. You should imagine that starting a thread means *copying*
    the underlying function somehow (its variables but not the instructions) and running
    it separately from any other thread. In this scenario, the same function will
    be copied as three different images, and each one of them will run independently
    of the others, hence each should have its own stack. On the other hand, the heap
    is shared between threads. So, basically, we arrive at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2837ebc0-a491-4359-aa66-f0e737feff43.png)'
  prefs: []
  type: TYPE_IMG
- en: As in the case of processes, threads running concurrently are not necessarily running
    in parallel. Each thread gets a small portion of CPU time to be run and, again,
    there is an overhead regarding the switching from one thread to another. Each
    paused thread's state should be stored somewhere to be recovered later when resuming
    it. The internal structure of the CPU defines whether threads could truly run
    in parallel. The number of CPU cores defines the number of threads that can truly
    run in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: The C++ thread library provides the `hardware_concurrency()` function to find
    out the number of threads that can truly run concurrently. You can refer to this
    number when designing concurrent code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts two CPUs having four cores each. Each core can
    run a thread independently of the other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82938768-6461-41e2-94dc-454295e6fd96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Not only do two processes run in parallel but also their threads are run in
    parallel using the CPU cores. Now, how will the situation change if we have several
    threads but one single-core CPU? Almost the same as we have illustrated earlier
    for processes. Look at the following diagram—it depicts how the CPU executes **Thread
    1** for some time slice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43a122ae-5378-4cb0-8770-cfea0b51cad2.png)'
  prefs: []
  type: TYPE_IMG
- en: The currently active **Process A** has two threads that run concurrently. At
    each specified point in time, only one of the threads is executed. When the time
    slice is up for **Thread 1**, **Thread 2** is executed. The difference from the
    model we discussed for processes is that threads share the resources of the process,
    which leads to unnatural behavior if we aren't concerned with concurrent code
    design issues. Let's dive into C++ threading support and find out what issues
    arise when using multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: Working with threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When the C++ program starts, that is, the `main()` function starts its execution,
    you can create and launch new threads that will run concurrently to the main thread. To
    start a thread in C++, you should declare a thread object and pass it the function
    that you want to run concurrently to the main thread. The following code demonstrates
    the declaration and starting of a thread using `std::thread` defined in `<thread>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it. We can create a better example to show how two threads work concurrently.
    Let''s say we print numbers in a loop concurrently to see which thread prints
    what:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding example will print both outputs with the `Main:` and `Background:` prefixes mixed
    together. An excerpt from the output might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Whenever the main thread finishes its work (printing to the screen one million
    times), the program wants to finish without waiting for the background thread
    to complete. It leads to program termination. Let's see how we should modify the
    previous example.
  prefs: []
  type: TYPE_NORMAL
- en: Waiting for threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `thread` class provides the `join()` function if you want to wait for it
    to finish. Here is a modified version of the previous example that waits for the
    `background` thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As we already discussed previously, the `thread` function is run as a separate
    entity independently from other threads- even the one that started it. It won't
    wait for the thread it has just started, and that's why you should explicitly
    tell the caller function to wait for it to finish. It is necessary to signal that
    the calling thread (the main thread) is waiting for the thread to finish before
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The symmetric opposite of the `join()` function is the `detach()` function.
    The `detach()` function indicates that the caller isn''t interested in waiting
    for the thread to finish. In this case, the thread can have an independent life.
    As shown here (like it''s already 18 years old):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Although detaching a thread might seem natural, there are plenty of scenarios
    when we need to wait for the thread to finish. For example, we might pass local
    to the caller variables to the running thread. In this case, we can''t let the
    caller detach the thread as the caller might finish its work earlier than the
    thread started in it. Let''s illustrate that for the sake of clarity. **Thread
    1** declares the `loc` variable and passes it to **Thread 2**, which has been
    started from **Thread 1**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8bcd907b-742d-4b19-9484-87822f730a68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Passing the address of `loc` to **Thread 2** is error-prone if **Thread 1**
    doesn''t join it. If **Thread 1** finishes its execution before **Thread 2**,
    then accessing `loc` by its address leads to an undefined behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58cea33e-6366-4f28-9ac5-84e785da0452.png)'
  prefs: []
  type: TYPE_IMG
- en: There is no such object anymore, so the best that we can hope for the program
    is to crash. It will lead to unexpected behavior because the running thread won't
    have access to the caller's local variables anymore. You should either join or
    detach a thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can pass any callable object to `std::thread`. The following example shows
    passing a lambda expression to the thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, we can use callable objects as thread arguments. Take a look at
    the following code declaring the `TestTask` class with the overridden `operator()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: One of the advantages of a functor (the `TestTask` class with the overridden
    `operator()` function ) is its ability to store state information. Functors are
    a beautiful implementation of the command design pattern that we will discuss
    in [Chapter 11](0e28887e-1a43-4510-a8ef-b3ad7531868d.xhtml), *Designing a Strategy
    Game Using Design Patterns*. Getting back to the threads, let's move on the a
    new addition in the language that allows for better ways to join threads.
  prefs: []
  type: TYPE_NORMAL
- en: Using std::jthread
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: C++20 introduces a joinable thread, `std::jthread`. It provides the same interface
    `std::thread` as provides, so we can replace all threads with jthreads in the
    code. It actually wraps `std::thread`, so basically it delegates down to the wrapped
    thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the version of your compiler doesn''t support `std::jthread`, you are free
    to go with the **RAII** (**Resource Acquisition Is Initialization**) idiom, which
    is perfectly applicable to threads. Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the preceding code lacks an additional check because a thread passed
    to the RAII class might have been already detached. To see whether the thread
    could be joined, we use the `joinable()` function. This is how we should overwrite
    the `thread_raii` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The destructor first tests whether the thread is joinable before calling the
    `join()` function. However, instead of dealing with idioms and being concerned
    about whether the thread has been joined already before joining it, we prefer
    using `std::jthread`. Here''s how we can do that using the `TestTask` function
    declared previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it—there''s no need to call `jt.join()`, and a new cooperative interruptible
    feature out of the box that we use by incorporating jthread. We say that jthread
    is cooperative interruptible because it provides the `request_stop()` function
    ,which does what its name says—requests the thread to stop. Although that request
    fulfillment is implementation-defined, it''s a nice way not to wait for the thread
    forever. Recall the example with the thread printing numbers in an infinite loop.
    We modified the main thread to wait for it, and that leads to waiting for it forever.
    Here''s how we can modify the thread using `std::jthread` to leverage the `request_stop()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `print_numbers_in_background()` function now receives a request and can
    behave accordingly. Now, let's see how to pass arguments to the thread function.
  prefs: []
  type: TYPE_NORMAL
- en: Passing arguments to the thread function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `std::thread` constructor takes arguments and forwards them to the underlying
    `thread` function. For example, to pass the arguments `4` and `2` to the `foo()`
    function here, we pass the arguments to the `std::thread` constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The arguments `4` and `2` will be passed as the first and second arguments to
    the `foo()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example illustrates the passing of an argument by reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: To understand why we named the function `error_prone`, we should know that the
    thread constructor copies the values passed to it and then passes them to the
    thread function with `rvalue` references. This is done to work with move-only
    types. So it will try to call the `make_changes()` function with `rvalue`, which
    will fail to compile (you can't pass `rvalue` to a function that expects a non-constant
    reference). We need to wrap the arguments that need to be a reference in `std::ref:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code emphasizes that the argument should be passed by reference.
    Working with threads requires being a little more attentive because there are
    many ways to get unexpected results or undefined behavior in the program. Let's
    see how we can manage threads to produce safer multithreaded applications.
  prefs: []
  type: TYPE_NORMAL
- en: Managing threads and sharing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed previously, the execution of threads involves pausing and resuming
    some of them if the number of threads exceeds the number of parallel running threads
    supported by the hardware. Besides that, the creation of a thread also has its
    overhead. One of the suggested practices to deal with having many threads in a
    project is using thread pools.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea of a thread pool lies in the concept of caching. We create and keep
    threads in some container to be used later. The container is called a pool. For
    example, the following vector represents a simple thread pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Whenever we need a new thread, instead of declaring the corresponding `std::thread`
    object, we use one already created in the pool. When we are done with the thread,
    we can push it back to the vector to use it later if necessary. This saves some
    time when working with 10 or more threads. A proper example would be a web server.
  prefs: []
  type: TYPE_NORMAL
- en: 'A web server is a program that waits for incoming client connections and creates
    a separate connection for each client to be processed independently from others.
    A typical web server usually deals with thousands of clients at the same time.
    Each time a new connection is initiated with some client, the web server creates
    a new thread and handles the client requests. The following pseudo-code demonstrates
    a simple implementation of a web server''s incoming connection management:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'When using a thread pool, the preceding code will avoid the creation of a thread
    each time it needs to process a client request. The creation of a new thread requires
    additional and rather expensive work from the OS. To save that time, we use a
    mechanism that omits creating new threads on each request. To make the pool even
    better, let''s replace its container with a queue. Whenever we ask for a thread,
    the pool will return a free thread, and whenever we are done with a thread, we
    push it back to the pool. A simple design of a thread pool would look like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor creates and pushes threads to the queue. In the following pseudo-code,
    we replace the direct creation of a thread for client request processing with
    `ThreadPool` ,which we looked at previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Supposing that the `handle_request()` function pushes the thread back to the
    pool when it's done, the pool behaves as a centralized store for connection threads.
    Though shown in the preceding snippet is far from being ready for production,
    it conveys the basic idea of using thread pools in intensive applications.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A race condition is something that programmers using multithreading are scared
    of and try to avoid as much as possible. Imagine two functions that work concurrently
    with the same data, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: A potential race condition is happening because threads `t1` and `t2` are modifying
    the same variable with more than one step. Any operation that is performed in
    a single thread-safe step is called an **atomic operation**. In this case, incrementing
    the value of the variable is not an atomic operation, even if we use the increment
    operator.
  prefs: []
  type: TYPE_NORMAL
- en: Protecting shared data using a mutex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To protect shared data, objects called **mutexes** are used widely. A mutex
    is an object that controls the running of a thread. Imagine threads as humans
    making a deal about how to work with data one by one. When a thread locks a mutex,
    the other thread waits until it is done with the data and unlocks the mutex. The
    other thread then locks the mutex and starts working with data. The following
    code demonstrates how we can solve the problem of a race condition using a mutex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: When `t1` starts executing `inc()`, it locks a mutex, which avoids any other
    thread to access the global variable unless the original thread doesn't unlock
    the next thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'C++17 introduced a lock guard that allows guarding the mutex in order not to
    forget unlock it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: It's always better to use language-provided guards if possible.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding deadlocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: New problems arise with mutexes, such as **deadlocks**. A deadlock is a condition
    of  multithreaded code when two or more threads lock a mutex and wait for the
    other to unlock another.
  prefs: []
  type: TYPE_NORMAL
- en: The common advice to avoid deadlock is to always lock two or more mutexes in
    the same order. C++ provides the `std::lock()` function, which serves the same
    purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates the `swap` function, which takes two arguments
    of type `X`. We suppose that `X` has a member, `mt` , which is a mutex. The implementation
    of the `swap` function locks the mutex of the left object first, then it locks
    the mutex of the right object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: To avoid deadlocks in general, avoid nested locks. That said, don't acquire
    a lock if you are already holding one. If this is not the case, then acquire locks
    in a fixed order. The fixed order will allow you to avoid deadlocks.
  prefs: []
  type: TYPE_NORMAL
- en: Designing concurrent code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Project complexity rises drastically when concurrency is introduced. It's much
    easier to deal with sequentially executing synchronous code compared to concurrent
    counterparts. Many systems avoid using multithreading at all by introducing event-driven
    development concepts, such as the event loop. The point of using an event loop
    is to introduce a manageable approach to asynchronous programming. To take the
    concept further, imagine any application providing a **Graphical User Interface**
    (**GUI**). Whenever the user clicks on any GUI component, such as buttons; types
    in fields; or even moves the mouse, the application receives so-called events
    regarding the user action. Whether it's `button_press`, `button_release`, `mouse_move`,
    or any other event, it represents a piece of information to the application to
    react properly. A popular approach is to incorporate an event loop to queue any
    event that occurred during user interaction.
  prefs: []
  type: TYPE_NORMAL
- en: While the application is busy with its current task, the events produced by
    user actions are queued to be processed at some time in the future. The processing
    involves calling handler functions attached to each event. They are called in
    the order they were put into the queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Introducing multithreading to the project brings additional complexity with
    it. You should now take care of race conditions and proper thread handling, maybe
    even using a thread pool to reuse thread objects. In sequentially executed code,
    you care for the code and only the code. Using multithreading, you now care a
    little bit more about the ways of execution of the very same code. For example,
    a simple design pattern such as the singleton behaves differently in a multithreading
    environment. The classic implementation of a singleton looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code starts two threads, both using the `MySingleton` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Threads `t1` and `t2` both call the `get_instance()` static member function
    of the `MySingleton` class. It''s possible that `t1` and `t2` both pass the check
    for the empty instance and both execute the new operator. Clearly, we have a race
    condition here. The resource, in this case, the class instance, should be protected
    from such a scenario. Here''s an obvious solution using a mutex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Using a mutex will solve the problem, but will make the function work more
    slowly because each time a thread requests an instance, a mutex will be locked
    instead (which involves additional operations by the OS kernel). The proper solution
    would be using the Double-Checked Locking pattern. Its basic idea is this:'
  prefs: []
  type: TYPE_NORMAL
- en: Lock the mutex after the `instance_` check.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the `instance_` again after the mutex has been locked because another
    thread might have passed the first check and wait for the mutex to unlock.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'See the code for details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Several threads may pass the first check and one of them will lock the mutex.
    Only one thread makes it to the new operator call. However, after unlocking the
    mutex, threads that have passed the first check will try to lock it and create
    the instance. The second check is there to prevent this. The preceding code allows
    us to reduce the performance overhead of the synchronized code. The approach we
    provided here is one of the ways to prepare yourself for concurrent code design.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent code design is very much based on the capabilities of the language
    itself. The evolution of C++ is marvelous. In its earliest versions, it didn't
    have built-in support for multithreading. Now, it has a solid thread library and
    the new C++20 standard provides us with even more powerful tools, such as coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing coroutines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed an example of asynchronous code execution when speaking about GUI
    applications. GUI components react to user actions by firing corresponding events,
    which are pushed in the event queue. This queue are then processed one by one
    by invoking attached handler functions. The described process happens in a loop;
    that's why we usually refer to the concept as the event loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Asynchronous systems are really useful in I/O operations because any input
    or output operation blocks the execution at the point of I/O call. For example,
    the following pseudo-code reads a file from a directory and then prints a welcome
    message to the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Attached to the synchronous execution pattern, we know that the message Welcome
    to the app! will be printed only after the `read_file()` function finishes executing.
    `process_file_contents()` will be invoked only after `cout` completes. When dealing
    with asynchronous code, all we know about code execution starts to behave like
    something unrecognizable. The following modified version of the preceding example
    uses the `read_file_async()` function to read the file contents asynchronously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Considering `read_file_async()` is an asynchronous function, the message Welcome
    to the app! will be printed sooner than the file contents. The very nature of
    asynchronous execution allows us to invoke functions to be executed in the background,
    which provides us with non-blocking input/output.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is a slight change in the way we treat the return value of the
    function. If we deal with an asynchronous function, its return value is considered
    as something called a **promise** or a **promise object**. It''s the way the system
    notifies us when the asynchronous function has completed. The promise object has
    three states:'
  prefs: []
  type: TYPE_NORMAL
- en: Pending
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rejected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fulfilled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A promise object is said to be fulfilled if the function is done and the result
    is ready to be processed. In the event of an error, the promise object will be
    in the rejected state. If the promise is not rejected nor fulfilled, it is in
    the pending state.
  prefs: []
  type: TYPE_NORMAL
- en: 'C++20 introduced coroutines as an addition to the classic asynchronous functions.
    Coroutines move the background execution of the code to next level; they allow
    a function to be paused and resumed when necessary. Imagine a function that reads
    file contents and stops in the middle, passes the execution context to another
    function, and then resumes the reading of the file to its end. So, before diving
    deeper, consider a coroutine as a function that can be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Started
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paused
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resumed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finished
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To make a function a coroutine, you would use one of the keywords `co_await`,
    `co_yield`, or `co_return`. `co_await` is a construct telling the code to wait
    for asynchronously executing code. It means the function can be suspended at that
    point and resume its execution when a result is ready. For example, the following
    code requests an image from the network using a socket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As the network request operation is also considered as an **input/output**
    operation, it might block the execution of the code. To prevent blocking, we use
    asynchronous calls. The line using `co_await` in the preceding example is a point
    where the function execution could be suspended. In simpler words, when the execution
    reaches the line with `co_await`, the following happens:'
  prefs: []
  type: TYPE_NORMAL
- en: It quits the function for a while (until there isn't ready data).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It continues executing from where it was before `process_image()` was called.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then comes back again to continue executing `process_image()` at the point
    where it left it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To achieve this, a coroutine (the `process_image()` function is a coroutine)
    is not handled the way regular functions are handled in C++. One of the interesting
    or even surprising features of coroutines is that they are **stackless.** We know
    that functions can't live without the stack. That's where the function pushes
    its arguments and local variables before even executing its instructions. Coroutines,
    on the other hand, instead of pushing anything to the stack, save their state
    in the heap and recover it when resumed.
  prefs: []
  type: TYPE_NORMAL
- en: This is tricky because there are also stackful coroutines. Stackful coroutines,
    also referred to as **fibers**, have a separate stack.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines are connected to callers. In the preceding example, the function
    that call `sprocess_image()` transfers execution to the coroutine and the pause
    by the coroutine (also known as **yielding**) transfers the execution back to
    the caller. As we stated, the heap is used to store the state of the coroutine,
    but the actual function-specific data (arguments, and local variables) are stored
    on the caller's stack. That's it—the coroutine is associated with an object that
    is stored on the caller function's stack. Obviously, the coroutine lives as long
    as its object.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines might give a wrong impression of redundant complexity added to the
    language, but their use cases are great in improving applications that use asynchronous
    I/O code (as in the preceding example) or lazy computations. That said, when we
    have to invent new patterns or introduce complexities into projects to handle,
    for instance, lazy computations, we now can improve our experience by using coroutines
    in C++. Please note that asynchronous I/O or lazy computations are just two examples
    of coroutine applications. There are more out there.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've discussed the concept of concurrency and showed the difference
    between parallelism. We learned the difference between a process and a thread,
    the latter being of interest. Multithreading allows us to manage a program to
    be more efficient, though it also brings additional complexity with it. To handle
    data races, we use synchronization primitives such as a mutex. A mutex is a way
    to lock the data used by one thread to avoid invalid behavior produced by simultaneously
    accessing the same data from several threads.
  prefs: []
  type: TYPE_NORMAL
- en: We also covered the idea that an input/output operation is considered blocking
    and asynchronous functions are one of the ways to make it non-blocking. Coroutines
    as a part of the asynchronous execution of code were introduced in C++20.
  prefs: []
  type: TYPE_NORMAL
- en: We learned how to create and start a thread. More importantly, we learned how
    to manage data between threads. In the next chapter, we will dive into data structures
    that are used in concurrent environments.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is concurrency?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between concurrency and parallelism?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the difference between a process and a thread?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write code to start a thread.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could you make the singleton pattern thread-safe?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rewrite the `MySingleton` class to use `std::shared_ptr` for the returned instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are coroutines and what is the `co_await` keyword used for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Anthony Williams, C++ Concurrency in Action*, [https://www.amazon.com/C-Concurrency-Action-Anthony-Williams/dp/1617294691/](https://www.amazon.com/C-Concurrency-Action-Anthony-Williams/dp/1617294691/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
