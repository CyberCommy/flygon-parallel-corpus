- en: Chapter 1. Gathering Open Source Intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Gathering information using the Shodan API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scripting a Google+ API search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading profile pictures using the Google+ API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harvesting additional results using the Google+ API pagination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting screenshots of websites using QtWebKit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Screenshots based on port lists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spidering websites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Open Source Intelligence** (**OSINT**) is the process of gathering information
    from Open (overt) sources. When it comes to testing a web application, that might
    seem a strange thing to do. However, a great deal of information can be learned
    about a particular website before even touching it. You might be able to find
    out what server-side language the website is written in, the underpinning framework,
    or even its credentials. Learning to use APIs and scripting these tasks can make
    the bulk of the gathering phase a lot easier.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at a few of the ways we can use Python to leverage
    the power of APIs to gain insight into our target.
  prefs: []
  type: TYPE_NORMAL
- en: Gathering information using the Shodan API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shodan is essentially a vulnerability search engine. By providing it with a
    name, an IP address, or even a port, it returns all the systems in its databases
    that match. This makes it one of the most effective sources for intelligence when
    it comes to infrastructure. It's like Google for internet-connected devices. Shodan
    constantly scans the Internet and saves the results into a public database. Whilst
    this database is searchable from the Shodan website ([https://www.shodan.io](https://www.shodan.io)),
    the results and services reported on are limited, unless you access it through
    the **Application Programming Interface** (**API**).
  prefs: []
  type: TYPE_NORMAL
- en: Our task for this section will be to gain information about the Packt Publishing
    website by using the Shodan API.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the time of writing this, Shodan membership is $49, and this is needed to
    get an API key. If you're serious about security, access to Shodan is invaluable.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't already have an API key for Shodan, visit [www.shodan.io/store/member](http://www.shodan.io/store/member)
    and sign up for it. Shodan has a really nice Python library, which is also well
    documented at [https://shodan.readthedocs.org/en/latest/](https://shodan.readthedocs.org/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: 'To get your Python environment set up to work with Shodan, all you need to
    do is simply install the library using `cheeseshop`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s the script that we are going to use for this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding script should produce an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: I've just chosen a few of the available data items that Shodan returns, but
    you can see that we get a fair bit of information back. In this particular instance,
    we can see that there is a potential vulnerability identified. We also see that
    this server is listening on ports `80` and `443` and that according to the banner
    information, it appears to be running `nginx` as the HTTP server.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Firstly, we set up our static strings within the code; this includes our API
    key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to create our API object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to search for information on a host using the API, we need to know
    the host''s IP address. Shodan has a DNS resolver but it''s not included in the
    Python library. To use Shodan''s DNS resolver, we simply have to make a GET request
    to the Shodan DNS Resolver URL and pass it the domain (or domains) we are interested
    in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The returned JSON data will be a dictionary of domains to IP addresses; as we
    only have one target in our case, we can simply pull out the IP address of our
    host using the `target` string as the key for the dictionary. If you were searching
    on multiple domains, you would probably want to iterate over this list to obtain
    all the IP addresses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we have the host''s IP address, we can use the Shodan libraries `host`
    function to obtain information on our host. The returned JSON data contains a
    wealth of information about the host, though in our case we will just pull out
    the IP address, organization, and if possible the operating system that is running.
    Then we will loop over all of the ports that were found to be open and their respective
    banners:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned data may also contain potential **Common Vulnerabilities and Exposures**
    (**CVE**) numbers for vulnerabilities that Shodan thinks the server may be susceptible
    to. This could be really beneficial to us, so we will iterate over the list of
    these (if there are any) and use another function from the Shodan library to get
    information on the exploit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: That's it for our script. Try running it against your own server.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've only really scratched the surface of the Shodan Python library with our
    script. It is well worth reading through the Shodan API reference documentation
    and playing around with the other search options. You can filter results based
    on "facets" to narrow down your searches. You can even use searches that other
    users have saved using the "tags" search.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Downloading the example code**'
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all the Packt Publishing books you have purchased. If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: Scripting a Google+ API search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Social media is a great way to gather information on a target company or person.
    Here, we will be showing you how to script a Google+ API search to find contact
    information for a company within the Google+ social sites.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some Google APIs require authorization to access them, but if you have a Google
    account, getting the API key is easy. Just go to [https://console.developers.google.com](https://console.developers.google.com)
    and create a new project. Click on **API & auth** | **Credentials**. Click on
    **Create new key** and **Server key**. Optionally enter your IP or just click
    on **Create**. Your API key will be displayed and ready to copy and paste into
    the following recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s a simple script to query the Google+ API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The preceding code makes a request to the Google+ search API (authenticated
    with your API key) and searches for accounts matching the target; `packtpub.com`.
    Similarly to the preceding Shodan script, we set up our static strings including
    the API key and target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step does two things: first, it sends the HTTP `GET` request to the
    API server, then it reads in the response and stores the output into an `api_response`
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This request returns a JSON formatted response; an example snippet of the results
    is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04044_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In our script, we convert the response into a list so it''s easier to parse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The final part of the code loops through the list and prints only the lines
    that contain `displayName`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04044_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: See also…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, *Downloading profile pictures using the Google+ API*, we
    will look at improving the formatting of these results.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By starting with a simple script to query the Google+ API, we can extend it
    to be more efficient and make use of more of the data returned. Another key aspect
    of the Google+ platform is that users may also have a matching account on another
    of Google's services, which means you can cross-reference accounts. Most Google
    products have an API available to developers, so a good place to start is [https://developers.google.com/products/](https://developers.google.com/products/).
    Grab an API key and plug the output from the previous script into it.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading profile pictures using the Google+ API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have established how to use the Google+ API, we can design a script
    to pull down pictures. The aim here is to put faces to names taken from web pages.
    We will send a request to the API through a URL, handle the response through JSON,
    and create picture files in the working directory of the script.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s a simple script to download profile pictures using the Google+ API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first change is to store the display name into a variable, as this is then
    reused later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we grab the image URL from the JSON response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The final part of the code does a number of things in three simple lines: firstly
    it opens a file on the local disk, with the filename set to the `name` variable.
    The `wb+` flag here indicates to the OS that it should create the file if it doesn''t
    exist and to write the data in a raw binary format. The second line makes a HTTP
    `GET` request to the image URL (stored in the `image` variable) and writes the
    response into the file. Finally, the file is closed to free system memory used
    to store the file contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: After the script is run, the console output will be the same as before, with
    the display names shown. However, your local directory will now also contain all
    the profile images, saved as JPEG files.
  prefs: []
  type: TYPE_NORMAL
- en: Harvesting additional results from the Google+ API using pagination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, the Google+ APIs return a maximum of 25 results, but we can extend
    the previous scripts by increasing the maximum value and harvesting more results
    through pagination. As before, we will communicate with the Google+ API through
    a URL and the `urllib` library. We will create arbitrary numbers that will increase
    as requests go ahead, so we can move across pages and gather more results.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following script shows how you can harvest additional results from the
    Google+ API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first big change in this script that is the main code has been moved into
    a `while` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the number of loops is set to a maximum of 10 to avoid sending too many
    requests to the API servers. This value can of course be changed to any positive
    integer. The next change is to the request URL itself; it now contains two additional
    trailing parameters `maxResults` and `pageToken`. Each response from the Google+
    API contains a `pageToken` value, which is a pointer to the next set of results.
    Note that if there are no more results, a `pageToken` value is still returned.
    The `maxResults` parameter is self-explanatory, but can only be increased to a
    maximum of 50:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The next part reads the same as before in the JSON response, but this time
    it also extracts the `nextPageToken` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The main `while` loop can stop if the `loops` variable increases up to 10,
    but sometimes you may only get one page of results. The next part in the code
    checks to see how many results were returned; if there were none, it exits the
    loop prematurely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we ensure that we increase the value of the `loops` integer each time.
    A common coding mistake is to leave this out, meaning the loop will continue forever:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Getting screenshots of websites with QtWebKit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: They say a picture is worth a thousand words. Sometimes, it's good to get screenshots
    of websites during the intelligence gathering phase. We may want to scan an IP
    range and get an idea of which IPs are serving up web pages, and more importantly
    what they look like. This could assist us in picking out interesting sites to
    focus on and we also might want to quickly scan ports on a particular IP address
    for the same reason. We will take a look at how we can accomplish this using the
    `QtWebKit` Python library.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The QtWebKit is a bit of a pain to install. The easiest way is to get the binaries
    from [http://www.riverbankcomputing.com/software/pyqt/download](http://www.riverbankcomputing.com/software/pyqt/download).
    For Windows users, make sure you pick the binaries that fit your `python/arch`
    path. For example, I will use the `PyQt4-4.11.3-gpl-Py2.7-Qt4.8.6-x32.exe` binary
    to install Qt4 on my Windows 32bit Virtual Machine that has Python version 2.7
    installed. If you are planning on compiling Qt4 from the source files, make sure
    you have already installed `SIP`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you''ve got PyQt4 installed, you''re pretty much ready to go. The following
    script is what we will use as the base for our screenshot class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Create the preceding script and save it in the Python `Lib` folder. We can then
    reference it as an import in our scripts.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The script makes use of `QWebView` to load the URL and then creates an image
    using QPainter. The `get_image` function takes a single parameter: our target.
    Knowing this, we can simply import it into another script and expand the functionality.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's break down the script and see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we set up our imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create our class definition; the class we are creating extends from
    `QWebView` by inheritance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create our initialization method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The initialization method sets the `self.__loaded` property. This is used along
    with the `__loadFinished` and `wait_load` functions to check the state of the
    application as it runs. It waits until the site has loaded before taking a screenshot.
    The actual screenshot code is contained in the `get_image` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Within this `get_image` function, we set the size of the viewport to the size
    of the contents within the main frame. We then set the image format, assign the
    image to a painter object, and then render the frame using the painter. Finally,
    we return the processed image.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use the class we''ve just made, we just import it into another script. For
    example, if we wanted to just save the image we get back, we could do something
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: That's all there is to it. In the next script, we will create something a little
    more useful.
  prefs: []
  type: TYPE_NORMAL
- en: Screenshots based on a port list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous script, we created our base function to return an image for
    a URL. We will now expand on that to loop over a list of ports that are commonly
    associated with web-based administration portals. This will allow us to point
    the script at an IP and automatically run through the possible ports that could
    be associated with a web server. This is to be used in cases when we don't know
    which ports are open on a server, rather than when where we are specifying the
    port and domain.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order for this script to work, we'll need to have the script created in the
    *Getting screenshots of a website with QtWeb Kit* recipe. This should be saved
    in the `Pythonxx/Lib` folder and named something clear and memorable. Here, we've
    named that script `screenshot.py`. The naming of your script is particularly essential
    as we reference it with an important declaration.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the script that we will be using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first create our import declarations. In this script, we use the `screenshot`
    script we created before and also the `requests` library. The `requests` library
    is used so that we can check the status of a request before trying to convert
    it to an image. We don't want to waste time trying to convert sites that don't
    exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we import our libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step sets up the array of common port numbers that we will be iterating
    over. We also set up a string with the IP address we will be using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create strings to hold the protocol part of the URL that we will be
    building later; this just makes the code later on a little bit neater:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create our method, which will do the work of building the URL string.
    After we''ve created the URL, we check whether we get a `200` response code back
    for our `get` request. If the request is successful, we convert the web page returned
    to an image and save it with the filename being the successful port number. The
    code is wrapped in a `try` block because if the site doesn''t exist when we make
    the request, it will throw an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our method is ready, we simply iterate over each port in the port
    list and call our method. We do this once for the HTTP protocol and then with
    HTTPS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: And that's it. Simply run the script and it will save the images to the same
    location as the script.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might notice that the script takes a while to run. This is because it has
    to check each port in turn. In practice, you would probably want to make this
    a multithreaded script so that it can check multiple URLs at the same time. Let's
    take a quick look at how we can modify the code to achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll need a couple more import declarations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to create a new function that we will call `threader`. This new
    function will handle putting our `testAndSave` functions into the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our new function, we just need to set up a new `Queue` object
    and make a few threading calls. We will take out the `testAndSave` calls from
    our `FOR` loop over the `portList` variable and replace it with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'So, our new script in total now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: If we run this now, we will get a much quicker execution of our code as the
    web requests are now being executed in parallel with each other.
  prefs: []
  type: TYPE_NORMAL
- en: You could try to further expand the script to work on a range of IP addresses
    too; this can be handy when you're testing an internal network range.
  prefs: []
  type: TYPE_NORMAL
- en: Spidering websites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many tools provide the ability to map out websites, but often you are limited
    to style of output or the location in which the results are provided. This base
    plate for a spidering script allows you to map out websites in short order with
    the ability to alter them as you please.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order for this script to work, you'll need the `BeautifulSoup` library, which
    is installable from the `apt` command with `apt-get install python-bs4` or alternatively
    `pip install beautifulsoup4`. It's as easy as that.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the script that we will be using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first import the necessary libraries and create two empty lists called `urls`
    and `urls2`. These will allow us to run through the spidering process twice. Next,
    we set up input to be added as an addendum to the script to be run from the command
    line. It will be run like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We then open the provided `url` variable and pass it to the `beautifulsoup`
    tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The `beautifulsoup` tool splits the content into parts and allows us to only
    pull the parts that we want to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We then pull all of the content that is marked as a tag in HTML and grab the
    element within the tag specified as `href`. This allows us to grab all the URLs
    listed in the page.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section handles relative and absolute links. If a link is relative,
    it starts with a slash to indicate that it is a page hosted locally to the web
    server. If a link is absolute, it contains the full address including the domain.
    What we do with the following code is ensure that we can, as external users, open
    all the links we find and list them as absolute links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We then repeat the process once more with the `urls` list that we identified
    from that page by iterating through each element in the original `url` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Other than a change in the referenced lists and variables, the code remains
    the same.
  prefs: []
  type: TYPE_NORMAL
- en: We combine the two lists and finally, for ease of output, we take the full list
    of the `urls` list and turn it into a set. This removes duplicates from the list
    and allows us to output it neatly. We iterate through the values in the set and
    output them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This tool can be tied in with any of the functionality shown earlier and later
    in this book. It can be tied to *Getting Screenshots of a website with QtWeb Kit*
    to allow you to take screenshots of every page. You can tie it to the email address
    finder in the [Chapter 2](ch02.html "Chapter 2. Enumeration"), *Enumeration*,
    to gain email addresses from every page, or you can find another use for this
    simple technique to map web pages.
  prefs: []
  type: TYPE_NORMAL
- en: The script can be easily changed to add in levels of depth to go from the current
    level of 2 links deep to any value set by system argument. The output can be changed
    to add in URLs present on each page, or to turn it into a CSV to allow you to
    map vulnerabilities to pages for easy notation.
  prefs: []
  type: TYPE_NORMAL
