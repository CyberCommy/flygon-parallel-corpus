- en: Chapter 4. Administration
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will see the following recipes related to MongoDB administration:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Renaming a collection
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viewing collection stats
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viewing database stats
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manually padding a document
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mongostat and mongotop utilities
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting current executing operations and killing them
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using profiler to profile operations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up users in Mongo
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interprocess security in Mongo
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying collection behavior using the collMod command
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up MongoDB as a Windows service
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replica set configurations
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stepping down as primary from the replica set
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the local database of a replica set
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding and analyzing oplogs
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building tagged Replica sets
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring the default shard for non-sharded collections
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual split and migration of chunks
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain-driven sharding using tags
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the config database in a sharded setup
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we will cover some of the tools and practices for administering
    MongoDB. The following recipes will help you collect statistics from your database,
    administer user access, analyze oplogs and look into some aspects of working with
    replica sets.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Renaming a collection
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever come across a scenario where you have named a table in a relational
    database and at a later point of time felt that the name could have been better?
    Or perhaps the organization you work for was late in realizing that the table
    names are really getting messy and enforce some standards on the names? Relational
    databases do have some proprietary ways to rename the tables and a database admin
    would do that for you.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: This raises a question though. In Mongo world, where collections are synonymous
    to tables, is there a way to rename a collection to some other name after it is
    created? In this recipe, we will explore this feature of Mongo where we rename
    an existing collection with some data in it.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would need to run a MongoDB instance to perform this collection renaming
    experiment. Refer to the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* for information on how to start the server. The operations we will perform
    would be from mongo shell.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the server is started and assuming it is listening for client connections
    on default port `27017`, execute the following command to connect to it from the
    shell:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once connected, using the default test database. Let''s create a collection
    with some test data. The collection we will use is named:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The test data will now be created (we may verify the data by querying the collection
    `sloppyNamedCollection`).
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rename the collection `neatNamedCollection` as follows:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Verify that the collection `sloppyNamedCollection` is no longer present by
    executing:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, query the `neatNamedCollection` collection to verify that the data
    originally in `sloppyNamedCollection` is indeed present in it. Simply execute
    the following on the mongo shell:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works…
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Renaming a collection is pretty simple. It is accomplished with the `renameCollection`
    method, which takes two arguments. Generally, the function signature is as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first argument is the name to which the collection is to be renamed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'The second parameter that we didn''t use is a Boolean value that tells the
    command whether to drop the target collection if it exists. This value defaults
    to false, which means do not drop the target but give an error. This is a sensible
    default, otherwise the results would be ghastly if we accidently gave a collection
    name that exists and didn''t wish to drop it. However, if you know what you are
    doing and want the target to be dropped while renaming the collection, pass the
    second parameter as true. The name of this parameter is `dropTarget`. In our case,
    the call would have been:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As an exercise, try creating the `sloppyNamedCollection` again and rename it
    without the second parameter (or false as the value). You should see mongo complaining
    that the target namespace exists. Then, again rename with the second parameter
    as true, and now the renaming operation executes successfully.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，尝试再次创建`sloppyNamedCollection`并将其重命名为没有第二个参数（或false作为值）。您应该看到mongo抱怨目标命名空间已存在。然后，再次使用第二个参数重命名为true，现在重命名操作执行成功。
- en: 'Note that the rename operation will keep the original and the newly renamed
    collection in the same database. This `renameCollection` method is not enough
    to move/rename the collection across another database. In such cases, we need
    to run the `renameCollection` command that looks like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，重命名操作将保留原始的和新重命名的集合在同一个数据库中。这个`renameCollection`方法不足以将集合移动/重命名到另一个数据库。在这种情况下，我们需要运行类似于以下命令的`renameCollection`命令：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Suppose we want to rename the collection `sloppyNamedCollection` to `neatNamedCollection`
    as well as move it from `test` database to `newDatabase`, we can do so by executing
    the following command. Note the switch `dropTarget: true` used is meant to remove
    the existing target collection (`newDatabase.neatNamedCollection`) if it exists.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '假设我们想要将集合`sloppyNamedCollection`重命名为`neatNamedCollection`，并将其从`test`数据库移动到`newDatabase`，我们可以通过执行以下命令来执行此操作。请注意，使用的`dropTarget:
    true`开关旨在删除现有的目标集合（`newDatabase.neatNamedCollection`）（如果存在）。'
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Also, the rename collection operation doesn't work on sharded collections.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，重命名集合操作不适用于分片集合。
- en: Viewing collection stats
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看集合统计信息
- en: Perhaps one of the interesting statistics from an administrative purpose when
    it comes to the usage of storage, the number of documents in collection possibly
    to estimate the future space, and memory requirements based on the growth of the
    data is to get a high level statistics of the collection.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 也许在管理目的上，关于存储使用情况的一个有趣的统计数据是集合中文档的数量，可能可以根据数据的增长来估算未来的空间和内存需求，以获得集合的高级统计信息。
- en: Getting ready
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: To find the stats of the collection we need to have a server up and running
    and a single node is what should be okay. Refer to the *Installing single node
    MongoDB* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server* for information on how to start the server.
    The data on which we would be operating needs to be imported in the database.
    The steps to import the data are given in the recipe *Creating Test Data* in [Chapter
    2](ch02.html "Chapter 2. Command-line Operations and Indexes"), *Command-line
    Operations and Indexes*. Once these steps are completed, we are all set to go
    ahead with this recipe.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要查找集合的统计信息，我们需要运行一个服务器，并且一个单节点应该是可以的。有关如何启动服务器的信息，请参阅[第1章](ch01.html "第1章。安装和启动服务器")中的*安装单节点MongoDB*，*安装和启动服务器*。我们将要操作的数据需要导入到数据库中。导入数据的步骤在[第2章](ch02.html
    "第2章。命令行操作和索引")的*创建测试数据*中给出。完成这些步骤后，我们就可以继续进行本教程了。
- en: How to do it…
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: We would be using `postalCodes` collection for viewing the stats.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`postalCodes`集合来查看统计信息。
- en: 'Open the mongo shell and connect to the running MongoDB instance. In case you
    have started the mongo on default port, execute the following:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开mongo shell并连接到正在运行的MongoDB实例。如果您在默认端口上启动了mongo，请执行以下操作：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With the data imported, create an index on the `pincode` field if one doesn''t
    exist:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据后，如果`pincode`字段上不存在索引，则在该字段上创建一个索引：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'On the mongo terminal, execute the following:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在mongo终端上执行以下操作：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Observe the output and execute the following on the shell:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察输出并在shell上执行以下操作：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Again, observe the output.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次观察输出。
- en: We will now see what these values printed out mean to us in the following section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看这些打印出的值对我们意味着什么。
- en: How it works…
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: If we observe the output for both these commands, we see that the second one
    has all the figures in KB whereas the first one is in bytes. The parameter provided
    is known as scale and all the figures indicating size are divided by this scale.
    In this case, since we gave the value as `1024`, we get all the values in KB whereas
    if `1024 * 1024` is passed as the value of scale (the size shown will be in MB).
    For our analysis, we will use the one that shows the sizes in KB.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们观察这两个命令的输出，我们会发现第二个命令中的所有数字都是以KB为单位，而第一个命令中的数字是以字节为单位。提供的参数称为比例，所有指示大小的数字都会除以这个比例。在这种情况下，由于我们给出的值是`1024`，我们得到的所有值都是以KB为单位，而如果将`1024
    * 1024`作为比例的值（显示的大小将以MB为单位）。对于我们的分析，我们将使用以KB显示大小的值。
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following table shows the meaning of the important fields:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了重要字段的含义：
- en: '| Field | Description |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Field | Description |'
- en: '| --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `ns` | The fully qualified name of the collection with a format `<database>.<collection
    name>`. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `ns` | 以`<database>.<collection name>`格式的集合的完全限定名称。|'
- en: '| `count` | The number of documents in the collection. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `count` | 集合中的文档数量。|'
- en: '| `size` | The actual storage size occupied by the documents in the collection.
    Addition, deletion, and updates to documents in the collection can change this
    figure. The scale parameter affects this field''s value and in our case this value
    is in KB as `1024` is the scale. This number does include padding, if any. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `size` | 集合中文档占用的实际存储空间大小。对集合中文档的添加、删除和更新可能会改变此数字。比例参数会影响此字段的值，在我们的情况下，此值以KB为单位，因为`1024`是比例。此数字包括填充（如果有）。'
- en: '| `avgObjSize` | This is the average size of the document in the collection.
    It is simply the size field divided by the count of documents in the collection
    (the preceding two fields). The scale parameter affects this field''s value and
    in our case this value is in KB as `1024` is the scale. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `avgObjSize` | 这是集合中文档的平均大小。它只是大小字段除以集合中文档的计数（前两个字段）。比例参数会影响此字段的值，在我们的情况下，此值以KB为单位，因为`1024`是比例。|'
- en: '| `storageSize` | Mongo preallocates the space on the disk to ensure that the
    documents in the collection are kept on continuous locations to provide better
    performance in disk access. This preallocation fills up the files with zeros and
    then starts allocating space to these documents inserted. This field tells the
    size on the storage used by this collection. This figure will generally be much
    more than the actual size of the collection. The scale parameter affects this
    field''s value and in our case this value is in KB as `1024` is the scale. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `storageSize` | Mongo在磁盘上预先分配空间，以确保集合中的文档保持在连续的位置，以提供更好的磁盘访问性能。这种预分配会用零填充文件，然后开始为插入的文档分配空间。该字段告诉此集合使用的存储空间大小。这个数字通常会比集合的实际大小大得多。比例参数影响此字段的值，在我们的情况下，此值以KB为单位，因为比例为`1024`。
    |'
- en: '| `numExtents` | As we saw, mongo pre allocates continuous disk space to the
    collections for performance purpose. However as the collection grows, new space
    needs to be allocated. This field gives the number of such continuous chunk allocation.
    This continuous chunk is called an extent. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `numExtents` | 正如我们所看到的，Mongo为了性能目的而预先分配了连续的磁盘空间给集合。然而，随着集合的增长，需要分配新的空间。该字段给出了这种连续块分配的数量。这个连续的块称为一个区段。
    |'
- en: '| `nindexes` | This field gives the number of indexes present on the collection.
    This value would be `1` even if we do not create an index on the collection as
    mongo implicitly creates an index on the field `_id`. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `nindexes` | 该字段给出了集合上存在的索引的数量。即使我们没有在集合上创建索引，该值也将为`1`，因为Mongo会在字段`_id`上隐式创建一个索引。
    |'
- en: '| `lastExtentSize` | The size of the last extent allocated. The scale parameter
    affects this field''s value and in our case this value is in KB as `1024` is the
    scale. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `lastExtentSize` | 分配的最后一个区段的大小。比例参数影响此字段的值，在我们的情况下，此值以KB为单位，因为比例为`1024`。
    |'
- en: '| `paddingFactor` | This parameter has been deprecated since version 3.0.0
    and is hardcoded to `1` for backward compatibility reasons. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `paddingFactor` | 自3.0.0版本起，此参数已被弃用，并且由于向后兼容性原因已硬编码为`1`。 |'
- en: '| `totalIndexSize` | Indexes take up space to store too. This field gives the
    total size taken up by the indexes on the disk. The scale parameter affects this
    field''s value and in our case this value is in KB as `1024` is the scale. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `totalIndexSize` | 索引也占用存储空间。该字段给出了磁盘上索引占用的总大小。比例参数影响此字段的值，在我们的情况下，此值以KB为单位，因为比例为`1024`。
    |'
- en: '| `indexSizes` | This field is a document with the key as the name of the index
    and value as the size of the index in question. In our case, we had created an
    index explicitly on the `pincode` field; thus, we see the name of the index as
    the key and the size of the index on disk as the value. The total of these values
    of all the index is same as the value given previously, `totalIndexSize`. The
    scale parameter affects this field''s value and in our case this value is in KB
    as `1024` is the scale. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `indexSizes` | 该字段是一个文档，其键是索引的名称，值是所讨论的索引的大小。在我们的情况下，我们在`pincode`字段上显式创建了一个索引；因此，我们看到索引的名称作为键，磁盘上索引的大小作为值。所有这些索引的值的总和与先前给出的值`totalIndexSize`相同。比例参数影响此字段的值，在我们的情况下，此值以KB为单位，因为比例为`1024`。|'
- en: Documents are placed on the storage device in continuous locations. If a document
    is updated, resulting in an increase in size, Mongo will have to relocate this
    document. This operation turns out to be expensive affecting the performance of
    such update operations. Starting with Mongo 3.0.0, two data allocation strategies
    are used. One is *The power of 2*, where documents are allocated space in power
    of 2 (for example, 32, 64, 128, and so on). The other is *No Padding*, where collections
    do not expect document sizes to be altered.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 文档被放置在存储设备上的连续位置。如果文档被更新，导致大小增加，Mongo将不得不重新定位这个文档。这个操作会变得昂贵，影响这样的更新操作的性能。从Mongo
    3.0.0开始，使用了两种数据分配策略。一种是*2的幂*，其中文档以2的幂分配空间（例如，32、64、128等）。另一种是*无填充*，其中集合不希望文档大小被改变。
- en: See also
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: In this recipe, we discussed viewing stats of a collection. See the next recipe
    to view the stats at a database level.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们讨论了查看集合的统计信息。查看下一个配方以在数据库级别查看统计信息。
- en: Viewing database stats
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看数据库统计信息
- en: In the previous recipe, we saw how to view some important statistics of a collection
    from an administrative perspective. In this recipe, we get an even higher picture,
    getting those (or most of those) statistics at the database level.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个配方中，我们看到了如何从管理角度查看集合的一些重要统计信息。在这个配方中，我们得到了一个更高的视角，获得了数据库级别的这些（或大部分）统计信息。
- en: Getting ready
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To find the stats of the database, we need to have a server up and running and
    a single node is what should be okay. Refer to the recipe *Installing single node
    MongoDB* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server* for information on how to start the server.
    The data on which we would be operating needs to be imported in the database.
    The steps to import the data are given in the recipe *Creating Test Data* in [Chapter
    2](ch02.html "Chapter 2. Command-line Operations and Indexes"), *Command-line
    Operations and Indexes*. Once these steps are completed, we are all set to go
    ahead with this recipe. Refer to the previous recipe if you need to see how to
    view stats at the collection level.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要查找数据库的统计信息，我们需要运行一个服务器，一个单节点应该是可以的。有关如何启动服务器的信息，请参阅[第1章](ch01.html "第1章。安装和启动服务器")中的配方*安装单节点MongoDB*，*安装和启动服务器*。我们将要操作的数据需要导入到数据库中。有关如何导入数据的步骤，请参阅[第2章](ch02.html
    "第2章。命令行操作和索引")中的配方*创建测试数据*，*命令行操作和索引*。完成这些步骤后，我们就可以继续进行这个配方了。如果需要查看如何在集合级别查看统计信息，请参阅上一个配方。
- en: How to do it…
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: We will use the `test` database for the purpose of this recipe. It already has
    a `postalCodes` collection in it.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`test`数据库来完成此配方的目的。它已经在其中有一个`postalCodes`集合。
- en: Connect to the server using the mongo shell by typing in the following command
    from the operating system terminal. It is assumed that the server is listening
    to port `27017`.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用mongo shell连接到服务器，通过在操作系统终端中输入以下命令。假设服务器正在监听端口`27017`。
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'On the shell, execute the following command and observe the output:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在shell上，执行以下命令并观察输出：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'On the shell, again execute the following but this time around we add the scale
    parameter. Observe the output:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在shell上，再次执行以下命令，但这次我们添加了scale参数。观察输出：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works…
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'The `scale` parameter, which is a parameter to the `stats` function, divides
    the number of bytes with the given scale value. In this case, it is `1024` and
    hence all the values will be in KB. We analyze the following output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`scale`参数是`stats`函数的一个参数，它将字节数除以给定的scale值。在这种情况下，它是`1024`，因此所有值将以KB为单位。我们分析以下输出：'
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following table shows the meaning of the important fields:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了重要字段的含义：
- en: '| Field | Description |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 字段 | 描述 |'
- en: '| --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `db` | This is the name of the database whose stats are being viewed. |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `db` | 这是正在查看统计信息的数据库的名称。 |'
- en: '| `collections` | This is the total number of collections in the database.
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| `collections` | 这是数据库中集合的总数。 |'
- en: '| `objects` | This is the count of documents across all collections in the
    database. If we find the stats of a collection using `db.<collection>.stats()`,
    we get the count of documents in the collection. This attribute is the sum of
    counts of all the collections in the database. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| `objects` | 这是数据库中所有集合中文档的计数。如果我们使用`db.<collection>.stats()`查找集合的统计信息，我们会得到集合中文档的计数。这个属性是数据库中所有集合计数的总和。
    |'
- en: '| `avgObjectSize` | This is simply the size in bytes of all the objects in
    all the collections in the database divided by the count of the documents across
    all the collections. This value is not affected by the scale provided, although
    this is a `size` field. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| `avgObjectSize` | 这只是数据库中所有集合中所有对象的字节大小除以所有集合中文档的计数。这个值不受提供的scale影响，尽管这是一个`size`字段。
    |'
- en: '| `dataSize` | This is the total size of the data held across all the collections
    in the database. This value is affected by the scale provided. |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| `dataSize` | 这是数据库中所有集合中保存的数据的总大小。这个值受提供的scale影响。 |'
- en: '| `storageSize` | This is the total amount of storage allocated to collections
    in this database for storing documents. This value is affected by the scale provided.
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| `storageSize` | 这是为存储文档而分配给该数据库中集合的总存储量。这个值受提供的scale影响。 |'
- en: '| `numExtents` | This is the count of all the number of extents in the database
    across all the collections. This is basically the number of extents (logical containers)
    in the collection stats for collections in this database. |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| `numExtents` | 这是数据库中所有集合的extent数量的总数。这基本上是该数据库中集合统计信息中extent（逻辑容器）的数量。 |'
- en: '| `indexes` | This is the sum of number of indexes across all collections in
    the database |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| `indexes` | 这是数据库中所有集合的索引数量的总和。 |'
- en: '| `indexSize` | This is the size in bytes for all the indexes of all the collections
    in the database. This value is affected by the scale provided. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| `indexSize` | 这是数据库中所有集合的所有索引的字节大小。这个值受提供的scale影响。 |'
- en: '| `fileSize` | This is a sum of the size of all the database files you should
    find on the filesystem for this database. The files would be named `test.0`, `test.1`,
    and so on for test database. This value is affected by the scale provided. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| `fileSize` | 这是应该在文件系统中找到的该数据库的所有数据库文件的大小总和。文件的名称将是`test.0`，`test.1`等等。这个值受提供的scale影响。
    |'
- en: '| `nsSizeMB` | This is the size of the file in MB for the `.ns` file of the
    database. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| `nsSizeMB` | 这是数据库的`.ns`文件的大小（以MB为单位）。 |'
- en: '| `extentFreeList.num` | This is the number of free extends in freelist. You
    can look at extent as an internal data structure of MongoDB. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| `extentFreeList.num` | 这是空闲列表中空闲extent的数量。你可以将extent看作是MongoDB的内部数据结构。 |'
- en: '| `extentFreeList.totalSize` | Size of the extents on the freelist. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| `extentFreeList.totalSize` | 空闲列表上extent的大小。 |'
- en: For more information on these, you can refer to books such as *Instant MongoDB*
    by *Packt Publishing* ([http://www.packtpub.com/big-data-and-business-inteliigence/instant-mongodb-instant](http://www.packtpub.com/big-data-and-business-inteliigence/instant-mongodb-instant)).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多信息，你可以参考《Instant MongoDB》这样的书籍，由*Packt Publishing*出版（[http://www.packtpub.com/big-data-and-business-inteliigence/instant-mongodb-instant](http://www.packtpub.com/big-data-and-business-inteliigence/instant-mongodb-instant)）。
- en: How it works…
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Let's start by looking at the `collections` field. If you look carefully at
    the number and execute the show collections command on the mongo shell, you will
    find one extra collection in the stats as compared to those by executing the command.
    The difference is for one collection, which is hidden. Its name is `system.namespaces`
    collection. You may do a `db.system.namespaces.find()` to view its contents.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`collections`字段开始。如果你仔细观察数字，并在mongo shell上执行`show collections`命令，你会发现与执行命令时相比，统计信息中多了一个隐藏的集合。这个差异是因为有一个隐藏的集合，它的名称是`system.namespaces`。你可以执行`db.system.namespaces.find()`来查看它的内容。
- en: Getting back to the output of stats operation on the database, the objects field
    in the result has an interesting value too. If we find the count of documents
    in the `postalCodes` collection, we see it is `39732`. The count shown here is
    `39738`, which means there are six more documents. These six documents come from
    the `system.namespaces` and `system.indexes` collection. Executing a count query
    on these two collections will confirm it. Note that the `test` database doesn't
    contain any other collection apart from `postalCodes`. The figures would change
    if the database contains more collections with documents in it.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to note is the value of the `avgObjectSize` and there is something
    weird in this value. Unlike this very field in the collection's stats, which is
    affected by the value of the scale provided, in database stats this value is always
    in bytes. This is pretty confusing and I am not really sure why this is not scaled
    according to the provided scale.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Manually padding a document
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Without getting too much into the internals of the storage, MongoDB uses memory
    mapped files, which means that the data is stored in files exactly as how it would
    be in memory and it would use the low level OS services to map these pages to
    memory. The documents are stored in continuous locations in mongo data files and
    problem arises when the document grows and no longer fits in the space. In such
    scenarios, mongo rewrites the document towards the end of the collection with
    the updated data and clearing up the space where it was originally placed (note
    that this space is not released to OS as free space).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: This is not a big problem for applications that don't expect the documents to
    grow in size. However, this is a big performance hit for those who foresee this
    growth in the document size over a period of time and potentially a lot of such
    document movements. With the release of MongoDB 3.0, the *Power of 2* method became
    the default size allocation strategy. As the name suggests, this method stores
    documents in space allocated in powers of 2\. This provides additional padding
    to the documents as well as better reuse of free space caused by relocation or
    deletion of documents.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: That said, if you still wish to introduce manual padding in your strategy, read
    on.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nothing is needed for this recipe unless you plan to try out this simple technique,
    in which case you would need a single instance up and running. Refer to the recipe
    *Installing single node MongoDB* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* for information
    on how to start the server.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea of this technique is to add some dummy data to the document to be inserted.
    This dummy data's size in addition to other data in the document is approximately
    same as the anticipated size of the document.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the average size of the document is estimated to be around 1200
    bytes over a period of time and there is 300 bytes of data present in the document
    while inserting it, we will add a dummy field of size around 900 bytes so that
    the total document size sums up to 1200 bytes.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Once the document is inserted, we unset this dummy field, which leaves a hole
    in the file between the two consecutive documents. This empty space would then
    be used when the document grows over a period of time minimizing the document
    movements. The empty space may also be used by another document. The more foolproof
    way is to remove the padding only when you are using the space. However, any document
    growing beyond the anticipated average growth will have to be copied by the server
    to the end of the collection. Needless to say, documents not growing to the anticipated
    size will tend to waste disk space.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The applications can come up with some intelligent strategy to perhaps the adjust
    the size of the padding field based on say some particular field of the document
    to take care of these shortcomings but that is something up to the application
    developers.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可以提出一些智能策略，也许根据文档的某个特定字段调整填充字段的大小，以解决这些缺陷，但这取决于应用程序开发人员。
- en: 'Let''s now see a sample of this approach:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下这种方法的示例：
- en: 'We define a small function that will add a field called `padField` with an
    array of string values to the document. Its code is as follows:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义一个小函数，它将向文档添加一个名为`padField`的字段，并将字符串值的数组添加到文档中。其代码如下：
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: It will add an array called `padField` and add 20 times a string called `Dummy`.
    There is no restriction on what type you add to the document and how many times
    it is added as long as it consumes the space you desire. The preceding code is
    just a sample.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 它将添加一个名为`padField`的数组，并添加20次名为`Dummy`的字符串。对于您添加到文档中的类型和添加的次数没有限制，只要它占用您所需的空间。上述代码只是一个示例。
- en: 'The next step is to insert a document. We will define another function called
    `insert` to do that:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是插入一个文档。我们将定义另一个名为`insert`的函数来执行：
- en: '[PRE19]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We will now put this into action by inserting a document in the collection
    `testCol` as follows:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将通过在集合`testCol`中插入一个文档来将其付诸实践：
- en: '[PRE20]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You may query the `testCol` using the following query and check if the document
    inserted exists or not:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用以下查询查询`testCol`，并检查插入的文档是否存在：
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that on querying you would not find the `padField` in it. However, the
    space once occupied by the array stays between the subsequently inserted documents
    even if the field was unset.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在查询时，您将在其中找不到`padField`。但是，即使未设置该字段，数组占用的空间仍将保留在随后插入的文档之间。
- en: How it works…
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'The `insert` function is self-explanatory and has comments in it to tell what
    it does. An obvious question is how do we believe if this indeed what we intent
    to do. For this purpose, we shall do a small activity as follow. We will work
    on a `manualPadTest` collection for this purpose. From the mongo shell, execute
    the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`insert`函数是不言自明的，并且其中有注释来说明它的作用。一个明显的问题是，我们如何相信这确实是我们打算做的事情。为此，我们将进行一个小活动如下。我们将在`manualPadTest`集合上进行这个目的。从mongo
    shell执行以下操作：'
- en: '[PRE22]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Take a note of the `avgObjSize` field in the stats. Next, execute the following
    from the mongo shell:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计信息中注意`avgObjSize`字段。接下来，从mongo shell执行以下操作：
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Take a note of the `avgObjSize` field in the stats. This figure is much larger
    than the one we saw earlier with a regular insert without padding. The `paddingFactor`
    as we see in both cases still is one, but the latter case has more buffer for
    the document to grow.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计信息中注意`avgObjSize`字段。这个数字比我们之前看到的普通插入的数字要大得多。`paddingFactor`在这两种情况下仍然是1，但后一种情况为文档提供了更多的缓冲区。
- en: One catch in the `insert` function we used in this recipe is that the insert
    into the collection and the update document operations are not atomic.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中我们使用的`insert`函数中，插入到集合和更新文档操作不是原子的。
- en: The mongostat and mongotop utilities
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: mongostat和mongotop实用程序
- en: Most of you might find these names similar to two popular Unix commands, `iostat`
    and `top`. For MongoDB, `mongostat` and `mongotop` are two utilities which does
    pretty much the same job as these two Unix commands do and there is no prize for
    guessing that these are used to monitor the mongo instance.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人可能会发现这些名称与两个流行的Unix命令`iostat`和`top`相似。对于MongoDB，`mongostat`和`mongotop`是两个实用程序，它们的工作与这两个Unix命令几乎相同，毫无疑问，它们用于监视mongo实例。
- en: Getting ready
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we would be simulating some operations on a standalone mongo
    instance by running a script that would attempt to keep your server busy, and
    then in another terminal we will run these utilities to monitor the `db` instance.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将通过运行一个脚本来模拟独立mongo实例上的一些操作，该脚本将尝试使您的服务器保持繁忙，然后在另一个终端中，我们将运行这些实用程序来监视`db`实例。
- en: You need to start a standalone server listening to any port for client connections;
    in this case, we will stick to the default `27017`. If you are not aware how to
    start a standalone server, refer to *Installing single node MongoDB* in [Chapter
    1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing and
    Starting the Server*. We also need to download the script `KeepServerBusy.js`
    from Packt site and keep it handy for execution on local drive. Also, it is assumed
    that the `bin` directory of your mongo installation is present in the path variable
    of your operating system. If not, then these commands need to be executed with
    the absolute path of the executable from the shell. These two utilities `mongostat`
    and `mongotop` comes standard with the mongo installation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要启动一个独立的服务器来监听任何端口以进行客户端连接；在这种情况下，我们将坚持使用默认的`27017`端口。如果您不知道如何启动独立服务器，请参阅[第1章](ch01.html
    "第1章。安装和启动服务器")中的*安装单节点MongoDB*，*安装和启动服务器*。我们还需要从Packt网站下载脚本`KeepServerBusy.js`并将其放在本地驱动器上以备执行。还假定您的mongo安装的`bin`目录存在于操作系统的路径变量中。如果没有，那么这些命令需要在shell中使用可执行文件的绝对路径来执行。这两个实用程序`mongostat`和`mongotop`是与mongo安装一起提供的标准工具。
- en: How to do it…
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: Start the MongoDB server, and let it listen to the default port for connections.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动MongoDB服务器，并让它监听默认端口以进行连接。
- en: 'In a separate terminal, execute the provided JavaScript `KeepServerBusy.js`
    as follows:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个终端中，执行提供的JavaScript `KeepServerBusy.js`如下：
- en: '[PRE24]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Open a new OS terminal and execute the following command:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的操作系统终端并执行以下命令：
- en: '[PRE25]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Capture the output content for some time and then hit *Ctrl* + *C* to stop the
    command from capturing more stats. Keep the terminal open or copy the stats to
    another file.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 捕获一段时间的输出内容，然后按下*Ctrl* + *C*停止命令捕获更多的统计信息。保持终端打开或将统计信息复制到另一个文件中。
- en: 'Now, execute the following command from the terminal:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，从终端执行以下命令：
- en: '[PRE26]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Capture the output content for some time and then hit *Ctrl* + *C* to stop the
    command from capturing more stats. Keep the terminal open or copy the stats to
    another file.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hit *Ctrl* + *C* in the shell where the provided JavaScript `KeepServerBusy.js`
    was executed to stop the operation that keeps the server busy.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's see what we have captured from these two utilities.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by analyzing `mongostat`. On my laptop, the capture using `mongostat`
    looks like this:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You may choose to look at what the script `KeepServerBusy.js` is doing to keep
    the server busy. All it does is insert 1000 documents in collection `monitoringTest`,
    then update them one by one to set a new key in it, executes a find and iterates
    through all of them, and finally deletes them one by one and is basically a write
    intensive operation.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: The output does look ugly with content wrapping, but let's analyze the fields
    one by one and see what the fields to keep an eye on.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '| Column(s) | Description |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| `insert`, `query`, `update`, `delete` | The first four columns are the number
    of `insert`, `query`, `update` and `delete` operation per second. It is per second
    as the time frame these figures are captured are separated by one second, which
    is indicated by the last column. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| `getmore` | When the cursor runs out of data for the query, it executes a
    `getmore` operation on the server to get more results for the query executed earlier.
    This column shows the number of `getmore` operations executed in this given time
    frame of 1 second. In our case, there are not many `getmore` operations that are
    executed. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| `commands` | This is the number of commands executed on the server in the
    given time frame of 1 second. In our case, it wasn''t much and was only one. The
    number after a `&#124;` is `0` in our case, as this was in standalone mode. Try
    executing `mongostat` connecting to a replica set primary and secondary. You should
    see slightly different figures there. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| `flushes` | This is the number of times data was flushed to disk in the interval
    of 1 second. (`fsync` in case of `MMAPv1` storage engine, and checkpoints triggered
    between polling interval in case of `WiredTiger` storage engine) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| `mapped`, `virtual`, and `resident memory` | Mapped memory is the amount
    of memory mapped by the Mongo process to the database. This will typically be
    same as the size of the database. Virtual memory on other hand is the memory allocated
    to the entire `mongod` process. This will be more than twice the size of mapped
    memory especially when journaling is enabled. Finally, resident memory is the
    actual of physical memory used by mongo. All these figures are given in MB. The
    total amount of physical memory might be a lot more than what is being used by
    Mongo, but that is really not a concern unless a lot of page faults occur (which
    does happen in the previously mentioned output). |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| `faults` | These are the number of page faults occurring per second. These
    numbers should be as less as possible. It indicates the number of times mongo
    had to go to disk to obtain the document/index that was missing in the main memory.
    This problem is not as big a problem when using SSD for persistent storage as
    it is when using spinning disk drives. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| `locked` | Since version 2.2, all write operations to a collection lock the
    database in which the collection is and does not acquire a global level lock.
    This field shows the database that was locked for a majority of the time in the
    given time interval. In our case, the `test` database is locked for a majority
    of time. |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| `idx miss %` | This field gives the number of times a particular index was
    needed and was not present in memory. This causes a page fault and the disk needs
    to be accessed to get the index. Another disk access might be needed to get the
    document as well. This figure too should be low. A high percentage of index miss
    is something that would need attention. |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| `qr` &#124; `qw` | These are the queued up reads and writes that are waiting
    for getting a chance to be executed. If this number goes up, it shows that the
    database is getting overwhelmed by the volume of read and writes than it could
    handle. If the values are too high, keep an eye on page faults and database lock
    percents in order to get more insights on increased queue counts. If the data
    set is too large, sharding the collection can improve the performance significantly.
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| `qr` &#124; `qw` | 这些是等待执行的读取和写入的排队数。如果这个数字增加，表明数据库受到了读取和写入量的压倒。如果值太高，要密切关注页面错误和数据库锁定百分比，以便更深入地了解排队计数的增加。如果数据集太大，分片集合可以显著提高性能。
    |'
- en: '| `ar` &#124; `aw` | This is the number of active readers and writers (clients).
    Not something to worry of even for a large number as far as other stats we saw
    previously are under control. |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| `ar` &#124; `aw` | 这是活动读者和写者（客户端）的数量。只要其他我们之前看到的统计数据都在控制之下，即使数量很大，也不用担心。
    |'
- en: '| `netIn` and `netOut` | The network traffic in and out of the mongo server
    in the given time frame. Figure is measured in bits. For example, 271k means 271
    kilobits. |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| `netIn`和`netOut` | 在给定时间范围内，mongo服务器的网络流量进出。数字以位为单位。例如，271k表示271千位。 |'
- en: '| `conn` | This indicates the number of open connections. Something to keep
    a watch on to see if this doesn''t keep getting higher. |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| `conn` | 这表示打开的连接数。要密切关注，看看是否会不断增加。 |'
- en: '| `time` | This is the time interval when this sample was captured. |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| `time` | 这是捕获此样本时的时间间隔。 |'
- en: There are some more fields seen if `mongostat` is connected to a replica set
    primary or secondary. As an assignment, once the stats or a standalone instance
    are collected, start a replica set server and execute the same script to keep
    the server busy. Use `mongostat` to connect to a primary and secondary instance
    and see different stats captured.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`mongostat`连接到副本集的主服务器或从服务器，会看到一些更多的字段。作为一个任务，一旦收集到统计数据或独立实例，启动一个副本集服务器并执行相同的脚本以使服务器保持繁忙。使用`mongostat`连接到主服务器和从服务器实例，并查看不同的统计数据。
- en: 'Apart from `mongostat`, we also used the `mongotop` utility to capture the
    stats. Let''s see its output and make some sense out of it:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`mongostat`，我们还使用了`mongotop`实用程序来捕获统计数据。让我们看看它的输出并理解一些：
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: There is not much to see in this stat. We see the total time a database was
    busy reading or writing in the given slice of 1 second. The value given in the
    total would be sum of the read and the write time. If we actually compare the
    `mongotop` and `mongostat` for the same time slice, the percentage of time duration
    for which the write was taking place would be very close to the figure given in
    the percentage time that the database was locked in the `mongostat` output.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个统计数据中没有太多可看的。我们看到数据库在给定的1秒时间片段内忙于读取或写入的总时间。总时间中给定的值将是读取和写入时间的总和。如果我们实际上比较相同时间片段的`mongotop`和`mongostat`，那么写入正在进行的时间所占的百分比将非常接近`mongostat`输出中数据库被锁定的百分比。
- en: 'The command `mongotop` accepts a parameter on the command line as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`mongotop`命令接受命令行上的参数，如下所示：'
- en: '[PRE29]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In this case, the interval after which the stats will be printed out will be
    5 seconds as opposed to the default value of 1 second.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，打印统计数据的时间间隔将是5秒，而不是默认值1秒。
- en: Note
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Starting with MongoDB 3.0, both `mongotop` and `mongostat` utilities allow output
    in JSON format using `--json` option. This can be very useful if you were to use
    custom monitoring or metrics collection scripts, which would rely on these utilities.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 从MongoDB 3.0开始，`mongotop`和`mongostat`实用程序都允许使用`--json`选项以JSON格式输出。如果您要使用自定义监视或度量收集脚本，这可能非常有用，这些脚本将依赖于这些实用程序。
- en: See also
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: In the recipe *Getting current executing operations and killing them*, we will
    see how to get the current executing operations from the shell and kill them if
    needed
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*获取当前执行操作并终止它们*的示例中，我们将看到如何从shell获取当前执行的操作，并在需要时终止它们。
- en: In the recipe *Using profiler to profile operations*, we will see how to use
    the inbuilt profiling feature of Mongo to log operation execution times.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*使用分析器来分析操作*的示例中，我们将看到如何使用Mongo的内置分析功能来记录操作执行时间。
- en: Getting current executing operations and killing them
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取当前执行操作并终止它们
- en: In this recipe, we will see how to view the current running operations and kill
    some operations that are running for a long time.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将看到如何查看当前运行的操作，并终止一些长时间运行的操作。
- en: Getting ready
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will simulate some operations on a standalone mongo instance. We need to
    start a standalone server listening to any port for client connections; in this
    case, we will stick to the default `27017`. If you are not aware how to start
    a standalone server, refer to *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server*. We also need to start two shells connected to the server started. One
    shell would be used for background index creation and another would be used to
    monitor the current operation and then kill it.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在独立的mongo实例上模拟一些操作。我们需要启动一个独立服务器，以便监听任何端口以进行客户端连接；在这种情况下，我们将使用默认的`27017`端口。如果您不知道如何启动独立服务器，请参阅[第1章](ch01.html
    "第1章。安装和启动服务器")中的*安装单节点MongoDB*，*安装和启动服务器*。我们还需要启动两个连接到已启动服务器的shell。一个shell将用于后台索引创建，另一个将用于监视当前操作，然后终止它。
- en: How to do it…
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: We would not be able to simulate the actual long running operation in our test
    environment. We will try to create an index and hope it takes long to create.
    Depending on your target hardware configuration, the operation may take some time.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们无法在测试环境中模拟实际长时间运行的操作。我们将尝试创建一个索引，并希望它需要很长时间来创建。根据您的目标硬件配置，该操作可能需要一些时间。
- en: 'To start with this test, let''s execute the following on the mongo shell:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始这个测试，让我们在mongo shell上执行以下操作：
- en: '[PRE30]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The preceding insertion might take some time to insert 10 million documents.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的插入可能需要一些时间来插入1000万个文档。
- en: Once the documents are inserted, we will execute an operation that would create
    the index in background. If you would like to know more about index creation,
    refer to the recipe *Creating a background and foreground index in the shell*
    in [Chapter 2](ch02.html "Chapter 2. Command-line Operations and Indexes"), *Command-line
    Operations and Indexes*, but it is not a prerequisite for this recipe.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a background index on the field `i` in the document. This index creation
    operation is what we will be viewing from the `currentOp` operation and is what
    we will attempt to kill from using the kill operation. Execute the following in
    one shell to initiate the background index creation operation. This takes fairly
    long time and on my laptop it took well over 100 seconds.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In the second shell, execute the following command to get the current executing
    operations:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Take a note of the progress of the operations and find the one that is necessary
    for index creation. In our case, it was the only in progress on test machine.
    It will be an operation on `system.indexes` and the operation will be insert.
    The keys to lookout for in the output document are `ns` and `op`, respectively.
    We need to note the first field of this operation, `opid`. In this case, it is
    `11587458`. The sample output of the command is given in next section.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kill the operation from the shell using the following command, using the `opid`
    (operation ID) we got earlier:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How it works…
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will split our explanation into two sections, the first about the current
    operation details and second about killing the operation.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: In our case, index creation process is the long-running operation that we intend
    to kill. We create a big collection with about 10 million documents and initiate
    a background index creation process.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'On executing the `db.currentOp()` operation, we get a document as the result
    with a field `inprog` whose value is an array of other documents each representing
    a currently running operation. It is common to get a big list of documents on
    a busy system. Here is a document taken for the index creation operation:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We will see what these fields mean in the following table:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '| Field | Description |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| `opid` | This is a unique operation ID identifying the operation. This is
    the ID to be used to kill an operation. |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| `active` | The Boolean value indicating whether the operation has started
    or not, it is false only if it is waiting for acquiring the lock to execute the
    operation. The value will be true once it starts even if at a point of time where
    it has yielded the lock and is not executing. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| `secs_running` | Gives the time in seconds the operation is executing for.
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| `op` | This is the type of the operation. In the case of index creation,
    it is inserted into a system collection of indexes. Possible values are `insert`,
    `query`, `getmore`, `update`, `remove`, and `command`. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| `ns` | This is the fully qualified namespace for the target. It would be
    in the form `<database name>.<collection name>`. |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| `insert` | This is the document that would be inserted in the collection.
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| `query` | This is a field that would be present for other operations, other
    than `insert`, `getmore`, and `command`. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| `client` | The ip address/hostname and the port of the client who initiated
    the operation. |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| `desc` | This is the description of the client, mostly the client connection
    name. |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| `connectionId` | This is the identifier of the client connection from which
    the request originated. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| `locks` | This is a document containing the locks held for this operation.
    The document shows the type and mode of locks held for the operation being analyzed.
    The possible modes are as follows:**R** represents Shared (S) lock.**W** represents
    Exclusive (X) lock.**r** represents Intent Shared (IS) lock.**w** represents Intent
    Exclusive (IX) lock. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| `waitingForLock` | This field indicates if the operation is waiting for a
    lock to be acquired. For instance, if the preceding index creation was not a background
    process, other operations on this database would queue up for the lock to be acquired.
    This flag for those operations would then be true. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| `msg` | This is a human-readable message for the operation. In this case,
    we do see the percentage of operation complete as this is an index creation operation.
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| `progress` | The state of the operation, the total gives the total number
    of documents in the collection and done gives the number indexed so far. In this
    case, the collection already had some more documents over 10 million documents.
    The percentage completion is computed from these figures. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| `numYields` | This is the number of times the process has yielded the lock
    to allow other operations to execute. Since this is the background index creation
    process, this number will keep on increasing as the server yields it frequently
    to let other operations execute. Had it been a foreground process, the lock would
    never be yielded till the operation completes. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| `lockStats` | This document has more nested documents giving the stats for
    the total time this operation has held the read or write lock and also the time
    it waited to acquire the lock. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: Note
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In case you have a replica set, there would be more lot of getmore operations
    on the oplog on primary from secondary.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the system operations being executed too, we need to pass a true value
    as the parameter to the `currentOp` function call as follows:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we will see how to kill the user initiated operation using the `killOp`
    function. The operation is simply called as follows:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In our case, the index creation process had the process ID 11587458 and thus
    it will be killed as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'On killing any operation, irrespective of whether the given operation ID exists
    or not, we see the following message on the console:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Thus, seeing this message doesn't mean that the operation was killed. It just
    means that the operation if it exists will be attempted to be killed.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'If some operation cannot be killed immediately and if the `killOp` command
    is issued for it, the field `killPending` in the `currentOp` will start appearing
    for the given operation. For example, execute the following query on the shell:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This will not return and the thread executing the query will sleep for 100 seconds.
    This is an operation that cannot be killed using `killOp`. Try executing the command
    `currentOp` from another shell (do not press *Tab* for auto completion, your shell
    may just hang), get the operation ID, and then kill it using the `killOp`. You
    should see that the process still would be running if you execute the `currentOp`
    command, but the document for the process details will now contain a new key `killPending`
    stating that the kill for this operation is requested but pending.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Using profiler to profile operations
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will look at mongo's inbuilt profiler that would be used
    to profile the operations executed on the mongo server. It is a utility that is
    used to log all or slow operations that could be used for analysis of the performance
    of the server.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will perform some operations on a standalone mongo instance
    and profile them. We need to start a standalone server listening to any port for
    client connections; in this case, we will stick to the default `27017`. If you
    are not aware how to start a standalone server, refer to *Installing single node
    MongoDB* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server*. We also need to start a shell that would
    be used to perform querying, enabling profiling, and viewing the profiling operation.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the server is started and the shell is connected to it, execute the following
    to get the current profiling level:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The default level should be `0` (no profiling, if we have not set it earlier).
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s set the profiling level to `1` (log slow operations only) and log all
    the operations slower than `50` ms. Execute the following on the shell:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, let''s execute an insert operation into a collection, and then execute
    a couple of queries:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, execute the query on the following collection:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: How it works…
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Profiling is something that would not be enabled by default. If you are happy
    about the performance of the database, there is no reason one would enable the
    profiler. It is only when one feels there is some room for improvement and wants
    to target some expensive operations taking place. An important question is what
    classifies an operation to be slow? The answer is, it depends from application
    to application. In mongo, slow means any operation above 100 ms. However, while
    setting the profiling level, you may choose the threshold value.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three possible values for profiling levels:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '`0`: Disable profiling'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1`: Enable profiling for slow operations, where the threshold value for an
    operation to be classified as slow is provided with the call while setting the
    profiling level'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2`: Profile all operations'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While profiling all operations might not be a very good idea and might not be
    commonly used as we shall soon see, setting the value to `1` and a threshold provided
    to it is a good way to monitor slow operations.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the steps that we executed, we see that we can get the current
    profiling level by executing the operation `db.getProfilingLevel()`. To get more
    information, for example, what value is set as a threshold for the slow operations,
    we can use `db.getProfilingStatus()`. This returns a document with the profiling
    level and the threshold value for slow operations.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: For setting the profiling level, we call the `db.setProfilingLevel()` method.
    In our case, we set it for logging all operations taking more than `50` ms as
    `db.setProfilingLevel(1, 50)`.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: To disable profiling, simply execute `db.setProfilingLevel(0)`.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Next we executed three operations, one to insert a document, one to find all
    documents, and finally a find that calls sleep with a value of `70` ms to slow
    it down.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: The final step was to see these profiled operations that are logged in the `system.profile`
    collection. We execute a find to see the operations logged. For my execution,
    the insert and the final `find` operation with the sleep were logged.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, this profiling has some overhead but it is negligible. Hence, we
    would not enable it by default but only when we want to profile slow operations.
    Also, another question would be, *Will this profiling collection increase over
    a period of time?* The answer is *No*, as this is a capped collection. Capped
    collections are fixed size collections that preserve insertion orders and act
    as a circular queue filling in the new documents, discarding the oldest when it
    gets full. A query on `system.namespaces` should show the stats. The query execution
    would show the following for the `system.profile` collection:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'As we can see, the size of the collection is 1 MB, which is incredibly small.
    Setting the profiling level to `2` thus would easily overwrite the data on busy
    systems. One may also choose to explicitly create a collection with the name `system.profile`
    as a capped collection and of any size they prefer should they choose to retain
    more operations in it. To create a capped collection explicitly, you can execute
    the following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Obviously, the size chosen is arbitrary and you are free to allocate any size
    to this collection based on how frequently the data gets filled and how much of
    profiling data you want to keep before it gets overwritten.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: As this is a capped collection and insertion order is preserved, a query with
    the `sort order {$natural:-1}` would be perfectly fine and very efficient to find
    the operations in the reverse order of the execution time.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'We would finally take a look at the document that got inserted in the `system.profile`
    collection and see what all operations it has logged:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: As we can see in the document, there are indeed some interesting stats. Let's
    look at some of them in the following table. Some of these fields are identical
    to the fields we see when we execute the `db.currentOp()` operation from the shell
    and we then discussed in the previous recipe.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '| Field | Description |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| `op` | This is the operation that got executed; in this case, it was a find
    and thus it is query in this case. |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| `ns` | This is the fully qualified name of the collection on which the operation
    was performed. It would be of the format `<database>.<collection name>`. |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| `query` | It shows the query that got executed on the server. |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| `nscanned` | This has a similar meaning to explain plan. It is the total
    number of documents and index entries scanned. |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| `numYields` | This is the number of times the lock was yielded when the operation
    was executed. Higher yields could indicate that the query required a lot of disk
    access. This could be a good indication of re-looking at the index or optimizing
    the query itself. |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| `lockStats` | Some interesting stats for the time taken to acquire the lock
    and the time for which the lock was held. |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| `nreturned` | The number of documents returned. |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| `responseLength` | The length of the response in bytes. |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| `millis` | Most important of all, the time taken in milliseconds to execute
    the operation. This can be a good starting point to catch slow queries. |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| `ts` | This is the time when the operation was executed. |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| `client` | This is the hostname/IP address of the client who executed the
    operation. |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: Setting up users in Mongo
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Security is one of the cornerstones of any enterprise-level system. Not always
    would you find a system in a completely safe and secure environment to allow unauthenticated
    user access to it. Apart from test environments, almost every production environment
    requires proper access rights and perhaps audit of the system access too. Mongo
    security has multiple aspects:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Access rights for the end users accessing the system. There would be multiple
    roles such as admin, read-only users, and read and write non-administrative users.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authentication of the nodes that are added to the replica set. In a replica
    set, one should only be allowed to add authenticated systems. The integrity of
    the system would be compromised if any unauthenticated node is added to the replica
    set.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encryption of the data that is transmitted across the wire between the nodes
    of the replica sets or even the client and the server (or the mongos process in
    case of sharded setup).
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this and the next recipe, we would be looking at how to address the first
    and the second point given here. The third point of encrypting the data being
    transmitted on the wire is not supported by default by the community edition of
    mongo and would need a rebuild of mongo database with the `ssl` option enabled.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will set up users for a standalone mongo instance. We need
    to start a standalone server listening to any port for client connections; in
    this case, we will stick to the default `27017`. If you are not aware how to start
    a standalone server, refer to *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server*. We also need to start a shell that would be used for this admin operation.
    For a replica set, we will only be connected to a primary and perform these operations.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will add an admin user, a read-only user for a test database, and a read-write
    user for test database in this recipe.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'It is assumed that at this point:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: The server is up and running, and we are connected to it from the shell.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The server is started without any special command-line argument other than those
    mentioned in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server* for *Starting a single node instance using
    command-line options* recipe. We thus have full access to the server for any user.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: The first step we will do is to create an admin user. All the commands assume
    that you are using MongoDB 3.0 and above.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we start by creating the admin user in admin database as follows:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We will add the `read_user` and `write_user` to test database. To add the users,
    execute the following from the mongo shell:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now shut down the mongo server and the close the shell too. Restart the mongo
    server but with the `--auth` option on the command line:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: If your mongod instance is using `/etc/mongod.conf`, then add the line `auth
    = true` in the configuration file and restart the mongod service.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'Now connect to the server from the newly opened mongo shell and execute the
    following:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The collection `testAuth` need not exist, but you should see an error that we
    are not authorized to query the collection.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will now log in from the shell using a `read_user` as follows:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We will now execute the same `find` operation as follows. It should not give
    an error and it might not return any results depending on whether the collection
    exists or not:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Now, we will try to insert a document as follows. We should get an error that
    you are not authorized to insert data in this collection.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We will now log out and log in again, but with a write user as follows. Note
    the difference in the way we login this time around as against the previous instance.
    We are providing a document as the parameter to the `auth` function, where as
    in previous case we passed two parameters for the username and password:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, execute the following on the shell. You should get the unauthorized error:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We will now switch to `admin` database. We are currently connected to the server
    using a `write_user` that has read-write permissions on the `test` database. From
    the mongo shell, try to do the following:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Close the mongo shell or open a new shell as follows from the operating system''s
    console. This should take us directly to admin database:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now execute the following on the shell. It should show us the collections in
    the admin database:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Try and execute the following operation:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: How it works…
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We executed a lot of steps and now we will take a closer look at them.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, the server is started without `--auth` option and hence no security
    is enforced by default. We create an admin user with the `db.createUser` method.
    The signature of the method to create the user is `createUser(user, writeConcern)`.
    The first parameter is the user, which actually is a JSON document and second
    is the write concern to use for user creation. The JSON document for the user
    has the following format:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The roles provided here can be provided as follows, assuming that the current
    database when the user is created is test on the shell:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This gives the user being created read access to the reports `db` and `readWrite`
    access to the `test` database. Let''s see the complete user creation call of the
    `test` user:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The write concern, which is an optional parameter, can be provided as the JSON
    document. Some examples values are `{w:1}`, `{w:'majority'}`.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to the admin user creation, we created the user in step 2 using
    the `createUser` method and gave three inbuilt roles to this user in the `admin`
    database.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: In step 3, we created the `read` and `read-write` users in `test` database using
    the same `createUser` method.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: We shut down the MongoDB server after the `admin`, `read`, and `read-write`
    user creation and restarted it with the `--auth` option.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: On starting the server again, we will connect to it from the shell in step 8,
    but unauthenticated. Here, we try to execute a `find` query on a collection in
    test database, which fails as we are unauthenticated. This indicates that the
    server now requires appropriate credentials to execute operations on it. In step
    8 and 9, we log in using the `read_user` and first execute a `find` operation
    (which succeeds), and then an insert that doesn't as the user has read privileges
    only. The way to authenticate a user by invoking from the shell `db.auth(<user
    name>, <password>)` and `db.logout()`, which will logout the current logged in
    user.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: In steps 10 to 12, we demonstrate that we can perform `insert` operations using
    `write_user` but admin operations like `db.serverStatus()` cannot be executed.
    This is because these operations execute an `admin command` on the server, which
    a non-admin user and not permitted to invoke these. Similarly, when we change
    the database to admin, the `write_user`, which is from `test` database, is not
    permitted to perform any operations like getting a list of collections or any
    operation to query a collection in `admin` database.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: In Step 14, we log in to the shell using the `admin` user to the `admin` database.
    Previously, we logged in to database using the `auth` method; in this case, we
    used the `-u` and `-p` options for providing the username and the password. We
    also provided the name of the database to connect to, which is admin in this case.
    Here, we are able to view the collections on the admin database and also execute
    admin operations like getting the server status. Executing the `db.serverStatus`
    call is possible as the user is given the `clusterAdmin` role.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: One final thing to note, apart from writing to a collection, a user with write
    privileges can also create indexes on the collection in which he has write access.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we saw how we can create different users and what permissions
    they have restricting some set of operations. In the following recipe, we will
    see how we can have authentication done at process level. That is, how can one
    mongo instance authenticate itself for being added to a replica set.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MongoDB comes with a lot of built-in user roles with various privileges associated
    to each of them. Refer to the following URL to get details of various in built
    roles: [http://docs.mongodb.org/manual/reference/built-in-roles/](http://docs.mongodb.org/manual/reference/built-in-roles/).'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MongoDB also supports custom user roles. Refer to the following URL for knowing
    more about defining custom user roles: [http://docs.mongodb.org/manual/core/authorization/#user-defined-roles](http://docs.mongodb.org/manual/core/authorization/#user-defined-roles).'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interprocess security in Mongo
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we saw how authentication can be enforced for user to
    be logged in before allowing any operations on Mongo. In this recipe, we will
    look at interprocess security. By the term interprocess security, we don't mean
    to encrypt the communication but only to ensure that the node being added to a
    replica set is authenticated before being added to the replica set.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will start multiple mongo instances as part of a replica
    set and thus you might have to refer to the recipe *Starting multiple instances
    as part of a replica set* from [Chapter 1](ch01.html "Chapter 1. Installing and
    Starting the Server"), *Installing and Starting the Server* if you are not aware
    of how to start a replica set. Apart from that, in this recipe, all we would be
    looking at how to generate key file to be used and the behavior when an unauthenticated
    node is added to the replica set.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To set the ground, we would be starting three instances, each listening to port
    `27000`, `27001`, and `27002`, respectively. The first two would be started by
    providing it a path to the key file and the third wouldn't be. Later, we will
    try to add these three instances to the same replica set.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s generate key the key file first. There is nothing spectacular about
    generating the key file. This is as simple as having a file with 6 to 1024 characters
    from the `base64` character set. On Linux filesystem, you may choose to generate
    pseudo random bytes using `openssl` and encode them to `base64`. The following
    command will generate 500 random bytes and those bytes will then be `base64` encoded
    and written to the file `keyfile`:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'On a Unix filesystem, the key file should not have permissions for world and
    group. Thus, we should do the following after it is created:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Not giving write permission to the creator ensures that we don''t overwrite
    the contents accidently. On Windows platform, however, `openssl` doesn''t come
    out of the box and thus you have to download it, the archive extracted, and the
    `bin` folder added to the OS path variable. For Windows, we can download it from
    the following URL: [http://gnuwin32.sourceforge.net/packages/openssl.htm](http://gnuwin32.sourceforge.net/packages/openssl.htm).'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You may even choose not to generate the key file using the approach mentioned
    here (using `openssl`) and can take an easy way out by just typing in plain text
    in the key file from any text editor or your choice. However, note that the characters
    `\r`, `\n`, and spaces are stripped off by mongo and the remainder text is considered
    as the key. For example, we may create a file with the following content added
    to the key file. Again, the file will be named `keyfile` with the following content:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Using any approach mentioned here, we must not have a `keyfile` in place that
    would be used for next steps of the recipe.
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now secure the mongo processes by starting the mongo instance as follows.
    I will start the following on windows, and my key file ID is named `keyfile` and
    is placed on `c:\MongoDB`. The data path is `c:\MongoDB\data\c1, c:\MongoDB\data\c2`,
    and `c:\MongoDB\data\c3` for the three instances, respectively.
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Start the first instance listening to port `27000` as follows:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Similarly, start the second server listening to port `27001` as follows:'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The third instance would be started but without the `--auth` and the `--keyFile`
    option listening to port `27002` as follows:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We then start a mongo shell and connect it to port `27000`, which is the first
    instance started. From the mongo shell, we type:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'In few seconds, the replica set would be initiated with just one instance in
    it. We will now try to add two new instances to this replica set. First, add the
    one listening on port `27001` as follows (you will need to add the appropriate
    hostname, `Amol-PC` is the hostname in my case):'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: We will execute `rs.status()` command to see the status of our replica set.
    In the command's output, we should see our newly added instance.
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will now finally try and add an instance that was started without the `--auth`
    and the `--keyFile` option as follows:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This should add the instance to the replica set, but using `rs.status()` will
    show the status of the instance as UNKNOWN. The server logs for the instance running
    on `27002` too should show some authentication errors.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'We would finally have to restart this instance; however, this time we provide
    the `--auth` and the `--keyFile` option as follows:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Once the server is started, connect to it from the shell again and type in `rs.status()`
    in few moments, it should come up as a secondary instance.
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more…
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we saw interprocess security for preventing unauthenticated
    nodes from being added to the mongo replica set. We still haven't secured the
    transport by encrypting the data that is being sent over the wire. In the *Appendix*,
    we will show how to build the mongo server from the source and how to enable encryption
    of the contents over the wire.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Modifying collection behavior using the collMod command
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a command that would be executed to change the behavior of a collection
    in mongo. It could be thought of as a *collection modify* operation (officially,
    it is not mentioned anywhere though).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: For a part of this recipe, knowledge of TTL indexes is required.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will execute the `collMod` operation on a collection. We
    need to start a standalone server listening to any port for client connections;
    in this case, we will stick to the default `27017`. If you are not aware how to
    start a standalone server, refer to *Installing single node MongoDB* in [Chapter
    1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing and
    Starting the Server*. We also need to start a shell that would be used for this
    administration. It is highly recommended to take a look at the recipes *Expiring
    documents after a fixed interval using the TTL index* and *Expiring documents
    at a given time using the TTL index* in [Chapter 2](ch02.html "Chapter 2. Command-line
    Operations and Indexes"), *Command-line Operations and Indexes* if you are not
    aware of them.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This operation can be used to do a couple of things:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming we have a collection with TTL index, as we saw in [Chapter 2](ch02.html
    "Chapter 2. Command-line Operations and Indexes"), *Command-line Operations*,
    let us see the list indexes by executing the following:'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'To change the expiry to `800` ms from `300` ms, execute the following:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: How it works…
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `collMod` command always has the following format: `{collMod : <name of
    the collection>, <collmod operation>}`.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the index operation using `collMod` to modify the TTL index. If a TTL
    index is already created and the time to live needs to be changed after creation,
    we use the `collMod` command. This operation-specific field to the command is
    as follows:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The `keyPattern` is the field, of the collection, on which the TTL index is
    created and the `expireAfterSeconds` will contain the new time to be changed to.
    On successful execution, we should see the following in the shell:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Setting up MongoDB as a windows service
  id: totrans-421
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Windows services are long-running applications that run in background, similar
    to daemon threads. Databases are good candidates for such type of services, whereby
    they would start and stop when the host machines starts and stops (you may, however,
    choose to manually start/stop a service). Many database vendors provide a feature
    to start the database as a service when installed on the server. MongoDB lets
    you do that as well and this is what we will see in this recipe.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Installing single node MongoDB with options from the config
    file* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server* for getting information on how to start a
    MongoDB server using an external configuration file. Since mongo is run as a service
    in this case, it cannot be provided with command-like arguments and configuring
    it from configuration file is the only alternative. Refer to the prerequisites
    of the *Installing single node MongoDB* recipe in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server*, which is all
    we would need for this recipe.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first create a config file with three configuration values the `port`,
    `dbpath`, and the `logpath` file. We name the file `mongo.conf` and keep it at
    location `c:\conf\mongo.conf` with the following three entries in it (you may
    choose any path for config file location, database and logs):'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Execute the following from the windows terminal, which you may need to execute
    as an administrator. On Windows 7, the following steps were executed:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press the Windows key on your keyboard.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Search programs and files space, type `cmd`.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the programs, the command prompt program will be seen; right-click on it
    and select **Run as administrator**.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the shell, execute the following:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: The log printed out on the console should confirm that the service is installed
    properly.
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The service can be started as follows from the console:'
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The service can be stopped as follows:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Type in `services.msc` in the Run window (Windows button + *R*). In the management
    console, search for MongoDB service. We should see it as follows:![How to do it…](img/B04831_04_01.jpg)
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The service is automatic, that is, it will be started when the operating system
    starts. It can be changed to manual by right-clicking on it and selecting **Properties**.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To remove a service, we need to execute the following from the command prompt:'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'There are more options available that can be used to configure the name of
    the service, display name, description, and the user account used to run the service.
    These can be provided as command-line arguments. Execute the following to see
    the possible options and take a look at the **Windows Service Control Manager**
    options:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Replica set configurations
  id: totrans-445
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have had a good discussion on what replica set is in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* in the recipe *Starting multiple instances as part of a replica set*,
    and we saw how to start a simple replica set. In the recipe *Interprocess security
    in Mongo* in this chapter, we saw how to start a replica set with interprocess
    authentication. To be honest, that is pretty much what we do in setting up a standard
    replica set. There are a few configurations that one must know and should be aware
    of how it affects the replica set's behavior. Note that we still are not discussing
    tag aware replication in this recipe and it would be taken up later in this chapter
    as a separate recipe *Building tagged replica sets*.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting multiple instances as part of a replica set* in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for the prerequisites and know about the replica set
    basics. Go ahead and set up a simple three-node replica set on your computer as
    mentioned in the recipe.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Before we go ahead with the configurations, we will see what elections are in
    a replica set and how they work from a high level. This is good to know about
    elections because some of the configuration options affect the voting process
    in the elections.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Elections in a replica set
  id: totrans-450
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mongo replica set has a single primary instance and multiple secondary instances.
    All database writes happen only through the primary instance and are replicated
    to the secondary instances. Read operations can happen from secondary instances
    depending on the read preference. Refer to the *Understanding ReadPreference for
    querying* in the [Appendix](apa.html "Appendix A. Concepts for Reference") to
    know what read preference is. If, however, the primary goes down or is not reachable
    for some reason, the replica set becomes unavailable for writes. MongoDB replica
    set has a feature to automatically failover to a secondary, by promoting it to
    a primary and make the set available to clients for both read and write operations.
    The replica set remains unavailable for that brief moment till a new primary comes
    up.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: It all sounds good but the question is, who decides upon who the new primary
    instance would be? The process of choosing a new primary happens through an election.
    Whenever any secondary detects that it cannot reach out to a primary, it asks
    all replica set nodes in the instance to elect itself as the new primary.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: 'All other nodes in the replica set who receive this request for election of
    primary will perform certain checks before they vote a Yes to the secondary requesting
    an election:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: They would first check if the existing primary is reachable. This is necessary
    because the secondary requesting the re-election is not able to reach the primary
    possibly because of a network partition in which case it should not be allowed
    to become a primary. In such case the instance receiving the request will vote
    a No.
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Secondly, the instance would check the state of replication of itself with the
    secondary requesting the election. If it finds that the requesting secondary is
    behind itself in the replicated data, it would vote a No.
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the primary is not reachable, but some instance with priority higher
    than the secondary requesting the re-election, is reachable from it. This is possible
    if the secondary requesting the re-election can't reach out to the secondary with
    higher priority possibly due to a network partition. In this scenario the instance
    receiving the request for election would vote a No.
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding checks are pretty much what would be happening (not necessarily
    in the order mentioned previously) during the re-election; if these checks pass,
    the instance votes a Yes.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: The election is void even if a single instance votes No. However, if none of
    the instances have voted a No, then the secondary that requests the election would
    become a new primary if it receives a Yes from majority of instances. If the election
    becomes void, there would be a re-election with the same secondary or any other
    instance requesting an election with the aforementioned process till a new primary
    is elected.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an idea about the elections in replica set and the terminologies,
    let's look at some of the replica set configurations. Few of these options are
    related to votes and we start by looking at these options first.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: Basic configuration for a replica set
  id: totrans-460
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From the first chapter when we set up a replica set, we had a configuration
    similar to the following one. The basic replica set configuration for a three
    member set is as follows:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'We would not be repeating the entire configuration in all the steps in the
    following sections. All the flags we would be mentioning would be added to the
    document of a particular member in the members array. In the preceding example,
    if node with `_id` as `2` is to be made arbiter, we would be having the following
    configuration for it in the configuration document shown previously:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Generally, the steps to reconfigure an existing replica set are as follows:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: Assign the configuration document to a variable. If the replica set is already
    configured, it can be obtained using the `rs.conf()` call from the shell.
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'The members field in the document is an array of documents for each individual
    member of a replica set. To add a new property to a particular member, we do the
    following. For instance, if we want to add the `votes` key and set its value to
    `2` for the third member of the replica set (index 2 in the array), we would do
    the following:'
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Just changing the JSON document won''t change the replica set. We need to reconfigure
    it if the replica set is already in place, as follows:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'If the configuration is done for the first time, we would call the following:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: For all the steps given next, you need to follow the preceding steps to reconfigure
    or initiate the replica set unless some other steps are mentioned explicitly.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will look at some of the possible configurations that can
    be done in a replica set. The explanation will be minimal with all the explanation
    done in the next section, as usual.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: 'The first configuration is `arbiterOnly` option. It is used to configure a
    replica set member as a member that holds no data but only has rights to vote.
    The following key need to be added to the configuration of the member who would
    be made an arbiter:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'One thing to remember regarding this configuration is that once a replica set
    is initiated, no existing member can be changed to an arbiter from a non-arbiter
    node and vice versa. We can, however, add arbiter to an existing replica set using
    the helper function `rs.addArb(<hostname>:<port>)`. For example, add an arbiter
    listening to port `27004` to an existing replica set. The following was done on
    my machine to add an arbiter:'
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: When the server is started to listen to port `27004` and `rs.status()` is executed
    from the mongo shell, we should see that the `state` and the `strState` for this
    member is `7` and `ARBITER`, respectively.
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next option `votes` affects the number of votes a member has in the election.
    By default, all members have one vote each, this option can be used to change
    the number of votes a particular member has. It can be set as follows:'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Votes of existing members of a replica set can be changed and the replica set
    can be reconfigured using the `rs.reconfig()` helper.
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Though the option `votes` is available, which can potentially change the number
    of votes to form a majority, it usually doesn't add much value and not a recommended
    option to use in production.
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next replica set configuration option is called the `priority`. It determines
    the eligibility of a replica set member to become a primary (or not to become
    a primary). The option is set as follows:'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Higher number indicates more likely hood of becoming a primary, the primary
    would always be the one with the highest priority amongst the members alive in
    a replica set. Setting this option in an already configured replica set will trigger
    an election.
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting the priority to `0` will ensure that a member will never become primary.
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next option we would be looking at is `hidden`. Setting the value of this option
    to true ensures that the replica set member is hidden. The option is set as follows:'
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: One thing to keep in mind is that when the replica set member is hidden, its
    priority too should be made `0` to ensure it doesn't become primary. Though this
    seems redundant; as of the current version, the value or priority needs to be
    set explicitly.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When a programming language client connects to a replica set, it would not be
    able to discover hidden members. However, after using `rs.status()` from the shell,
    the member's status would be visible.
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s look at the `slaveDelay` option now. This option is used to set lag
    in time for the slave from the primary of the replica set. The option is set as
    follows:'
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Like the hidden member, slave delayed members too should have the priority set
    to `0` to ensure they don't ever become primary. This needs to be set explicitly.
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s look at the final configuration option: `buildIndexes`. This value if
    not specified by default, is true, which indicates if an index is created on the
    primary, it needs to be replicated on the secondary too. The option is set as
    follows:'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: When using this option with a value set to false, the priority is set to `0`
    to ensure they don't ever become primary. This needs to be set explicitly. Also,
    this option cannot be set after the replica set is initiated. Just like an arbiter
    node, this needs to be set when the replica set is being created or when a new
    member node is being added to the replica set.
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will explain and understand the significance of different
    types of members and the configuration options we saw in the previous section.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: Replica set member as an arbiter
  id: totrans-502
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The English meaning of the word *arbiter* is a judge who resolves a dispute.
    In the case of replica sets, the arbiter node is present just to vote in case
    of elections and not replicate any data. This is in fact, a pretty common scenario
    due to a fact that that a Mongo replica set needs to have at least three instances
    (and preferably odd number of instances, 3 or more). A lot of applications do
    not need to maintain three copies of data and are happy with just two instances,
    one primary and a secondary with the data.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: Consider the scenario where only two instances are present in the replica set.
    When the primary goes down, the secondary instance cannot form a proper majority
    because it only has 50 percent votes (its own vote) and thus cannot become a primary.
    If a majority of secondary instances goes down, then the primary instance steps
    down from primary and becomes a secondary, thus making the replica set unavailable
    for writes. Thus, a two-node replica set is useless as it doesn't stay available
    even when any of the instances goes down. It defeats the purpose of setting up
    a replica set and thus at least three instances are needed in a replica set.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: Arbiters come handy in such scenarios. We set up a replica set instance with
    three instances with only two having data and one acting as an arbiter. We need
    not maintain three copies of data at the same time we eliminate the problem we
    faced by setting up a two-instance replica set.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: Priority of replica set members
  id: totrans-506
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The priority flag can be used by itself or in conjunction with other options
    like `hidden`, `slaveDelay`, and `buildIndexes`, where we don't want the member
    with one of these three options to be ever made primary. We will look at these
    options soon.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: 'Some more possible use cases where we would never want a replica set to become
    a primary are as follows:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: When the hardware configuration of a member would not be able to deal with the
    write and read requests should it become a primary and the only reason it is being
    put in there is for replicating the data.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a multi data centers setup where one replica set instance is present
    in another data center for the sake of geographically distributing the data for
    DR purposes. Ideally, the network latency between the application server hosting
    the application and the database should be minimal for optimum performance. This
    could be achieved if both the servers (application server and the database server)
    are in the same data center. Not changing the priority of the replica set instance
    in another data center makes it equally eligible for being chosen as a primary
    and thus compromising on the application's performance if the server in other
    data center gets chosen as primary. In such scenarios, we can set the priority
    to be `0` for the server in the second data center and a manual cutover would
    be needed by the administrator to fail over to another data center should an emergency
    arise.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both scenarios mentioned here, we could also have the respective members
    hidden so that the application client doesn't have a view of these members in
    the first place.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: Similar to setting a priority to `0` for not allowing one to be primary, we
    can also be biased to one member to be primary whenever it is available by setting
    its priority to a value greater than 1, because the default value of priority
    is `1`.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a scenario for budget reasons we have one of the members storing
    data on SSDs and remaining on spinning disks. We would ideally want the member
    with SSDs to be the primary whenever it is up and running. It is only when it
    is not available we would want another member to become a primary, In such scenarios
    we can set the priority of the member running on SSD to a value greater than 1\.
    The value doesn't really matter as long as it is greater than the rest, that is,
    setting it to `1.5` or `2` makes no difference as long as priority of other members
    is less.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: Hidden, slave delayed, and build index configuration
  id: totrans-514
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The term hidden for a replica set node is from an application client that is
    connected to the replica set and not for an administrator. For an administrator,
    the hidden members are equally important to be monitored and thus their state
    is seen in the `rs.status()` response. Hidden members participate in elections
    too like all other members.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: For the `slaveDelay` option, most common use case is to ensure that the data
    in a member as a particular point of time lags behind the primary by the provided
    number of seconds and can be restored in case some unforeseen error has happened,
    say a human error for erroneously updating some data. Remember, longer the time
    delay, more is the time we get recover but at the cost of possibly stale data.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: The `buildIndexes` option is useful in cases where we have a replica set member
    with non-production standard hardware and the cost of maintaining the indexes
    are not worth it. You may choose to set this option for members where no queries
    are executed on it. Obviously, if you set this option it can never become a primary
    member, and thus the priority option is forced to be set to `0`.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  id: totrans-518
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can achieve some interesting things using tags in replica sets. This would
    be discussed in a later recipe, after we learn about tags in the recipe *Building
    tagged replica sets*.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: Stepping down as primary from the replica set
  id: totrans-520
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are times when for some maintenance activity during business hours we
    would like to take a server out from the replica set, perform the maintenance
    and put it back in the replica set. If the server to be worked upon is the primary,
    we somehow need to step down from the primary member position, perform re-election
    and ensure that it doesn't get re-elected for a minimum given time frame. After
    the server becomes secondary once the step down operation is successful, we can
    take it out of the replica set, perform the maintenance activity and put it back
    in the replica set.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-522
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting multiple instances as part of a replica set* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for the prerequisites and know about the replica set
    basics. Set up a simple three-node replica set on your computer, as mentioned
    in the recipe.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-524
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assuming at this point of time we have a replica set up and running, do the
    following:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following from the shell connected to one of the replica set members
    and see which instance currently is the primary:'
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Connect to that primary instance from the mongo shell and execute the following
    on the shell:'
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'The shell should reconnect again and you should see that the instance connected
    to and initially a primary instance now becomes secondary. Execute the following
    from the shell so that a new primary is now re-elected:'
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: You can now connect to the primary, modify the replica set configuration and
    go ahead with the administration on the servers.
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-533
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The preceding steps mentioned are pretty simple, but there are a couple of interesting
    things that we will see.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: The method we saw previously, `rs.stepDown()` did not have any parameters. The
    function can in fact take a numeric value, which is the number of seconds for
    which the instance stepped down won't participate in the elections and won't become
    a primary and the default value for this is `60` seconds.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting thing to try out is what if the instance that was asked
    to step down has a higher priority than other instances. Well, it turns out that
    the priority doesn't matter when you step down. The instance stepped down will
    not become primary no matter what for the provided number of seconds. However,
    if priority is set for the instance stepped down and it is higher than others,
    then after the time given to `stepDown` elapses an election will happen and the
    instance with higher priority will become primary again.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the local database of a replica set
  id: totrans-537
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore the local database from a replica set's perspective.
    The local database may contain collections that are not specific to replica sets,
    but we will focus only on the replica set specific collections and try to take
    a look at what's in them and what they mean.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-539
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting multiple instances as part of a replica set* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for the prerequisites and know about the replica set
    basics. Go ahead and set up a simple three-node replica set on your computer,
    as mentioned in the recipe.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-541
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the replica set up and running, we need to open a shell connected to the
    primary. You may connect randomly to any one member; use `rs.status()` and then
    determine the primary.
  id: totrans-542
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With shell open, first switch to `local` database and the view the collections
    in the `local` database as follows:'
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'You should find a collection called `me`. Querying this collection should show
    us one document and it contains the hostname of the server to which we are currently
    connected to:'
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: There would be two fields, the hostname and the `_id` field. Take a note of
    the `_id` field—it is important.
  id: totrans-547
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Take a look at the `replset.minvalid` collection. You will have to connect
    to a secondary member from the shell to execute the following query. Switch to
    the `local` database first:'
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: This collection just contains the single document with a key `ts` and a value
    that is the timestamp till the time the secondary we are connected to is synchronized.
    Note down this time.
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From the shell in primary, insert a document in any collection. We will use
    the database as test. Execute the following from the shell of the primary member:'
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Query the secondary again, as follows:'
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: We should see that the time against the field `ts` has now incremented corresponding
    to the time this replication happened from primary to secondary. With a slave
    delayed node, you will see this time getting updated only after the delay period
    has elapsed.
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we will see the collection `system.replset`. This collection is the
    place where the replica set configuration is stored. Execute the following:'
  id: totrans-556
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Actually, when we execute `rs.conf()`, the following query gets executed:'
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  id: totrans-559
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: How it works…
  id: totrans-560
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The database local is a special (non-replicated) database that is used to hold
    the replication and instance specific details in it. Try creating a collection
    of your own in the local database and insert some data in it; it would not be
    replicated to the secondary nodes.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: This database gives us some view of the data stored by mongo for internal use.
    However, as an administrator, it is good to know about these collections and the
    type of data in it.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: Most the collections are pretty straightforward. From the shell of the secondary
    execute the query `db.me.findOne()` in the local database and we should see that
    `_id` there should match the `_id` field of the document present in the slaves
    collection.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: The config document we see gives the hostname of the secondary instance that
    we are referring to. Note that the port and other configuration options of the
    replica set member are not present in this document. Finally, the `syncedTo` time
    tells us till what time the secondary instances are synced up with the primary.
    We saw the collection `replset.minvalid` on the secondary, which tells us the
    time till which it is synced with primary. This value in the `syncedTo` time on
    primary would be same as in `replset.minvalid` on respective secondary.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  id: totrans-565
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have not seen the oplog, which is interesting to look at. We would take a
    look at this special collection in a separate recipe, *Understanding and analyzing
    oplogs*.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and analyzing oplogs
  id: totrans-567
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Oplog is a special collection and forms the backbone of the MongoDB replication.
    When any write operation or configuration changes are done on the replica set's
    primary, they are written to the oplog on the primary. All the secondary members
    then tail this collection to get the changes to be replicated. Tailing is synonymous
    to tail command in Unix and can only be done on a special type of collection called
    capped collection. Capped collections are fixed size collections which maintain
    the insertion order just like a queue. When the collection's allocated space becomes
    full, the oldest data is overwritten. If you are not aware of capped collections
    and what tailable cursors are, please refer to *Creating and tailing a capped
    collection cursors in MongoDB* in [Chapter 5](ch05.html "Chapter 5. Advanced Operations"),
    *Advanced Operations* for more details.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: Oplog is a capped collection present in the non-replicated database called **local**.
    In our previous recipe, we saw what a `local` database is and what collections
    are present in it. Oplog is something we didn't discuss in last recipe, as it
    demands a lot more explanation and a dedicated recipe is needed to do justice.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-570
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting multiple instances as part of a replica set* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for the prerequisites and know about the replica set
    basics. Go ahead and set up a simple three-node replica set on your computer as
    mentioned in the recipe. Open a shell and connect to the primary member of the
    replica set. You will need to start the mongo shell and connect to the primary
    instance.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-572
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Execute the following steps after connecting to a primary from the shell to
    get the timestamp of the last operation present in the oplog. We would be interested
    in looking at the operations after this time.
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  id: totrans-574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Execute the following from the shell. Keep the output in the shell or copy
    it to some place. We will analyze it later:'
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  id: totrans-576
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Insert 10 documents as follows:'
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  id: totrans-578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Execute the following update to set a string value for all documents with value
    of `i` greater than `5`, which is 6, 7, 8 and 9 in our case. It would be a multiupdate
    operation:'
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  id: totrans-580
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Now, create the index as follows:'
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Execute the following query on oplog:'
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  id: totrans-584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: How it works…
  id: totrans-585
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For those aware of messaging and its terminologies, Oplog can be looked at as
    a topic in messaging world with one producer, the primary instance, and multiple
    consumers, the secondary instances. Primary instance writes to an oplog all the
    contents that need to be replicated. Thus, any create, update, and delete operations
    as well as any reconfigurations on the replica sets would be written to the oplog
    and the secondary instances would tail (continuously read the contents of the
    oplog being added to it, similar to a tail with `-f` option command in Unix) the
    collection to get documents written by the primary. If the secondary has a `slaveDelay`
    configured, it will not read documents more than the maximum time minus the `slaveDelay`
    time from the oplog.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: We started by saving an instance of the local database in the variable called
    `local` and identified a cutoff time that we would use for querying all the operations
    we will perform in this recipe from the oplog.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: Executing a query on the `system.namespaces` collection in the local database
    shows us that the collection is a capped collection with a fixed size. For performance
    reasons capped collections are allocated continuous space on the filesystem and
    are preallocated. The size allocated by the server is dependent on the OS and
    CPU architecture. While starting the server the option `oplogSize` can be provided
    to mention the size of the oplog. The defaults are generally good enough for most
    cases. However, for development purpose, you can choose to override this value
    for a smaller value. Oplogs are capped collections that need to be preallocated
    a space on disk. This preallocation not only takes time when the replica set is
    first initialized but takes up a fixed amount of disk space. For development purpose,
    we generally start multiple MongoDB processes as part of the same replica set
    on same machine and would want them to be up and running as quickly as possible
    with minimum resource usage. Also, having the entire oplog in memory becomes possible
    if the oplog size is small. For all these reasons, it is advisable to start the
    local instances for development purpose with a small oplog size.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: 'We performed some operations such as insert 10 documents and update four documents
    using a multi update and create an index. If we query the oplog for entries after
    the cutoff, we computed earlier we see 10 documents for each insert in it. The
    document looks something like this:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  id: totrans-590
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'As we can see, we first look at the three fields: `op`, `ns`, and `o`. These
    stand for the operation, the fully qualified name of the collection into which
    the data is being inserted, and the actual object to be inserted. The operation
    `i` stand for insert operation. Note that the value of `o`, which is the document
    to be inserted, contains the `_id` field that got generated on the primary. We
    should see 10 such documents, one for each insert. What is interesting to see
    is what happens on a multi update operation. The primary puts four documents,
    one for each of them affected for the updates. In this case, the value `op` is
    `u`, which is for update and the query used to match the document is not the same
    as what we gave in the update function, but it is a query that uniquely finds
    a document based on the `_id` field. Since there is an index already in place
    for the `_id` field (created automatically for each collection), this operation
    to find the document to be updated is not expensive. The value of the field `o`
    is the same document we passed to the update function from the shell. The sample
    document in the oplog for the update is as follows:'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  id: totrans-592
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: The update in the oplog is the same as the one we provided. This is because
    the `$set` operation is idempotent, which means you may apply an operation safely
    any number of times.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: 'However, update using `$inc` operator is not idempotent. Let''s execute the
    following update:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  id: totrans-595
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: In this case, the oplog would have the following as the value of `o`.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  id: totrans-597
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: This non-idempotent operation is put into oplog by Mongo smartly as an idempotent
    operation with the value of i set to a value that is expected to be after the
    increment operation once. Thus it is safe to replay an oplog any number of times
    without corrupting the data.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can see that the index creation process is put in the oplog as an
    insert operation in the `system.indexes` collection. For large collections, index
    creation can take hours and thus the size of the oplog is very important to let
    the secondary catch up from where it hasn't replicated since the index creation
    started. However, since version 2.6, index creation initiated in background on
    primary will also be built in background on secondary instances.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details on the index creation on replica sets, visit the following
    URL: [http://docs.mongodb.org/master/tutorial/build-indexes-on-replica-sets/](http://docs.mongodb.org/master/tutorial/build-indexes-on-replica-sets/).'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: Building tagged replica sets
  id: totrans-601
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*, we saw how to set up a simple replica in *Starting multiple
    instances as part of a replica set* and saw what is the purpose of a replica set.
    We also have a good deal of explanation on what `WriteConcern` is in the [Appendix](apa.html
    "Appendix A. Concepts for Reference") of the book and why it is used. What we
    saw about write concerns is that it offers a minimum level guarantee for a certain
    write operation. However, with the concept of tags and write concerns, we can
    define a variety of rules and conditions which must be satisfied before a write
    operation is deemed successful and a response is sent to the user.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider some common use cases such as the following:'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: Application wants the write operation to be propagated to at least one server
    in each of its data center. This ensures that in event of a data center shutdown,
    other data centers will have the data that was written by the application.
  id: totrans-604
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there are no multiple data centers, at least one member of a replica set
    is kept on different rack. For instance, if the rack's power supply goes down,
    the replica set will still be available (not necessarily for writes) as at least
    one member is running on a different rack. In such scenarios, we would want the
    write to be propagated to at least two racks before responding back to the client
    with a successful write.
  id: totrans-605
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is possible that a reporting application queries a group of secondary of
    a replica set for generating some reports regularly. (Such secondary might be
    configured to never become a primary). After each write, we want to ensure that
    the write operation is replicated to at least one reporting replica member before
    acknowledging the write as successful.
  id: totrans-606
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding use cases are a few of the common use cases that arise and are
    not addressed using simple write concerns that we have seen earlier. We need a
    different mechanism to cater to these requirements and replica sets with tags
    is what we need.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, the next question is what exactly are tags? Let''s take an example
    of a blog. Various posts in the blog have different tags attached to them. These
    tags allow us to easily search, group, and relate posts together. Tags are some
    user defined text with some meaning attached to it. If we draw an analogy between
    the blog post and the replica set members, similar to how we attach tags to a
    post, we can attach tags to each replica set member. For example, in a multiple
    data center scenario with two replica set members in data center 1 (`dc1`) and
    one member in data center 2 (`dc2`), we can have the following tags assigned to
    the members. The name of the key and the value assigned to the tag is arbitrary
    and is chosen during design of the application; you may choose to even assign
    any tags like the administrator who set up the server if you really find it useful
    to address your use case:'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: '| Replica Set Member | Tag |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
- en: '| Replica set member 1 | `{''datacentre'': ''dc1'', ''rack'': ''rack-dc1-1''}`
    |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
- en: '| Replica set member 2 | `{''datacentre'': ''dc1'', ''rack'': ''rack-dc1-2''}`
    |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
- en: '| Replica set member 3 | `{''datacentre'': ''dc2'', ''rack'': ''rack-dc2-2''}`
    |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
- en: That is good enough to lay the foundation of what a replica set tags are. In
    this recipe, we will see how to assign tags to replica set members and more importantly,
    how to make use of them to address some of the sample use cases we saw earlier.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-615
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting multiple instances as part of a replica set from
    Chapter 1, Installing and Starting the Server* for the prerequisites and know
    about the replica set basics. Go ahead and set up a simple three-node replica
    set on your computer, as mentioned in the recipe. Open a shell and connect to
    the primary member of the replica set.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: If you need to know about write concerns, refer to the overview of write concerns
    in the [Appendix](apa.html "Appendix A. Concepts for Reference") of the book.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: For inserting documents in the database, we will use Python as it gives us an
    interactive interface like the mongo shell. Refer to the recipe *Connecting to
    a single node using a Python client* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* for steps on how
    to install pymongo. Mongo shell would have been the most ideal candidate for the
    demonstration of the insert operations, but there are certain limitations around
    the usage of the shell with our custom write concern. Technically, any programming
    language with the write concerns mentioned in the recipe for insert operations
    would work fine.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-619
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the replica set started, we will add tags to it and reconfigure it as
    follows. The following commands are executed from the mongo shell:'
  id: totrans-620
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  id: totrans-621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'With the replica set tags set (not that we have not yet reconfigured the replica
    set), we need to define some custom write concerns. First, we define one that
    will ensure that the data gets replicated to at least to one server in each data
    center. Execute the following in the mongo shell again:'
  id: totrans-622
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Start the python shell and execute the following:'
  id: totrans-624
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  id: totrans-625
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'We will now execute the following insert:'
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: The preceding insert goes through successfully and the `ObjectId` would be printed
    out; you may query the collection to confirm from either the mongo shell or Python
    shell.
  id: totrans-628
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since our primary is one of the servers in data centre `1`, we will now stop
    the server listening to port `27002`, which is the one with priority `0` and tagged
    to be in a different data center.
  id: totrans-629
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the server is stopped (you may confirm using the `rs.status()` helper
    function from the mongo shell), execute the following insert again, this insert
    should error out:'
  id: totrans-630
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  id: totrans-631
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Restart the stopped mongo server.
  id: totrans-632
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Similarly, we can achieve rack awareness by ensuring that the write propagates
    to at least two racks (in any data centre) by defining a new configuration as
    follows from the mongo shell:'
  id: totrans-633
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  id: totrans-634
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'The settings value of the conf object would then be as follows. Once set, reconfigure
    the replica set again using `rs.reconfig(conf)` from the mongo shell:'
  id: totrans-635
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  id: totrans-636
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: We saw `WriteConcern` used with replica set tags to achieve some functionality
    like data center and rack awareness. Let's see how we can use replica set tags
    with read operations.
  id: totrans-637
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will see how to make use of replica set tags with read preference. Let's
    reconfigure the set by adding one more tag to mark a secondary member that will
    be used to execute some hourly stats reporting.
  id: totrans-638
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Execute the following steps to reconfigure the set from the mongo shell:'
  id: totrans-639
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  id: totrans-640
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: This will configure the same member with priority `0` and the one in a different
    data center with an additional tag called type with a value reports.
  id: totrans-641
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We now go back to the python shell and perform the following steps:'
  id: totrans-642
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  id: totrans-643
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: The preceding execution should show us one document from the collection (as
    we has inserted data in this test collection in previous steps).
  id: totrans-644
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stop the instance which we have tagged for reporting, that is, the server listening
    to connections on port `27002` and execute the following on the python shell again:'
  id: totrans-645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  id: totrans-646
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: This time around, the execution should fail and state that no secondary found
    with the required tag sets.
  id: totrans-647
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-648
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we did a lot of operations on tagged replica sets and saw how
    it can affect the write operations using `WriteConcern` and read operations using
    `ReadPreference`. Let's look at them in some details now.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: WriteConcern in tagged replica sets
  id: totrans-650
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We set up a replica set that was up and running, which we reconfigured to add
    tags. We tagged the first two servers in datacenter 1 and in different racks (servers
    running listening to port `27000` and `27001` for client connections) and the
    third one in datacenter 2 (server listening to port `27002` for client connections).
    We also ensured that the member in datacenter 2 doesn't become a primary by setting
    its priority to `0`.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first objective is to ensure that the write operations to the replica set
    gets replicated to at least one member in the two datacenters. To ensure this,
    we define a write concern as follows `{''MultiDC'':{datacentre : 2}}`. Here, we
    first define the name of the write concern as MultiDC. The value which is a JSON
    object has one key with name datacenter, which is same as the key used for the
    tag we attached to the replica set and the value is a number `2`, which will be
    looked as the number of distinct values of the given tag that should acknowledge
    the write before it is deemed successful.'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in our case, when the write comes to server 1 in datacenter 1,
    the number of distinct values of the tag datacenter is 1\. If the write operation
    gets replicated to the second server, the number still stays one as the value
    of the tag datacenter is same as the first member. It is only when the third server
    acknowledges the write operation, the write satisfies the defined condition of
    replicating the write to distinct two values of the tag *datacenter* in the replica
    set. Note that the value can only be a number and not have something like `{datacentre
    : ''dc1''}` this definition is invalid and an error will be thrown while re-configuring
    the replica set.'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: But we need to register this write concern somewhere with the server. This is
    done in the final step of the configuration by setting the settings value in configuration
    JSON. The value to set is `getLastErrorModes`. The value of `getLastErrorModes`
    is a JSON document with all possible write concerns defined in it. We later defined
    one more write concern for write propagated to at least two racks. This is conceptually
    in line with MultiDC write concern and thus we will not be discussing it in details
    here. After setting all the required tags and the settings, we reconfigure the
    replica set for the changes to take effect.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: Once reconfigured, we perform some write operations using the MultiDC write
    concern. When two members in two distinct datacenters are available, the write
    goes through successfully. However, when the server in second datacenter goes
    down, the write operation times out and throws an exception to the client initiating
    the write. This demonstrates that the write operation will succeed or fail as
    per how we intended.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: We just saw how these custom tags can be used to address some interesting use
    cases, which are not supported by the product implicitly as far as write operations
    are concerned. Similar to write operations, read operations can take full advantages
    of these tags to address some use cases such as reading from a fixed set of secondary
    members that are tagged with a particular value.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: ReadPreference in tagged replica sets
  id: totrans-657
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We added another custom tag annotating a member to be used for reporting purposes,
    we then fire a query operation with the read preference to query a secondary and
    provide the tag sets that should be looked for before considering the member as
    a candidate for read operation. Remember that when using primary as the read preference,
    we cannot use tags and that is reason we explicitly specified the value of the
    `read_preference` to `SECONDARY`.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the default shard for non-sharded collections
  id: totrans-659
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the recipe *Starting a simple sharded environment of two shards* in [Chapter
    1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing and
    Starting the Server* we set up a simple two-shard server. In the recipe *Connecting
    to a shard in the shell and performing operations* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* we added data
    to a person collection that was sharded. However, for any collection that is not
    sharded, all the documents end up on one shard called the primary shard. This
    situation is acceptable for small databases with relatively small number of collections.
    However, if the database size increases and at the same time the number of un-sharded
    collections increase, we end up overloading a particular shard (which is the primary
    shard for a database) with a lot of data from these un-sharded collections. All
    query operations for such un-sharded collections as well as those on the collections
    whose particular range in the shard reside on this server instance will be directed
    to this it. In such scenario, we can have the primary shard of a database changed
    to some other instance so that these un-sharded collections get balanced out across
    different instances.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to view this primary shard and change it to
    some other server whenever needed.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-662
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Following the recipe *Starting a simple sharded environment of two shards* in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* set up and start a sharded environment. From the shell,
    connect to the started mongos process. Also, assuming that the two shards servers
    are listening to port `27000` and `27001`, connect from the shell to these two
    processes. So, we have a total of three shells opened, one connected to the mongos
    process and two to these individual shards.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: 'We need are using the `test` database for this recipe and sharding has to be
    enabled on it. If it not, then you need to execute the following on the shell
    connected to the mongos process:'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  id: totrans-665
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: How to do it…
  id: totrans-666
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From the shell connected to the mongos process, execute the following two commands:'
  id: totrans-667
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE127]'
  id: totrans-668
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'In the databases, look out for `test` database and take a note of the `primary`.
    Suppose the following is a part (showing the part under databases only) of the
    output of `sh.status()`:'
  id: totrans-669
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  id: totrans-670
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: The second document under the databases shows us that the database `test` is
    enabled for sharding (because partitioned is true) and the primary shard is `shard0000`.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary shard, which is `shard0000` in our case, is the mongod process
    listening to port `27000`. Open the shell connected to this process and execute
    the following in it:'
  id: totrans-672
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  id: totrans-673
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'Now, connect to another mongod process listening to port `27001` and again
    execute the following query:'
  id: totrans-674
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  id: totrans-675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: Note that the data would be found only on the primary shard and not on other
    shard.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following command from the mongos shell:'
  id: totrans-677
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  id: totrans-678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'Execute the following command from mongo shell connected to the mongos process:'
  id: totrans-679
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  id: totrans-680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'From the shell connected to the mongos processes running on port `27000` and
    `27001`, execute the following query:'
  id: totrans-681
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  id: totrans-682
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: How it works…
  id: totrans-683
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We started a sharded setup and connected to it from the mongos process. We started
    by inserting a document in the `testCol` collection that is not enabled for sharding
    in the test database, which is not enabled for sharding as well. In such cases,
    the data lies on shard called the **primary shard**. Do not misunderstand this
    for the primary of a replica set. This is a shard (that itself can be a replica
    set) and it is the shard chosen by default for all database and collection for
    which sharding is not enabled.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
- en: 'When we add the data to a non-sharded collection, it was seen only on the shard
    that is primary. Executing `sh.status()` tells us the primary shard. To change
    the primary, we need to execute a command from the admin database from the shell
    connected to the mongos process. The command is as follows:'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  id: totrans-686
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: Once the primary shard was changed, all existing data of non-sharded database
    and collection was migrated to the new primary and all subsequent writes to non-sharded
    collections will go to this shard.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: Use this command with caution as it will migrate all the unsharded collections
    to the new primary, which may take time for big collections.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
- en: Manual split and migration of chunks
  id: totrans-689
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though MongoDB does a good job of splitting and migrating chunks across shards
    to maintain the balance, under some circumstances such as a small number of documents
    or relatively large number of small documents where the automatic balancer doesn't
    split the collection, an administrator might want to split and migrate the chunks
    manually. In this recipe, we will see how to split and migrate the collection
    manually across shards. For this recipe, we will set up a simple shard as we saw
    in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-691
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting a simple sharded environment of two shards* in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* to set up and start a sharded environment. It is preferred
    to start a clean environment without any data in it. From the shell, connect to
    the started mongos process.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-693
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Connect to the mongos process from the mongo shell and enable sharding on the
    `test` database and the `splitAndMoveTest` collection as follows:'
  id: totrans-694
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  id: totrans-695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'Let''s load the data in the collection as follows:'
  id: totrans-696
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  id: totrans-697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'Once the data is loaded, execute the following:'
  id: totrans-698
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  id: totrans-699
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: Note the number of documents in two shards in the plan. The value to lookout
    for is in the two documents under the shards key in the result of explain plan.
    Within these two documents the field to lookout for is `n`.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following to see the splits of the collection:'
  id: totrans-701
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  id: totrans-702
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'Split the chunk into two at `5000` as follows:'
  id: totrans-703
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  id: totrans-704
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'Splitting it doesn''t migrate it to the second server. See what exactly happened
    with the chunks by executing the following query again:'
  id: totrans-705
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  id: totrans-706
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'We will now move the second chunk to the second shard:'
  id: totrans-707
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  id: totrans-708
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'Execute the following query again and confirm the migration:'
  id: totrans-709
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE142]'
  id: totrans-710
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: 'Alternatively, the following explain plan will show a split of about 50-50:'
  id: totrans-711
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE143]'
  id: totrans-712
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: How it works…
  id: totrans-713
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We simulate a small data load by adding monotonically increasing numbers and
    discover that the numbers are not split across two shards evenly by viewing the
    query plan. It is not a problem as the chunk size needs to reach a particular
    threshold, 64 MB by default, before the balancer decides to migrate the chunks
    across the shards to maintain balance. This is pretty perfect as in real world,
    when the data size gets huge we will see that eventually over a period of time
    the shards are well balanced.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
- en: However, if the administration does decide to split and migrate the chunks,
    it is possible to do it manually. The two helper functions `sh.splitAt` and `sh.moveChunk`
    are there to do this work. Let's look at their signatures and see what they do.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
- en: The function `sh.splitAt` takes two arguments, first is the namespace, which
    has the format `<database>.<collection name>` and the second parameter is the
    query that acts as the split point to split the chunk into two, possibly two uneven
    portions depending on where the given document is in the chunk. There is another
    method, `sh.splitFind`, which will try and split the chunk in two equal portions.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: Splitting doesn't mean the chunk moves to another shard, it just breaks one
    big chunk into two, but the data stays on the same shard. It is an inexpensive
    operation which involves updating the config DB.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
- en: Next, we executed was to migrate the chunk to a different shard after we split
    it into two. The operation `sh.MoveChunk` is used just to do that. This function
    takes three parameters, first one is again the namespace of the collection that
    has the format `<database>.<collection name>`, second parameter is a query a document
    whose chunk would be migrated, and the third parameter is the destination chunk.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
- en: Once the migration is done, the query's plan shows us that the data is split
    in two chunks.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: Domain-driven sharding using tags
  id: totrans-720
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recipes *Starting a simple sharded environment of two shards* and *Connecting
    to a shard in the shell and performing operations* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* explained how
    to start a simple two server shard and then insert data in a collection after
    choosing a shard key. The data that gets sharded is more technical where the data
    chunk is kept to a manageable size by Mongo by splitting it into multiple chunks
    and migrating the chunks across shards to keep the chunk distribution even across
    shards. But what if we want the sharding to be more domain oriented? Suppose we
    have a database for storing postal addresses and we shard based on postal codes
    where we know the postal code range of a city. What we can do is tag the shard
    servers according to the city name as the tag, add shard range (postal codes),
    and associate this range with the tag. This way, we can state which servers can
    contain the postal addresses of which cities. For instance, we know that Mumbai
    being most populous city, the number of addresses would be huge and thus we add
    two shards for Mumbai. On the other hand, one shard should be enough to cope up
    with the volumes of the Pune city. For now we tag just one shard. In this recipe,
    we will see how to achieve this use case using tag aware sharding. If the description
    is confusing, don't worry, we will see how to implement what we just discussed.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-722
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting a simple sharded environment of two shard* in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for information on how to start a simple shard. However,
    for this recipe, we will add an additional shard. So, we will now start three
    mongo servers listening to port `27000`, `27001`, and `27002`. Again, it is recommended
    to start off with a clean database. For the purpose of this recipe, we will be
    using the collection `userAddress` to store the data.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-724
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assuming that we have three shard up and running, let''s execute the following:'
  id: totrans-725
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  id: totrans-726
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'With tags defined, let''s define range of pin codes that will map to a tag:'
  id: totrans-727
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE145]'
  id: totrans-728
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'Enable sharding for the test database and the `userAddress` collection as follows:'
  id: totrans-729
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE146]'
  id: totrans-730
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'Insert the following documents in the `userAddress` collection:'
  id: totrans-731
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE147]'
  id: totrans-732
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: 'Execute the following plans:'
  id: totrans-733
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE148]'
  id: totrans-734
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: How it works…
  id: totrans-735
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we want to partition data driven by domain in a shard, we can use tag
    aware sharding. It is an excellent mechanism that lets us tag the shards and then
    split the data range across shards identified by the tags. We don't really have
    to bother about the actual machines and their address hosting the shard. Tags
    act as a good abstraction in the way, we can tag a shard with multiple tags and
    one tag can be applied to multiple shards.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we have three shards and we apply tags to each of them using the
    `sh.addShardTag` method. The method takes the shard ID, which we can see in the
    `sh.status` call with the *shards* key. This `sh.addShardTag` method can be used
    to keep adding tags to a shard. Similarly, there is a helper method `sh.removeShardTag`
    to remove an assignment of the tag from the shard. Both these methods take two
    parameters, the first one is the shard ID and second one of the tag to remove.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
- en: Once the tagging is done, we assign range of the values of the shard key to
    the tag. The method `sh.addTagRange` is used to do that. It accepts four parameters,
    first one is the namespace, which is the fully qualified name of the collection,
    second and third parameters are the start and end value of the range for this
    shard key and the fourth parameter is the tag name of the shards hosting the range
    being added. For example, the call `sh.addTagRange('test.userAddress', {pincode:400001},
    {pincode:400999}, 'Mumbai')` says we are adding the shard range `400001` to `400999`
    for the collection `test.userAddress`, and this range will be stored in the shards
    tagged as `Mumbai`.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
- en: Once the tagging and adding tag range is done, we enabled sharding on database
    and collection and add data to it from Mumbai and Pune city with respective pin
    codes. We then query and explain the plan to see that the data did indeed reside
    on the shards we have tagged for Pune and Mumbai city. We can also add new shards
    to this sharded setup and accordingly tag the new shard. The balancer will then
    accordingly balance the data based on the value it is tagged. For instance, if
    the addresses in Pune increase overloading a shard, we can add a new shard with
    tag as Pune. The postal address for Pune will then be sharded across these two
    server instances for tagged for Pune city.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the config database in a sharded setup
  id: totrans-740
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Config database is the backbone of a sharded setup in Mongo. It stores all the
    metadata of the shard setup and has a dedicated mongod process running for it.
    When a mongos process is started we provide it with the config servers' URL. In
    this recipe, we will take a look at some collections in the config database and
    dive deep into their content and significance.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-742
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need a sharded setup for this recipe. Refer to the recipe *Starting a simple
    sharded environment of two shard* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* for information
    on how to start a simple shard. Additionally, connect to the mongos process from
    a shell.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-744
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From the console connected to the mongos process, switch to the config database
    and execute the following:'
  id: totrans-745
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE149]'
  id: totrans-746
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: 'From the list of all collections, we will visit a few. We start with the databases
    collection. This keeps a track of all the databases on this shard. Execute the
    following from the shell:'
  id: totrans-747
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE150]'
  id: totrans-748
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: The content of the result is pretty straightforward, the value of the field
    `_id` is for the database. The value of field partitioned tells us whether sharding
    is enabled for the database or not; true indicates it is enabled and the field
    primary gives the primary shard where the data of non-sharded collections reside
    upon.
  id: totrans-749
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will visit the `collections` collection. Execute the following from
    the shell:'
  id: totrans-750
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE151]'
  id: totrans-751
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: This collection, unlike the databases collection we saw earlier, contains only
    those collections for which we have enabled sharding. The field `_id` gives the
    namespace of the collection in the `<database>.<collection name>` format, the
    field key gives the shard key and the field unique, indicates whether the shard
    key is unique or not. These three fields come as the three parameters of the `sh.shardCollection`
    function in that very order.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we look at the `chunks` collection. Execute the following on the shell.
    If the database was clean when we started this recipe, we won''t have a lot of
    data in this:'
  id: totrans-753
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE152]'
  id: totrans-754
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'We then look at the tags collection and execute the following query:'
  id: totrans-755
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE153]'
  id: totrans-756
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: Let's query the mongos collection as follows.
  id: totrans-757
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE154]'
  id: totrans-758
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: This is a simple collection that gives the list of all mongos instances connected
    to the shard with the details like the host and port on which the mongos instance
    is running, which forms the `_id` field. The version and figures like for how
    much time the process is up and running in seconds.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we look at the version collection. Execute the following query. Note
    that is not similar to other queries we execute:'
  id: totrans-760
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE155]'
  id: totrans-761
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: How it works…
  id: totrans-762
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We saw the collections and databases collection while we queried them and they
    are pretty simple. Let''s look at the collection called `chunks`. Here is a sample
    document from this collection:'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  id: totrans-764
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: The fields of interest are `ns`, `min`, `max`, and `shard`, which are the namespace
    of the collection, the minimum value present in the chunk, the maximum value present
    in the chunk, and the shard on which this chunk lies, respectively. The value
    of the chunk size is 64 MB by default. This can be seen in the settings collection.
    Execute `db.settings.find()` from the shell and look at the value of the field
    value, which is the size of the chunk in MB. Chunks are restricted to this small
    size to ease the migration process across shards, if needed. When the size of
    the chunk exceeds this threshold, mongo server finds a suitable point in the existing
    chunk to break it into two and adds a new entry in this chunks collection. This
    operation is called splitting, which is inexpensive as the data stays where it
    is; it is just logically split into multiple chunks. The balancer on mongo tries
    to keep the chunks across shards balanced and the moment it sees some imbalance,
    it migrates these chunks to a different shard. This is expensive and also depends
    largely on the network bandwidth. If we use `sh.status()`, the implementation
    actually queries the collections we saw and prints the pretty formatted result.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
