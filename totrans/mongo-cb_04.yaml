- en: Chapter 4. Administration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will see the following recipes related to MongoDB administration:'
  prefs: []
  type: TYPE_NORMAL
- en: Renaming a collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viewing collection stats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viewing database stats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manually padding a document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mongostat and mongotop utilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting current executing operations and killing them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using profiler to profile operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up users in Mongo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interprocess security in Mongo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying collection behavior using the collMod command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up MongoDB as a Windows service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replica set configurations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stepping down as primary from the replica set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the local database of a replica set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding and analyzing oplogs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building tagged Replica sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring the default shard for non-sharded collections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual split and migration of chunks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain-driven sharding using tags
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the config database in a sharded setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we will cover some of the tools and practices for administering
    MongoDB. The following recipes will help you collect statistics from your database,
    administer user access, analyze oplogs and look into some aspects of working with
    replica sets.
  prefs: []
  type: TYPE_NORMAL
- en: Renaming a collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever come across a scenario where you have named a table in a relational
    database and at a later point of time felt that the name could have been better?
    Or perhaps the organization you work for was late in realizing that the table
    names are really getting messy and enforce some standards on the names? Relational
    databases do have some proprietary ways to rename the tables and a database admin
    would do that for you.
  prefs: []
  type: TYPE_NORMAL
- en: This raises a question though. In Mongo world, where collections are synonymous
    to tables, is there a way to rename a collection to some other name after it is
    created? In this recipe, we will explore this feature of Mongo where we rename
    an existing collection with some data in it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would need to run a MongoDB instance to perform this collection renaming
    experiment. Refer to the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* for information on how to start the server. The operations we will perform
    would be from mongo shell.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the server is started and assuming it is listening for client connections
    on default port `27017`, execute the following command to connect to it from the
    shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once connected, using the default test database. Let''s create a collection
    with some test data. The collection we will use is named:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The test data will now be created (we may verify the data by querying the collection
    `sloppyNamedCollection`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rename the collection `neatNamedCollection` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that the collection `sloppyNamedCollection` is no longer present by
    executing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, query the `neatNamedCollection` collection to verify that the data
    originally in `sloppyNamedCollection` is indeed present in it. Simply execute
    the following on the mongo shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Renaming a collection is pretty simple. It is accomplished with the `renameCollection`
    method, which takes two arguments. Generally, the function signature is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The first argument is the name to which the collection is to be renamed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second parameter that we didn''t use is a Boolean value that tells the
    command whether to drop the target collection if it exists. This value defaults
    to false, which means do not drop the target but give an error. This is a sensible
    default, otherwise the results would be ghastly if we accidently gave a collection
    name that exists and didn''t wish to drop it. However, if you know what you are
    doing and want the target to be dropped while renaming the collection, pass the
    second parameter as true. The name of this parameter is `dropTarget`. In our case,
    the call would have been:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As an exercise, try creating the `sloppyNamedCollection` again and rename it
    without the second parameter (or false as the value). You should see mongo complaining
    that the target namespace exists. Then, again rename with the second parameter
    as true, and now the renaming operation executes successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the rename operation will keep the original and the newly renamed
    collection in the same database. This `renameCollection` method is not enough
    to move/rename the collection across another database. In such cases, we need
    to run the `renameCollection` command that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose we want to rename the collection `sloppyNamedCollection` to `neatNamedCollection`
    as well as move it from `test` database to `newDatabase`, we can do so by executing
    the following command. Note the switch `dropTarget: true` used is meant to remove
    the existing target collection (`newDatabase.neatNamedCollection`) if it exists.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Also, the rename collection operation doesn't work on sharded collections.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing collection stats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps one of the interesting statistics from an administrative purpose when
    it comes to the usage of storage, the number of documents in collection possibly
    to estimate the future space, and memory requirements based on the growth of the
    data is to get a high level statistics of the collection.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To find the stats of the collection we need to have a server up and running
    and a single node is what should be okay. Refer to the *Installing single node
    MongoDB* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server* for information on how to start the server.
    The data on which we would be operating needs to be imported in the database.
    The steps to import the data are given in the recipe *Creating Test Data* in [Chapter
    2](ch02.html "Chapter 2. Command-line Operations and Indexes"), *Command-line
    Operations and Indexes*. Once these steps are completed, we are all set to go
    ahead with this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would be using `postalCodes` collection for viewing the stats.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the mongo shell and connect to the running MongoDB instance. In case you
    have started the mongo on default port, execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'With the data imported, create an index on the `pincode` field if one doesn''t
    exist:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'On the mongo terminal, execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Observe the output and execute the following on the shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Again, observe the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now see what these values printed out mean to us in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we observe the output for both these commands, we see that the second one
    has all the figures in KB whereas the first one is in bytes. The parameter provided
    is known as scale and all the figures indicating size are divided by this scale.
    In this case, since we gave the value as `1024`, we get all the values in KB whereas
    if `1024 * 1024` is passed as the value of scale (the size shown will be in MB).
    For our analysis, we will use the one that shows the sizes in KB.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table shows the meaning of the important fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Field | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `ns` | The fully qualified name of the collection with a format `<database>.<collection
    name>`. |'
  prefs: []
  type: TYPE_TB
- en: '| `count` | The number of documents in the collection. |'
  prefs: []
  type: TYPE_TB
- en: '| `size` | The actual storage size occupied by the documents in the collection.
    Addition, deletion, and updates to documents in the collection can change this
    figure. The scale parameter affects this field''s value and in our case this value
    is in KB as `1024` is the scale. This number does include padding, if any. |'
  prefs: []
  type: TYPE_TB
- en: '| `avgObjSize` | This is the average size of the document in the collection.
    It is simply the size field divided by the count of documents in the collection
    (the preceding two fields). The scale parameter affects this field''s value and
    in our case this value is in KB as `1024` is the scale. |'
  prefs: []
  type: TYPE_TB
- en: '| `storageSize` | Mongo preallocates the space on the disk to ensure that the
    documents in the collection are kept on continuous locations to provide better
    performance in disk access. This preallocation fills up the files with zeros and
    then starts allocating space to these documents inserted. This field tells the
    size on the storage used by this collection. This figure will generally be much
    more than the actual size of the collection. The scale parameter affects this
    field''s value and in our case this value is in KB as `1024` is the scale. |'
  prefs: []
  type: TYPE_TB
- en: '| `numExtents` | As we saw, mongo pre allocates continuous disk space to the
    collections for performance purpose. However as the collection grows, new space
    needs to be allocated. This field gives the number of such continuous chunk allocation.
    This continuous chunk is called an extent. |'
  prefs: []
  type: TYPE_TB
- en: '| `nindexes` | This field gives the number of indexes present on the collection.
    This value would be `1` even if we do not create an index on the collection as
    mongo implicitly creates an index on the field `_id`. |'
  prefs: []
  type: TYPE_TB
- en: '| `lastExtentSize` | The size of the last extent allocated. The scale parameter
    affects this field''s value and in our case this value is in KB as `1024` is the
    scale. |'
  prefs: []
  type: TYPE_TB
- en: '| `paddingFactor` | This parameter has been deprecated since version 3.0.0
    and is hardcoded to `1` for backward compatibility reasons. |'
  prefs: []
  type: TYPE_TB
- en: '| `totalIndexSize` | Indexes take up space to store too. This field gives the
    total size taken up by the indexes on the disk. The scale parameter affects this
    field''s value and in our case this value is in KB as `1024` is the scale. |'
  prefs: []
  type: TYPE_TB
- en: '| `indexSizes` | This field is a document with the key as the name of the index
    and value as the size of the index in question. In our case, we had created an
    index explicitly on the `pincode` field; thus, we see the name of the index as
    the key and the size of the index on disk as the value. The total of these values
    of all the index is same as the value given previously, `totalIndexSize`. The
    scale parameter affects this field''s value and in our case this value is in KB
    as `1024` is the scale. |'
  prefs: []
  type: TYPE_TB
- en: Documents are placed on the storage device in continuous locations. If a document
    is updated, resulting in an increase in size, Mongo will have to relocate this
    document. This operation turns out to be expensive affecting the performance of
    such update operations. Starting with Mongo 3.0.0, two data allocation strategies
    are used. One is *The power of 2*, where documents are allocated space in power
    of 2 (for example, 32, 64, 128, and so on). The other is *No Padding*, where collections
    do not expect document sizes to be altered.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we discussed viewing stats of a collection. See the next recipe
    to view the stats at a database level.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing database stats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we saw how to view some important statistics of a collection
    from an administrative perspective. In this recipe, we get an even higher picture,
    getting those (or most of those) statistics at the database level.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To find the stats of the database, we need to have a server up and running and
    a single node is what should be okay. Refer to the recipe *Installing single node
    MongoDB* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server* for information on how to start the server.
    The data on which we would be operating needs to be imported in the database.
    The steps to import the data are given in the recipe *Creating Test Data* in [Chapter
    2](ch02.html "Chapter 2. Command-line Operations and Indexes"), *Command-line
    Operations and Indexes*. Once these steps are completed, we are all set to go
    ahead with this recipe. Refer to the previous recipe if you need to see how to
    view stats at the collection level.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `test` database for the purpose of this recipe. It already has
    a `postalCodes` collection in it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect to the server using the mongo shell by typing in the following command
    from the operating system terminal. It is assumed that the server is listening
    to port `27017`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'On the shell, execute the following command and observe the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'On the shell, again execute the following but this time around we add the scale
    parameter. Observe the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `scale` parameter, which is a parameter to the `stats` function, divides
    the number of bytes with the given scale value. In this case, it is `1024` and
    hence all the values will be in KB. We analyze the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table shows the meaning of the important fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Field | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `db` | This is the name of the database whose stats are being viewed. |'
  prefs: []
  type: TYPE_TB
- en: '| `collections` | This is the total number of collections in the database.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `objects` | This is the count of documents across all collections in the
    database. If we find the stats of a collection using `db.<collection>.stats()`,
    we get the count of documents in the collection. This attribute is the sum of
    counts of all the collections in the database. |'
  prefs: []
  type: TYPE_TB
- en: '| `avgObjectSize` | This is simply the size in bytes of all the objects in
    all the collections in the database divided by the count of the documents across
    all the collections. This value is not affected by the scale provided, although
    this is a `size` field. |'
  prefs: []
  type: TYPE_TB
- en: '| `dataSize` | This is the total size of the data held across all the collections
    in the database. This value is affected by the scale provided. |'
  prefs: []
  type: TYPE_TB
- en: '| `storageSize` | This is the total amount of storage allocated to collections
    in this database for storing documents. This value is affected by the scale provided.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `numExtents` | This is the count of all the number of extents in the database
    across all the collections. This is basically the number of extents (logical containers)
    in the collection stats for collections in this database. |'
  prefs: []
  type: TYPE_TB
- en: '| `indexes` | This is the sum of number of indexes across all collections in
    the database |'
  prefs: []
  type: TYPE_TB
- en: '| `indexSize` | This is the size in bytes for all the indexes of all the collections
    in the database. This value is affected by the scale provided. |'
  prefs: []
  type: TYPE_TB
- en: '| `fileSize` | This is a sum of the size of all the database files you should
    find on the filesystem for this database. The files would be named `test.0`, `test.1`,
    and so on for test database. This value is affected by the scale provided. |'
  prefs: []
  type: TYPE_TB
- en: '| `nsSizeMB` | This is the size of the file in MB for the `.ns` file of the
    database. |'
  prefs: []
  type: TYPE_TB
- en: '| `extentFreeList.num` | This is the number of free extends in freelist. You
    can look at extent as an internal data structure of MongoDB. |'
  prefs: []
  type: TYPE_TB
- en: '| `extentFreeList.totalSize` | Size of the extents on the freelist. |'
  prefs: []
  type: TYPE_TB
- en: For more information on these, you can refer to books such as *Instant MongoDB*
    by *Packt Publishing* ([http://www.packtpub.com/big-data-and-business-inteliigence/instant-mongodb-instant](http://www.packtpub.com/big-data-and-business-inteliigence/instant-mongodb-instant)).
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start by looking at the `collections` field. If you look carefully at
    the number and execute the show collections command on the mongo shell, you will
    find one extra collection in the stats as compared to those by executing the command.
    The difference is for one collection, which is hidden. Its name is `system.namespaces`
    collection. You may do a `db.system.namespaces.find()` to view its contents.
  prefs: []
  type: TYPE_NORMAL
- en: Getting back to the output of stats operation on the database, the objects field
    in the result has an interesting value too. If we find the count of documents
    in the `postalCodes` collection, we see it is `39732`. The count shown here is
    `39738`, which means there are six more documents. These six documents come from
    the `system.namespaces` and `system.indexes` collection. Executing a count query
    on these two collections will confirm it. Note that the `test` database doesn't
    contain any other collection apart from `postalCodes`. The figures would change
    if the database contains more collections with documents in it.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to note is the value of the `avgObjectSize` and there is something
    weird in this value. Unlike this very field in the collection's stats, which is
    affected by the value of the scale provided, in database stats this value is always
    in bytes. This is pretty confusing and I am not really sure why this is not scaled
    according to the provided scale.
  prefs: []
  type: TYPE_NORMAL
- en: Manually padding a document
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Without getting too much into the internals of the storage, MongoDB uses memory
    mapped files, which means that the data is stored in files exactly as how it would
    be in memory and it would use the low level OS services to map these pages to
    memory. The documents are stored in continuous locations in mongo data files and
    problem arises when the document grows and no longer fits in the space. In such
    scenarios, mongo rewrites the document towards the end of the collection with
    the updated data and clearing up the space where it was originally placed (note
    that this space is not released to OS as free space).
  prefs: []
  type: TYPE_NORMAL
- en: This is not a big problem for applications that don't expect the documents to
    grow in size. However, this is a big performance hit for those who foresee this
    growth in the document size over a period of time and potentially a lot of such
    document movements. With the release of MongoDB 3.0, the *Power of 2* method became
    the default size allocation strategy. As the name suggests, this method stores
    documents in space allocated in powers of 2\. This provides additional padding
    to the documents as well as better reuse of free space caused by relocation or
    deletion of documents.
  prefs: []
  type: TYPE_NORMAL
- en: That said, if you still wish to introduce manual padding in your strategy, read
    on.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nothing is needed for this recipe unless you plan to try out this simple technique,
    in which case you would need a single instance up and running. Refer to the recipe
    *Installing single node MongoDB* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* for information
    on how to start the server.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea of this technique is to add some dummy data to the document to be inserted.
    This dummy data's size in addition to other data in the document is approximately
    same as the anticipated size of the document.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the average size of the document is estimated to be around 1200
    bytes over a period of time and there is 300 bytes of data present in the document
    while inserting it, we will add a dummy field of size around 900 bytes so that
    the total document size sums up to 1200 bytes.
  prefs: []
  type: TYPE_NORMAL
- en: Once the document is inserted, we unset this dummy field, which leaves a hole
    in the file between the two consecutive documents. This empty space would then
    be used when the document grows over a period of time minimizing the document
    movements. The empty space may also be used by another document. The more foolproof
    way is to remove the padding only when you are using the space. However, any document
    growing beyond the anticipated average growth will have to be copied by the server
    to the end of the collection. Needless to say, documents not growing to the anticipated
    size will tend to waste disk space.
  prefs: []
  type: TYPE_NORMAL
- en: The applications can come up with some intelligent strategy to perhaps the adjust
    the size of the padding field based on say some particular field of the document
    to take care of these shortcomings but that is something up to the application
    developers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see a sample of this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a small function that will add a field called `padField` with an
    array of string values to the document. Its code is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It will add an array called `padField` and add 20 times a string called `Dummy`.
    There is no restriction on what type you add to the document and how many times
    it is added as long as it consumes the space you desire. The preceding code is
    just a sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to insert a document. We will define another function called
    `insert` to do that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now put this into action by inserting a document in the collection
    `testCol` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You may query the `testCol` using the following query and check if the document
    inserted exists or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that on querying you would not find the `padField` in it. However, the
    space once occupied by the array stays between the subsequently inserted documents
    even if the field was unset.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `insert` function is self-explanatory and has comments in it to tell what
    it does. An obvious question is how do we believe if this indeed what we intent
    to do. For this purpose, we shall do a small activity as follow. We will work
    on a `manualPadTest` collection for this purpose. From the mongo shell, execute
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a note of the `avgObjSize` field in the stats. Next, execute the following
    from the mongo shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Take a note of the `avgObjSize` field in the stats. This figure is much larger
    than the one we saw earlier with a regular insert without padding. The `paddingFactor`
    as we see in both cases still is one, but the latter case has more buffer for
    the document to grow.
  prefs: []
  type: TYPE_NORMAL
- en: One catch in the `insert` function we used in this recipe is that the insert
    into the collection and the update document operations are not atomic.
  prefs: []
  type: TYPE_NORMAL
- en: The mongostat and mongotop utilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of you might find these names similar to two popular Unix commands, `iostat`
    and `top`. For MongoDB, `mongostat` and `mongotop` are two utilities which does
    pretty much the same job as these two Unix commands do and there is no prize for
    guessing that these are used to monitor the mongo instance.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we would be simulating some operations on a standalone mongo
    instance by running a script that would attempt to keep your server busy, and
    then in another terminal we will run these utilities to monitor the `db` instance.
  prefs: []
  type: TYPE_NORMAL
- en: You need to start a standalone server listening to any port for client connections;
    in this case, we will stick to the default `27017`. If you are not aware how to
    start a standalone server, refer to *Installing single node MongoDB* in [Chapter
    1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing and
    Starting the Server*. We also need to download the script `KeepServerBusy.js`
    from Packt site and keep it handy for execution on local drive. Also, it is assumed
    that the `bin` directory of your mongo installation is present in the path variable
    of your operating system. If not, then these commands need to be executed with
    the absolute path of the executable from the shell. These two utilities `mongostat`
    and `mongotop` comes standard with the mongo installation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start the MongoDB server, and let it listen to the default port for connections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In a separate terminal, execute the provided JavaScript `KeepServerBusy.js`
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Open a new OS terminal and execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Capture the output content for some time and then hit *Ctrl* + *C* to stop the
    command from capturing more stats. Keep the terminal open or copy the stats to
    another file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, execute the following command from the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Capture the output content for some time and then hit *Ctrl* + *C* to stop the
    command from capturing more stats. Keep the terminal open or copy the stats to
    another file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hit *Ctrl* + *C* in the shell where the provided JavaScript `KeepServerBusy.js`
    was executed to stop the operation that keeps the server busy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's see what we have captured from these two utilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by analyzing `mongostat`. On my laptop, the capture using `mongostat`
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: You may choose to look at what the script `KeepServerBusy.js` is doing to keep
    the server busy. All it does is insert 1000 documents in collection `monitoringTest`,
    then update them one by one to set a new key in it, executes a find and iterates
    through all of them, and finally deletes them one by one and is basically a write
    intensive operation.
  prefs: []
  type: TYPE_NORMAL
- en: The output does look ugly with content wrapping, but let's analyze the fields
    one by one and see what the fields to keep an eye on.
  prefs: []
  type: TYPE_NORMAL
- en: '| Column(s) | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `insert`, `query`, `update`, `delete` | The first four columns are the number
    of `insert`, `query`, `update` and `delete` operation per second. It is per second
    as the time frame these figures are captured are separated by one second, which
    is indicated by the last column. |'
  prefs: []
  type: TYPE_TB
- en: '| `getmore` | When the cursor runs out of data for the query, it executes a
    `getmore` operation on the server to get more results for the query executed earlier.
    This column shows the number of `getmore` operations executed in this given time
    frame of 1 second. In our case, there are not many `getmore` operations that are
    executed. |'
  prefs: []
  type: TYPE_TB
- en: '| `commands` | This is the number of commands executed on the server in the
    given time frame of 1 second. In our case, it wasn''t much and was only one. The
    number after a `&#124;` is `0` in our case, as this was in standalone mode. Try
    executing `mongostat` connecting to a replica set primary and secondary. You should
    see slightly different figures there. |'
  prefs: []
  type: TYPE_TB
- en: '| `flushes` | This is the number of times data was flushed to disk in the interval
    of 1 second. (`fsync` in case of `MMAPv1` storage engine, and checkpoints triggered
    between polling interval in case of `WiredTiger` storage engine) |'
  prefs: []
  type: TYPE_TB
- en: '| `mapped`, `virtual`, and `resident memory` | Mapped memory is the amount
    of memory mapped by the Mongo process to the database. This will typically be
    same as the size of the database. Virtual memory on other hand is the memory allocated
    to the entire `mongod` process. This will be more than twice the size of mapped
    memory especially when journaling is enabled. Finally, resident memory is the
    actual of physical memory used by mongo. All these figures are given in MB. The
    total amount of physical memory might be a lot more than what is being used by
    Mongo, but that is really not a concern unless a lot of page faults occur (which
    does happen in the previously mentioned output). |'
  prefs: []
  type: TYPE_TB
- en: '| `faults` | These are the number of page faults occurring per second. These
    numbers should be as less as possible. It indicates the number of times mongo
    had to go to disk to obtain the document/index that was missing in the main memory.
    This problem is not as big a problem when using SSD for persistent storage as
    it is when using spinning disk drives. |'
  prefs: []
  type: TYPE_TB
- en: '| `locked` | Since version 2.2, all write operations to a collection lock the
    database in which the collection is and does not acquire a global level lock.
    This field shows the database that was locked for a majority of the time in the
    given time interval. In our case, the `test` database is locked for a majority
    of time. |'
  prefs: []
  type: TYPE_TB
- en: '| `idx miss %` | This field gives the number of times a particular index was
    needed and was not present in memory. This causes a page fault and the disk needs
    to be accessed to get the index. Another disk access might be needed to get the
    document as well. This figure too should be low. A high percentage of index miss
    is something that would need attention. |'
  prefs: []
  type: TYPE_TB
- en: '| `qr` &#124; `qw` | These are the queued up reads and writes that are waiting
    for getting a chance to be executed. If this number goes up, it shows that the
    database is getting overwhelmed by the volume of read and writes than it could
    handle. If the values are too high, keep an eye on page faults and database lock
    percents in order to get more insights on increased queue counts. If the data
    set is too large, sharding the collection can improve the performance significantly.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `ar` &#124; `aw` | This is the number of active readers and writers (clients).
    Not something to worry of even for a large number as far as other stats we saw
    previously are under control. |'
  prefs: []
  type: TYPE_TB
- en: '| `netIn` and `netOut` | The network traffic in and out of the mongo server
    in the given time frame. Figure is measured in bits. For example, 271k means 271
    kilobits. |'
  prefs: []
  type: TYPE_TB
- en: '| `conn` | This indicates the number of open connections. Something to keep
    a watch on to see if this doesn''t keep getting higher. |'
  prefs: []
  type: TYPE_TB
- en: '| `time` | This is the time interval when this sample was captured. |'
  prefs: []
  type: TYPE_TB
- en: There are some more fields seen if `mongostat` is connected to a replica set
    primary or secondary. As an assignment, once the stats or a standalone instance
    are collected, start a replica set server and execute the same script to keep
    the server busy. Use `mongostat` to connect to a primary and secondary instance
    and see different stats captured.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from `mongostat`, we also used the `mongotop` utility to capture the
    stats. Let''s see its output and make some sense out of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: There is not much to see in this stat. We see the total time a database was
    busy reading or writing in the given slice of 1 second. The value given in the
    total would be sum of the read and the write time. If we actually compare the
    `mongotop` and `mongostat` for the same time slice, the percentage of time duration
    for which the write was taking place would be very close to the figure given in
    the percentage time that the database was locked in the `mongostat` output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command `mongotop` accepts a parameter on the command line as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the interval after which the stats will be printed out will be
    5 seconds as opposed to the default value of 1 second.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Starting with MongoDB 3.0, both `mongotop` and `mongostat` utilities allow output
    in JSON format using `--json` option. This can be very useful if you were to use
    custom monitoring or metrics collection scripts, which would rely on these utilities.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the recipe *Getting current executing operations and killing them*, we will
    see how to get the current executing operations from the shell and kill them if
    needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the recipe *Using profiler to profile operations*, we will see how to use
    the inbuilt profiling feature of Mongo to log operation execution times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting current executing operations and killing them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to view the current running operations and kill
    some operations that are running for a long time.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will simulate some operations on a standalone mongo instance. We need to
    start a standalone server listening to any port for client connections; in this
    case, we will stick to the default `27017`. If you are not aware how to start
    a standalone server, refer to *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server*. We also need to start two shells connected to the server started. One
    shell would be used for background index creation and another would be used to
    monitor the current operation and then kill it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would not be able to simulate the actual long running operation in our test
    environment. We will try to create an index and hope it takes long to create.
    Depending on your target hardware configuration, the operation may take some time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To start with this test, let''s execute the following on the mongo shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The preceding insertion might take some time to insert 10 million documents.
  prefs: []
  type: TYPE_NORMAL
- en: Once the documents are inserted, we will execute an operation that would create
    the index in background. If you would like to know more about index creation,
    refer to the recipe *Creating a background and foreground index in the shell*
    in [Chapter 2](ch02.html "Chapter 2. Command-line Operations and Indexes"), *Command-line
    Operations and Indexes*, but it is not a prerequisite for this recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a background index on the field `i` in the document. This index creation
    operation is what we will be viewing from the `currentOp` operation and is what
    we will attempt to kill from using the kill operation. Execute the following in
    one shell to initiate the background index creation operation. This takes fairly
    long time and on my laptop it took well over 100 seconds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In the second shell, execute the following command to get the current executing
    operations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Take a note of the progress of the operations and find the one that is necessary
    for index creation. In our case, it was the only in progress on test machine.
    It will be an operation on `system.indexes` and the operation will be insert.
    The keys to lookout for in the output document are `ns` and `op`, respectively.
    We need to note the first field of this operation, `opid`. In this case, it is
    `11587458`. The sample output of the command is given in next section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kill the operation from the shell using the following command, using the `opid`
    (operation ID) we got earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will split our explanation into two sections, the first about the current
    operation details and second about killing the operation.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, index creation process is the long-running operation that we intend
    to kill. We create a big collection with about 10 million documents and initiate
    a background index creation process.
  prefs: []
  type: TYPE_NORMAL
- en: 'On executing the `db.currentOp()` operation, we get a document as the result
    with a field `inprog` whose value is an array of other documents each representing
    a currently running operation. It is common to get a big list of documents on
    a busy system. Here is a document taken for the index creation operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see what these fields mean in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Field | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `opid` | This is a unique operation ID identifying the operation. This is
    the ID to be used to kill an operation. |'
  prefs: []
  type: TYPE_TB
- en: '| `active` | The Boolean value indicating whether the operation has started
    or not, it is false only if it is waiting for acquiring the lock to execute the
    operation. The value will be true once it starts even if at a point of time where
    it has yielded the lock and is not executing. |'
  prefs: []
  type: TYPE_TB
- en: '| `secs_running` | Gives the time in seconds the operation is executing for.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `op` | This is the type of the operation. In the case of index creation,
    it is inserted into a system collection of indexes. Possible values are `insert`,
    `query`, `getmore`, `update`, `remove`, and `command`. |'
  prefs: []
  type: TYPE_TB
- en: '| `ns` | This is the fully qualified namespace for the target. It would be
    in the form `<database name>.<collection name>`. |'
  prefs: []
  type: TYPE_TB
- en: '| `insert` | This is the document that would be inserted in the collection.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `query` | This is a field that would be present for other operations, other
    than `insert`, `getmore`, and `command`. |'
  prefs: []
  type: TYPE_TB
- en: '| `client` | The ip address/hostname and the port of the client who initiated
    the operation. |'
  prefs: []
  type: TYPE_TB
- en: '| `desc` | This is the description of the client, mostly the client connection
    name. |'
  prefs: []
  type: TYPE_TB
- en: '| `connectionId` | This is the identifier of the client connection from which
    the request originated. |'
  prefs: []
  type: TYPE_TB
- en: '| `locks` | This is a document containing the locks held for this operation.
    The document shows the type and mode of locks held for the operation being analyzed.
    The possible modes are as follows:**R** represents Shared (S) lock.**W** represents
    Exclusive (X) lock.**r** represents Intent Shared (IS) lock.**w** represents Intent
    Exclusive (IX) lock. |'
  prefs: []
  type: TYPE_TB
- en: '| `waitingForLock` | This field indicates if the operation is waiting for a
    lock to be acquired. For instance, if the preceding index creation was not a background
    process, other operations on this database would queue up for the lock to be acquired.
    This flag for those operations would then be true. |'
  prefs: []
  type: TYPE_TB
- en: '| `msg` | This is a human-readable message for the operation. In this case,
    we do see the percentage of operation complete as this is an index creation operation.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `progress` | The state of the operation, the total gives the total number
    of documents in the collection and done gives the number indexed so far. In this
    case, the collection already had some more documents over 10 million documents.
    The percentage completion is computed from these figures. |'
  prefs: []
  type: TYPE_TB
- en: '| `numYields` | This is the number of times the process has yielded the lock
    to allow other operations to execute. Since this is the background index creation
    process, this number will keep on increasing as the server yields it frequently
    to let other operations execute. Had it been a foreground process, the lock would
    never be yielded till the operation completes. |'
  prefs: []
  type: TYPE_TB
- en: '| `lockStats` | This document has more nested documents giving the stats for
    the total time this operation has held the read or write lock and also the time
    it waited to acquire the lock. |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In case you have a replica set, there would be more lot of getmore operations
    on the oplog on primary from secondary.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the system operations being executed too, we need to pass a true value
    as the parameter to the `currentOp` function call as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will see how to kill the user initiated operation using the `killOp`
    function. The operation is simply called as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, the index creation process had the process ID 11587458 and thus
    it will be killed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'On killing any operation, irrespective of whether the given operation ID exists
    or not, we see the following message on the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Thus, seeing this message doesn't mean that the operation was killed. It just
    means that the operation if it exists will be attempted to be killed.
  prefs: []
  type: TYPE_NORMAL
- en: 'If some operation cannot be killed immediately and if the `killOp` command
    is issued for it, the field `killPending` in the `currentOp` will start appearing
    for the given operation. For example, execute the following query on the shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This will not return and the thread executing the query will sleep for 100 seconds.
    This is an operation that cannot be killed using `killOp`. Try executing the command
    `currentOp` from another shell (do not press *Tab* for auto completion, your shell
    may just hang), get the operation ID, and then kill it using the `killOp`. You
    should see that the process still would be running if you execute the `currentOp`
    command, but the document for the process details will now contain a new key `killPending`
    stating that the kill for this operation is requested but pending.
  prefs: []
  type: TYPE_NORMAL
- en: Using profiler to profile operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will look at mongo's inbuilt profiler that would be used
    to profile the operations executed on the mongo server. It is a utility that is
    used to log all or slow operations that could be used for analysis of the performance
    of the server.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will perform some operations on a standalone mongo instance
    and profile them. We need to start a standalone server listening to any port for
    client connections; in this case, we will stick to the default `27017`. If you
    are not aware how to start a standalone server, refer to *Installing single node
    MongoDB* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server*. We also need to start a shell that would
    be used to perform querying, enabling profiling, and viewing the profiling operation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the server is started and the shell is connected to it, execute the following
    to get the current profiling level:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The default level should be `0` (no profiling, if we have not set it earlier).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s set the profiling level to `1` (log slow operations only) and log all
    the operations slower than `50` ms. Execute the following on the shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s execute an insert operation into a collection, and then execute
    a couple of queries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, execute the query on the following collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Profiling is something that would not be enabled by default. If you are happy
    about the performance of the database, there is no reason one would enable the
    profiler. It is only when one feels there is some room for improvement and wants
    to target some expensive operations taking place. An important question is what
    classifies an operation to be slow? The answer is, it depends from application
    to application. In mongo, slow means any operation above 100 ms. However, while
    setting the profiling level, you may choose the threshold value.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three possible values for profiling levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0`: Disable profiling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1`: Enable profiling for slow operations, where the threshold value for an
    operation to be classified as slow is provided with the call while setting the
    profiling level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2`: Profile all operations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While profiling all operations might not be a very good idea and might not be
    commonly used as we shall soon see, setting the value to `1` and a threshold provided
    to it is a good way to monitor slow operations.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the steps that we executed, we see that we can get the current
    profiling level by executing the operation `db.getProfilingLevel()`. To get more
    information, for example, what value is set as a threshold for the slow operations,
    we can use `db.getProfilingStatus()`. This returns a document with the profiling
    level and the threshold value for slow operations.
  prefs: []
  type: TYPE_NORMAL
- en: For setting the profiling level, we call the `db.setProfilingLevel()` method.
    In our case, we set it for logging all operations taking more than `50` ms as
    `db.setProfilingLevel(1, 50)`.
  prefs: []
  type: TYPE_NORMAL
- en: To disable profiling, simply execute `db.setProfilingLevel(0)`.
  prefs: []
  type: TYPE_NORMAL
- en: Next we executed three operations, one to insert a document, one to find all
    documents, and finally a find that calls sleep with a value of `70` ms to slow
    it down.
  prefs: []
  type: TYPE_NORMAL
- en: The final step was to see these profiled operations that are logged in the `system.profile`
    collection. We execute a find to see the operations logged. For my execution,
    the insert and the final `find` operation with the sleep were logged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, this profiling has some overhead but it is negligible. Hence, we
    would not enable it by default but only when we want to profile slow operations.
    Also, another question would be, *Will this profiling collection increase over
    a period of time?* The answer is *No*, as this is a capped collection. Capped
    collections are fixed size collections that preserve insertion orders and act
    as a circular queue filling in the new documents, discarding the oldest when it
    gets full. A query on `system.namespaces` should show the stats. The query execution
    would show the following for the `system.profile` collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the size of the collection is 1 MB, which is incredibly small.
    Setting the profiling level to `2` thus would easily overwrite the data on busy
    systems. One may also choose to explicitly create a collection with the name `system.profile`
    as a capped collection and of any size they prefer should they choose to retain
    more operations in it. To create a capped collection explicitly, you can execute
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, the size chosen is arbitrary and you are free to allocate any size
    to this collection based on how frequently the data gets filled and how much of
    profiling data you want to keep before it gets overwritten.
  prefs: []
  type: TYPE_NORMAL
- en: As this is a capped collection and insertion order is preserved, a query with
    the `sort order {$natural:-1}` would be perfectly fine and very efficient to find
    the operations in the reverse order of the execution time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We would finally take a look at the document that got inserted in the `system.profile`
    collection and see what all operations it has logged:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: As we can see in the document, there are indeed some interesting stats. Let's
    look at some of them in the following table. Some of these fields are identical
    to the fields we see when we execute the `db.currentOp()` operation from the shell
    and we then discussed in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: '| Field | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `op` | This is the operation that got executed; in this case, it was a find
    and thus it is query in this case. |'
  prefs: []
  type: TYPE_TB
- en: '| `ns` | This is the fully qualified name of the collection on which the operation
    was performed. It would be of the format `<database>.<collection name>`. |'
  prefs: []
  type: TYPE_TB
- en: '| `query` | It shows the query that got executed on the server. |'
  prefs: []
  type: TYPE_TB
- en: '| `nscanned` | This has a similar meaning to explain plan. It is the total
    number of documents and index entries scanned. |'
  prefs: []
  type: TYPE_TB
- en: '| `numYields` | This is the number of times the lock was yielded when the operation
    was executed. Higher yields could indicate that the query required a lot of disk
    access. This could be a good indication of re-looking at the index or optimizing
    the query itself. |'
  prefs: []
  type: TYPE_TB
- en: '| `lockStats` | Some interesting stats for the time taken to acquire the lock
    and the time for which the lock was held. |'
  prefs: []
  type: TYPE_TB
- en: '| `nreturned` | The number of documents returned. |'
  prefs: []
  type: TYPE_TB
- en: '| `responseLength` | The length of the response in bytes. |'
  prefs: []
  type: TYPE_TB
- en: '| `millis` | Most important of all, the time taken in milliseconds to execute
    the operation. This can be a good starting point to catch slow queries. |'
  prefs: []
  type: TYPE_TB
- en: '| `ts` | This is the time when the operation was executed. |'
  prefs: []
  type: TYPE_TB
- en: '| `client` | This is the hostname/IP address of the client who executed the
    operation. |'
  prefs: []
  type: TYPE_TB
- en: Setting up users in Mongo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Security is one of the cornerstones of any enterprise-level system. Not always
    would you find a system in a completely safe and secure environment to allow unauthenticated
    user access to it. Apart from test environments, almost every production environment
    requires proper access rights and perhaps audit of the system access too. Mongo
    security has multiple aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Access rights for the end users accessing the system. There would be multiple
    roles such as admin, read-only users, and read and write non-administrative users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authentication of the nodes that are added to the replica set. In a replica
    set, one should only be allowed to add authenticated systems. The integrity of
    the system would be compromised if any unauthenticated node is added to the replica
    set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encryption of the data that is transmitted across the wire between the nodes
    of the replica sets or even the client and the server (or the mongos process in
    case of sharded setup).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this and the next recipe, we would be looking at how to address the first
    and the second point given here. The third point of encrypting the data being
    transmitted on the wire is not supported by default by the community edition of
    mongo and would need a rebuild of mongo database with the `ssl` option enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will set up users for a standalone mongo instance. We need
    to start a standalone server listening to any port for client connections; in
    this case, we will stick to the default `27017`. If you are not aware how to start
    a standalone server, refer to *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server*. We also need to start a shell that would be used for this admin operation.
    For a replica set, we will only be connected to a primary and perform these operations.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will add an admin user, a read-only user for a test database, and a read-write
    user for test database in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is assumed that at this point:'
  prefs: []
  type: TYPE_NORMAL
- en: The server is up and running, and we are connected to it from the shell.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The server is started without any special command-line argument other than those
    mentioned in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server* for *Starting a single node instance using
    command-line options* recipe. We thus have full access to the server for any user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step we will do is to create an admin user. All the commands assume
    that you are using MongoDB 3.0 and above.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we start by creating the admin user in admin database as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We will add the `read_user` and `write_user` to test database. To add the users,
    execute the following from the mongo shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now shut down the mongo server and the close the shell too. Restart the mongo
    server but with the `--auth` option on the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: If your mongod instance is using `/etc/mongod.conf`, then add the line `auth
    = true` in the configuration file and restart the mongod service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now connect to the server from the newly opened mongo shell and execute the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The collection `testAuth` need not exist, but you should see an error that we
    are not authorized to query the collection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will now log in from the shell using a `read_user` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now execute the same `find` operation as follows. It should not give
    an error and it might not return any results depending on whether the collection
    exists or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will try to insert a document as follows. We should get an error that
    you are not authorized to insert data in this collection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now log out and log in again, but with a write user as follows. Note
    the difference in the way we login this time around as against the previous instance.
    We are providing a document as the parameter to the `auth` function, where as
    in previous case we passed two parameters for the username and password:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, execute the following on the shell. You should get the unauthorized error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now switch to `admin` database. We are currently connected to the server
    using a `write_user` that has read-write permissions on the `test` database. From
    the mongo shell, try to do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Close the mongo shell or open a new shell as follows from the operating system''s
    console. This should take us directly to admin database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now execute the following on the shell. It should show us the collections in
    the admin database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Try and execute the following operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We executed a lot of steps and now we will take a closer look at them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, the server is started without `--auth` option and hence no security
    is enforced by default. We create an admin user with the `db.createUser` method.
    The signature of the method to create the user is `createUser(user, writeConcern)`.
    The first parameter is the user, which actually is a JSON document and second
    is the write concern to use for user creation. The JSON document for the user
    has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The roles provided here can be provided as follows, assuming that the current
    database when the user is created is test on the shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the user being created read access to the reports `db` and `readWrite`
    access to the `test` database. Let''s see the complete user creation call of the
    `test` user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The write concern, which is an optional parameter, can be provided as the JSON
    document. Some examples values are `{w:1}`, `{w:'majority'}`.
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to the admin user creation, we created the user in step 2 using
    the `createUser` method and gave three inbuilt roles to this user in the `admin`
    database.
  prefs: []
  type: TYPE_NORMAL
- en: In step 3, we created the `read` and `read-write` users in `test` database using
    the same `createUser` method.
  prefs: []
  type: TYPE_NORMAL
- en: We shut down the MongoDB server after the `admin`, `read`, and `read-write`
    user creation and restarted it with the `--auth` option.
  prefs: []
  type: TYPE_NORMAL
- en: On starting the server again, we will connect to it from the shell in step 8,
    but unauthenticated. Here, we try to execute a `find` query on a collection in
    test database, which fails as we are unauthenticated. This indicates that the
    server now requires appropriate credentials to execute operations on it. In step
    8 and 9, we log in using the `read_user` and first execute a `find` operation
    (which succeeds), and then an insert that doesn't as the user has read privileges
    only. The way to authenticate a user by invoking from the shell `db.auth(<user
    name>, <password>)` and `db.logout()`, which will logout the current logged in
    user.
  prefs: []
  type: TYPE_NORMAL
- en: In steps 10 to 12, we demonstrate that we can perform `insert` operations using
    `write_user` but admin operations like `db.serverStatus()` cannot be executed.
    This is because these operations execute an `admin command` on the server, which
    a non-admin user and not permitted to invoke these. Similarly, when we change
    the database to admin, the `write_user`, which is from `test` database, is not
    permitted to perform any operations like getting a list of collections or any
    operation to query a collection in `admin` database.
  prefs: []
  type: TYPE_NORMAL
- en: In Step 14, we log in to the shell using the `admin` user to the `admin` database.
    Previously, we logged in to database using the `auth` method; in this case, we
    used the `-u` and `-p` options for providing the username and the password. We
    also provided the name of the database to connect to, which is admin in this case.
    Here, we are able to view the collections on the admin database and also execute
    admin operations like getting the server status. Executing the `db.serverStatus`
    call is possible as the user is given the `clusterAdmin` role.
  prefs: []
  type: TYPE_NORMAL
- en: One final thing to note, apart from writing to a collection, a user with write
    privileges can also create indexes on the collection in which he has write access.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we saw how we can create different users and what permissions
    they have restricting some set of operations. In the following recipe, we will
    see how we can have authentication done at process level. That is, how can one
    mongo instance authenticate itself for being added to a replica set.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MongoDB comes with a lot of built-in user roles with various privileges associated
    to each of them. Refer to the following URL to get details of various in built
    roles: [http://docs.mongodb.org/manual/reference/built-in-roles/](http://docs.mongodb.org/manual/reference/built-in-roles/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MongoDB also supports custom user roles. Refer to the following URL for knowing
    more about defining custom user roles: [http://docs.mongodb.org/manual/core/authorization/#user-defined-roles](http://docs.mongodb.org/manual/core/authorization/#user-defined-roles).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interprocess security in Mongo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we saw how authentication can be enforced for user to
    be logged in before allowing any operations on Mongo. In this recipe, we will
    look at interprocess security. By the term interprocess security, we don't mean
    to encrypt the communication but only to ensure that the node being added to a
    replica set is authenticated before being added to the replica set.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will start multiple mongo instances as part of a replica
    set and thus you might have to refer to the recipe *Starting multiple instances
    as part of a replica set* from [Chapter 1](ch01.html "Chapter 1. Installing and
    Starting the Server"), *Installing and Starting the Server* if you are not aware
    of how to start a replica set. Apart from that, in this recipe, all we would be
    looking at how to generate key file to be used and the behavior when an unauthenticated
    node is added to the replica set.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To set the ground, we would be starting three instances, each listening to port
    `27000`, `27001`, and `27002`, respectively. The first two would be started by
    providing it a path to the key file and the third wouldn't be. Later, we will
    try to add these three instances to the same replica set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s generate key the key file first. There is nothing spectacular about
    generating the key file. This is as simple as having a file with 6 to 1024 characters
    from the `base64` character set. On Linux filesystem, you may choose to generate
    pseudo random bytes using `openssl` and encode them to `base64`. The following
    command will generate 500 random bytes and those bytes will then be `base64` encoded
    and written to the file `keyfile`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'On a Unix filesystem, the key file should not have permissions for world and
    group. Thus, we should do the following after it is created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Not giving write permission to the creator ensures that we don''t overwrite
    the contents accidently. On Windows platform, however, `openssl` doesn''t come
    out of the box and thus you have to download it, the archive extracted, and the
    `bin` folder added to the OS path variable. For Windows, we can download it from
    the following URL: [http://gnuwin32.sourceforge.net/packages/openssl.htm](http://gnuwin32.sourceforge.net/packages/openssl.htm).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You may even choose not to generate the key file using the approach mentioned
    here (using `openssl`) and can take an easy way out by just typing in plain text
    in the key file from any text editor or your choice. However, note that the characters
    `\r`, `\n`, and spaces are stripped off by mongo and the remainder text is considered
    as the key. For example, we may create a file with the following content added
    to the key file. Again, the file will be named `keyfile` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Using any approach mentioned here, we must not have a `keyfile` in place that
    would be used for next steps of the recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now secure the mongo processes by starting the mongo instance as follows.
    I will start the following on windows, and my key file ID is named `keyfile` and
    is placed on `c:\MongoDB`. The data path is `c:\MongoDB\data\c1, c:\MongoDB\data\c2`,
    and `c:\MongoDB\data\c3` for the three instances, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Start the first instance listening to port `27000` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, start the second server listening to port `27001` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The third instance would be started but without the `--auth` and the `--keyFile`
    option listening to port `27002` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We then start a mongo shell and connect it to port `27000`, which is the first
    instance started. From the mongo shell, we type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'In few seconds, the replica set would be initiated with just one instance in
    it. We will now try to add two new instances to this replica set. First, add the
    one listening on port `27001` as follows (you will need to add the appropriate
    hostname, `Amol-PC` is the hostname in my case):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: We will execute `rs.status()` command to see the status of our replica set.
    In the command's output, we should see our newly added instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will now finally try and add an instance that was started without the `--auth`
    and the `--keyFile` option as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: This should add the instance to the replica set, but using `rs.status()` will
    show the status of the instance as UNKNOWN. The server logs for the instance running
    on `27002` too should show some authentication errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We would finally have to restart this instance; however, this time we provide
    the `--auth` and the `--keyFile` option as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Once the server is started, connect to it from the shell again and type in `rs.status()`
    in few moments, it should come up as a secondary instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we saw interprocess security for preventing unauthenticated
    nodes from being added to the mongo replica set. We still haven't secured the
    transport by encrypting the data that is being sent over the wire. In the *Appendix*,
    we will show how to build the mongo server from the source and how to enable encryption
    of the contents over the wire.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying collection behavior using the collMod command
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a command that would be executed to change the behavior of a collection
    in mongo. It could be thought of as a *collection modify* operation (officially,
    it is not mentioned anywhere though).
  prefs: []
  type: TYPE_NORMAL
- en: For a part of this recipe, knowledge of TTL indexes is required.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will execute the `collMod` operation on a collection. We
    need to start a standalone server listening to any port for client connections;
    in this case, we will stick to the default `27017`. If you are not aware how to
    start a standalone server, refer to *Installing single node MongoDB* in [Chapter
    1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing and
    Starting the Server*. We also need to start a shell that would be used for this
    administration. It is highly recommended to take a look at the recipes *Expiring
    documents after a fixed interval using the TTL index* and *Expiring documents
    at a given time using the TTL index* in [Chapter 2](ch02.html "Chapter 2. Command-line
    Operations and Indexes"), *Command-line Operations and Indexes* if you are not
    aware of them.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This operation can be used to do a couple of things:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming we have a collection with TTL index, as we saw in [Chapter 2](ch02.html
    "Chapter 2. Command-line Operations and Indexes"), *Command-line Operations*,
    let us see the list indexes by executing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'To change the expiry to `800` ms from `300` ms, execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `collMod` command always has the following format: `{collMod : <name of
    the collection>, <collmod operation>}`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the index operation using `collMod` to modify the TTL index. If a TTL
    index is already created and the time to live needs to be changed after creation,
    we use the `collMod` command. This operation-specific field to the command is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The `keyPattern` is the field, of the collection, on which the TTL index is
    created and the `expireAfterSeconds` will contain the new time to be changed to.
    On successful execution, we should see the following in the shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Setting up MongoDB as a windows service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Windows services are long-running applications that run in background, similar
    to daemon threads. Databases are good candidates for such type of services, whereby
    they would start and stop when the host machines starts and stops (you may, however,
    choose to manually start/stop a service). Many database vendors provide a feature
    to start the database as a service when installed on the server. MongoDB lets
    you do that as well and this is what we will see in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Installing single node MongoDB with options from the config
    file* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server* for getting information on how to start a
    MongoDB server using an external configuration file. Since mongo is run as a service
    in this case, it cannot be provided with command-like arguments and configuring
    it from configuration file is the only alternative. Refer to the prerequisites
    of the *Installing single node MongoDB* recipe in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server*, which is all
    we would need for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first create a config file with three configuration values the `port`,
    `dbpath`, and the `logpath` file. We name the file `mongo.conf` and keep it at
    location `c:\conf\mongo.conf` with the following three entries in it (you may
    choose any path for config file location, database and logs):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following from the windows terminal, which you may need to execute
    as an administrator. On Windows 7, the following steps were executed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press the Windows key on your keyboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Search programs and files space, type `cmd`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the programs, the command prompt program will be seen; right-click on it
    and select **Run as administrator**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the shell, execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: The log printed out on the console should confirm that the service is installed
    properly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The service can be started as follows from the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The service can be stopped as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Type in `services.msc` in the Run window (Windows button + *R*). In the management
    console, search for MongoDB service. We should see it as follows:![How to do it…](img/B04831_04_01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The service is automatic, that is, it will be started when the operating system
    starts. It can be changed to manual by right-clicking on it and selecting **Properties**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To remove a service, we need to execute the following from the command prompt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'There are more options available that can be used to configure the name of
    the service, display name, description, and the user account used to run the service.
    These can be provided as command-line arguments. Execute the following to see
    the possible options and take a look at the **Windows Service Control Manager**
    options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Replica set configurations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have had a good discussion on what replica set is in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* in the recipe *Starting multiple instances as part of a replica set*,
    and we saw how to start a simple replica set. In the recipe *Interprocess security
    in Mongo* in this chapter, we saw how to start a replica set with interprocess
    authentication. To be honest, that is pretty much what we do in setting up a standard
    replica set. There are a few configurations that one must know and should be aware
    of how it affects the replica set's behavior. Note that we still are not discussing
    tag aware replication in this recipe and it would be taken up later in this chapter
    as a separate recipe *Building tagged replica sets*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting multiple instances as part of a replica set* in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for the prerequisites and know about the replica set
    basics. Go ahead and set up a simple three-node replica set on your computer as
    mentioned in the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Before we go ahead with the configurations, we will see what elections are in
    a replica set and how they work from a high level. This is good to know about
    elections because some of the configuration options affect the voting process
    in the elections.
  prefs: []
  type: TYPE_NORMAL
- en: Elections in a replica set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mongo replica set has a single primary instance and multiple secondary instances.
    All database writes happen only through the primary instance and are replicated
    to the secondary instances. Read operations can happen from secondary instances
    depending on the read preference. Refer to the *Understanding ReadPreference for
    querying* in the [Appendix](apa.html "Appendix A. Concepts for Reference") to
    know what read preference is. If, however, the primary goes down or is not reachable
    for some reason, the replica set becomes unavailable for writes. MongoDB replica
    set has a feature to automatically failover to a secondary, by promoting it to
    a primary and make the set available to clients for both read and write operations.
    The replica set remains unavailable for that brief moment till a new primary comes
    up.
  prefs: []
  type: TYPE_NORMAL
- en: It all sounds good but the question is, who decides upon who the new primary
    instance would be? The process of choosing a new primary happens through an election.
    Whenever any secondary detects that it cannot reach out to a primary, it asks
    all replica set nodes in the instance to elect itself as the new primary.
  prefs: []
  type: TYPE_NORMAL
- en: 'All other nodes in the replica set who receive this request for election of
    primary will perform certain checks before they vote a Yes to the secondary requesting
    an election:'
  prefs: []
  type: TYPE_NORMAL
- en: They would first check if the existing primary is reachable. This is necessary
    because the secondary requesting the re-election is not able to reach the primary
    possibly because of a network partition in which case it should not be allowed
    to become a primary. In such case the instance receiving the request will vote
    a No.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Secondly, the instance would check the state of replication of itself with the
    secondary requesting the election. If it finds that the requesting secondary is
    behind itself in the replicated data, it would vote a No.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the primary is not reachable, but some instance with priority higher
    than the secondary requesting the re-election, is reachable from it. This is possible
    if the secondary requesting the re-election can't reach out to the secondary with
    higher priority possibly due to a network partition. In this scenario the instance
    receiving the request for election would vote a No.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding checks are pretty much what would be happening (not necessarily
    in the order mentioned previously) during the re-election; if these checks pass,
    the instance votes a Yes.
  prefs: []
  type: TYPE_NORMAL
- en: The election is void even if a single instance votes No. However, if none of
    the instances have voted a No, then the secondary that requests the election would
    become a new primary if it receives a Yes from majority of instances. If the election
    becomes void, there would be a re-election with the same secondary or any other
    instance requesting an election with the aforementioned process till a new primary
    is elected.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an idea about the elections in replica set and the terminologies,
    let's look at some of the replica set configurations. Few of these options are
    related to votes and we start by looking at these options first.
  prefs: []
  type: TYPE_NORMAL
- en: Basic configuration for a replica set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From the first chapter when we set up a replica set, we had a configuration
    similar to the following one. The basic replica set configuration for a three
    member set is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'We would not be repeating the entire configuration in all the steps in the
    following sections. All the flags we would be mentioning would be added to the
    document of a particular member in the members array. In the preceding example,
    if node with `_id` as `2` is to be made arbiter, we would be having the following
    configuration for it in the configuration document shown previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Generally, the steps to reconfigure an existing replica set are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Assign the configuration document to a variable. If the replica set is already
    configured, it can be obtained using the `rs.conf()` call from the shell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'The members field in the document is an array of documents for each individual
    member of a replica set. To add a new property to a particular member, we do the
    following. For instance, if we want to add the `votes` key and set its value to
    `2` for the third member of the replica set (index 2 in the array), we would do
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Just changing the JSON document won''t change the replica set. We need to reconfigure
    it if the replica set is already in place, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'If the configuration is done for the first time, we would call the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: For all the steps given next, you need to follow the preceding steps to reconfigure
    or initiate the replica set unless some other steps are mentioned explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will look at some of the possible configurations that can
    be done in a replica set. The explanation will be minimal with all the explanation
    done in the next section, as usual.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first configuration is `arbiterOnly` option. It is used to configure a
    replica set member as a member that holds no data but only has rights to vote.
    The following key need to be added to the configuration of the member who would
    be made an arbiter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'One thing to remember regarding this configuration is that once a replica set
    is initiated, no existing member can be changed to an arbiter from a non-arbiter
    node and vice versa. We can, however, add arbiter to an existing replica set using
    the helper function `rs.addArb(<hostname>:<port>)`. For example, add an arbiter
    listening to port `27004` to an existing replica set. The following was done on
    my machine to add an arbiter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: When the server is started to listen to port `27004` and `rs.status()` is executed
    from the mongo shell, we should see that the `state` and the `strState` for this
    member is `7` and `ARBITER`, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next option `votes` affects the number of votes a member has in the election.
    By default, all members have one vote each, this option can be used to change
    the number of votes a particular member has. It can be set as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Votes of existing members of a replica set can be changed and the replica set
    can be reconfigured using the `rs.reconfig()` helper.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Though the option `votes` is available, which can potentially change the number
    of votes to form a majority, it usually doesn't add much value and not a recommended
    option to use in production.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next replica set configuration option is called the `priority`. It determines
    the eligibility of a replica set member to become a primary (or not to become
    a primary). The option is set as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Higher number indicates more likely hood of becoming a primary, the primary
    would always be the one with the highest priority amongst the members alive in
    a replica set. Setting this option in an already configured replica set will trigger
    an election.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting the priority to `0` will ensure that a member will never become primary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next option we would be looking at is `hidden`. Setting the value of this option
    to true ensures that the replica set member is hidden. The option is set as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: One thing to keep in mind is that when the replica set member is hidden, its
    priority too should be made `0` to ensure it doesn't become primary. Though this
    seems redundant; as of the current version, the value or priority needs to be
    set explicitly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When a programming language client connects to a replica set, it would not be
    able to discover hidden members. However, after using `rs.status()` from the shell,
    the member's status would be visible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s look at the `slaveDelay` option now. This option is used to set lag
    in time for the slave from the primary of the replica set. The option is set as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: Like the hidden member, slave delayed members too should have the priority set
    to `0` to ensure they don't ever become primary. This needs to be set explicitly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s look at the final configuration option: `buildIndexes`. This value if
    not specified by default, is true, which indicates if an index is created on the
    primary, it needs to be replicated on the secondary too. The option is set as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: When using this option with a value set to false, the priority is set to `0`
    to ensure they don't ever become primary. This needs to be set explicitly. Also,
    this option cannot be set after the replica set is initiated. Just like an arbiter
    node, this needs to be set when the replica set is being created or when a new
    member node is being added to the replica set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will explain and understand the significance of different
    types of members and the configuration options we saw in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Replica set member as an arbiter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The English meaning of the word *arbiter* is a judge who resolves a dispute.
    In the case of replica sets, the arbiter node is present just to vote in case
    of elections and not replicate any data. This is in fact, a pretty common scenario
    due to a fact that that a Mongo replica set needs to have at least three instances
    (and preferably odd number of instances, 3 or more). A lot of applications do
    not need to maintain three copies of data and are happy with just two instances,
    one primary and a secondary with the data.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the scenario where only two instances are present in the replica set.
    When the primary goes down, the secondary instance cannot form a proper majority
    because it only has 50 percent votes (its own vote) and thus cannot become a primary.
    If a majority of secondary instances goes down, then the primary instance steps
    down from primary and becomes a secondary, thus making the replica set unavailable
    for writes. Thus, a two-node replica set is useless as it doesn't stay available
    even when any of the instances goes down. It defeats the purpose of setting up
    a replica set and thus at least three instances are needed in a replica set.
  prefs: []
  type: TYPE_NORMAL
- en: Arbiters come handy in such scenarios. We set up a replica set instance with
    three instances with only two having data and one acting as an arbiter. We need
    not maintain three copies of data at the same time we eliminate the problem we
    faced by setting up a two-instance replica set.
  prefs: []
  type: TYPE_NORMAL
- en: Priority of replica set members
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The priority flag can be used by itself or in conjunction with other options
    like `hidden`, `slaveDelay`, and `buildIndexes`, where we don't want the member
    with one of these three options to be ever made primary. We will look at these
    options soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some more possible use cases where we would never want a replica set to become
    a primary are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: When the hardware configuration of a member would not be able to deal with the
    write and read requests should it become a primary and the only reason it is being
    put in there is for replicating the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a multi data centers setup where one replica set instance is present
    in another data center for the sake of geographically distributing the data for
    DR purposes. Ideally, the network latency between the application server hosting
    the application and the database should be minimal for optimum performance. This
    could be achieved if both the servers (application server and the database server)
    are in the same data center. Not changing the priority of the replica set instance
    in another data center makes it equally eligible for being chosen as a primary
    and thus compromising on the application's performance if the server in other
    data center gets chosen as primary. In such scenarios, we can set the priority
    to be `0` for the server in the second data center and a manual cutover would
    be needed by the administrator to fail over to another data center should an emergency
    arise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both scenarios mentioned here, we could also have the respective members
    hidden so that the application client doesn't have a view of these members in
    the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to setting a priority to `0` for not allowing one to be primary, we
    can also be biased to one member to be primary whenever it is available by setting
    its priority to a value greater than 1, because the default value of priority
    is `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a scenario for budget reasons we have one of the members storing
    data on SSDs and remaining on spinning disks. We would ideally want the member
    with SSDs to be the primary whenever it is up and running. It is only when it
    is not available we would want another member to become a primary, In such scenarios
    we can set the priority of the member running on SSD to a value greater than 1\.
    The value doesn't really matter as long as it is greater than the rest, that is,
    setting it to `1.5` or `2` makes no difference as long as priority of other members
    is less.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden, slave delayed, and build index configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The term hidden for a replica set node is from an application client that is
    connected to the replica set and not for an administrator. For an administrator,
    the hidden members are equally important to be monitored and thus their state
    is seen in the `rs.status()` response. Hidden members participate in elections
    too like all other members.
  prefs: []
  type: TYPE_NORMAL
- en: For the `slaveDelay` option, most common use case is to ensure that the data
    in a member as a particular point of time lags behind the primary by the provided
    number of seconds and can be restored in case some unforeseen error has happened,
    say a human error for erroneously updating some data. Remember, longer the time
    delay, more is the time we get recover but at the cost of possibly stale data.
  prefs: []
  type: TYPE_NORMAL
- en: The `buildIndexes` option is useful in cases where we have a replica set member
    with non-production standard hardware and the cost of maintaining the indexes
    are not worth it. You may choose to set this option for members where no queries
    are executed on it. Obviously, if you set this option it can never become a primary
    member, and thus the priority option is forced to be set to `0`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can achieve some interesting things using tags in replica sets. This would
    be discussed in a later recipe, after we learn about tags in the recipe *Building
    tagged replica sets*.
  prefs: []
  type: TYPE_NORMAL
- en: Stepping down as primary from the replica set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are times when for some maintenance activity during business hours we
    would like to take a server out from the replica set, perform the maintenance
    and put it back in the replica set. If the server to be worked upon is the primary,
    we somehow need to step down from the primary member position, perform re-election
    and ensure that it doesn't get re-elected for a minimum given time frame. After
    the server becomes secondary once the step down operation is successful, we can
    take it out of the replica set, perform the maintenance activity and put it back
    in the replica set.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting multiple instances as part of a replica set* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for the prerequisites and know about the replica set
    basics. Set up a simple three-node replica set on your computer, as mentioned
    in the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assuming at this point of time we have a replica set up and running, do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following from the shell connected to one of the replica set members
    and see which instance currently is the primary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Connect to that primary instance from the mongo shell and execute the following
    on the shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'The shell should reconnect again and you should see that the instance connected
    to and initially a primary instance now becomes secondary. Execute the following
    from the shell so that a new primary is now re-elected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: You can now connect to the primary, modify the replica set configuration and
    go ahead with the administration on the servers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The preceding steps mentioned are pretty simple, but there are a couple of interesting
    things that we will see.
  prefs: []
  type: TYPE_NORMAL
- en: The method we saw previously, `rs.stepDown()` did not have any parameters. The
    function can in fact take a numeric value, which is the number of seconds for
    which the instance stepped down won't participate in the elections and won't become
    a primary and the default value for this is `60` seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting thing to try out is what if the instance that was asked
    to step down has a higher priority than other instances. Well, it turns out that
    the priority doesn't matter when you step down. The instance stepped down will
    not become primary no matter what for the provided number of seconds. However,
    if priority is set for the instance stepped down and it is higher than others,
    then after the time given to `stepDown` elapses an election will happen and the
    instance with higher priority will become primary again.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the local database of a replica set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore the local database from a replica set's perspective.
    The local database may contain collections that are not specific to replica sets,
    but we will focus only on the replica set specific collections and try to take
    a look at what's in them and what they mean.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting multiple instances as part of a replica set* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for the prerequisites and know about the replica set
    basics. Go ahead and set up a simple three-node replica set on your computer,
    as mentioned in the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the replica set up and running, we need to open a shell connected to the
    primary. You may connect randomly to any one member; use `rs.status()` and then
    determine the primary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With shell open, first switch to `local` database and the view the collections
    in the `local` database as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'You should find a collection called `me`. Querying this collection should show
    us one document and it contains the hostname of the server to which we are currently
    connected to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: There would be two fields, the hostname and the `_id` field. Take a note of
    the `_id` field—it is important.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Take a look at the `replset.minvalid` collection. You will have to connect
    to a secondary member from the shell to execute the following query. Switch to
    the `local` database first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: This collection just contains the single document with a key `ts` and a value
    that is the timestamp till the time the secondary we are connected to is synchronized.
    Note down this time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From the shell in primary, insert a document in any collection. We will use
    the database as test. Execute the following from the shell of the primary member:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Query the secondary again, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: We should see that the time against the field `ts` has now incremented corresponding
    to the time this replication happened from primary to secondary. With a slave
    delayed node, you will see this time getting updated only after the delay period
    has elapsed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we will see the collection `system.replset`. This collection is the
    place where the replica set configuration is stored. Execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Actually, when we execute `rs.conf()`, the following query gets executed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The database local is a special (non-replicated) database that is used to hold
    the replication and instance specific details in it. Try creating a collection
    of your own in the local database and insert some data in it; it would not be
    replicated to the secondary nodes.
  prefs: []
  type: TYPE_NORMAL
- en: This database gives us some view of the data stored by mongo for internal use.
    However, as an administrator, it is good to know about these collections and the
    type of data in it.
  prefs: []
  type: TYPE_NORMAL
- en: Most the collections are pretty straightforward. From the shell of the secondary
    execute the query `db.me.findOne()` in the local database and we should see that
    `_id` there should match the `_id` field of the document present in the slaves
    collection.
  prefs: []
  type: TYPE_NORMAL
- en: The config document we see gives the hostname of the secondary instance that
    we are referring to. Note that the port and other configuration options of the
    replica set member are not present in this document. Finally, the `syncedTo` time
    tells us till what time the secondary instances are synced up with the primary.
    We saw the collection `replset.minvalid` on the secondary, which tells us the
    time till which it is synced with primary. This value in the `syncedTo` time on
    primary would be same as in `replset.minvalid` on respective secondary.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have not seen the oplog, which is interesting to look at. We would take a
    look at this special collection in a separate recipe, *Understanding and analyzing
    oplogs*.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and analyzing oplogs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Oplog is a special collection and forms the backbone of the MongoDB replication.
    When any write operation or configuration changes are done on the replica set's
    primary, they are written to the oplog on the primary. All the secondary members
    then tail this collection to get the changes to be replicated. Tailing is synonymous
    to tail command in Unix and can only be done on a special type of collection called
    capped collection. Capped collections are fixed size collections which maintain
    the insertion order just like a queue. When the collection's allocated space becomes
    full, the oldest data is overwritten. If you are not aware of capped collections
    and what tailable cursors are, please refer to *Creating and tailing a capped
    collection cursors in MongoDB* in [Chapter 5](ch05.html "Chapter 5. Advanced Operations"),
    *Advanced Operations* for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Oplog is a capped collection present in the non-replicated database called **local**.
    In our previous recipe, we saw what a `local` database is and what collections
    are present in it. Oplog is something we didn't discuss in last recipe, as it
    demands a lot more explanation and a dedicated recipe is needed to do justice.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting multiple instances as part of a replica set* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for the prerequisites and know about the replica set
    basics. Go ahead and set up a simple three-node replica set on your computer as
    mentioned in the recipe. Open a shell and connect to the primary member of the
    replica set. You will need to start the mongo shell and connect to the primary
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Execute the following steps after connecting to a primary from the shell to
    get the timestamp of the last operation present in the oplog. We would be interested
    in looking at the operations after this time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following from the shell. Keep the output in the shell or copy
    it to some place. We will analyze it later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'Insert 10 documents as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following update to set a string value for all documents with value
    of `i` greater than `5`, which is 6, 7, 8 and 9 in our case. It would be a multiupdate
    operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create the index as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following query on oplog:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For those aware of messaging and its terminologies, Oplog can be looked at as
    a topic in messaging world with one producer, the primary instance, and multiple
    consumers, the secondary instances. Primary instance writes to an oplog all the
    contents that need to be replicated. Thus, any create, update, and delete operations
    as well as any reconfigurations on the replica sets would be written to the oplog
    and the secondary instances would tail (continuously read the contents of the
    oplog being added to it, similar to a tail with `-f` option command in Unix) the
    collection to get documents written by the primary. If the secondary has a `slaveDelay`
    configured, it will not read documents more than the maximum time minus the `slaveDelay`
    time from the oplog.
  prefs: []
  type: TYPE_NORMAL
- en: We started by saving an instance of the local database in the variable called
    `local` and identified a cutoff time that we would use for querying all the operations
    we will perform in this recipe from the oplog.
  prefs: []
  type: TYPE_NORMAL
- en: Executing a query on the `system.namespaces` collection in the local database
    shows us that the collection is a capped collection with a fixed size. For performance
    reasons capped collections are allocated continuous space on the filesystem and
    are preallocated. The size allocated by the server is dependent on the OS and
    CPU architecture. While starting the server the option `oplogSize` can be provided
    to mention the size of the oplog. The defaults are generally good enough for most
    cases. However, for development purpose, you can choose to override this value
    for a smaller value. Oplogs are capped collections that need to be preallocated
    a space on disk. This preallocation not only takes time when the replica set is
    first initialized but takes up a fixed amount of disk space. For development purpose,
    we generally start multiple MongoDB processes as part of the same replica set
    on same machine and would want them to be up and running as quickly as possible
    with minimum resource usage. Also, having the entire oplog in memory becomes possible
    if the oplog size is small. For all these reasons, it is advisable to start the
    local instances for development purpose with a small oplog size.
  prefs: []
  type: TYPE_NORMAL
- en: 'We performed some operations such as insert 10 documents and update four documents
    using a multi update and create an index. If we query the oplog for entries after
    the cutoff, we computed earlier we see 10 documents for each insert in it. The
    document looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, we first look at the three fields: `op`, `ns`, and `o`. These
    stand for the operation, the fully qualified name of the collection into which
    the data is being inserted, and the actual object to be inserted. The operation
    `i` stand for insert operation. Note that the value of `o`, which is the document
    to be inserted, contains the `_id` field that got generated on the primary. We
    should see 10 such documents, one for each insert. What is interesting to see
    is what happens on a multi update operation. The primary puts four documents,
    one for each of them affected for the updates. In this case, the value `op` is
    `u`, which is for update and the query used to match the document is not the same
    as what we gave in the update function, but it is a query that uniquely finds
    a document based on the `_id` field. Since there is an index already in place
    for the `_id` field (created automatically for each collection), this operation
    to find the document to be updated is not expensive. The value of the field `o`
    is the same document we passed to the update function from the shell. The sample
    document in the oplog for the update is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: The update in the oplog is the same as the one we provided. This is because
    the `$set` operation is idempotent, which means you may apply an operation safely
    any number of times.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, update using `$inc` operator is not idempotent. Let''s execute the
    following update:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the oplog would have the following as the value of `o`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: This non-idempotent operation is put into oplog by Mongo smartly as an idempotent
    operation with the value of i set to a value that is expected to be after the
    increment operation once. Thus it is safe to replay an oplog any number of times
    without corrupting the data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can see that the index creation process is put in the oplog as an
    insert operation in the `system.indexes` collection. For large collections, index
    creation can take hours and thus the size of the oplog is very important to let
    the secondary catch up from where it hasn't replicated since the index creation
    started. However, since version 2.6, index creation initiated in background on
    primary will also be built in background on secondary instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details on the index creation on replica sets, visit the following
    URL: [http://docs.mongodb.org/master/tutorial/build-indexes-on-replica-sets/](http://docs.mongodb.org/master/tutorial/build-indexes-on-replica-sets/).'
  prefs: []
  type: TYPE_NORMAL
- en: Building tagged replica sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*, we saw how to set up a simple replica in *Starting multiple
    instances as part of a replica set* and saw what is the purpose of a replica set.
    We also have a good deal of explanation on what `WriteConcern` is in the [Appendix](apa.html
    "Appendix A. Concepts for Reference") of the book and why it is used. What we
    saw about write concerns is that it offers a minimum level guarantee for a certain
    write operation. However, with the concept of tags and write concerns, we can
    define a variety of rules and conditions which must be satisfied before a write
    operation is deemed successful and a response is sent to the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider some common use cases such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Application wants the write operation to be propagated to at least one server
    in each of its data center. This ensures that in event of a data center shutdown,
    other data centers will have the data that was written by the application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there are no multiple data centers, at least one member of a replica set
    is kept on different rack. For instance, if the rack's power supply goes down,
    the replica set will still be available (not necessarily for writes) as at least
    one member is running on a different rack. In such scenarios, we would want the
    write to be propagated to at least two racks before responding back to the client
    with a successful write.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is possible that a reporting application queries a group of secondary of
    a replica set for generating some reports regularly. (Such secondary might be
    configured to never become a primary). After each write, we want to ensure that
    the write operation is replicated to at least one reporting replica member before
    acknowledging the write as successful.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding use cases are a few of the common use cases that arise and are
    not addressed using simple write concerns that we have seen earlier. We need a
    different mechanism to cater to these requirements and replica sets with tags
    is what we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, the next question is what exactly are tags? Let''s take an example
    of a blog. Various posts in the blog have different tags attached to them. These
    tags allow us to easily search, group, and relate posts together. Tags are some
    user defined text with some meaning attached to it. If we draw an analogy between
    the blog post and the replica set members, similar to how we attach tags to a
    post, we can attach tags to each replica set member. For example, in a multiple
    data center scenario with two replica set members in data center 1 (`dc1`) and
    one member in data center 2 (`dc2`), we can have the following tags assigned to
    the members. The name of the key and the value assigned to the tag is arbitrary
    and is chosen during design of the application; you may choose to even assign
    any tags like the administrator who set up the server if you really find it useful
    to address your use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Replica Set Member | Tag |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Replica set member 1 | `{''datacentre'': ''dc1'', ''rack'': ''rack-dc1-1''}`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Replica set member 2 | `{''datacentre'': ''dc1'', ''rack'': ''rack-dc1-2''}`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Replica set member 3 | `{''datacentre'': ''dc2'', ''rack'': ''rack-dc2-2''}`
    |'
  prefs: []
  type: TYPE_TB
- en: That is good enough to lay the foundation of what a replica set tags are. In
    this recipe, we will see how to assign tags to replica set members and more importantly,
    how to make use of them to address some of the sample use cases we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting multiple instances as part of a replica set from
    Chapter 1, Installing and Starting the Server* for the prerequisites and know
    about the replica set basics. Go ahead and set up a simple three-node replica
    set on your computer, as mentioned in the recipe. Open a shell and connect to
    the primary member of the replica set.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to know about write concerns, refer to the overview of write concerns
    in the [Appendix](apa.html "Appendix A. Concepts for Reference") of the book.
  prefs: []
  type: TYPE_NORMAL
- en: For inserting documents in the database, we will use Python as it gives us an
    interactive interface like the mongo shell. Refer to the recipe *Connecting to
    a single node using a Python client* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* for steps on how
    to install pymongo. Mongo shell would have been the most ideal candidate for the
    demonstration of the insert operations, but there are certain limitations around
    the usage of the shell with our custom write concern. Technically, any programming
    language with the write concerns mentioned in the recipe for insert operations
    would work fine.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the replica set started, we will add tags to it and reconfigure it as
    follows. The following commands are executed from the mongo shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'With the replica set tags set (not that we have not yet reconfigured the replica
    set), we need to define some custom write concerns. First, we define one that
    will ensure that the data gets replicated to at least to one server in each data
    center. Execute the following in the mongo shell again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the python shell and execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now execute the following insert:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: The preceding insert goes through successfully and the `ObjectId` would be printed
    out; you may query the collection to confirm from either the mongo shell or Python
    shell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since our primary is one of the servers in data centre `1`, we will now stop
    the server listening to port `27002`, which is the one with priority `0` and tagged
    to be in a different data center.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the server is stopped (you may confirm using the `rs.status()` helper
    function from the mongo shell), execute the following insert again, this insert
    should error out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: Restart the stopped mongo server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Similarly, we can achieve rack awareness by ensuring that the write propagates
    to at least two racks (in any data centre) by defining a new configuration as
    follows from the mongo shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'The settings value of the conf object would then be as follows. Once set, reconfigure
    the replica set again using `rs.reconfig(conf)` from the mongo shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: We saw `WriteConcern` used with replica set tags to achieve some functionality
    like data center and rack awareness. Let's see how we can use replica set tags
    with read operations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will see how to make use of replica set tags with read preference. Let's
    reconfigure the set by adding one more tag to mark a secondary member that will
    be used to execute some hourly stats reporting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Execute the following steps to reconfigure the set from the mongo shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: This will configure the same member with priority `0` and the one in a different
    data center with an additional tag called type with a value reports.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We now go back to the python shell and perform the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: The preceding execution should show us one document from the collection (as
    we has inserted data in this test collection in previous steps).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stop the instance which we have tagged for reporting, that is, the server listening
    to connections on port `27002` and execute the following on the python shell again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: This time around, the execution should fail and state that no secondary found
    with the required tag sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we did a lot of operations on tagged replica sets and saw how
    it can affect the write operations using `WriteConcern` and read operations using
    `ReadPreference`. Let's look at them in some details now.
  prefs: []
  type: TYPE_NORMAL
- en: WriteConcern in tagged replica sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We set up a replica set that was up and running, which we reconfigured to add
    tags. We tagged the first two servers in datacenter 1 and in different racks (servers
    running listening to port `27000` and `27001` for client connections) and the
    third one in datacenter 2 (server listening to port `27002` for client connections).
    We also ensured that the member in datacenter 2 doesn't become a primary by setting
    its priority to `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first objective is to ensure that the write operations to the replica set
    gets replicated to at least one member in the two datacenters. To ensure this,
    we define a write concern as follows `{''MultiDC'':{datacentre : 2}}`. Here, we
    first define the name of the write concern as MultiDC. The value which is a JSON
    object has one key with name datacenter, which is same as the key used for the
    tag we attached to the replica set and the value is a number `2`, which will be
    looked as the number of distinct values of the given tag that should acknowledge
    the write before it is deemed successful.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in our case, when the write comes to server 1 in datacenter 1,
    the number of distinct values of the tag datacenter is 1\. If the write operation
    gets replicated to the second server, the number still stays one as the value
    of the tag datacenter is same as the first member. It is only when the third server
    acknowledges the write operation, the write satisfies the defined condition of
    replicating the write to distinct two values of the tag *datacenter* in the replica
    set. Note that the value can only be a number and not have something like `{datacentre
    : ''dc1''}` this definition is invalid and an error will be thrown while re-configuring
    the replica set.'
  prefs: []
  type: TYPE_NORMAL
- en: But we need to register this write concern somewhere with the server. This is
    done in the final step of the configuration by setting the settings value in configuration
    JSON. The value to set is `getLastErrorModes`. The value of `getLastErrorModes`
    is a JSON document with all possible write concerns defined in it. We later defined
    one more write concern for write propagated to at least two racks. This is conceptually
    in line with MultiDC write concern and thus we will not be discussing it in details
    here. After setting all the required tags and the settings, we reconfigure the
    replica set for the changes to take effect.
  prefs: []
  type: TYPE_NORMAL
- en: Once reconfigured, we perform some write operations using the MultiDC write
    concern. When two members in two distinct datacenters are available, the write
    goes through successfully. However, when the server in second datacenter goes
    down, the write operation times out and throws an exception to the client initiating
    the write. This demonstrates that the write operation will succeed or fail as
    per how we intended.
  prefs: []
  type: TYPE_NORMAL
- en: We just saw how these custom tags can be used to address some interesting use
    cases, which are not supported by the product implicitly as far as write operations
    are concerned. Similar to write operations, read operations can take full advantages
    of these tags to address some use cases such as reading from a fixed set of secondary
    members that are tagged with a particular value.
  prefs: []
  type: TYPE_NORMAL
- en: ReadPreference in tagged replica sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We added another custom tag annotating a member to be used for reporting purposes,
    we then fire a query operation with the read preference to query a secondary and
    provide the tag sets that should be looked for before considering the member as
    a candidate for read operation. Remember that when using primary as the read preference,
    we cannot use tags and that is reason we explicitly specified the value of the
    `read_preference` to `SECONDARY`.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the default shard for non-sharded collections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the recipe *Starting a simple sharded environment of two shards* in [Chapter
    1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing and
    Starting the Server* we set up a simple two-shard server. In the recipe *Connecting
    to a shard in the shell and performing operations* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* we added data
    to a person collection that was sharded. However, for any collection that is not
    sharded, all the documents end up on one shard called the primary shard. This
    situation is acceptable for small databases with relatively small number of collections.
    However, if the database size increases and at the same time the number of un-sharded
    collections increase, we end up overloading a particular shard (which is the primary
    shard for a database) with a lot of data from these un-sharded collections. All
    query operations for such un-sharded collections as well as those on the collections
    whose particular range in the shard reside on this server instance will be directed
    to this it. In such scenario, we can have the primary shard of a database changed
    to some other instance so that these un-sharded collections get balanced out across
    different instances.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to view this primary shard and change it to
    some other server whenever needed.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Following the recipe *Starting a simple sharded environment of two shards* in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* set up and start a sharded environment. From the shell,
    connect to the started mongos process. Also, assuming that the two shards servers
    are listening to port `27000` and `27001`, connect from the shell to these two
    processes. So, we have a total of three shells opened, one connected to the mongos
    process and two to these individual shards.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need are using the `test` database for this recipe and sharding has to be
    enabled on it. If it not, then you need to execute the following on the shell
    connected to the mongos process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From the shell connected to the mongos process, execute the following two commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'In the databases, look out for `test` database and take a note of the `primary`.
    Suppose the following is a part (showing the part under databases only) of the
    output of `sh.status()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: The second document under the databases shows us that the database `test` is
    enabled for sharding (because partitioned is true) and the primary shard is `shard0000`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary shard, which is `shard0000` in our case, is the mongod process
    listening to port `27000`. Open the shell connected to this process and execute
    the following in it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, connect to another mongod process listening to port `27001` and again
    execute the following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: Note that the data would be found only on the primary shard and not on other
    shard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following command from the mongos shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following command from mongo shell connected to the mongos process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'From the shell connected to the mongos processes running on port `27000` and
    `27001`, execute the following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We started a sharded setup and connected to it from the mongos process. We started
    by inserting a document in the `testCol` collection that is not enabled for sharding
    in the test database, which is not enabled for sharding as well. In such cases,
    the data lies on shard called the **primary shard**. Do not misunderstand this
    for the primary of a replica set. This is a shard (that itself can be a replica
    set) and it is the shard chosen by default for all database and collection for
    which sharding is not enabled.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we add the data to a non-sharded collection, it was seen only on the shard
    that is primary. Executing `sh.status()` tells us the primary shard. To change
    the primary, we need to execute a command from the admin database from the shell
    connected to the mongos process. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: Once the primary shard was changed, all existing data of non-sharded database
    and collection was migrated to the new primary and all subsequent writes to non-sharded
    collections will go to this shard.
  prefs: []
  type: TYPE_NORMAL
- en: Use this command with caution as it will migrate all the unsharded collections
    to the new primary, which may take time for big collections.
  prefs: []
  type: TYPE_NORMAL
- en: Manual split and migration of chunks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though MongoDB does a good job of splitting and migrating chunks across shards
    to maintain the balance, under some circumstances such as a small number of documents
    or relatively large number of small documents where the automatic balancer doesn't
    split the collection, an administrator might want to split and migrate the chunks
    manually. In this recipe, we will see how to split and migrate the collection
    manually across shards. For this recipe, we will set up a simple shard as we saw
    in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting a simple sharded environment of two shards* in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* to set up and start a sharded environment. It is preferred
    to start a clean environment without any data in it. From the shell, connect to
    the started mongos process.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Connect to the mongos process from the mongo shell and enable sharding on the
    `test` database and the `splitAndMoveTest` collection as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s load the data in the collection as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the data is loaded, execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: Note the number of documents in two shards in the plan. The value to lookout
    for is in the two documents under the shards key in the result of explain plan.
    Within these two documents the field to lookout for is `n`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following to see the splits of the collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the chunk into two at `5000` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: 'Splitting it doesn''t migrate it to the second server. See what exactly happened
    with the chunks by executing the following query again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now move the second chunk to the second shard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following query again and confirm the migration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, the following explain plan will show a split of about 50-50:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We simulate a small data load by adding monotonically increasing numbers and
    discover that the numbers are not split across two shards evenly by viewing the
    query plan. It is not a problem as the chunk size needs to reach a particular
    threshold, 64 MB by default, before the balancer decides to migrate the chunks
    across the shards to maintain balance. This is pretty perfect as in real world,
    when the data size gets huge we will see that eventually over a period of time
    the shards are well balanced.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the administration does decide to split and migrate the chunks,
    it is possible to do it manually. The two helper functions `sh.splitAt` and `sh.moveChunk`
    are there to do this work. Let's look at their signatures and see what they do.
  prefs: []
  type: TYPE_NORMAL
- en: The function `sh.splitAt` takes two arguments, first is the namespace, which
    has the format `<database>.<collection name>` and the second parameter is the
    query that acts as the split point to split the chunk into two, possibly two uneven
    portions depending on where the given document is in the chunk. There is another
    method, `sh.splitFind`, which will try and split the chunk in two equal portions.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting doesn't mean the chunk moves to another shard, it just breaks one
    big chunk into two, but the data stays on the same shard. It is an inexpensive
    operation which involves updating the config DB.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we executed was to migrate the chunk to a different shard after we split
    it into two. The operation `sh.MoveChunk` is used just to do that. This function
    takes three parameters, first one is again the namespace of the collection that
    has the format `<database>.<collection name>`, second parameter is a query a document
    whose chunk would be migrated, and the third parameter is the destination chunk.
  prefs: []
  type: TYPE_NORMAL
- en: Once the migration is done, the query's plan shows us that the data is split
    in two chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Domain-driven sharding using tags
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recipes *Starting a simple sharded environment of two shards* and *Connecting
    to a shard in the shell and performing operations* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* explained how
    to start a simple two server shard and then insert data in a collection after
    choosing a shard key. The data that gets sharded is more technical where the data
    chunk is kept to a manageable size by Mongo by splitting it into multiple chunks
    and migrating the chunks across shards to keep the chunk distribution even across
    shards. But what if we want the sharding to be more domain oriented? Suppose we
    have a database for storing postal addresses and we shard based on postal codes
    where we know the postal code range of a city. What we can do is tag the shard
    servers according to the city name as the tag, add shard range (postal codes),
    and associate this range with the tag. This way, we can state which servers can
    contain the postal addresses of which cities. For instance, we know that Mumbai
    being most populous city, the number of addresses would be huge and thus we add
    two shards for Mumbai. On the other hand, one shard should be enough to cope up
    with the volumes of the Pune city. For now we tag just one shard. In this recipe,
    we will see how to achieve this use case using tag aware sharding. If the description
    is confusing, don't worry, we will see how to implement what we just discussed.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting a simple sharded environment of two shard* in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for information on how to start a simple shard. However,
    for this recipe, we will add an additional shard. So, we will now start three
    mongo servers listening to port `27000`, `27001`, and `27002`. Again, it is recommended
    to start off with a clean database. For the purpose of this recipe, we will be
    using the collection `userAddress` to store the data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assuming that we have three shard up and running, let''s execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'With tags defined, let''s define range of pin codes that will map to a tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: 'Enable sharding for the test database and the `userAddress` collection as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'Insert the following documents in the `userAddress` collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following plans:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we want to partition data driven by domain in a shard, we can use tag
    aware sharding. It is an excellent mechanism that lets us tag the shards and then
    split the data range across shards identified by the tags. We don't really have
    to bother about the actual machines and their address hosting the shard. Tags
    act as a good abstraction in the way, we can tag a shard with multiple tags and
    one tag can be applied to multiple shards.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we have three shards and we apply tags to each of them using the
    `sh.addShardTag` method. The method takes the shard ID, which we can see in the
    `sh.status` call with the *shards* key. This `sh.addShardTag` method can be used
    to keep adding tags to a shard. Similarly, there is a helper method `sh.removeShardTag`
    to remove an assignment of the tag from the shard. Both these methods take two
    parameters, the first one is the shard ID and second one of the tag to remove.
  prefs: []
  type: TYPE_NORMAL
- en: Once the tagging is done, we assign range of the values of the shard key to
    the tag. The method `sh.addTagRange` is used to do that. It accepts four parameters,
    first one is the namespace, which is the fully qualified name of the collection,
    second and third parameters are the start and end value of the range for this
    shard key and the fourth parameter is the tag name of the shards hosting the range
    being added. For example, the call `sh.addTagRange('test.userAddress', {pincode:400001},
    {pincode:400999}, 'Mumbai')` says we are adding the shard range `400001` to `400999`
    for the collection `test.userAddress`, and this range will be stored in the shards
    tagged as `Mumbai`.
  prefs: []
  type: TYPE_NORMAL
- en: Once the tagging and adding tag range is done, we enabled sharding on database
    and collection and add data to it from Mumbai and Pune city with respective pin
    codes. We then query and explain the plan to see that the data did indeed reside
    on the shards we have tagged for Pune and Mumbai city. We can also add new shards
    to this sharded setup and accordingly tag the new shard. The balancer will then
    accordingly balance the data based on the value it is tagged. For instance, if
    the addresses in Pune increase overloading a shard, we can add a new shard with
    tag as Pune. The postal address for Pune will then be sharded across these two
    server instances for tagged for Pune city.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the config database in a sharded setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Config database is the backbone of a sharded setup in Mongo. It stores all the
    metadata of the shard setup and has a dedicated mongod process running for it.
    When a mongos process is started we provide it with the config servers' URL. In
    this recipe, we will take a look at some collections in the config database and
    dive deep into their content and significance.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need a sharded setup for this recipe. Refer to the recipe *Starting a simple
    sharded environment of two shard* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* for information
    on how to start a simple shard. Additionally, connect to the mongos process from
    a shell.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From the console connected to the mongos process, switch to the config database
    and execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: 'From the list of all collections, we will visit a few. We start with the databases
    collection. This keeps a track of all the databases on this shard. Execute the
    following from the shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: The content of the result is pretty straightforward, the value of the field
    `_id` is for the database. The value of field partitioned tells us whether sharding
    is enabled for the database or not; true indicates it is enabled and the field
    primary gives the primary shard where the data of non-sharded collections reside
    upon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will visit the `collections` collection. Execute the following from
    the shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: This collection, unlike the databases collection we saw earlier, contains only
    those collections for which we have enabled sharding. The field `_id` gives the
    namespace of the collection in the `<database>.<collection name>` format, the
    field key gives the shard key and the field unique, indicates whether the shard
    key is unique or not. These three fields come as the three parameters of the `sh.shardCollection`
    function in that very order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we look at the `chunks` collection. Execute the following on the shell.
    If the database was clean when we started this recipe, we won''t have a lot of
    data in this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: 'We then look at the tags collection and execute the following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: Let's query the mongos collection as follows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: This is a simple collection that gives the list of all mongos instances connected
    to the shard with the details like the host and port on which the mongos instance
    is running, which forms the `_id` field. The version and figures like for how
    much time the process is up and running in seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we look at the version collection. Execute the following query. Note
    that is not similar to other queries we execute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We saw the collections and databases collection while we queried them and they
    are pretty simple. Let''s look at the collection called `chunks`. Here is a sample
    document from this collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: The fields of interest are `ns`, `min`, `max`, and `shard`, which are the namespace
    of the collection, the minimum value present in the chunk, the maximum value present
    in the chunk, and the shard on which this chunk lies, respectively. The value
    of the chunk size is 64 MB by default. This can be seen in the settings collection.
    Execute `db.settings.find()` from the shell and look at the value of the field
    value, which is the size of the chunk in MB. Chunks are restricted to this small
    size to ease the migration process across shards, if needed. When the size of
    the chunk exceeds this threshold, mongo server finds a suitable point in the existing
    chunk to break it into two and adds a new entry in this chunks collection. This
    operation is called splitting, which is inexpensive as the data stays where it
    is; it is just logically split into multiple chunks. The balancer on mongo tries
    to keep the chunks across shards balanced and the moment it sees some imbalance,
    it migrates these chunks to a different shard. This is expensive and also depends
    largely on the network bandwidth. If we use `sh.status()`, the implementation
    actually queries the collections we saw and prints the pretty formatted result.
  prefs: []
  type: TYPE_NORMAL
