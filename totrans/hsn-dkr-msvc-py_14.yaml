- en: Monitoring Logs and Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In real-life operations, the ability to quickly detect and debug a problem is
    critical. In this chapter, we will discuss the two most important tools we can
    use to discover what's happening in a production cluster processing a high number
    of requests. The first tool is logs, which help us to understand what's happening
    within a single request, while the other tool is metrics, which categorizes the
    aggregated performance of the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Observability of a live system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting problems through logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being proactive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you'll know how to add logs so that they are available
    to detect problems and how to add and plot metrics and understand the differences
    between both of them.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the example system for this chapter and tweaking it to include
    centralized logging and metrics. The code for this chapter can be found in this
    book''s GitHub repository: [https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter10](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter10).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the cluster, you need to build each individual microservice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The microservices in this chapter are the same ones that we introduced previously,
    but they add extra log and metrics configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to create the example namespace and start the Kubernetes cluster
    using the `find` configuration in the `Chapter10/kubernetes` subdirectory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To be able to access the different services, you need to update your `/etc/hosts`
    file so that it includes the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With that, you will be able to access the logs and metrics for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Observability of a live system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Observability is the capability of knowing what's going on in a live system.
    We can deal with low-observability systems, where we have no way of knowing what's
    going on, or high-observability systems, where we can infer the events and internal
    state from the outside through tools.
  prefs: []
  type: TYPE_NORMAL
- en: Observability is a property of the system itself. Typically, monitoring is the
    action of obtaining information about the current or past state of the system.
    It's all a bit of a naming debate, but you monitor the system to collect the observable
    parts of it.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, monitoring is easy. There are great tools out there that
    can help us capture and analyze information and present it in all kinds of ways.
    However, the system needs to expose the relevant information so that it can be
    collected.
  prefs: []
  type: TYPE_NORMAL
- en: Exposing the correct amount of information is difficult. Too much information
    will produce a lot of noise that will hide the relevant signal. Too little information
    will not be enough to detect problems. In this chapter, we will look at different
    strategies to combat this, but every system will have to explore and discover
    this on its own. Expect to experiment and make changes in your own system!
  prefs: []
  type: TYPE_NORMAL
- en: Distributed systems, such as the ones that follow a microservice architecture,
    also present problems as the complexity of the system can make it difficult to
    understand its internal state. Behavior can be also unpredictable in some circumstances.
    This kind of system at scale is inherently never completely healthy; there will
    always be minor problems here and there. You need to develop a priority system
    to determine what problems require immediate action and which ones can be solved
    at a later stage.
  prefs: []
  type: TYPE_NORMAL
- en: The main tools for the observability of microservices are **logs** and **metrics**.
    They are well-understood and used by the community, and there are plenty of tools
    that greatly simplify their usage, both as packages that can be installed locally
    and as cloud services that can help with data retention and the reduction of maintenance
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: Using cloud services for monitoring will save you from maintenance costs. We
    will talk about this later in the *Setting up logs* and *Setting up metrics* sections.
  prefs: []
  type: TYPE_NORMAL
- en: Another alternative when it comes to observability is services such as Data
    Dog ([https://www.datadoghq.com/](https://www.datadoghq.com/)) and New Relic ([https://newrelic.com/](https://newrelic.com/)).
    They receive events – normally logs – and are able to derive metrics from there.
  prefs: []
  type: TYPE_NORMAL
- en: The most important details of the state of the cluster can be checked through
    `kubectl`, as we saw in previous chapters. This will include details such as the
    versions that have been deployed, restarts, pulling images, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'For production environments, it may be good to deploy a web-based tool to display
    this kind of information. Check out Weave Scope, an open source tool that shows
    data in a web page similar to the one that can be obtained with `kubectl`, but
    in a nicer and more graphical way. You can find out more about this tool here:
    [https://www.weave.works/oss/scope/](https://www.weave.works/oss/scope/).'
  prefs: []
  type: TYPE_NORMAL
- en: Logs and metrics have different objectives, and both can be intricate. We will
    look at some common usages of them in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logs track unique events that occur in the system. Each log stores a message,
    which is produced when a specific part of the code is executed. Logs can be totally
    generic (*function X is called*) or include specific details (*function X is called
    with parameter A*).
  prefs: []
  type: TYPE_NORMAL
- en: The most common format for logs is to generate them as plain strings. This is
    very flexible, and normally log-related tools work with text searches.
  prefs: []
  type: TYPE_NORMAL
- en: Each log includes some metadata about who produced the log, what time it was
    created, and more. This is also normally encoded as text, at the beginning of
    the log. A standard format helps with sorting and filtering.
  prefs: []
  type: TYPE_NORMAL
- en: Logs also include a severity level. This allows for categorization so that we
    can capture the importance of the messages. The severity level can be, in order
    of importance, `DEBUG`, `INFO`, `WARNING`, or `ERROR`. This severity allows us
    to filter out unimportant logs and determine actions that we should take. The
    logging facility can be configured to set a threshold; less severe logs are ignored.
  prefs: []
  type: TYPE_NORMAL
- en: There are many severity levels, and you can define custom intermediate levels
    if you wish. However, this isn't very useful except in very specific situations.
    Later in this chapter, in the *Detecting problems through logs* section, we will
    describe how to set a strategy per level; too many levels can add confusion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a web service environment, most of the logs will be generated as part of
    the response for a web request. This means that a request will arrive at the system,
    be processed, and return a value. Several logs will be generated along the way.
    Keep in mind that, in a system under load, multiple requests will be happening
    simultaneously, so the logs from multiple requests will also be generated simultaneously.
    For example, note how the second log comes from a different IP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A common request ID can be added to group all the related logs that have been
    produced for a single request. We will see how to do this later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Each individual log can be relatively big and, in aggregate, use significant
    disk space. Logs can quickly grow out of proportion in a system under load. The
    different log systems allow us to tweak their retention time, which means that
    we only keep them for a certain amount of time. Finding the balance between keeping
    logs to see what happened in the past and using a sane amount of space is important.
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to check the retention policies when enabling any new log service, whether
    it be local or cloud-based. You won't be able to analyze what happened before
    the time window. Double-check that the progress rate is as expected – you don't
    want to find out that you went unexpectedly over quota while you were tracking
    a bug.
  prefs: []
  type: TYPE_NORMAL
- en: Some tools allow us to use raw logs to generate aggregated results. They can
    count the number of times a particular log appears and generate the average times
    per minute or other statistics. This is expensive, though, as each log takes space.
    To observe this aggregated behavior, it is better to use a specific metrics system.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metrics deal with aggregated information. They show information related not
    to a single event, but a group of them. This allows us to check the general status
    of the cluster in a better way than using logs.
  prefs: []
  type: TYPE_NORMAL
- en: We will use typical examples related to web services, mainly dealing with requests
    metrics, but don't feel restricted by them. You can generate your own metrics
    that are specific to your service!
  prefs: []
  type: TYPE_NORMAL
- en: Where a log keeps information about each individual event, metrics reduce the
    information to the number of times the event happens or reduce them to a value
    that can then be averaged or aggregated in a certain way.
  prefs: []
  type: TYPE_NORMAL
- en: This makes metrics much more lightweight than logs and allows us to plot them
    against time. Metrics present information such as the number of requests per minute,
    the average time for a request during a minute, the number of queued requests,
    the number of errors per minute, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The resolution of the metrics may depend on the tool that was used to aggregate
    them. Keep in mind that a higher resolution will require more resources. A typical
    resolution is 1 minute, which is small enough to present detailed information
    unless you have a very active system that receives 10 or more requests per second.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing and analyzing information related to performance, such as the average
    request time, allows us to detect possible bottlenecks and act quickly in order
    to improve the performance of the system. This is much easier to deal with on
    average since a single request may not capture enough information for us to see
    the big picture. It also helps us predict future bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many different kinds of metrics, depending on the tool that''s used.
    The most commonly supported ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Counter**: A trigger is generated each time something happens. This will
    be counted and aggregated. An example of this is the number of requests and the
    number of errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gauge**: A single number that is unique. It can go up or down, but the last
    value overwrites the previous. An example of this is the number of requests in
    the queue and the number of available workers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measure**: Events that have a number associated with them. These numbers
    can be averaged, summed, or aggregated in some way. Compared with gauges, the
    difference is that previous measures are still independent; for example, when
    we request time in milliseconds and request size in bytes. Measures can also work
    as counters since their number can be important; for example, tracking the request
    time also counts the number of requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two main ways in which metrics work:'
  prefs: []
  type: TYPE_NORMAL
- en: Each time something happens, an event gets *pushed* toward the metrics collector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each system maintains their own metrics, which are then *pulled* from the metrics
    system periodically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each way has its own pros and cons. Pushing events produces higher traffic as
    every event needs to be sent; this can cause bottlenecks and delays. Pulling events
    will only sample the information and miss exactly what happened between the samples,
    but it's inherently more scalable.
  prefs: []
  type: TYPE_NORMAL
- en: While both approaches are used, the trend is moving toward pulling systems for
    metrics. They reduce the maintenance that's required for pushing systems and are
    much more easier to scale.
  prefs: []
  type: TYPE_NORMAL
- en: We will set up Prometheus, which uses the second approach. The most used exponent
    of the first approach is Graphite.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics can also be combined to generate other metrics; for example, we can
    divide the number of requests that return errors by the total number of requests
    that generate error requests. Such derived metrics can help us present information
    in a meaningful way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple metrics can be displayed in dashboards so that we can understand the
    status of a service or cluster. At a glance, these graphical tools allow us to
    detect the general state of the system. We will set Grafana so that it displays
    graphical information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1d334374-d1df-4f9f-a7ac-07ccd296c87a.png)'
  prefs: []
  type: TYPE_IMG
- en: Compared to logs, metrics take up much less space and they can capture a bigger
    window of time. It's even possible to keep metrics for the system's life. This
    differs compared to logs, which can never be stored for that long.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will centralize all the logs that are generated by the system into a single
    pod. In local development, this pod will expose all the received logs through
    a web interface.
  prefs: []
  type: TYPE_NORMAL
- en: The logs will be sent over the `syslog` protocol, which is the most standard
    way of transmitting them. There's native support for `syslog` in Python, as well
    as in virtually any system that deals with logging and has Unix support.
  prefs: []
  type: TYPE_NORMAL
- en: Using a single container makes it easy to aggregate logs. In production, this
    system should be replaced with a container that relays the received logs to a
    cloud service such as Loggly or Splunk.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple `syslog` servers that are capable of receiving logs and aggregating
    them; `syslog-ng` ([https://www.syslog-ng.com/](https://www.syslog-ng.com/)) and
    `rsyslog` ([https://www.rsyslog.com/](https://www.rsyslog.com/)) are the most
    common ones. The simplest method is to receive the logs and to store them in a
    file. Let's start a container with an `rsyslog` server, which will store the received
    logs.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an rsyslog container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will create our own `rsyslog` server. This is a very simple
    container, and you can check `docker-compose` and `Dockerfile` on GitHub for more
    information regarding logs ([https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter10/kubernetes/logs](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter10/kubernetes/logs)).
  prefs: []
  type: TYPE_NORMAL
- en: We will set up logs using the UDP protocol. This is the standard protocol for
    `syslog`, but it's less common than the usual HTTP over TCP that's used for web
    development.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference is that UDP is connectionless, so the log is sent and no
    confirmation that it has been delivered is received. This makes UDP lighter and
    faster, but also less reliable. If there's a problem in the network, some logs
    may disappear without warning.
  prefs: []
  type: TYPE_NORMAL
- en: This is normally an adequate trade-off since the number of logs is high and
    the implications of losing a few isn't big. `syslog` can also work over TCP, thus
    increasing reliability but also reducing the performance of the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Dockerfile installs `rsyslog` and copies its configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration file mainly starts the server at port `5140` and stores the
    received files in `/var/log/syslog`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: With log rotation, we set a limit on the side of the `/var/log/syslog` file
    so that it doesn't grow without limits.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can build the container with the usual `docker-compose` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This will create a combination of a pod, a service, and an Ingress, as we did
    with the other microservices, to collect logs and allow external access from a
    browser.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the syslog pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `syslog` pod will contain the `rsyslog` container and another container
    to display the logs.
  prefs: []
  type: TYPE_NORMAL
- en: To display the logs, we will use front rail, an application that streams log
    files to a web server. We need to share the file across both containers in the
    same pod, and the simplest way to do this is through a volume.
  prefs: []
  type: TYPE_NORMAL
- en: We control the pod using a deployment. You can check the deployment configuration
    file at [https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter10/kubernetes/logs/deployment.yaml](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter10/kubernetes/logs/deployment.yaml).
    Let's take a look at its most interesting parts in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: log-volume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`log-volume` creates an empty directory that is shared across both containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This allows the containers to communicate while storing information in a file.
    The `syslog` container will write to it while the front rail one will read from
    it.
  prefs: []
  type: TYPE_NORMAL
- en: syslog container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `syslog` container starts an `rsyslogd` process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `rsyslogd -n -f /etc/rsyslog.d/rsyslog.conf` command starts the server with
    the configured file we described previously. The `-n` parameter keeps the process
    in the foreground, thereby keeping the container running.
  prefs: []
  type: TYPE_NORMAL
- en: The UDP port `5140`, which is the defined port to receive logs, is specified,
    and `log-volume` is mounted to `/var/log`. Later in the file, `log-volume` will
    be defined.
  prefs: []
  type: TYPE_NORMAL
- en: The front rail container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The front rail container is started from the official container image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We start it with the `frontrail /var/log/syslog` command, specify port `9001`
    (which is the one we use to access `frontrail`), and mount `/var/log`, just like
    we did with the `syslog` container, to share the log file.
  prefs: []
  type: TYPE_NORMAL
- en: Allowing external access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we did with the other microservices, we will create a service and an Ingress.
    The service will be used by other microservices so they can send their logs. The
    Ingress will be used to access the web interface so that we can see the logs as
    they arrive.
  prefs: []
  type: TYPE_NORMAL
- en: The YAML files are on GitHub ([https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter10/kubernetes/logs](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter10/kubernetes/logs))
    in the `service.yaml` and `ingress.yaml` files, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The service is very straightforward; the only peculiarity is that it has two
    ports – one TCP port and one UDP port – and each one connects to a different container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The Ingress only exposes the front rail port, which means we can access it
    through the browser. Remember that the DNS needs to be added to your `/etc/host`
    file, as described at the start of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Going to `http://syslog.example.local` in your browser will allow you to access
    the front rail interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f6ccd237-8812-48f4-90a1-b5c971772d3a.png)'
  prefs: []
  type: TYPE_IMG
- en: You can filter the logs using the box in the top-right corner.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that, most of the time, logs reflect the readiness and liveness probes,
    as shown in the preceding screenshot. The more health checks you have in your
    system, the more noise you'll get.
  prefs: []
  type: TYPE_NORMAL
- en: You can filter them out at the `syslog` level by configuring the `rsyslog.conf`
    file, but be careful not to leave out any relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we need to see how the other microservices configure and send their logs
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Sending logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to configure the microservices in uWSGI so that we can forward the logs
    to the logging service. We will use the Thoughts Backend as an example, even though
    the Frontend and Users Backend, which can be found under the `Chapter10/microservices`
    directory, also have this configuration enabled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `uwsgi.ini` configuration file ([https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter10/microservices/thoughts_backend/docker/app/uwsgi.ini](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter10/microservices/thoughts_backend/docker/app/uwsgi.ini)).
    You''ll see the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This sends the logs, in `rsyslog` format, toward the `syslog` service at port
    `5140`. We also add the *facility*, which is where the logs come from. This adds
    the string to all the logs coming from this service, which helps with sorting
    and filtering. Each `uwsgi.ini` file should have its own facility to help with
    filtering.
  prefs: []
  type: TYPE_NORMAL
- en: In old systems that support the `syslog` protocol, the facility needs to fit
    predetermined values such as `KERN`, `LOCAL_7`, and more. But in most modern systems,
    this is an arbitrary string that can take any value.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic logs by uWSGI are interesting, but we also need to set up our own
    logs for custom tracking. Let's see how.
  prefs: []
  type: TYPE_NORMAL
- en: Generating application logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Flask automatically configures a logger for the app. We need to add a log in
    the following way, as shown in the `api_namespace.py` file ([https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter10/microservices/thoughts_backend/ThoughtsBackend/thoughts_backend/api_namespace.py#L102](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter10/microservices/thoughts_backend/ThoughtsBackend/thoughts_backend/api_namespace.py#L102)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`app.logger` can call `.debug`, `.info`, `.warning`, or `.error` to generate
    a log. Note that `app` can be retrieved by importing `current_app`.'
  prefs: []
  type: TYPE_NORMAL
- en: The logger follows the standard `logging` module in Python. It can be configured
    in different ways. Take a look at the `app.py` file ([https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter10/microservices/thoughts_backend/ThoughtsBackend/thoughts_backend/app.py](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter10/microservices/thoughts_backend/ThoughtsBackend/thoughts_backend/app.py))
    to view the different configuration we'll be going through in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first level of logging goes through the default `dictConfig` variable.
    This variable is automatically defined by Flask and allows us to configure the
    logs in the way that''s defined in the Python documentation ([https://docs.python.org/3.7/library/logging.config.html](https://docs.python.org/3.7/library/logging.config.html)).
    You can check the definition of logging in the `app.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `dictConfig` dictionary has three main levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '`formatters`: This checks how the log is formatted. To define the format, you
    can use the automatic values that are available in the Python documentation ([https://docs.python.org/3/library/logging.html#logrecord-attributes](https://docs.python.org/3/library/logging.html#logrecord-attributes)).
    This gathers information for every log.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`handlers`: This checks where the log goes to. You can assign one or more to
    the loggers. We defined a handler called `wsgi` and configured it so that it goes
    up, toward uWSGI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`root`: This is the top level for logs, so anything that wasn''t previously
    logged will refer to this level. We configure the `INFO` logging level here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This sets up default configuration so that we don't miss any logs. However,
    we can create even more complex logging handlers.
  prefs: []
  type: TYPE_NORMAL
- en: Logging a request ID
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the problems when analyzing a large number of logs is correlating them.
    We need to see which ones are related to each other. One possibility is to filter
    logs by the pod that's generating them, which is stored at the start of the log
    (for example, `10-1-0-27.frontend-service.example.svc.cluster.local`). This is
    analogous to the host generating the logs. This process, however, is cumbersome
    and, in some cases, a single container can process two requests simultaneously.
    We need a unique identifier per request that gets added to all the logs for a
    single request.
  prefs: []
  type: TYPE_NORMAL
- en: To do so, we will use the `flask-request-id-header` package ([https://pypi.org/project/flask-request-id-header/](https://pypi.org/project/flask-request-id-header/)).
    This adds an `X-Request-ID` header (if not present) that we can use to log each
    individual request.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we set up a header instead of storing a randomly generated value in memory
    for the request? This is a common pattern that allows us to inject the request
    ID into the backend. The request ID allows us to carry over the same request identifier
    through the life cycle of a request for different microservices. For example,
    we can generate it on the Frontend and pass it over to the Thoughts Backend so
    that we can trace several internal requests that have the same origin.
  prefs: []
  type: TYPE_NORMAL
- en: Although we won't be including this in our example for simplicity, as a microservices
    system grows, this becomes crucial for determining flows and origins. Generating
    a module so that we can automatically pass it over internal calls is a good investment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the flow between a **frontend** and two services.
    Note that the `X-Request-ID` header is not set up for the **frontend** service
    upon arrival and that it needs to be forwarded to any call:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/158370df-bab7-416c-ab69-63d258408159.png)'
  prefs: []
  type: TYPE_IMG
- en: We need to also send the logs straight toward the `syslog` service so that we
    can create a handler that does this for us.
  prefs: []
  type: TYPE_NORMAL
- en: When executing code from a script, compared to running the code in a web server,
    we don't use this handler. When running a script directly, we want our logs to
    go to the default logger we defined previously. In `create_app`, we will set up
    a parameter to differentiate between them.
  prefs: []
  type: TYPE_NORMAL
- en: The Python logging module has a lot of interesting features. Check out the Python
    documentation for more information ([https://docs.python.org/3/library/logging.html](https://docs.python.org/3/library/logging.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Setting logs properly is trickier than it looks. Don't be discouraged and keep
    tweaking them until they work.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will set up all the logging configuration in the `app.py` file. Let''s break
    up each part of the configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will generate a formatter that appends the `request_id` so that it''s
    available when generating logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `HTTP_X_REQUEST_ID` header is available in the `request.environ`
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Later, in `create_app`, we will set up the handler that we append to the `application`
    logger:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We only set up the handler if the run happens out of a script. `SysLogHandler`
    is included in Python. After this, we set up the format, which includes `request_id`.
    The formatter uses the `RequestFormatter` that we defined previously.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are hardcoding the values of the logger level to `INFO` and the `syslog`
    host to `syslog`, which corresponds to the service. Kubernetes will resolve this
    DNS correctly. Both values can be passed through environment variables, but we
    didn't do this here for the sake of simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: The logger hasn't been propagated, so avoid sending it to the `root` logger,
    which will duplicate the log.
  prefs: []
  type: TYPE_NORMAL
- en: Logging each request
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are common elements in every request that we need to capture. Flask allows
    us to execute code before and after a request, so we can use that to log the common
    elements of each request. Let's learn how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the `app.py` file, we will define the `logging_before` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This creates a log with the word `REQUEST` and two essential parts of each request
    – the method and the URI – which come from `request.environ`. Then, they're added
    to an `INFO` log with the app logger.
  prefs: []
  type: TYPE_NORMAL
- en: We also use the `g` object to store the time when the request is started.
  prefs: []
  type: TYPE_NORMAL
- en: The `g` object allows us to store values through a request. We will use it to
    calculate the time the request is going to take.
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s the corresponding `logging_after` function as well. It gathers the
    time at the end of the request and calculates the difference in milliseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This will allow us to detect requests that are taking longer and will be stored
    in metrics, as we will see in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the functions are enabled in the `create_app` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This creates a set of logs each time we generate a request.
  prefs: []
  type: TYPE_NORMAL
- en: With the logs generated, we can search for them in the `frontrail` interface.
  prefs: []
  type: TYPE_NORMAL
- en: Searching through all the logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the different logs from different applications will be centralized and available
    to search for at `http://syslog.example.local`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you make a call to `http://frontend.example.local/search?search=speak` to
    search for thoughts, you will see the corresponding Thoughts Backend in the logs,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1254a0cc-90d6-4340-b901-95536a0a34e0.png)'
  prefs: []
  type: TYPE_IMG
- en: We can filter by the request ID, that is, `63517c17-5a40-4856-9f3b-904b180688f6`,
    to get the Thoughts Backend request logs. Just after this are the `thoughts_backend_uwsgi`
    and `frontend_uwsgi` request logs, which show the flow of the request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you can see all the elements we talked about previously:'
  prefs: []
  type: TYPE_NORMAL
- en: The `REQUEST` log before the request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `api_namespace` request, which contains app data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The after `RESPONSE` logs, which contain the result and time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Within the code for the Thoughts Backend, we left an error on purpose. It will
    be triggered if a user tries to share a new thought. We will use this to learn
    how to debug issues through logs.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting problems through logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For any problem in your running system, there are two kinds of errors that
    can occur: expected errors and unexpected errors.'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting expected errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Expected errors are errors that are raised by explicitly creating an `ERROR`
    log in the code. If an error log is being generated, this means that it reflects
    a situation that has been planned in advance; for example, you can't connect to
    the database, or there is some data stored in an old, deprecated format. We don't
    want this to happen, but we saw the possibility of it happening and prepared the
    code to deal with it. They normally describe the situation well enough that the
    issue is obvious, even if the solution isn't.
  prefs: []
  type: TYPE_NORMAL
- en: They are relatively easy to deal with since they describe foreseen problems.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing unexpected errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unexpected errors are the other types of errors that can occur. Things break
    in unforeseen ways. Unexpected errors are normally produced by Python exceptions
    being raised at some point in the code and not being captured.
  prefs: []
  type: TYPE_NORMAL
- en: If logging has been properly configured, any exceptions or errors that haven't
    been caught will trigger an `ERROR` log, which will include the stack trace. These
    errors may not be immediately obvious and will require further investigation.
  prefs: []
  type: TYPE_NORMAL
- en: To help explain these errors, we introduced an exception in the code for the
    Thoughts Backend in the `Chapter10` code. You can check the code on GitHub ([https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter10/microservices/thoughts_backend/ThoughtsBackend/thoughts_backend](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter10/microservices/thoughts_backend/ThoughtsBackend/thoughts_backend)).
    This simulates an unexpected exception.
  prefs: []
  type: TYPE_NORMAL
- en: 'While trying to post a new thought for a logged user, we get a weird behavior
    and see the following error in the logs. As shown in the top-right corner of the
    following screenshot, we are filtering by `ERROR` to filter for problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3bad0ee5-9505-4648-8158-cf878d1969ad.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the stack trace is displayed in a single line. This may depend
    on how you capture and display the logs. Flask will automatically generate an
    HTTP response with a status code of 500\. This may trigger more errors along the
    path if the caller isn't ready to receive a 500 response.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the stack trace will let you know what broke. In this case, we can see
    that there's a `raise Exception` command in the `api_namespace.py` file at line
    80. This allows us to locate the exception.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is a synthetic error that's been generated specifically as an example,
    it is actually easy to find out the root cause. In the example code, we are explicitly
    raising an exception, which produces an error. This may not be the case in a real
    use case, where the exception could be generated in a different place than the
    actual error. Exceptions can be also originated in a different microservice within
    the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: After detecting the error, the objective should be to replicate it with a unit
    test in the microservice in order to generate the exception. This will allow us
    to replicate the conditions in a controlled environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the tests for the Thoughts Backend code that''s available in `Chapter10`,
    we will see errors because of this. Note that the logs are being displayed in
    failing tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Once the error has been reproduced in unit tests, fixing it will often be trivial.
    Add a unit test to capture the set of conditions that trigger the error and then
    fix it. The new unit test will detect whether the error has been reintroduced
    on each automated build.
  prefs: []
  type: TYPE_NORMAL
- en: To fix the example code, remove the `raise` line of code. Then, things will
    work again.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, the problem cannot be solved as it may be external. Maybe there's
    a problem in some of the rows in our database or maybe another service is returning
    incorrectly formatted data. In those cases, we can't completely avoid the root
    cause of the error. However, it's possible to capture the problem, do some remediation,
    and move from an unexpected error to an expected error.
  prefs: []
  type: TYPE_NORMAL
- en: Note that not every detected unexpected error is worth spending time on. Sometimes,
    uncaptured errors provide enough information on what the problem is, which is
    out of the realm of what the web service should handle; for example, there may
    be a network problem and the web service can't connect to the database. Use your
    judgment when you want to spend time on development.
  prefs: []
  type: TYPE_NORMAL
- en: Logging strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's a problem when we're dealing with logs. What is the adequate level for
    a particular message? Is this a `WARNING` or an `ERROR`? Should this be an `INFO`
    statement?
  prefs: []
  type: TYPE_NORMAL
- en: Most of the log level descriptions use definitions such as *the program shows
    a potentially harmful situation* or *the program highlights the progress of the
    request*. These are vague and not very useful in a real-life environment. Instead,
    try to define each log level by relating them to the expected follow-up action.
    This helps provide clarity on what to do when a log of a particular level is found.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows some examples of the different levels and what action
    should be taken:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Log level** | **Action to take** | **Comments** |'
  prefs: []
  type: TYPE_TB
- en: '| `DEBUG` | Nothing. | Not tracked. |'
  prefs: []
  type: TYPE_TB
- en: '| `INFO` | Nothing. | The `INFO` logs show generic information about the flow
    of the request to help track problems. |'
  prefs: []
  type: TYPE_TB
- en: '| `WARNING` | Track number. Alert on raising levels. | The `WARNING` logs track
    errors that have been automatically fixed, such as retries to connect (but finally
    connecting) or fixable formatting errors in the database''s data. A sudden increase
    may require investigation. |'
  prefs: []
  type: TYPE_TB
- en: '| `ERROR` | Track number. Alert on raising levels. Review all. | The `ERROR`
    logs track errors that can''t be fixed. A sudden increase may require immediate
    action so that this can be remediated. |'
  prefs: []
  type: TYPE_TB
- en: '| `CRITICAL` | Immediate response. | A `CRITICAL` log indicates a catastrophic
    failure in the system. Even one will indicate that the system is not working and
    can''t recover. |'
  prefs: []
  type: TYPE_TB
- en: This is just a recommendation, but it sets clear expectations on how to respond.
    Depending on how your teams and your expected level of service work, you can adapt
    them to your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the hierarchy is very clear, and there's an acceptance that a certain
    number of `ERROR` logs will be generated. Not everything needs to be fixed immediately,
    but they should be noted and reviewed.
  prefs: []
  type: TYPE_NORMAL
- en: In real life, `ERROR` logs will be typically categorized as "we're doomed" or
    "meh." Development teams should actively either fix or remove "mehs" to reduce
    them as much as possible. That may include lowering the level of logs if they
    aren't covering actual errors. You want as few `ERROR` logs as possible, but all
    of them need to be meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: Be pragmatic, though. Sometimes, errors can't be fixed straight away and time
    is best utilized in other tasks. However, teams should reserve time to reduce
    the number of errors that occur. Failing to do so will compromise the reliability
    of the system in the medium term.
  prefs: []
  type: TYPE_NORMAL
- en: '`WARNING` logs are indications that something may not be working as smoothly
    as we expected, but there''s no need to panic unless the numbers grow. `INFO`
    is just there to give us context if there''s a problem, but otherwise should be
    ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid the temptation to produce an `ERROR` log when there's a request returning
    a 400 BAD REQUEST status code. Some developers will argue that if the customer
    sent a malformed request, it is actually an error. But this isn't something that
    you should care about if the request has been properly detected and returned.
    This is business as usual. If this behavior can lead to indicate something else,
    such as repeated attempts to send incorrect passwords, you can set a `WARNING`
    log. There's no point in generating `ERROR` logs when your system is behaving
    as expected.
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb, if a request is not returning some sort of 500 error (500,
    502, 504, and so on), it should not generate an `ERROR` log. Remember the categorization
    of 400 errors as *you (customer) have a problem* versus 500 errors, which are
    categorized as *I have a problem*.
  prefs: []
  type: TYPE_NORMAL
- en: This is not absolute, though. For example, a spike in authentication errors
    that are normally 4XX errors may indicate that users cannot create logs due to
    a real internal problem.
  prefs: []
  type: TYPE_NORMAL
- en: With these definitions in mind, your development and operations teams will have
    a shared understanding that will help them take meaningful actions.
  prefs: []
  type: TYPE_NORMAL
- en: Expect to tweak the system and change some of the levels of the logs as your
    system matures.
  prefs: []
  type: TYPE_NORMAL
- en: Adding logs while developing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we've already seen, properly configuring `pytest` will make any errors in
    tests display the captured logs.
  prefs: []
  type: TYPE_NORMAL
- en: This is an opportunity to check that the expected logs are being generated while
    a feature is in development. Any test that checks error conditions should also
    add its corresponding logs and check that they are being generated during the
    development of the feature.
  prefs: []
  type: TYPE_NORMAL
- en: You can check the logs as part of testing with a tool such as `pytest-catchlog`
    ([https://pypi.org/project/pytest-catchlog/](https://pypi.org/project/pytest-catchlog/))
    to enforce that the proper logs are being produced.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, though, just taking a bit of care and checking during development
    that logs are produced is enough for most cases. However, be sure that developers
    understand why it's useful to have logs while they're developing.
  prefs: []
  type: TYPE_NORMAL
- en: During development, `DEBUG` logs can be used to show extra information about
    the flow that will be too much for production. This may fill in the gaps between
    `INFO` logs and help us develop the habit of adding logs. A `DEBUG` log may be
    promoted to `INFO` if, during tests, it's discovered that it will be useful for
    tracking problems in production.
  prefs: []
  type: TYPE_NORMAL
- en: Potentially, `DEBUG` logs can be enabled in production in controlled cases to
    track some difficult problems, but be aware of the implications of having a large
    number of logs.
  prefs: []
  type: TYPE_NORMAL
- en: Be sensible with the information that's presented in `INFO` logs. In terms of
    the information that's displayed, avoid sensible data such as passwords, secret
    keys, credit card numbers, or personal information. This is the same for the number
    of logs.
  prefs: []
  type: TYPE_NORMAL
- en: Keep an eye on any size limitations and how quickly logs are being generated.
    Growing systems may have a log explosion while new features are being added, more
    requests are flowing through the system, and new workers are being added.
  prefs: []
  type: TYPE_NORMAL
- en: Also, double-check that the logs are being generated and captured correctly
    and that they work at all the different levels and environments. All of this configuration
    may take a bit of time, but you need to be very sure that you can capture unexpected
    errors in production and that all the plumbing is set correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the other key element when it comes to observability:
    metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To set up metrics with Prometheus, we need to understand how the process works.
    Its key component is that each service that's measured has its own Prometheus
    client that keeps track of the metrics. The data in the Prometheus server will
    be available for a Grafana service that will plot the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the general architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5b14900d-d4cd-4768-a10b-7a918425d553.png)'
  prefs: []
  type: TYPE_IMG
- en: The Prometheus server pulls information at regular intervals. This method of
    operation is very lightweight since registering metrics just updates the local
    memory of the service and scales well. On the other hand, it shows sampled data
    at certain times and doesn't register each individual event. This has certain
    implications in terms of storing and representing data and imposes limitations
    on the resolution of the data, especially for very low rates.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are lots of available metrics exporters that will expose standard metrics
    in different systems, such as databases, hardware, HTTP servers, or storage. Check
    out the Prometheus documentation for more information: [https://prometheus.io/docs/instrumenting/exporters/](https://prometheus.io/docs/instrumenting/exporters/).'
  prefs: []
  type: TYPE_NORMAL
- en: This means that each of our services needs to install a Prometheus client and
    expose its collected metrics in some way. We will use standard clients for Flask
    and Django.
  prefs: []
  type: TYPE_NORMAL
- en: Defining metrics for the Thoughts Backend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For Flask applications, we will use the `prometheus-flask-exporter` package
    ([https://github.com/rycus86/prometheus_flask_exporter](https://github.com/rycus86/prometheus_flask_exporter)),
    which has been added to `requirements.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: It gets activated in the `app.py` file ([https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter10/microservices/thoughts_backend/ThoughtsBackend/thoughts_backend/app.py#L95](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter10/microservices/thoughts_backend/ThoughtsBackend/thoughts_backend/app.py#L95))
    when the application is created.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `metrics` object is set up with no app, and is then instantiated in the
    `created_app` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates an endpoint in the `/metrics` service endpoint, that is, `http://thoughts.example.local/metrics`,
    which returns the data in Prometheus format. The Prometheus format is plain text,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/81ba132d-890c-4b57-96db-46bbfca38f44.png)'
  prefs: []
  type: TYPE_IMG
- en: The default metrics that are captured by `prometheus-flask-exporter` are request
    calls based on the endpoint and the method (`flask_http_request_total`), as well
    as the time they took (`flask_http_request_duration_seconds`).
  prefs: []
  type: TYPE_NORMAL
- en: Adding custom metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We may want to add more specific metrics when it comes to application details.
    We also added some extra code at the end of the request so that we can store similar
    information to the metric that `prometheus-flask-exporter` allows us to.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we added this code to the `logging_after` function ([https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter10/microservices/thoughts_backend/ThoughtsBackend/thoughts_backend/app.py#L72](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/blob/master/Chapter10/microservices/thoughts_backend/ThoughtsBackend/thoughts_backend/app.py#L72))
    using the lower-level `prometheus_client`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This code creates `Counter` and `Histogram`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we''ve created two metrics: a counter called `requests` and a histogram
    called `req_time`. A histogram is a Prometheus implementation of measures and
    events that have a specific value, such as the request time (in our case).'
  prefs: []
  type: TYPE_NORMAL
- en: The histogram stores the values in buckets, thereby making it possible for us
    to calculate quantiles. Quantiles are very useful to determine metrics such as
    the 95% value for times, such as the aggregate time, where 95% comes lower than
    it. This is much more useful than averages since outliers won't pull from the
    average.
  prefs: []
  type: TYPE_NORMAL
- en: There's another similar metric called summary. The differences are subtle, but
    generally, the metric we should use is a histogram. Check out the Prometheus documentation
    for more details ([https://prometheus.io/docs/practices/histograms/](https://prometheus.io/docs/practices/histograms/)).
  prefs: []
  type: TYPE_NORMAL
- en: The metrics are defined in `METRIC_REQUESTS` and `METRIC_REQ_TIME` by their
    name, their measurement, and the labels they define. Each label is an extra dimension
    of the metric, so you will be able to filter and aggregate by them. Here, we define
    the endpoint, the HTTP method, and the resulting HTTP status code.
  prefs: []
  type: TYPE_NORMAL
- en: For each request, the metric is updated. We need to set up the labels, the counter
    calls, that is, `.inc()`, and the histogram calls, that is, `.observe(time)`.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the documentation for the Prometheus client at [https://github.com/prometheus/client_python](https://github.com/prometheus/client_python).
  prefs: []
  type: TYPE_NORMAL
- en: We can see the `request` and `req_time` metrics on the metrics page.
  prefs: []
  type: TYPE_NORMAL
- en: '**Setting up metrics for the Users Backend follows a similar pattern.** The
    Users Backend is a similar Flask application, so we install `prometheus-flask-exporter`
    as well, but no custom metrics. You can access these metrics at `http://users.example.local/metrics`.'
  prefs: []
  type: TYPE_NORMAL
- en: The next stage is to set up a Prometheus server so that we can collect the metrics
    and aggregate them properly.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting the metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this, we need to deploy the metrics using Kubernetes. We prepared a YAML
    file with everything set up already in the `Chapter10/kubernetes/prometheus.yaml`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: This YAML file contains a deployment, a `ConfigMap`, which contains the configuration
    file, a service, and an Ingress. The service and Ingress are pretty standard,
    so we won't comment on them here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ConfigMap` allows us to define a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note how the `prometheus.yaml` file is generated after the `|` symbol. This
    is a minimal Prometheus configuration scraping from the `thoughts-service`, `users-service`,
    and `frontend-service` servers. As we know from the previous chapters, these names
    access the services and will connect to the pods that are serving the applications.
    They will automatically search for the `/metrics` path.
  prefs: []
  type: TYPE_NORMAL
- en: There is a small caveat here. From the point of view of Prometheus, everything
    behind the service is the same server. If you have more than one pod being served,
    the metrics that are being accessed by Prometheus will be load balanced and the
    metrics won't be correct.
  prefs: []
  type: TYPE_NORMAL
- en: This is fixable with a more complicated Prometheus setup whereby we install
    the Prometheus operator, but this is out of the scope of this book. However, this
    is highly recommended for a production system. In essence, it allows us to annotate
    each of the different deployments so that the Prometheus configuration is dynamically
    changed. This means we can access all the metrics endpoints exposed by the pods
    automatically once this has been set up. Prometheus Operator annotations make
    it very easy for us to add new elements to the metrics system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following article if you want to learn how to do this: [https://sysdig.com/blog/kubernetes-monitoring-prometheus-operator-part3](https://sysdig.com/blog/kubernetes-monitoring-prometheus-operator-part3).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The deployment creates a container from the public Prometheus image in `prom/prometheus`,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: It also mounts `ConfigMap` as a volume, and then as a file in `/etc/prometheus/prometheus.yml`.
    This starts the Prometheus server with that configuration. The container opens
    port `9090`, which is the default for Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, note how we delegated for the Prometheus container. This is
    one of the advantages of using Kubernetes: we can use standard available containers
    to add features to our cluster with minimal configuration. We don''t even have
    to worry about the operating system or the packaging of the Prometheus container.
    This simplifies operations and allows us to standardize the tools we use.'
  prefs: []
  type: TYPE_NORMAL
- en: The deployed Prometheus server can be accessed at `http://prometheus.example.local/`,
    as described in the Ingress and service.
  prefs: []
  type: TYPE_NORMAL
- en: 'This displays a graphic interface that can be used to plot the graphs, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/98c116d8-05e9-461b-b13b-d8a24a240609.png)'
  prefs: []
  type: TYPE_IMG
- en: The Expression search box will also autocomplete metrics, helping with the discovery
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The interface also displays other elements from Prometheus that are interesting,
    such as the configuration or the statuses of the targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4a164941-5d6b-4364-903e-5123751f6476.png)'
  prefs: []
  type: TYPE_IMG
- en: The graphs in this interface are usable, but we can set up more complicated
    and useful dashboards through Grafana. Let's see how this setup works.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting graphs and dashboards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The required Kubernetes configuration, `grafana.yaml`, is available in this
    book's GitHub repository in the `Chapter10/kubernetes/metrics` directory. Just
    like we did with Prometheus, we used a single file to configure Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: 'We won''t show the Ingress and service for the same reason we explained previously.
    The deployment is simple, but we mount two volumes instead of one, as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `volume-config` volume shares a single file that configures Grafana. The
    `volume-dashboard` volume adds a dashboard. The latter mounts a directory that
    contains two files. Both mounts are in the default location that Grafana expects
    for configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `volume-config` volume sets up the data source in the place where Grafana
    will receive the data to plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The data comes from `http://prometheus-service` and points to the Prometheus
    service we configured previously.
  prefs: []
  type: TYPE_NORMAL
- en: '`volume-dashboard` defines two files, `dashboard.yaml` and `dashboard.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`dashboard.yaml` is a simple file that points to the directory where we can
    find JSON files describing the available dashboards for the system. We point to
    the same directory to mount everything with a single volume.'
  prefs: []
  type: TYPE_NORMAL
- en: '`dashboard.json` is redacted here to save space; check out this book''s GitHub
    repository for the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '`dashboard.json` describes a dashboard in JSON format. This file can be automatically
    generated through the Grafana UI. Adding more `.json` files will create new dashboards.'
  prefs: []
  type: TYPE_NORMAL
- en: Grafana UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By accessing `http://grafana.example.local` and using your login/password details,
    that is, `admin/admin` (the default values), you can access the Grafana UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e0ed3527-9a22-4a49-8356-5e58795741ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From there, you can check the dashboard, which can be found in the left central
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/15311ba9-bf88-4c0c-8b8b-15a29b88edf3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This captures the calls to Flask, both in terms of numbers and in *95^(th)*
    percentile time. Each individual graph can be edited so that we can see the recipe
    that produces it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bfba98e1-1532-46a9-8bbd-126745e3ee24.png)'
  prefs: []
  type: TYPE_IMG
- en: The icons on the left allow us to change the queries that are running in the
    system, change the visualization (units, colors, bars or lines, kind of scale
    to plot, and so on), add general information such as name, and create alerts.
  prefs: []
  type: TYPE_NORMAL
- en: The Grafana UI allows us to experiment and so is highly interactive. Take some
    time to try out the different options and learn how to present the data.
  prefs: []
  type: TYPE_NORMAL
- en: The Query section allows us to add and display metrics from Prometheus. Note
    the Prometheus logo near default, which is the data source.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the queries has a Metrics section that extracts data from Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Querying Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prometheus has its own query language called PromQL. The language is very powerful,
    but it presents some peculiarities.
  prefs: []
  type: TYPE_NORMAL
- en: The Grafana UI helps by autocompleting the query, which makes it easy for us
    to search for metric names. You can experiment directly in the dashboard, but
    there's a page on Grafana called Explore that allows you to make queries out of
    any dashboard and has some nice tips, including basic elements. This is denoted
    by a compass icon in the left sidebar.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to keep in mind is understanding the Prometheus metrics. Given
    its sampling approach, most of them are monotonically increasing. This means that
    plotting the metrics will show a line going up and up.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the rate at which the value changes over a period of time, you need
    to use `rate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the requests per second, on average, with a moving window of
    `5` minutes. The rate can be further aggregated using `sum` and `by`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To calculate the times, you can use `avg` instead. You can also group by more
    than one label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'However, you can also set up quantiles, just like we can in graphs. We multiply
    by 100 to get the time in milliseconds instead of seconds and group by `method`
    and `path`. Now, `le` is a special tag that''s created automatically and divides
    the data into multiple buckets. The `histogram_quantile` function uses this to
    calculate the quantiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Metrics can be filtered so that only specific labels are displayed. They can
    also be used for different functions, such as division, multiplication, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus queries can be a bit long and complicated when we're trying to display
    the result of several metrics, such as the percentage of successful requests over
    the total. Be sure to test that the result is what you expect it to be and allocate
    time to tweak the requests, later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Be sure to check out the Prometheus documentation if you want to find out more:
    [https://prometheus.io/docs/prometheus/latest/querying/basics/](https://prometheus.io/docs/prometheus/latest/querying/basics/).'
  prefs: []
  type: TYPE_NORMAL
- en: Updating dashboards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dashboards can be interactively changed and saved, but in our Kubernetes configuration,
    we set up the volumes that contain the files as non-persistent. Due to this, restarting
    Grafana will discard any changes and reapply the defined configuration in `volume-dashboard`
    in the `Chapter10/kubernetes/metrics/grafana.yaml` file.
  prefs: []
  type: TYPE_NORMAL
- en: This is actually a good thing since we apply the same GitOps principles to store
    the full configuration in the repository under Git source control.
  prefs: []
  type: TYPE_NORMAL
- en: However, as you can see, the full JSON description of the dashboard contained
    in the `grafana.yaml` file is very long, given the number of parameters and the
    difficulty to change them manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best approach is to change the dashboard interactively and then export
    it into a JSON file with the Share file button at the top of the menu. Then, the
    JSON file can be added to the configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e3802571-88e1-4692-85bb-4560de9d23ea.png)'
  prefs: []
  type: TYPE_IMG
- en: The Grafana pod can then be redeployed and will contain the saved changes in
    the dashboard. The Kubernetes configuration can then be updated in Git through
    the usual process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Be sure to explore all the possibilities for dashboards, including the option
    to set up variables so that you can use the same dashboard to monitor different
    applications or environments and the different kinds of visualization tools. See
    the full Grafana documentation for more information: [https://grafana.com/docs/reference/](https://grafana.com/docs/reference/).'
  prefs: []
  type: TYPE_NORMAL
- en: Having metrics available allows us to use them to proactively understand the
    system and anticipate any problems.
  prefs: []
  type: TYPE_NORMAL
- en: Being proactive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metrics show an aggregated point of view for the status of the whole cluster.
    They allow us to detect trending problems, but it's difficult to find a single
    spurious error.
  prefs: []
  type: TYPE_NORMAL
- en: Don't underestimate them, though. They are critical for successful monitoring
    because they tell us whether the system is healthy. In some companies, the most
    critical metrics are prominently displayed in screens on the wall so that the
    operations team can see them at all times and quickly react.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finding the proper balance for metrics in a system is not a straightforward
    task and will require time and trial and error. There are four metrics for online
    services that are always important, though. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency**: How many milliseconds the system takes to respond to a request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the times, a different time unit, such as seconds or microseconds,
    can be used. From my experience, milliseconds is adequate since most of the requests
    in a web application system should take between 50 ms and 1 second to respond.
    Here, a system that takes 50 ms is too slow and one that takes 1 second is a very
    performant one.
  prefs: []
  type: TYPE_NORMAL
- en: '**Traffic**: The number of requests flowing through the system per unit of
    time, that is, requests per second or per minute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Errors**: The percentage of requests received that return an error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Saturation**: Whether the capacity of the cluster has enough headroom. This
    includes elements such as hard drive space, memory, and so on. For example, there
    is 20% available RAM memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To measure saturation, remember to install the available exporters that will
    collect most of the hardware information (memory, hard disk space, and so on)
    automatically. If you use a cloud provider, normally, they expose their own set
    of related metrics as well, for example, CloudWatch for AWS.
  prefs: []
  type: TYPE_NORMAL
- en: These metrics can be found in the Google SRE Book as *the Four Golden Signals*
    and are recognized as the most important high-level elements for successful monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When problems arise in metrics, an automatic alert should be generated. Prometheus
    has an included alert system that will trigger when a defined metric fulfills
    the defined condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the Prometheus documentation on alerting for more information: [https://prometheus.io/docs/alerting/overview/](https://prometheus.io/docs/alerting/overview/).'
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus' Alertmanager can perform certain actions, such as sending emails
    to be notified based on rules. This system can be connected to an integrated incident
    solution such as OpsGenie ([https://www.opsgenie.com](https://www.opsgenie.com))
    in order to generate all kinds of alerts and notifications, such as emails, SMS,
    calls, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Logs can also be used to create alerts. There are certain tools that allow us
    to create an entry when an `ERROR` is raised, such as **Sentry**. This allows
    us to detect problems and proactively remediate them, even if the health of the
    cluster hasn't been compromised.
  prefs: []
  type: TYPE_NORMAL
- en: Some commercial tools that handle logs, such as Loggly, allow us to derive metrics
    from the logs themselves, plotting graphs either based on the kind of log or extracting
    values from them and using them as values. While not as complete as a system such
    as Prometheus, they can monitor some values. They also allow us to notify if thresholds
    are reached.
  prefs: []
  type: TYPE_NORMAL
- en: The monitoring space is full of products, both free and paid, that can help
    us to handle this. While it's possible to create a completely in-house monitoring
    system, being able to analyze whether commercial cloud tools will be of help is
    crucial. The level of features and their integration with useful tools such as
    external alerting systems will be difficult to replicate and maintain.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting is also an ongoing process. Some elements will be discovered down the
    line and new alerts will have to be created. Be sure to invest time so that everything
    works as expected. Logs and metrics will be used while the system is unhealthy,
    and in those moments, time is critical. You don't want to be guessing about logs
    because the host parameter hasn't been configured correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Being prepared
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the same way that a backup is not useful unless the recovery process has
    been tested and is working, be proactive when checking that the monitoring system
    is producing useful information.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, try to standardize the logs so that there's a good expectation
    about what information to include and how it's structured. Different systems may
    produce different logs, but it's good to make all the microservices log in the
    same format. Double-check that any parameters, such as client references or hosts,
    are being logged correctly.
  prefs: []
  type: TYPE_NORMAL
- en: The same applies to metrics. Having a set of metrics and dashboards that everyone
    understands will save a lot of time when you're tracking a problem.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to work with logs and metrics, as well as how
    to set up logs and send them to a centralized container using the `syslog` protocol.
    We described how to add logs to different applications, how to include a request
    ID, and how to raise custom logs from the different microservices. Then, we learned
    how to define a strategy to ensure that the logs are useful in production.
  prefs: []
  type: TYPE_NORMAL
- en: We also described how to set up standard and custom Prometheus metrics in all
    the microservices. We started a Prometheus server and configured it so that it
    collects metrics from our services. We started a Grafana service so that we can
    plot the metrics and created dashboards so that we can display the status of the
    cluster and the different services that are running.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we introduced you to the alert system in Prometheus and how it can be
    used so that it notifies us of problems. Remember that there are commercial services
    to help you with logs, metrics, and alerts. Analyze your options as they can save
    you a lot of time and money in terms of maintenance costs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to manage changes and dependencies that
    affect several microservices and how to handle configurations and secrets.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the observability of a system?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the different severity levels that are available in logs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are metrics used for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do you need to add a request ID to logs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the available kinds of metrics in Prometheus?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the 75th percentile in a metric and how does it differ from the average?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the four golden signals?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can learn more about monitoring with different tools and techniques while
    using Docker by reading *Monitoring Docker* ([https://www.packtpub.com/virtualization-and-cloud/monitoring-docker](https://www.packtpub.com/virtualization-and-cloud/monitoring-docker)).
    To find out more about Prometheus and Grafana, including how to set up alerts,
    please read *Hands-On Infrastructure Monitoring with Prometheus* ([https://www.packtpub.com/virtualization-and-cloud/hands-infrastructure-monitoring-prometheus](https://www.packtpub.com/virtualization-and-cloud/hands-infrastructure-monitoring-prometheus)).
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring is only the starting point of successfully running services reliably.
    To find out how to successfully improve your operations, check out *Real-World
    SRE* ([https://www.packtpub.com/web-development/real-world-sre](https://www.packtpub.com/web-development/real-world-sre)).
  prefs: []
  type: TYPE_NORMAL
