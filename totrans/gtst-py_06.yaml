- en: Principles of Algorithm Design
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法设计原则
- en: Why do we want to study algorithm design? There are of course many reasons,
    and our motivation for learning something is very much dependent on our own circumstances.
    There are without doubt important professional reasons for being interested in
    algorithm design. Algorithms are the foundations of all computing. We think of
    a computer as being a piece of hardware, a hard drive, memory chips, processors,
    and so on. However, the essential component, the thing that, if missing, would
    render modern technology impossible, is algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要学习算法设计？当然有很多原因，我们学习某事的动机很大程度上取决于我们自己的情况。对于对算法设计感兴趣有重要的职业原因。算法是所有计算的基础。我们认为计算机是硬件，硬盘、内存芯片、处理器等等。然而，如果缺少的是算法，现代技术将不可能存在。
- en: The theoretical foundation of algorithms, in the form of the Turing machine,
    was established several decades before digital logic circuits could actually implement
    such a machine. The Turing machine is essentially a mathematical model that, using
    a predefined set of rules, translates a set of inputs into a set of outputs. The
    first implementations of Turing machines were mechanical and the next generation
    may likely see digital logic circuits replaced by quantum circuits or something
    similar. Regardless of the platform, algorithms play a central predominant role.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的理论基础是图灵机，几十年前就建立了这种机器的数学模型，而数字逻辑电路实际上能够实现这样的机器。图灵机本质上是一个数学模型，它使用预定义的一组规则，将一组输入转换为一组输出。图灵机的第一批实现是机械的，下一代可能会看到数字逻辑电路被量子电路或类似的东西所取代。无论平台如何，算法都起着中心主导的作用。
- en: Another aspect is the effect algorithms have in technological innovation. As
    an obvious example, consider the page rank search algorithm, a variation of which
    the Google search engine is based on. Using this and similar algorithms allows
    researchers, scientists, technicians, and others to quickly search through vast
    amounts of information extremely quickly. This has a massive effect on the rate
    at which new research can be carried out, new discoveries made, and new innovative
    technologies developed.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的另一个方面是它对技术创新的影响。显而易见的例子是页面排名搜索算法，谷歌搜索引擎就是基于它的变体。使用这种算法和类似的算法允许研究人员、科学家、技术人员和其他人能够快速地搜索大量信息。这对新研究的速度、新发现的速度以及新的创新技术的发展速度都有巨大影响。
- en: 'The study of algorithms is also important because it trains us to think very
    specifically about certain problems. It can serve to increase our mental and problem
    solving abilities by helping us isolate the components of a problem and define
    relationships between these components. In summary, there are four broad reasons
    for studying algorithms:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的研究也很重要，因为它训练我们对某些问题进行非常具体的思考。它可以通过帮助我们分离问题的组成部分并定义这些组成部分之间的关系，来增强我们的思维和问题解决能力。总之，学习算法有四个主要原因：
- en: They are essential for computer science and *intelligent* systems.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们对计算机科学和*智能*系统至关重要。
- en: They are important in many other domains (computational biology, economics,
    ecology, communications, ecology, physics, and so on).
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们在许多其他领域（计算生物学、经济学、生态学、通信、生态学、物理学等）中都很重要。
- en: They play a role in technology innovation.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们在技术创新中发挥作用。
- en: They improve problem solving and analytical thinking.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们改善问题解决和分析思维能力。
- en: Algorithms, in their simplest form, are just a sequence of actions, a list of
    instructions. It may just be a linear construct of the form do *x*, then do *y*,
    then do *z*, then finish. However, to make things more useful we add clauses to
    the effect of, *x* then do *y*, in Python the `if-else` statements. Here, the
    future course of action is dependent on some conditions; say the state of a data
    structure. To this we also add the operation, iteration, the while, and for statements.
    Expanding our algorithmic literacy further we add recursion. Recursion can often
    achieve the same result as iteration, however, they are fundamentally different.
    A recursive function calls itself, applying the same function to progressively
    smaller inputs. The input of any recursive step is the output of the previous
    recursive step.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在最简单的形式中只是一系列操作，一系列指令。它可能只是一个线性结构，形式为做*x*，然后做*y*，然后做*z*，然后完成。然而，为了使事情更有用，我们添加了诸如在Python中的`if-else`语句的子句。在这里，未来的行动取决于某些条件；比如数据结构的状态。我们还添加了操作、迭代，while和for语句。进一步扩展我们的算法素养，我们添加了递归。递归通常可以实现与迭代相同的结果，但它们在根本上是不同的。递归函数调用自身，将相同的函数应用于逐渐减小的输入。任何递归步骤的输入都是前一个递归步骤的输出。
- en: 'Essentially, we can say that algorithms are composed of the following four
    elements:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们可以说算法由以下四个元素组成：
- en: Sequential operations
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序操作
- en: Actions based on the state of a data structure
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于数据结构状态的操作
- en: Iteration, repeating an action a number of times
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代，重复执行一定次数的操作
- en: Recursion, calling itself on a subset of inputs
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归，在一组输入上调用自身
- en: Algorithm design paradigms
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法设计范式
- en: 'In general, we can discern three broad approaches to algorithm design. They
    are:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们可以分辨出三种广泛的算法设计方法。它们是：
- en: Divide and conquer
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分而治之
- en: Greedy algorithms
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贪婪算法
- en: Dynamic programming
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态规划
- en: As the name suggests, the divide and conquer paradigm involves breaking a problem
    into smaller sub problems, and then in some way combining the results to obtain
    a global solution. This is a very common and natural problem solving technique,
    and is, arguably, the most commonly used approach to algorithm design.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，分而治之范式涉及将问题分解为较小的子问题，然后以某种方式将结果组合起来以获得全局解。这是一种非常常见和自然的问题解决技术，可以说是最常用的算法设计方法。
- en: Greedy algorithms often involve optimization and combinatorial problems; the
    classic example is applying it to the traveling salesperson problem, where a greedy
    approach always chooses the closest destination first. This shortest path strategy
    involves finding the best solution to a local problem in the hope that this will
    lead to a global solution.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: The dynamic programming approach is useful when our sub problems overlap. This
    is different from divide and conquer. Rather than break our problem into independent
    sub problems, with dynamic programming, intermediate results are cached and can
    be used in subsequent operations. Like divide and conquer it uses recursion; however,
    dynamic programming allows us to compare results at different stages. This can
    have a performance advantage over divide and conquer for some problems because
    it is often quicker to retrieve a previously calculated result from memory rather
    than having to recalculate it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Recursion and backtracking
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recursion is particularly useful for divide and conquer problems; however,
    it can be difficult to understand exactly what is happening, since each recursive
    call is itself spinning off other recursive calls. At the core of a recursive
    function are two types of cases: base cases, which tell the recursion when to
    terminate, and recursive cases that call the function they are in. A simple problem
    that naturally lends itself to a recursive solution is calculating factorials.
    The recursive factorial algorithm defines two cases: the base case when *n* is
    zero, and the recursive case when *n* is greater than zero. A typical implementation
    is the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This code prints out the digits 1, 2, 4, 24\. To calculate 4 requires four
    recursive calls plus the initial parent call. On each recursion, a copy of the
    methods variables is stored in memory. Once the method returns it is removed from
    memory. The following is a way we can visualize this process:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ceaff8fe-caa4-42cf-8dd5-f0e739a5d7fa.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: 'It may not necessarily be clear if recursion or iteration is a better solution
    to a particular problem; after all they both repeat a series of operations and
    both are very well suited to divide and conquer approaches to algorithm design.
    Iteration churns away until the problem is done. Recursion breaks the problem
    down into smaller and smaller chunks and then combines the results. Iteration
    is often easier for programmers, because control stays local to a loop, whereas
    recursion can more closely represent mathematical concepts such as factorials.
    Recursive calls are stored in memory, whereas iterations are not. This creates
    a trade off between processor cycles and memory usage, so choosing which one to
    use may depend on whether the task is processor or memory intensive. The following
    table outlines the key differences between recursion and iteration:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '| **Recursion** | **Iteration** |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| Terminates when a base case is reached | Terminates when a defined condition
    is met |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| Each recursive call requires space in memory | Each iteration is not stored
    in memory |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| An infinite recursion results in a stack overflow error | An infinite iteration
    will run while the hardware is powered |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| Some problems are naturally better suited to recursive solutions | Iterative
    solutions may not always be obvious |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: Backtracking
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Backtracking is a form of recursion that is particularly useful for types of
    problems such as traversing tree structures, where we are presented with a number
    of options at each node, from which we must choose one. Subsequently we are presented
    with a different set of options, and depending on the series of choices made either
    a goal state or a dead end is reached. If it is the latter, we must backtrack
    to a previous node and traverse a different branch. Backtracking is a divide and
    conquer method for exhaustive search. Importantly backtracking **prunes** branches
    that cannot give a result.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of back tracking is given in the following example. Here, we have
    used a recursive approach to generating all the possible permutations of a given
    string, *s*, of a given length *n*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This generates the following output:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b1ab5929-7ac9-4dec-b033-e9fde81b5b2b.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: Notice the double list compression and the two recursive calls within this comprehension.
    This recursively concatenates each element of the initial sequence, returned when
    `*n* = 1`, with each element of the string generated in the previous recursive
    call. In this sense it is *backtracking* to uncover previously ingenerated combinations.
    The final string that is returned is all *n* letter combinations of the initial
    string.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Divide and conquer - long multiplication
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For recursion to be more than just a clever trick, we need to understand how
    to compare it to other approaches, such as iteration, and to understand when its
    use will lead to a faster algorithm. An iterative algorithm that we are all familiar
    with is the procedure we learned in primary math classes, used to multiply two
    large numbers. That is, long multiplication. If you remember, long multiplication
    involved iterative multiplying and carry operations followed by a shifting and
    addition operation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Our aim here is to examine ways to measure how efficient this procedure is and
    attempt to answer the question; is this the most efficient procedure we can use
    for multiplying two large numbers together?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, we can see that multiplying two 4 digit numbers together
    requires 16 multiplication operations, and we can generalize to say that an *n*
    digit number requires, approximately, *n²* multiplication operations:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a349984a-9f69-4c49-b418-7ff885bc6a42.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: This method of analyzing algorithms, in terms of the number of computational
    primitives such as multiplication and addition, is important because it gives
    us a way to understand the relationship between the time it takes to complete
    a certain computation and the size of the input to that computation. In particular,
    we want to know what happens when the input, the number of digits, n, is very
    large. This topic, called asymptotic analysis, or time complexity, is essential
    to our study of algorithms and we will revisit it often during this chapter and
    the rest of this book.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Can we do better? A recursive approach
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It turns out that in the case of long multiplication the answer is yes, there
    are in fact several algorithms for multiplying large numbers that require less
    operations. One of the most well-known alternatives to long multiplication is
    the **Karatsuba algorithm**, first published in 1962\. This takes a fundamentally
    different approach: rather than iteratively multiplying single digit numbers,
    it recursively carries out multiplication operations on progressively smaller
    inputs. Recursive programs call themselves on smaller subsets of the input. The
    first step in building a recursive algorithm is to decompose a large number into
    several smaller numbers. The most natural way to do this is to simply split the
    number in to two halves, the first half of most significant digits, and a second
    half of least significant digits. For example, our four-digit number, 2345, becomes
    a pair of two-digit numbers, 23 and 45\. We can write a more general decomposition
    of any 2 *n* digit numbers, *x* and *y* using the following, where *m* is any
    positive integer less than *n*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0ca172df-86c2-4780-bf6a-9570e18aab94.png)![](assets/55df9f14-ece8-4cbb-ae31-8b23c3267211.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: 'So now we can rewrite our multiplication problem *x*, *y* as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/35b80ce5-cbcc-4638-8799-346532ee2154.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: 'When we expand and gather like terms we get the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d5f89699-bf11-480c-9db2-0deae29ac8eb.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: 'More conveniently, we can write it like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ad955c99-182e-4f77-a7d2-df5800b6215f.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7bb07889-0803-436c-84b5-8edb5a6eb21d.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: It should be pointed out that this suggests a recursive approach to multiplying
    two numbers since this procedure does itself involve multiplication. Specifically,
    the products *ac*, *ad*, *bc*, and *bd* all involve numbers smaller than the input
    number and so it is conceivable that we could apply the same operation as a partial
    solution to the overall problem. This algorithm, so far, consists of four recursive
    multiplication steps and it is not immediately clear if it will be faster than
    the classic long multiplication approach.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 应该指出，这表明了一种递归方法来计算两个数字的乘法，因为这个过程本身涉及乘法。具体来说，乘积*ac*、*ad*、*bc*和*bd*都涉及比输入数字小的数字，因此我们可以将相同的操作作为整体问题的部分解决方案。到目前为止，这个算法包括四个递归乘法步骤，目前还不清楚它是否比经典的长乘法方法更快。
- en: What we have discussed so far in regards to the recursive approach to multiplication,
    has been well known to mathematicians since the late 19^(th) century. The Karatsuba
    algorithm improves on this is by making the following observation. We really only
    need to know three quantities: *z[2]*= *ac* ; *z[1]=ad +bc*, and *z[0]*= *bd*
    to solve equation 3.1\. We need to know the values of *a, b, c, d* only in so
    far as they contribute to the overall sum and products involved in calculating
    the quantities *z[2]*, *z[1]*, and *z[0]*. This suggests the possibility that
    perhaps we can reduce the number of recursive steps. It turns out that this is
    indeed the situation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，关于递归乘法的讨论对数学家来说自19世纪末就已经很熟悉了。卡拉茨巴算法改进了这一点，方法是做出以下观察。我们实际上只需要知道三个量：*z[2]*=
    *ac*；*z[1]=ad +bc*，和*z[0]*= *bd* 来解方程3.1。我们只需要知道*a, b, c, d*的值，因为它们对计算*z[2]*,
    *z[1]*, 和*z[0]*所涉及的总和和乘积有贡献。这表明也许我们可以减少递归步骤的数量。事实证明的确是这种情况。
- en: 'Since the products *ac* and *bd* are already in their simplest form, it seems
    unlikely that we can eliminate these calculations. We can however make the following
    observation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于乘积*ac*和*bd*已经处于最简形式，似乎我们不太可能消除这些计算。然而，我们可以做出以下观察：
- en: '![](assets/319a0a26-74e5-4319-9eff-7b0e7dea8bef.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/319a0a26-74e5-4319-9eff-7b0e7dea8bef.jpg)'
- en: 'When we subtract the quantities *ac* and *bd,* which we have calculated in
    the previous recursive step, we get the quantity we need, namely (*ad* + *bc*):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们减去我们在上一个递归步骤中计算的*ac*和*bd*时，我们得到我们需要的数量，即(*ad* + *bc*)：
- en: '![](assets/03bddaec-240b-473a-a996-0718fc542efd.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/03bddaec-240b-473a-a996-0718fc542efd.jpg)'
- en: 'This shows that we can indeed compute the sum of *ad + bc* without separately
    computing each of the individual quantities. In summary, we can improve on equation
    3.1 by reducing from four recursive steps to three. These three steps are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们确实可以计算*ad + bc*的和，而无需单独计算每个单独的数量。总之，我们可以通过将递归步骤从四步减少到三步来改进方程3.1。这三个步骤如下：
- en: Recursively calculate *ac.*
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归计算*ac*。
- en: Recursively calculate *bd.*
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归计算*bd*。
- en: Recursively calculate (*a* +*b*)(*c* + *d*) and subtract *ac* and *bd.*
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归计算(*a* +*b*)(*c* + *d*)并减去*ac*和*bd*。
- en: 'The following example shows a Python implementation of the Karatsuba algorithm:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了卡拉茨巴算法的Python实现：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To satisfy ourselves that this does indeed work, we can run the following test
    function:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保这确实有效，我们可以运行以下测试函数：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Runtime analysis
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行时间分析
- en: It should be becoming clear that an important aspect to algorithm design is
    gauging the efficiency both in terms of space (memory) and time (number of operations).
    This second measure, called runtime performance, is the subject of this section.
    It should be mentioned that an identical metric is used to measure an algorithm's
    memory performance. There are a number of ways we could, conceivably, measure
    run time and probably the most obvious is simply to measure the time the algorithm
    takes to complete. The major problem with this approach is that the time it takes
    for an algorithm to run is very much dependent on the hardware it is run on. A
    platform-independent way to gauge an algorithm's runtime is to count the number
    of operations involved. However, this is also problematic in that there is no
    definitive way to quantify an operation. This is dependent on the programming
    language, the coding style, and how we decide to count operations. We can use
    this idea, though, of counting operations, if we combine it with the expectation
    that as the size of the input increases the runtime will increase in a specific
    way. That is, there is a mathematical relationship between *n*, the size of the
    input, and the time it takes for the algorithm to run.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，算法设计的一个重要方面是评估效率，无论是在空间（内存）还是时间（操作次数）方面。第二个度量，称为运行性能，是本节的主题。值得一提的是，用于衡量算法内存性能的度量标准与此相同。我们可以以多种方式衡量运行时间，最明显的可能是简单地测量算法完成所需的时间。这种方法的主要问题在于算法运行所需的时间很大程度上取决于其运行的硬件。衡量算法运行时间的一个与平台无关的方法是计算所涉及的操作次数。然而，这也存在问题，因为没有明确的方法来量化一个操作。这取决于编程语言、编码风格以及我们决定如何计算操作。然而，如果我们将这种计算操作的想法与一个期望相结合，即随着输入规模的增加，运行时间将以特定方式增加，那么我们就可以使用这个想法。也就是说，输入规模*n*与算法运行时间之间存在数学关系。
- en: 'Much of the discussion that follows will be framed by the following three guiding
    principles. The rational and importance of these principles should become clearer
    as we proceed. These principles are as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的讨论大部分将围绕以下三个指导原则展开。随着我们的进行，这些原则的合理性和重要性将变得更加清晰。这些原则如下：
- en: Worst case analysis. Make no assumptions on the input data.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最坏情况分析。不对输入数据做任何假设。
- en: Ignore or suppress constant factors and lower order terms. At large inputs higher
    order terms dominate.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忽略或抑制常数因子和低阶项。在大输入中，高阶项占主导地位。
- en: Focus on problems with large input sizes.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关注输入规模较大的问题。
- en: Worst case analysis is useful because it gives us a tight upper bound that our
    algorithm is guaranteed not to exceed. Ignoring small constant factors, and lower
    order terms is really just about ignoring the things that, at large values of
    the input size, *n*, do not contribute, in a large degree, to the overall run
    time. Not only does it make our work mathematically easier, it also allows us
    to focus on the things that are having the most impact on performance.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: We saw with the Karatsuba algorithm that the number of multiplication operations
    increased to the square of the size, *n*, of the input. If we have a four-digit
    number the number of multiplication operations is 16; an eight-digit number requires
    64 operations. Typically, though, we are not really interested in the behavior
    of an algorithm at small values of *n*, so we most often ignore factors that increase
    at slower rates, say linearly with *n*. This is because at high values of *n*,
    the operations that increase the fastest as we increase *n*, will dominate.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: We will explain this in more detail with an example, the merge sort algorithm.
    Sorting is the subject of [Chapter 13](40b124ee-3b32-4a76-9524-463dbe813217.xhtml),
    *Sorting*, however, as a precursor and as a useful way to learn about runtime
    performance, we will introduce merge sort here.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The merge sort algorithm is a classic algorithm developed over 60 years ago.
    It is still used widely in many of the most popular sorting libraries. It is relatively
    simple and efficient. It is a recursive algorithm that uses a divide and conquer
    approach. This involves breaking the problem into smaller sub problems, recursively
    solving them, and then somehow combining the results. Merge sort is one of the
    most obvious demonstrations of the divide and conquer paradigm.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'The merge sort algorithm consists of three simple steps:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Recursively sort the left half of the input array.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recursively sort the right half of the input array.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge two sorted sub arrays into one.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A typical problem is sorting a list of numbers into a numerical order. Merge
    sort works by splitting the input into two halves and working on each half in
    parallel. We can illustrate this process schematically with the following diagram:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/250a5ae4-7c53-4800-9d19-06c6c7f8dc5c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'Here is the Python code for the merge sort algorithm:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We run this program for the following results:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/038ce698-89ab-4d99-82b2-ac75765f7e84.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: The problem that we are interested in is how we determine the running time performance,
    that is, what is the rate of growth in the time it takes for the algorithm to
    complete relative to the size of *n*. To understand this a bit better, we can
    map each recursive call onto a tree structure.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Each node in the tree is a recursive call working on progressively smaller
    sub problems:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bdb1209e-db62-4741-8468-d4809f3d0f48.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: Each invocation of merge-sort subsequently creates two recursive calls, so we
    can represent this with a binary tree. Each of the child nodes receives a sub
    set of the input. Ultimately we want to know the total time it takes for the algorithm
    to complete relative to the size of *n*. To begin with we can calculate the amount
    of work and the number of operations at each level of the tree.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Focusing on the runtime analysis, at level 1, the problem is split into two
    *n*/2 sub problems, at level 2 there is four *n*/4 sub problems, and so on. The
    question is when does the recursion bottom out, that is, when does it reach its
    base case. This is simply when the array is either zero or one.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: The number of recursive levels is exactly the number of times you need to divide
    *n* by 2 until you get a number that is at most 1\. This is precisely the definition
    of log2\. Since we are counting the initial recursive call as level 0, the total
    number of levels is log[2]*n* + 1.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Let's just pause to refine our definitions. So far we have been describing the
    number of elements in our input by the letter *n*. This refers to the number of
    elements in the first level of the recursion, that is, the length of the initial
    input. We are going to need to differentiate between the size of the input at
    subsequent recursive levels. For this we will use the letter *m* or specifically
    *m[j]* for the length of the input at recursive level *j.*
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Also there are a few details we have overlooked, and I am sure you are beginning
    to wonder about. For example, what happens when *m*/2 is not an integer, or when
    we have duplicates in our input array. It turns out that this does not have an
    important impact on our analysis here.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using a recursion tree to analyze algorithms is that we can
    calculate the work done at each level of the recursion. How to define this work
    is simply as the total number of operations and this of course is related to the
    size of the input. It is important to measure and compare the performance of algorithms
    in a platform independent way. The actual run time will of course be dependent
    on the hardware on which it is run. Counting the number of operations is important
    because it gives us a metric that is directly related to an algorithm's performance,
    independent of the platform.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, since each invocation of merge sort is making two recursive calls,
    the number of calls is doubling at each level. At the same time each of these
    calls is working on an input that is half of its parents. We can formalize this
    and say that:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: For level j , where *j* is an integer 0, 1, 2 ... log[2]*n*, there are two ^j
    sub problems each of size *n*/2^j.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the total number of operations, we need to know the number of operations
    encompassed by a single merge of two sub arrays. Let's count the number of operations
    in the previous Python code. What we are interested in is all the code after the
    two recursive calls have been made. Firstly, we have the three assignment operations.
    This is followed by three while loops. In the first loop we have an if else statement
    and within each of are two operations, a comparison followed by an assignment.
    Since there are only one of these sets of operations within the if else statements,
    we can count this block of code as two operations carried out *m* times. This
    is followed by two while loops with an assignment operation each. This makes a
    total of 4*m* + 3 operations for each recursion of merge sort.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Since *m* must be at least 1, the upper bound for the number of operations is
    7*m*. It has to be said that this has no pretense at being an exact number. We
    could of course decide to count operations in a different way. We have not counted
    the increment operations or any of the housekeeping operations; however, this
    is not so important as we are more concerned with the rate of growth of the runtime
    with respect to *n* at high values of *n*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: This may seem a little daunting since each call of a recursive call itself spins
    off more recursive calls, and seemingly explodes exponentially. The key fact that
    makes this manageable is that as the number of recursive calls doubles, the size
    of each sub problem halves. These two opposing forces cancel out nicely as we
    can demonstrate.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the maximum number of operations at each level of the recursion
    tree we simply multiply the number of sub problems by the number of operations
    in each sub problem as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a6d82944-b30f-4e9c-b46e-9e467765e528.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: Importantly this shows that, because the 2^j cancels out the number of operations
    at each level is independent of the level. This gives us an upper bound to the
    number of operations carried out on each level, in this example, 7*n*. It should
    be pointed out that this includes the number of operations performed by each recursive
    call on that level, not the recursive calls made on subsequent levels. This shows
    that the work done, as the number of recursive calls doubles with each level,
    is exactly counter balanced by the fact that the input size for each sub problem
    is halved.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the total number of operations for a complete merge sort we simply
    multiply the number of operations on each level by the number of levels. This
    gives us the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ac440c24-f7c5-48a7-a083-75d67cff8b8f.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'When we expand this out, we get the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d4c151e6-ea4e-4fb6-804c-648353a68837.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'The key point to take from this is that there is a logarithmic component to
    the relationship between the size of the input and the total running time. If
    you remember from school mathematics, the distinguishing characteristic of the
    logarithm function is that it flattens off very quickly. As an input variable,
    *x*, increases in size, the output variable, *y* increases by smaller and smaller
    amounts. For example, compare the log function to a linear function:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/983df66e-1889-4df2-98ba-d00d8853a7c0.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: In the previous example, multiplying the *n*log[2]*n* component and comparing
    it to *n*² .
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0c0197f3-2cd7-4b71-91f8-c4bb4a4f5f8c.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Notice how for very low values of *n*, the time to complete, *t* , is actually
    lower for an algorithm that runs in n² time. However, for values above about 40,
    the log function begins to dominate, flattening the output until at the comparatively
    moderate size *n* = 100, the performance is more than twice than that of an algorithm
    running in *n*² time. Notice also that the disappearance of the constant factor,
    + 7 is irrelevant at high values of *n*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'The code used to generate these graphs is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You will need to install the matplotlib library, if it is not installed already,
    for this to work. Details can be found at the following address; I encourage you
    to experiment with this list comprehension expression used to generate the plots.
    For example, adding the following plot statement:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Gives the following output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4129159f-5b74-4138-94d0-02abb037cc78.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: The preceding graph shows the difference between counting six operations or
    seven operations. We can see how the two cases diverge, and this is important
    when we are talking about the specifics of an application. However, what we are
    more interested in here is a way to characterize growth rates. We are not so much
    concerned with the absolute values, but how these values change as we increase
    *n*. In this way we can see that the two lower curves have similar growth rates,
    when compared to the top (*x*²) curve. We say that these two lower curves have
    the same **complexity class**. This is a way to understand and describe different
    runtime behaviors. We will formalize this performance metric in the next section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Asymptotic analysis
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are essentially three things that characterize an algorithm''s runtime
    performance. They are:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Worst case - Use an input that gives the slowest performance
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best case - Use an input that give, the best results
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average case - Assumes the input is random
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To calculate each of these, we need to know the upper and lower bounds. We have
    seen a way to represent an algorithm's runtime using mathematical expressions,
    essentially adding and multiplying operations. To use asymptotic analyses, we
    simply create two expressions, one each for the best and worst cases.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Big O notation
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The letter "O" in big *O* notation stands for order, in recognition that rates
    of growth are defined as the order of a function. We say that one function *T*(*n*)
    is a big O of another function, *F*(*n*), and we define this as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4b0a93a3-4b55-4fd3-8226-64d255fca9cc.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'The function, *g*(*n*), of the input size, *n*, is based on the observation
    that for all sufficiently large values of *n*, *g*(*n*) is bounded above by a
    constant multiple of *f*(*n*). The objective is to find the smallest rate of growth
    that is less than or equal to *f*(*n*). We only care what happens at higher values
    of *n*. The variable *n[0]*represents the threshold below which the rate of growth
    is not important, The function T(n) represents the **tight upper bound** F(n).
    In the following plot we see that *T*(*n*) = *n²* + 500 = *O*(*n²*) with *C* =
    2 and *n[0]* is approximately 23:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6ba4595b-ba4b-4157-9a3f-632eaa3e382a.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: You will also see the notation *f*(*n*) = *O*(*g*(*n*)). This describes the
    fact that *O*(*g*(*n*)) is really a set of functions that include all functions
    with the same or smaller rates of growth than *f*(n). For example, *O*(*n²*) also
    includes the functions *O*(*n*), *O*(*n*log*n*), and so on.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we list the most common growth rates in order from
    lowest to highest. We sometimes call these growth rates the **time complexity**
    of a function, or the complexity class of a function:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '| **Complexity Class** | **Name** | **Example operations** |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| O(1) | Constant | append, get item, set item. |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| O(log*n*) | Logarithmic | Finding an element in a sorted array. |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| O(n) | Linear | copy, insert, delete, iteration. |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| *n*Log*n* | Linear-Logarithmic | Sort a list, merge - sort. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| *n²* | Quadratic | Find the shortest path between two nodes in a graph. Nested
    loops. |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| *n³* | Cubic | Matrix multiplication. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| 2*^n* | Exponential | ''Towers of Hanoi'' problem, backtracking. |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: Composing complexity classes
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normally, we need to find the total running time of a number of basic operations.
    It turns out that we can combine the complexity classes of simple operations to
    find the complexity class of more complex, combined operations. The goal is to
    analyze the combined statements in a function or method to understand the total
    time complexity of executing several operations. The simplest way to combine two
    complexity classes is to add them. This occurs when we have two sequential operations.
    For example, consider the two operations of inserting an element into a list and
    then sorting that list. We can see that inserting an item occurs in O(*n*) time
    and sorting is O(*n*log*n*) time. We can write the total time complexity as O(*n*
    + *n*log*n*), that is, we bring the two functions inside the O(...). We are only
    interested in the highest order term, so this leaves us with just O(*n*log*n*).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'If we repeat an operation, for example, in a while loop, then we multiply the
    complexity class by the number of times the operation is carried out. If an operation
    with time complexity O(*f*(*n*)) is repeated O(*n*) times then we multiply the
    two complexities:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: O(*f*(*n*) * O(*n*)) = O(*nf*(*n*)).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose the function f(...) has a time complexity of O(*n*²) and
    it is executed *n* times in a while loop as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The time complexity of this loop then becomes O(*n*²) * O(*n*) = O(*n * n²*)
    = O(*n³*). Here we are simply multiplying the time complexity of the operation
    with the number of times this operation executes. The running time of a loop is
    at most the running time of the statements inside the loop multiplied by the number
    of iterations. A single nested loop, that is, one loop nested inside another loop,
    will run in *n*² time assuming both loops run *n* times. For example:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Each statement is a constant, c, executed *n**n* times, so we can express the
    running time as ; *c**n* *n* = *cn*² = O(*n*2).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'For consecutive statements within nested loops we add the time complexities
    of each statement and multiply by the number of times the statement executed.
    For example:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This can be written as *c*[0] +*c*[1]*n* + *cn*² = O(*n*²).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define (base 2) logarithmic complexity, reducing the size of the problem
    by ½, in constant time. For example, consider the following snippet:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Notice that `i` is doubling on each iteration, if we run this with *n* = 10
    we see that it prints out four numbers; 2, 4, 8, and 16\. If we double *n* we
    see it prints out five numbers. With each subsequent doubling of n the number
    of iterations is only increased by 1\. If we assume *k* iterations, we can write
    this as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e7efcb8e-d08f-4c5c-8ba8-ff765a96cdbf.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: From this we can conclude that the total time = **O**(*log(n)*).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Although Big O is the most used notation involved in asymptotic analysis, there
    are two other related notations that should be briefly mentioned. They are Omega
    notation and Theta notation.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Omega notation (Ω)
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a similar way that Big O notation describes the upper bound, Omega notation
    describes a **tight lower bound**. The definition is as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0be56dbf-c79b-4d73-b1f6-7b1411bb36f6.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: The objective is to give the largest rate of growth that is equal to or less
    than the given algorithms, T(*n*), rate of growth.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Theta notation (ϴ)
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is often the case where both the upper and lower bounds of a given function
    are the same and the purpose of Theta notation is to determine if this is the
    case. The definition is as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1da75a4c-29dc-4ab9-a25e-ae5b5209db08.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: Although Omega and Theta notations are required to completely describe growth
    rates, the most practically useful is Big O notation and this is the one you will
    see most often.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Amortized analysis
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often we are not so interested in the time complexity of individual operations,
    but rather the time averaged running time of sequences of operations. This is
    called amortized analysis. It is different from average case analysis, which we
    will discuss shortly, in that it makes no assumptions regarding the data distribution
    of input values. It does, however, take into account the state change of data
    structures. For example, if a list is sorted it should make any subsequent find
    operations quicker. Amortized analysis can take into account the state change
    of data structures because it analyzes sequences of operations, rather then simply
    aggregating single operations.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Amortized analysis finds an upper bound on runtime by imposing an artificial
    cost on each operation in a sequence of operations, and then combining each of
    these costs. The artificial cost of a sequence takes in to account that the initial
    expensive operations can make subsequent operations cheaper.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: When we have a small number of expensive operations, such as sorting, and lots
    of cheaper operations such as lookups, standard worst case analysis can lead to
    overly pessimistic results, since it assumes that each lookup must compare each
    element in the list until a match is found. We should take into account that once
    we sort the list we can make subsequent find operations cheaper.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'So far in our runtime analysis we have assumed that the input data was completely
    random and have only looked at the effect the size of the input has on the runtime.
    There are two other common approaches to algorithm analysis; they are:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Average case analysis
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmarking
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average case analysis finds the average running time based on some assumptions
    regarding the relative frequencies of various input values. Using real-world data,
    or data that replicates the distribution of real-world data, is many times on
    a particular data distribution and the average running time is calculated.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking is simply having an agreed set of typical inputs that are used
    to measure performance. Both benchmarking and average time analysis rely on having
    some domain knowledge. We need to know what the typical or expected datasets are.
    Ultimately we will try to find ways to improve performance by fine-tuning to a
    very specific application setting.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a straightforward way to benchmark an algorithm's runtime performance.
    This can be done by simply timing how long the algorithm takes to complete given
    various input sizes. As we mentioned earlier, this way of measuring runtime performance
    is dependent on the hardware that it is run on. Obviously faster processors will
    give better results, however, the relative growth rates as we increase the input
    size will retain characteristics of the algorithm itself rather than the hardware
    it is run on. The absolute time values will differ between hardware (and software)
    platforms; however, their relative growth will still be bound by the time complexity
    of the algorithm.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a simple example of a nested loop. It should be fairly obvious
    that the time complexity of this algorithm is O(n²) since for each n iterations
    in the outer loop there are also n iterations in the inter loop. For example,
    our simple nested for loop consists of a simple statement executed on the inner
    loop:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following code is a simple test function that runs the nest function with
    increasing values of `n`. With each iteration we calculate the time this function
    takes to complete using the `timeit.timeit` function. The `timeit` function, in
    this example, takes three arguments, a string representation of the function to
    be timed, a setup function that imports the nest function, and an `int` parameter
    that indicates the number of times to execute the main statement. Since we are
    interested in the time the nest function takes to complete relative to the input
    size, `n`, it is sufficient, for our purposes, to call the nest function once
    on each iteration. The following function returns a list of the calculated runtimes
    for each value of n:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the following code we run the test2 function and graph the results, together
    with the appropriately scaled n² function for comparison, represented by the dashed
    line:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This gives the following results:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/44c2f6b6-a425-4914-93b4-03030c9acde8.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: As we can see, this gives us pretty much what we expect. It should be remembered
    that this represents both the performance of the algorithm itself as well as the
    behavior of underlying software and hardware platforms, as indicated by both the
    variability in the measured runtime and the relative magnitude of the runtime.
    Obviously a faster processor will result in faster runtimes, and also performance
    will be affected by other running processes, memory constraints, clock speed,
    and so on.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have taken a general overview of algorithm design. Importantly,
    we saw a platform independent way to measure an algorithm's performance. We looked
    at some different approaches to algorithmic problems. We looked at a way to recursively
    multiply large numbers and also a recursive approach for merge sort. We saw how
    to use backtracking for exhaustive search and generating strings. We also introduced
    the idea of benchmarking and a simple platform-dependent way to measure runtime.
    In the following chapters, we will revisit many of these ideas with reference
    to specific data structures. In the next chapter, we will discuss linked lists
    and other pointer structures.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
