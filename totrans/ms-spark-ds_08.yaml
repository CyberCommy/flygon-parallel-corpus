- en: Chapter 8. Building a Recommendation System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If one were to choose an algorithm to showcase data science to the public, a
    recommendation system would certainly be in the frame. Today, recommendation systems
    are everywhere. The reason for their popularity is down to their versatility,
    usefulness, and broad applicability. Whether they are used to recommend products
    based on user's shopping behavior or to suggest new movies based on viewing preferences,
    recommenders are now a fact of life. It is even possible that this book was magically
    suggested based on what marketing companies know about you, such as your social
    network preferences, your job status, or your browsing history.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will demonstrate how to recommend music content using raw
    audio signal. For that purpose, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark to process audio files stored on HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about *Fourier transform* for audio signal transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Cassandra as a caching layer between online and offline layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using *PageRank* as an unsupervised recommendation algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating Spark Job Server with the Play framework to build an end-to-end
    prototype
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The end goal of a recommendation system is to suggest new items based on a
    user''s historical usage and preferences. The basic idea is to use a ranking for
    any product that a customer has been interested in in the past. This ranking can
    be explicit (asking a user to rank a movie from 1 to 5) or implicit (how many
    times a user visited this page). Whether it is a product to buy, a song to listen
    to, or an article to read, data scientists usually address this issue from two
    different angles: *collaborative filtering* and *content-based filtering*.'
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using this approach, we leverage big data by collecting more information about
    the behavior of people. Although an individual is by definition unique, their
    shopping behavior is usually not, and some similarities can always be found with
    others. The recommended items will be targeted for a particular individual, but
    they will be derived by combining the user''s behavior with that of similar users.
    This is the famous quote from most retail websites:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"People who bought this also bought that..."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Of course, this requires prior knowledge about the customer, their past purchases
    and you must also have enough information about other customers to compare against.
    Therefore, a major limiting factor is that items must have been viewed at least
    once in order to be shortlisted as a potential recommended item. In fact, we cannot
    recommend an item until it has been seen/bought at least once.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The iris dataset of collaborative filtering is usually done using samples of
    the LastFM dataset: [http://labrosa.ee.columbia.edu/millionsong/lastfm](http://labrosa.ee.columbia.edu/millionsong/lastfm).
  prefs: []
  type: TYPE_NORMAL
- en: Content-based filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative approach, rather than using similarities with other users, involves
    looking at the product itself and the type of products a customer has been interested
    in in the past. If you are interested in both *classical music* and *speed meta*l,
    it is safe to assume that you would probably buy (at least consider) any new albums
    mixing up both classical rhythms with heavy metal riffs. Such a recommendation
    would be difficult to find in a collaborative filtering approach as no one in
    your neighborhood shares your musical taste.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of this approach is that, assuming we have enough knowledge
    about the content to recommend (such as the categories, labels, and so on), we
    can recommend a new item even when no one has seen it before. The downside is
    that the model can be more difficult to build and selecting the right features
    with no loss of information can be challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Custom approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the focus of this book is *Mastering Spark for Data Science* we wish to
    provide the reader with a new and innovative way of addressing the recommendation
    issue, rather than just explaining the standard collaborative filtering algorithm
    that anyone could build using the out-of-the-box Spark APIs and following a basic
    tutorial [http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html).
    Let''s start with a hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If we were to recommend songs to end-users, couldn''t we build a system that
    would recommend songs, not based on what people like or dislike, nor on the song
    attributes (genre, artist), but rather on how the song really sounds and how you
    feel about it?*'
  prefs: []
  type: TYPE_NORMAL
- en: In order to demonstrate how to build such a system, (and since you likely do
    not have access to a public dataset containing both music content and ranking
    a legitimate one at least), we will explain how to construct it locally using
    your own personal music library. Feel free to play along!
  prefs: []
  type: TYPE_NORMAL
- en: Uninformed data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following technique could be seen as something of a game changer in how
    most modern data scientists work. While it is common to work with structured and
    unstructured text, it is less common to work on raw binary data the reason being
    the gap between computer science and data science. Textual processing is limited
    to a standard set of operations that most will be familiar with, that is, acquiring,
    parsing and storing, and so on. Instead of restricting ourselves to these operations,
    we will work directly with audio transforming and enrich the uninformed signal
    data into informed transcription. In doing this, we enable a new type of data
    pipeline that is analogous to teaching a computer to *hear* the voice from audio
    files.
  prefs: []
  type: TYPE_NORMAL
- en: A second (breakthrough) idea that we encourage here is a shift in thinking around
    how data scientists engage with Hadoop and big data nowadays. While many still
    consider these technologies as just *yet another database*, we want to showcase
    the vast array of possibilities that become available with these tools. After
    all, no one laughs at the data scientist who can train a machine to talk to customers
    or make sense of call center recordings.
  prefs: []
  type: TYPE_NORMAL
- en: Processing bytes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing to consider is the audio file format. The `.wav` files can
    be processed pretty much as they are using the `AudioSystem` library (from `javax.sound`),
    while an `.mp3` would require pre-processing using external codec libraries. If
    we read a file from an `InputStream`, we can create an output byte array containing
    audio signals as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Songs are usually encoded using a sample rate of 44KHz, which, according to
    the **Nyquist** theorem, is twice as large as the highest frequency that the human
    ear can perceive (covering ranges from 20Hz to 20KHz).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information about the Nyquist theorem, please visit: [http://redwood.berkeley.edu/bruno/npb261/aliasing.pdf](http://redwood.berkeley.edu/bruno/npb261/aliasing.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to represent the sound that a human being can hear, we would need
    around 44,000 samples per seconds, hence 176,400 bytes per second for stereo (two
    channels). The latter is the following byte frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we access the audio signal by processing the output byte array and
    plotting the first few bytes of our sample data (in this case, *Figure 1*, shows
    the Mario Bros theme song). Note the timestamp that can be retrieved using both
    the byte index and the byte frequency values, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Processing bytes](img/image_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Mario Bros theme - time domain'
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience, we wrap all these audio characteristics into a case class
    `Audio` (shown in the following snippet) to which we will add additional utility
    methods as we go along in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Creating a scalable code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have created functions to extract audio signals from `.wav` files
    (via a `FileInputStream`), naturally the next step is to use it to process the
    remaining records stored on HDFS. As already highlighted in previous chapters,
    this isn''t a difficult task once the logic works on a single record. In fact,
    Spark comes with a utility to process binary data out of the box, so we simply
    plug in the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We make sure we only send `.wav` files to our processor and get a new RDD made
    of a filename (the song name) and its corresponding `Audio` case class (including
    the extracted audio signal).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `binaryFiles` method of Spark reads a file whole (without splitting it)
    and outputs an RDD containing both the file path and its corresponding input stream.
    Therefore, it is advised to work on relatively small files (perhaps just a few
    megabytes) as it clearly affects memory consumption and hence performance.
  prefs: []
  type: TYPE_NORMAL
- en: From time to frequency domain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Accessing the audio time domain is a great achievement, but sadly it's not much
    value on its own. However, we can use it to better understand what the signal
    truly represents, that is, to extract the hidden frequencies it comprises. Naturally,
    we can convert the time domain signal to a frequency domain using *Fourier transform*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can learn more about *Fourier transform* at [http://www.phys.hawaii.edu/~jgl/p274/fourier_intro_Shatkay.pdf](http://www.phys.hawaii.edu/~jgl/p274/fourier_intro_Shatkay.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: As a summary, without going into too much detail or having to tackle the complex
    equations, the basic assumption that Joseph Fourier makes in his legendary and
    eponymous formula is that all signals are made of an infinite accumulation of
    sine waves from different frequencies and phases.
  prefs: []
  type: TYPE_NORMAL
- en: Fast Fourier transform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Discrete Fourier transform** (**DFT**) is the summation of different sine
    waves and can be represented using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fast Fourier transform](img/image_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although this algorithm is trivial to implement using a brute force approach,
    it is highly inefficient *O(n²)* since for each data point *n*, we have to compute
    the sum of *n* exponents. Therefore, a three-minute song would generate *(3 x
    60 x 176,400)²≈ 10^(15)* number of operations. Instead, Cooley and Tukey contributed
    a **Fast Fourier transform** (**FFT**) using a divide and conquer approach to
    the DFT that reduces the overall time complexity to *O(n.log(n))*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The official paper describing the Cooley and Tukey algorithm can be found online: [http://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf](http://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately for us, there are existing FFT implementations available, and so
    we will compute the FFT using a Java-based library provided by `org.apache.commons.math3`.
    When using this library, we need only to ensure that our input data is padded
    with zeros so that the total length is a power of two and can be divided into odd
    and even sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns an array of `Complex` numbers that are made of real and imaginary
    parts and can be easily converted to a frequency and amplitude (or magnitude)
    as follows. According to the Nyquist theorem, we only need half the frequencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we include these functions in the `Audio` case class and plot the
    frequency domain for the first few seconds of the Mario Bros theme song:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fast Fourier transform](img/image_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Mario Bros theme - frequency domain'
  prefs: []
  type: TYPE_NORMAL
- en: In Figure 2, significant peaks can be seen in the medium-high frequency range
    (between 4KHz and 7KHz) and we will use these as a fingerprint for the song.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling by time window
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although more efficient, the FFT is still an expensive operation due to its
    high memory consumption (remember, a typical three-minute song would have around
    *3 x 60 x 176,400* points to process). This becomes especially problematic when
    applied to a large number of data points and so must be taken into consideration
    for large scale processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of looking at the full spectrum, we sample our songs using a time window.
    In fact, a full FFT wouldn''t be of any use anyway since we want to know the time
    each major frequency was heard. Therefore, we iteratively split each `Audio` class
    into smaller case classes of 20 millisecond samples. This timeframe should be
    small enough for the purpose of analysis meaning small enough so that the FFT
    can be computed and dense enough to ensure that sufficient frequencies are extracted
    to provide an adequate audio fingerprint. The produced chunks of 20 milliseconds
    will drastically increase the overall size of our RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While this is not our primary focus, one could rebuild the full FFT spectrum
    of the entire signal by recombining samples with inner and outer FFT and applying
    a twiddle factor [https://en.wikipedia.org/wiki/Twiddle_factor](https://en.wikipedia.org/wiki/Twiddle_factor).
    This could be useful when processing large records with a limited amount of available
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting audio signatures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we have multiple samples at regular time intervals, we can extract frequency
    signatures using FFT. In order to generate a sample signature, instead of using
    the exact peak (which could be approximate), we try to find the closest note in
    different frequency bands. This offers an approximation, but in doing so it overcomes
    any noise issues present in the original signal, as noise interferes with our
    signatures.
  prefs: []
  type: TYPE_NORMAL
- en: We look at the following frequency bands 20-60 Hz, 60-250Hz, 250-2000Hz, 2-4Kz,
    and 4-6Kz and find the closest note according to the following frequency reference
    table. These bands are not random. They correspond to the different ranges of
    musical instruments (for example, double bass spans between 50 to 200Hz, piccolo
    from 500 to 5KHz).
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting audio signatures](img/B05261_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Frequency note reference table'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4*, shows the first sample of our Mario Bros theme song in a lower
    frequency band. We can see that the highest magnitude of 43Hz corresponds to the
    prime octave of note **F**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting audio signatures](img/image_08_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Mario Bros theme-lower frequencies'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each sample, we build a hash composed of five letters (such as [**E**-**D#**-**A**-**B**-**B**-**F**])
    corresponding to the strongest note (the highest peak) in each of the preceding
    frequency bands. We consider this hash a fingerprint for that particular 20 milliseconds
    time window. We then build a new RDD made of hash values as follows (we include
    a hashing function within our `Audio` case class):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we group all song IDs sharing the same hash in order to build an RDD of
    unique hashes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Our assumption is that when a hash is defined in a song at a particular time
    window, similar songs could potentially share similar hashes, but two songs having
    all the same hashes (and in order) would be truly identical; one could share part
    of my DNA, but one having the exact same DNA would be a perfect clone of myself.
  prefs: []
  type: TYPE_NORMAL
- en: If a music aficionado is feeling blessed listening to the *concerto in D* by
    Tchaikovsky, can we recommend *Pachelbel's Canon in D* just because both of them
    share a musical cadence (that is, common frequencies around *D*)?
  prefs: []
  type: TYPE_NORMAL
- en: Is it valid (and feasible) to recommend playlists that are only based on certain
    frequency bands? Surely the frequencies themselves would not be enough to fully
    describe a song. What about tempo, timbre, or rhythm? Is this model complete enough
    to accurately represent all the nuances of musical diversity and range? Probably
    not, but for the purpose of data science, it's worth investigating anyway!
  prefs: []
  type: TYPE_NORMAL
- en: Building a song analyzer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However, before deep diving into the recommender itself, the reader may have
    noticed an important property that we were able to extract out of the signal data.
    Since we generated audio signatures at regular time intervals, we can compare
    signatures and find potential duplicates. For example, given a random song, we
    should be able to guess the title, based on previously indexed signatures. In
    fact, this is the exact approach taken by many companies when providing music
    recognition services. To take it one step further, we could potentially provide
    insight into a band's musical influences, or further, perhaps even identify song
    plagiarism, once and for all settling the *Stairway to Heave*n dispute between
    Led Zeppelin and the American rock band Spirit [http://consequenceofsound.net/2014/05/did-led-zeppelin-steal-stairway-to-heaven-legendary-rock-band-facing-lawsuit-from-former-tourmates/](http://consequenceofsound.net/2014/05/did-led-zeppelin-steal-stairway-to-heaven-legendary-rock-band-facing-lawsuit-from-former-tourmates/).
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, we will take a detour from our recommendation use case by
    continuing our investigation into song identification a little further. Next,
    we build an analyzer system capable of anonymously receiving a song, analyzing
    its stream, and returning the title of the song (in our case, the original filename).
  prefs: []
  type: TYPE_NORMAL
- en: Selling data science is all about selling cupcakes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sadly, an all too often neglected aspect of the data science journey is data visualization.
    In other words, how to present your results back to end users. While many data
    scientists are content to present their findings in an Excel spreadsheet, today's
    end users are keen for richer, more immersive experiences. Often they want to
    play around, *interacting* with data. Indeed, providing an end user with a full,
    end-to-end user experience even a simple one can be a great way to spark interest
    in your science; making a simple proof of concept into a prototype people can
    easily understand. And due to the prevalence of Web 2.0 technologies, user expectations
    are high, but thankfully, there are a variety of free, open source products that
    can help, for example, Mike Bostock’s D3.js, is a popular framework that provides
    a toolkit for creating just such user interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Selling data science without rich data visualization is like trying to sell
    a cake without icing, few people will trust in the finished product. Therefore,
    we will build a user interface for our analyzer system. But first, let's get the
    audio data out of Spark (our hashes are currently stored in memory inside an RDD)
    and into a web-scale datastore.
  prefs: []
  type: TYPE_NORMAL
- en: Using Cassandra
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need a fast, efficient, and distributed key-value store to keep all our
    hash values. Although many databases are fit for this purpose, we''ll choose Cassandra
    in order to demonstrate its integration with Spark. First, import the Cassandra
    input and output formats using the Maven dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As you would expect, persisting (and retrieving) RDDs from Spark to Cassandra
    is relatively trivial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a new table `hashes` on keyspace `gzet`, inferring the schema
    from the `HashSongsPair` object. The following is the equivalent SQL statement
    executed (provided here for information only):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Using the Play framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As our Web UI will front the complex processing required to transform a song
    into frequency hashes, we want it to be an interactive web application rather
    than a simple set of static HTML pages. Furthermore, this must be done in the
    exact same way and with the same functions as we did using Spark (that is, the
    same song should generate the same hashes). The Play framework ([https://www.playframework.com/](https://www.playframework.com/))
    will allow us to do this, and Twitter's bootstrap ([http://getbootstrap.com/](http://getbootstrap.com/))
    will be used to put the icing on the cake, for a more professional look and feel.
  prefs: []
  type: TYPE_NORMAL
- en: Although this book is not about building user interfaces, we will introduce
    some concepts related to the Play framework, as if used well it can provide a
    source of great value for data scientists. As always, the full code is available
    in our GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a **data access layer**, responsible for handling connections
    and queries to Cassandra. For any given hash, we return the list of matching song
    IDs. Similarly, for any given ID, we return the song name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a simple **view**, made of three objects, a `text` field, a
    file `Upload`, and a `submit` button. These few lines are enough to provide our
    user interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we create a **controller** that will handle both `GET` and `POST` HTTP
    requests through the `index` and `submit` methods, respectively. The latter will
    process the uploaded file by converting a `FileInputStream` into an `Audio` case
    class, splitting it into 20 millisecond chunks, extracting the FFT signatures
    (hashes) and querying Cassandra for matching IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we return the matching result (if any) through a flashing message
    and we chain both the view and controller together by defining our new routes
    for our `Analyze` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting UI is reported in *Figure 5*, and works perfectly with our own
    music library:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the Play framework](img/image_08_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Sound analyser UI'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following *Figure 6* shows the end-to-end process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the Play framework](img/image_08_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Sound analyser process'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, the Play framework shares some pieces of code with our offline
    Spark job. This is made possible because we are programming in a functional style
    and have applied a good separation of concerns. While the Play framework does
    not work natively with Spark (in terms of RDDs and Spark context objects), because
    they are not dependent on Spark, we can use any of the functions we created earlier
    (such as the ones in the Audio class). This is one of the many advantages of functional
    programming; functions, by definition, are stateless and represent a key component
    in the adoption of a hexagonal **architecture**: [http://wiki.c2.com/?HexagonalArchitecture](http://wiki.c2.com/?HexagonalArchitecture).
    Isolated functions can always be called by different actors, whether it is inside
    of an RDD or within a Play controller.
  prefs: []
  type: TYPE_NORMAL
- en: Building a recommender
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've explored our song analyzer, let's get back on track with the
    recommendation engine. As discussed earlier, we would like to recommend songs
    based on frequency hashes extracted from audio signals. Taking as an example the
    dispute between Led Zeppelin and Spirit, we would expect both songs to be relatively
    close to each other, as the allegation is that they share a melody. Using this
    thought as our main assumption, we could potentially recommend *Taurus* to someone
    interested in *Stairway to Heaven*.
  prefs: []
  type: TYPE_NORMAL
- en: The PageRank algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of recommending a specific song, we will recommend playlists. A playlist
    would consist of a list of all our songs ranked by relevance, most to least relevant.
    Let's begin with the assumption that people listen to music in a similar way to
    the way they browse articles on the web, that is, following a logical path from
    link to link, but occasionally switching direction, or teleporting, and browsing
    to a totally different website. Continuing with the analogy, while listening to
    music one can either carry on listening to music of a similar style (and hence
    follow their most expected journey), or skip to a random song in a totally different
    genre. It turns out that this is exactly how Google ranks websites by popularity
    using a **PageRank** algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more details on the PageRank algorithm visit: [http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf](http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: The popularity of a website is measured by the number of links it points to
    (and is referred from). In our music use case, the popularity is built as the
    number hashes a given song shares with all its neighbors. Instead of popularity,
    we introduce the concept of song commonality.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Graph of Frequency Co-occurrence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start by reading our hash values back from Cassandra and re-establishing
    the list of song IDs for each distinct hash. Once we have this, we can count the
    number of hashes for each song using a simple `reduceByKey` function, and because
    the audio library is relatively small, we collect and broadcast it to our Spark
    executors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we build a co-occurrence matrix by getting the cross product of every
    song sharing a same hash value, and count how many times the same tuple is observed.
    Finally, we wrap the song IDs and the normalized (using the term frequency we
    just broadcast) frequency count inside of an `Edge` class from GraphX:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We are only keeping edges with a weight (meaning a hash co-occurrence) greater
    than a predefined threshold in order to build our hash frequency graph.
  prefs: []
  type: TYPE_NORMAL
- en: Running PageRank
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contrary to what one would normally expect when running a PageRank, our graph
    is undirected. It turns out that for our recommender, the lack of direction does
    not matter, since we are simply trying to find similarities between Led Zeppelin
    and Spirit. A possible way of introducing direction could be to look at the song
    publishing date. In order to find musical influences, we could certainly introduce
    a chronology from the oldest to newest songs giving directionality to our edges.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following `pageRank`, we define a probability of 15% to skip, or **teleport**
    as it is known, to any random song, but this can be obviously tuned for different
    needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we extract the page ranked vertices and save them as a playlist in
    Cassandra via an RDD of the `Song` case class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The reader may be pondering the exact purpose of PageRank here, and how it could
    be used as a recommender? In fact, our use of PageRank means that the highest
    ranking songs would be the ones that share many frequencies with other songs.
    This could be due to a common arrangement, key theme, or melody; or maybe because
    a particular artist was a major influence on a musical trend. However, these songs
    should be, at least in theory, more popular (by virtue of the fact they occur
    more often), meaning that they are more likely to have mass appeal.
  prefs: []
  type: TYPE_NORMAL
- en: On the other end of the spectrum, low ranking songs are ones where we did not
    find any similarity with anything we know. Either these songs are so avant-garde
    that no one has explored these musical ideas before, or alternatively are so bad
    that no one ever wanted to copy them! Maybe they were even composed by that up-and-coming
    artist you were listening to in your rebellious teenage years. Either way, the
    chance of a random user liking these songs is treated as negligible. Surprisingly,
    whether it is a pure coincidence or whether this assumption really makes sense,
    the lowest ranked song from this particular audio library is Daft Punk's--*Motherboard*
    it is a title that is quite original (a brilliant one though) and a definite unique
    sound.
  prefs: []
  type: TYPE_NORMAL
- en: Building personalized playlists
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We just have seen that a simple PageRank could help us create a general-purpose
    playlist. And although this isn't targeted towards any individual, it could serve
    as a playlist for a random user. It is the best recommendation we can make without
    any information about a user's preferences. The more we will learn about a user,
    the better we can personalize the playlist towards what they truly prefer. To
    do this, we would probably follow a content-based recommendation approach.
  prefs: []
  type: TYPE_NORMAL
- en: Without up-front information about a user's preferences, we can seek to collect
    our own information whenever a user plays a song, and hence personalize their
    playlist at runtime. To do this, we will assume that our user was enjoying the
    previous song that they listened to. We will also need to disable teleporting
    and generate a new playlist that is seeded from that particular song ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'PageRank and personalized PageRank are identical in the way that they compute
    scores (using the weight of incoming/outgoing edges), but the personalized version
    only allows users to teleport to the provided ID. A simple modification of the
    code allows us to personalize PageRank using a certain community ID (see [Chapter
    7](ch07.xhtml "Chapter 7. Building Communities"), *Building Communities*, for
    a definition of communities) or using a certain music attribute such as the artist
    or the genre. Given our previous graph, a personalized page rank is implemented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Here, the chance of teleporting to a random song is zero. There is still a 10%
    chance of skipping, but within only a very small tolerance of the provided song
    ID. In other words, regardless of the song we are currently listening to, we essentially
    defined a 10% chance of playing the song that we provide as a seed.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding our cupcake factory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to our song analyzer prototype, we want to present our suggested playlist
    back to our imaginary customer in a nice and tidy user interface.
  prefs: []
  type: TYPE_NORMAL
- en: Building a playlist service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Still using the Play framework, our technology stack stays the same, this time
    we simply create a new endpoint (a new route):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Just as before, we create an additional controller that handles simple GET
    requests (triggered when a user loads the playlist webpage). We load the generic
    playlist stored in Cassandra, wrap all these songs inside of a `Playlist` case
    class, and send it back to the `playlist.scala.html` view. The controller model
    looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The view remains reasonably simple, as we iterate through all the songs to
    display, ordered by commonality (from the most to least common ones):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note the `href` attribute in each list item - any time a user clicks on a song
    from that list, we will be generating a new `REST` call to the /playlist/id endpoint
    (this is described in the following section).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we are pleased to unveil the recommended (generic) playlist in *Figure
    7*. For some reason unknown to us, apparently a novice to classical music should
    start listening to *Gustav Mahler, Symphony No. 5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building a playlist service](img/B05261_08_08-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Playlist recommender'
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging the Spark job server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here comes yet another interesting challenge. Although our list of songs for
    a generic playlist and PageRank score is stored on Cassandra, this is not feasible
    for personalized playlists, as it would require pre-computation of all the PageRank
    scores for all the possible song IDs. And as we want to build the personalized
    playlists in pseudo real time, and we might also be loading new songs fairly regularly,
    we need to find a better approach than starting up a `SparkContext` upon every
    request.
  prefs: []
  type: TYPE_NORMAL
- en: The first constraint is that the PageRank function by nature is a distributed
    process and cannot be used outside of the context of Spark (that is, inside our
    Play framework's JVM). We appreciate that creating a new Spark job on each http
    request would certainly be a bit of an overkill, so we would like to start one
    single Spark job and process new graphs only when needed, ideally through a simple
    REST API call.
  prefs: []
  type: TYPE_NORMAL
- en: The second challenge is that we do not wish to load the same graph dataset from
    Cassandra repeatedly. This should be loaded once and cached in Spark memory and
    shared across different jobs. In Spark terminology, this would require an RDD
    to be accessible from a shared context.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, both points are addressed with Spark **job server** ([https://github.com/spark-jobserver/spark-jobserver](https://github.com/spark-jobserver/spark-jobserver)).
    Although this project is fairly immature (or at least not quite production-ready
    yet), it is a perfectly viable solution for showcasing data science.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purpose of this book, we compile and deploy a Spark job server using
    a local configuration only. We strongly encourage the reader to have a deeper
    look at the job server website (see previous link) for more information about
    packaging and deployment. Once our server starts, we need to create a new context
    (meaning starting up a new Spark job) with additional configuration settings for
    handling connection to Cassandra. We give this context a name so that we can use
    it later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to modify our code to be Spark job server compliant. We need
    the following dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We modify our SparkJob using the signature of the `SparkJob` interface that
    comes with job server. This is a requirement of all Spark job server jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the `validate` method, we ensure that all the job requirements will be satisfied
    (such as the input configuration needed for that job), and in `runJob` we execute
    our normal Spark logic just as we did before. The last change is that, while we
    will still be storing our generic playlist into Cassandra, we will cache the nodes
    and edges RDDs in Spark shared memory where it will be made available to further
    jobs. This can be done by extending the `NamedRddSupport` trait.
  prefs: []
  type: TYPE_NORMAL
- en: 'We simply have to save both edges and node RDDs (note that saving a `Graph`
    object is not supported yet) to keep accessing the graph in subsequent jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'From the personalized `Playlist` job, we retrieve and process our RDDs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We then execute our personalized PageRank, but instead of saving the results
    back to Cassandra, we will simply collect the first 50 songs. When deployed, this
    action will implicitly output this list back to the client thanks to the magic
    of the job server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We compile our code and publish our shaded jar file into job server by giving
    it an application name as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we are almost ready to deploy our recommendation system, let''s recap
    over what we are going to demo. We will be executing the two different user flows
    shortly:'
  prefs: []
  type: TYPE_NORMAL
- en: When a user logs in to the recommendation page, we retrieve the latest generic
    playlist available in Cassandra. Alternatively, we start a new asynchronous job
    to create a new one if needed. This will load the required RDDs within the Spark
    context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a user plays a new song from our recommended items, we spin up a synchronous
    call to the Spark job server and build the next playlist based around this song
    ID.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The flow for the generic PageRank playlist is shown in *Figure 8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Leveraging the Spark job server](img/image_08_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Playlist recommender process'
  prefs: []
  type: TYPE_NORMAL
- en: 'The flow for the personalized PageRank playlist is shown in Figure 9:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Leveraging the Spark job server](img/image_08_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Personalized playlist recommender process'
  prefs: []
  type: TYPE_NORMAL
- en: User interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The final remaining piece of the puzzle is to make a call to the Spark job
    server from the service layer in Play Framework. Although this is done programmatically
    using the `java.net` package, as it''s a REST API the equivalent `curl` requests
    are shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Initially, when we built our HTML code, we introduced a link, or `href`, to
    `/playlist/${id}`. This REST call will be converted for you into a GET request
    to the `Playlist` controller and bound to your `personalize` function, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The first call to the Spark job server will spin up a new Spark job synchronously,
    read the results back from the job output, and redirect to the same page view
    with an updated playlist, this time based around this song ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: And voila, the resulting UI is shown in *Figure 10*. Anytime a user plays a
    song, the playlist will be updated and displayed, acting as a full-blown ranking
    recommendation engine.
  prefs: []
  type: TYPE_NORMAL
- en: '![User interface](img/B05261_08_11-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Personalized playlist recommender process'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While our recommendation system may not have taken the typical textbook approach,
    nor may it be the most accurate recommender possible, it does represent a fully
    demonstrable and incredibly interesting approach to one of the most commonplace
    techniques in data science today. Further, with persistent data storage, a REST
    API interface, distributed shared memory caching, and a modern web 2.0-based user
    interface, it provides a reasonably complete and rounded candidate solution.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, building a production-grade product out of this prototype would still
    require much effort and expertise. There are still improvements to be sought in
    the area of signal processing. For example, one could improve the sound pressure
    and reduce the signal noise by using a loudness filter, [http://languagelog.ldc.upenn.edu/myl/StevensJASA1955.pdf](http://languagelog.ldc.upenn.edu/myl/StevensJASA1955.pdf),
    by extracting pitches and melodies, or most importantly, by converting stereo
    to a mono signal.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All these processes are actually part of an active area of research - readers
    can look at some of the following publications: [http://www.justinsalamon.com/publications.html](http://www.justinsalamon.com/publications.html)
    and [http://www.mattmcvicar.com/publications/](http://www.mattmcvicar.com/publications/).
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we questioned how one can improve data science demonstrations by
    using simple (interactive) user interfaces. As mentioned, this is an often overlooked
    aspect and a key feature of presentation. Even in the early stages of a project,
    it's worth investing some time in data visualization, as it can be especially
    useful when convincing business people of the viability of your product.
  prefs: []
  type: TYPE_NORMAL
- en: One final thought, as an aspirational chapter we explored innovative ways to
    address data science use cases in a Spark environment. By balancing skills between
    mathematics and computer science, data scientists should feel free to explore,
    to be creative, to push back the frontier of what is feasible, to undertake what
    people say is not, but most importantly, to have fun with data. For this is the
    main reason why being a data scientist is considered the *sexiest job of the 21st
    century*.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter was a musical interlude. In the next chapter, we will be looking
    at classifying GDELT articles by bootstrapping a classification model using Twitter
    data, another ambitious task to say the least.
  prefs: []
  type: TYPE_NORMAL
