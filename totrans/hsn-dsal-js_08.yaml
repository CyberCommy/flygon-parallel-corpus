- en: Big O Notation, Space, and Time Complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapters, we have often spoken about optimizing our code/algorithms
    and have briefly used the terms space and time complexity and how we want to reduce
    them. As the name suggests, it is the complexity of the code that we want to keep
    to a minimum, but what does that entail? What are the different levels of this
    said complexity? How do we calculate the space and time complexity of an algorithm?
    These are the questions that we will address in this chapter while discussing
    the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Varying degrees of time complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Space complexity and Auxiliary space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The terminology used when discussing the space and time complexity of an algorithm
    is something that one, as a developer, will come across very often. Popular terms
    such as **Big-O***, *also known as **O (something)***, *and some not-so-popular
    terms such as **Omega (something)** or **Theta (something)** are often used to
    describe the complexity of an algorithm. The O actually stands for Order, which
    represents the order of the function.
  prefs: []
  type: TYPE_NORMAL
- en: Let's first talk only about the time complexity of an algorithm. This basically
    boils down to us trying to figure out how long it will take for a system to execute
    our algorithm for a given dataset (D). We can technically run this algorithm on
    the said system and log its performance, but since not all systems are the same
    (for example, OS, number of processors, and read write speeds), we can't necessarily
    expect the result to truly represent the average time it would take to execute
    our algorithm for our dataset, D. At the same time, we would also need to know
    how our algorithm performs with the changing size of our dataset, D. Does it take
    the same time for 10 elements and 1000 elements? Or does the time taken increase
    exponentially?
  prefs: []
  type: TYPE_NORMAL
- en: With all the above, how do we clearly understand the complexity of an algorithm?
    We do this by breaking down an algorithm into a set of basic operations and then
    combine them to get the overall number/complexity of each operation. This truly
    defines the time complexity of an algorithm as the rate of growth of time with
    the size of the input dataset, D.
  prefs: []
  type: TYPE_NORMAL
- en: Now, to calculate the time complexity in abstract terms, let's assume that we
    have a machine that takes one unit of time to perform some basic operations such
    as read, write, assignments, arithmetic, and logical calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that said, let''s take a simple function that returns the square of a
    given number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We have defined our machine, which consumes one unit of time, to perform the
    multiplication and another 1 unit to return the result. Irrespective of the input,
    our algorithm always takes only 2 units of time, and since this is not changing,
    it is referred to as a constant time algorithm. It does not matter that the constant
    time taken here is k units of time. We can represent all the similar functions,
    which take a constant time to execute as a set of functions `O(1)` or `big-O(1)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take another example in which we loop over a list of size n and multiply
    each of the elements by a factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To calculate the time complexity of this function, we will need to first calculate
    the cost to execute each statement in this function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first statement executes *n + 1* times before it breaks out of it, and
    each time it is performed, it costs 1 unit to increment and 1 unit to make the
    comparison check among other things. In other words, we can assume that it costs
    us *C*[*1* ]units of time in each iteration, so the total cost is *C[1]*(n+1)*
    for the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next statement, we will multiply the value in the array at a given index
    by a factor of 2\. Since this is within the loop, this statement is executed n times,
    and each time it does, let''s assume it costs us *C[2]* units. So, the total cost
    of execution of this line would be *C[2]* n*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we finally have the return statement, which also takes a constant amount
    of time—*C[3]*—to return the final array to the caller. Putting all these costs
    together, we have the total cost of the method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can see that, in this case, the cost of the method is directly proportional
    to the size of the input array, `N`. Thus, this set of functions can be represented
    by `O(n)`, indicating that they are directly proportional to the input size.
  prefs: []
  type: TYPE_NORMAL
- en: However, before we jump to more examples, let's first take a look at how we
    will represent the complexity without all the calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Asymptotic Notations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Asymptotic Notations come in handy when we want to derive and compare the time
    complexity of two or more algorithms. What Asymptotic Notations mean is that,
    once we have the time complexity of an algorithm calculated, we are simply going
    to start replacing *n* (the size of the input of our algorithm) with a very large
    number (tending toward infinity) and then get rid of the constant from the equation.
    Doing this would leave us with the only factors that truly affect our execution
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the same example as we did in the preceding section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: When we apply the rules we just described relating to Asymptotic Notations,
    that is, *n -> Infinity*, we can quickly see that the effects of `C[4]` are rather
    insignificant and can be dropped. We can also say the same for the multiplication
    factor `C[5]`. What we are left with is that this time, `T[double]`, is directly
    proportional to the size of the input array `(n)` and thus we were able to denote
    this with the `O(n)` notation since the size n is the only variable that matters
    in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main types of Asymptotic Notations, which can be used to classify
    the running time of an algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Big-O**: Represents the upper bound of the rate of growth of runtime'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Omega**: Represents the lower bound of the rate of growth of runtime'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Theta**: Represents the tight bound of the rate of growth of runtime'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big-O notation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assume that we have a `f(n)` method, which we want to represent with
    a time complexity function (that is, a Set) `g(n)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`f(n)` is `O(g(n))` if, and only if, there exists constants c and n[0], where
    `f(n) <= cg(n)` and where the input size `n >= n[0]`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s try to apply this to our previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For this example, we denoted it with the set `O(n)`, that is, `g(n) = n`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our time complexity assertion to hold, we will need to satisfy the following
    condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This equation is satisfied for values `c = 5` and `n[0] = 1`. Also, since the
    definition is met, we can safely say that the `f(n)` function is `big-O(g(n))`,
    that is, `O(g(n))` or, in this case, `O(n)`. We can also see this when plotted
    on a graph, as shown in the following diagram; after the value of `n = 1`, we
    can see that the value of `c * g(n)` is always greater than `f(n)` asymptotically.
    Take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bf044bdb-d2b6-4447-b244-932702ba8e11.png)'
  prefs: []
  type: TYPE_IMG
- en: Omega notation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to the Big O notation discussed earlier, Omega notation denotes the
    lower bound rate of growth of the runtime of an algorithm. So, if we have a `f(n)` method, which
    we want to represent with a time complexity function (that is, a Set) `g(n)`,
    then Omega notation can be defined as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`f(n)` is `O(g(n))` if and only if there exists constants c and n[0] where
    `f(n) >= cg(n)` where the input size `n >= n[0]`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking the same preceding example, we have `f(n) = 4n + 1` and then `g(n) =
    n`. We will need to validate the existence of c and n[0] such that the preceding
    condition holds, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that this condition holds for `c = 4` and `n[0] = 0`. Thus, we can
    say that our function `f(n)` is `Ω(n)`. We can plot this as well on a chart and
    take a look at how it represents our function f(n) and its upper and lower bounds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/70c13bfd-ef7e-4149-bf2f-c75c67e23ac2.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding diagram, we can see that our function—`f(n)`—(in black) lies
    in between our asymptotic upper and lower bounds (in gray). The *x*-axis indicates
    the value of the size (*n*).
  prefs: []
  type: TYPE_NORMAL
- en: Theta Notation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having calculated the upper and lower bound rates of growth of our function
    `f(n)`, we can now determine the tight bound or theta of our function `f(n)` as
    well. So, if we have a `f(n)` method, which we want to represent with a time complexity
    function (also known as a set) `g(n)`, then the tight bound of a function can
    be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: f(n) is O(g(n)) if , and only if , there exists constants c and n[0,] where
    c[1]g(n) < = f(n) <= c[2]g(n) where the input size n >= n[0]
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding operation, from the previous two sections, is already calculated
    for our function, that is, `f(n) = 4n + 1`: the value of `c[1] = 4` , `c[2] =
    5`, and `n[0] = 1`.'
  prefs: []
  type: TYPE_NORMAL
- en: This provides us with the tight bound of our function `f(n)` and, since the
    function always lies within the tight bound after `n = 1`, we can safely say that
    our function f(n) has a tightly bound rate of growth as `θ(n)`.
  prefs: []
  type: TYPE_NORMAL
- en: Recap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s quickly take a look at what we discussed the different types of notations
    before moving on to the next topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '`O` means that the growth rate of `f(n)` is asymptotically less than or equal
    to the growth rate of *g(n)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ω` means that the growth rate of `f(n)` is asymptotically greater than or
    equal to the growth rate of *g(n)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`θ` means that the growth rate of `f(n)` is asymptotically equal to the growth
    rate of `g(n)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of time complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now examine some examples of time complexity calculations, since in 99%
    of the cases we need to know the maximum time a function might take to execute;
    we will be mostly analyzing the worst case time complexity, that is, the upper
    bound of the rate of growth based on the input of a function.
  prefs: []
  type: TYPE_NORMAL
- en: Constant time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A constant time function is one which takes the same amount of time to execute,
    irrespective of the size of the input that is passed into the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet is an example of a constant time function and is
    denoted by O(1). Constant time algorithms are the most sought out algorithms for
    obvious reasons, such as them running in a constant time, irrespective of the
    size of the input.
  prefs: []
  type: TYPE_NORMAL
- en: Logarithmic time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Logarithmic time function is one in which the time of execution is proportional
    to the logarithm of the input size. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that in any given iteration, the value of *i = 2^i*, so in the *n^(th)*
    iteration, the value of *i = 2^n*. Also, we know that the value of *i* is always
    less than the size of the loop itself (*N*). From that, we can deduce the following
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we can see that the number of iterations would always
    be less than the log on the input size. Hence, the worst-case time complexity
    of such an algorithm would be `O(log(n))`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider another example, where we halve the value of `i` for the next
    iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the value of `i` in the `n^(th)` iteration will be `N/2^n`, and we know
    that the loop ends with the value of `1`. So, for our loop to stop, the value
    of `i` needs to be `<= 1`; now, by combining those two conditions, we get the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can again come to a similar conclusion as our first example, that the number
    of iterations will always be less than the log value of the input size or value.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to notice is that this is not limited to the doubling or halving phenomenon
    only. This can be applied to any algorithm in which the number of the steps are
    being cut down by a factor, `k`. The worst case time complexity of such algorithms
    would be `O(log[k](N))`, and, in our preceding examples, `k` happens to be `2`.
  prefs: []
  type: TYPE_NORMAL
- en: Logarithmic Time complexity algorithms are the next favorites since they consume
    time logarithmically. Even if the size of the input doubles, the running time
    of the algorithm only increases by a small number additively (which is the definition
    of a logarithm).
  prefs: []
  type: TYPE_NORMAL
- en: Linear time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now discuss one of the most common time complexities, the linear time.
    As one can guess, linear time complexity of a method indicates that the method
    takes linear time to execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This is a very basic `for` loop, within which we are performing some constant
    time operations. As the size of N increases, the number of the times the loop
    gets executed also increases.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the value of `i` in each iteration is incremented by a constant,
    `c`, and not by `1`. This is because it does not matter what the increments are,
    as long as they are linear.
  prefs: []
  type: TYPE_NORMAL
- en: In the first iteration, the value of `i = 0`; in the second iteration, the value
    of `i = c`, then its `c + c = 2c` in the third iteration, and `3c` in the fourth
    iteration, and so on. So, in the nth iteration, we have the value of `i = c(n-1)`,
    which asymptotically is `O(n).`
  prefs: []
  type: TYPE_NORMAL
- en: Depending on what your use case is, linear time complexity may, or may not,
    be good. This is kind of the gray area, which you may sometimes want to let go
    if you are unsure of whether further optimization is necessary or not.
  prefs: []
  type: TYPE_NORMAL
- en: Quadratic time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With quadratic time algorithms, we have now entered the dark side of the time
    complexity. As the name suggests, the size of the input quadratically affects
    the running time of the algorithm. One common example is nested loops:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding example, for `i = 0`, the inner loop runs
    *n* times, and the same for `i = 1`, and `i = 2`, and so on. The inner loop always
    runs n times and is not dependent on the value of n, thus making the algorithms
    time complexity `O(n²)`.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Polynomial time complexity is the running time complexity of algorithms, which
    runs to the order of `n^k`. Quadratic time algorithms are certain types of polynomial
    time algorithms where `k = 2`. A very simple example of such an algorithm would
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this example is just an extension of the example in the quadratic
    time section. The worst case complexity of this case is `O(n³)`.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial time complexity classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have started this conversation, most of the time complexity types
    that we have discussed here so far are of the `O(n^k)` type, for example, it is
    a constant time complexity for `n = 1`, whereas it is quadratic complexity for `k
    = 2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of polynomial time complexity leads us into a class of problems,
    which are defined based on the complexity of their solutions. The following are
    the types of classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**P**: Any problem that can be solved in polynomial time `O(n^k)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NP**: Any problem that can be verified in polynomial time. There can exist
    problems (such as sudoku solving) that can be solved in non-deterministic polynomial
    time. If the solution to these problems can be verified in polynomial time, then
    the problem is classified as an NP-class problem. NP-class problems are a superset
    of the P-class problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NP-Complete**: Any NP problem that can be reduced as a function of another
    NP problem in polynomial time can be classified as an NP-Complete problem. This
    means that if we know the solution to a certain **NP** problem, then a solution
    to another NP problem can be derived in polynomial time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NP-Hard**: A problem can be classified as an NP-Hard problem (H) if there
    exists an **NP-Complete** problem (C) that can be reduced to H in polynomial time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a majority of the real-world scenarios, we will encounter a lot of P and
    NP problems, a classic example of NP-class problem is Traveling Salesman, where
    a salesman wants to visit `n` number of cities to start and end his trip from
    his house. With a limited amount of gasoline and an upper limit on the total miles
    that can be driven, can the salesman visit all the cities without running out
    of gas?
  prefs: []
  type: TYPE_NORMAL
- en: Recursion and additive complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Until now, we have seen some examples that are pretty straightforward: they
    all have a single loop or nested loops. However, a lot of times, there will be
    scenarios in which we will have to handle multiple loops/function calls/branches
    originating from the same algorithm. Let us see an example of how we can calculate
    the complexity in that case?'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have subsequent loops/function calls, we will need to calculate the
    individual complexity of each step and then add them to get the overall complexity,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The collective complexity of this code would be the summation of the complexity
    of both the sections. So, in this case, the overall complexity would be `O(n +
    log n)`, which asymptotically will be `O(n)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have branches in our function with varying time complexity, depending
    on what type of runtime complexity we are talking about, we will need to pick
    the correct choice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the worst case complexity will be decided by whatever is worst
    of the two branches, which would be `O(n)`, but the best case complexity would
    be `O(log(n))`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recursive algorithms are a little tricky compared to their non-recursive counterparts,
    since not only do we need to determine what the complexity of our algorithm is,
    we also need to keep in mind how many times recursion would get triggered because
    that would contribute toward the overall complexity of the algorithm as shown
    in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Although our method only performs some `O(1)` operations, it constantly changes
    the input and calls itself until the size of the input array is zero. So, our
    method ends up executing n times, making the overall time complexity of `O(n)`.
  prefs: []
  type: TYPE_NORMAL
- en: Space complexity and Auxiliary space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Space complexity and Auxiliary space are two of the most often confused and
    interchangeably used terms when talking about the space complexity of a certain
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Auxiliary Space: **The extra space that is taken by an algorithm temporarily
    to finish its work'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Space Complexity: **Space complexity is the total space taken by the algorithm
    with respect to the input size plus the auxiliary space that the algorithm uses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we try to compare two algorithms, we usually have a similar type of input,
    that is, the size of the input can be disregarded and thus what we do end up comparing
    is the auxiliary space of the algorithms. It's not a big deal to use either of
    the terms, as long as we understand the distinction between the two and use them
    correctly.
  prefs: []
  type: TYPE_NORMAL
- en: If we were using a low-level language such as C, then we can break down the
    memory required/consumed based on the data type, for example, 2 bytes to store
    an integer, 4 bytes to store floating point, and so on. However, since we are
    working with JavaScript, which is a high-level language, this is not as so straightforward,
    as we do not make a distinction between different data types explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of Space complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to space complexity of an algorithm, we have types similar to
    that of the time complexity, such as constant space `S(1)` and linear space `S(N)`.
    Let's take a look at some of the examples in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Constant space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A constant space algorithm is one in which the space consumed by an algorithm
    does not change by the size of the input or the algorithms input parameters in
    any way.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, I would like to reiterate that when we talk about the space
    complexity of an algorithm we are talking about the auxiliary space that is consumed
    by the algorithm. This implies that even if our array is of size *n*, the auxiliary
    (or the extra) space that is consumed by our algorithm will remain constant, as
    shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We can see here that the `firstElement` method does not take any more space,
    irrespective of what the input is. Hence, we can denote this as space complexity
    `S(1)`.
  prefs: []
  type: TYPE_NORMAL
- en: Linear space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A linear space algorithm is one in which the amount of space taken by an algorithm
    is directly proportional to the size of the input, for example, algorithms that
    loop over an array and push values to a new array before returning them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, although redundant, we are creating a new array and pushing
    all the values into that array, which will use up the same space as that of the
    input array. Consider the situation in which you have a condition before the `push`,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In the worst case, the `someCondition` flag is always true, and we end up with
    the result that is again of the same size as that of the input. Thus, we can assert
    that the space complexity of the preceding method is `S(n)`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we scratched the surface of a beast known as computational
    complexity. There is a lot more to computational complexity than we discussed
    in this chapter. However, the topics and examples discussed in this chapter are
    the ones that most of us face in our day-to-day work. There are more advanced
    topics in space complexity, such as LSPACE, which is a class of problems that
    can be solved in logarithmic space and NLSPACE, which is the amount of space but
    using a non-deterministic Turing machine. The main goal of this chapter is to
    ensure that we understand how the complexity of our algorithms is calculated and
    how it affects the overall output. In the next chapter, we will be discussing
    what kind of micro-optimizations we can make to our applications, and understand
    the internals of how browsers (mostly Chrome) work and how we can leverage them
    to better our applications.
  prefs: []
  type: TYPE_NORMAL
