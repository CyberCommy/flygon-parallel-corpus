- en: Storage Engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MongoDB introduced the concept of pluggable storage engines in version 3.0\.
    After the acquisition of WiredTiger, it introduced its storage engine as optional
    at first, and then as the default storage engine for the current version of MongoDB.
    In this chapter, we will dive deeply into the concept of storage engines, why
    they matter, and how we can choose the best one according to our workload.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Pluggable storage engines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WiredTiger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encrypted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MMAPv1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locking in MongoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pluggable storage engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With MongoDB breaking out from the web application paradigm into domains with
    different requirements, storage has become an increasingly important consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple storage engines can be seen as an alternative way to using different
    storage solutions and databases in our infrastructure stack. This way, we can
    reduce operational complexity and development time to market with the application
    layer being agnostic of the underlying storage layer.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB currently offers four different storage engines that we will examine
    in further detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: WiredTiger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As of version 3.2, WiredTiger is the default storage engine, and also the best
    choice for most workloads. By providing document-level locking, it overcomes one
    of the most significant drawbacks earlier versions of MongoDB had—lock contention
    under high load.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore some of WiredTiger's benefits in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Document-level locking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Locking is so important that we will explain the performance implications that
    fine-grained locking has in further detail at the end of this section. Having
    document-level locking as opposed to MMAPv1 collection-level locking can make
    a huge difference in many real-world use cases, and is one of the main reasons
    to choose WiredTiger over MMAPv1.
  prefs: []
  type: TYPE_NORMAL
- en: Snapshots and checkpoints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: WiredTiger uses **Multi-Version Concurrency Control** (**MVCC**). MVCC is based
    upon the concept that the database keeps multiple versions of an object so that
    readers will be able to view consistent data that doesn't change during a read.
  prefs: []
  type: TYPE_NORMAL
- en: In a database, if we have multiple readers accessing data at the same time that
    writers are modifying the data, we can end up with a case where readers view an
    inconsistent view of this data. The simplest and easiest way to solve this problem
    is to block all readers until the writers are done modifying data.
  prefs: []
  type: TYPE_NORMAL
- en: This will, of course, cause severe performance degradation. MVCC solves this
    problem by providing a snapshot of the database for each reader. When the read
    starts, each reader is guaranteed to view data as exactly it was at the point
    in time that the read started. Any changes made by writers will only be seen by
    readers after the write has been completed, or, in database terms, after the transaction
    is committed.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this goal, when a write is coming in, updated data will be kept in
    a separate location on disk and MongoDB will mark the affected document as obsolete.
    MVCC is said to provide point-in-time consistent views. This is equivalent to
    a read committed isolation level in traditional RDBMS systems.
  prefs: []
  type: TYPE_NORMAL
- en: For every operation, WiredTiger will snapshot our data at the exact moment that
    it happens and provide a consistent view of application data to the application.
    When we write data, WiredTiger will create a snapshot every 2 GB of journal data
    or 60 seconds, whichever comes first. WiredTiger relies on its built-in journal
    to recover any data after the latest checkpoint in case of failure.
  prefs: []
  type: TYPE_NORMAL
- en: We can disable journaling using WiredTiger, but if the server crashes, we will
    lose any data after the last checkpoint is written.
  prefs: []
  type: TYPE_NORMAL
- en: Journaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained in the *Snapshots and checkpoints* section, journaling is the cornerstone
    of WiredTiger crash recovery protection.
  prefs: []
  type: TYPE_NORMAL
- en: 'WiredTiger compresses the journal using the snappy compression algorithm. We
    can use the following setting to set a different compression algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also disable journaling for WiredTiger by setting the following to `false`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If we use a replica set, we may be able to recover our data from a secondary
    that will get elected as a primary and start taking writes in the event that our
    primary fails. It is recommended to always use journaling, unless we understand
    and can take the risk of suffering through the consequences of not using it.
  prefs: []
  type: TYPE_NORMAL
- en: Data compression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MongoDB uses the snappy compression algorithm by default to compress data and
    prefixes for indexes. Index-prefixed compression means that identical index key
    prefixes are stored only once per page of memory. Compression not only reduces
    our storage footprint, but will increase I/O operations per second, as less data
    needs to be stored and moved to and from disk. Using more aggressive compression
    can lead to performance gains if our workload is I/O bound and not CPU bound.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define `.zlib` compression instead of snappy or no compression by setting
    the following parameter to `false`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Data compression uses less storage at the expense of CPU. `.zlib` compression
    achieves better compression at the expense of higher CPU usage, as opposed to
    the default snappy compression algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can disable index prefixes compression by setting the following parameter
    to `false`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also configure storage per-index during creation using the following parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Memory usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: WiredTiger is significantly different to MMAPv1 in how it uses RAM. MMAPv1 is
    essentially using the underlying operating system's filesystem cache to page data
    from disk to memory and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: WiredTiger, on the contrary, introduces the new concept of the WiredTiger internal
    cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'The WiredTiger internal cache is, by default, the larger of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 50% of RAM minus 1 GB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 256 MB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This means if our server has 8 GB RAM we will get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*max(3 GB , 256 MB) = WiredTiger will use 3 GB of RAM*'
  prefs: []
  type: TYPE_NORMAL
- en: 'And if our server has 2,512 MB RAM we will get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*max(256 MB, 256 MB) = WiredTiger will use 256 MB of RAM*'
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, for any server that has less than 2,512 MB RAM, WiredTiger will
    use 256 MB for its internal cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can change the size of the WiredTiger internal cache by setting the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also do this from the command line using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Apart from the WiredTiger internal cache that is uncompressed for higher performance,
    MongoDB also uses the filesystem cache that is compressed, just like MMAPv1, and
    will end up using all available memory in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: The WiredTiger internal cache can provide similar performance to in-memory storage.
    As such, it is important to grow it as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: We can achieve better performance when using WiredTiger with multi-core processors.
    This is also a big win compared to MMAPv1, which does not scale as well.
  prefs: []
  type: TYPE_NORMAL
- en: We can, and should, use Docker or other containerization technologies to isolate
    the `mongod` processes from each other and make sure that we know how much memory
    each process can, and should, use in a production environment. It is not recommended
    to increase the WiredTiger internal cache above its default value. The filesystem
    cache should not be less than 20% of the total RAM.
  prefs: []
  type: TYPE_NORMAL
- en: readConcern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: WiredTiger supports multiple `readConcern` levels. Just like `writeConcern`,
    which is supported by every storage engine in MongoDB, with `readConcern`, we
    can customize how many servers in a replica set must acknowledge the query results
    for the document to be returned in the result set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Available options for read concern are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`local`: Default option. Will return most recent data from the server. Data
    may, or may not, have propagated to the other servers in a replica set, and we
    run the risk of a rollback.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`linearizable`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only applicable for reads from the primary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only applicable in queries that return a single result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data returns satisfy two conditions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`majority`, ``writeConcern``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data was acknowledged before the start of the read operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, if we have set `writeConcernMajorityJournalDefault` to `true`,
    we are guaranteed that the data won't get rolled back.
  prefs: []
  type: TYPE_NORMAL
- en: If we have set `writeConcernMajorityJournalDefault` to `false`, MongoDB will
    not wait for `majority` writes to be durable before acknowledging the write. In
    this case our data may be rolled back in the event of a loss of a member from
    the replica set. Data returned has already been propagated and acknowledged from
    `majority` of the servers before read has started.
  prefs: []
  type: TYPE_NORMAL
- en: We need to use `maxTimeMS` when using `linearizable` and `majority` read concern
    levels in case we can't establish `majority writeConcern` to avoid blocking, forever
    waiting for the response. In this case, the operation will return a timeout error.
  prefs: []
  type: TYPE_NORMAL
- en: MMAPv1 is the older storage engine and is considered, in many aspects, as being
    deprecated, but many deployments are still using it.
  prefs: []
  type: TYPE_NORMAL
- en: '`local` and `linearizable` read concerns are available for MMAPv1 as well.'
  prefs: []
  type: TYPE_NORMAL
- en: WiredTiger collection-level options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we create a new collection, we can pass in options to WiredTiger like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This helps to create our `mongo_books` collection with a key-value pair from
    the available ones that WiredTiger exposes through its API. Some of the most widely
    used key-value pairs are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Key** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| `block_allocation` | Best or first |'
  prefs: []
  type: TYPE_TB
- en: '| `allocation_size` | 512 bytes through to 4 KB; default 4 KB |'
  prefs: []
  type: TYPE_TB
- en: '| `block_compressor` | None, `.lz4`, `.snappy`, `.zlib`, `.zstd`, or custom
    compressor identifier string depending on configuration |'
  prefs: []
  type: TYPE_TB
- en: '| `memory_page_max` | 512 bytes through to 10 TB; default 5 MB |'
  prefs: []
  type: TYPE_TB
- en: '| `os_cache_max` | Integer greater than zero; default zero |'
  prefs: []
  type: TYPE_TB
- en: 'This is taken directly from the definition in the WiredTiger documents located
    at [http://source.wiredtiger.com/mongodb-3.4/struct_w_t___s_e_s_s_i_o_n.html](http://source.wiredtiger.com/mongodb-3.4/struct_w_t___s_e_s_s_i_o_n.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Collection-level options allow for flexibility in configuring storage but should
    be used with extreme care and after careful testing in development/staging environments.
  prefs: []
  type: TYPE_NORMAL
- en: Collection-level options will get propagated to secondaries if applied to a
    primary in a replica set. `block_compressor` can also be configured from the command
    line globally for the database using the `--wiredTigerCollectionBlockCompressor`
    option.
  prefs: []
  type: TYPE_NORMAL
- en: WiredTiger performance strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed earlier in this chapter, WiredTiger uses an internal cache to optimize
    performance. On top of it, there is always the filesystem cache that the operating
    system (and MMAPv1) uses to fetch data from disk.
  prefs: []
  type: TYPE_NORMAL
- en: By default, we have 50% of RAM dedicated to the filesystem cache and 50% to
    the WiredTiger internal cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'The filesystem cache will keep data compressed as it is stored on disk. The
    internal cache will decompress it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Strategy 1**: Allocate 80% or more to the internal cache. This has the goal
    of fitting our working set in WiredTiger''s internal cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strategy 2**: Allocate 80% or more to the filesystem cache. Our goal here
    is to avoid using the internal cache as much as possible, and rely on the filesystem
    cache for our needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strategy 3**: Use an SSD as the underlying storage for fast seek time and
    keep defaults at 50-50% allocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strategy 4**: Enable compression in our storage layer through MongoDB''s
    configuration to save on storage, and potentially improve our performance by having
    a smaller working set size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our workload will dictate whether we need to deviate from the default Strategy
    1 to any of the rest. In general, we should use SSDs wherever possible, and with
    MongoDB's configurable storage, we can even use SSDs for some of the nodes where
    we need the best performance and keep HDDs for analytics workloads.
  prefs: []
  type: TYPE_NORMAL
- en: WiredTiger B-tree versus LSM indexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: B-tree is the most common data structure for indexes across different database
    systems. WiredTiger offers the option to use a **Log Structured Merge** (**LSM**)
    tree instead of a B-tree for indexing.
  prefs: []
  type: TYPE_NORMAL
- en: An LSM tree can provide better performance when we have a workload of random
    inserts that would otherwise overflow our page cache and start paging in data
    from disk to keep our index up to date.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSM indexes can be selected from the command line like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command chooses `lsm` as `type`, and `block_compressor` is `zlib`
    for indexes in this `mongod` instance.
  prefs: []
  type: TYPE_NORMAL
- en: Encrypted
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The encrypted storage engine was added to support a series of special use cases,
    mostly revolving around finance, retail, healthcare, education, and government.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to have encryption for the rest of our data if we have to comply to
    a set of regulations, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: PCI DSS for handling credit card information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HIPAA for healthcare applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NIST for government
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FISMA for government
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: STIG for government
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be done in several ways, and cloud service providers, such as EC2,
    provide EBS storage volumes with built-in encryption. Encrypted storage supports
    Intel's AES-NI equipped CPUs for acceleration of the encryption/decryption process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The encryption algorithms supported are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: AES-256, CBC (default)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AES-256, GCM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FIPS, FIPS-140-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encryption is supported at page level for better performance. When a change
    is made in a document, instead of re-encrypting/decrypting the entire underlying
    file, only the page that is affected gets modified.
  prefs: []
  type: TYPE_NORMAL
- en: Encryption key management is a huge aspect of encrypted storage security. Most
    specifications previously mentioned require key rotation at least once per year.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB's encrypted storage uses an internal database key per node. This key
    is wrapped by an external (master) key that must be used to start the node's `mongod`
    process. By using the underlying operating system's protection mechanisms such
    as `mlock` or `VirtualLock`, MongoDB can guarantee that the external key will
    never be leaked from memory to disk by page faults.
  prefs: []
  type: TYPE_NORMAL
- en: The external (master) key can be managed either by using the **Key Management
    Interoperability Protocol** (**KMIP**) or by using local key management via a
    keyfile.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB can achieve key rotation by performing rolling restarts of the replica
    set members. Using KMIP, MongoDB can rotate only the external key and not the
    underlying database files. This delivers significant performance benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Using KMIP is the recommended approach for encrypted data storage. Encrypted
    storage is based on WiredTiger, so all its advantages can be enjoyed using encryption
    as well. Encrypted storage is a part of MongoDB Enterprise Edition, the paid offering
    by MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: Using MongoDB's encrypted storage gives the advantage of increased performance
    versus encrypted storage volumes. MongoDB's encrypted storage has an overhead
    of around 15% as compared to 25% or more for third-party encrypted storage solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, if we need to use encrypted storage, we will know it well in
    advance from the application design phase, and we can perform benchmarks against
    different solutions to choose the one that best fits our use case.
  prefs: []
  type: TYPE_NORMAL
- en: In-memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MongoDB storage in-memory is a risky task with high rewards. Keeping data in-memory
    can be up to 100,000 times faster than durable storage on disk.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of using in-memory storage is that we can achieve predictable
    latency when we write or read data. Some use cases dictate for latency that does
    not deviate from normal, no matter what the operation is.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, by keeping data in-memory we are open to power loss and application
    failure where we can lose all of our data. Using a replica set can safeguard against
    some classes of errors, but we will always be more exposed to data loss if we
    store in data as opposed to storing on disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are some use cases in which we may not care that much about
    losing older data. For example, in the financial world we may have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: High-frequency trading/algorithmic trading, where higher latency in the case
    of high traffic can lead to transactions not being fulfilled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In fraud detection systems, we are concerned about real-time detection being
    as fast as possible and we can safely store only the cases that require further
    investigation, or the definitely positive ones, to durable storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Credit card authorizations, trade ordering reconciliation, and other high-traffic
    systems that demand a real-time answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the web applications ecosystem we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In intrusion detection systems, such as fraud detection, we are concerned with
    detecting intrusion as fast as possible without so much concern for false positive
    cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of the product search cache, losing data is not mission-critical,
    but rather a small inconvenience from the customer's perspective.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For real-time personalized product recommendations, there is a low-risk operation
    in terms of data loss. We can always rebuild the index even if we suffer data
    loss.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A major disadvantage of an in-memory storage engine is that our dataset has
    to fit in-memory. This means we must know and keep track of our data usage so
    we don't exceed the memory of our server.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, using the MongoDB in-memory storage engine may be useful in some edge
    use cases, but lacking durability in a database system can be a blocking factor
    in its adoption.
  prefs: []
  type: TYPE_NORMAL
- en: In-memory storage is part of MongoDB Enterprise Edition, the paid offering by
    MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: MMAPv1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the introduction of WiredTiger and its many benefits, such as document-level
    locking, many MongoDB users are questioning whether it's worth discussing MMAPv1
    anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'In reality, the only cases where we should consider using MMAPv1 instead of
    WiredTiger would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Legacy systems**: If we have a system that fits our needs, we may upgrade
    to MongoDB 3.0+ and not transition to WiredTiger.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Version downgrade**: Once we upgrade to MongoDB 3.0+ and convert our storage
    to WiredTiger, we cannot downgrade to a version lower than 2.6.8\. This should
    be kept in mind if we want the flexibility to downgrade at a later time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As shown previously, WiredTiger is a better choice than MMAPv1, and we should
    use it whenever we get the chance. This book is oriented around WiredTiger, and
    assumes we will be able to use the latest stable version of MongoDB (3.4 at the
    time of writing).
  prefs: []
  type: TYPE_NORMAL
- en: MMAPv1, as of version 3.4, only supports collection-level locking, as opposed
    to the document-level locking supported by WiredTiger. This can lead to a performance
    penalty in high contention database loads and is one of the main reasons why we
    should use WiredTiger whenever possible.
  prefs: []
  type: TYPE_NORMAL
- en: MMAPv1 storage optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MongoDB, by default, uses the power-of-two allocation strategy. When a document
    is created it will get allocated a power of size two. That is, `ceiling(document_size)`.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we create a document of 127 bytes, MongoDB will allocate 128
    bytes (*2^7*), while if we create a document of 129 bytes, MongoDB will allocate
    256 bytes (*2^8*). This is helpful when updating documents, as we can update them
    and not move the underlying document on disk until it exceeds the space allocated.
  prefs: []
  type: TYPE_NORMAL
- en: If a document is moved on disk (that is, adding a new subdocument or an element
    in an array of the document that forces the size to exceed the allocated storage),
    a new power-of-two allocation size will be used.
  prefs: []
  type: TYPE_NORMAL
- en: If the operation doesn't affect its size (that is, changing an integer value
    from one to two), the document will remain stored in the same physical location
    on disk. This concept is called **padding**. We can configure padding using the
    compact administration command as well.
  prefs: []
  type: TYPE_NORMAL
- en: When we move documents on disk, we have non-contiguous blocks of data stored,
    essentially holes in storage. We can prevent this from happening by setting `paddingFactor`
    at collection level.
  prefs: []
  type: TYPE_NORMAL
- en: '`paddingFactor` has a default value of `1.0` (no padding) and a maximum of
    `4.0` (expanding size by three times as much as the original document size). For
    example, `paddingFactor` of `1.4` will allow the document to expand by 40% before
    getting moved on to a new location on disk.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, with our favorite `books` collection, to get 40% more space, we
    would do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also set padding in terms of bytes per document. This way we get *x*
    bytes padding from the initial creation of each document in our collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This will allow a document created at 200 bytes to grow to 500 bytes, while
    a document created at 4,000 bytes will be allowed to grow to 4,300 bytes.
  prefs: []
  type: TYPE_NORMAL
- en: We can eliminate holes altogether by running a `compact` command with no parameters,
    but this means that every update that increases document size will have to move
    documents, essentially creating new holes in storage.
  prefs: []
  type: TYPE_NORMAL
- en: Mixed usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we have an application with MongoDB as the underlying database, we can
    set it up to use different replica sets for different operations at application
    level, matching their requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in our financial application, we can use one connection pool for
    the fraud detection module utilizing in-memory nodes, and a different one for
    the rest of our system, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2eb80ef3-4570-4763-a401-27e581114ac0.png)'
  prefs: []
  type: TYPE_IMG
- en: In addition, storage engine configuration in MongoDB is applied per node, which
    allows for some interesting setups.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding architectural diagram, we can use a mix of different
    storage engines in different members of a replica set. In this case, we are using
    the in-memory engine for optimal performance in the primary node, while one of
    the secondaries uses WiredTiger to ensure data durability. We can use `priority=1`
    in the in-memory secondary node to make sure that, if the primary fails, the secondary
    will get elected right away. If we don't do it, we risk having a failing primary
    server exactly when we have high load in the system, and the secondary has not
    kept up with the primary's writes in-memory.
  prefs: []
  type: TYPE_NORMAL
- en: The mixed storage approach is widely used in the microservice architecture.
    By decoupling service and database and using the appropriate database for each
    use case, we can easily horizontally scale our infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'All storage engines support a common baseline functionality, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Querying
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indexing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ops and Cloud Manager support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authentication and authorization semantics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other storage engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modular MongoDB architecture allows for third parties to develop their own storage
    engines.
  prefs: []
  type: TYPE_NORMAL
- en: RocksDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RocksDB is an embedded database for key-value data. It's a fork of `LevelDB`
    storing key-value pairs in arbitrary byte arrays. It was started at Facebook in
    2012, and now serves as the backend for the interestingly named **CockroachDB**,
    the open source DB inspired by Google Spanner.
  prefs: []
  type: TYPE_NORMAL
- en: MongoRocks is a project backed by Percona and Facebook aiming to bring RocksDB
    backend to MongoDB. RocksDB can achieve higher performance than WiredTiger for
    some workloads, and is worth investigating.
  prefs: []
  type: TYPE_NORMAL
- en: TokuMX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another widely used storage engine is TokuMX by Percona. TokuMX was designed
    with both MySQL and MongoDB in mind, but since 2016, Percona has focused its efforts
    on the MySQL version, instead switching over to **RocksDB** for MongoDB storage
    support.
  prefs: []
  type: TYPE_NORMAL
- en: Locking in MongoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Document-level and collection-level locking is mentioned throughout this chapter
    and also in several other chapters in this book. It is important to understand
    how locking works and why it is important.
  prefs: []
  type: TYPE_NORMAL
- en: Database systems use the concept of locks to achieve ACID properties. When there
    are multiple read or write requests coming in parallel, we need to lock our data
    so that all readers and writers have consistent and predictable results.
  prefs: []
  type: TYPE_NORMAL
- en: 'MongoDB uses multi-granularity locking. The available granularity levels in
    descending order are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Global
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The locks that MongoDB and other databases use are the following, in order
    of granularity:'
  prefs: []
  type: TYPE_NORMAL
- en: '*IS*: Intent shared'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*IX*: Intent exclusive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*S*: Shared'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X*: Exclusive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we use locking at a granularity level with *S* or *X* locks, then all higher
    levels need to be locked with an intent lock of the same type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other rules for locks are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A single database can simultaneously be locked in *IS* and *IX* mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An exclusive (*X*) lock cannot coexist with any other lock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A shared (*S*) lock can only coexist with *IS* locks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reads and writes requesting locks are generally queued in **first-in, first-out**
    (**FIFO**) order. The only optimization that MongoDB will actually do is reordering
    requests according to the next request in queue to be serviced.
  prefs: []
  type: TYPE_NORMAL
- en: 'What this means is that if we have an *IS(1)* request coming up next and our
    current queue has the following *IS(1)->IS(2)->X(3)->S(4)->IS(5)* as shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eaed832c-b511-4484-a5cc-34840526172f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then MongoDB will reorder requests like this, *IS(1)->IS(2)->S(4)->IS(5)->X(3)
    a*s shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac8c0989-a235-4176-ab63-e60ea5fa7742.png)'
  prefs: []
  type: TYPE_IMG
- en: If, during servicing, the *IS(1)* request, new *IS*, or *S* requests come in,
    let's say *IS(6)* and *S(7),* in that order, they will still be added at the end
    of the queue and won't be considered until the *X(3)* request is done with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our new queue will now look like *IS(2)->S(4)->IS(5)->X(3)->IS(6)->S(7)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00940bca-eff2-4efe-9fb7-00c1e0ea3676.png)'
  prefs: []
  type: TYPE_IMG
- en: This is done to prevent the starvation of the *X(3)* request that would end
    up getting pushed back in the queue all the time because new *IS* and *S* requests
    come in. It is important to understand the difference between intent locks and
    locks themselves. WiredTiger storage engine will only use intent locks for global,
    database, and collection levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'It uses intent locks at higher levels (that is, collection, database, global),
    when a new request comes in and according to the compatibility matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bf12ed6-cd4a-4da4-80fd-b0b1df10bca9.png)'
  prefs: []
  type: TYPE_IMG
- en: MongoDB will first acquire intention locks in all ancestors before acquiring
    the lock on the document itself. This way, when a new request comes in, it can
    quickly identify if it cannot be serviced based on less granular locks.
  prefs: []
  type: TYPE_NORMAL
- en: WiredTiger will use *S* and *X* locks at the document level. The only exception
    to that is for typically infrequent and/or short-lived operations involving multiple
    databases. These will still require a global lock, similar to the behavior MongoDB
    had in pre 2.x versions.
  prefs: []
  type: TYPE_NORMAL
- en: Administrative operations, such as dropping a collection, still require an exclusive
    database lock.
  prefs: []
  type: TYPE_NORMAL
- en: MMAPv1, as explained previously, uses collection-level locks. Operations that
    span a single collection, but may or may not be spanning a single document, will
    still lock up the entire collection. This is the main reason why WiredTiger is
    the preferred storage solution for all new deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Lock reporting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can inspect lock status using any of the following tools and commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`db.serverStatus()` through the `locks` document'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`db.currentOp()` through the `locks` field'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mongotop`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mongostat`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MongoDB Cloud Manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MongoDB Ops Manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lock contention is a really important metric to keep track of, as it can bring
    our database to its knees if it grows out of control.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to terminate an operation, we have to use the `db.killOp()` shell
    command.
  prefs: []
  type: TYPE_NORMAL
- en: Lock yield
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A database with a database-level lock will not be really useful under stress,
    and will end being locked up most of the time. A smart solution to that in the
    early versions of MongoDB was getting operations to yield their locks on the basis
    of some heuristics.
  prefs: []
  type: TYPE_NORMAL
- en: '`update()` commands affecting multiple documents would yield their *X* lock
    to improve concurrency.'
  prefs: []
  type: TYPE_NORMAL
- en: MMAPv1's predecessor in earlier versions of MongoDB would use these heuristics
    to predict whether data was already in-memory before performing the requested
    operation. If it wasn't, it would yield the lock until the underlying operating
    system pages data in-memory, and then re-acquire the lock to continue with servicing
    the request.
  prefs: []
  type: TYPE_NORMAL
- en: The most notable exceptions to these are index scans, where the operation will
    not yield its lock and will just block on waiting for the data to get loaded from
    disk.
  prefs: []
  type: TYPE_NORMAL
- en: Since WiredTiger is only using intent locks at collection level and above, it
    doesn't really need these heuristics, as intent locks don't block other readers
    and writers.
  prefs: []
  type: TYPE_NORMAL
- en: Commonly used commands and locks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The commonly used commands and locks are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Command** | **Lock** |'
  prefs: []
  type: TYPE_TB
- en: '| `find()` | *S* |'
  prefs: []
  type: TYPE_TB
- en: '| `it() (query cursor)` | *S* |'
  prefs: []
  type: TYPE_TB
- en: '| `insert()` | *X* |'
  prefs: []
  type: TYPE_TB
- en: '| `remove()` | *X* |'
  prefs: []
  type: TYPE_TB
- en: '| `update()` | *X* |'
  prefs: []
  type: TYPE_TB
- en: '| `mapreduce()` | Both *S* and *X*, depending on the case. Some MapReduce chunks
    can run in parallel. |'
  prefs: []
  type: TYPE_TB
- en: '| `index()` |'
  prefs: []
  type: TYPE_TB
- en: '**Foreground indexing**: Database lock.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Background indexing**: No lock, except for administrative commands that will
    return an error. Also, background indexing will take considerably more time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `aggregate()` | *S* |'
  prefs: []
  type: TYPE_TB
- en: Commands requiring a database lock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following commands require a database lock. We should plan in advance before
    issuing them in a production environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '`db.collection.createIndex()` in the (default) foreground mode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reIndex`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`compact`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`db.repairDatabase()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`db.createCollection()` if creating a multiple GB capped collection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`db.collection.validate()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`db.copyDatabase()`, which may lock more than one database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also have some commands that lock the entire database for a really short
    period of time:'
  prefs: []
  type: TYPE_NORMAL
- en: '`db.collection.dropIndex()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`db.getLastError()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`db.isMaster()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any `rs.status()` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`db.serverStatus()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`db.auth()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`db.addUser()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These commands shouldn't take more than a few milliseconds to operate and so
    we shouldn't worry about it, unless we have automated scripts with these commands
    in place, in which case we must take note to throttle how often they would occur.
  prefs: []
  type: TYPE_NORMAL
- en: In a sharded environment, each `mongod` applies its own locks, thus greatly
    improving concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: In replica sets, our primary server must take all write operations. For these
    to be replicated correctly to the secondaries we must lock the local database
    that holds the oplog of operations at the same time that we lock our primary document/collection/database.
    This is usually a short-lived lock that, again, we shouldn't worry about.
  prefs: []
  type: TYPE_NORMAL
- en: Secondaries in replica sets will fetch write operations from the primary local
    database's oplog, apply the appropriate *X* lock, and apply service reads once
    the *X* locks are done with.
  prefs: []
  type: TYPE_NORMAL
- en: From the long preceding explanation, it's evident that locking should be avoided
    at all costs in MongoDB. We should design our database so that we avoid as many
    *X* locks as possible, and when we need to take *X* locks over one or multiple
    databases, do so in a maintenance window with a backup plan in case operations
    take longer than expected.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following links for further reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.mongodb.com/manual/faq/concurrency/](https://docs.mongodb.com/manual/faq/concurrency/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.mongodb.com/manual/core/storage-engines/](https://docs.mongodb.com/manual/core/storage-engines/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.mongodb.com/blog/post/building-applications-with-mongodbs-pluggable-storage-engines-part-1](https://www.mongodb.com/blog/post/building-applications-with-mongodbs-pluggable-storage-engines-part-1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.mongodb.com/blog/post/building-applications-with-mongodbs-pluggable-storage-engines-part-2](https://www.mongodb.com/blog/post/building-applications-with-mongodbs-pluggable-storage-engines-part-2?jmp=docs&_ga=2.154506616.1736193377.1502822527-355279797.1491859629)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.mongodb.com/manual/core/wiredtiger/](https://docs.mongodb.com/manual/core/wiredtiger/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.mongodb.com/manual/reference/method/db.collection.createIndex/#createindex-options](https://docs.mongodb.com/manual/reference/method/db.collection.createIndex/#createindex-options)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.mongodb.com/manual/core/mmapv1/](https://docs.mongodb.com/manual/core/mmapv1/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.mongodb.com/manual/reference/method/db.createCollection/#create-collection-storage-engine-options](https://docs.mongodb.com/manual/reference/method/db.createCollection/#create-collection-storage-engine-options)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://source.wiredtiger.com/mongodb-3.4/struct_w_t___s_e_s_s_i_o_n.html](http://source.wiredtiger.com/mongodb-3.4/struct_w_t___s_e_s_s_i_o_n.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://webassets.mongodb.com/microservices_white_paper.pdf?_ga=2.158920114.90404900.1503061618-355279797.1491859629](https://webassets.mongodb.com/microservices_white_paper.pdf?_ga=2.158920114.90404900.1503061618-355279797.1491859629)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://webassets.mongodb.com/storage_engines_adress_wide_range_of_use_cases.pdf?_ga=2.125749506.90404900.1503061618-355279797.1491859629](https://webassets.mongodb.com/storage_engines_adress_wide_range_of_use_cases.pdf?_ga=2.125749506.90404900.1503061618-355279797.1491859629)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.mongodb.com/manual/reference/method/db.createCollection/#create-collection-storage-engine-options](https://docs.mongodb.com/manual/reference/method/db.createCollection/#create-collection-storage-engine-options)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://source.wiredtiger.com/mongodb-3.4/struct_w_t___s_e_s_s_i_o_n.html](http://source.wiredtiger.com/mongodb-3.4/struct_w_t___s_e_s_s_i_o_n.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.mongodb.com/manual/reference/read-concern/](https://docs.mongodb.com/manual/reference/read-concern/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.percona.com/live/17/sessions/comparing-mongorocks-wiredtiger-and-mmapv1-performance-and-efficiency](https://www.percona.com/live/17/sessions/comparing-mongorocks-wiredtiger-and-mmapv1-performance-and-efficiency)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.percona.com/blog/2016/06/01/embracing-mongorocks/](https://www.percona.com/blog/2016/06/01/embracing-mongorocks/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.percona.com/software/mongo-database/percona-tokumx](https://www.percona.com/software/mongo-database/percona-tokumx)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.slideshare.net/profyclub_ru/4-understanding-and-tuning-wired-tiger-the-new-high-performance-database-engine-in-mongodb-henrik-ingo-mongodb](https://www.slideshare.net/profyclub_ru/4-understanding-and-tuning-wired-tiger-the-new-high-performance-database-engine-in-mongodb-henrik-ingo-mongodb/27)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about different storage engines in MongoDB. We identified
    the pros and cons of each one and the use cases for choosing each storage engine.
  prefs: []
  type: TYPE_NORMAL
- en: We learned about using multiple storage engines, how we can use them, and the
    benefits. A big part of this chapter was also dedicated to database locking, how
    it can happen, why it is bad, and how we can avoid it.
  prefs: []
  type: TYPE_NORMAL
- en: We split our operations by the lock they need. This way, when we design and
    implement our application, we can make sure that we have a design that locks our
    database as little as possible.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about MongoDB and how we can use it to ingest
    and process big data.
  prefs: []
  type: TYPE_NORMAL
