- en: Collecting and Querying Metrics and Sending Alerts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Insufficient facts always invite danger.
  prefs: []
  type: TYPE_NORMAL
- en: '- *Spock*'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we explored how to leverage some of Kubernetes core features. We used
    HorizontalPodAutoscaler and Cluster Autoscaler. While the former relies on Metrics
    Server, the latter is not based on metrics, but on Scheduler's inability to place
    Pods within the existing cluster capacity. Even though Metrics Server does provide
    some basic metrics, we are in desperate need for more.
  prefs: []
  type: TYPE_NORMAL
- en: We have to be able to monitor our cluster and Metrics Server is just not enough.
    It contains a limited amount of metrics, it keeps them for a very short period,
    and it does not allow us to execute anything but simplest queries. I can't say
    that we are blind if we rely only on Metrics Server, but that we are severely
    impaired. Without increasing the number of metrics we're collecting, as well as
    their retention, we get only a glimpse into what's going on in our Kubernetes
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to fetch and store metrics cannot be the goal by itself. We also
    need to be able to query them in search for a cause of an issue. For that, we
    need metrics to be "rich" with information, and we need a powerful query language.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, being able to find the cause of a problem is not worth much without
    being able to be notified that there is an issue in the first place. That means
    that we need a system that will allow us to define alerts that, when certain thresholds
    are reached, will send us notifications or, when appropriate, send them to other
    parts of the system that can automatically execute steps that will remedy issues.
  prefs: []
  type: TYPE_NORMAL
- en: If we accomplish that, we'll be a step closer to having not only a self-healing
    (Kubernetes already does that) but also a self-adaptive system that will react
    to changed conditions. We might go even further and try to predict that "bad things"
    will happen in the future and be proactive in resolving them before they even
    arise.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, we need a tool, or a set of tools, that will allow us to fetch and
    store "rich" metrics, that will allow us to query them, and that will notify us
    when an issue happens or, even better, when a problem is about to occur.
  prefs: []
  type: TYPE_NORMAL
- en: We might not be able to build a self-adapting system in this chapter, but we
    can try to create a foundation. But, first things first, we need a cluster that
    will allow us to "play" with some new tools and concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll continue using definitions from the `vfarcic/k8s-specs` ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    repository. To be on the safe side, we'll pull the latest version first.
  prefs: []
  type: TYPE_NORMAL
- en: All the commands from this chapter are available in the `03-monitor.sh` ([https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9](https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9))
    Gist.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this chapter, we'll need a few things that were not requirements before,
    even though you probably already used them.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start using UIs so we'll need NGINX Ingress Controller that will route
    traffic from outside the cluster. We'll also need environment variable `LB_IP`
    with the IP through which we can access worker nodes. We'll use it to configure
    a few Ingress resources.
  prefs: []
  type: TYPE_NORMAL
- en: The Gists used to test the examples in this chapters are below. Please use them
    as they are, or as inspiration to create your own cluster or to confirm whether
    the one you already have meets the requirements. Due to new requirements (Ingress
    and `LB_IP`), all the cluster setup Gists are new.
  prefs: []
  type: TYPE_NORMAL
- en: A note to Docker for Desktop users You'll notice `LB_IP=[...]` command at the
    end of the Gist. You'll have to replace `[...]` with the IP of your cluster. Probably
    the easiest way to find it is through the `ifconfig` command. Just remember that
    it cannot be `localhost`, but the IP of your laptop (for example, `192.168.0.152)`.A
    note to minikube and Docker for Desktop users We have to increase memory to 3
    GB. Please have that in mind in case you were planning only to skim through the
    Gist that matches your Kubernetes flavor.
  prefs: []
  type: TYPE_NORMAL
- en: The Gists are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '`gke-monitor.sh`: **GKE** with 3 n1-standard-1 worker nodes, **nginx Ingress**,
    **tiller**, and cluster IP stored in environment variable **LB_IP** ([https://gist.github.com/vfarcic/10e14bfbec466347d70d11a78fe7eec4](https://gist.github.com/vfarcic/10e14bfbec466347d70d11a78fe7eec4)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eks-monitor.sh`: **EKS** with 3 t2.small worker nodes, **nginx Ingress**,
    **tiller**, **Metrics Server**, and cluster IP stored in environment variable
    **LB_IP** ([https://gist.github.com/vfarcic/211f8dbe204131f8109f417605dbddd5](https://gist.github.com/vfarcic/211f8dbe204131f8109f417605dbddd5)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aks-monitor.sh`: **AKS** with 3 Standard_B2s worker nodes, **nginx Ingress**,
    and **tiller**, and cluster IP stored in environment variable **LB_IP** ([https://gist.github.com/vfarcic/5fe5c238047db39cb002cdfdadcfbad2](https://gist.github.com/vfarcic/5fe5c238047db39cb002cdfdadcfbad2)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker-monitor.sh`: **Docker for Desktop** with **2 CPUs**, **3 GB RAM**,
    **nginx Ingress**, **tiller**, **Metrics Server**, and cluster IP stored in environment
    variable **LB_IP** ([https://gist.github.com/vfarcic/4d9ab04058cf00b9dd0faac11bda8f13](https://gist.github.com/vfarcic/4d9ab04058cf00b9dd0faac11bda8f13)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minikube-monitor.sh`: **minikube** with **2 CPUs**, **3 GB RAM**, **ingress**,
    **storage-provisioner**, **default-storageclass**, and **metrics-server** addons
    enabled, **tiller**, and cluster IP stored in environment variable **LB_IP** ([https://gist.github.com/vfarcic/892c783bf51fc06dd7f31b939bc90248](https://gist.github.com/vfarcic/892c783bf51fc06dd7f31b939bc90248)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a cluster, we'll need to choose the tools we'll use to accomplish
    our goals.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the tools for storing and querying metrics and alerting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**HorizontalPodAutoscaler** (**HPA**) and **Cluster Autoscaler** (**CA**) provide
    essential, yet very rudimentary mechanisms to scale our Pods and clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: While they do scaling decently well, they do not solve our need to be alerted
    when there's something wrong, nor do they provide enough information required
    to find the cause of an issue. We'll need to expand our setup with additional
    tools that will allow us to store and query metrics as well as to receive notifications
    when there is an issue.
  prefs: []
  type: TYPE_NORMAL
- en: If we focus on tools that we can install and manage ourselves, there is very
    little doubt about what to use. If we look at the list of *Cloud Native Computing
    Foundation (CNCF)* projects ([https://www.cncf.io/projects/](https://www.cncf.io/projects/)),
    only two graduated so far (October 2018). Those are *Kubernetes* and *Prometheus*
    ([https://prometheus.io/](https://prometheus.io/)). Given that we are looking
    for a tool that will allow us to store and query metrics and that Prometheus fulfills
    that need, the choice is straightforward. That is not to say that there are no
    other similar tools worth considering. There are, but they are all service based.
    We might explore them later but, for now, we're focused on those that we can run
    inside our cluster. So, we'll add Prometheus to the mix and try to answer a simple
    question. What is Prometheus?
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus is a database (of sorts) designed to fetch (pull) and store highly
    dimensional time series data.
  prefs: []
  type: TYPE_NORMAL
- en: Time series are identified by a metric name and a set of key-value pairs. Data
    is stored both in memory and on disk. Former allows fast retrieval of information,
    while the latter exists for fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus' query language allows us to easily find data that can be used both
    for graphs and, more importantly, for alerting. It does not attempt to provide
    "great" visualization experience. For that, it integrates with *Grafana* ([https://grafana.com/](https://grafana.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: Unlike most other similar tools, we do not push data to Prometheus. Or, to be
    more precise, that is not the common way of getting metrics. Instead, Prometheus
    is a pull-based system that periodically fetches metrics from exporters. There
    are many third-party exporters we can use. But, in our case, the most crucial
    exporter is baked into Kubernetes. Prometheus can pull data from an exporter that
    transforms information from Kube API. Through it, we can fetch (almost) everything
    we might need. Or, at least, that's where the bulk of the information will be
    coming from.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, storing metrics in Prometheus would not be of much use if we are not
    notified when there's something wrong. Even when we do integrate Prometheus with
    Grafana, that will only provide us with dashboards. I assume that you have better
    things to do than to stare at colorful graphs. So, we'll need a way to send alerts
    from Prometheus to, let's say, Slack. Luckily, *Alertmanager* ([https://prometheus.io/docs/alerting/alertmanager/](https://prometheus.io/docs/alerting/alertmanager/))
    allows us just that. It is a separate application maintained by the same community.
  prefs: []
  type: TYPE_NORMAL
- en: We'll see how all those pieces fit together through hands-on exercises. So,
    let's get going and install Prometheus, Alertmanager, and a few other applications.
  prefs: []
  type: TYPE_NORMAL
- en: A quick introduction to Prometheus and Alertmanager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll continue the trend of using Helm as the installation mechanism. Prometheus'
    Helm Chart is maintained as one of the official Charts. You can find more info
    in the project's *README* ([https://github.com/helm/charts/tree/master/stable/prometheus](https://github.com/helm/charts/tree/master/stable/prometheus)).
    If you focus on the variables in the *Configuration section* ([https://github.com/helm/charts/tree/master/stable/prometheus#configuration](https://github.com/helm/charts/tree/master/stable/prometheus#configuration)),
    you'll notice that there are quite a few things we can tweak. We won't go through
    all the variables. You can check the official documentation for that. Instead,
    we'll start with a basic setup, and extend it as our needs increase.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the variables we'll use as a start.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: All we're doing for now is defining `resources` for all five applications we'll
    install, as well as enabling Ingress with a few annotations that will make sure
    that we are not redirected to HTTPS version since we do not have certificates
    for our ad-hoc domains. We'll dive into the applications that'll be installed
    later. For now, we'll define the addresses for Prometheus and Alertmanager UIs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let's install the Chart.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The command we just executed should be self-explanatory, so we'll jump into
    the relevant parts of the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the Chart installed one DeamonSet and four Deployments.
  prefs: []
  type: TYPE_NORMAL
- en: The DeamonSet is Node Exporter, and it'll run a Pod on every node of the cluster.
    It provides node-specific metrics that will be pulled by Prometheus. The second
    exporter (Kube State Metrics) runs as a single replica Deployment. It fetches
    data from Kube API and transforms them into the Prometheus-friendly format. The
    two will provide most of the metrics we'll need. Later on, we might choose to
    expand them with additional exporters. For now, those two together with metrics
    fetched directly from Kube API should provide more metrics than we can absorb
    in a single chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Further on, we got the Server, which is Prometheus itself. Alertmanager will
    forward alerts to their destination. Finally, there is Pushgateway that we might
    explore in one of the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: While waiting for all those apps to become operational, we might explore the
    flow between them.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus Server pulls data from exporters. In our case, those are Node Exporter
    and Kube State Metrics. The job of those exporters is to fetch data from the source
    and transform it into the Prometheus-friendly format. Node Exporter gets the data
    from `/proc` and `/sys` volumes mounted on the nodes, while Kube State Metrics
    gets it from Kube API. Metrics are stored internally in Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from being able to query that data, we can define alerts. When an alert
    reaches its threshold, it is forwarded to Alertmanager that acts as a crossroad.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on its internal rules, it can forward those alerts further to various
    destinations like Slack, email, and HipChat (only to name a few).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/701f20e3-39b9-495f-b689-ccf64772ece1.png)Figure 3-1: The flow of
    data to and from Prometheus (arrows indicate the direction)'
  prefs: []
  type: TYPE_IMG
- en: By now, Prometheus Server probably rolled out. We'll confirm that just in case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let's take a look at what is inside the Pod created through the `prometheus-server`
    Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the relevant parts, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Besides the container based on the `prom/prometheus` image, we got another one
    created from `jimmidyson/configmap-reload`. The job of the latter is to reload
    Prometheus whenever we change the configuration stored in a ConfigMap.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we might want to take a look at the `prometheus-server` ConfigMap, since
    it stores all the configuration Prometheus needs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the relevant parts, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the `alerts` are still empty. We'll change that soon.
  prefs: []
  type: TYPE_NORMAL
- en: Further down is the `prometheus.yml` config with `scrape_configs` taking most
    of the space. We could spend a whole chapter explaining the current config and
    the ways we could modify it. We will not do that because the config in front of
    you is bordering insanity. It's the prime example of how something can be made
    more complicated than it should be. In most cases, you should keep it as-is. If
    you do want to fiddle with it, please consult the official documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll take a quick look at Prometheus' screens.
  prefs: []
  type: TYPE_NORMAL
- en: A note to Windows users Git Bash might not be able to use the `open` command.
    If that's the case, replace `open` with `echo`. As a result, you'll get the full
    address that should be opened directly in your browser of choice.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The config screen reflects the same information we already saw in the `prometheus-server`
    ConfigMap, so we'll move on.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's take a look at the targets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: That screen contains seven targets, each providing different metrics. Prometheus
    is periodically pulling data from those targets.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the outputs and screenshots in this chapter are taken from AKS. You might
    see some differences depending on your Kubernetes flavor.You might notice that
    this chapter contains much more screenshots than any other. Even though it might
    look like there are too many, I wanted to make sure that you can compare your
    results with mine, since there will be inevitable differences that might sometimes
    look confusing if you do not have a reference (my screenshots).![](assets/f207a763-f021-4f45-966b-948bae855230.png)Figure
    3-2: Prometheus'' targets screenA note to AKS users The *kubernetes-apiservers*
    target might be red indicating that Prometheus cannot connect to it. That''s OK
    since we won''t use its metrics.A note to minikube users The *kubernetes-service-endpoints*
    target might have a few sources in red. There''s no reason for alarm. Those are
    not reachable, but that won''t affect our exercises.'
  prefs: []
  type: TYPE_NORMAL
- en: We cannot find out what each of those targets provides from that screen. We'll
    try to query the exporters in the same way as Prometheus pulls them.
  prefs: []
  type: TYPE_NORMAL
- en: To do that, we'll need to find out the Services through which we can access
    the exporters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The output, from AKS, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We are interested in `prometheus-kube-state-metrics` and `prometheus-node-exporter`
    since they provide access to data from the exporters we'll use in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll create a temporary Pod through which we'll access the data available
    through the exporters behind those Services.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We created a new Pod based on `appropriate/curl`. That image serves a single
    purpose of providing `curl`. We specified `prometheus-node-exporter:9100/metrics`
    as the command, which is equivalent to running `curl` with that address. As a
    result, a lot of metrics were output. They are all in the same `key/value` format
    with optional labels surrounded by curly braces (`{` and `}`). On top of each
    metric, there is a `HELP` entry that explains its function as well as `TYPE` (for
    example, `gauge`). One of the metrics is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We can see that it provides `Memory information field MemTotal_bytes` and that
    the type is `gauge`. Below the `TYPE` is the actual metric with the key (`node_memory_MemTotal_bytes`)
    and value `3.878477824e+09`.
  prefs: []
  type: TYPE_NORMAL
- en: Most of Node Exporter metrics are without labels. So, we'll have to look for
    an example in the `prometheus-kube-state-metrics` exporter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the Kube State metrics follow the same pattern as those from
    the Node Exporter. The major difference is that most of them do have labels. An
    example is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: That metric represents the time the Deployment `prometheus-server` was created
    inside the `metrics` Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: I'll leave it to you to explore those metrics in more detail. We'll use quite
    a few of them soon.
  prefs: []
  type: TYPE_NORMAL
- en: For now, just remember that with the combination of the metrics coming from
    the Node Exporter, Kube State Metrics, and those coming from Kubernetes itself,
    we can cover most of our needs. Or, to be more precise, those provide data required
    for most of the basic and common use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll take a look at the alerts screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The screen is empty. Do not despair. We'll get back to that screen quite a few
    times. The alerts we'll be increasing as we progress. For now, just remember that's
    where you can find your alerts.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we'll open the graph screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: That is where you'll spend your time debugging issues you'll discover through
    alerts.
  prefs: []
  type: TYPE_NORMAL
- en: As our first task, we'll try to retrieve information about our nodes. We'll
    use `kube_node_info` so let's take a look at its description (help) and its type.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the `HELP` and `TYPE` entries, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You are likely to see variations between your results and mine. That's normal
    since our clusters probably have different amounts of resources, my bandwidth
    might be different, and so on. In some cases, my alerts will fire, and yours won't,
    or the other way around. I'll do my best to explain my experience and provide
    screenshots that accompany them. You'll have to compare that with what you see
    on your screen.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's try using that metric in Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the following query in the expression field.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Click the Execute button to retrieve the values of the `kube_node_info` metric.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike previous chapters, the Gist from this one (`03-monitor.sh` ([https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9](https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9)))
    contains not only the commands but also Prometheus expressions. They are all commented
    (with `#`). If you're planning to copy and paste the expressions from the Gist,
    please exclude the comments. Each expression has `# Prometheus expression` comment
    on top to help you identify it. As an example, the one you just executed is written
    in the Gist as follows. `# Prometheus expression` `# kube_node_info`
  prefs: []
  type: TYPE_NORMAL
- en: If you check the `HELP` entry of the `kube_node_info`, you'll see that it provides
    `information about a cluster node` and that it is a `gauge`. A **gauge** ([https://prometheus.io/docs/concepts/metric_types/#gauge](https://prometheus.io/docs/concepts/metric_types/#gauge))
    is a metric that represents a single numerical value that can arbitrarily go up
    and down.
  prefs: []
  type: TYPE_NORMAL
- en: That makes sense for information about nodes since their number can increase
    or decrease over time.
  prefs: []
  type: TYPE_NORMAL
- en: A Prometheus gauge is a metric that represents a single numerical value that
    can arbitrarily go up and down.
  prefs: []
  type: TYPE_NORMAL
- en: If we focus on the output, you'll notice that there are as many entries as there
    are worker nodes in the cluster. The value (`1`) is useless in this context. Labels,
    on the other hand, can provide some useful information. For example, in my case,
    operating system (`os_image`) is `Ubuntu 16.04.5 LTS`. Through that example, we
    can see that we can use the metrics not only to calculate values (for example,
    available memory) but also to get a glimpse into the specifics of our system.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f2eea74f-64b6-4499-887d-fa8f42a957ef.png)Figure 3-3: Prometheus''
    console output of the kube_node_info metric'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see if we can get a more meaningful query by combining that metric with
    one of the Prometheus' functions. We'll `count` the number of worker nodes in
    our cluster. The `count` is one of Prometheus' *aggregation operators* ([https://prometheus.io/docs/prometheus/latest/querying/operators/#aggregation-operators](https://prometheus.io/docs/prometheus/latest/querying/operators/#aggregation-operators)).
  prefs: []
  type: TYPE_NORMAL
- en: Please execute the expression that follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The output should show the total number of worker nodes in your cluster. In
    my case (AKS) there are `3`. On the first look, that might not be very helpful.
    You might think that you should know without Prometheus how many nodes you have
    in your cluster. But that might not be true. One of the nodes might have failed,
    and it did not recuperate. That is especially true if you're running your cluster
    on-prem without scaling groups. Or maybe Cluster Autoscaler increased or decreased
    the number of nodes. Everything changes over time, either due to failures, through
    human actions, or through a system that adapts itself. No matter the reasons for
    volatility, we might want to be notified when something reaches a threshold. We'll
    use nodes as the first example.
  prefs: []
  type: TYPE_NORMAL
- en: Our mission is to define an alert that will notify us if there are more than
    three or less than one nodes in the cluster. We'll imagine that those are our
    limits and that we want to know if the lower or the upper thresholds are reached
    due to failures or Cluster Autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: We'll take a look at a new definition of the Prometheus Chart's values. Since
    the definition is big and it will grow with time, from now on, we'll only look
    at the differences.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We added a new entry `serverFiles.alerts`. If you check Prometheus' Helm documentation,
    you'll see that it allows us to define alerts (hence the name). Inside it, we're
    using the "standard" Prometheus syntax for defining alerts.
  prefs: []
  type: TYPE_NORMAL
- en: Please consult *Alerting Rules documentation* ([https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/))
    for more info about the syntax.
  prefs: []
  type: TYPE_NORMAL
- en: We defined only one group of rules called `nodes`. Inside it are two `rules`.
    The first one (`TooManyNodes`) will notify us if there are more than `3` nodes
    `for` more than `15` minutes. The other (`TooFewNodes`) will do the opposite.
    It'll tell us if there are no nodes (`<1`) for `15` minutes. Both `rules` have
    `labels` and `annotations` that, for now, serve only informational purposes. Later
    on, we'll see their real usage.
  prefs: []
  type: TYPE_NORMAL
- en: Let's upgrade our Prometheus' Chart and see the effect of the new alerts.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: It'll take a few moments until the new configuration is "discovered" and Prometheus
    is reloaded. After a while, we can open the Prometheus alerts screen and check
    whether we got our first entries.
  prefs: []
  type: TYPE_NORMAL
- en: From now on, I won't comment (much) on the need to wait for a while until next
    config is propagated. If what you see on the screen does not coincide with what
    you're expecting, please wait for a while and refresh it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You should see two alerts.
  prefs: []
  type: TYPE_NORMAL
- en: Both alerts are green since none evaluates to `true`. Depending on the Kuberentes
    flavor you choose, you either have only one node (for example, Docker for Desktop
    and minikube) or you have three nodes (for example, GKE, EKS, AKS). Since our
    alerts are checking whether we have less than one, or more than three nodes, neither
    of the conditions are met, no matter which Kubernetes flavor you're using.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your cluster was not created through one of the Gists provided at the beginning
    of this chapter, then you might have more than three nodes in your cluster, and
    the alert will fire. If that''s the case, I suggest you modify the `mon/prom-values-nodes.yml`
    file to adjust the threshold of the alert.![](assets/c8da3d50-5320-4ddc-b3ed-5175750546d8.png)Figure
    3-4: Prometheus'' alerts screen'
  prefs: []
  type: TYPE_NORMAL
- en: Seeing inactive alerts is boring, so I want to show you one that fires (becomes
    red). To do that, we can add more nodes to the cluster (unless you're using a
    single node cluster like Docker for Desktop and minikube). However, it would be
    easier to modify the expression of one of the alerts, so that's what we'll do
    next.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The new definition changed the condition of the `TooManyNodes` alert to fire
    if there are more than zero nodes. We also changed the `for` statement so that
    we do not need to wait for `15` minutes before the alert fires.
  prefs: []
  type: TYPE_NORMAL
- en: Let's upgrade the Chart one more time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '... and we''ll go back to the alerts screen.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: A few moments later (don't forget to refresh the screen), the alert will switch
    to the pending state, and the color will change to yellow. That means that the
    conditions for the alert are met (we do have more than zero nodes) but the `for`
    period did not yet expire.
  prefs: []
  type: TYPE_NORMAL
- en: Wait for a minute (duration of the `for` period) and refresh the screen. The
    alert's state switched to firing and the color changed to red. Prometheus sent
    our first alert.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cb890924-63a6-49f3-953b-783f7495ba3b.png)Figure 3-5: Prometheus''
    alerts screen with one of the alerts firing'
  prefs: []
  type: TYPE_NORMAL
- en: Where was the alert sent? Prometheus Helm Chart deployed Alertmanager and pre-configured
    Prometheus to send its alerts there. Let's take a look at it's UI.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We can see that one alert reached Alertmanager. If we click the + info button
    next to the `TooManyNodes` alert, we'll see the annotations (summary and description)
    as well as the labels (severity).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2de2ea86-984f-4529-ad8a-f54a3680104a.png)Figure 3-6: Alertmanager
    UI with one of the alerts expanded'
  prefs: []
  type: TYPE_NORMAL
- en: We are likely not going to sit in front of the Alertmanager waiting for issues
    to appear. If that would be our goal, we could just as well wait for the alerts
    in Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying alerts is indeed not the reason why we have Alertmanager. It is supposed
    to receive alerts and dispatch them further. It is not doing anything of that
    sort simply because we did not yet define the rules it should use to forward alerts.
    That's our next task.
  prefs: []
  type: TYPE_NORMAL
- en: We'll take a look at yet another update of the Prometheus Chart values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: When we apply that definition, we'll add `alertmanager.yml` file to Alertmanager.
    If contains the rules it should use to dispatch alerts. The `route` section contains
    general rules that will be applied to all alerts that do not match one of the
    `routes`. The `group_wait` value makes Alertmanager wait for `10` seconds in case
    additional alerts from the same group arrive. That way, we'll avoid receiving
    multiple alerts of the same type.
  prefs: []
  type: TYPE_NORMAL
- en: When the first alert of a group is dispatched, it'll use the value of the `group_interval`
    field (`5m`) before sending the next batch of the new alerts from the same group.
  prefs: []
  type: TYPE_NORMAL
- en: The `receiver` field in the `route` section defines the default destination
    of the alerts. Those destinations are defined in the `receivers` section below.
    In our case, we're sending the alerts to the `slack` receiver by default.
  prefs: []
  type: TYPE_NORMAL
- en: The `repeat_interval` (set to `3h`) defines the period after which alerts will
    be resent if Alertmanager continues receiving them.
  prefs: []
  type: TYPE_NORMAL
- en: The `routes` section defines specific rules. Only if none of them match, those
    in the `route` section above will be used. The `routes` section inherits properties
    from above so only those that we define in this section will change. We'll keep
    sending matching `routes` to `slack`, and the only change is the increase of the
    `repeat_interval` from `3h` to `5d`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The critical part of the `routes` is the `match` section. It defines filters
    that are used to decide whether an alert is a match or not. In our case, only
    those with the labels `severity: notify` and `frequency: low` will be considered
    a match.'
  prefs: []
  type: TYPE_NORMAL
- en: All in all, the alerts with `severity` label set to `notify` and `frequency`
    set to `low` will be resent every five days. All the other alerts will have a
    frequency of three hours.
  prefs: []
  type: TYPE_NORMAL
- en: The last section of our Alertmanager config is `receivers`. We have only one
    receiver named `slack`. Below the `name` is `slack_config`. It contains Slack-specific
    configuration. We could have used `hipchat_config`, `pagerduty_config`, or any
    other of the supported ones. Even if our destination is not one of those, we could
    always fall back to `webhook_config` and send a custom request to the API of our
    tool of choice.
  prefs: []
  type: TYPE_NORMAL
- en: For the list of all the supported `receivers`, please consult *Alertmanager
    Configuration* page ([https://prometheus.io/docs/alerting/configuration/](https://prometheus.io/docs/alerting/configuration/)).
  prefs: []
  type: TYPE_NORMAL
- en: Inside `slack_configs` section, we have the `api_url` that contains the Slack
    address with the token from one of the rooms in the *devops20* channel.
  prefs: []
  type: TYPE_NORMAL
- en: For information how to general an incoming webhook address for your Slack channel,
    please visit the *Incoming Webhooks* page ([https://api.slack.com/incoming-webhooks](https://api.slack.com/incoming-webhooks)).
  prefs: []
  type: TYPE_NORMAL
- en: Next is the `send_resolved` flag. When set to `true`, Alertmanager will send
    notifications not only when an alert is fired, but also when the issue that caused
    it is resolved.
  prefs: []
  type: TYPE_NORMAL
- en: We're using `summary` annotation as the `title` of the message, and the `description`
    annotation for the `text`. Both are using *Go Templates* ([https://golang.org/pkg/text/template/](https://golang.org/pkg/text/template/)).
    Those are the same annotations we defined in the Prometheus' alerts.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `title_link` is set to `http://my-prometheus.com/alerts`. That
    is indeed not the address of your Prometheus UI but, since I could not know in
    advance what will be your domain, I put a non-existing one. Feel free to change
    `my-prometheus.com` to the value of the environment variable `$PROM_ADDR`. Or
    just leave it as-is knowing that if you click the link, it will not take you to
    your Prometheus UI.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we explored Alertmanager configuration, we can proceed and upgrade
    the Chart.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: A few moments later, Alertmanager will be reconfigured, and the next time it
    receives the alert from Prometheus, it'll dispatch it to Slack. We can confirm
    that by visiting the `devops20.slack.com` workspace. If you did not register already,
    please go to [slack.devops20toolkit.com](http://slack.devops20toolkit.com). Once
    you are a member, we can visit the `devops25-tests` channel.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: You should see the `Cluster increased` notification. Don't get confused if you
    see other messages. You are likely not the only one running the exercises from
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d1936f65-8d48-452a-91f0-03da31c126ca.png)Figure 3-7: Slack with
    an alert message received from AlertmanagerSometimes, for reasons I could not
    figure out, Slack receives empty notifications from Alertmanager. For now, I''m
    ignoring the issue out of laziness.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we went through the basic usage of Prometheus and Alertmanager, we'll
    take a break from hands-on exercises and discuss the types of metrics we might
    want to use.
  prefs: []
  type: TYPE_NORMAL
- en: Which metric types should we use?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If this is the first time you're using Prometheus hooked into metrics from Kube
    API, the sheer amount might be overwhelming. On top of that, consider that the
    configuration excluded many of the metrics offered by Kube API and that we could
    extend the scope even further with additional exporters.
  prefs: []
  type: TYPE_NORMAL
- en: While every situation is different and you are likely to need some metrics specific
    to your organization and architecture, there are some guidelines that we should
    follow. In this section, we'll discuss the key metrics. Once you understand them
    through a few examples, you should be able to extend their use to your specific
    use-cases.
  prefs: []
  type: TYPE_NORMAL
- en: The four key metrics everyone should utilize are latency, traffic, errors, and
    saturation.
  prefs: []
  type: TYPE_NORMAL
- en: Those four metrics been championed by Google **Site Reliability Engineers**
    (**SREs**) as the most fundamental metrics for tracking performance and health
    of a system.
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency** represents the time it takes to service to respond to a request.
    The focus should not be only on duration but also on distinguishing between the
    latency of successful requests and the latency of failed requests.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Traffic** is a measure of demand that is being placed on services. An example
    would be the number of HTTP requests per second.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errors** are measured by the rate of requests that fail. Most of the time
    those failures are explicit (for example, HTTP 500 errors) but they can be implicit
    as well (for example, an HTTP 200 response with the body describing that the query
    did not return any results).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Saturation** can be described by "fullness" of a service or a system. A typical
    example would be lack of CPU that results in throttling and, consequently, degrades
    the performance of the applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Over time, different monitoring methods were developed. We got, for example,
    the **USE** method that states that for every resource, we should check **utilization**,
    **saturation**, and **errors**. Another one is the **RED** method that defines
    **rate**, **errors**, and **duration** as the key metrics. Those and many others
    are similar in their essence and do not differ significantly from SREs demand
    to measure latency, traffic, errors, and saturation.
  prefs: []
  type: TYPE_NORMAL
- en: We'll go through each of the four types of measurements described by SREs and
    provide a few examples. We might even extend them with metrics that do not necessarily
    fit into any of the four categories. The first in line is latency.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting on latency-related issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll use the `go-demo-5` application to measure latency, so our first step
    is to install it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We generated an address that we'll use as Ingress entry-point, and we deployed
    the application using Helm. Now we should wait until it rolls out.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Before we proceed, we'll check whether the application is indeed working correctly
    by sending an HTTP request.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The output should be the familiar `hello, world!` message.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see whether we can, for example, get the duration of requests entering
    the system through Ingress.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: If you click on the - insert metrics at cursor - drop-down list, you'll be able
    to browse through all the available metrics. The one we're looking for is `nginx_ingress_controller_request_duration_seconds_bucket`.
    As its name implies, the metric comes from NGINX Ingress Controller, and provide
    request durations in seconds and grouped in buckets.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows and click the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In this case, seeing the raw values might not be very useful, so please click
    the Graph tab.
  prefs: []
  type: TYPE_NORMAL
- en: You should see graphs, one for each Ingress. Each is increasing because the
    metric in question is a counter ([https://prometheus.io/docs/concepts/metric_types/#counter](https://prometheus.io/docs/concepts/metric_types/#counter)).
    Its value is growing with each request.
  prefs: []
  type: TYPE_NORMAL
- en: A Prometheus counter is a cumulative metric whose value can only increase, or
    be reset to zero on restart.
  prefs: []
  type: TYPE_NORMAL
- en: What we need is to calculate the rate of requests over a period of time. We'll
    accomplish that by combining `sum` and `rate` ([https://prometheus.io/docs/prometheus/latest/querying/functions/#rate()](https://prometheus.io/docs/prometheus/latest/querying/functions/#rate()))
    functions. The former should be self-explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus' rate function calculates the per-second average rate of increase
    of the time series in the range vector.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The resulting graph shows us the per-second rate of all the requests entering
    the system through Ingress. The rate is calculated based on five minutes intervals.
    If you hover one of the lines, you'll see the additional information like the
    value and the Ingress. The `by` statement allows us to group the results by `ingress`.
  prefs: []
  type: TYPE_NORMAL
- en: Still, the result by itself is not very useful, so let's redefine our requirement.
    We should be able to find out how many of the requests are slower than 0.25 seconds.
    We cannot do that directly. Instead, we can retrieve all those that are 0.25 second
    or faster.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: What we really want is to find the percentage of requests that fall into 0.25
    seconds bucket. To accomplish that, we'll get the rate of the requests faster
    than or equal to 0.25 seconds, and divide the result with the rate of all the
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: You probably won't see much in the graph since we did not yet generate much
    traffic, beyond occasional interaction with Prometheus and Alertmanager and a
    single request we sent to `go-demo-5`. Nevertheless, the few lines you can see
    display the percentage of the requests that responded within 0.25 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: For now, we are interested only in `go-demo-5` requests, so we'll refine the
    expression further, to limit the results only to `go-demo-5` Ingress.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The graph should be almost empty since we sent only one request. Or, maybe you
    received the `no datapoints found` message. It's time to generate some traffic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We sent thirty requests to `go-demo-5`. The application has a "hidden" feature
    to delay response to a request. Given that we want to generate traffic with random
    response time, we used `DELAY` variable with a random value up to thousand milliseconds.
    Now we can re-run the same query and see whether we can get some more meaningful
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Please wait for a while until data from new requests is gathered, then type
    the expression that follows (in Prometheus), and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This time, we can see the emergence of a new line. In my case (screenshot following),
    around twenty-five percent of requests have durations that are within 0.25 seconds.
    Or, to put it into different words, around a quarter of the requests are slower
    than expected.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/399a7670-e5f9-4753-8c2e-a02ef8ef2241.png)Figure 3-8: Prometheus''
    graph screen with the percentage of requests with 0.25 seconds duration'
  prefs: []
  type: TYPE_NORMAL
- en: Filtering metrics for a specific application (Ingress) is useful when we do
    know that there is a problem and we want to dig further into it. However, we still
    need an alert that will tell us that there is an issue. For that, we'll execute
    a similar query, but this time without limiting the results to a specific application
    (Ingress). We'll also have to define a condition that will fire the alert, so
    we'll set the threshold to ninety-five percent (0.95). Without such a threshold,
    we'd get a notification every time a single request is slow. As a result, we'd
    get swarmed with alarms and would probably start ignoring them soon afterward.
    After all, no system is in danger if a single request is slow, but only if a considerable
    number of them is. In our case, that is five percent of slow requests or, to be
    more precise, less than ninety-five percent of fast requests.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We can see occasional cases when less than ninety-five percent of requests are
    within 0.25 second. In my case (screenshot following), we can see that Prometheus,
    Alertmanager, and `go-demo-5` are occasionally slow.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/35c99ae7-7fd0-453c-bc6f-0d4b25288425.png)Figure 3-9: Prometheus''
    graph screen with the percentage of requests within 0.25 seconds duration and
    limited only to results higher than ninety-five percent'
  prefs: []
  type: TYPE_NORMAL
- en: The only thing missing is to define an alert based on the previous expression.
    As a result, we should get a notification whenever less than ninety-five percent
    of requests have a duration less than 0.25 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: I prepared an updated set of Prometheus' Chart values, so let's take a look
    at the differences when compared with the one we're using currently.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We added a new alert `AppTooSlow`. It'll trigger if the percentage of requests
    with the duration of 0.25 seconds or less is smaller than ninety-five percent
    (`0.95`).
  prefs: []
  type: TYPE_NORMAL
- en: We also we reverted the threshold of the `TooManyNodes` back to its original
    value of `3`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll update the `prometheus` Chart with the new values and open the alerts
    screen to confirm whether the new alert was indeed added.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: If the `AppTooSlow` alert is still not available, please wait a few moments
    and refresh the screen.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/68c98ad5-8654-4de8-b3d3-f34e05f3ce6e.png)Figure 3-10: Prometheus''
    alerts screen'
  prefs: []
  type: TYPE_NORMAL
- en: The newly added alert is (probably) green (not triggering). We need to generate
    a few slow requests to see it in action.
  prefs: []
  type: TYPE_NORMAL
- en: Please execute the command that follows to send thirty requests with a random
    response time of up to ten thousand milliseconds (ten seconds).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: It'll take a few moments until Prometheus scrapes new metrics and for the alert
    to detect that the threshold is reached. After a while, we can open the alerts
    screen again and check whether the alert is indeed firing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the state of the alert is firing. If that's not your case, please
    wait a while longer and refresh the screen. In my case (screenshot following),
    the value is 0.125, meaning that only 12.5 percent of requests have a duration
    of 0.25 seconds or less.
  prefs: []
  type: TYPE_NORMAL
- en: 'There might be two or more active alerts inside `AppTooSlow` if `prometheus-server`,
    `prometheus-alertmanager`, or some other application is responding slow.![](assets/17e9cd6e-41dc-486b-9ee1-cfaf8eedb886.png)Figure
    3-11: Prometheus'' alerts screen with one alert firing'
  prefs: []
  type: TYPE_NORMAL
- en: The alert is red meaning that Prometheus sent it to Alertmanager which, in turn,
    forwarded it to Slack. Let's confirm that.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: As you can see (screenshot following), we received two notification. Since we
    reverted the threshold of the `TooManyNodes` alert back to greater than three
    nodes, and our cluster has less, Prometheus sent a notification to Alertmanager
    that the problem is resolved. As a result, we got a new notification in Slack.
    This time, the color of the message is green.
  prefs: []
  type: TYPE_NORMAL
- en: Further on, a new red message appeared indicating that an `Application is too
    slow`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f8549f2d-8a88-4179-933a-57b9f13b7e5a.png)Figure 3-12: Slack with
    alerts firing (red) and resolved (green) messages'
  prefs: []
  type: TYPE_NORMAL
- en: We often cannot rely on a single rule that will fit all the applications. Prometheus
    and, for example, Jenkins would be a good candidate of internal applications which
    we cannot expect to have less than five percent of response times above 0.25 seconds.
    So, we might want to filter further the alerts. We can use any number of labels
    for that. To keep it simple, we'll continue leveraging `ingress` label but, this
    time, we'll use regular expressions to exclude some applications (Ingresses) from
    the alert.
  prefs: []
  type: TYPE_NORMAL
- en: Let's open the graph screen one more time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Please type the expression that follows, press the Execute button, and switch
    to the *Graph* tab.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The addition to the previous query is the `ingress!~"prometheus-server|jenkins"`
    filter. The `!~` is used to select metrics with labels that do NOT regex match
    the `prometheus-server|jenkins` string. Since `|` is equivalent to the `or` statement,
    we can translate that filter as "everything that is NOT `prometheus-server` or
    is NOT `jenkins`." We do not have Jenkins in our cluster. I just wanted to show
    you a way to exclude multiple values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0e6442a6-12b5-48d9-ab0c-b7da6c30abdc.png)Figure 3-13: Prometheus
    graph screen with the percentage of requests with 0.25 seconds duration and the
    results excluding prometheus-server and jenkins'
  prefs: []
  type: TYPE_NORMAL
- en: We could have complicated it a bit more and specified `ingress!~"prometheus.+|jenkins.+`
    as the filter. In that case, it would exclude all Ingresses with the name that
    starts with `prometheus` and `jenkins`. The key is in the `.+` addition that,
    in RegEx, matches one or more entries (`+`) of any character (`.`).
  prefs: []
  type: TYPE_NORMAL
- en: We won't go into an explanation of RegEx syntax. I expect you to be already
    familiar with it. If you're not, you might want to Google it or to visit *Regular
    expression Wiki* page ([https://en.wikipedia.org/wiki/Regular_expression](https://en.wikipedia.org/wiki/Regular_expression)).
  prefs: []
  type: TYPE_NORMAL
- en: The previous expression retrieves only the results that are NOT `prometheus-server`
    and `jenkins`. We would probably need to create another one that includes only
    those two.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The only difference, when compared with the previous expression, is that this
    time we used the `=~` operator. It selects labels that regex-match the provided
    string. Also, the bucket (`le`) is now set to `0.5` seconds, given that both applications
    might need more time to respond and we are OK with that.
  prefs: []
  type: TYPE_NORMAL
- en: In my case, the graph shows `prometheus-server` as having one hundred percent
    requests with durations within 0.5 seconds (in your case that might not be true).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1a0043b8-213c-4c76-9209-2fa3a1ca851a.png)Figure 3-14: Prometheus
    graph screen with the percentage of requests with 0.5 seconds duration and the
    results including only prometheus-server and jenkins'
  prefs: []
  type: TYPE_NORMAL
- en: The few latency examples should be enough to get you going with that type of
    metrics, so we'll move to traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting on traffic-related issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we measured the latency of our applications, and we created alerts that
    fire when certain thresholds based on request duration are reached. Those alerts
    are not based on the number of requests coming in (traffic), but on the percentage
    of slow requests. The `AppTooSlow` would fire even if only one single request
    enters an application, as long as the duration is above the threshold. For completeness,
    we need to start measuring traffic or, to be more precise, the number of requests
    sent to each application and the system as a whole. Through that, we can know
    if our system is under a lot of stress and make a decision on whether to scale
    our applications, add more workers, or apply some other solution to mitigate the
    problem. We might even choose to block part of the incoming traffic if the number
    of requests reaches abnormal numbers providing a clear indication that we are
    under **Denial of Service** (**DoS**) attack ([https://en.wikipedia.org/wiki/Denial-of-service_attack](https://en.wikipedia.org/wiki/Denial-of-service_attack)).
  prefs: []
  type: TYPE_NORMAL
- en: We'll start by creating a bit of traffic that we can use to visualize requests.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: We sent a hundred requests to the `go-demo-5` application and opened Prometheus'
    graph screen.
  prefs: []
  type: TYPE_NORMAL
- en: We can retrieve the number of requests coming into the Ingress controller through
    the `nginx_ingress_controller_requests`. Since it is a counter, we can continue
    using `rate` function combined with `sum`. Finally, we probably want to know the
    rate of requests grouped by the `ingress` label.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, press the Execute button, and switch
    to the *Graph* tab.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We can see a spike on the right side of the graph. It shows the requests that
    went to the `go-demo-5` applications through the Ingress with the same name.
  prefs: []
  type: TYPE_NORMAL
- en: In my case (screenshot following), the peak is close to one request per second
    (yours will be different).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/713e0fd7-3d7e-414a-b4df-02526e34bb83.png)Figure 3-15: Prometheus''
    graph screen with the rate of the number of requests'
  prefs: []
  type: TYPE_NORMAL
- en: We are probably more interested in the number of requests per second per replica
    of an application, so our next task is to find a way to retrieve that data. Since
    `go-demo-5` is a Deployment, we can use `kube_deployment_status_replicas`.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We can see the number of replicas of each Deployment in the system. The `go-demo-5`
    application, in my case painted in red (screenshot following), has three replicas.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f1c0b08a-afb3-4988-87d2-d90ec102adb9.png)Figure 3-16: Prometheus''
    graph screen with the number of replicas of Deployments'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we should combine the two expressions, to get the number of requests per
    second per replica. However, we are facing a problem. For two metrics to join,
    they need to have matching labels. Both the Deployment and the Ingress of `go-demo-5`
    have the same name so we can use that to our benefit, given that we can rename
    one of the labels. We'll do that with the help of the `label_join` ([https://prometheus.io/docs/prometheus/latest/querying/functions/#label_join()](https://prometheus.io/docs/prometheus/latest/querying/functions/#label_join()))
    function.
  prefs: []
  type: TYPE_NORMAL
- en: For each timeseries in v, `label_join(v instant-vector, dst_label string, separator
    string, src_label_1 string, src_label_2 string, ...)` joins all the values of
    all the `src_labels` using the separator and returns the timeseries with the label
    `dst_label` containing the joined value.
  prefs: []
  type: TYPE_NORMAL
- en: If the previous explanation of the `label_join` function was confusing, you're
    not alone. Instead, let's go through the example that will transform `kube_deployment_status_replicas`
    by adding `ingress` label that will contain values from the `deployment` label.
    If we are successful, we'll be able to combine the result with `nginx_ingress_controller_requests`
    since both will have the same matching labels (`ingress`).
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Since we are, this time, interested mostly in values of the labels, please switch
    to the Console view by clicking the tab.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the output, each metric now contains an additional label
    `ingress` with the same value as `deployment`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d8145093-7933-49c7-9fdf-65a9fcf30124.png)Figure 3-17: Prometheus''
    console view of Deployment replicas status and a new label ingress created from
    the deployment label'
  prefs: []
  type: TYPE_NORMAL
- en: Now we can combine the two metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Switch back to the *Graph* view.
  prefs: []
  type: TYPE_NORMAL
- en: We calculated the rate of the number of requests per application (`ingress`)
    and divided it with the total number of replicas per application (`ingress`).
    The end result is the rate of the number of requests per application (`ingress`)
    per replica.
  prefs: []
  type: TYPE_NORMAL
- en: It might be worth noting that we can not retrieve the number of requests for
    each specific replica, but rather the average number of requests per replica.
    This method should work, given that Kubernetes networking in most cases performs
    round robin that results in more or less the same amount of requests being sent
    to each replica.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, now we know how many requests our replicas are receiving per second.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d4f2cc20-dd55-4a80-af2a-177918f74576.png)Figure 3-18: Prometheus''
    graph screen with the rate of requests divided by the number of Deployment replicas'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we learned how to write an expression to retrieve the rate of the number
    of requests per second per replica, we should convert it into an alert.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's take a look at the difference between the old and the new definition
    of Prometheus' Chart values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the expression is almost the same as the one we used in Prometheus'
    graph screen. The only difference is the threshold which we set to `0.1`. As a
    result, that alert should notify us whenever a replica receives more than a rate
    of `0.1` requests per second, calculated over the period of five minutes (`[5m]`).
    As you might have guessed, `0.1` requests per second is too low of a figure to
    use it in production. However, it'll allow us to trigger the alert easily and
    see it in action.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's upgrade our Chart, and open Prometheus' alerts screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Please refresh the screen until the `TooManyRequests` alert appears.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/84c0a44a-b5bd-4f17-a428-6f1513dec3d7.png)Figure 3-19: Prometheus''
    alerts screen'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll generate some traffic so that we can see the alert is generated
    and sent through Alertmanager to Slack.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: We sent two hundred requests and reopened the Prometheus' alerts screen. Now
    we should refresh the screen until the `TooManyRequests` alert becomes red.
  prefs: []
  type: TYPE_NORMAL
- en: Once Prometheus fired the alert, it was sent to Alertmanager and, from there,
    forwarded to Slack. Let's confirm that.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: We can see the `Too many requests` notification, thus proving that the flow
    of this alert works.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/499f9841-7cb1-4865-a6d1-b02dff0a4761.png)Figure 3-20: Slack with
    alert messages'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll jump into errors-related metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting on error-related issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should always be aware of whether our applications or the system is producing
    errors. However, we cannot start panicking on the first occurrence of an error
    since that would generate too many notifications that we'd likely end up ignoring.
  prefs: []
  type: TYPE_NORMAL
- en: Errors happen often, and many are caused by issues that are fixed automatically
    or are due to circumstances that are out of our control. If we are to perform
    an action on every error, we'd need an army of people working 24/7 only on fixing
    issues that often do not need to be fixed. As an example, entering into a "panic"
    mode because there is a single response with code in 500 range would almost certainly
    produce a permanent crisis. Instead, we should monitor the rate of errors compared
    to the total number of requests and react only if it passes a certain threshold.
    After all, if an error persists, that rate will undoubtedly increase. On the other
    hand, if it continues being low, it means that the issue was fixed automatically
    by the system (for example, Kubernetes rescheduled the Pods from the failed node)
    or that it was an isolated case that does not repeat.
  prefs: []
  type: TYPE_NORMAL
- en: Our next mission is to retrieve requests and distinguish separate them by their
    statuses. If we can do that, we should be able to calculate the rate of errors.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start by generating a bit of traffic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: We sent a hundred requests and opened Prometheus' graph screen.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see whether the `nginx_ingress_controller_requests` metric we used previously
    provides the statuses of the requests.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: We can see all the data recently scraped by Prometheus. If we pay closer attention
    to the labels, we can see that, among others, there is `status`. We can use it
    to calculate the percentage of those with errors (for example, 500 range) based
    on the total number of requests.
  prefs: []
  type: TYPE_NORMAL
- en: We already saw that we can use the `ingress` label to separate calculations
    per application, assuming that we are interested only in those that are public-facing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/adbe5b3c-eff0-4170-b518-20e9cb90f82d.png)Figure 3-21: Prometheus''
    console view with requests entering through Ingress'
  prefs: []
  type: TYPE_NORMAL
- en: The `go-demo-5` app has a special endpoint `/demo/random-error` that will generate
    random error responses. Approximately, one out of ten requests to that address
    will produce an error. We can use that to test our expressions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: We sent a hundred requests to the `/demo/random-error` endpoint and approximately
    ten percent of those responded with errors (HTTP status code `500`).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll have to wait for a few moments for Prometheus to scrape the new
    batch of metrics. After that, we can open the Graph screen and try to write an
    expression that will retrieve the error rate of our applications.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: We used `5..` RegEx to calculate the rate of the requests with errors grouped
    by `ingress`, and we divided the result with all the rate of all the requests.
    The result is grouped by `ingress`. In my case (screenshot following), the result
    is approximately 4 percent (`0.04`). Prometheus did not yet scrape all the metrics,
    and I expect that number to get closer to ten percent in the next scraping iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/080bb7c6-42d1-486b-9b17-4b938b5c1624.png)Figure 3-22: Prometheus''
    graph screen with the percentage with the requests with error responses'
  prefs: []
  type: TYPE_NORMAL
- en: Let's compare the updated version of the Chart's values file with the one we
    used previously.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: The alert will fire if the rate of errors is over 2.5% of the total rate of
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can upgrade our Prometheus' Chart.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: There's probably no need to confirm that the alerting works. We already saw
    that Prometheus sends all the alerts to Alertmanager and that they are forwarded
    from there to Slack.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll move to saturation metrics and alerts.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting on saturation-related issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Saturation measures fullness of our services and the system. We should be aware
    if replicas of our services are processing too many requests and being forced
    to queue some of them. We should also monitor whether usage of our CPUs, memory,
    disks, and other resources reaches critical limits.
  prefs: []
  type: TYPE_NORMAL
- en: For now, we'll focus on CPU usage. We'll start by opening the Prometheus' graph
    screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Let's see if we can get the rate of used CPU by node (`instance`). We can use
    `node_cpu_seconds_total` metric for that. However, it is split into different
    modes, and we'll have to exclude a few of them to get the "real" CPU usage. Those
    will be `idle`, `iowait`, and any type of `guest` cycles.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Switch to the *Graph* view.
  prefs: []
  type: TYPE_NORMAL
- en: The output represents the actual usage of CPU in the system. In my case (screenshot
    following), excluding a temporary spike, all nodes are using less than a hundred
    CPU milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: The system is far from being under stress.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/52dcc7f8-4b9f-4e7f-b176-b73855232337.png)Figure 3-23: Prometheus''
    graph screen with the rate of used CPU grouped by node instances'
  prefs: []
  type: TYPE_NORMAL
- en: As you already noticed, absolute numbers are rarely useful. We should try to
    discover the percentage of used CPU. We'll need to find out how much CPU our nodes
    have. We can do that by counting the number of metrics. Each CPU gets its own
    data entry, one for each mode. If we limit the result to a single mode (for example,
    `system`), we should be able to get the total number of CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: In my case (screenshot following), there are six cores in total. Yours is likely
    to be six as well if you're using GKE, EKS, or AKS from the Gists. If, on the
    other hand, you're running the cluster in Docker for Desktop or minikube, the
    result should be one node.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can combine the two queries to get the percentage of used CPU
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: We summarized the rate of used CPUs and divided it by the total number of CPUs.
    In my case (screenshot following), only three to four percent of CPU is currently
    used.
  prefs: []
  type: TYPE_NORMAL
- en: That is not a surprise since most of the system is at rest. Not much is going
    on in our cluster right now.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2d2bf1f8-7ac6-4d21-a067-c6c523f0e45e.png)Figure 3-24: Prometheus''
    graph screen with the percentage of available CPU'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to fetch the percentage of used CPU of the whole cluster,
    we'll move our attention to applications.
  prefs: []
  type: TYPE_NORMAL
- en: We'll try to discover how many allocatable cores we have. From application's
    perspective, at least when they're running in Kubernetes, allocatable CPUs show
    how much can be requested for Pods. Allocatable CPU is always lower than total
    CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: The output should be lower than the number of cores used by our VMs. The allocatable
    cores show the amount of CPU that can be assigned containers. To be more precise,
    allocatable cores are the number of CPUs assigned to nodes minus those reserved
    by system-level processes. In my case (screenshot following), there are almost
    two full allocatable CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7283ecb4-bc9a-4432-ad59-3d1469a62c5f.png)Figure 3-25: Prometheus''
    graph screen with the allocatable CPU for each of the nodes in the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: However, in this context, we are interested in the total amount of allocatable
    CPUs since we are trying to discover how much is used by our Pods inside the whole
    cluster. So, we'll sum the allocatable cores.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: In my case, the total allocatable CPUs is somewhere around 5.8 cores. For the
    exact number, please hover on the graph line.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how many allocatable CPUs we have, we should try to discover
    how much was requested by Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that requested resources are not the same as used resources. We'll
    get to that use-case later. For now, we want to know how much we requested from
    the system.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: We can see that requested CPU is relatively low. In my case, all the containers
    with requested CPU have their values below 0.15 (hundred and fifty milliseconds).
    Your result might differ.
  prefs: []
  type: TYPE_NORMAL
- en: Just as with allocatable CPU, we are interested in the sum of requested CPU.
    Later on, we'll be able to combine the two results and deduce how much is left
    unreserved in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: We summed all the CPU resource requests. As a result, in my case (screenshot
    following), all the requested CPUs are slightly below 1.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0a3f5d81-4d68-4f87-8a3a-6c8c8bcfc4dd.png)Figure 3-26: Prometheus''
    graph screen with the sum of the requested CPU'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's combine the two expressions and see the percentage of requested CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: In my case, the output shows that around a quarter (0.25) of all allocatable
    CPU is reserved. That means that we could have four times as many CPU requests
    before we reach the need to expand the cluster. Of course, you already know that,
    if present, Cluster Autoscaler will add nodes before that happens. Still, knowing
    that we are close to reaching CPU limits is important. Cluster Autoscaler might
    not be working correctly, or it might not even be active. The latter case is true
    for most, if not all on-prem clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see whether we can convert the expressions we explored into alerts.
  prefs: []
  type: TYPE_NORMAL
- en: We'll explore yet another difference between a new set of Chart values and those
    we used before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: We can see from the differences that we restored the original threshold of `TooManyRequests`
    back to `1` and that we added two new alerts called `NotEnoughCPU` and `TooMuchCPURequested`.
  prefs: []
  type: TYPE_NORMAL
- en: The `NotEnoughCPU` alert will fire if more than ninety percent of CPU across
    the whole cluster is used for over thirty minutes. That way we'll avoid settings
    alarms if there is a temporary spike in CPU usage.
  prefs: []
  type: TYPE_NORMAL
- en: The `TooMuchCPURequested` also has the threshold of ninety percent and will
    be triggered if it persists for over thirty minutes. The expression computes the
    total amount of requested divided with the total among allocatable CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Both alerts are reflections of the Prometheus expressions we executed short
    while ago, so you should already be familiar with their purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Let's upgrade Prometheus' Chart with the new values and open the alerts screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: All that's left is to wait until the two new alerts appear. If they are not
    already there, please refresh your screen.
  prefs: []
  type: TYPE_NORMAL
- en: There's probably no need to see the new alerts in action. By now, you should
    trust the flow, and there's no reason to believe that they would not trigger.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/73c6acaf-55dd-42ec-a35c-ccfadef135cd.png)Figure 3-27: Prometheus''
    alerts screen'
  prefs: []
  type: TYPE_NORMAL
- en: In the "real world" scenario, receiving one of the two alerts might provoke
    different reactions depending on the Kubernetes flavor we're using.
  prefs: []
  type: TYPE_NORMAL
- en: If we do have Cluster Autoscaler (CA), we might not need `NotEnoughCPU` and
    `TooMuchCPURequested` alerts. The fact that ninety percent of node CPUs is in
    use does not prevent the cluster from operating correctly, just as long as our
    CPU requests as set correctly. Similarly, having ninety percent of allocatable
    CPU reserved is also not an issue. If Kubernetes cannot schedule a new Pod due
    to all CPU being reserved, it will scale up the cluster. As a matter of fact,
    reaching almost full CPU usage or having nearly all allocatable CPU reserved is
    a good thing. That means that we are having as much CPU as we need and that we
    are not paying for unused resources. Still, that logic works mostly with Cloud
    providers and not even all of them. Today (October 2018), Cluster Autoscaler works
    only in AWS, GCE, and Azure.
  prefs: []
  type: TYPE_NORMAL
- en: All that does not mean that we should rely only on Cluster Autoscaler. It can
    malfunction, like anything else. However, since CA is based on watching for unschedulable
    Pods, if it does fail to work, we should detect that through observing Pod statuses,
    not CPU usage. Still, it might not be a bad idea to receive alerts when CPU usage
    is too high, but in that case, we might want to increase the threshold to a value
    closer to a hundred percent.
  prefs: []
  type: TYPE_NORMAL
- en: If our cluster is on-prem or, to be more precise, if it does not have Cluster
    Autoscaler, the alerts we explored are essential if our process for scaling up
    the cluster is not automated or if it's slow. The logic is simple. If we need
    more than a couple of minutes to add new nodes to the cluster, we cannot wait
    until Pods are unschedulable. That would be too late. Instead, we need to know
    that we are out of available capacity before the cluster becomes full (saturated)
    so that we have enough time to react by adding new nodes to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Still, having a cluster that does not auto-scale because Cluster Autoscaler
    does not work is not an excuse good enough. There are plenty of other tools that
    we can use to automate our infrastructure. When we do manage to get to such a
    place that we can automatically add new nodes to the cluster, the destination
    of the alert should change. Instead of receiving notifications to Slack, we might
    want to send a request to a service that will execute the script that will result
    in a new node being added to the cluster. If our cluster is running on VMs, we
    can always add more through a script (or some tool).
  prefs: []
  type: TYPE_NORMAL
- en: The only real excuse to receive those notifications to Slack is if our cluster
    is running on bare-metal. In such a case, we cannot expect scripts to create new
    servers magically. For everyone else, Slack notifications when too much CPU is
    used or all allocated CPU is reserved should be only a temporary solution until
    proper automation is in place.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's try to accomplish similar goals but, this time, by measuring memory
    usage and reservations.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring memory consumption is similar to CPU, and yet there are a few differences
    that we should take into account. But, before we get there, let's go back to the
    Prometheus' graph screen and explore our first memory-related metric.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Just as with CPU, first we need to find out how much memory each of our nodes
    has.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, press the Execute button, and switch
    to the *Graph* tab.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Your result is likely to be different than mine. In my case, each node has around
    4 GB of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing how much RAM each node has is of no use without knowing how much RAM
    is currently available. We can get that info through the `node_memory_MemAvailable_bytes`
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: We can see the available memory on each of the nodes of the cluster. In my case
    (screenshot following), each has around 3 GB of available RAM.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/67a644d2-43b6-4eb3-902f-56c07fffd073.png)Figure 3-28: Prometheus''
    graph screen with available memory in each of the nodes of the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to get total and available memory from each of the nodes,
    we should combine the queries to get the percentage of the used memory of the
    whole cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: Since we are searching for the percentage of used memory, and we have the metric
    with available memory, we started the expression with `1 -` that will invert the
    result. The rest of the expression is a simple division of available and total
    memory. In my case (screenshot following), less than thirty percent of memory
    is used on each of the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9144a5c6-047c-4b6d-b6cf-d3e1e63235a4.png)Figure 3-29: Prometheus''
    graph screen with the percentage of available memory'
  prefs: []
  type: TYPE_NORMAL
- en: Just as with CPU, available and total memory does not paint the whole picture.
    While that is useful information and a base for a potential alert, we also need
    to know how much memory is allocatable and how much of it is in use by Pods. We
    can get the first figure through the `kube_node_status_allocatable_memory_bytes`
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: Depending on the Kubernetes flavor and the hosting provider you're using, there
    might be a very small, or a big discrepancy between the total and allocatable
    memory. I am running the cluster in AKS, and allocatable memory is a whole GB
    less than total memory. While the former is around 3 GB RAM, the latter was approximately
    4 GB RAM. That's a big difference. I do not have full 4 GB for my Pods, but around
    one quarter less than that. The rest, around 1 GB RAM, is spent on system-level
    services. To make things worse, that's 1 GB RAM spent on each node which, in my
    case, results in 3 GB less in total since my cluster has three nodes. Given such
    a huge difference between the total and the allocatable amount of RAM, there is
    a clear benefit for having less number of bigger nodes. Still, not everyone needs
    big nodes and reducing their number to less than three might not be a good idea
    if we'd like to have our nodes spread in all the zones.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to retrieve the amount of allocatable memory, let's see
    how to get the amount of requested memory for each of the applications.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: We can see that Prometheus (server) has the most requested memory (500 MB),
    with all the others being way below. Bear in mind that we are seeing only the
    Pods that do have reservations. Those without are not present in the results of
    that query. As you already know, it is OK not to define reservations and limits
    only in exceptional cases like, for example, for short-lived Pods used in CI/CD
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6968bff2-2962-4746-946f-61dbd8f5dd69.png)Figure 3-30: Prometheus''
    graph screen with requested memory for each of the Pods'
  prefs: []
  type: TYPE_NORMAL
- en: The previous expression returned the amount of memory used by each Pod. However,
    our mission is to discover how much requested memory we have in the system as
    a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: In my case, the total amount of requested memory is around 1.6 GB RAM.
  prefs: []
  type: TYPE_NORMAL
- en: All that's left is to divide the total requested memory with the amount of all
    the allocatable memory in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: In my case (screenshot following), the total of the requested memory is around
    twenty percent (`0.2`) of the cluster's allocatable RAM. I am far from being in
    any type of danger, nor there is a need to scale up the cluster. If anything,
    I have too much unused memory and might want to scale down. However, we are at
    the moment only concerned with scaling up. Later we'll explore alerts that might
    result in scaling down.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f2bbaa51-31e9-4115-80a1-7994bc281168.png)Figure 3-31: Prometheus''
    graph screen with the percentage of the requested memory of the total allocatable
    memory in the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the differences between the old Chart's values and those
    we are about to use.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: We added two new alerts (`NotEnoughMemory` and `TooMuchMemoryRequested`). The
    definitions themselves should be straightforward since we already created quite
    a few alerts. The expressions are the same as the ones we used in Prometheus graph
    screen, with the addition of the greater than ninety percent (`> 0.9`) threshold.
    So, we'll skip further explanation.
  prefs: []
  type: TYPE_NORMAL
- en: We'll upgrade our Prometheus Chart with the new values, and open the alerts
    screen to confirm that they
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: If the alerts `NotEnoughMemory` and `TooMuchMemoryRequested` are not yet available,
    please wait a few moments, and refresh the screen.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c87a9f02-ac71-489f-907e-bd38b2233064.png)Figure 3-32: Prometheus''
    alerts screen'
  prefs: []
  type: TYPE_NORMAL
- en: The actions based on the memory-based alerts we created so far should be similar
    to those we discussed with CPU. We can use them to decide whether and when to
    scale up our cluster, either through manual actions or through automated scripts.
    Just as before, if we do have our cluster hosted with one of the vendors supported
    by the Cluster Autoscaler (CA), those alerts should be purely informative, while
    on-prem or with unsupported Cloud providers, they are much more than simple notifications.
    They are an indication that we are about to run out of capacity, at least when
    memory is concerned.
  prefs: []
  type: TYPE_NORMAL
- en: The CPU and memory examples are all focused on the need to know when is the
    right time to scale our cluster. We might create similar alerts that would notify
    us when the usage of CPU or memory is too low. That would give us a clear indication
    that we have too many nodes in the cluster and that we might want to remove some.
    That, again, assumes that we do not have Cluster Autoscaler up-and-running. Still,
    taking only CPU or only memory into account for scaling-down is too risky and
    can lead to unexpected results.
  prefs: []
  type: TYPE_NORMAL
- en: Let's imagine that only twelve percent of allocatable CPU is reserved and that
    we have three worker nodes in the cluster. Such a low CPU usage surely does not
    warrant that many nodes since on average, each has a relatively small amount of
    reserved CPU. As a result, we can choose to scale down, and we remove one of the
    nodes thus allowing other clusters to reuse it. Was that a good thing to do? Well,
    it depends on other resources. If the percentage of memory reservations was low
    as well, removing a node was a good idea. On the other hand, if the reserved memory
    were over sixty-six percent, removal of a node would result in insufficient resources.
    When we removed one of the three nodes, over sixty-six percent of reserved memory
    across three nodes becomes over one hundred percent on two nodes.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, if we are to receive notifications that our cluster is in need to
    scale down (and we do NOT have Cluster Autoscaler), we need to combine memory
    and CPU, and probably a few other metrics as alert thresholds. Fortunately, the
    expressions are very similar to those we used before. We just need to combine
    them into a single alert and change the thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, the expressions we used before are as follows (there's no need
    to re-run them).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's compare yet another update of the Chart's values with those we're
    using right now.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: We're adding a new alert called `TooMuchCPUAndMemory`. It is a combination of
    the previous two alerts. It will fire only if both CPU and memory usage are below
    fifty percent. That way we'll avoid sending false positives and we will not be
    tempted to de-scale the cluster only because one of the resource reservations
    (CPU or memory) is too low, while the other might be high.
  prefs: []
  type: TYPE_NORMAL
- en: All that's left, before we move into the next subject (or metric type), is to
    upgrade Prometheus' Chart and confirm that the new alert is indeed operational.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: Please refresh the alerts screen if the alert is still not present. In my case
    (screenshot following), the total of reserved memory and CPU is below fifty percent,
    and the alert is in the pending state. In your case, that might not be true, and
    the alert might not have reached its thresholds. Nevertheless, I'll continue explaining
    my case, where both CPU and memory usage is less than fifty percent of total available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thirty minutes later (`for: 30m`), the alert fired. It waited for a while (`30m`)
    to confirm that the drop in memory and CPU usage is not temporary. Given that
    I''m running my cluster in AKS, Cluster Autoscaler would remove one of the nodes
    long before thirty minutes. But, since it is configured to operate with a minimum
    of three nodes, CA will not perform that action. As a result, I might need to
    reconsider whether paying for three nodes is a worthwhile investment. If, on the
    other hand, my cluster would be without Cluster Autoscaler, and assuming that
    I do not want to waste resources while other clusters might need more, I would
    need to remove one of the nodes (manually or automatically). If that removal were
    automatic, the destination would not be Slack, but the API of the tool in charge
    of removing nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/88738e53-69bf-4683-9ca4-c8fdf35a59b8.png)Figure 3-33: Prometheus''
    alerts screen with one alert in the pending state'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a few examples of saturation, we covered each of the metrics
    championed by Google Site Reliability Engineers and almost any other monitoring
    method. Still, we're not done. There are a few other metrics and alerts I'd like
    to explore. They might not fall into any of the discussed categories, yet they
    might prove to be very useful.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting on unschedulable or failed pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knowing whether our applications are having trouble to respond fast to requests,
    whether they are being bombed with more requests than they could handle, whether
    they produce too many errors, and whether they are saturated, is of no use if
    they are not even running. Even if our alerts detect that something is wrong by
    notifying us that there are too many errors or that response times are slow due
    to an insufficient number of replicas, we should still be informed if, for example,
    one, or even all replicas failed to run. In the best case scenario, such a notification
    would provide additional info about the cause of an issue. In the much worse situation,
    we might find out that one of the replicas of the DB is not running. That would
    not necessarily slow it down nor it would produce any errors but would put us
    in a situation where data could not be replicated (additional replicas are not
    running) and we might face a total loss of its state if the last standing replica
    fails as well.
  prefs: []
  type: TYPE_NORMAL
- en: There are many reasons why an application would fail to run. There might not
    be enough unreserved resources in the cluster. Cluster Autoscaler will deal with
    that problem if we have it. But, there are many other potential issues. Maybe
    the image of the new release is not available in the registry. Or, perhaps the
    Pods are requesting PersistentVolumes that cannot be claimed. As you might have
    guessed, the list of the things that might cause our Pods to fail, be unschedulable,
    or in an unknown state, is almost infinite.
  prefs: []
  type: TYPE_NORMAL
- en: We cannot address all of the causes of problems with Pods individually. However,
    we can be notified if the phase of one or more Pods is `Failed`, `Unknown`, or
    `Pending`. Over time, we might extend our self-healing scripts to address some
    of the specific causes of those statuses. For now, our best first step is to be
    notified if a Pod is in one of those phases for a prolonged period of time (for
    example, fifteen minutes). Alerting as soon as the status of a Pod indicates a
    problem would be silly because that would generate too many false positives. We
    should get an alert and choose how to act only after waiting for a while, thus
    giving Kubernetes time to fix an issue. We should perform some reactive actions
    only if Kubernetes fails to remedy the situation.
  prefs: []
  type: TYPE_NORMAL
- en: Over time, we'll notice some patterns in the alerts we're receiving. When we
    do, alerts should be converted into automated responses that will remedy selected
    issues without our involvement. We already explored some of the low hanging fruits
    through HorizontalPodAutoscaler and Cluster Autoscaler. For now, we'll focus on
    receiving alerts for all other cases, and failed and unschedulable Pods are a
    few of those. Later on, we might explore how to automate responses. But, that
    moment is not now, so we'll move forward with yet another alert that will result
    in a notification to Slack.
  prefs: []
  type: TYPE_NORMAL
- en: Let's open Prometheus' graph screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: Please type the expression that follows and click the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: The output shows us each of the Pods in the cluster. If you take a closer look,
    you'll notice that there are five results for each Pod, one for each of the five
    possible phases. If you focus on the `phase` field, you'll see that there is an
    entry for `Failed`, `Pending`, `Running`, `Succeeded`, and `Unknown`. So, each
    Pod has five results, but only one has the value `1`, while the values of the
    other four are all set to `0`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5f93fbd0-1d80-4b28-9ee1-fd26930ac478.png)Figure 3-34: Prometheus''
    console view with the phases of the Pods'
  prefs: []
  type: TYPE_NORMAL
- en: For now, our interest lies primarily with alerts, and they should, in most cases,
    be generic and not related to a specific node, an application, a replica, or some
    other type of resources. Only when we are alerted that there is an issue, should
    we start digging deeper and look for more granular data. With that in mind, we'll
    rewrite our expression to retrieve the number of Pods in each of the phases.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows and click the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: The output should show that all the Pods are in the `Running` phase. In my case,
    there are twenty-seven running Pods and none in any of the other phases.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we should not really care about healthy Pods. They are running, and there's
    nothing we should do about that. Instead, we should focus on those that are problematic.
    So, we might just as well rewrite the previous expression to retrieve the sum
    only of those that are in the `Failed`, `Unknown`, or `Pending` phase.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows and click the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: As expected, unless you messed up something, the values of the output are all
    set to `0`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d0db1323-5bea-4eb6-bdf0-4a1717233f5e.png)Figure 3-35: Prometheus''
    console view with the sum of the Pods in Failed, Unknown, or Pending phases'
  prefs: []
  type: TYPE_NORMAL
- en: So far, there are no Pods we should worry about. We'll change that by creating
    one that will intentionally fail, by using an image that apparently does not exist.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: As we can see from the output, the `pod/problem` was `created`. If we created
    it through a script (for example, CI/CD pipeline), we'd think that everything
    is OK. Even if we'd follow it with `kubectl rollout status`, we would only ensure
    that it started working, not that it continues working.
  prefs: []
  type: TYPE_NORMAL
- en: But, since we did not create that Pod through a CI/CD pipeline, but manually,
    we can just as well list all the Pods in the `default` Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: We'll imagine that we have only short-term memory and that we already forgot
    that the `image` is set to `i-do-not-exist`. What can be the issue? Well, the
    first step would be to describe the Pod.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the messages of the `Events` section, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: The problem is clearly manifested through the `Back-off pulling image "i-do-not-exist"`
    message. Further down, we can see the message from the container server stating
    that `it failed to pull image "i-do-not-exist"`.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we knew in advance that will be the result, but something similar
    could happen without us noticing that there is an issue. The cause could be a
    failure to pull the image, or one of the myriads of others. Nevertheless, we are
    not supposed to sit in front of a terminal, listing and describing Pods and other
    types of resources. Instead, we should receive an alert that Kubernetes failed
    to run a Pod, and only after that, we should start digging for the cause of the
    issue. So, let's create one more alert that will notify us when Pods fail and
    do not recuperate.
  prefs: []
  type: TYPE_NORMAL
- en: Like many times before, we'll take a look at the differences between the old
    and the new definition of Prometheus' Chart values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: We defined a new group of alerts called `pod`. Inside it, we have an `alert`
    named `ProblematicPods` that will fire if there are one or more Pods with the
    `Failed`, `Unknown`, or `Pending` phase for more than one minute (`1m`). I intentionally
    set it to very low `for` duration so that we can test it easily. Later on, we'll
    switch to fifteen minutes interval that will be more than enough to allow Kubernetes
    to remedy the issue before we get a notification that will send us into the panic
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: Let's update Prometheus' Chart with the updated values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: Since we did not yet fix the issue with the `problem` Pod, we should see a new
    notification in Slack soon. Let's confirm that.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: If your notification did not yet arrive, please wait for a few moments.
  prefs: []
  type: TYPE_NORMAL
- en: We got the message stating that `at least one Pod could not run`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a4035775-47d5-4dbe-a3bd-8142a286be0b.png)Figure 3-36: Slack with
    an alert message'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we received a notification that there is a problem with one of the
    Pods, we should go to Prometheus, dig through data until we find the cause of
    the issue, and fix it. But, since we already know what the problem is (we created
    it intentionally), we'll skip all that, and remove the faulty Pod, before we move
    onto the next subject.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: Upgrading old Pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our primary goal should be to prevent issues from happening by being proactive.
    In cases when we cannot predict that a problem is about to materialize, we must,
    at least, be quick with our reactive actions that mitigate the issues after they
    occur. Still, there is a third category that can only loosely be characterized
    as being proactive. We should keep our system clean and up-to-date.
  prefs: []
  type: TYPE_NORMAL
- en: Among many things we could do to keep the system up-to-date, is to make sure
    that our software is relatively recent (patched, updated, and so on). A reasonable
    rule could be to try to renew software after ninety days, if not earlier. That
    does not mean that everything we run in our cluster should be newer than ninety
    days, but that it might be a good starting point. Further on, we might create
    finer policies that would allow some kinds of applications (usually third-party)
    to live up to, let's say, half a year without being upgraded. Others, especially
    software we're actively developing, will probably be upgraded much more frequently.
    Nevertheless, our starting point is to detect all the applications that were not
    upgraded in ninety days or more.
  prefs: []
  type: TYPE_NORMAL
- en: Just as in almost all other exercises in this chapter, we'll start by opening
    Prometheus' graph screen, and explore the metrics that might help us reach our
    goal.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: If we inspect the available metrics, we'll see that there is `kube_pod_start_time`.
    Its name provides a clear indication of its purpose. It provides the Unix timestamp
    that represents the start time of each Pod in the form of a Gauge. Let's see it
    in action.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows and click the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: Those values alone are of no use, and there's no point in teaching you how to
    calculate the human date from those values. What matters, is the difference between
    now and those timestamps.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d9204c74-681c-4097-8ce8-b76c8b78b6ab.png)Figure 3-37: Prometheus''
    console view with the start time of the Pods'
  prefs: []
  type: TYPE_NORMAL
- en: We can use Prometheus' `time()` function to return the number of seconds since
    January 1, 1970 UTC (or Unix timestamp).
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows and click the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: Just as with the `kube_pod_start_time`, we got a long number that represents
    seconds since 1970\. The only noticeable difference, besides the value, is that
    there is only one entry, while with `kube_pod_start_time` we got a result for
    each Pod in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's combine the two metrics in an attempt to retrieve the age of each
    of the Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows and click the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: The results are this time much smaller numbers representing the seconds between
    now and creation of each of the Pods. In my case (screenshot following), the first
    Pod (one of the `go-demo-5` replicas), is slightly over six thousand seconds old.
    That would be around a hundred minutes (6096 / 60), or less than two hours (100
    min / 60 min = 1.666 h).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d38868b1-8abb-4ad3-aab7-915d496b32bc.png)Figure 3-38: Prometheus''
    console view with the time passed since the creation of the Pods'
  prefs: []
  type: TYPE_NORMAL
- en: Since there are probably no Pods older than our target of ninety days, we'll
    lower it temporarily to a minute (sixty seconds).
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows and click the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: In my case, all the Pods are older than a minute (as are probably yours as well).
    We confirmed that it works so we can increase the threshold to ninety days. To
    get to ninety days, we should multiply the threshold with sixty to get minutes,
    with another sixty to get hours, with twenty-four to get days, and, finally, with
    ninety. The formula would be `60 * 60 * 24 * 90`. We could use the final value
    of `7776000`, but that would make the query harder to decipher. I prefer using
    the formula instead.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows and click the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: It should come as no surprise that there are (probably) no results. If you created
    a new cluster for this chapter, you'd need to be the slowest reader on earth if
    it took you ninety days to get here. This might be the longest chapter I've written
    so far, but it's still not worth ninety days of reading.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know which expression to use, we can add one more alert to our setup.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the difference between the old and the new values is in the
    `OldPods` alert. It contains the same expression we used a few moments ago.
  prefs: []
  type: TYPE_NORMAL
- en: We kept the low threshold of `60` seconds so that we can see the alert in action.
    Later on, we'll increase that value to ninety days.
  prefs: []
  type: TYPE_NORMAL
- en: There was no need to specify `for` duration. The alert should fire the moment
    the age of one of the Pods reaches three months (give or take).
  prefs: []
  type: TYPE_NORMAL
- en: Let's upgrade our Prometheus' Chart with the updated values and open the Slack
    channel where we should see the new message.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: All that's left is to wait for a few moments until the new message arrives.
    It should contain the title *Old Pods* and the text stating that *At least one
    Pod has not been updated to more than 90 days*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/83874a98-4079-4787-9dce-83f28f3c7db4.png)Figure 3-39: Slack with
    multiple fired and resolved alert messages'
  prefs: []
  type: TYPE_NORMAL
- en: Such a generic alert might not work for all your use-cases. But, I'm sure that
    you'll be able to split it into multiple alerts based on Namespaces, names, or
    something similar.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a mechanism to receive notifications when our Pods are too
    old and might require upgrades, we'll jump into the next topic and explore how
    to retrieve memory and CPU used by our containers.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring containers memory and CPU usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are familiar with Kubernetes, you understand the importance of defining
    resource requests and limits. Since we already explored `kubectl top pods` command,
    you might have set the requested resources to match the current usage, and you
    might have defined the limits as being above the requests. That approach might
    work on the first day. But, with time, those numbers will change and we will not
    be able to get the full picture through `kubectl top pods`. We need to know how
    much memory and CPU containers use when on their peak loads, and how much when
    they are under less stress. We should observe those metrics over time, and adjust
    periodically.
  prefs: []
  type: TYPE_NORMAL
- en: Even if we do somehow manage to guess how much memory and CPU a container needs,
    those numbers might change from one release to another. Maybe we introduced a
    feature that requires more memory or CPU?
  prefs: []
  type: TYPE_NORMAL
- en: What we need is to observe resource usage over time and to make sure that it
    does not change with new releases or with increased (or decreased) number of users.
    For now, we'll focus on the former case and explore how to see how much memory
    and CPU our containers used over time.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we'll start by opening the Prometheus' graph screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: We can retrieve container memory usage through the `container_memory_usage_bytes`.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, press the Execute button, and switch
    to the *Graph* screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: If you take a closer look at the top usage, you'll probably end up confused.
    It seems that some containers are using way more than the expected amount of memory.
  prefs: []
  type: TYPE_NORMAL
- en: The truth is that some of the `container_memory_usage_bytes` records contain
    cumulative values, and we should exclude them so that only memory usage of individual
    containers is retrieved. We can do that by retrieving only the records that have
    a value in the `container_name` field.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: Now the result makes much more sense. It reflects memory usage of the containers
    running inside our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We'll get to alerts based on container resources a bit later. For now, we'll
    imagine that we'd like to check memory usage of a specific container (for example,
    `prometheus-server`). Since we already know that one of the available labels is
    `container_name`, retrieving the data we need should be straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: We can see the oscillations in memory usage of the container over the last hour.
    Normally, we'd be interested in a longer period like a day or a week. We can accomplish
    that by clicking - and + buttons above the graph, or by typing the value directly
    in the field between them (for example, `1w`). However, changing the duration
    might not help much since we haven't been running the cluster for too long. We
    might not be able to squeeze more data than a few hours unless you are a slow
    reader.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a6c281c9-3cdc-451b-88db-a3e4409d6e53.png)Figure 3-40: Prometheus''
    graph screen with container memory usage limited to prometheus-server'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we should be able to retrieve CPU usage of a container as well. In
    that case, the metric we're looking for could be `container_cpu_usage_seconds_total`.
    However, unlike `container_memory_usage_bytes` that is a gauge, `container_cpu_usage_seconds_total`
    is a counter, and we'll have to combine `sum` and `rate` to get the changes in
    values over time.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: The query shows summed CPU seconds rate over five minutes intervals. We added
    `by (pod_name)` to the mix so that we can distinguish different Pods and see when
    one was created, and the other was destroyed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3c443319-d0bf-42c8-bb32-fd7c0f423ed4.png)Figure 3-41: Prometheus''
    graph screen with the rate of container CPU usage limited to prometheus-server'
  prefs: []
  type: TYPE_NORMAL
- en: If that were a "real world" situation, our next step would be to compare actual
    resource usage with what we defined as Prometheus `resources`. If what we defined
    differs considerably compared to what it actually is, we should probably update
    our Pod definition (the `resources` section).
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that using "real" resource usage to define Kubernetes `resources`
    better will provide valid values only temporarily. Over time, our resource usage
    will change. The load might increase, new features might be more resource-hungry,
    and so on. No matter the reasons, the critical thing to note is that everything
    is dynamic and that there is no reason to think otherwise for resources. In that
    spirit, our next challenge is to figure out how to get a notification when the
    actual resource usage differs too much from what we defined in container `resources`.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing actual resource usage with defined requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we define container `resources` inside a Pod and without relying on actual
    usage, we are just guessing how much memory and CPU we expect a container to use.
    I'm sure that you already know why guessing, in the software industry, is a terrible
    idea, so I'll focus on Kubernetes aspects only.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes treats Pods with containers that do not have specified resources
    as the **BestEffort Quality of Service** (**QoS**). As a result, if it ever runs
    out of memory or CPU to serve all the Pods, those are the first to be forcefully
    removed to leave space for others. If such Pods are short lived as, for example,
    those used as one-shot agents for continuous delivery processes, BestEffort QoS
    is not a bad idea. But, when our applications are long-lived, BestEffort QoS should
    be unacceptable. That means that in most cases, we do have to define container
    `resources`.
  prefs: []
  type: TYPE_NORMAL
- en: If container `resources` are (almost always) a must, we need to know which values
    to put. I often see teams that merely guess. "It's a database; therefore it needs
    a lot of RAM" and "it's only an API, it shouldn't need much" are only a few of
    the sentences I hear a lot. Those guesstimates are often the result of not being
    able to measure actual usage. When something would blow up, those teams would
    just double the allocated memory and CPU. Problem solved!
  prefs: []
  type: TYPE_NORMAL
- en: I never understood why would anyone invent how much memory and CPU an application
    needs. Even without any "fancy" tools, we always had `top` command in Linux. We
    could know how much our application uses. Over time, better tools were developed,
    and all we had to do is Google "how to measure memory and CPU of my applications."
  prefs: []
  type: TYPE_NORMAL
- en: You already saw `kubectl top pods` in action when you need current data, and
    you are becoming familiar with the power of Prometheus to give you much more.
    You do not have an excuse to guesstimate.
  prefs: []
  type: TYPE_NORMAL
- en: But, why do we care about resource usage compared with requested resources?
    Besides the fact that might reveal a potential problem (for example, memory leak),
    inaccurate resource requests and limits prevent Kubernetes from doing its job
    efficiently. If, for example, we define memory request to 1 GB RAM, that's how
    much Kubernetes will remove from allocatable memory. If a node has 2 GB of allocatable
    RAM, only two such containers could run there, even if each uses only 50 MB RAM.
    Our nodes would use only a fraction of allocatable memory and, if we have Cluster
    Autoscaler, new nodes would be added even if the old ones still have plenty of
    unused memory.
  prefs: []
  type: TYPE_NORMAL
- en: Even though now we know how to get actual memory usage, it would be a waste
    of time to start every day by comparing YAML files with the results in Prometheus.
    Instead, we'll create yet another alert that will send us a notification whenever
    the requested memory and CPU differs too much from the actual usage. That's our
    next mission.
  prefs: []
  type: TYPE_NORMAL
- en: First, we'll reopen Prometheus' graph screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: We already know how to get memory usage through `container_memory_usage_bytes`,
    so we'll jump straight into retrieving requested memory. If we can combine the
    two, we'll get the discrepancy between the requested and the actual memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: The metric we're looking for is `kube_pod_container_resource_requests_memory_bytes`,
    so let's take it for a spin with, let's say, `prometheus-server` Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, press the Execute button, and switch
    to the *Graph* tab.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: We can see from the result that we requested 500 MB RAM for the `prometheus-server`
    container.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c2002dc2-0574-48b0-93a5-22d701f67efa.png)Figure 3-42: Prometheus''
    graph screen with container requested memory limited to prometheus-server'
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that the `kube_pod_container_resource_requests_memory_bytes`
    metric has, among others, `pod` label while, on the other hand, `container_memory_usage_bytes`
    uses `pod_name`. If we are to combine the two, we need to transform the label
    `pod` into `pod_name`. Fortunately, this is not the first time we're facing that
    problem, and we already know that the solution is to use the `label_join` function
    that will create a new label based on one or more of the existing labels.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: This time, not only that we added a new label to the metric, but we also grouped
    the results by that very same label (`by (pod)`).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ca12f3cb-0be1-487f-93c4-626630e0b2b0.png)Figure 3-43: Prometheus''
    graph screen with container memory usage limited to prometheus-server and grouped
    by the pod label extracted from pod_name'
  prefs: []
  type: TYPE_NORMAL
- en: Now we can combine the two metrics and find out the discrepancy between the
    requested and the actual memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: In my case (screenshot following), the discrepancy was becoming gradually smaller.
    It started somewhere around sixty percent, and now it's approximately seventy-five
    percent. Such a difference is not big enough for us to take any corrective action.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2f253a04-5003-490d-8be4-ed23bc20e24f.png)Figure 3-44: Prometheus''
    graph screen with the percentage of container memory usage based on requested
    memory'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we saw how to get the difference between reserved and actual memory
    usage for a single container, we should probably make the expression more general
    and get all the containers in the cluster. However, all might be a bit too much.
    We probably do not want to mess with the Pods running in the `kube-system` Namespace.
    They are likely pre-installed in the cluster, and we might want to leave them
    as they are, at least for now. So, we'll exclude them from the query.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: The result should be the list of percentages of a difference between requested
    and actual memory, with the Pods in the `kube-system` excluded.
  prefs: []
  type: TYPE_NORMAL
- en: In my case, there are quite a few containers that use quite a lot more memory
    than what we requested. The main culprit is `prometheus-alertmanager` which uses
    more than three times more memory than what we requested. That can be due to several
    reasons. Maybe we requested too little memory, or perhaps it contains containers
    that do not have `requests` specified. In either case, we should probably redefine
    requests not only for the Alertmanager but also for all the other Pods that use
    more than, let's say 50% more memory than requested.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9f028f63-5a85-46be-8f15-2727d4f2c585.png)Figure 3-45: Prometheus''
    graph screen with the percentage of container memory usage based on requested
    memory and with those from the kube-system Namespace excluded'
  prefs: []
  type: TYPE_NORMAL
- en: We are about to define a new alert that will deal with cases when requested
    memory is much more or much less than the actual usage. But, before we do that,
    we should discuss the conditions we should use. One alert could fire when actual
    memory usage is over 150% of the requested memory for over an hour. That would
    remove false positives caused by a temporary spike in memory usage (that's why
    we have `limits` as well). The other alert could deal with the situation when
    memory usage is more than 50% below the requested amount. But, in case of that
    alert, we might add another condition.
  prefs: []
  type: TYPE_NORMAL
- en: Some applications are too small, and we might never be able to fine-tune their
    requests. We can exclude those cases by adding another condition that will ignore
    the Pods with only 5 MB reserved RAM, or less.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this alert might not need to fire as frequently as the previous one.
    We should know relatively quickly if our application uses more memory than we
    intended to give since that can be a sign of a memory leak, significantly increased
    traffic, or some other, potentially dangerous situation. But, if memory uses much
    less than intended, the issue is not as critical. We should correct it, but there
    is no need to act urgently. Therefore, we'll set the duration of the latter alert
    to six hours.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we set a few rules we should follow, we can take a look at yet another
    differences between the old and the new set of Chart's values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: First, we set the threshold of the `OldPods` alert back to its intended value
    of ninety days (`60 * 60 * 24 * 90`). That way we'll stop it from firing alerts
    only for test purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we defined a new alert called `ReservedMemTooLow`. It will fire if used
    memory is more than `1.5` times bigger than the requested memory. The duration
    for the pending state of the alert is set to `1m`, only so that we can see the
    outcome without waiting for the full hour. Later on, we'll restore it back to
    `1h`.
  prefs: []
  type: TYPE_NORMAL
- en: The `ReservedMemTooHigh` alert is (partly) similar to the previous one, except
    that it has the condition that will cause the alert to fire if the difference
    between the actual and the requested memory is less than `0.5` and if that continues
    being the case for over `6m` (we'll change it later to `6h`). The second part
    of the expression is new. It requires that all the containers in a Pod have more
    than 5 MB of the requested memory (`5.25e+06`). Through that second statement
    (separated with `and`), we're saving ourselves from dealing with too small applications.
    If it requires less than 5 MB RAM, we should ignore it and, probably, congratulate
    the team behind it for making it that efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's upgrade our Prometheus' Chart with the updated values and open the
    graph screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: We won't wait until the alerts start firing. Instead, we'll try to accomplish
    similar objectives, but with CPU.
  prefs: []
  type: TYPE_NORMAL
- en: There's probably no need to go through the process of explaining the expressions
    we'll use. We'll jump straight into the CPU-based alerts by exploring the difference
    between the old and the new set of Chart's values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: The first two sets of differences are defining more sensible thresholds for
    `ReservedMemTooLow` and `ReservedMemTooHigh` alerts we explored previously. Further
    down, we can see the two new alerts.
  prefs: []
  type: TYPE_NORMAL
- en: The `ReservedCPUTooLow` alert will fire if CPU usage is more than 1.5 times
    bigger than requested. Similarly, the `ReservedCPUTooHigh` alert will fire only
    if CPU usage is less than half of the requested and if we requested more than
    5 CPU milliseconds. Getting notifications because 5 MB RAM is too much would be
    a waste of time.
  prefs: []
  type: TYPE_NORMAL
- en: Both alerts are set to fire if the issues persist for a short period (`1m` and
    `6m`) so that we can see them in action without having to wait for too long.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's upgrade our Prometheus' Chart with the updated values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: I'll leave it to you to check whether any of the alerts fire and whether they
    are forwarded from Alertmanager to Slack. You should know how to do that by now.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll move to the last alert in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing actual resource usage with defined limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knowing when a container uses too much or too few resources compared to requests
    helps us be more precise with resource definitions and, ultimately, help Kubernetes
    make better decisions where to schedule our Pods. In most cases, having too big
    of a discrepancy between requested and actual resource usage will not result in
    malfunctioning. Instead, it is more likely to result in an unbalanced distribution
    of Pods or in having more nodes than we need. Limits, on the other hand, are a
    different story.
  prefs: []
  type: TYPE_NORMAL
- en: If resource usage of our containers enveloped as Pods reaches the specified
    `limits`, Kubernetes might kill those containers if there's not enough memory
    for all. It does that as a way to protect the integrity of the rest of the system.
    Killed Pods are not a permanent problem since Kubernetes will almost immediately
    reschedule them if there is enough capacity.
  prefs: []
  type: TYPE_NORMAL
- en: If we do use Cluster Autoscaling, even if there isn't enough capacity, new nodes
    will be added as soon as it detects that some Pods are in the pending state (unschedulable).
    So, the world is not likely to end if resource usage goes over the limits.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, killing and rescheduling Pods can result in downtime. There are,
    apparently, worse scenarios that might happen. But we won't go into them. Instead,
    we'll assume that we should be aware that a Pod is about to reach its limits and
    that we might want to investigate what's going on and we that might need to take
    some corrective measures. Maybe the latest release introduced a memory leak? Or
    perhaps the load increased beyond what we expected and tested and that results
    in increased memory usage. The cause of using memory that is close to the limit
    is not the focus right now. Detecting that we are reaching the limit is.
  prefs: []
  type: TYPE_NORMAL
- en: First, we'll go back to Prometheus' graph screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: We already know that we can get actual memory usage through the `container_memory_usage_bytes`
    metric. Since we already explored how to get requested memory, we can guess that
    limits are similar. They indeed are, and they can be retrieved through the `kube_pod_container_resource_limits_memory_bytes`.
    Since one of the metrics is the same as before, and the other is very similar,
    we'll jump straight into executing the full query.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, press the Execute button, and switch
    to the *Graph* tab.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: In my case (screenshot following), we can see that quite a few Pods use more
    memory than what is defined as their limits.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, I do have spare capacity in my cluster, and there is no imminent
    need for Kubernetes to kill any of the Pods. Moreover, the issue might not be
    in Pods using more than what is set as their limits, but that not all containers
    in those Pods have the limits set. In either case, I should probably update the
    definition of those Pods/containers and make sure that their limits are above
    their average usage over a few days or even weeks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c57b7c9d-4904-4167-b320-ac2a31877b8a.png)Figure 3-46: Prometheus''
    graph screen with the percentage of container memory usage based on memory limits
    and with those from the kube-system Namespace excluded'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll go through the drill of exploring the difference between the old
    and the new version of the values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: Apart from restoring sensible thresholds for the alerts we used before, we defined
    a new alert called `MemoryAtTheLimit`. It will fire if the actual usage is over
    eighty percent (`0.8`) of the limit for more than one hour (`1h`).
  prefs: []
  type: TYPE_NORMAL
- en: Next is the upgrade of our Prometheus Chart.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can open the Prometheus' alerts screen and confirm that the new
    alert was indeed added to the mix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: We won't go through the drill of creating a similar alert for CPU. You should
    know how to do that yourself.
  prefs: []
  type: TYPE_NORMAL
- en: What now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We explored quite a few Prometheus metrics, expressions, and alerts. We saw
    how to connect Prometheus alerts with Alertmanager and, from there, to forward
    them to one application to another.
  prefs: []
  type: TYPE_NORMAL
- en: What we did so far is only the tip of the iceberg. If would take too much time
    (and space) to explore all the metrics and expressions we might use. Nevertheless,
    I believe that now you know some of the more useful ones and that you'll be able
    to extend them with those specific to you.
  prefs: []
  type: TYPE_NORMAL
- en: I urge you to send me expressions and alerts you find useful. You know where
    to find me (*DevOps20* ([http://slack.devops20toolkit.com/](http://slack.devops20toolkit.com/))
    Slack, `viktor@farcic` email, `@vfarcic` on Twitter, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: For now, I'll leave you to decide whether to move straight into the next chapter,
    to destroy the entire cluster, or only to remove the resources we installed. If
    you choose the latter, please use the commands that follow.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: Before you leave, you might want to go over the main points of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus is a database (of sorts) designed to fetch (pull) and store highly
    dimensional time series data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The four key metrics everyone should utilize are latency, traffic, errors, and
    saturation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
