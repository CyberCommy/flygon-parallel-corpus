- en: Chapter 7. Operationalizing Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this last chapter, we will be exploring tools available for Kafka cluster
    administration and Kafka topic administration. Additionally, we will also be discussing
    in brief Kafka cluster mirroring and Kafka's integration with third-party tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main focus areas for this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Kafka administration tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka cluster mirroring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with other tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka administration tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of tools or utilities provided by Kafka 0.8.x to administrate
    features such as cluster management, topic tools, cluster mirroring, and so on.
    Let's have a quick look at these tools.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka cluster tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cluster management is one of the prime responsibilities of the Kafka administrator.
    Once the cluster is started successfully, it needs to be maintained for activities
    such as server shutdown, leader balancing, replication, cluster mirroring, and
    expanding Kafka clusters. Let's talk about these in detail.
  prefs: []
  type: TYPE_NORMAL
- en: As we have learned from Kafka's design, in replication multiple partitions can
    have replicated data, and out of these multiple replicas, one replica acts as
    a lead, and the rest of the replicas act as in-sync followers of the lead replica.
    In the event of non-availability of a lead replica, maybe due to broker shutdown,
    a new lead replica needs to be selected.
  prefs: []
  type: TYPE_NORMAL
- en: For scenarios such as shutting down the Kafka broker for maintenance activity,
    election of the new leader is done sequentially, and this causes significant read/write
    operations for Zookeeper. In any big cluster with many topics/partitions, sequential
    election of lead replicas causes delay in availability.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure high availability, Kafka provides tools for a controlled shutdown
    of Kafka brokers. If the broker has the lead partition shut down, this tool transfers
    the leadership proactively to other in-sync replicas on another broker. If there
    is no in-sync replica available, the tool will fail to shut down the broker in
    order to ensure no data is lost.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the format for using this tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The ZooKeeper host and the broker ID that need to be shut down are mandatory
    parameters. We can also specify optional parameters, the number of retries (`--num.retries,
    default value 0`) and the retry interval in milliseconds (`--retry.interval.ms,
    default value 1000`) with a controlled shutdown tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a server is stopped gracefully, it will sync all its logs automatically
    to disk to avoid any log recovery whenever it is restarted again, as log recovery
    is a time-consuming activity. Before shutting down, it also migrates the leader
    partitions on the server to other replicas. This ensures minimal downtime for
    each partition (up to a few milliseconds). Controlled shutdown of a server also
    needs to be enabled as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, in any big Kafka cluster with many brokers, topics, and partitions, Kafka
    ensures that the preferred/lead replicas for partitions are equally distributed
    among the brokers. However, if a shutdown (controlled as well) or broker failure
    happens, this equal distribution of lead replicas might get imbalanced within
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka provides a tool that is used to maintain a balanced distribution of lead
    replicas within the Kafka cluster across available brokers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the format for using this tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This tool updates the ZooKeeper path with the list of topic partitions whose
    leader needs to be moved to the preferred replica list. Once the list is updated,
    the controller retrieves the list of preferred topic partitions from ZooKeeper
    asynchronously and, for each topic partition, controller verifies whether the
    preferred replica is the leader. If controller finds that the preferred replica
    is not the leader and is not present in the ISR list, it raises a request to the
    broker to make the preferred replica the leader for the partition to create a
    balanced distribution. If the preferred replica is not in the ISR list, the controller
    fails the operation to avoid any data loss. For this tool, the list of topic partitions
    in a JSON file format can also be provided as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the format of the `topicPartitionList.json` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Adding servers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to add servers to a Kafka cluster, a unique broker ID needs to be assigned
    to the new server to set up/start Kafka on the new servers. This way of adding
    a new server does not automatically assign any data partitions. Hence, a newly
    added server will not perform any work unless existing partitions are migrated
    to the server or new topics are created.
  prefs: []
  type: TYPE_NORMAL
- en: 'The migration process for existing partitions is initiated manually by the
    Kafka administrator, as admin has to find out which topics or partitions should
    be moved. Once the partitions are identified by the administrator, the partition
    reassignment tool (`bin/kafka-reassign-partitions.sh`) is used to move partitions
    across brokers, which takes care of everything. As a migration process, Kafka
    will make this newly added server a follower of the partition it is migrating.
    This allows the new server to fully replicate the existing data in that partition.
    Once the new server has fully replicated the partition''s contents and has become
    a part of the in-sync replica, one of the existing replicas will delete the partition''s
    data. The partition reassignment tool (`kafka-reassign-partitions.sh`) runs in
    three different modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--generate`: In this mode, the tool generates a candidate reassignment to
    move all partitions of the specified topics to the new server based on the list
    of topics and brokers shared with the tool'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '--execute: In this mode, the tool starts the reassignment of partitions based
    on the user-provided reassignment plan specified with the `--reassignment-json-file`
    option'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--verify`: In this mode, the tool verifies the status (completed successfully/failed/in
    progress) of the reassignment for all partitions listed during the last `--execute`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The partition reassignment tool can be used to move selected topics from the
    current set of brokers to newly added brokers (servers). Administrator should
    provide a list of topics to be moved to the new server and a target list of new
    broker IDs. This tool evenly distributes all partitions of a given topic across
    the new brokers and also moves the replicas for all partitions for the input list
    of topics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command generates the assignment (`new-topic-reassignment.json`)
    plan to move all partitions for topics `kafkatopic` and `kafkatopic1` to the new
    set of brokers having IDs `4` and `5`. At the end of this move, all partitions
    for topics `foo1` and `foo2` will only exist on brokers `5` and `6`. To initiate
    the assignment, the `kafka-reassign-partitions.sh` tool is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This tool can also be used to selectively move the partitions from the existing
    broker to the new broker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command selectively moves some replicas for certain partitions
    to the new server. Once the reassignment is done, the operation can be verified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To decommission any server from the Kafka cluster, the admin has to move the
    replica for all partitions hosted on the broker (server) to be decommissioned,
    to the remaining brokers with even distribution. The `kafka-reassign-partitions.sh`
    tool can also be used to increase the replication factor of the partition as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command assumes that partition `0` of the `kafkatopic` topic has
    replication factor `1` that existed on broker 2; and now it increases the replication
    factor from `1` to `2` and also creates the new replica on broker 3.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka topic tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, Kafka creates topics with a default number of partitions and replication
    factors (the default value is `1` for both). But, in real-life scenarios, we may
    need to define the number of partitions and replication factors more than once.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the command to create a topic with specific parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding command, the replication factor controls how many servers will
    replicate each message published by the message producer. For example, replication
    factor `3` means that up to two servers can fail before access is lost to the
    data. The partition count that enables parallelism for consumers reflects the
    number of logs the topic will be sharded into. Here, each partition must fit entirely
    on a single server. For example, if 10 partitions are defined for a topic, the
    full data set will be handled by no more than 10 servers excluding replicas.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kafka topic utility `kafka-topics.sh` can also be used to alter the Kafka
    topic as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding command, 10 more partitions are added to the Kafka topic created
    in the previous example. Currently Kafka does not support reducing the number
    of partitions or changing the replication factor for topics. To delete the Kafka
    topic, the following command is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `kafka-topics.sh` Kafka topic utility, configuration can also be
    added to the Kafka topic as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To remove configuration from the Kafka topic, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Kafka also provides a utility to search for the list of topics within the Kafka
    server. The List Topic tool provides a listing of topics and information about
    their partitions, replicas, or leaders by querying Zookeeper.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command obtains a list of topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'On execution of the preceding command, you should get the output shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kafka topic tools](img/3090OS_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding console output shows that we can get information about the topic
    and partitions that have replicated data. The output from the previous screenshot
    can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`leader`: This is a randomly selected node for a specific portion of the partitions
    and is responsible for all reads and writes for this partition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`replicas`: This represents the list of nodes that holds the log for a specified
    partition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`isr`: This represents the subset of the in-sync replicas'' list that is currently
    alive and in sync with the leader'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `kafkatopic` has two partitions (partitions `0` and `1`) with three
    replications, whereas `othertopic` has just one partition with two replications.
  prefs: []
  type: TYPE_NORMAL
- en: 'While getting a list of Kafka topics, two optional arguments can also be provided
    as: `under-replicated-partitions` and `unavailable-partitions`. The `under-replicated-partitions`
    argument is used to get details of those topics/partitions that have replicas
    that are under-replicated. The `unavailable-partitions` argument is used to get
    details of those topics/partitions whose leader is not available.'
  prefs: []
  type: TYPE_NORMAL
- en: Kafka cluster mirroring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kafka mirroring feature is used to create a replication of an existing cluster—for
    example, replicating an active datacenter into a passive datacenter. Kafka provides
    a mirror maker tool for mirroring the source cluster into a target cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts the mirroring tool placement in architectural
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kafka cluster mirroring](img/3090OS_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this architecture, the job of the mirror tool is to consume the messages
    from the source cluster and republish them on the target cluster using the embedded
    producer. A similar approach is used by the Kafka migration tool to migrate from
    the 0.7.x Kafka cluster to the 0.8.x Kafka cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To mirror the source cluster, bring up the target cluster and start the MirrorMaker
    processes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The minimum parameters required to start the MirrorMaker tool successfully are
    one or more consumer configurations, a producer configuration, and either a whitelist
    or a blacklist as standard Java regex patterns—for example, mirroring two topics
    named `A` and `B` using `--whitelist 'A|B'` or mirroring all topics using `--whitelist
    '*'`. The `--blacklist` configuration can also be used as standard Java regex
    patterns to specify what to exclude while mirroring. It also requires the consumer
    of the mirror tool to connect to the source cluster's ZooKeeper, the producer
    to the mirror cluster's ZooKeeper, or the `broker.list` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: For high throughput, an asynchronous embedded producer configured in blocking
    mode is used. This ensures that messages will not be lost and the blocking producer
    will wait till the messages are written to the target cluster if the producer's
    queue is full. The producer's queue being full consistently indicates that the
    MirrorMaker is bottle-necked on republishing messages to the target mirror cluster
    and/or flushing messages to disk. The `--num.producers` option can also be used
    to represent a producer pool in the MirrorMaker to increase throughput as multiple
    producer requests can be handled by multiple consumption streams of the target
    cluster. The `--num.streams` option specifies the number of mirror consumer threads
    to create.
  prefs: []
  type: TYPE_NORMAL
- en: Mirroring is often used in cross data center scenarios and, in general, a high
    value is used for the socket buffer size (`socket.buffersize`) on the MirrorMaker's
    consumer configuration and `socket.send.buffer` on the source cluster broker configuration.
    Also, the MirrorMaker consumer's fetch size (`fetch.size`) should be higher than
    the consumer's socket buffer size. If `broker.list` is used in the producer configuration
    along with the hardware load balancer, configuration for the number of retry attempts
    on producer failures can also be provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka also provides tools to check the position of the consumer while mirroring
    or in general. This tool shows the position of all the consumers in a consumer
    group and how far behind the end of the log consumers are; it indicates how well
    cluster mirroring is performing. This tool can be used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here the `--zkconnect` argument points to the source cluster's ZooKeeper (for
    example, the source data center). The `--topic` parameter is an optional parameter
    and, if the topic is not specified, then the tool prints information for all topics
    under the specified consumer group.
  prefs: []
  type: TYPE_NORMAL
- en: Integration with other tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section discusses the contributions by many contributors providing integration
    with Apache Kafka for various needs such as logging, packaging, cloud integration,
    and Hadoop integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Camus ([https://github.com/linkedin/camus](https://github.com/linkedin/camus))
    which provides a pipeline from Kafka to HDFS. Under this project, a single MapReduce
    job performs the following steps to load data to HDFS in a distributed manner:'
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, it discovers the latest topics and partition offsets from ZooKeeper.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each task in the MapReduce job fetches events from the Kafka broker and commits
    the pulled data along with the audit count to the output folders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the completion of the job, final offsets are written to HDFS and can be
    further consumed by subsequent MapReduce jobs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Information about the consumed messages is also updated in the Kafka cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some other useful contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: Automated deployment and configuration of Kafka and ZooKeeper on Amazon ([https://github.com/nathanmarz/kafka-deploy](https://github.com/nathanmarz/kafka-deploy))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A logging utility ([https://github.com/leandrosilva/klogd2](https://github.com/leandrosilva/klogd2))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A REST service for Mozilla Metrics ([https://github.com/mozilla-metrics/bagheera](https://github.com/mozilla-metrics/bagheera))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Camel-Kafka integration ([https://github.com/BreizhBeans/camel-kafka/wiki](https://github.com/BreizhBeans/camel-kafka/wiki))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a detailed list of Kafka ecosystem tools, please refer to [https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem](https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have added some more information about Kafka, such as its
    administrator tools, its integration, and Kafka non-Java clients.
  prefs: []
  type: TYPE_NORMAL
- en: During this complete journey through Apache Kafka, we have touched upon many
    important facts about Kafka. You have learned the reason why Kafka was developed,
    its installation procedures, and its support for different types of clusters.
    We also explored the Kafka's design approach, and wrote a few basic producers
    and consumers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discussed Kafka's integration with technologies such as Hadoop and
    Storm.
  prefs: []
  type: TYPE_NORMAL
- en: The journey of evolution never ends.
  prefs: []
  type: TYPE_NORMAL
