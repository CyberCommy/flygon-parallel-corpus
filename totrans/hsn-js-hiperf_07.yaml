- en: Streams - Understanding Streams and Non-Blocking I/O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have touched on almost all of the subjects that help us write highly performant
    code for the server with JavaScript. The two last topics that should be discussed
    are streams and data formats. While these two topics can go hand in hand (since
    most data formats are implemented through read/write streams), we will focus on
    streaming in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming gives us the capability to write systems that can process data without
    taking up a lot of working memory and without blocking the event queue. For those
    that have been reading this book sequentially, this may sound familiar to the
    concept of generators, and this is correct. We will focus on the four different
    types of streams that Node.js provides and that we can extend easily. From there,
    we will look at how we can combine streams and generators to process data with
    the built-in generator concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Streaming basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Readable streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writable streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplex streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transform streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aside – generators and streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are prerequisites for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A code editor or IDE, preferably VS Code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An operating system that can run Node.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code found at [https://github.com/PacktPublishing/Hands-On-High-Performance-Web-Development-with-JavaScript/tree/master/Chapter07](https://github.com/PacktPublishing/Hands-On-High-Performance-Web-Development-with-JavaScript/tree/master/Chapter07).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Streaming is the act of working on an infinite dataset. This does not mean
    that it is, but it means that we have the possibility of having an unlimited data
    source. If we think in the traditional context of processing data, we usually
    run through three main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open/get access to a data source.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process the data source once it is fully loaded in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spit out computed data to another location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can think of this as the basics of **input and output** (**I/O**). Most of
    our concepts of I/O involve batch processing or working on all or almost all of
    the data. This means that we know the limits of that data ahead of time. We can
    make sure that we have enough memory, storage space, computing power, and so on,
    to deal with the process. Once we are done with the process, we kill the program
    or queue up the next batch of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple example of this is seen as follows, where we count the number of lines
    that the file has:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We bring in the `readFileSync` method from the `fs` module and then read in
    the `input.txt` file. From here, we split on `\n` or `\r\n`, which gives us an
    array of all the lines of the file. From there, we get the length and put it on
    our standard output channel. This seems quite simple and seems to work quite well.
    For small- to medium-length files, this works great, but what happens when the
    file becomes abnormally large? Let's go ahead and see. Head over to [https://loremipsum.io](https://loremipsum.io)
    and give it an input of 100 paragraphs. Copy this and paste it a few times into
    the `input.txt` file. Now, when we run this program, we can see in our task manager
    a spike in memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are loading a roughly 3 MB file into memory, counting the number of newlines,
    and then printing this out. This should still be quite fast, but we are now starting
    to utilize a good chunk of memory. Let''s do something a bit more complex with
    this file. We will count the number of times the word `lorem` appears in the text. We
    can do that with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Again, this should process quite quickly, but there should be some lag in how
    it processes. While the use of a regular expression here could give us some false
    positives, it does showcase that we are batch processing on this file. In many
    cases, when we are working in a high-speed environment, we are working with files
    that can be close to or above 1 GB. When we get into these types of files, we
    do not want to load them all into memory. This is where streaming comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Many systems that are considered big data are working with terabytes of data.
    While there are some in-memory applications that will store large amounts of data
    in memory, a good chunk of this type of data processing uses a mix of both streaming
    with files and using in-memory data sources to work with the data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take our first example. We are reading from a file and trying to count
    the number of lines in the file. Well, instead of thinking about the number of
    lines as a whole, we can look for the character that denotes a newline. The character(s)
    we are looking for in our regular expression are the use of a newline character
    (`\n`) or the carriage return plus the newline (`\r\n`) character. With this in
    mind, we should be able to build a streaming application that can read the file
    and count the number of lines without loading the file completely into memory.
  prefs: []
  type: TYPE_NORMAL
- en: This example presents the API for utilizing a stream. We will go over what each
    Stream API gives us and how we can utilize it for our purposes. For now, take
    the code examples and run them to see how these types of applications work.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be seen in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We grab a `Readable` stream from the `fs` module and create one. We also create
    a constant for the newline character represented in HEX format. We then listen
    for the data event so we can process data as it comes in. Then, we process each
    byte to see whether it is the same as the newline character. If it is, then we
    have a newline, otherwise we just keep searching. We do not need to explicitly
    look for the carriage return since we know it should be followed by a newline
    character.
  prefs: []
  type: TYPE_NORMAL
- en: While this will be slower than loading the entire file into memory, it does
    save us quite a bit of memory when we are processing the data. Another great thing
    about this method is that these are all events. With our full processing example,
    we are taking up the entire event loop until we are done processing. With the
    stream, we have events for when the data comes in. This means that we can have
    multiple streams running at the same time on the same thread without having to
    worry too much about blocking (as long as we are not spending too much time on
    the processing of the data chunk).
  prefs: []
  type: TYPE_NORMAL
- en: 'With the previous example, we can see how we could write the counterexample
    in streaming form. Just to drive the point home, let''s go ahead and do just that.
    It should look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: First, we create a read `stream` as we did before. Next, we create a `Buffer`
    form of the keyword that we are looking for (working on the raw bytes can be faster
    than trying to convert the stream into text, even if the API allows us to do that).
    Next, we maintain a `found` count and an `actual` count. The `found` count will
    let us know whether we have found the word; the other count keeps track of how
    many instances of `lorem` we have found. Next, we process each byte when a chunk
    comes in on the data event. If we find that the next byte is not the character
    we are looking for, we automatically return the `found` count to `0` (we did not
    find this particular string of text). After this check, we will see whether we
    have the full byte length found. If we do, we can increase the count and move
    `found` back to `0`. We keep the `found` counter outside the data event because
    we receive the data in chunks. Since it is chunked, one part of `lorem` could
    come at the end of one chunk and the other piece of `lorem` could come at the
    beginning of the next chunk. Once the stream ends, we output the count.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we run both versions, we will find that the first actually catches more
    `lorem`. We added the case insensitive flag for regular expressions. If we turn
    this off by removing the trailing `i` and we remove the need for the character
    sequence to be by itself (the `\s` surrounding our character sequence), we will
    see that we get the same result. This example showcases how writing streams can
    be a bit more complicated than the batch processing version, but it usually leads
    to lower memory use and sometimes faster code.
  prefs: []
  type: TYPE_NORMAL
- en: While utilizing built-in streams such as the streams inside of the `zlib` and
    `fs` modules will get us quite far, we will see how we can be the producers of
    our own custom streams. We will take each one and write an extended stream type
    that will handle the data framing that we were doing in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: For those that have forgotten or skipped to this chapter, we were framing all
    of our messages over a socket with the `!!!BEGIN!!!` and `!!!END!!!` tags to let
    us know when the full data had been streamed to us.
  prefs: []
  type: TYPE_NORMAL
- en: Building a custom Readable stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A `Readable` stream does exactly what it states, it reads from a streaming source.
    It outputs data based on some criteria. Our example of this is a take on the simple
    example that is shown in the Node.js documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to take our example of counting the number of `lorem` in the text
    file, but we are going to output the location in the file that we found `lorem`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `Readable` class and the `createReadStream` method from their respective
    modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a class that extends the `Readable` class and set up some private variables
    to track the internal state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a constructor that initializes our `#file` variable to a `Readable` stream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the constructor, we are going to utilize a `#data` private variable
    that will be a function. We will utilize it to read from our `#file` stream and
    to check for the locations of `lorem`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We run through each byte and check whether we currently have the byte we are
    looking for in the `lorem` word. If we do and it is the `l` of the word, then
    we set our location `#startByteLoc` variable. If we find the entire word, we output
    `#startByteLoc`, otherwise, we reset our lookup variable and keep looping. Once
    we have finished looping, we add to our `#totalCount` the number of bytes we read
    and wait for our `#data` function to get called again. To end our stream and let
    others know that we have fully consumed the resource, we output a `null` value.
  prefs: []
  type: TYPE_NORMAL
- en: The final piece we add is the `_read` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will get called either through the `Readable.read` method or through hooking
    a data event up. This is how we can make sure that a *primitive* stream such as `FileStream`
    is consumed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can add some test code to make sure that this stream is working properly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: With all of these concepts, we can see how we are able to consume primitive
    streams and be able to wrap them with a superset stream. Now that we have this
    stream, we could always use the pipe interface and pipe it into a `Writable` stream.
    Let's write the indices out to a file. To do this, we can do something as simple
    as `loremFinder.pipe(writeable)`.
  prefs: []
  type: TYPE_NORMAL
- en: If we open the file, we will see that it is just a bunch of random data. The
    reason for this is that we encoded all of the indices into 32-bit buffers. If
    we wanted to see them, we could rewrite our stream implementation just a little
    bit. This modification could look like this: `this.push(this.#startByteLoc.toString()
    + "\r\n");`.
  prefs: []
  type: TYPE_NORMAL
- en: With this modification, we can now look at the `output.txt` file and see all
    of the indices. It should start to become apparent how powerful it is writing
    streams and being able to just pipe one to the next, on top of how readable the
    code can become if we just keep piping them through various stages.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Readable stream interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Readable` stream has a few properties that we have available to us. All
    of them are explained in the Node.js documentation, but the main ones that we
    are interested in are `highWaterMark` and `objectMode`.
  prefs: []
  type: TYPE_NORMAL
- en: '`highWaterMark` allows us to state how much data the internal buffer should
    hold before the stream will state that it can no longer take any data. One problem
    with our implementation is that we do not handle pauses. A stream can pause if
    this `highWaterMark` is reached. While we may not be worried about it most of
    the time, it can cause problems and is usually where stream implementors will
    run into issues. By setting a higher `highWaterMark` , we can prevent these problems.
    Another way of handling this would be to check the outcome of running `this.push`.
    If it comes back `true` , then we are able to write more data to the stream, otherwise,
    we should pause the stream and then resume when we get the signal from the other
    stream. The default `highWaterMark` for streams is around 16 KB.'
  prefs: []
  type: TYPE_NORMAL
- en: '`objectMode` allows us to build streams that are not `Buffer` based. This is
    great when we want to run through a list of objects. Instead of utilizing a `for`
    loop or even a `map` function, we could set up a piping system that moves objects
    through the stream and performs some type of operation on it. We are also not
    limited to plain old objects, but to almost any data type other than the `Buffer`.
    One thing to note about `objectMode` is it changes what the `highWaterMark` counts.
    Instead of it counting how much data to store in the internal buffer, it counts
    the number of objects that it will store until it pauses the stream. This defaults
    to `16`, but we can always change it if need be.'
  prefs: []
  type: TYPE_NORMAL
- en: With these two properties explained, we should discuss the various internal
    methods that are available to us. For each stream type, there is a method that
    we *need* to implement and there are methods that we *can* implement.
  prefs: []
  type: TYPE_NORMAL
- en: For the `Readable` stream, we only need to implement the `_read` method. This
    method gives us a `size` argument, which is the number of bytes to read from our
    underlying data source. We do not always need to heed this number, but it is available
    to us if we want to implement our stream with it.
  prefs: []
  type: TYPE_NORMAL
- en: Other than the `_read` method, we need to utilize the `push` method. This is
    what pushes data onto our internal buffer and helps emit the data event as we
    have seen previously. As we stated before, the `push` method returns a Boolean.
    If this value is `true`, we can continue using `push`, otherwise, we should stop
    pushing data until our `_read` implementation gets called again.
  prefs: []
  type: TYPE_NORMAL
- en: As stated previously, when first implementing a `Readable` stream, the return
    value can be ignored. However, if we notice that data is not flowing or data is
    getting lost, the usual culprit is that the `push` method returned `false` and
    we continued to try pushing data on our stream. Once this happens, we should implement
    pausing by stopping the use of the `push` method until `_read` gets called again.
  prefs: []
  type: TYPE_NORMAL
- en: Two other pieces of the readable interface are the `_destroy` method and how
    to make our stream error out if something comes through that we are not able to
    handle. The `_destroy` method should be implemented if there are any low-level
    resources that we need to let go of.
  prefs: []
  type: TYPE_NORMAL
- en: This can be file handles opened with the `fs.open` command or sockets created
    with the `net` module. If an error occurred, we should also use that so we can
    emit an error event.
  prefs: []
  type: TYPE_NORMAL
- en: To handle errors that may come up with our streams, we should emit our error
    through the `this.emit` system. If we throw an error, as per the documentation,
    it can lead to unexpected results. By emitting an error, we let the user of our
    stream deal with the error and handle it as they see fit.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Readable stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From what we have learned here, let's implement the framing system that we talked
    about earlier. From our previous example, it should be obvious how we might handle
    this. We will hold the underlying resource, in this case, a socket. From there,
    we will find the `!!!BEGIN!!!` buffer and let it pass. We will then start to store
    the data that is held. Once we reach the `!!!END!!!` buffer, we will push out
    the data chunk.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are holding on to quite a bit of data in this case, but it showcases how
    we might handle framing. The duplex stream will showcase how we might handle a
    simple protocol. The example is seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `Readable` stream and create a class called `ReadMessagePassStream`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Add some private variables to hold our internal state for the stream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `#data` method like the one before. We will now be looking for the
    beginning and end frame buffers that we set up before, `#bufBegin` and `#bufEnd`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the constructor for the class to initialize our private variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: One new piece of information is the `objectMode` property, which can be passed
    into our stream. This allows our streams to read objects instead of raw buffers.
    In our case, we do not want this to happen; we want to work with the raw data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the `_read` method to make sure that our stream will start up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'With this code, we now have a way of handling a socket without having to listen
    to the data event in our main code; it is now wrapped in this `Readable` stream.
    On top of this, we now have the capability of piping this stream into another.
    The following test harness code shows this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We have a server being hosted on localhost at port `3333`. We create a `write`
    stream and pipe any data from our `ReadMessagePassStream` to that file. If we
    hook this up to the server in the test harness, we will notice that an output
    file is created and holds only the data that we sent over it, not the framing
    code.
  prefs: []
  type: TYPE_NORMAL
- en: The framing technique that we are utilizing is not always going to work. Just
    as it has been shown in the `lorem` example that our data can get chunked at any
    point, we could have our `!!!START!!!` and `!!!END!!!` end up on one of the chunk
    boundaries. If this happened, our streams would fail. There is additional code
    that we would need to handle these cases, but these examples should provide all
    the necessary ideas to implement the streaming code.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will take a look at the `Writable` stream interface.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Writable stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A `Writable` stream is one that we write data to and it can pipe into a `Readable`,
    `Duplex`, or `Transform` stream. We can use these streams to write data in a chunked
    manner so a consuming stream can process the data in chunks instead of all at
    once. The API for a writable stream is quite similar to that of a `Readable` stream
    except for the methods that are available to us.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Writable stream interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A writable stream gives us nearly the same options that are available to a
    `Readable` stream so we will not go into that. Instead, we will take a look at
    the four methods that are available to us—one that we *must* implement and the
    rest that we *can* implement:'
  prefs: []
  type: TYPE_NORMAL
- en: The `_write` method allows us to perform any type of transformations or data
    manipulations that we need to and provides us with the ability to use a callback.
    This callback is what signals that the write stream is able to take in more data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While not inherently true, it is what pops data off the internal buffer if there
    is any. However, for our purposes, it is best to think of the callback as a way
    to process more data.
  prefs: []
  type: TYPE_NORMAL
- en: We can utilize this to wrap a more primitive stream and add our own data before
    or after the main data chunk. We will see this with the practical counterpart
    to our `Readable` stream.
  prefs: []
  type: TYPE_NORMAL
- en: The `_final` method allows us to perform any actions that we need to before
    the writable stream closes. This could be a cleanup of resources or sending out
    any data that we may have been holding on to. We will usually not implement this
    method unless we are holding on to something such as a file descriptor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `_destroy` method is the same as the `Readable` stream and should be treated
    similar to the `_final` method, except we can potentially get errors on this method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `_writev` method gives us the capability of handling multiple chunks at
    the same time. We can implement this if we have some sort of ordering system for
    the chunks or we do not care what order the chunks come in. While this may not
    be apparent now, we will implement this method when we implement the duplex stream
    next. The use cases can be somewhat limited, but it still can be beneficial.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Writable stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following `Writable` stream implementation showcases our framing method
    and how we can use it to put the `!!!START!!!` and `!!!END!!!` frames on our data.
    While simplistic, it does showcase the power of framing and building more complex
    streams around the primitive ones:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `Writable` class from the stream module and create the shell for `WriteMessagePassStream`.
    Set this as the default export for this file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the private state variables and the constructor. Make sure not to allow
    `objectMode` through since we want to work on the raw data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `_write` method to our class. It will be explained as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'With this code, we can see some similar points to how we handled the readable
    side. Some notable exceptions include the following items:'
  prefs: []
  type: TYPE_NORMAL
- en: We implement the `_write` method. We are ignoring, again, the encoding parameter
    of this function, but we should check this in case we get an encoding that we
    are not expecting. The chunk is the data that is being written, and the callback
    is what we call when we are finished processing the write for this chunk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we are wrapping a socket and we do not want to kill it once we are done
    sending the data, we need to send some type of stop signal to our stream. In our
    case, we are using the simple `0x00` byte. In a more robust implementation, we
    would utilize something else, but this should work for now.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No matter what, we either use the framing or we just write to the underlying
    socket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We call the callback once we are finished with our processing. In our case,
    if we have the `writing` flag set, this means we are still in a frame and we want
    to return early, otherwise, we want to put our stream into writing mode and write
    out the `!!!START!!!` and then the chunk. Again, if we never use the callback,
    our stream will be infinitely paused. The callback is what tells the internal
    mechanism to pull more data from the internal buffer for us to consume.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this code, we can now look at the test harness and how we are utilizing
    it to create a server and handle incoming `Readable` streams that implement our
    framing context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a server and listen in on port `3333` for localhost. Whenever we
    receive a connection, we wrap it with our `Writable` stream. We then send down
    a bunch of test data and, once that is finished, we write out the `0x00` signal
    to tell our stream this frame is done, and we then call the `end` method to say
    we are finished with this socket. If we added another test run after our first,
    we can see how our framing system works. Let''s go ahead and do just that. Add
    the following code after `wrapped.write(Buffer.from([0x00]))`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If we ever hit the `highWaterMark` of our stream, the write stream will pause
    until the read stream has started to consume from it.
  prefs: []
  type: TYPE_NORMAL
- en: If we now run the test harness with our `Readable` stream from before, we will
    see that we are processing all of this data and writing out to our file without
    any of the framing passing through. With these two stream implementations, we
    are now able to pipe data across a socket with a custom framing option. We would
    now be able to use this system to implement our data-passing system from the previous
    chapter. However, we will instead implement a `Duplex` stream that will improve
    on this system and allow us to work with multiple writable chunks, which is what
    we will see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Duplex stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A duplex stream is just that, one that works both ways. It combines a `Readable`
    and `Writable` stream into a single interface. With this type of stream, we can
    now just pipe from the socket into our custom stream instead of wrapping the stream
    like we have been (even though we will still implement it as a wrapped stream).
  prefs: []
  type: TYPE_NORMAL
- en: 'There is not much more to talk about with `Duplex` streams other than one fact
    that trips up newcomers to the stream type. There are two separate buffers: one
    for `Readable` and one for `Writable`. We need to make sure to treat them as separate
    instances. This means whatever we use for the `_read` method in terms of variables,
    should not be used for the `_write` and `_writev` method implementations, otherwise,
    we could run into bad bugs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated before, the following code implements a `Duplex` stream along with
    a counting mechanism so, that way, we can utilize the `_writev` method. As mentioned
    in the *Understanding the Writable stream interface* section, the `_writev` method
    allows us to work on multiple chunks of data at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `Duplex` class from the `stream` module and add the shell for our
    `MessageTranslator` class. Export this class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Add all the internal state variables. Each of them will be explained in the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the constructor for our class. We will handle the data event for our `#socket`
    inside of this constructor instead of creating another method as we have in the
    past:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We will automatically assume we have a single message per chunk. This makes
    the processing much easier. When we do get data, we will read in the packet number,
    which should be the first four bytes of data. We then read in the size of the
    message, which is the next `4` bytes of data. Finally, we push the rest of the
    data into our internal buffer. Once we have finished reading the entire message,
    we will put all the internal chunks together and push them out. Finally, we will
    reset our internal buffer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the `_writev` and `_write` methods to our class. Remember that the `_writev`
    method is utilized for multiple chunks, so we will have to loop through them and
    write each one out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Add helper methods to process the chunks and to actually write them out. We
    will utilize the number `-1` as a `4`-byte message to state we are done with this
    message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Our `#processChunkHelper` method checks to see whether we hit the magical `-1`
    `4`-byte message to say we have finished writing our message. If we do not, we
    keep adding to our internal buffer (the array). Once we have reached the end,
    we will put all of the data together and then move onto the next packet of data.
  prefs: []
  type: TYPE_NORMAL
- en: Our `#writeHelper` method will loop through all of those packets and check to
    see whether any of them are finished. If they are, it will get the packet number,
    the size of the buffer, the data itself, and concatenate it all together. Once
    it has done this, it will reset the internal buffer to make sure we are not leaking
    memory. We will write all of this data to the socket and then call our callback
    to state that we are done writing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finish up the `Duplex` stream by implementing our `_read` method as we have
    before. The `_final` method should just call the callback since there is no processing
    left:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`_writev` should really be used when order does not matter and we are just
    processing the data and possibly turning it into something else. This could be
    a hashing algorithm or something similar to that. In almost all cases, the `_write`
    method should be used.'
  prefs: []
  type: TYPE_NORMAL
- en: While this implementation has quite a few flaws (one being that we do not look
    for possible other packets if we reach the `-1` number), it does showcase how
    we can build a `Duplex` stream and also another way of handling messages. It is
    not recommended to come up with our own schemes of moving data across a socket
    (as we will see in the next chapter), but if there is a new specification that
    comes out, we could always write for it utilizing the `Duplex` socket.
  prefs: []
  type: TYPE_NORMAL
- en: If we test this implementation with our test harness, we should get a file called
    `output.txt` that has the duplex plus the number message written 100,000 times,
    plus a trailing end-of-line character. Again, a `Duplex` stream is just a separate
    `Readable` and `Writable` stream put together and should be used when implementing
    a data transmission protocol.
  prefs: []
  type: TYPE_NORMAL
- en: The final stream that we will take a look at is the `Transform` stream.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Transform stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Out of the four streams, this may be the most useful and possibly the most used
    stream out of the group. A `Transform` stream hooks up the readable and writable
    portions of a stream and allows us to manipulate the data that comes across it.
    This may sound similar to a `Duplex`. Well, a `Transform` stream is a special
    type of `Duplex` stream!
  prefs: []
  type: TYPE_NORMAL
- en: Built-in implementations of `Transform` streams include any of the streams implemented
    in the `zlib` module. The basic idea is that we are not just trying to pass information
    from one end to the other; we are trying to manipulate that data and turn it into
    something else. That is what the `zlib` streams give us. They compress and decompress
    the data. `Transform` streams change the data into another form. This also means
    that we can make a transform stream be a one-way transformation; whatever is output
    from the transform stream cannot be undone. We will create one of these `Transform`
    streams here, specifically creating a hash of a string.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's go through the interface for a `Transform` stream.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Transform stream interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have access to two methods that we almost always want to implement, no matter
    what. One gives us access to the underlying chunk of data and allows us to perform
    the transformation on it. We implement this one with the `_transform` method.
    It takes three arguments: the chunk of data that we are working on, the encoding,
    and a callback to let the underlying system know that we are ready to process
    more information.'
  prefs: []
  type: TYPE_NORMAL
- en: One special thing about the callback function that differs from the `_write`
    callback for a `Writable` stream is that we can pass data to it to emit data on
    the readable side of the `Transform` stream, or we can pass nothing to it to signal
    that we want to process more data. This allows us to only send out data events
    when we want to instead of almost always needing to pass them out.
  prefs: []
  type: TYPE_NORMAL
- en: The other method is the `_flush` method. This allows us to finish any processing
    of any data that we may still be holding. Or, it will allow us to output once all
    of the data that has been sent into the stream. This is what we will implement
    with our string hashing function.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Transform stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our `Transform` stream is going to take in string data and keep running a hashing
    algorithm. Once it has finished, it will output the final hash that was calculated.
    A hashing function is one where we take some form of input and output a unique
    piece of data. This unique piece of data (in our case, a number) should not be
    vulnerable to collisions. Collisions is the concept that two values that differ
    from each other could come to the exact same hash value. In our case, we are converting
    the string to a 32-bit integer in JavaScript so we have a low chance of collision,
    but not an impossible chance of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Each function of the previous stream is explained below:'
  prefs: []
  type: TYPE_NORMAL
- en: The only thing that we need to persist until the stream is destroyed is the
    current hash code. This will allow the hash function to keep track of what we
    have already passed into it and work off of the data after each write.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We do a check here to see whether the chunk we received is a `Buffer`. Since
    we made sure to turn the option of `decodeStrings` on, this means that we should
    always get buffers, but it still helps to check.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While the contents of the hash function can be seen at the URL provided, the
    only major thing that we need to worry about is that we call our callback, just
    like we had to do when we were implementing our `Writable` stream.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we are ready to produce data, we utilize the `push` method, just like we
    did with the `Readable` stream. Remember, `Transform` streams are just special
    `Duplex` streams that allow us to manipulate the data that is being input and
    change it into something for the output. We can also change the last two lines
    of code to just do `callback(null, buf)`; this is just the shorthand of what we've
    seen previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, if we run some test cases on the previous code, we will see that we do
    get a unique hash code for each unique string that we enter, but we get the same
    hash code when we input the exact same thing. This means that our hashing function
    is good and we can hook it up to a streaming application.
  prefs: []
  type: TYPE_NORMAL
- en: Using generators with streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everything that we have seen up to this point showcases how we can utilize all
    of the built-in systems in Node.js to create streaming applications. However,
    for those that have been following sequentially in the book, we have discussed
    generators. Those that have been keen to think about them would notice a strong
    correlation between streams and generators. This is actually the case! We can
    utilize generators to hook into the Streaming API.
  prefs: []
  type: TYPE_NORMAL
- en: With this concept, we could build generators that can both work in the browser
    and inside of Node.js without that much overhead. We have even seen in a [Chapter
    6](7dc2a5fd-0c78-49e7-9e84-f789eab14ca5.xhtml), *Message Passing – Learning about
    the Different Types*,how we can get at the underlying stream for the Fetch API.
    Now, we can write a generator that can work with both of these subsystems.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let''s just look at an example of an `async` generator and how we
    can hook them into the Node.js streaming system. The example will be to see how
    we can have a generator as the input for a `Readable` stream:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to set up a `Readable` stream to read out the 26 lowercase characters
    of the English alphabet. We can do this fairly easily by writing the following
    generator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'While the character code is below `123`, we keep sending data. We can then
    wrap this in a `Readable` stream, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: If we now run this code, we will see the characters *a* through *z* appear in
    our console. The `Readable` stream knows that it is the end because a generator
    produces an object with two keys. The `value` field gives us the value from the
    `yield` expression and the `done` tells us if the generator has finished running.
  prefs: []
  type: TYPE_NORMAL
- en: 'This lets the readable interface know when to send out `data` events (through
    us yielding a value) and when to close the stream (through the `done` key being
    set to `true`). We could also pipe the output of our readable system into that
    of the writable to chain the process. This can easily be seen with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Implementing streams through generators and `async`/`await` may seem like a
    good idea, but we should only utilize this if we are trying to put an already
    `async`/`await` piece of code with a stream. Always try to go for readability;
    utilizing the generator or `async`/`await` method will most likely lead to something
    unreadable.
  prefs: []
  type: TYPE_NORMAL
- en: With the previous example, we have combined the readable from a generator and
    we have utilized the piping mechanism to send it to a file. With `async`/`await`
    and generators becoming constructs in the JavaScript language, it won't be long
    before we have streaming as a first-class concept.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streaming is one of the pillars of writing highly performant Node.js code. It
    allows us to not block the main thread, while still being able to process data.
    The Streaming API allows us to write different types of streams for our purposes.
    While most of these streams will be in the form of transform streams, it is nice
    to see how we could implement the other three.
  prefs: []
  type: TYPE_NORMAL
- en: The final topic we will look at in the next chapter is data formats. Handling
    different data formats besides JSON will allow us to interface with many big data
    providers and be able to handle the data formats that they like to use. We will
    see how they utilize streaming to implement all of their format specifications.
  prefs: []
  type: TYPE_NORMAL
