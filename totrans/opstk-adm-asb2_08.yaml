- en: Chapter 8. Setting Up Active-Active Regions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will focus on demonstrating one of the very useful built-in
    features of OpenStack. This would be the capability of being able to centrally
    manage multiple OpenStack regions that could be running in separate geographical
    locations. The concept of regions within OpenStack is not a new one, but ask yourself
    whether you have ever actually seen it done. On many occasions, I found myself
    unclear on the steps needed to accomplish this. Well today is the day you will
    have a positive response to that question.
  prefs: []
  type: TYPE_NORMAL
- en: 'With stability and availability currently being popular topics within the OpenStack
    community, I thought it would be good to share a viable use case to accomplish
    cloud high availability. This will be just one of the many ways a cloud operator
    could set this up. As we may already know, OpenStack can meet numerous high-availability
    requirements. We will briefly review those scenarios and then transition to why
    you would use this functionality. As with all the previous chapters, we will then
    complete the chapter by demonstrating how to automate setting up Active-Active
    cloud regions using Ansible. We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing OpenStack high availability scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why to use Active-Active cloud regions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Active-Active cloud regions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and setting up Admin region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring active regions' authentication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding the playbooks and roles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing the playbook and roles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing OpenStack high availability scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This topic happens to be one of those I always enjoy discussing. **high availability**
    (**HA**) and disaster recovery always become very emotional conversations among
    IT folks for obvious reasons. Your neck is on the line, so to speak, to make sure
    that your organization's systems remain online in the event of a disaster/failure.
    In days of old local system, HA and cold (unused) disaster recovery sites were
    good enough. The current agility of cloud now offers up new and better options
    for system stability. Do not settle for the old solutions. You have options!
  prefs: []
  type: TYPE_NORMAL
- en: 'As repeated earlier, there are multiple ways to accomplish HA with OpenStack.
    We will outline three possible scenarios that I have found to be successful and
    would meet most organizations'' HA requirements. The three possible scenarios
    are listed here with a diagram to add additional context:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiple data centers**: Multiple OpenStack regions are spanned across multiple
    geographically located data centers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single data center**: Multiple OpenStack regions are within one data center'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability zones**: Using paired availability zones within a single OpenStack
    region located within one data center'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Reviewing OpenStack high availability scenarios](graphics/image_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Multiple data centers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would start with the most complex of the three scenarios. This scenario includes
    the concept of deploying multiple sets of OpenStack regions across numerous data
    centers and having them operate as one cloud system. While this may sound complicated,
    it is not as difficult as it sounds. The complexity comes into play when it comes
    time to tie them altogether and then of course when you go to support/manage them
    all. This model not only gives you HA across data centers (multiple Active-Active
    regions), but it also provides HA within each data center individually (independent
    Active-Active region). You would have to have multiple layers of failures in order
    to take your cloud offline.
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple data centers](graphics/image_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Single data center
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the preceding scenario, with the major difference being that, it
    is only restricted to a single data center. In this scenario, you could deploy
    a single set of OpenStack Active-Active regions restrained to just one data center.
    This model would only provide HA within the data center where the regions are
    running. If that particular data center catches fire, your cloud would be **So
    Out of Luck** (**SOL**).
  prefs: []
  type: TYPE_NORMAL
- en: If left with few options, this model could still save you from complete cloud
    failure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Single data center](graphics/image_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Availability Zones
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This last scenario may be the simplest option but can certainly do the job in
    delivering guest-level HA. Yes, it does fall short if you are seeking to gain
    a true disaster recovery design. By leveraging multiple AZs, you can spread the
    instances across separate compute nodes using the anti-affinity filter, in essence
    providing the guest-level HA.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's focus on a simple paired down version of the multiple data center
    model we described earlier. We will review why you may be interested in using
    the Active-Active region approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![Availability Zones](graphics/B06086_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Why to use Active-Active cloud regions?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outside of just the plain awesomeness of being able to actively use more than
    one OpenStack region, the Active-Active cloud region approach provides the best
    use of your overall cloud investment. No more are the days of having to perform
    DR tests simply because the second site is not regularly used. Plus you gain the
    added bonus of a centralized management region. A *win-win* situation all over
    the place.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s go deeper into the architecture in order to deliver an OpenStack
    Active-Active region. The following diagram explains the architecture in its simplest
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why to use Active-Active cloud regions?](graphics/image_08_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The components of the preceding architecture are:'
  prefs: []
  type: TYPE_NORMAL
- en: Two separate OpenStack cloud deployments, which in turn equates to two regions.
    In this example, we have **Region A** and **Region B**. These regions run the
    core OpenStack services except Keystone and Horizon. Each region can have any
    and as many complimentary AZs as you wish.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create another OpenStack region solely dedicated to hosting the Keystone and
    Horizon services. This region could be classified as the Admin region.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regions A and B would then leverage the Admin region to handle authentication
    and GUI web interface by centralizing user, tenant, and project management/creation
    as well as providing a single web dashboard to manage all the active regions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Active-Active cloud regions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process to implement it is relatively straightforward, but it does require
    distinct attention to detail. Outlining the steps beforehand I have found to be
    very useful and avoid missing steps. I have also learned that performing the changes
    manually (aka by hand) does not normally end well either. The process of editing
    the services configuration files does leave open the door of making edits by mistake,
    which leads to services that will not start. No good!!! Not even mentioning that
    it makes the process to implement take three times as long. First, we will review
    the steps manually and then in the following section, we will learn how to automate
    the setup process as much as possible. All I can say is thank heavens for Ansible!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will review the manual steps to set up Active-Active OpenStack
    cloud regions. A brief snapshot of the steps is outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: Inventory each region's endpoints and take note of the URLs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create service user accounts on the Admin region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create services on the Admin region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register each region's endpoints to the Admin region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjust the Admin region's identity endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure each of the region's services to authenticate against the Admin region
    identity service instead of the local region's identity service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's go step-by-step through each configuration step shown here demonstrating
    the working configuration examples.
  prefs: []
  type: TYPE_NORMAL
- en: Region endpoint inventory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This step would be a simple query of the endpoints of each region you want
    to include in the Active-Active setup. Since we are using **openstack-ansible**
    (**OSA**) to deploy our OpenStack clouds, you will need to connect into the utility
    container of each region in order to use the OpenStack CLI. Once connected and
    you source the OpenRC file, the command would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this command should look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Region endpoint inventory](graphics/image_08_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Keep in mind that our focus here is to just take note of the public endpoints
    available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since openstack-ansible installs the OpenStack services into LXC containers,
    you will need to know how to connect to each container for use of the CLI and
    to configure/maintain the services. The LXC command to list all the containers
    running on the control plane server is `lxc-ls -fancy`, and the output will look
    similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Region endpoint inventory](graphics/image_08_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Admin region configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next steps will now involve customizing the install and configuration of
    the Admin region. This will be your centralized management region servicing only
    authentication requests. The Admin region can exist in the same data center as
    one of the other regions or in an entirely separate region from the other. Obviously,
    network connectivity between the data centers would be required. Please follow
    the instructions given later in order to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Creating service user accounts on the Admin region
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At this point, you should have a functioning Admin region running only the
    identity service (Keystone) and the web dashboard (Horizon). Only those two services
    should be present and active. Since we want to use the Admin region to manage
    the other regions, you must make it aware of the other regions services and endpoints.
    This process starts with creating the service user accounts on the Admin region:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this step, we will create the service user accounts using the CLI with
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A working example of the command would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we must assign the new user just created a role with the proper permissions.
    The CLI command to accomplish this is here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'A working example of the command would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the service user accounts created, we can transition on to
    the next step of registering the new services on the Admin region.
  prefs: []
  type: TYPE_NORMAL
- en: Creating services on the Admin region
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this step we are simply creating placeholders on the Admin region for the
    services running on the active regions. Remember that the active regions have
    the other core services running on them and the Admin region will handle the authentication
    for them. The Admin region then has to be aware of the services.
  prefs: []
  type: TYPE_NORMAL
- en: 'The services will be registered on the Admin region using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A working example of the command would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The next step will now be to register the active regions endpoints on the Admin
    region. This step requires a level of precision, as the endpoint URL is what the
    Admin region will use to make functional calls. If the URL is incorrect or mistyped,
    the service will be considered down per the Admin region.
  prefs: []
  type: TYPE_NORMAL
- en: Registering each region's endpoints to the Admin region
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The process of registering the active region's endpoints involves using the
    endpoint inventory we started with earlier. The key points here are that you must
    use the IP address from the public endpoints of each region. The IP address assigned
    to the public endpoint needs to be a public IP address (accessible over the Internet)
    or an internal IP address accessible between each data center. Again, the Admin
    region will use this URL to make service calls, so the endpoint must be reachable.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be required to register two types of endpoints: **public** and **internal**.
    I discovered this key component during the setup process. Some of the OpenStack
    services leverage the internal endpoints solely, whereas others will use the public
    endpoints. In order to avoid any issues, we will register both. Technically, there
    is zero risk to registering both and it is a good practice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of the command to register the service is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'A set of working examples of the command would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The preceding step needs to be repeated for every active region you wish to
    join under the Admin region. As depicted in the earlier example, we would execute
    this step for **Region A** and **Region B**.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting the Admin regions' identity endpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last step in setting up the Admin region is to make sure that the active
    regions can successfully connect to the identity service running there. The same
    principle shared earlier about having to expose the services public endpoint applies
    here as well  for Keystone . Every cloud setup may differ slightly, so this step
    may not be required for all clouds.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to evaluate if you need to make this adjustment, execute the following
    command and determine if the public and admin endpoints have local IP addresses
    configured for the URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If the output looks similar to this, you must disable the public and admin
    endpoints after creating new ones with either a public IP or IP address accessible
    between data centers. More details on how to handle this will be shared here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adjusting the Admin regions'' identity endpoint](graphics/image_08_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to create the new public and admin endpoints and then disable the
    current ones, you would execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once completed, upon issuing the `openstack endpoint list --service identity`
    command, the output should look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adjusting the Admin regions'' identity endpoint](graphics/image_08_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Active region configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section will include the steps to setting up the active regions that will
    be part of your Active-Active cloud design. These are the regions running the
    core OpenStack services. At this point, we have the Admin region set up to communicate
    with these active regions. Now we must configure the core services to authenticate
    through the Admin region instead of using the local identity service (Keystone).
  prefs: []
  type: TYPE_NORMAL
- en: You cannot deploy an OpenStack cloud without first setting up a local identity
    service. The identity service has to be the first service installed and thus would
    exist on the active regions. To have the services not use the local identity service,
    you must reconfigure each service. Simply disabling the local identity service
    is not enough to make this work. The process of reconfiguring each core service
    includes editing the configuration file. As already mentioned earlier, editing
    the service configuration files leaves the door open to make edits by mistake
    that could then lead to that service not starting.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where you must work smarter and not harder. Ask yourself: Is there
    a tool that can assist in such a task? Yes, the answer yet again is Ansible! Ansible
    can assist in making those many service configuration changes greatly minimizing
    typos. In [Chapter 2](ch02.html "Chapter 2. Introduction to Ansible"), *Introduction
    to Ansible*, we briefly discussed Ansible ad hoc commands. Ad hoc commands allow
    direct module commands to be run without wrapping the task into a playbook or
    role.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A basic example of an ad hoc command would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In our situation, we need to connect to a specific container running on the
    control plane and make a change to that services configuration file. This needs
    to be repeated for each core service running on that active region. Good news
    is that we can leverage the dynamic inventory part of the openstack-ansible deployment
    to make the overall process simpler. Let's use the following example as a sample
    to show how it can be accomplished.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will attempt to make the required changes to the image
    service (Glance) on the Alpha region. So, the things we know are:'
  prefs: []
  type: TYPE_NORMAL
- en: You must connect to the Glance container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `sed` command, we will need to leverage the shell Ansible module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a `sed` command prepared that will change the `auth_url` value within
    the `glance-api.conf` file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A further breakdown of the command parameters would now be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to leverage the dynamic inventory feature part of the *openstack-ansible
    install*, you must execute these commands from the deployment node (the node used
    to deploy the region). As well as, you much execute the commands while within
    the `/opt/openstack-ansible/playbooks` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'A working example of the command would then look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You can use the preceding principles to make the required changes to all the
    services on your active regions. Make sure to remember to restart the services
    after the configuration file changes are made.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Coding the playbooks and roles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will now create the playbooks and roles to set up the Admin
    region. We will also then outline the Ansible ad hoc commands needed to complete
    the other steps to set up the Active-Active clouds. When creating Ansible automation
    code for something of this nature, I typically like to create multiple tasks broken
    out into separate roles. This format allows you to be able to reuse roles created
    with other playbooks. We will end up with two playbooks and two roles to automate
    the steps to set up the Admin region. In the end, we will then recap the playbooks
    consuming those roles.
  prefs: []
  type: TYPE_NORMAL
- en: In the other half of this section, we will also outline the Ansible ad hoc commands
    needed to complete the other steps to set up the Active-Active clouds. You could
    surely collect the commands together to create playbooks and roles. I felt that
    this would be a few hundred lines of unnecessary code, so I went with drafting
    the commands and using search-and-replace.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Admin region
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first role we will create will include those tasks needed to configure
    the Admin region. The name of the file will be `main.yml`, located within the
    role directory named `config-admin-region/tasks`. The contents of this file will
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The first task will create the service user accounts on the Admin region. The
    second task will then assign the admin role to the users just created. The last
    task will then create the placeholder for the services hosted on the active regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next role to be created will handle the tasks of registering each region''s
    endpoints within the Admin region. Just as with the previous role, the file will
    be named `main.yml`, located within the role directory named `register-endpoints/tasks`.
    The contents of this file will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The role only has one task that is to use the CLI command of service endpoint
    `create to register` the endpoints. In this circumstance, we used the `with_together`
    parameter so that we could loop through the four parameters defined as variables.
    This way you can rerun the playbook with only having to adjust the variable values.
    As in our case, we would need to run this playbook twice, one for internal endpoints
    and one for the public endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: To support these roles, we now need to create the variable file that will go
    along with it. For these two roles, we will use role-defined variable files to
    simplify things a bit. The variable file will be stored within the `role` directory
    inside another directory named `vars`. The file inside of that directory will
    be named `main.yml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The contents of the variable file corresponding to the role named `config-admin-region`
    will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The contents of the second variable file corresponding to the role named `register-endpoints`
    will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Keep in mind that the values defined in the variable file are intended to be
    changed before each execution for normal everyday use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a moment to break down the variables and their expected use. The
    summary would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'With the variable file completed, we can move on to creating the master playbook
    files. For our demonstration, I decided to break up the playbook files into two
    separate files. This was totally my choice and could be combined into one file
    with no issues. I felt that having two separate master playbooks would make it
    easier to rerun when you need to register multiple sets of endpoints. The list
    of playbook files will be described here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The playbook and role names can be anything you choose. Specific names have
    been provided here in order to allow you to easily follow along and reference
    the completed code found in the GitHub repository. The only warning is whatever
    you decide to name the roles must remain uniform when referenced from within the
    playbook(s).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Active regions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is where we will use the Ansible ad hoc commands to finish up the configuration.
    As mentioned earlier, we will leverage the dynamic inventory capabilities part
    of openstack-ansible deployment model to accomplish this. These commands will
    reconfigure the OpenStack services to use the Admin region to authenticate. Here
    is a snippet of the commands that you need to execute to reconfigure the core
    services on each region becoming part of the Active-Active region setup.  The
    full list of commands can be found in the **os-admin-with-ansible/os-admin-with-ansible-v2**
    Github repository within a file named `configure-region-authentication.txt` located
    on the `root` directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The approach I have found to be the best and most efficient is to do a search
    for the placeholder of `<admin region IP>` and replace it with the public IP or
    internal IP associated with Admin region. You can do it with any text editor and
    it can be set with commands to execute against any region.
  prefs: []
  type: TYPE_NORMAL
- en: Well done everyone! You just configured your OpenStack cloud with multiple regions
    that are all active. As always, for us to keep up with our tradition, we will
    finish up the chapter with a quick review of the playbook and role just created.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing playbooks and roles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's jump right into examining the roles we created.
  prefs: []
  type: TYPE_NORMAL
- en: 'The completed role and file, named `main.yml`, located in the `config-admin-region/tasks`
    directory, looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The completed role and file, named `main.yml`, located in the `register-endpoints/tasks`
    directory, looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding role local variable files are both named `main.yml` and are
    saved to the `vars` directory of the role:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we created the following master playbook files; all will be located in
    the `root` directory of the `playbook` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`config-admin.yml`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`register-endpoints.yml`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In the end of this, we created the `hosts` file, which also is located in the
    `root` directory of the `playbook` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The complete set of code can again be found in the GitHub repository at [https://github.com/os-admin-with-ansible/os-admin-with-ansible-v2](https://github.com/os-admin-with-ansible/os-admin-with-ansible-v2).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the fun part, time to test out our new playbooks and roles. You will need
    to also execute the additional ad hoc commands described earlier to completely
    test out this functionality. Assuming that you have cloned the GitHub repository
    mentioned earlier, the command to test out the playbook from the Deployment node
    would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you would execute the commands found in the file named `configure-region-authentication.txt`,
    located in the `root` directory of the `playbook` directory. If all went well
    you would be able to log into the web dashboard of the Admin region and see the
    following when you click on the project name on the top header of the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reviewing playbooks and roles](graphics/B06086_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yes! You just set up your OpenStack cloud in an Active-Active design. The flexibility
    and reliability you just gained solves for most mainstream HA requirements. Have
    fun jumping between the regions and separating out your application resources
    within one or two clicks. Before concluding this chapter, let's take a moment
    to recap this chapter. We talked through the benefits that OpenStack offers out
    of the box to handle high-availability requirements. Then, we transitioned to
    some possible reasons you would want to use Active-Active cloud regions. Next,
    we walked through the steps of how to set up the Active-Active cloud regions.
    Finally, we developed Ansible playbooks and roles to automate setting up the Admin
    region.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter happens to also be something that came in as a customer demand
    for a pretty large OpenStack cloud. There is no cloud operator out there who does
    not want to know or have a complete inventory of their cloud. Tracking resources,
    auditing users, and recapping network utilization are just a few things part of
    the daily/weekly routine for us. Imagine that you can have a complete report created
    in one command. Is it possible? Well I am not telling. You will have to read on
    to [Chapter 9](ch09.html "Chapter 9. Inventory Your Cloud"), *Inventory your Cloud*,
    to find out.
  prefs: []
  type: TYPE_NORMAL
