- en: Machine Learning with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we get into machine learning and how to actually implement
    machine learning models in Python.
  prefs: []
  type: TYPE_NORMAL
- en: We'll examine what supervised and unsupervised learning means, and how they're
    different from each other. We'll see techniques to prevent overfitting, and then
    look at an interesting example where we implement a spam classifier. We'll analyze
    what K-Means clustering is a long the way, with a working example that clusters
    people based on their income and age using scikit-learn!
  prefs: []
  type: TYPE_NORMAL
- en: We'll also cover a really interesting application of machine learning called
    **decision trees** and we'll build a working example in Python that predict shiring
    decisions in a company. Finally, we'll walk through the fascinating concepts of
    ensemble learning and SVMs, which are some of my favourite machine learning areas!
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised and unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding overfitting by using train/test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of an e-mail spam classifier with NaÃ¯ve Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concept of K-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example of clustering in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entropy and how to measure it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concept of decision trees and its example in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is ensemble learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support Vector Machine** (**SVM**) and its example using scikit-learn'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning and train/test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So what is machine learning? Well, if you look it up on Wikipedia or whatever,
    it'll say that it is algorithms that can learn from observational data and can
    make predictions based on it. It sounds really fancy, right? Like artificial intelligence
    stuff, like you have a throbbing brain inside of your computer. But in reality,
    these techniques are usually very simple.
  prefs: []
  type: TYPE_NORMAL
- en: We've already looked at regressions, where we took a set of observational data,
    we fitted a line to it, and we used that line to make predictions. So by our new
    definition, that was machine learning! And your brain works that way too.
  prefs: []
  type: TYPE_NORMAL
- en: Another fundamental concept in machine learning is something called **train**/**test**,
    which lets us very cleverly evaluate how good a machine learning model we've made.
    As we look now at unsupervised and supervised learning, you'll see why train/test
    is so important to machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s talk in detail now about two different types of machine learning: supervised
    and unsupervised learning. Sometimes there can be kind of a blurry line between
    the two, but the basic definition of unsupervised learning is that you''re not
    giving your model any answers to learn from. You''re just presenting it with a
    group of data and your machine learning algorithm tries to make sense out of it
    given no additional information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54c8b166-bc40-4c03-8a14-a47bd74e93dc.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's say I give it a bunch of different objects, like these balls and cubes
    and sets of dice and what not. Let's then say have some algorithm that will cluster
    these objects into things that are similar to each other based on some similarity
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: Now I haven't told the machine learning algorithm, ahead of time, what categories
    certain objects belong to. I don't have a cheat sheet that it can learn from where
    I have a set of existing objects and my correct categorization of it. The machine
    learning algorithm must infer those categories on its own. This is an example
    of unsupervised learning, where I don't have a set of answers that I'm getting
    it learn from. I'm just trying to let the algorithm gather its own answers based
    on the data presented to it alone.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this is that we don't necessarily know what the algorithm will
    come up with! If I gave it that bunch of objects shown in the preceding image,
    is it going to group things into things that are round, things that are large
    versus small, things that are red versus blue, I don't know. It's going to depend
    on the metric that I give it for similarity between items primarily. But sometimes
    you'll find clusters that are surprising, and emerged that you didn't expect to
    see.
  prefs: []
  type: TYPE_NORMAL
- en: 'So that''s really the point of unsupervised learning: if you don''t know what
    you''re looking for, it can be a powerful tool for discovering classifications
    that you didn''t even know were there. We call this a **latent variable**. Some
    property of your data that you didn''t even know was there originally, can be
    teased out by unsupervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's take another example around unsupervised learning. Say I was clustering
    people instead of balls and dice. I'm writing a dating site and I want to see
    what sorts of people tend to cluster together. There are some attributes that
    people tend to cluster around, which decide whether they tend to like each other
    and date each other for example. Now you might find that the clusters that emerge
    don't conform to your predisposed stereotypes. Maybe it's not about college students
    versus middle-aged people, or people who are divorced and whatnot, or their religious
    beliefs. Maybe if you look at the clusters that actually emerged from that analysis,
    you'll learn something new about your users and actually figure out that there's
    something more important than any of those existing features of your people that
    really count toward, to decide whether they like each other. So that's an example
    of supervised learning providing useful results.
  prefs: []
  type: TYPE_NORMAL
- en: Another example could be clustering movies based on their properties. If you
    were to run clustering on a set of movies from like IMDb or something, maybe the
    results would surprise you. Perhaps it's not just about the genre of the movie.
    Maybe there are other properties, like the age of the movie or the running length
    or what country it was released in, that are more important. You just never know.
    Or we could analyze the text of product descriptions and try to find the terms
    that carry the most meaning for a certain category. Again, we might not necessarily
    know ahead of time what terms, or what words, are most indicative of a product
    being in a certain category; but through unsupervised learning, we can tease out
    that latent information.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now in contrast, supervised learning is a case where we have a set of answers
    that the model can learn from. We give it a set of training data, that the model
    learns from. It can then infer relationships between the features and the categories
    that we want, and apply that to unseen new values - and predict information about
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to our earlier example, where we were trying to predict car prices
    based on the attributes of those cars. That's an example where we are training
    our model using actual answers. So I have a set of known cars and their actual
    prices that they sold for. I train the model on that set of complete answers,
    and then I can create a model that I'm able to use to predict the prices of new
    cars that I haven't seen before. That's an example of supervised learning, where
    you're giving it a set of answers to learn from. You've already assigned categories
    or some organizing criteria to a set of data, and your algorithm then uses that
    criteria to build a model from which it can predict new values.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So how do you evaluate supervised learning? Well, the beautiful thing about
    supervised learning is that we can use a trick called train/test. The idea here
    is to split our observational data that I want my model to learn from into two
    groups, a training set and a testing set. So when I train/build my model based
    on the data that I have, I only do that with part of my data that I'm calling
    my training set, and I reserve another part of my data that I'm going to use for
    testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: I can build my model using a subset of my data for training data, and then I'm
    in a position to evaluate the model that comes out of that, and see if it can
    successfully predict the correct answers for my testing data.
  prefs: []
  type: TYPE_NORMAL
- en: So you see what I did there? I have a set of data where I already have the answers
    that I can train my model from, but I'm going to withhold a portion of that data
    and actually use that to test my model that was generated using the training set!
    That it gives me a very concrete way to test how good my model is on unseen data
    because I actually have a bit of data that I set aside that I can test it with.
  prefs: []
  type: TYPE_NORMAL
- en: You can then measure quantitatively how well it did using r-squared or some
    other metric, like root-mean-square error, for example. You can use that to test
    one model versus another and see what the best model is for a given problem. You
    can tune the parameters of that model and use train/test to maximize the accuracy
    of that model on your testing data. So this is a great way to prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: There are some caveats to supervised learning. need to make sure that both your
    training and test datasets are large enough to actually be representative of your
    data. You also need to make sure that you're catching all the different categories
    and outliers that you care about, in both training and testing, to get a good
    measure of its success, and to build a good model.
  prefs: []
  type: TYPE_NORMAL
- en: You have to make sure that you've selected from those datasets randomly, and
    that you're not just carving your dataset in two and saying everything left of
    here is training and right here is testing. You want to sample that randomly,
    because there could be some pattern sequentially in your data that you don't know
    about.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if your model is overfitting, and just going out of its way to accept outliers
    in your training data, then that's going to be revealed when you put it against
    unset scene of testing data. This is because all that gyrations for outliers won't
    help with the outliers that it hasn't seen before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s be clear here that train/test is not perfect, and it is possible to
    get misleading results from it. Maybe your sample sizes are too small, like we
    already talked about, or maybe just due to random chance your training data and
    your test data look remarkably similar, they actually do have a similar set of
    outliers - and you can still be overfitting. As you can see in the following example,
    it really can happen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfd6dacd-41fc-415b-ab5b-dda9abf05c09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: K-fold cross validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now there is a way around this problem, called k-fold cross-validation, and
    we'll look at an example of this later in the book, but the basic concept is you
    train/test many times. So you actually split your data not into just one training
    set and one test set, but into multiple randomly assigned segments, k segments.
    That's where the k comes from. And you reserve one of those segments as your test
    data, and then you start training your model on the remaining segments and measure
    their performance against your test dataset. Then you take the average performance
    from each of those training sets' models' results and take their r-squared average
    score.
  prefs: []
  type: TYPE_NORMAL
- en: So this way, you're actually training on different slices of your data, measuring
    them against the same test set, and if you have a model that's overfitting to
    a particular segment of your training data, then it will get averaged out by the
    other ones that are contributing to k-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the K-fold cross validation steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Split your data into K randomly-assigned segments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reserve one segment as your test data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train on each of the remaining K-1 segments and measure their performance against
    the test set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the average of the K-1 r-squared scores
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will make more sense later in the book, right now I would just like for
    you to know that this tool exists for actually making train/test even more robust
    than it already is. So let's go and actually play with some data and actually
    evaluate it using train/test next.
  prefs: []
  type: TYPE_NORMAL
- en: Using train/test to prevent overfitting of a polynomial regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's put train/test into action. So you might remember that a regression can
    be thought of as a form of supervised machine learning. Let's just take a polynomial
    regression, which we covered earlier, and use train/test to try to find the right
    degree polynomial to fit a given set of data.
  prefs: []
  type: TYPE_NORMAL
- en: Just like in our previous example, we're going to set up a little fake dataset
    of randomly generated page speeds and purchase amounts, and I'm going to create
    a quirky little relationship between them that's exponential in nature.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go ahead and generate that data. We''ll use a normal distribution of
    random data for both page speeds and purchase amount using the relationship as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a215de99-d26a-4838-9482-274b254b3d20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Next, we'll split that data. We'll take 80% of our data, and we're going to
    reserve that for our training data. So only 80% of these points are going to be
    used for training the model, and then we're going to reserve the other 20% for
    testing that model against unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use Python''s syntax here for splitting the list. The first 80 points
    are going to go to the training set, and the last 20, everything after 80, is
    going to go to test set. You may remember this from our Python basics chapter
    earlier on, where we covered the syntax to do this, and we''ll do the same thing
    for purchase amounts here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now in our earlier sections, I've said that you shouldn't just slice your dataset
    in two like this, but that you should randomly sample it for training and testing.
    In this case though, it works out because my original data was randomly generated
    anyway, so there's really no rhyme or reason to where things fell. But in real-world
    data you'll want to shuffle that data before you split it.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look now at a handy method that you can use for that purpose of shuffling
    your data. Also, if you're using the pandas package, there's some handy functions
    in there for making training and test datasets automatically for you. But we're
    going to do it using a Python list here. So let's visualize our training dataset
    that we ended up with. We'll do a scatter plot of our training page speeds and
    purchase amounts.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what your output should now look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0fc1eec-b655-424f-956f-798327e5a2f8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Basically, 80 points that were selected at random from the original complete
    dataset have been plotted. It has basically the same shape, so that's a good thing.
    It's representative of our data. That's important!
  prefs: []
  type: TYPE_NORMAL
- en: Now let's plot the remaining 20 points that we reserved as test data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/534acada-d2fb-4cc7-b4b7-82c40fb4fd64.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we see our remaining 20 for testing also has the same general shape as
    our original data. So I think that's a representative test set too. It's a little
    bit smaller than you would like to see in the real world, for sure. You probably
    get a little bit of a better result if you had 1,000 points instead of 100, for
    example, to choose from and reserved 200 instead of 20.
  prefs: []
  type: TYPE_NORMAL
- en: Now we're going to try to fit an 8th degree polynomial to this data, and we'll
    just pick the number `8` at random because I know it's a really high order and
    is probably overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and fit our 8th degree polynomial using `np.poly1d(np.polyfit(x,
    y, 8))`, where *x* is an array of the training data only, and *y* is an array
    of the training data only. We are finding our model using only those 80 points
    that we reserved for training. Now we have this `p4` function that results that
    we can use to predict new values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we''ll plot the polynomial this came up with against the training data.
    We can scatter our original data for the training data set, and then we can plot
    our predicted values against them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see in the following graph that it looks like a pretty good fit, but
    you know that clearly it''s doing some overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16b521a0-d571-4cd7-a5a8-e5557dcb7c1f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'What''s this craziness out at the right? I''m pretty sure our real data, if
    we had it out there, wouldn''t be crazy high, as this function would implicate.
    So this is a great example of overfitting your data. It fits the data you gave
    it very well, but it would do a terrible job of predicting new values beyond the
    point where the graph is going crazy high on the right. So let''s try to tease
    that out. Let''s give it our test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, if we plot our test data against that same function, well, it doesn't
    actually look that bad.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b6b4a61-0d3b-45a7-b7fd-4bfb2d6ad3fa.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We got lucky and none of our test is actually out here to begin with, but you
    can see that it''s a reasonable fit, but far from perfect. And in fact, if you
    actually measure the r-squared score, it''s worse than you might think. We can
    measure that using the `r2_score()` function from `sklearn.metrics`. We just give
    it our original data and our predicted values and it just goes through and measures
    all the variances from the predictions and squares them all up for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We end up with an r-squared score of just `0.3`. So that''s not that hot! You
    can see that it fits the training data a lot better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The r-squared value turns out to be `0.6`, which isn't too surprising, because
    we trained it on the training data. The test data is sort of its unknown, its
    test, and it did fail the test, quite frankly. 30%, that's an F!
  prefs: []
  type: TYPE_NORMAL
- en: So this has been an example where we've used train/test to evaluate a supervised
    learning algorithm, and like I said before, pandas has some means of making this
    even easier. We'll look at that a little bit later, and we'll also look at more
    examples of train/test, including k-fold cross validation, later in the book as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can probably guess what your homework is. So we know that an 8th order polynomial
    isn't very useful. Can you do better? So I want you to go back through our example,
    and use different values for the degree polynomial that you're going to use to
    fit. Change that 8 to different values and see if you can figure out what degree
    polynomial actually scores best using train/test as a metric. Where do you get
    your best r-squared score for your test data? What degree fits here? Go play with
    that. It should be a pretty easy exercise and a very enlightening one for you
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: So that's train/test in action, a very important technique to have under your
    belt, and you're going to use it over and over again to make sure that your results
    are a good fit for the model that you have, and that your results are a good predictor
    of unseen values. It's a great way to prevent overfitting when you're doing your
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian methods - Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Did you ever wonder how the spam classifier in your e-mail works? How does it
    know that an e-mail might be spam or not? Well, one popular technique is something
    called Naive Bayes, and that's an example of a Bayesian method. Let's learn more
    about how that works. Let's discuss Bayesian methods.
  prefs: []
  type: TYPE_NORMAL
- en: We did talk about Bayes' theorem earlier in this book in the context of talking
    about how things like drug tests could be very misleading in their results. But
    you can actually apply the same Bayes' theorem to larger problems, like spam classifiers.
    So let's dive into how that might work, it's called a Bayesian method.
  prefs: []
  type: TYPE_NORMAL
- en: 'So just a refresher on Bayes'' theorem -remember, the probability of A given
    B is equal to the overall probability of A times the probability of B given A
    over the overall probability of B:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95ca9a4e-01b9-4dbe-a066-5ff2db89e08d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'How can we use that in machine learning? I can actually build a spam classifier
    for that: an algorithm that can analyze a set of known spam e-mails and a known
    set of non-spam e-mails, and train a model to actually predict whether new e-mails
    are spam or not. This is a real technique used in actual spam classifiers in the
    real world.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s just figure out the probability of an e-mail being spam
    given that it contains the word "free". If people are promising you free stuff,
    it''s probably spam! So let''s work that out. The probability of an email being
    spam given that you have the word "free" in that e-mail works out to the overall
    probability of it being a spam message times the probability of containing the
    word "free" given that it''s spam over the probability overall of being free:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6992cb5a-12dd-4397-b787-571b1d13777c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The numerator can just be thought of as the probability of a message being
    `Spam` and containing the word `Free`. But that''s a little bit different than
    what we''re looking for, because that''s the odds out of the complete dataset
    and not just the odds within things that contain the word `Free`. The denominator
    is just the overall probability of containing the word `Free`. Sometimes that
    won''t be immediately accessible to you from the data that you have. If it''s
    not, you can expand that out to the following expression if you need to derive
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca351b51-13d1-432c-814b-43bcf6a55db0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This gives you the percentage of e-mails that contain the word "free" that are
    spam, which would be a useful thing to know when you're trying to figure out if
    it's spam or not.
  prefs: []
  type: TYPE_NORMAL
- en: What about all the other words in the English language, though? So our spam
    classifier should know about more than just the word "free". It should automatically
    pick up every word in the message, ideally, and figure out how much does that
    contribute to the likelihood of a particular e-mail being spam. So what we can
    do is train our model on every word that we encounter during training, throwing
    out things like "a" and "the" and "and" and meaningless words like that. Then
    when we go through all the words in a new e-mail, we can multiply the probability
    of being spam for each word together, and we get the overall probability of that
    e-mail being spam.
  prefs: []
  type: TYPE_NORMAL
- en: Now it's called Naive Bayes for a reason. It's naive is because we're assuming
    that there's no relationships between the words themselves. We're just looking
    at each word in isolation, individually within a message, and basically combining
    all the probabilities of each word's contribution to it being spam or not. We're
    not looking at the relationships between the words. So a better spam classifier
    would do that, but obviously that's a lot harder.
  prefs: []
  type: TYPE_NORMAL
- en: So this sounds like a lot of work. But the overall idea is not that hard, and
    scikit-learn in Python makes it actually pretty easy to do. It offers a feature
    called **CountVectorizer** that makes it very simple to actually split up an e-mail
    to all of its component words and process those words individually. Then it has
    a `MultinomialNB` function, where NB stands for Naive Bayes, which will do all
    the heavy lifting for Naive Bayes for us.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a spam classifier with Naïve Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s write a spam classifier using Naive Bayes. You''re going to be surprised
    how easy this is. In fact, most of the work ends up just being reading all the
    input data that we''re going to train on and actually parsing that data in. The
    actual spam classification bit, the machine learning bit, is itself just a few
    lines of code. So that''s usually how it works out: reading in and massaging and
    cleaning up your data is usually most of the work when you''re doing data science,
    so get used to the idea!'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: So the first thing we need to do is read all those e-mails in somehow, and we're
    going to again use pandas to make this a little bit easier. Again, pandas is a
    useful tool for handling tabular data. We import all the different packages that
    we're going to use within our example here, that includes the os library, the
    io library, numpy, pandas, and CountVectorizer and MultinomialNB from scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through this code in detail now. We can skip past the function definitions
    of `readFiles()` and `dataFrameFromDirectory()`for now and go down to the first
    thing that our code actually does which is to create a pandas DataFrame object.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to construct this from a dictionary that initially contains a
    little empty list for messages in an empty list of class. So this syntax is saying,
    "I want a DataFrame that has two columns: one that contains the message, the actual
    text of each e-mail; and one that contains the class of each e-mail, that is,
    whether it''s spam or ham". So it''s saying I want to create a little database
    of e-mails, and this database has two columns: the actual text of the e-mail and
    whether it''s spam or not.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we needed to put something in that database, that is, into that DataFrame,
    in Python syntax. So we call the two methods `append()` and `dataFrameFromDirectory()`
    to actually throw into the DataFrame all the spam e-mails from my `spam` folder,
    and all the ham e-mails from the `ham` folder.
  prefs: []
  type: TYPE_NORMAL
- en: If you are playing along here, make sure you modify the path passed to the `dataFrameFromDirectory()`
    function to match wherever you installed the book materials in your system! And
    again, if you're on Mac or Linux, please pay attention to backslashes and forward
    slashes and all that stuff. In this case, it doesn't matter, but you won't have
    a drive letter, if you're not on Windows. So just make sure those paths are actually
    pointing to where your `spam` and `ham` folders are for this example.
  prefs: []
  type: TYPE_NORMAL
- en: Next, `dataFrameFromDirectory()` is a function I wrote, which basically says
    I have a path to a directory, and I know it's given classification, spam or ham,
    then it uses the `readFiles()` function, that I also wrote, which will iterate
    through every single file in a directory. So `readFiles()` is using the `os.walk()`
    function to find all the files in a directory. Then it builds up the full pathname
    for each individual file in that directory, and then it reads it in. And while
    it's reading it in, it actually skips the header for each e-mail and just goes
    straight to the text, and it does that by looking for the first blank line.
  prefs: []
  type: TYPE_NORMAL
- en: It knows that everything after the first empty line is actually the message
    body, and everything in front of that first empty line is just a bunch of header
    information that I don't actually want to train my spam classifier on. So it gives
    me back both, the full path to each file and the body of the message. So that's
    how we read in all of the data, and that's the majority of the code!
  prefs: []
  type: TYPE_NORMAL
- en: 'So what I have at the end of the day is a DataFrame object, basically a database
    with two columns, that contains message bodies, and whether it''s spam or not.
    We can go ahead and run that, and we can use the `head` command from the DataFrame
    to actually preview what this looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The first few entries in our DataFrame look like this: for each path to a given
    file full of e-mails we have a classification and we have the message body:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5eb289f1-dcfb-460a-984b-6b07d9c8dc56.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Alright, now for the fun part, we're going to use the `MultinomialNB()` function
    from scikit-learn to actually perform Naive Bayes on the data that we have.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what your output should now look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/389232ea-b526-4051-9297-89bc6f7f7d09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Once we build a `MultinomialNB` classifier, it needs two inputs. It needs the
    actual data that we're training on (`counts`), and the targets for each thing
    (`targets`). So `counts` is basically a list of all the words in each e-mail and
    the number of times that word occurs.
  prefs: []
  type: TYPE_NORMAL
- en: 'So this is what `CountVectorizer()` does: it takes the `message` column from
    the DataFrame and takes all the values from it. I''m going to call `vectorizer.fit_transform`
    which basically tokenizes or converts all the individual words seen in my data
    into numbers, into values. It then counts up how many times each word occurs.'
  prefs: []
  type: TYPE_NORMAL
- en: This is a more compact way of representing how many times each word occurs in
    an e-mail. Instead of actually preserving the words themselves, I'm representing
    those words as different values in a sparse matrix, which is basically saying
    that I'm treating each word as a number, as a numerical index, into an array.
    What that does is, just in plain English, it split each message up into a list
    of words that are in it, and counts how many times each word occurs. So we're
    calling that `counts`. It's basically that information of how many times each
    word occurs in each individual message. Mean while `targets` is the actual classification
    data for each e-mail that I've encountered. So I can call `classifier.fit()` using
    my `MultinomialNB()` function to actually create a model using Naive Bayes, which
    will predict whether new e-mails are spam or not based on the information we've
    given it.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go ahead and run that. It runs pretty quickly! I'm going to use a couple
    of examples here. Let's try a message body that just says `Free Money now!!!`
    which is pretty clearly spam, and a more innocent message that just says `"Hi
    Bob, how about a game of golf tomorrow?"` So we're going to pass these in.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing we do is convert the messages into the same format that I trained
    my model on. So I use that same vectorizer that I created when creating the model
    to convert each message into a list of words and their frequencies, where the
    words are represented by positions in an array. Then once I''ve done that transformation,
    I can actually use the `predict()` function on my classifier, on that array of
    examples that have transformed into lists of words, and see what we come up with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9cd78d24-18d0-4700-b154-aa2ac869322b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: And sure enough, it works! So, given this array of two input messages, `Free
    Money now!!!` and `Hi Bob`, it's telling me that the first result came back as
    spam and the second result came back as ham, which is what I would expect. That's
    pretty cool. So there you have it.
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We had a pretty small dataset here, so you could try running some different
    e-mails through it if you want and see if you get different results. If you really
    want to challenge yourself, try applying train/test to this example. So the real
    measure of whether or not my spam classifier is good or not is not just intuitively
    whether it can figure out that `Free Money now!!!` is spam. You want to measure
    that quantitatively.
  prefs: []
  type: TYPE_NORMAL
- en: So if you want a little bit of a challenge, go ahead and try to split this data
    up into a training set and a test dataset. You can actually look up online how
    pandas can split data up into train sets and testing sets pretty easily for you,
    or you can do it by hand. Whatever works for you. See if you can actually apply
    your `MultinomialNB` classifier to a test dataset and measure its performance.
    So, if you want a little bit of an exercise, a little bit of a challenge, go ahead
    and give that a try.
  prefs: []
  type: TYPE_NORMAL
- en: How cool is that? We just wrote our own spam classifier just using a few lines
    of code in Python. It's pretty easy using scikit-learn and Python. That's Naive
    Bayes in action, and you can actually go and classify some spam or ham messages
    now that you have that under your belt. Pretty cool stuff. Let's talk about clustering
    next.
  prefs: []
  type: TYPE_NORMAL
- en: K-Means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we're going to talk about k-means clustering, and this is an unsupervised
    learning technique where you have a collection of stuff that you want to group
    together into various clusters. Maybe it's movie genres or demographics of people,
    who knows? But it's actually a pretty simple idea, so let's see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering is a very common technique in machine learning where you
    just try to take a bunch of data and find interesting clusters of things just
    based on the attributes of the data itself. Sounds fancy, but it's actually pretty
    simple. All we do in k-means clustering is try to split our data into K groups
    - that's where the K comes from, it's how many different groups you're trying
    to split your data into - and it does this by finding K centroids.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, basically, what group a given data point belongs to is defined by which
    of these centroid points it''s closest to in your scatter plot. You can visualize
    this in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df576023-6bcc-4691-9d5d-baa565c15445.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is showing an example of k-means clustering with K of three, and the squares
    represent data points in a scatter plot. The circles represent the centroids that
    the k-means clustering algorithm came up with, and each point is assigned a cluster
    based on which centroid it's closest to. So that's all there is to it, really.
    It's an example of unsupervised learning. It isn't a case where we have a bunch
    of data and we already know the correct cluster for a given set of training data;
    rather, you're just given the data itself and it tries to converge on these clusters
    naturally just based on the attributes of the data alone. It's also an example
    where you are trying to find clusters or categorizations that you didn't even
    know were there. As with most unsupervised learning techniques, the point is to
    find latent values, things you didn't really realize were there until the algorithm
    showed them to you.
  prefs: []
  type: TYPE_NORMAL
- en: For example, where do millionaires live? I don't know, maybe there is some interesting
    geographical cluster where rich people tend to live, and k-means clustering could
    help you figure that out. Maybe I don't really know if today's genres of music
    are meaningful. What does it mean to be alternative these days? Not much, right?
    But by using k-means clustering on attributes of songs, maybe I could find interesting
    clusters of songs that are related to each other and come up with new names for
    what those clusters represent. Or maybe I can look at demographic data, and maybe
    existing stereotypes are no longer useful. Maybe Hispanic has lost its meaning
    and there's actually other attributes that define groups of people, for example,
    that I could uncover with clustering. Sounds fancy, doesn't it? Really complicated
    stuff. Unsupervised machine learning with K clusters, it sounds fancy, but as
    with most techniques in data science, it's actually a very simple idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the algorithm for us in plain English:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Randomly pick K centroids (k-means):** We start off with a randomly chosen
    set of centroids. So if we have a K of three we''re going to look for three clusters
    in our group, and we will assign three randomly positioned centroids in our scatter
    plot.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Assign each data point to the centroid it is closest to:** We then assign
    each data point to the randomly assigned centroid that it is closest to.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Recompute the centroids based on the average position of each centroid''s
    points**: Then recompute the centroid for each cluster that we come up with. That
    is, for a given cluster that we end up with, we will move that centroid to be
    the actual center of all those points.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterate until points stop changing assignment to centroids:** We will do
    it all again until those centroids stop moving, we hit some threshold value that
    says OK, we have converged on something here.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Predict the cluster for new points:** To predict the clusters for new points
    that I haven''t seen before, we can just go through our centroid locations and
    figure out which centroid it''s closest to to predict its cluster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's look at a graphical example to make a little bit more sense. We'll call
    the first figure in the following image as A, second as B, third as C and the
    fourth as D.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1e5ec68-fe61-4e99-b6e6-395b3354fef1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The gray squares in image A represent data points in our scatter plot. The axes
    represent some different features of something. Maybe it's age and income; it's
    an example I keep using, but it could be anything. And the gray squares might
    represent individual people or individual songs or individual something that I
    want to find relationships between.
  prefs: []
  type: TYPE_NORMAL
- en: So I start off by just picking three points at random on my scatterplot. Could
    be anywhere. Got to start somewhere, right? The three points (centroids) I selected
    have been shown as circles in image A. So the next thing I'm going to do is for
    each centroid I'll compute which one of the gray points it's closest to. By doing
    that, the points shaded in blue are associated with this blue centroid. The green
    points are closest to the green centroid, and this single red point is closest
    to that red random point that I picked out.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can see that's not really reflective of where the actual clusters
    appear to be. So what I'm going to do is take the points that ended up in each
    cluster and compute the actual center of those points. For example, in the green
    cluster, the actual center of all data turns out to be a little bit lower. We're
    going to move the centroid down a little bit. The red cluster only had one point,
    so its center moves down to where that single point is. And the blue point was
    actually pretty close to the center, so that just moves a little bit. On this
    next iteration we end up with something that looks like image D. Now you can see
    that our cluster for red things has grown a little bit and things have moved a
    little bit, that is, those got taken from the green cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If we do that again, you can probably predict what's going to happen next. The
    green centroid will move a little bit, the blue centroid will still be about where
    it is. But at the end of the day you're going to end up with the clusters you'd
    probably expect to see. That's how k-means works. So it just keeps iterating,
    trying to find the right centroids until things start moving around and we converge
    on a solution.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations to k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So there are some limitations to k-means clustering. Here they are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Choosing K:** First of all, we need to choose the right value of K, and that''s
    not a straightforward thing to do at all. The principal way of choosing K is to
    just start low and keep increasing the value of K depending on how many groups
    you want, until you stop getting large reductions in squared error. If you look
    at the distances from each point to their centroids, you can think of that as
    an error metric. At the point where you stop reducing that error metric, you know
    you probably have too many clusters. So you''re not really gaining any more information
    by adding additional clusters at that point.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Avoiding local minima:** Also, there is a problem of local minima. You could
    just get very unlucky with those initial choices of centroids and they might end
    up just converging on local phenomena instead of more global clusters, so usually,
    you want to run this a few times and maybe average the results together. We call
    that ensemble learning. We''ll talk about that more a little bit later on, but
    it''s always a good idea to run k-means more than once using a different set of
    random initial values and just see if you do in fact end up with the same overall
    results or not.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Labeling the clusters:** Finally, the main problem with k-means clustering
    is that there''s no labels for the clusters that you get. It will just tell you
    that this group of data points are somehow related, but you can''t put a name
    on it. It can''t tell you the actual meaning of that cluster. Let''s say I have
    a bunch of movies that I''m looking at, and k-means clustering tells me that bunch
    of science fiction movies are over here, but it''s not going to call them "science
    fiction" movies for me. It''s up to me to actually dig into the data and figure
    out, well, what do these things really have in common? How might I describe that
    in English? That''s the hard part, and k-means won''t help you with that. So again,
    scikit-learn makes it very easy to do this.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's now work up an example and put k-means clustering into action.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering people based on income and age
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see just how easy it is to do k-means clustering using scikit-learn and
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we're going to do is create some random data that we want to
    try to cluster. Just to make it easier, we'll actually build some clusters into
    our fake test data. So let's pretend there's some real fundamental relationship
    between these data, and there are some real natural clusters that exist in it.
  prefs: []
  type: TYPE_NORMAL
- en: 'So to do that, we can work with this little `createClusteredData()` function
    in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The function starts off with a consistent random seed so you'll get the same
    result every time. We want to create clusters of N people in k clusters. So we
    pass `N` and `k` to `createClusteredData().`
  prefs: []
  type: TYPE_NORMAL
- en: Our code figures out how many points per cluster that works out to first and
    stores it in `pointsPerCluster`. Then, it builds up list `X` that starts off empty.
    For each cluster, we're going to create some random centroid of income (`incomeCentroid`)
    between 20,000 and 200,000 dollars and some random centroid of age (`ageCentroid`)
    between the age of 20 and 70.
  prefs: []
  type: TYPE_NORMAL
- en: What we're doing here is creating a fake scatter plot that will show income
    versus age for `N` people and `k` clusters. So for each random centroid that we
    created, I'm then going to create a normally distributed set of random data with
    a standard deviation of 10,000 in income and a standard deviation of 2 in age.
    That will give us back a bunch of age income data that is clustered into some
    pre-existing clusters that we can chose at random. OK, let's go ahead and run
    that.
  prefs: []
  type: TYPE_NORMAL
- en: Now, to actually do k-means, you'll see how easy it is.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: All you need to do is import `KMeans` from scikit-learn's `cluster` package.
    We're also going to import `matplotlib` so we can visualize things, and also import
    `scale` so we can take a look at how that works.
  prefs: []
  type: TYPE_NORMAL
- en: So we use our `createClusteredData()` function to say 100 random people around
    5 clusters. So there are 5 natural clusters for the data that I'm creating. We
    then create a model, a KMeans model with k of 5, so we're picking 5 clusters because
    we know that's the right answer. But again, in unsupervised learning you don't
    necessarily know what the real value of `k` is. You need to iterate and converge
    on it yourself. And then we just call `model.fit` using my KMeans `model` using
    the data that we had.
  prefs: []
  type: TYPE_NORMAL
- en: Now the scale I alluded to earlier, that's normalizing the data. One important
    thing with k-means is that it works best if your data is all normalized. That
    means everything is at the same scale. So a problem that I have here is that my
    ages range from 20 to 70, but my incomes range all the way up to 200,000\. So
    these values are not really comparable. The incomes are much larger than the age
    values. `Scale` will take all that data and scale it together to a consistent
    scale so I can actually compare these things as apples to apples, and that will
    help a lot with your k-means results.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, once we''ve actually called `fit` on our model, we can actually look at
    the resulting labels that we got. Then we can actually visualize it using a little
    bit of `matplotlib` magic. You can see in the code we have a little trick where
    we assigned the color to the labels that we ended up with converted to some floating
    point number. That''s just a little trick you can use to assign arbitrary colors
    to a given value. So let''s see what we end up with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5799932c-1451-4c6a-a10a-14372dbc79b6.jpg)![](img/e988ba57-00c7-4e2e-a3c5-45cf7165bd4c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It didn't take that long. You see the results are basically what clusters I
    assigned everything into. We know that our fake data is already pre-clustered,
    so it seems that it identified the first and second clusters pretty easily. It
    got a little bit confused beyond that point, though, because our clusters in the
    middle are actually a little bit mushed together. They're not really that distinct,
    so that was a challenge for k-means. But regardless, it did come up with some
    reasonable guesses at the clusters. This is probably an example of where four
    clusters would more naturally fit the data.
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So what I want you to do for an activity is to try a different value of k and
    see what you end up with. Just eyeballing the preceding graph, it looks like four
    would work well. Does it really? What happens if I increase k too large? What
    happens to my results? What does it try to split things into, and does it even
    make sense? So, play around with it, try different values of `k`. So in the `n_clusters()`
    function, change the 5 to something else. Run all through it again and see you
    end up with.
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s all there is to k-means clustering. It''s just that simple. You can
    just use scikit-learn''s `KMeans` thing from `cluster`. The only real gotcha:
    make sure you scale the data, normalize it. You want to make sure the things that
    you''re using k-means on are comparable to each other, and the `scale()` function
    will do that for you. So those are the main things for k-means clustering. Pretty
    simple concept, even simpler to do it using scikit-learn.'
  prefs: []
  type: TYPE_NORMAL
- en: That's all there is to it. That's k-means clustering. So if you have a bunch
    of data that is unclassified and you don't really have the right answers ahead
    of time, it's a good way to try to naturally find interesting groupings of your
    data, and maybe that can give you some insight into what that data is. It's a
    good tool to have. I've used it before in the real world and it's really not that
    hard to use, so keep that in your tool chest.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quite soon we're going to get to one of the cooler parts of machine learning,
    at least I think so, called decision trees. But before we can talk about that,
    it's a necessary to understand the concept of entropy in data science.
  prefs: []
  type: TYPE_NORMAL
- en: So entropy, just like it is in physics and thermodynamics, is a measure of a
    dataset's disorder, of how same or different the dataset is. So imagine we have
    a dataset of different classifications, for example, animals. Let's say I have
    a bunch of animals that I have classified by species. Now, if all of the animals
    in my dataset are an iguana, I have very low entropy because they're all the same.
    But if every animal in my dataset is a different animal, I have iguanas and pigs
    and sloths and who knows what else, then I would have a higher entropy because
    there's more disorder in my dataset. Things are more different than they are the
    same.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy is just a way of quantifying that sameness or difference throughout
    my data. So, an entropy of 0 implies all the classes in the data are the same,
    whereas if everything is different, I would have a high entropy, and something
    in between would be a number in between. Entropy just describes how same or different
    the things in a dataset are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now mathematically, it''s a little bit more involved than that, so when I actually
    compute a number for entropy, it''s computed using the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7dd736b4-b705-487c-94e6-df5a8640a08a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So for every different class that I have in my data, I''m going to have one
    of these p terms, p[1], p[2], and so on and so forth through p[n], for n different
    classes that I might have. The p just represents the proportion of the data that
    is that class. And if you actually plot what this looks like for each term- `pi*
    ln * pi`, it''ll look a little bit something like the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3ae9e6d-662c-4568-b28f-e9fed56739e7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You add these up for each individual class. For example, if the proportion of
    the data, that is, for a given class is 0, then the contribution to the overall
    entropy is 0\. And if everything is that class, then again the contribution to
    the overall entropy is 0 because in either case, if nothing is this class or everything
    is this class, that's not really contributing anything to the overall entropy.
  prefs: []
  type: TYPE_NORMAL
- en: It's the things in the middle that contribute entropy of the class, where there's
    some mixture of this classification and other stuff. When you add all these terms
    together, you end up with an overall entropy for the entire dataset. So mathematically,
    that's how it works out, but again, the concept is very simple. It's just a measure
    of how disordered your dataset, how same or different the things in your data
    are.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees - Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Believe it or not, given a set of training data, you can actually get Python
    to generate a flowchart for you to make a decision. So if you have something you're
    trying to predict on some classification, you can use a decision tree to actually
    look at multiple attributes that you can decide upon at each level in the flowchart.
    You can print out an actual flowchart for you to use to make a decision from,
    based on actual machine learning. How cool is that? Let's see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: I personally find decision trees are one of the most interesting applications
    of machine learning. A decision tree basically gives you a flowchart of how to
    make some decision.You have some dependent variable, like whether or not I should
    go play outside today or not based on the weather. When you have a decision like
    that that depends on multiple attributes or multiple variables, a decision tree
    could be a good choice.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different aspects of the weather that might influence my decision
    of whether I should go outside and play. It might have to do with the humidity,
    the temperature, whether it's sunny or not, for example. A decision tree can look
    at all these different attributes of the weather, or anything else, and decide
    what are the thresholds? What are the decisions I need to make on each one of
    those attributes before I arrive at a decision of whether or not I should go play
    outside? That's all a decision tree is. So it's a form of supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way it would work in this example would be as follows. I would have some
    sort of dataset of historical weather, and data about whether or not people went
    outside to play on a particular day. I would feed the model this data of whether
    it was sunny or not on each day, what the humidity was, and if it was windy or
    not; and whether or not it was a good day to go play outside. Given that training
    data, a decision tree algorithm can then arrive at a tree that gives us a flowchart
    that we can print out. It looks just like the following flow chart. You can just
    walk through and figure out whether or not it''s a good day to play outside based
    on the current attributes. You can use that to predict the decision for a new
    set of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0d013a2-8886-4d48-8a56-3b4e166a551a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How cool is that? We have an algorithm that will make a flowchart for you automatically
    just based on observational data. What's even cooler is how simple it all works
    once you learn how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's say I want to build a system that will automatically filter out resumes
    based on the information in them. A big problem that technology companies have
    is that we get tons and tons of resumes for our positions. We have to decide who
    we actually bring in for an interview, because it can be expensive to fly somebody
    out and actually take the time out of the day to conduct an interview. So what
    if there were a way to actually take historical data on who actually got hired
    and map that to things that are found on their resume?
  prefs: []
  type: TYPE_NORMAL
- en: We could construct a decision tree that will let us go through an individual
    resume and say, "OK, this person actually has a high likelihood of getting hired,
    or not". We can train a decision tree on that historical data and walk through
    that for future candidates. Wouldn't that be a wonderful thing to have?
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s make some totally fabricated hiring data that we''re going to use
    in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4a0232c-b18c-41a8-a032-92b7466ca4a0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding table, we have candidates that are just identified by numerical
    identifiers. I'm going to pick some attributes that I think might be interesting
    or helpful to predict whether or not they're a good hire or not. How many years
    of experience do they have? Are they currently employed? How many employers have
    they had previous to this one? What's their level of education? What degree do
    they have? Did they go to what we classify as a top-tier school? Did they do an
    internship while they were in college? We can take a look at this historical data,
    and the dependent variable here is `Hired`. Did this person actually get a job
    offer or not based on that information?
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, obviously there''s a lot of information that isn''t in this model that
    might be very important, but the decision tree that we train from this data might
    actually be useful in doing an initial pass at weeding out some candidates. What
    we end up with might be a tree that looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47031034-ef5a-4c31-b66f-63953d852abf.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So it just turns out that in my totally fabricated data, anyone that did an
    internship in college actually ended up getting a job offer. So my first decision
    point is "did this person do an internship or not?" If yes, go ahead and bring
    them in. In my experience, internships are actually a pretty good predictor of
    how good a person is. If they have the initiative to actually go out and do an
    internship, and actually learn something at that internship, that's a good sign.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do they currently have a job? Well, if they are currently employed, in my very
    small fake dataset it turned out that they are worth hiring, just because somebody
    else thought they were worth hiring too. Obviously it would be a little bit more
    of a nuanced decision in the real world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If they're not currently employed, do they have less than one prior employer?
    If yes, this person has never held a job and they never did an internship either.
    Probably not a good hire decision. Don't hire that person.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But if they did have a previous employer, did they at least go to a top-tier
    school? If not, it's kind of iffy. If so, then yes, we should hire this person
    based on the data that we trained on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walking through a decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So that's how you walk through the results of a decision tree. It's just like
    going through a flowchart, and it's kind of awesome that an algorithm can produce
    this for you. The algorithm itself is actually very simple. Let me explain how
    the algorithm works.
  prefs: []
  type: TYPE_NORMAL
- en: 'At each step of the decision tree flowchart, we find the attribute that we
    can partition our data on that minimizes the entropy of the data at the next step.
    So we have a resulting set of classifications: in this case hire or don''t hire,
    and we want to choose the attribute decision at that step that will minimize the
    entropy at the next step.'
  prefs: []
  type: TYPE_NORMAL
- en: At each step we want to make all of the remaining choices result in either as
    many no hires or as many hire decisions as possible. We want to make that data
    more and more uniform so as we work our way down the flowchart, and we ultimately
    end up with a set of candidates that are either all hires or all no hires so we
    can classify into yes/no decisions on a decision tree. So we just walk down the
    tree, minimize entropy at each step by choosing the right attribute to decide
    on, and we keep on going until we run out.
  prefs: []
  type: TYPE_NORMAL
- en: There's a fancy name for this algorithm. It's called **ID3** (**Iterative Dichotomiser
    3**). It is what's known as a greedy algorithm. So as it goes down the tree, it
    just picks the attribute that will minimize entropy at that point. Now that might
    not actually result in an optimal tree that minimizes the number of choices that
    you have to make, but it will result in a tree that works, given the data that
    you gave it.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests technique
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now one problem with decision trees is that they are very prone to overfitting,
    so you can end up with a decision tree that works beautifully for the data that
    you trained it on, but it might not be that great for actually predicting the
    correct classification for new people that it hasn't seen before. Decision trees
    are all about arriving at the right decision for the training data that you gave
    it, but maybe you didn't really take into account the right attributes, maybe
    you didn't give it enough of a representative sample of people to learn from.
    This can result in real problems.
  prefs: []
  type: TYPE_NORMAL
- en: So to combat this issue, we use a technique called random forests, where the
    idea is that we sample the data that we train on, in different ways, for multiple
    different decision trees. Each decision tree takes a different random sample from
    our set of training data and constructs a tree from it. Then each resulting tree
    can vote on the right result.
  prefs: []
  type: TYPE_NORMAL
- en: Now that technique of randomly resampling our data with the same model is a
    term called bootstrap aggregating, or bagging. This is a form of what we call
    ensemble learning, which we'll cover in more detail shortly. But the basic idea
    is that we have multiple trees, a forest of trees if you will, each that uses
    a random subsample of the data that we have to train on. Then each of these trees
    can vote on the final result, and that will help us combat overfitting for a given
    set of training data.
  prefs: []
  type: TYPE_NORMAL
- en: The other thing random forests can do is actually restrict the number of attributes
    that it can choose, between at each stage, while it is trying to minimize the
    entropy as it goes. And we can randomly pick which attributes it can choose from
    at each level. So that also gives us more variation from tree to tree, and therefore
    we get more of a variety of algorithms that can compete with each other. They
    can all vote on the final result using slightly different approaches to arriving
    at the same answer.
  prefs: []
  type: TYPE_NORMAL
- en: So that's how random forests work. Basically, it is a forest of decision trees
    where they are drawing from different samples and also different sets of attributes
    at each stage that it can choose between.
  prefs: []
  type: TYPE_NORMAL
- en: So, with all that, let's go make some decision trees. We'll use random forests
    as well when we're done, because scikit-learn makes it really really easy to do,
    as you'll see soon.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees - Predicting hiring decisions using Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Turns out that it's easy to make decision trees; in fact it's crazy just how
    easy it is, with just a few lines of Python code. So let's give it a try.
  prefs: []
  type: TYPE_NORMAL
- en: I've included a `PastHires.csv` file with your book materials, and that just
    includes some fabricated data, that I made up, about people that either got a
    job offer or not based on the attributes of those candidates.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You'll want to please immediately change that path I used here for my own system
    (`c:/spark/DataScience/PastHires.csv`) to wherever you have installed the materials
    for this book. I'm not sure where you put it, but it's almost certainly not there.
  prefs: []
  type: TYPE_NORMAL
- en: We will use `pandas` to read our CSV in, and create a DataFrame object out of
    it. Let's go ahead and run our code, and we can use the `head()` function on the
    DataFrame to print out the first few lines and make sure that it looks like it
    makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Sure enough we have some valid data in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39aa9b7f-af63-4996-ae11-fc8d96174087.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, for each candidate ID, we have their years of past experience, whether or
    not they were employed, their number of previous employers, their highest level
    of education, whether they went to a top-tier school, and whether they did an
    internship; and finally here, in the Hired column, the answer - where we knew
    that we either extended a job offer to this person or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, most of the work is just in massaging your data, preparing your data,
    before you actually run the algorithms on it, and that''s what we need to do here.
    Now scikit-learn requires everything to be numerical, so we can''t have Ys and
    Ns and BSs and MSs and PhDs. We have to convert all those things to numbers for
    the decision tree model to work. The way to do this is to use some short-hand
    in pandas, which makes these things easy. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Basically, we're making a dictionary in Python that maps the letter Y to the
    number 1, and the letter N to the value 0\. So, we want to convert all our Ys
    to 1s and Ns to 0s. So 1 will mean yes and 0 will mean no. What we do is just
    take the Hired column from the DataFrame, and call `map()` on it, using a dictionary.
    This will go through the entire Hired column, in the entire DataFrame and use
    that dictionary lookup to transform all the entries in that column. It returns
    a new DataFrame column that I'm putting back into the Hired column. This replaces
    the Hired column with one that's been mapped to 1s and 0s.
  prefs: []
  type: TYPE_NORMAL
- en: 'We do the same thing for Employed, Top-tier school and Interned, so all those
    get mapped using the yes/no dictionary. So, the Ys and Ns become 1s and 0s instead.
    For the Level of Education, we do the same trick, we just create a dictionary
    that assigns BS to 0, MS to 1, and PhD to 2 and uses that to remap those degree
    names to actual numerical values. So if I go ahead and run that and do a `head()`
    again, you can see that it worked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f975ef7-dde7-4e2c-988f-a115252ad95d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: All my yeses are 1's, my nos are 0's, and my Level of Education is now represented
    by a numerical value that has real meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Next we need to prepare everything to actually go into our decision tree classifier,
    which isn't that hard. To do that, we need to separate our feature information,
    which are the attributes that we're trying to predict from, and our target column,
    which contains the thing that we're trying to predict.To extract the list of feature
    name columns, we are just going to create a list of columns up to number 6\. We
    go ahead and print that out.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b121561b-c7b2-4418-aad6-bc33f5c608d2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Above are the column names that contain our feature information: Years Experience,
    Employed?, Previous employers, Level of Education, Top-tier school, and Interned.
    These are the attributes of candidates that we want to predict hiring on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we construct our *y* vector which is assigned what we''re trying to predict,
    that is our Hired column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This code extracts the entire Hired column and calls it `y`. Then it takes all
    of our columns for the feature data and puts them in something called `X`. This
    is a collection of all of the data and all of the feature columns, and `X` and
    `y` are the two things that our decision tree classifier needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To actually create the classifier itself, two lines of code: we call `tree.DecisionTreeClassifier()`
    to create our classifier, and then we fit it to our feature data (`X`) and the
    answers (`y`)- whether or not people were hired. So, let''s go ahead and run that.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Displaying graphical data is a little bit tricky, and I don''t want to distract
    us too much with the details here, so please just consider the following boilerplate
    code. You don''t need to get into how Graph viz works here - and dot files and
    all that stuff: it''s not important to our journey right now. The code you need
    to actually display the end results of a decision tree is simply:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: So let's go ahead and run this.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what your output should now look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19fbf5b0-4476-46de-897e-ce3bb7a5237f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There we have it! How cool is that?! We have an actual flow chart here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let me show you how to read it. At each stage, we have a decision. Remember
    most of our data which is yes or no, is going to be **0** or **1**. So, the first
    decision point becomes: is Employed? less than **0.5**? Meaning that if we have
    an employment value of 0, that is no, we''re going to go left.If employment is
    1, that is yes, we''re going to go right.'
  prefs: []
  type: TYPE_NORMAL
- en: So, were they previously employed? If not go left, if yes go right. It turns
    out that in my sample data, everyone who is currently employed actually got a
    job offer, so I can very quickly say if you are currently employed, yes, you're
    worth bringing in, we're going to follow down to the second level here.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do you interpret this? The gini score is basically a measure of entropy
    that it's using at each step. Remember as we're going down the algorithm is trying
    to minimize the amount of entropy. And the samples are the remaining number of
    samples that haven't beensectioned off by a previous decision.
  prefs: []
  type: TYPE_NORMAL
- en: So say this person was employed. The way to read the right leaf node is the
    value column that tells you at this point we have 0 candidates that were no hires
    and 5 that were hires. So again, the way to interpret the first decision point
    is if Employed? was 1, I'm going to go to the right, meaning that they are currently
    employed, and this brings me to a world where everybody got a job offer. So, that
    means I should hire this person.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's say that this person doesn't currently have a job. The next thing
    I'm going to look at is, do they have an internship. If yes, then we're at a point
    where in our training data everybody got a job offer. So, at that point, we can
    say our entropy is now 0 (`gini=0.0000`), because everyone's the same, and they
    all got an offer at that point. However, you know if we keep going down(where
    the person has not done an internship),we'll be at a point where the entropy is
    0.32\. It's getting lower and lower, that's a good thing.
  prefs: []
  type: TYPE_NORMAL
- en: Next we're going to look at how much experience they have, do they have less
    than one year of experience? And, if the case is that they do have some experience
    and they've gotten this far they're a pretty good no hire decision. We end up
    at the point where we have zero entropy but, all three remaining samples in our
    training set were no hires. We have 3 no hires and 0 hires. But, if they do have
    less experience, then they're probably fresh out of college, they still might
    be worth looking at.
  prefs: []
  type: TYPE_NORMAL
- en: The final thing we're going to look at is whether or not they went to a Top-tier
    school, and if so, they end up being a good prediction for being a hire. If not,
    they end up being a no hire. We end up with one candidate that fell into that
    category that was a no hire and 0 that were a hire. Whereas, in the case candidates
    did go to a top tier school, we have 0 no hires and 1 hire.
  prefs: []
  type: TYPE_NORMAL
- en: So, you can see we just keep going until we reach an entropy of 0, if at all
    possible, for every case.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning – Using a random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's say we want to use a random forest, you know, we're worried that
    we might be over fitting our training data. It's actually very easy to create
    a random forest classifier of multiple decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to do that, we can use the same data that we created before. You just need
    your `*X*` and `*y*` vectors, that is the set of features and the column that
    you''re trying to predict on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We make a random forest classifier, also available from scikit-learn, and pass
    it the number of trees we want in our forest. So, we made ten trees in our random
    forest in the code above. We then fit that to the model.
  prefs: []
  type: TYPE_NORMAL
- en: You don't have to walk through the trees by hand, and when you're dealing with
    a random forest you can't really do that anyway. So, instead we use the `predict()`
    function on the model, that is on the classifier that we made. We pass in a list
    of all the different features for a given candidate that we want to predict employment
    for.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you remember this maps to these columns: Years Experience, Employed?, Previous
    employers, Level of Education, Top-tier school, and Interned; interpreted as numerical
    values. We predict the employment of an employed 10-year veteran. We also predict
    the employment of an unemployed 10-year veteran. And, sure enough, we get a result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76760aeb-0024-4cec-8ca3-5cd9c64c2ebe.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, in this particular case, we ended up with a hire decision on both. But,
    what's interesting is there is a random component to that. You don't actually
    get the same result every time! More often than not, the unemployed person does
    not get a job offer, and if you keep running this you'll see that's usually the
    case. But, the random nature of bagging, of bootstrap aggregating each one of
    those trees, means you're not going to get the same result every time. So, maybe
    10 isn't quite enough trees. So, anyway, that's a good lesson to learn here!
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For an activity, if you want to go back and play with this, mess around with
    my input data. Go ahead and edit the code we've been exploring, and create an
    alternate universe where it's a topsy turvy world; for example, everyone that
    I gave a job offer to now doesn't get one and vice versa. See what that does to
    your decision tree. Just mess around with it and see what you can do and try to
    interpret the results.
  prefs: []
  type: TYPE_NORMAL
- en: So, that's decision trees and random forests, one of the more interesting bits
    of machine learning, in my opinion. I always think it's pretty cool to just generate
    a flowchart out of thin air like that. So, hopefully you'll find that useful.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talked about random forests, that was an example of ensemble learning,
    where we're actually combining multiple models together to come up with a better
    result than any single model could come up with. So, let's learn about that in
    a little bit more depth. Let's talk about ensemble learning a little bit more.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, remember random forests? We had a bunch of decision trees that were using
    different subsamples of the input data, and different sets of attributes that
    it would branch on, and they all voted on the final result when you were trying
    to classify something at the end. That''s an example of ensemble learning. Another
    example: when we were talking about k-means clustering, we had the idea of maybe
    using different k-means models with different initial random centroids, and letting
    them all vote on the final result as well. That is also an example of ensemble
    learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Basically, the idea is that you have more than one model, and they might be
    the same kind of model or it might be different kinds of models, but you run them
    all, on your set of training data, and they all vote on the final result for whatever
    it is you're trying to predict. And oftentimes, you'll find that this ensemble
    of different models produces better results than any single model could on its
    own.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good example, from a few years ago, was the Netflix prize. Netflix ran a
    contest where they offered, I think it was a million dollars, to any researcher
    who could outperform their existing movie recommendation algorithm. The ones that
    won were ensemble approaches, where they actually ran multiple recommender algorithms
    at once and let them all vote on the final result. So, ensemble learning can be
    a very powerful, yet simple tool, for increasing the quality of your final results
    in machine learning. Let us now try to explore various types of ensemble learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bootstrap aggregating or bagging:** Now, random forests use a technique called
    bagging, short for bootstrap aggregating. This means that we take random subsamples
    of our training data and feed them into different versions of the same model and
    let them all vote on the final result. If you remember, random forests took many
    different decision trees that use a different random sample of the training data
    to train on, and then they all came together in the end to vote on a final result.
    That''s bagging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting:** Boosting is an alternate model, and the idea here is that you
    start with a model, but each subsequent model boosts the attributes that address
    the areas that were misclassified by the previous model. So, you run train/tests
    on a model, you figure out what are the attributes that it''s basically getting
    wrong, and then you boost those attributes in subsequent models - in hopes that
    those subsequent models will pay more attention to them, and get them right. So,
    that''s the general idea behind boosting. You run a model, figure out its weak
    points, amplify the focus on those weak points as you go, and keep building more
    and more models that keep refining that model, based on the weaknesses of the
    previous one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bucket of models:** Another technique, and this is what that Netflix prize-winner
    did, is called a bucket of models, where you might have entirely different models
    that try to predict something. Maybe I''m using k-means, a decision tree, and
    regression. I can run all three of those models together on a set of training
    data and let them all vote on the final classification result when I''m trying
    to predict something. And maybe that would be better than using any one of those
    models in isolation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stacking:** Stacking has the same idea. So, you run multiple models on the
    data, combine the results together somehow. The subtle difference here between
    bucket of models and stacking, is that you pick the model that wins. So, you''d
    run train/test, you find the model that works best for your data, and you use
    that model. By contrast, stacking will combine the results of all those models
    together, to arrive at a final result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, there is a whole field of research on ensemble learning that tries to find
    the optimal ways of doing ensemble learning, and if you want to sound smart, usually
    that involves using the word Bayes a lot. So, there are some very advanced methods
    of doing ensemble learning but all of them have weak points, and I think this
    is yet another lesson in that we should always use the simplest technique that
    works well for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now these are all very complicated techniques that I can''t really get into
    in the scope of this book, but at the end of the day, it''s hard to outperform
    just the simple techniques that we''ve already talked about. A few of the complex
    techniques are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayes optical classifier:** In theory, there''s something called the Bayes
    Optimal Classifier that will always be the best, but it''s impractical, because
    it''s computationally prohibitive to do it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian parameter averaging:** Many people have tried to do variations of
    the Bayes Optimal Classifier to make it more practical, like the Bayesian Parameter
    Averaging variation. But it''s still susceptible to overfitting and it''s often
    outperformed by bagging, which is the same idea behind random forests; you just
    resample the data multiple times, run different models, and let them all vote
    on the final result. Turns out that works just as well, and it''s a heck of a
    lot simpler!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian model combination:** Finally, there''s something called Bayesian
    Model Combination that tries to solve all the shortcomings of Bayes Optimal Classifier
    and Bayesian Parameter Averaging. But, at the end of the day, it doesn''t do much
    better than just cross validating against the combination of models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, these are very complex techniques that are very difficult to use. In
    practice, we're better off with the simpler ones that we've talked about in more
    detail. But, if you want to sound smart and use the word Bayes a lot it's good
    to be familiar with these techniques at least, and know what they are.
  prefs: []
  type: TYPE_NORMAL
- en: So, that's ensemble learning. Again, the takeaway is that the simple techniques,
    like bootstrap aggregating, or bagging, or boosting, or stacking, or bucket of
    models, are usually the right choices. There are some much fancier techniques
    out there but they're largely theoretical. But, at least you know about them now.
  prefs: []
  type: TYPE_NORMAL
- en: It's always a good idea to try ensemble learning out. It's been proven time
    and time again that it will produce better results than any single model, so definitely
    consider it!
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machine overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we're going to talk about **support vector machines** (**SVM**), which
    is a very advanced way of clustering or classifying higher dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: So, what if you have multiple features that you want to predict from? SVM can
    be a very powerful tool for doing that, and the results can be scarily good! It's
    very complicated under the hood, but the important things are understanding when
    to use it, and how it works at a higher level. So, let's cover SVM now.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines is a fancy name for what actually is a fancy concept.
    But fortunately, it's pretty easy to use. The important thing is knowing what
    it does, and what it's good for. So, support vector machines works well for classifying
    higher-dimensional data, and by that I mean lots of different features. So, it's
    easy to use something like k-means clustering, to cluster data that has two dimensions,
    you know, maybe age on one axis and income on another. But, what if I have many,
    many different features that I'm trying to predict from. Well, support vector
    machines might be a good way of doing that.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines finds higher-dimensional support vectors across which
    to divide the data (mathematically, these support vectors define hyperplanes).
    That is, mathematically, what support vector machines can do is find higher dimensional
    support vectors (that's where it gets its name from) that define the higher-dimensional
    planes that split the data into different clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously the math gets pretty weird pretty quickly with all this. Fortunately,
    the `scikit-learn` package will do it all for you, without you having to actually
    get into it. Under the hood, you need to understand though that it uses something
    called the kernel trick to actually find those support vectors or hyperplanes
    that might not be apparent in lower dimensions. There are different kernels you
    can use, to do this in different ways. The main point is that SVM's are a good
    choice if you have higher- dimensional data with lots of different features, and
    there are different kernels you can use that have varying computational costs
    and might be better fits for the problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: The important point is that SVMs employ some advanced mathematical trickery
    to cluster data, and it can handle data sets with lots of features. It's also
    fairly expensive - the "kernel trick" is the only thing that makes it possible.
  prefs: []
  type: TYPE_NORMAL
- en: I want to point out that SVM is a supervised learning technique. So, we're actually
    going to train it on a set of training data, and we can use that to make predictions
    for future unseen data or test data. It's a little bit different than k-means
    clustering and that k-means was completely unsupervised; with a support vector
    machine, by contrast, it is training based on actual training data where you have
    the answer of the correct classification for some set of data that it can learn
    from. So, SVM's are useful for classification and clustering, if you will - but
    it's a supervised technique!
  prefs: []
  type: TYPE_NORMAL
- en: One example that you often see with SVMs is using something called support vector
    classification. The typical example uses the Iris dataset which is one of the
    sample datasets that comes with scikit-learn. This set is a classification of
    different flowers, different observations of different Iris flowers and their
    species. The idea is to classify these using information about the length and
    width of the petal on each flower, and the length and width of the sepal of each
    flower. (The sepal, apparently, is a little support structure underneath the petal.
    I didn't know that until now either.) You have four dimensions of attributes there;
    you have the length and width of the petal, and the length and the width of the
    sepal. You can use that to predict the species of an Iris given that information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example of doing that with SVC: basically, we have sepal width and
    sepal length projected down to two dimensions so we can actually visualize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11af3028-7b21-40ac-94ae-b9db94d97655.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With different kernels you might get different results. SVC with a linear kernel
    will produce something very much as you see in the preceding image. You can use
    polynomial kernels or fancier kernels that might project down to curves in two
    dimensions as shown in the image. You can do some pretty fancy classification
    this way.
  prefs: []
  type: TYPE_NORMAL
- en: These have increasing computational costs, and they can produce more complex
    relationships. But again, it's a case where too much complexity can yield misleading
    results, so you need to be careful and actually use train/test when appropriate.
    Since we are doing supervised learning, you can actually do train/test and find
    the right model that works, or maybe use an ensemble approach.
  prefs: []
  type: TYPE_NORMAL
- en: You need to arrive at the right kernel for the task at hand. For things like
    polynomial SVC, what's the right degree polynomial to use? Even things like linear
    SVC will have different parameters associated with them that you might need to
    optimize for. This will make more sense with a real example, so let's dive into
    some actual Python code and see how it works!
  prefs: []
  type: TYPE_NORMAL
- en: Using SVM to cluster people by using scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's try out some support vector machines here. Fortunately, it's a lot easier
    to use than it is to understand. We're going to go back to the same example I
    used for k-means clustering, where I'm going to create some fabricated cluster
    data about ages and incomes of a hundred random people.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to go back to the k-means clustering section, you can learn more
    about kind of the idea behind this code that generates the fake data. And if you''re
    ready, please consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Please note that because we're using supervised learning here, we not only need
    the feature data again, but we also need the actual answers for our training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: What the `createClusteredData()` function does here, is to create a bunch of
    random data for people that are clustered around `k` points, based on age and
    income, and it returns two arrays. The first array is the feature array, that
    we're calling `X`, and then we have the array of the thing we're trying to predict
    for, which we're calling `y`. A lot of times in scikit-learn when you're creating
    a model that you can predict from, those are the two inputs that it will take,
    a list of feature vectors, and the thing that you're trying to predict, that it
    can learn from. So, we'll go ahead and run that.
  prefs: []
  type: TYPE_NORMAL
- en: 'So now we''re going to use the `createClusteredData()` function to create 100
    random people with 5 different clusters. We will just create a scatter plot to
    illustrate those, and see where they land up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The following graph shows our data that we're playing with. Every time you run
    this you're going to get a different set of clusters. So, you know, I didn't actually
    use a random seed... to make life interesting.
  prefs: []
  type: TYPE_NORMAL
- en: 'A couple of new things here--I''m using the `figsize` parameter on `plt.figure()`
    to actually make a larger plot. So, if you ever need to adjust the size in `matplotlib`,
    that''s how you do it. I''m using that same trick of using the color as the classification
    number that I end up with. So the number of the cluster that I started with is
    being plotted as the color of these data points. You can see, it''s a pretty challenging
    problem, there''s definitely some intermingling of clusters here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18ecc82e-4a00-47b9-b93e-7b7de259f4ab.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now we can use linear SVC (SVC is a form of SVM), to actually partition that
    into clusters. We're going to use SVM with a linear kernel, and with a C value
    of `1.0`. C is just an error penalty term that you can adjust; it's `1` by default.
    Normally, you won't want to mess with that, but if you're doing some sort of convergence
    on the right model using ensemble learning or train/test, that's one of the things
    you can play with. Then, we will fit that model to our feature data, and the actual
    classifications that we have for our training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: So, let's go ahead and run that. I don't want to get too much into how we're
    actually going to visualize the results here, just take it on faith that `plotPredictions()`
    is a function that can plot the classification ranges and SVC.
  prefs: []
  type: TYPE_NORMAL
- en: 'It helps us visualize where different classifications come out. Basically,
    it''s creating a mesh across the entire grid, and it will plot different classifications
    from the SVC models as different colors on that grid, and then we''re going to
    plot our original data on top of that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s see how that works out. SVC is computationally expensive, so it
    takes a long time to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1342ff6-9d4a-4546-a76a-ed1092efdbb0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can see here that it did its best. Given that it had to draw straight lines,
    and polygonal shapes, it did a decent job of fitting to the data that we had.
    So, you know, it did miss a few - but by and large, the results are pretty good.
  prefs: []
  type: TYPE_NORMAL
- en: 'SVC is actually a very powerful technique; it''s real strength is in higher
    dimensional feature data. Go ahead and play with it. By the way if you want to
    not just visualize the results, you can use the `predict()` function on the SVC
    model, just like on pretty much any model in scikit-learn, to pass in a feature
    array that you''re interested in. If I want to predict the classification for
    someone making $200,000 a year who was 40 years old, I would use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This would put that person in, in our case, cluster number 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a7954d2-6d7d-482b-a3dc-05b75deeab61.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If I had a someone making $50,000 here who was 65, I would use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what your output should now look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30e1d81a-b05c-48a1-9f17-ac864d6e9cd6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That person would end up in cluster number 2, whatever that represents in this
    example. So, go ahead and play with it.
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, linear is just one of many kernels that you can use, like I said there
    are many different kernels you can use. One of them is a polynomial model, so
    you might want to play with that. Please do go ahead and look up the documentation.
    It's good practice for you to looking at the docs. If you're going to be using
    scikit-learn in any sort of depth, there's a lot of different capabilities and
    options that you have available to you. So, go look up scikit-learn online, find
    out what the other kernels are for the SVC method, and try them out, see if you
    actually get better results or not.
  prefs: []
  type: TYPE_NORMAL
- en: This is a little exercise, not just in playing with SVM and different kinds
    of SVC, but also in familiarizing yourself with how to learn more on your own
    about SVC. And, honestly, a very important trait of any data scientist or engineer
    is going to be the ability to go and look up information yourself when you don't
    know the answers.
  prefs: []
  type: TYPE_NORMAL
- en: So, you know, I'm not being lazy by not telling you what those other kernels
    are, I want you to get used to the idea of having to look this stuff up on your
    own, because if you have to ask someone else about these things all the time you're
    going to get really annoying, really fast in a workplace. So, go look that up,
    play around it, see what you come up with.
  prefs: []
  type: TYPE_NORMAL
- en: So, that's SVM/SVC, a very high power technique that you can use for classifying
    data, in supervised learning. Now you know how it works and how to use it, so
    keep that in your bag of tricks!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw some interesting machine learning techniques. We covered
    one of the fundamental concepts behind machine learning called train/test. We
    saw how to use train/test to try to find the right degree polynomial to fit a
    given set of data. We then analyzed the difference between supervised and unsupervised
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how to implement a spam classifier and enable it to determine whether
    an email is spam or not using the Naive Bayes technique. We talked about k-means
    clustering, an unsupervised learning technique, which helps group data into clusters.
    We also looked at an example using scikit-learn which clustered people based on
    their income and age.
  prefs: []
  type: TYPE_NORMAL
- en: We then went on to look at the concept of entropy and how to measure it. We
    walked through the concept of decision trees and how, given a set of training
    data, you can actually get Python to generate a flowchart for you to actually
    make a decision. We also built a system that automatically filters out resumes
    based on the information in them and predicts the hiring decision of a person.
  prefs: []
  type: TYPE_NORMAL
- en: We learned along the way the concept of ensemble learning, and we concluded
    by talking about support vector machines, which is a very advanced way of clustering
    or classifying higher dimensional data. We then moved on to use SVM to cluster
    people using scikit-learn. In the next chapter, we'll talk about recommender systems.
  prefs: []
  type: TYPE_NORMAL
