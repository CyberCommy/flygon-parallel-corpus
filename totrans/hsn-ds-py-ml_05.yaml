- en: Machine Learning with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python进行机器学习
- en: In this chapter, we get into machine learning and how to actually implement
    machine learning models in Python.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍机器学习以及如何在Python中实际实现机器学习模型。
- en: We'll examine what supervised and unsupervised learning means, and how they're
    different from each other. We'll see techniques to prevent overfitting, and then
    look at an interesting example where we implement a spam classifier. We'll analyze
    what K-Means clustering is a long the way, with a working example that clusters
    people based on their income and age using scikit-learn!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究监督学习和无监督学习的含义，以及它们之间的区别。我们将看到防止过拟合的技术，然后看一个有趣的示例，我们在其中实现了一个垃圾邮件分类器。我们将分析K均值聚类是什么，并使用scikit-learn对基于收入和年龄的人群进行聚类的工作示例！
- en: We'll also cover a really interesting application of machine learning called
    **decision trees** and we'll build a working example in Python that predict shiring
    decisions in a company. Finally, we'll walk through the fascinating concepts of
    ensemble learning and SVMs, which are some of my favourite machine learning areas!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将介绍一种非常有趣的机器学习应用，称为**决策树**，并且我们将在Python中构建一个工作示例，用于预测公司的招聘决策。最后，我们将深入探讨集成学习和SVM的迷人概念，这些是我最喜欢的机器学习领域之一！
- en: 'More specifically, we''ll cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们将涵盖以下主题：
- en: Supervised and unsupervised learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督和无监督学习
- en: Avoiding overfitting by using train/test
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用训练/测试来避免过拟合
- en: Bayesian methods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯方法
- en: Implementation of an e-mail spam classifier with NaÃ¯ve Bayes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯实现电子邮件垃圾邮件分类器
- en: Concept of K-means clustering
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值聚类的概念
- en: Example of clustering in Python
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python中聚类的示例
- en: Entropy and how to measure it
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵及其测量方法
- en: Concept of decision trees and its example in Python
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的概念及其在Python中的示例
- en: What is ensemble learning
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是集成学习
- en: '**Support Vector Machine** (**SVM**) and its example using scikit-learn'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）及其在scikit-learn中的示例'
- en: Machine learning and train/test
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习和训练/测试
- en: So what is machine learning? Well, if you look it up on Wikipedia or whatever,
    it'll say that it is algorithms that can learn from observational data and can
    make predictions based on it. It sounds really fancy, right? Like artificial intelligence
    stuff, like you have a throbbing brain inside of your computer. But in reality,
    these techniques are usually very simple.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么什么是机器学习？如果您在维基百科或其他地方查找，它会说这是一种可以从观测数据中学习并基于此进行预测的算法。听起来很花哨，对吧？就像人工智能一样，就像您的计算机内部有一个跳动的大脑。但实际上，这些技术通常非常简单。
- en: We've already looked at regressions, where we took a set of observational data,
    we fitted a line to it, and we used that line to make predictions. So by our new
    definition, that was machine learning! And your brain works that way too.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过回归，我们从中获取了一组观测数据，我们对其进行了拟合，并使用该线进行预测。所以按照我们的新定义，那就是机器学习！您的大脑也是这样工作的。
- en: Another fundamental concept in machine learning is something called **train**/**test**,
    which lets us very cleverly evaluate how good a machine learning model we've made.
    As we look now at unsupervised and supervised learning, you'll see why train/test
    is so important to machine learning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的另一个基本概念是称为**训练**/**测试**的东西，它让我们非常聪明地评估我们制作的机器学习模型有多好。当我们现在看无监督和监督学习时，您将看到为什么训练/测试对机器学习如此重要。
- en: Unsupervised learning
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'Let''s talk in detail now about two different types of machine learning: supervised
    and unsupervised learning. Sometimes there can be kind of a blurry line between
    the two, but the basic definition of unsupervised learning is that you''re not
    giving your model any answers to learn from. You''re just presenting it with a
    group of data and your machine learning algorithm tries to make sense out of it
    given no additional information:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细讨论两种不同类型的机器学习：监督学习和无监督学习。有时两者之间可能存在一种模糊的界限，但无监督学习的基本定义是，您不会给模型任何答案来学习。您只是向其提供一组数据，您的机器学习算法会尝试在没有额外信息的情况下理解它：
- en: '![](img/54c8b166-bc40-4c03-8a14-a47bd74e93dc.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54c8b166-bc40-4c03-8a14-a47bd74e93dc.jpg)'
- en: Let's say I give it a bunch of different objects, like these balls and cubes
    and sets of dice and what not. Let's then say have some algorithm that will cluster
    these objects into things that are similar to each other based on some similarity
    metric.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我给它一堆不同的对象，比如这些球和立方体以及一些骰子之类的东西。然后让我们假设有一些算法，它将根据某种相似性度量将这些对象聚类成相互相似的东西。
- en: Now I haven't told the machine learning algorithm, ahead of time, what categories
    certain objects belong to. I don't have a cheat sheet that it can learn from where
    I have a set of existing objects and my correct categorization of it. The machine
    learning algorithm must infer those categories on its own. This is an example
    of unsupervised learning, where I don't have a set of answers that I'm getting
    it learn from. I'm just trying to let the algorithm gather its own answers based
    on the data presented to it alone.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我没有提前告诉机器学习算法，某些对象属于哪些类别。我没有一个可以从中学习的作弊表，其中有一组现有对象和我对其正确分类的信息。机器学习算法必须自行推断这些类别。这是无监督学习的一个例子，我没有一组答案可以让它学习。我只是试图让算法根据所呈现给它的数据自行收集答案。
- en: The problem with this is that we don't necessarily know what the algorithm will
    come up with! If I gave it that bunch of objects shown in the preceding image,
    is it going to group things into things that are round, things that are large
    versus small, things that are red versus blue, I don't know. It's going to depend
    on the metric that I give it for similarity between items primarily. But sometimes
    you'll find clusters that are surprising, and emerged that you didn't expect to
    see.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于我们不一定知道算法会得出什么结果！如果我给它前面图像中显示的一堆物体，它会将物品分成圆形的，大的与小的，红色的与蓝色的吗，我不知道。这将取决于我为物品之间的相似性给出的度量标准。但有时您会发现令人惊讶的聚类，并且会出现您没有预料到的结果。
- en: 'So that''s really the point of unsupervised learning: if you don''t know what
    you''re looking for, it can be a powerful tool for discovering classifications
    that you didn''t even know were there. We call this a **latent variable**. Some
    property of your data that you didn''t even know was there originally, can be
    teased out by unsupervised learning.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这确实是无监督学习的重点：如果你不知道你在寻找什么，它可以成为一个强大的工具，用来发现你甚至不知道存在的分类。我们称之为**潜在变量**。你最初甚至不知道的数据属性，可以通过无监督学习来挖掘出来。
- en: Let's take another example around unsupervised learning. Say I was clustering
    people instead of balls and dice. I'm writing a dating site and I want to see
    what sorts of people tend to cluster together. There are some attributes that
    people tend to cluster around, which decide whether they tend to like each other
    and date each other for example. Now you might find that the clusters that emerge
    don't conform to your predisposed stereotypes. Maybe it's not about college students
    versus middle-aged people, or people who are divorced and whatnot, or their religious
    beliefs. Maybe if you look at the clusters that actually emerged from that analysis,
    you'll learn something new about your users and actually figure out that there's
    something more important than any of those existing features of your people that
    really count toward, to decide whether they like each other. So that's an example
    of supervised learning providing useful results.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个无监督学习的例子。假设我是在对人群进行聚类而不是对球和骰子进行聚类。我正在写一个约会网站，我想看看哪些类型的人倾向于聚集在一起。人们倾向于围绕一些属性进行聚类，这些属性决定了他们是否倾向于彼此喜欢和约会。现在你可能会发现出现的聚类并不符合你的先入为主的刻板印象。也许这与大学生与中年人、离婚者等无关，或者他们的宗教信仰。也许如果你看看实际从分析中出现的聚类，你会对你的用户学到一些新东西，并且真正发现有些东西比你的人群的任何现有特征更重要，以决定他们是否喜欢彼此。这就是监督学习提供有用结果的一个例子。
- en: Another example could be clustering movies based on their properties. If you
    were to run clustering on a set of movies from like IMDb or something, maybe the
    results would surprise you. Perhaps it's not just about the genre of the movie.
    Maybe there are other properties, like the age of the movie or the running length
    or what country it was released in, that are more important. You just never know.
    Or we could analyze the text of product descriptions and try to find the terms
    that carry the most meaning for a certain category. Again, we might not necessarily
    know ahead of time what terms, or what words, are most indicative of a product
    being in a certain category; but through unsupervised learning, we can tease out
    that latent information.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子可能是根据电影的属性对电影进行聚类。如果你对像IMDb这样的一组电影运行聚类，也许结果会让你惊讶。也许这不仅仅是关于电影的类型。也许还有其他属性，比如电影的年龄或播放时长或上映国家，更重要。你永远不知道。或者我们可以分析产品描述的文本，尝试找出对某个类别具有最重要意义的术语。同样，我们可能并不一定知道哪些术语或词语最能表明产品属于某个类别；但通过无监督学习，我们可以挖掘出这些潜在信息。
- en: Supervised learning
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: Now in contrast, supervised learning is a case where we have a set of answers
    that the model can learn from. We give it a set of training data, that the model
    learns from. It can then infer relationships between the features and the categories
    that we want, and apply that to unseen new values - and predict information about
    them.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，监督学习是一种模型可以从一组答案中学习的情况。我们给它一组训练数据，模型从中学习。然后它可以推断我们想要的特征和类别之间的关系，并将其应用于未见过的新值，并预测有关它们的信息。
- en: Going back to our earlier example, where we were trying to predict car prices
    based on the attributes of those cars. That's an example where we are training
    our model using actual answers. So I have a set of known cars and their actual
    prices that they sold for. I train the model on that set of complete answers,
    and then I can create a model that I'm able to use to predict the prices of new
    cars that I haven't seen before. That's an example of supervised learning, where
    you're giving it a set of answers to learn from. You've already assigned categories
    or some organizing criteria to a set of data, and your algorithm then uses that
    criteria to build a model from which it can predict new values.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们之前的例子，我们试图根据这些汽车的属性来预测汽车价格。这是一个我们使用实际答案来训练模型的例子。所以我有一组已知的汽车及其实际售价。我在这组完整答案上训练模型，然后我可以创建一个能够预测我以前没有见过的新车价格的模型。这是一个监督学习的例子，你给它一组答案来学习。你已经给一组数据分配了类别或一些组织标准，然后你的算法使用这些标准来建立一个模型，从中可以预测新的数值。
- en: Evaluating supervised learning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估监督学习
- en: So how do you evaluate supervised learning? Well, the beautiful thing about
    supervised learning is that we can use a trick called train/test. The idea here
    is to split our observational data that I want my model to learn from into two
    groups, a training set and a testing set. So when I train/build my model based
    on the data that I have, I only do that with part of my data that I'm calling
    my training set, and I reserve another part of my data that I'm going to use for
    testing purposes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如何评估监督学习呢？监督学习的美妙之处在于我们可以使用一个称为训练/测试的技巧。这里的想法是将我希望我的模型学习的观察数据分成两组，一个训练集和一个测试集。所以当我基于我拥有的数据来训练/构建我的模型时，我只使用我称之为训练集的部分数据，然后我保留另一部分数据，我将用于测试目的。
- en: I can build my model using a subset of my data for training data, and then I'm
    in a position to evaluate the model that comes out of that, and see if it can
    successfully predict the correct answers for my testing data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以使用我的数据子集来构建我的模型作为训练数据，然后我可以评估从中得出的模型，并看看它是否能成功地预测我的测试数据的正确答案。
- en: So you see what I did there? I have a set of data where I already have the answers
    that I can train my model from, but I'm going to withhold a portion of that data
    and actually use that to test my model that was generated using the training set!
    That it gives me a very concrete way to test how good my model is on unseen data
    because I actually have a bit of data that I set aside that I can test it with.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: You can then measure quantitatively how well it did using r-squared or some
    other metric, like root-mean-square error, for example. You can use that to test
    one model versus another and see what the best model is for a given problem. You
    can tune the parameters of that model and use train/test to maximize the accuracy
    of that model on your testing data. So this is a great way to prevent overfitting.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: There are some caveats to supervised learning. need to make sure that both your
    training and test datasets are large enough to actually be representative of your
    data. You also need to make sure that you're catching all the different categories
    and outliers that you care about, in both training and testing, to get a good
    measure of its success, and to build a good model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: You have to make sure that you've selected from those datasets randomly, and
    that you're not just carving your dataset in two and saying everything left of
    here is training and right here is testing. You want to sample that randomly,
    because there could be some pattern sequentially in your data that you don't know
    about.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Now, if your model is overfitting, and just going out of its way to accept outliers
    in your training data, then that's going to be revealed when you put it against
    unset scene of testing data. This is because all that gyrations for outliers won't
    help with the outliers that it hasn't seen before.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s be clear here that train/test is not perfect, and it is possible to
    get misleading results from it. Maybe your sample sizes are too small, like we
    already talked about, or maybe just due to random chance your training data and
    your test data look remarkably similar, they actually do have a similar set of
    outliers - and you can still be overfitting. As you can see in the following example,
    it really can happen:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfd6dacd-41fc-415b-ab5b-dda9abf05c09.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: K-fold cross validation
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now there is a way around this problem, called k-fold cross-validation, and
    we'll look at an example of this later in the book, but the basic concept is you
    train/test many times. So you actually split your data not into just one training
    set and one test set, but into multiple randomly assigned segments, k segments.
    That's where the k comes from. And you reserve one of those segments as your test
    data, and then you start training your model on the remaining segments and measure
    their performance against your test dataset. Then you take the average performance
    from each of those training sets' models' results and take their r-squared average
    score.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: So this way, you're actually training on different slices of your data, measuring
    them against the same test set, and if you have a model that's overfitting to
    a particular segment of your training data, then it will get averaged out by the
    other ones that are contributing to k-fold cross-validation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the K-fold cross validation steps:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Split your data into K randomly-assigned segments
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reserve one segment as your test data
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train on each of the remaining K-1 segments and measure their performance against
    the test set
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the average of the K-1 r-squared scores
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will make more sense later in the book, right now I would just like for
    you to know that this tool exists for actually making train/test even more robust
    than it already is. So let's go and actually play with some data and actually
    evaluate it using train/test next.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Using train/test to prevent overfitting of a polynomial regression
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's put train/test into action. So you might remember that a regression can
    be thought of as a form of supervised machine learning. Let's just take a polynomial
    regression, which we covered earlier, and use train/test to try to find the right
    degree polynomial to fit a given set of data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Just like in our previous example, we're going to set up a little fake dataset
    of randomly generated page speeds and purchase amounts, and I'm going to create
    a quirky little relationship between them that's exponential in nature.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s go ahead and generate that data. We''ll use a normal distribution of
    random data for both page speeds and purchase amount using the relationship as
    shown in the following screenshot:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a215de99-d26a-4838-9482-274b254b3d20.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Next, we'll split that data. We'll take 80% of our data, and we're going to
    reserve that for our training data. So only 80% of these points are going to be
    used for training the model, and then we're going to reserve the other 20% for
    testing that model against unseen data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use Python''s syntax here for splitting the list. The first 80 points
    are going to go to the training set, and the last 20, everything after 80, is
    going to go to test set. You may remember this from our Python basics chapter
    earlier on, where we covered the syntax to do this, and we''ll do the same thing
    for purchase amounts here:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now in our earlier sections, I've said that you shouldn't just slice your dataset
    in two like this, but that you should randomly sample it for training and testing.
    In this case though, it works out because my original data was randomly generated
    anyway, so there's really no rhyme or reason to where things fell. But in real-world
    data you'll want to shuffle that data before you split it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: We'll look now at a handy method that you can use for that purpose of shuffling
    your data. Also, if you're using the pandas package, there's some handy functions
    in there for making training and test datasets automatically for you. But we're
    going to do it using a Python list here. So let's visualize our training dataset
    that we ended up with. We'll do a scatter plot of our training page speeds and
    purchase amounts.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is what your output should now look like:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0fc1eec-b655-424f-956f-798327e5a2f8.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: Basically, 80 points that were selected at random from the original complete
    dataset have been plotted. It has basically the same shape, so that's a good thing.
    It's representative of our data. That's important!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Now let's plot the remaining 20 points that we reserved as test data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/534acada-d2fb-4cc7-b4b7-82c40fb4fd64.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: Here, we see our remaining 20 for testing also has the same general shape as
    our original data. So I think that's a representative test set too. It's a little
    bit smaller than you would like to see in the real world, for sure. You probably
    get a little bit of a better result if you had 1,000 points instead of 100, for
    example, to choose from and reserved 200 instead of 20.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Now we're going to try to fit an 8th degree polynomial to this data, and we'll
    just pick the number `8` at random because I know it's a really high order and
    is probably overfitting.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and fit our 8th degree polynomial using `np.poly1d(np.polyfit(x,
    y, 8))`, where *x* is an array of the training data only, and *y* is an array
    of the training data only. We are finding our model using only those 80 points
    that we reserved for training. Now we have this `p4` function that results that
    we can use to predict new values:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we''ll plot the polynomial this came up with against the training data.
    We can scatter our original data for the training data set, and then we can plot
    our predicted values against them:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can see in the following graph that it looks like a pretty good fit, but
    you know that clearly it''s doing some overfitting:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16b521a0-d571-4cd7-a5a8-e5557dcb7c1f.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: 'What''s this craziness out at the right? I''m pretty sure our real data, if
    we had it out there, wouldn''t be crazy high, as this function would implicate.
    So this is a great example of overfitting your data. It fits the data you gave
    it very well, but it would do a terrible job of predicting new values beyond the
    point where the graph is going crazy high on the right. So let''s try to tease
    that out. Let''s give it our test dataset:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Indeed, if we plot our test data against that same function, well, it doesn't
    actually look that bad.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b6b4a61-0d3b-45a7-b7fd-4bfb2d6ad3fa.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: 'We got lucky and none of our test is actually out here to begin with, but you
    can see that it''s a reasonable fit, but far from perfect. And in fact, if you
    actually measure the r-squared score, it''s worse than you might think. We can
    measure that using the `r2_score()` function from `sklearn.metrics`. We just give
    it our original data and our predicted values and it just goes through and measures
    all the variances from the predictions and squares them all up for you:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We end up with an r-squared score of just `0.3`. So that''s not that hot! You
    can see that it fits the training data a lot better:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The r-squared value turns out to be `0.6`, which isn't too surprising, because
    we trained it on the training data. The test data is sort of its unknown, its
    test, and it did fail the test, quite frankly. 30%, that's an F!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: So this has been an example where we've used train/test to evaluate a supervised
    learning algorithm, and like I said before, pandas has some means of making this
    even easier. We'll look at that a little bit later, and we'll also look at more
    examples of train/test, including k-fold cross validation, later in the book as
    well.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can probably guess what your homework is. So we know that an 8th order polynomial
    isn't very useful. Can you do better? So I want you to go back through our example,
    and use different values for the degree polynomial that you're going to use to
    fit. Change that 8 to different values and see if you can figure out what degree
    polynomial actually scores best using train/test as a metric. Where do you get
    your best r-squared score for your test data? What degree fits here? Go play with
    that. It should be a pretty easy exercise and a very enlightening one for you
    as well.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: So that's train/test in action, a very important technique to have under your
    belt, and you're going to use it over and over again to make sure that your results
    are a good fit for the model that you have, and that your results are a good predictor
    of unseen values. It's a great way to prevent overfitting when you're doing your
    modeling.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian methods - Concepts
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Did you ever wonder how the spam classifier in your e-mail works? How does it
    know that an e-mail might be spam or not? Well, one popular technique is something
    called Naive Bayes, and that's an example of a Bayesian method. Let's learn more
    about how that works. Let's discuss Bayesian methods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: We did talk about Bayes' theorem earlier in this book in the context of talking
    about how things like drug tests could be very misleading in their results. But
    you can actually apply the same Bayes' theorem to larger problems, like spam classifiers.
    So let's dive into how that might work, it's called a Bayesian method.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'So just a refresher on Bayes'' theorem -remember, the probability of A given
    B is equal to the overall probability of A times the probability of B given A
    over the overall probability of B:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95ca9a4e-01b9-4dbe-a066-5ff2db89e08d.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: 'How can we use that in machine learning? I can actually build a spam classifier
    for that: an algorithm that can analyze a set of known spam e-mails and a known
    set of non-spam e-mails, and train a model to actually predict whether new e-mails
    are spam or not. This is a real technique used in actual spam classifiers in the
    real world.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s just figure out the probability of an e-mail being spam
    given that it contains the word "free". If people are promising you free stuff,
    it''s probably spam! So let''s work that out. The probability of an email being
    spam given that you have the word "free" in that e-mail works out to the overall
    probability of it being a spam message times the probability of containing the
    word "free" given that it''s spam over the probability overall of being free:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6992cb5a-12dd-4397-b787-571b1d13777c.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: 'The numerator can just be thought of as the probability of a message being
    `Spam` and containing the word `Free`. But that''s a little bit different than
    what we''re looking for, because that''s the odds out of the complete dataset
    and not just the odds within things that contain the word `Free`. The denominator
    is just the overall probability of containing the word `Free`. Sometimes that
    won''t be immediately accessible to you from the data that you have. If it''s
    not, you can expand that out to the following expression if you need to derive
    it:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca351b51-13d1-432c-814b-43bcf6a55db0.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: This gives you the percentage of e-mails that contain the word "free" that are
    spam, which would be a useful thing to know when you're trying to figure out if
    it's spam or not.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: What about all the other words in the English language, though? So our spam
    classifier should know about more than just the word "free". It should automatically
    pick up every word in the message, ideally, and figure out how much does that
    contribute to the likelihood of a particular e-mail being spam. So what we can
    do is train our model on every word that we encounter during training, throwing
    out things like "a" and "the" and "and" and meaningless words like that. Then
    when we go through all the words in a new e-mail, we can multiply the probability
    of being spam for each word together, and we get the overall probability of that
    e-mail being spam.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Now it's called Naive Bayes for a reason. It's naive is because we're assuming
    that there's no relationships between the words themselves. We're just looking
    at each word in isolation, individually within a message, and basically combining
    all the probabilities of each word's contribution to it being spam or not. We're
    not looking at the relationships between the words. So a better spam classifier
    would do that, but obviously that's a lot harder.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: So this sounds like a lot of work. But the overall idea is not that hard, and
    scikit-learn in Python makes it actually pretty easy to do. It offers a feature
    called **CountVectorizer** that makes it very simple to actually split up an e-mail
    to all of its component words and process those words individually. Then it has
    a `MultinomialNB` function, where NB stands for Naive Bayes, which will do all
    the heavy lifting for Naive Bayes for us.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a spam classifier with Naïve Bayes
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s write a spam classifier using Naive Bayes. You''re going to be surprised
    how easy this is. In fact, most of the work ends up just being reading all the
    input data that we''re going to train on and actually parsing that data in. The
    actual spam classification bit, the machine learning bit, is itself just a few
    lines of code. So that''s usually how it works out: reading in and massaging and
    cleaning up your data is usually most of the work when you''re doing data science,
    so get used to the idea!'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So the first thing we need to do is read all those e-mails in somehow, and we're
    going to again use pandas to make this a little bit easier. Again, pandas is a
    useful tool for handling tabular data. We import all the different packages that
    we're going to use within our example here, that includes the os library, the
    io library, numpy, pandas, and CountVectorizer and MultinomialNB from scikit-learn.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through this code in detail now. We can skip past the function definitions
    of `readFiles()` and `dataFrameFromDirectory()`for now and go down to the first
    thing that our code actually does which is to create a pandas DataFrame object.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to construct this from a dictionary that initially contains a
    little empty list for messages in an empty list of class. So this syntax is saying,
    "I want a DataFrame that has two columns: one that contains the message, the actual
    text of each e-mail; and one that contains the class of each e-mail, that is,
    whether it''s spam or ham". So it''s saying I want to create a little database
    of e-mails, and this database has two columns: the actual text of the e-mail and
    whether it''s spam or not.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Now we needed to put something in that database, that is, into that DataFrame,
    in Python syntax. So we call the two methods `append()` and `dataFrameFromDirectory()`
    to actually throw into the DataFrame all the spam e-mails from my `spam` folder,
    and all the ham e-mails from the `ham` folder.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: If you are playing along here, make sure you modify the path passed to the `dataFrameFromDirectory()`
    function to match wherever you installed the book materials in your system! And
    again, if you're on Mac or Linux, please pay attention to backslashes and forward
    slashes and all that stuff. In this case, it doesn't matter, but you won't have
    a drive letter, if you're not on Windows. So just make sure those paths are actually
    pointing to where your `spam` and `ham` folders are for this example.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Next, `dataFrameFromDirectory()` is a function I wrote, which basically says
    I have a path to a directory, and I know it's given classification, spam or ham,
    then it uses the `readFiles()` function, that I also wrote, which will iterate
    through every single file in a directory. So `readFiles()` is using the `os.walk()`
    function to find all the files in a directory. Then it builds up the full pathname
    for each individual file in that directory, and then it reads it in. And while
    it's reading it in, it actually skips the header for each e-mail and just goes
    straight to the text, and it does that by looking for the first blank line.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: It knows that everything after the first empty line is actually the message
    body, and everything in front of that first empty line is just a bunch of header
    information that I don't actually want to train my spam classifier on. So it gives
    me back both, the full path to each file and the body of the message. So that's
    how we read in all of the data, and that's the majority of the code!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'So what I have at the end of the day is a DataFrame object, basically a database
    with two columns, that contains message bodies, and whether it''s spam or not.
    We can go ahead and run that, and we can use the `head` command from the DataFrame
    to actually preview what this looks like:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The first few entries in our DataFrame look like this: for each path to a given
    file full of e-mails we have a classification and we have the message body:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5eb289f1-dcfb-460a-984b-6b07d9c8dc56.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Alright, now for the fun part, we're going to use the `MultinomialNB()` function
    from scikit-learn to actually perform Naive Bayes on the data that we have.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This is what your output should now look like:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/389232ea-b526-4051-9297-89bc6f7f7d09.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: Once we build a `MultinomialNB` classifier, it needs two inputs. It needs the
    actual data that we're training on (`counts`), and the targets for each thing
    (`targets`). So `counts` is basically a list of all the words in each e-mail and
    the number of times that word occurs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'So this is what `CountVectorizer()` does: it takes the `message` column from
    the DataFrame and takes all the values from it. I''m going to call `vectorizer.fit_transform`
    which basically tokenizes or converts all the individual words seen in my data
    into numbers, into values. It then counts up how many times each word occurs.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: This is a more compact way of representing how many times each word occurs in
    an e-mail. Instead of actually preserving the words themselves, I'm representing
    those words as different values in a sparse matrix, which is basically saying
    that I'm treating each word as a number, as a numerical index, into an array.
    What that does is, just in plain English, it split each message up into a list
    of words that are in it, and counts how many times each word occurs. So we're
    calling that `counts`. It's basically that information of how many times each
    word occurs in each individual message. Mean while `targets` is the actual classification
    data for each e-mail that I've encountered. So I can call `classifier.fit()` using
    my `MultinomialNB()` function to actually create a model using Naive Bayes, which
    will predict whether new e-mails are spam or not based on the information we've
    given it.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种更紧凑的方式来表示每个单词在电子邮件中出现的次数。我不是保留单词本身，而是将这些单词表示为稀疏矩阵中的不同值，这基本上是说我将每个单词视为一个数字，作为一个数值索引，进入一个数组。它所做的是，用简单的英语说，将每个消息拆分成其中包含的单词列表，并计算每个单词出现的次数。所以我们称之为“counts”。它基本上是每个单词在每个单独消息中出现的次数的信息。同时，“targets”是我遇到的每封电子邮件的实际分类数据。所以我可以使用我的MultinomialNB()函数调用classifier.fit()来实际使用朴素贝叶斯创建一个模型，该模型将根据我们提供的信息预测新的电子邮件是否是垃圾邮件。
- en: Let's go ahead and run that. It runs pretty quickly! I'm going to use a couple
    of examples here. Let's try a message body that just says `Free Money now!!!`
    which is pretty clearly spam, and a more innocent message that just says `"Hi
    Bob, how about a game of golf tomorrow?"` So we're going to pass these in.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续运行。它运行得相当快！我将在这里使用几个例子。让我们尝试一个只说“现在免费赚钱！”的消息正文，这显然是垃圾邮件，还有一个更无辜的消息，只是说“嗨鲍勃，明天打一场高尔夫怎么样？”所以我们要传递这些消息。
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The first thing we do is convert the messages into the same format that I trained
    my model on. So I use that same vectorizer that I created when creating the model
    to convert each message into a list of words and their frequencies, where the
    words are represented by positions in an array. Then once I''ve done that transformation,
    I can actually use the `predict()` function on my classifier, on that array of
    examples that have transformed into lists of words, and see what we come up with:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是将消息转换为我训练模型的相同格式。所以我使用了创建模型时创建的相同的向量化器，将每条消息转换为一个单词和它们的频率的列表，其中单词由数组中的位置表示。一旦我完成了这个转换，我实际上可以在我的分类器上使用predict()函数，对已经转换成单词列表的示例数组进行预测，看看我们得到了什么：
- en: '![](img/9cd78d24-18d0-4700-b154-aa2ac869322b.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9cd78d24-18d0-4700-b154-aa2ac869322b.jpg)'
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: And sure enough, it works! So, given this array of two input messages, `Free
    Money now!!!` and `Hi Bob`, it's telling me that the first result came back as
    spam and the second result came back as ham, which is what I would expect. That's
    pretty cool. So there you have it.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，它有效！所以，给定这两个输入消息的数组，“现在免费赚钱！”和“嗨鲍勃”，它告诉我第一个结果是垃圾邮件，第二个结果是正常邮件，这正是我所期望的。这很酷。就是这样。
- en: Activity
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动
- en: We had a pretty small dataset here, so you could try running some different
    e-mails through it if you want and see if you get different results. If you really
    want to challenge yourself, try applying train/test to this example. So the real
    measure of whether or not my spam classifier is good or not is not just intuitively
    whether it can figure out that `Free Money now!!!` is spam. You want to measure
    that quantitatively.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里有一个相当小的数据集，所以如果你愿意，你可以尝试通过一些不同的电子邮件，并查看是否会得到不同的结果。如果你真的想挑战自己，尝试将训练/测试应用到这个例子中。所以是否我的垃圾邮件分类器好不好的真正衡量标准不仅仅是它是否能直观地判断“现在免费赚钱！”是垃圾邮件。你想要定量地衡量它。
- en: So if you want a little bit of a challenge, go ahead and try to split this data
    up into a training set and a test dataset. You can actually look up online how
    pandas can split data up into train sets and testing sets pretty easily for you,
    or you can do it by hand. Whatever works for you. See if you can actually apply
    your `MultinomialNB` classifier to a test dataset and measure its performance.
    So, if you want a little bit of an exercise, a little bit of a challenge, go ahead
    and give that a try.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你想挑战一下，尝试将这些数据分成一个训练集和一个测试数据集。你实际上可以在网上查找如何使用pandas很容易地将数据分成训练集和测试集，或者你可以手动操作。无论哪种方式都可以。看看你是否能够将你的MultinomialNB分类器应用到一个测试数据集上，并衡量其性能。所以，如果你想要一点挑战，一点挑战，那就试试看吧。
- en: How cool is that? We just wrote our own spam classifier just using a few lines
    of code in Python. It's pretty easy using scikit-learn and Python. That's Naive
    Bayes in action, and you can actually go and classify some spam or ham messages
    now that you have that under your belt. Pretty cool stuff. Let's talk about clustering
    next.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这有多酷？我们只是用几行Python代码编写了自己的垃圾邮件分类器。使用scikit-learn和Python非常容易。这就是朴素贝叶斯的实际应用，现在你可以去分类一些垃圾邮件或正常邮件了。非常酷。接下来让我们谈谈聚类。
- en: K-Means clustering
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-Means聚类
- en: Next, we're going to talk about k-means clustering, and this is an unsupervised
    learning technique where you have a collection of stuff that you want to group
    together into various clusters. Maybe it's movie genres or demographics of people,
    who knows? But it's actually a pretty simple idea, so let's see how it works.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论K均值聚类，这是一种无监督学习技术，你有一堆东西想要分成各种不同的簇。也许是电影类型或人口统计学，谁知道呢？但这实际上是一个相当简单的想法，所以让我们看看它是如何工作的。
- en: K-means clustering is a very common technique in machine learning where you
    just try to take a bunch of data and find interesting clusters of things just
    based on the attributes of the data itself. Sounds fancy, but it's actually pretty
    simple. All we do in k-means clustering is try to split our data into K groups
    - that's where the K comes from, it's how many different groups you're trying
    to split your data into - and it does this by finding K centroids.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'So, basically, what group a given data point belongs to is defined by which
    of these centroid points it''s closest to in your scatter plot. You can visualize
    this in the following image:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df576023-6bcc-4691-9d5d-baa565c15445.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: This is showing an example of k-means clustering with K of three, and the squares
    represent data points in a scatter plot. The circles represent the centroids that
    the k-means clustering algorithm came up with, and each point is assigned a cluster
    based on which centroid it's closest to. So that's all there is to it, really.
    It's an example of unsupervised learning. It isn't a case where we have a bunch
    of data and we already know the correct cluster for a given set of training data;
    rather, you're just given the data itself and it tries to converge on these clusters
    naturally just based on the attributes of the data alone. It's also an example
    where you are trying to find clusters or categorizations that you didn't even
    know were there. As with most unsupervised learning techniques, the point is to
    find latent values, things you didn't really realize were there until the algorithm
    showed them to you.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: For example, where do millionaires live? I don't know, maybe there is some interesting
    geographical cluster where rich people tend to live, and k-means clustering could
    help you figure that out. Maybe I don't really know if today's genres of music
    are meaningful. What does it mean to be alternative these days? Not much, right?
    But by using k-means clustering on attributes of songs, maybe I could find interesting
    clusters of songs that are related to each other and come up with new names for
    what those clusters represent. Or maybe I can look at demographic data, and maybe
    existing stereotypes are no longer useful. Maybe Hispanic has lost its meaning
    and there's actually other attributes that define groups of people, for example,
    that I could uncover with clustering. Sounds fancy, doesn't it? Really complicated
    stuff. Unsupervised machine learning with K clusters, it sounds fancy, but as
    with most techniques in data science, it's actually a very simple idea.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the algorithm for us in plain English:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '**Randomly pick K centroids (k-means):** We start off with a randomly chosen
    set of centroids. So if we have a K of three we''re going to look for three clusters
    in our group, and we will assign three randomly positioned centroids in our scatter
    plot.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Assign each data point to the centroid it is closest to:** We then assign
    each data point to the randomly assigned centroid that it is closest to.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Recompute the centroids based on the average position of each centroid''s
    points**: Then recompute the centroid for each cluster that we come up with. That
    is, for a given cluster that we end up with, we will move that centroid to be
    the actual center of all those points.'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterate until points stop changing assignment to centroids:** We will do
    it all again until those centroids stop moving, we hit some threshold value that
    says OK, we have converged on something here.'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Predict the cluster for new points:** To predict the clusters for new points
    that I haven''t seen before, we can just go through our centroid locations and
    figure out which centroid it''s closest to to predict its cluster.'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's look at a graphical example to make a little bit more sense. We'll call
    the first figure in the following image as A, second as B, third as C and the
    fourth as D.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1e5ec68-fe61-4e99-b6e6-395b3354fef1.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: The gray squares in image A represent data points in our scatter plot. The axes
    represent some different features of something. Maybe it's age and income; it's
    an example I keep using, but it could be anything. And the gray squares might
    represent individual people or individual songs or individual something that I
    want to find relationships between.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图A中的灰色方块代表我们散点图中的数据点。坐标轴代表某些不同特征。也许是年龄和收入；这是我一直在使用的一个例子，但它可以是任何东西。灰色方块可能代表个体人员、个体歌曲或我想要找到它们之间关系的任何东西。
- en: So I start off by just picking three points at random on my scatterplot. Could
    be anywhere. Got to start somewhere, right? The three points (centroids) I selected
    have been shown as circles in image A. So the next thing I'm going to do is for
    each centroid I'll compute which one of the gray points it's closest to. By doing
    that, the points shaded in blue are associated with this blue centroid. The green
    points are closest to the green centroid, and this single red point is closest
    to that red random point that I picked out.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我首先随机在我的散点图上选择了三个点。可以是任何地方。总得从某个地方开始，对吧？我选择的三个点（质心）在图A中被表示为圆圈。接下来，我要做的是对于每个质心，计算它最接近的灰色点是哪一个。通过这样做，图中蓝色阴影的点与蓝色质心相关联。绿色点最接近绿色质心，而这个单独的红点最接近我选择的那个红色随机点。
- en: Of course, you can see that's not really reflective of where the actual clusters
    appear to be. So what I'm going to do is take the points that ended up in each
    cluster and compute the actual center of those points. For example, in the green
    cluster, the actual center of all data turns out to be a little bit lower. We're
    going to move the centroid down a little bit. The red cluster only had one point,
    so its center moves down to where that single point is. And the blue point was
    actually pretty close to the center, so that just moves a little bit. On this
    next iteration we end up with something that looks like image D. Now you can see
    that our cluster for red things has grown a little bit and things have moved a
    little bit, that is, those got taken from the green cluster.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以看到这并不真正反映实际聚类的情况。因此，我要做的是取出每个聚类中的点，并计算这些点的实际中心。例如，在绿色聚类中，所有数据的实际中心实际上要低一点。我们会将质心向下移动一点。红色聚类只有一个点，所以它的中心移动到那个单个点的位置。而蓝色点实际上离中心相当近，所以只是移动了一点。在下一次迭代中，我们得到了类似图D的结果。现在你可以看到我们红色聚类的范围有所增加，事物也有所移动，也就是说，它们被从绿色聚类中取走了。
- en: If we do that again, you can probably predict what's going to happen next. The
    green centroid will move a little bit, the blue centroid will still be about where
    it is. But at the end of the day you're going to end up with the clusters you'd
    probably expect to see. That's how k-means works. So it just keeps iterating,
    trying to find the right centroids until things start moving around and we converge
    on a solution.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次这样做，你可能可以预测接下来会发生什么。绿色的质心会移动一点，蓝色的质心仍然会保持在原来的位置。但最终你会得到你可能期望看到的聚类。这就是k-means的工作原理。因此，它会不断迭代，试图找到正确的质心，直到事物开始移动并收敛于一个解决方案。
- en: Limitations to k-means clustering
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means聚类的局限性
- en: 'So there are some limitations to k-means clustering. Here they are:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，k-means聚类存在一些局限性。以下是其中一些：
- en: '**Choosing K:** First of all, we need to choose the right value of K, and that''s
    not a straightforward thing to do at all. The principal way of choosing K is to
    just start low and keep increasing the value of K depending on how many groups
    you want, until you stop getting large reductions in squared error. If you look
    at the distances from each point to their centroids, you can think of that as
    an error metric. At the point where you stop reducing that error metric, you know
    you probably have too many clusters. So you''re not really gaining any more information
    by adding additional clusters at that point.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择K：**首先，我们需要选择正确的K值，这并不是一件简单的事情。选择K的主要方法是从较低的值开始，根据你想要的群组数量不断增加K的值，直到停止获得平方误差的大幅减少。如果你观察每个点到它们的质心的距离，你可以将其视为一个误差度量。当你停止减少这个误差度量时，你就知道你可能有太多的聚类。因此，在那一点上，通过添加额外的聚类，你实际上并没有获得更多的信息。'
- en: '**Avoiding local minima:** Also, there is a problem of local minima. You could
    just get very unlucky with those initial choices of centroids and they might end
    up just converging on local phenomena instead of more global clusters, so usually,
    you want to run this a few times and maybe average the results together. We call
    that ensemble learning. We''ll talk about that more a little bit later on, but
    it''s always a good idea to run k-means more than once using a different set of
    random initial values and just see if you do in fact end up with the same overall
    results or not.'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**避免局部最小值：**此外，还存在局部最小值的问题。你可能会因为初始质心的选择而非常不幸，它们最终可能只会收敛于局部现象，而不是更全局的聚类，因此通常情况下，你需要多次运行这个过程，然后将结果进行平均。我们称之为集成学习。我们稍后会更详细地讨论这个问题，但多次运行k-means并使用不同的随机初始值是一个很好的主意，看看你最终是否得到了相同的结果。'
- en: '**Labeling the clusters:** Finally, the main problem with k-means clustering
    is that there''s no labels for the clusters that you get. It will just tell you
    that this group of data points are somehow related, but you can''t put a name
    on it. It can''t tell you the actual meaning of that cluster. Let''s say I have
    a bunch of movies that I''m looking at, and k-means clustering tells me that bunch
    of science fiction movies are over here, but it''s not going to call them "science
    fiction" movies for me. It''s up to me to actually dig into the data and figure
    out, well, what do these things really have in common? How might I describe that
    in English? That''s the hard part, and k-means won''t help you with that. So again,
    scikit-learn makes it very easy to do this.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's now work up an example and put k-means clustering into action.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Clustering people based on income and age
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see just how easy it is to do k-means clustering using scikit-learn and
    Python.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we're going to do is create some random data that we want to
    try to cluster. Just to make it easier, we'll actually build some clusters into
    our fake test data. So let's pretend there's some real fundamental relationship
    between these data, and there are some real natural clusters that exist in it.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'So to do that, we can work with this little `createClusteredData()` function
    in Python:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The function starts off with a consistent random seed so you'll get the same
    result every time. We want to create clusters of N people in k clusters. So we
    pass `N` and `k` to `createClusteredData().`
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Our code figures out how many points per cluster that works out to first and
    stores it in `pointsPerCluster`. Then, it builds up list `X` that starts off empty.
    For each cluster, we're going to create some random centroid of income (`incomeCentroid`)
    between 20,000 and 200,000 dollars and some random centroid of age (`ageCentroid`)
    between the age of 20 and 70.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: What we're doing here is creating a fake scatter plot that will show income
    versus age for `N` people and `k` clusters. So for each random centroid that we
    created, I'm then going to create a normally distributed set of random data with
    a standard deviation of 10,000 in income and a standard deviation of 2 in age.
    That will give us back a bunch of age income data that is clustered into some
    pre-existing clusters that we can chose at random. OK, let's go ahead and run
    that.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Now, to actually do k-means, you'll see how easy it is.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: All you need to do is import `KMeans` from scikit-learn's `cluster` package.
    We're also going to import `matplotlib` so we can visualize things, and also import
    `scale` so we can take a look at how that works.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: So we use our `createClusteredData()` function to say 100 random people around
    5 clusters. So there are 5 natural clusters for the data that I'm creating. We
    then create a model, a KMeans model with k of 5, so we're picking 5 clusters because
    we know that's the right answer. But again, in unsupervised learning you don't
    necessarily know what the real value of `k` is. You need to iterate and converge
    on it yourself. And then we just call `model.fit` using my KMeans `model` using
    the data that we had.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Now the scale I alluded to earlier, that's normalizing the data. One important
    thing with k-means is that it works best if your data is all normalized. That
    means everything is at the same scale. So a problem that I have here is that my
    ages range from 20 to 70, but my incomes range all the way up to 200,000\. So
    these values are not really comparable. The incomes are much larger than the age
    values. `Scale` will take all that data and scale it together to a consistent
    scale so I can actually compare these things as apples to apples, and that will
    help a lot with your k-means results.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'So, once we''ve actually called `fit` on our model, we can actually look at
    the resulting labels that we got. Then we can actually visualize it using a little
    bit of `matplotlib` magic. You can see in the code we have a little trick where
    we assigned the color to the labels that we ended up with converted to some floating
    point number. That''s just a little trick you can use to assign arbitrary colors
    to a given value. So let''s see what we end up with:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5799932c-1451-4c6a-a10a-14372dbc79b6.jpg)![](img/e988ba57-00c7-4e2e-a3c5-45cf7165bd4c.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: It didn't take that long. You see the results are basically what clusters I
    assigned everything into. We know that our fake data is already pre-clustered,
    so it seems that it identified the first and second clusters pretty easily. It
    got a little bit confused beyond that point, though, because our clusters in the
    middle are actually a little bit mushed together. They're not really that distinct,
    so that was a challenge for k-means. But regardless, it did come up with some
    reasonable guesses at the clusters. This is probably an example of where four
    clusters would more naturally fit the data.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So what I want you to do for an activity is to try a different value of k and
    see what you end up with. Just eyeballing the preceding graph, it looks like four
    would work well. Does it really? What happens if I increase k too large? What
    happens to my results? What does it try to split things into, and does it even
    make sense? So, play around with it, try different values of `k`. So in the `n_clusters()`
    function, change the 5 to something else. Run all through it again and see you
    end up with.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s all there is to k-means clustering. It''s just that simple. You can
    just use scikit-learn''s `KMeans` thing from `cluster`. The only real gotcha:
    make sure you scale the data, normalize it. You want to make sure the things that
    you''re using k-means on are comparable to each other, and the `scale()` function
    will do that for you. So those are the main things for k-means clustering. Pretty
    simple concept, even simpler to do it using scikit-learn.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: That's all there is to it. That's k-means clustering. So if you have a bunch
    of data that is unclassified and you don't really have the right answers ahead
    of time, it's a good way to try to naturally find interesting groupings of your
    data, and maybe that can give you some insight into what that data is. It's a
    good tool to have. I've used it before in the real world and it's really not that
    hard to use, so keep that in your tool chest.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Measuring entropy
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quite soon we're going to get to one of the cooler parts of machine learning,
    at least I think so, called decision trees. But before we can talk about that,
    it's a necessary to understand the concept of entropy in data science.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: So entropy, just like it is in physics and thermodynamics, is a measure of a
    dataset's disorder, of how same or different the dataset is. So imagine we have
    a dataset of different classifications, for example, animals. Let's say I have
    a bunch of animals that I have classified by species. Now, if all of the animals
    in my dataset are an iguana, I have very low entropy because they're all the same.
    But if every animal in my dataset is a different animal, I have iguanas and pigs
    and sloths and who knows what else, then I would have a higher entropy because
    there's more disorder in my dataset. Things are more different than they are the
    same.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Entropy is just a way of quantifying that sameness or difference throughout
    my data. So, an entropy of 0 implies all the classes in the data are the same,
    whereas if everything is different, I would have a high entropy, and something
    in between would be a number in between. Entropy just describes how same or different
    the things in a dataset are.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Now mathematically, it''s a little bit more involved than that, so when I actually
    compute a number for entropy, it''s computed using the following expression:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7dd736b4-b705-487c-94e6-df5a8640a08a.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'So for every different class that I have in my data, I''m going to have one
    of these p terms, p[1], p[2], and so on and so forth through p[n], for n different
    classes that I might have. The p just represents the proportion of the data that
    is that class. And if you actually plot what this looks like for each term- `pi*
    ln * pi`, it''ll look a little bit something like the following graph:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3ae9e6d-662c-4568-b28f-e9fed56739e7.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: You add these up for each individual class. For example, if the proportion of
    the data, that is, for a given class is 0, then the contribution to the overall
    entropy is 0\. And if everything is that class, then again the contribution to
    the overall entropy is 0 because in either case, if nothing is this class or everything
    is this class, that's not really contributing anything to the overall entropy.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: It's the things in the middle that contribute entropy of the class, where there's
    some mixture of this classification and other stuff. When you add all these terms
    together, you end up with an overall entropy for the entire dataset. So mathematically,
    that's how it works out, but again, the concept is very simple. It's just a measure
    of how disordered your dataset, how same or different the things in your data
    are.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees - Concepts
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Believe it or not, given a set of training data, you can actually get Python
    to generate a flowchart for you to make a decision. So if you have something you're
    trying to predict on some classification, you can use a decision tree to actually
    look at multiple attributes that you can decide upon at each level in the flowchart.
    You can print out an actual flowchart for you to use to make a decision from,
    based on actual machine learning. How cool is that? Let's see how it works.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: I personally find decision trees are one of the most interesting applications
    of machine learning. A decision tree basically gives you a flowchart of how to
    make some decision.You have some dependent variable, like whether or not I should
    go play outside today or not based on the weather. When you have a decision like
    that that depends on multiple attributes or multiple variables, a decision tree
    could be a good choice.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: There are many different aspects of the weather that might influence my decision
    of whether I should go outside and play. It might have to do with the humidity,
    the temperature, whether it's sunny or not, for example. A decision tree can look
    at all these different attributes of the weather, or anything else, and decide
    what are the thresholds? What are the decisions I need to make on each one of
    those attributes before I arrive at a decision of whether or not I should go play
    outside? That's all a decision tree is. So it's a form of supervised learning.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'The way it would work in this example would be as follows. I would have some
    sort of dataset of historical weather, and data about whether or not people went
    outside to play on a particular day. I would feed the model this data of whether
    it was sunny or not on each day, what the humidity was, and if it was windy or
    not; and whether or not it was a good day to go play outside. Given that training
    data, a decision tree algorithm can then arrive at a tree that gives us a flowchart
    that we can print out. It looks just like the following flow chart. You can just
    walk through and figure out whether or not it''s a good day to play outside based
    on the current attributes. You can use that to predict the decision for a new
    set of values:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0d013a2-8886-4d48-8a56-3b4e166a551a.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: How cool is that? We have an algorithm that will make a flowchart for you automatically
    just based on observational data. What's even cooler is how simple it all works
    once you learn how it works.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree example
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's say I want to build a system that will automatically filter out resumes
    based on the information in them. A big problem that technology companies have
    is that we get tons and tons of resumes for our positions. We have to decide who
    we actually bring in for an interview, because it can be expensive to fly somebody
    out and actually take the time out of the day to conduct an interview. So what
    if there were a way to actually take historical data on who actually got hired
    and map that to things that are found on their resume?
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: We could construct a decision tree that will let us go through an individual
    resume and say, "OK, this person actually has a high likelihood of getting hired,
    or not". We can train a decision tree on that historical data and walk through
    that for future candidates. Wouldn't that be a wonderful thing to have?
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s make some totally fabricated hiring data that we''re going to use
    in this example:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4a0232c-b18c-41a8-a032-92b7466ca4a0.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: In the preceding table, we have candidates that are just identified by numerical
    identifiers. I'm going to pick some attributes that I think might be interesting
    or helpful to predict whether or not they're a good hire or not. How many years
    of experience do they have? Are they currently employed? How many employers have
    they had previous to this one? What's their level of education? What degree do
    they have? Did they go to what we classify as a top-tier school? Did they do an
    internship while they were in college? We can take a look at this historical data,
    and the dependent variable here is `Hired`. Did this person actually get a job
    offer or not based on that information?
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, obviously there''s a lot of information that isn''t in this model that
    might be very important, but the decision tree that we train from this data might
    actually be useful in doing an initial pass at weeding out some candidates. What
    we end up with might be a tree that looks like the following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47031034-ef5a-4c31-b66f-63953d852abf.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: So it just turns out that in my totally fabricated data, anyone that did an
    internship in college actually ended up getting a job offer. So my first decision
    point is "did this person do an internship or not?" If yes, go ahead and bring
    them in. In my experience, internships are actually a pretty good predictor of
    how good a person is. If they have the initiative to actually go out and do an
    internship, and actually learn something at that internship, that's a good sign.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do they currently have a job? Well, if they are currently employed, in my very
    small fake dataset it turned out that they are worth hiring, just because somebody
    else thought they were worth hiring too. Obviously it would be a little bit more
    of a nuanced decision in the real world.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If they're not currently employed, do they have less than one prior employer?
    If yes, this person has never held a job and they never did an internship either.
    Probably not a good hire decision. Don't hire that person.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But if they did have a previous employer, did they at least go to a top-tier
    school? If not, it's kind of iffy. If so, then yes, we should hire this person
    based on the data that we trained on.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walking through a decision tree
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So that's how you walk through the results of a decision tree. It's just like
    going through a flowchart, and it's kind of awesome that an algorithm can produce
    this for you. The algorithm itself is actually very simple. Let me explain how
    the algorithm works.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'At each step of the decision tree flowchart, we find the attribute that we
    can partition our data on that minimizes the entropy of the data at the next step.
    So we have a resulting set of classifications: in this case hire or don''t hire,
    and we want to choose the attribute decision at that step that will minimize the
    entropy at the next step.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: At each step we want to make all of the remaining choices result in either as
    many no hires or as many hire decisions as possible. We want to make that data
    more and more uniform so as we work our way down the flowchart, and we ultimately
    end up with a set of candidates that are either all hires or all no hires so we
    can classify into yes/no decisions on a decision tree. So we just walk down the
    tree, minimize entropy at each step by choosing the right attribute to decide
    on, and we keep on going until we run out.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: There's a fancy name for this algorithm. It's called **ID3** (**Iterative Dichotomiser
    3**). It is what's known as a greedy algorithm. So as it goes down the tree, it
    just picks the attribute that will minimize entropy at that point. Now that might
    not actually result in an optimal tree that minimizes the number of choices that
    you have to make, but it will result in a tree that works, given the data that
    you gave it.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Random forests technique
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now one problem with decision trees is that they are very prone to overfitting,
    so you can end up with a decision tree that works beautifully for the data that
    you trained it on, but it might not be that great for actually predicting the
    correct classification for new people that it hasn't seen before. Decision trees
    are all about arriving at the right decision for the training data that you gave
    it, but maybe you didn't really take into account the right attributes, maybe
    you didn't give it enough of a representative sample of people to learn from.
    This can result in real problems.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: So to combat this issue, we use a technique called random forests, where the
    idea is that we sample the data that we train on, in different ways, for multiple
    different decision trees. Each decision tree takes a different random sample from
    our set of training data and constructs a tree from it. Then each resulting tree
    can vote on the right result.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Now that technique of randomly resampling our data with the same model is a
    term called bootstrap aggregating, or bagging. This is a form of what we call
    ensemble learning, which we'll cover in more detail shortly. But the basic idea
    is that we have multiple trees, a forest of trees if you will, each that uses
    a random subsample of the data that we have to train on. Then each of these trees
    can vote on the final result, and that will help us combat overfitting for a given
    set of training data.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: The other thing random forests can do is actually restrict the number of attributes
    that it can choose, between at each stage, while it is trying to minimize the
    entropy as it goes. And we can randomly pick which attributes it can choose from
    at each level. So that also gives us more variation from tree to tree, and therefore
    we get more of a variety of algorithms that can compete with each other. They
    can all vote on the final result using slightly different approaches to arriving
    at the same answer.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: So that's how random forests work. Basically, it is a forest of decision trees
    where they are drawing from different samples and also different sets of attributes
    at each stage that it can choose between.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: So, with all that, let's go make some decision trees. We'll use random forests
    as well when we're done, because scikit-learn makes it really really easy to do,
    as you'll see soon.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees - Predicting hiring decisions using Python
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Turns out that it's easy to make decision trees; in fact it's crazy just how
    easy it is, with just a few lines of Python code. So let's give it a try.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: I've included a `PastHires.csv` file with your book materials, and that just
    includes some fabricated data, that I made up, about people that either got a
    job offer or not based on the attributes of those candidates.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You'll want to please immediately change that path I used here for my own system
    (`c:/spark/DataScience/PastHires.csv`) to wherever you have installed the materials
    for this book. I'm not sure where you put it, but it's almost certainly not there.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: We will use `pandas` to read our CSV in, and create a DataFrame object out of
    it. Let's go ahead and run our code, and we can use the `head()` function on the
    DataFrame to print out the first few lines and make sure that it looks like it
    makes sense.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Sure enough we have some valid data in the output:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39aa9b7f-af63-4996-ae11-fc8d96174087.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: So, for each candidate ID, we have their years of past experience, whether or
    not they were employed, their number of previous employers, their highest level
    of education, whether they went to a top-tier school, and whether they did an
    internship; and finally here, in the Hired column, the answer - where we knew
    that we either extended a job offer to this person or not.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, most of the work is just in massaging your data, preparing your data,
    before you actually run the algorithms on it, and that''s what we need to do here.
    Now scikit-learn requires everything to be numerical, so we can''t have Ys and
    Ns and BSs and MSs and PhDs. We have to convert all those things to numbers for
    the decision tree model to work. The way to do this is to use some short-hand
    in pandas, which makes these things easy. For example:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Basically, we're making a dictionary in Python that maps the letter Y to the
    number 1, and the letter N to the value 0\. So, we want to convert all our Ys
    to 1s and Ns to 0s. So 1 will mean yes and 0 will mean no. What we do is just
    take the Hired column from the DataFrame, and call `map()` on it, using a dictionary.
    This will go through the entire Hired column, in the entire DataFrame and use
    that dictionary lookup to transform all the entries in that column. It returns
    a new DataFrame column that I'm putting back into the Hired column. This replaces
    the Hired column with one that's been mapped to 1s and 0s.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'We do the same thing for Employed, Top-tier school and Interned, so all those
    get mapped using the yes/no dictionary. So, the Ys and Ns become 1s and 0s instead.
    For the Level of Education, we do the same trick, we just create a dictionary
    that assigns BS to 0, MS to 1, and PhD to 2 and uses that to remap those degree
    names to actual numerical values. So if I go ahead and run that and do a `head()`
    again, you can see that it worked:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f975ef7-dde7-4e2c-988f-a115252ad95d.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: All my yeses are 1's, my nos are 0's, and my Level of Education is now represented
    by a numerical value that has real meaning.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Next we need to prepare everything to actually go into our decision tree classifier,
    which isn't that hard. To do that, we need to separate our feature information,
    which are the attributes that we're trying to predict from, and our target column,
    which contains the thing that we're trying to predict.To extract the list of feature
    name columns, we are just going to create a list of columns up to number 6\. We
    go ahead and print that out.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We get the following output:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b121561b-c7b2-4418-aad6-bc33f5c608d2.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: 'Above are the column names that contain our feature information: Years Experience,
    Employed?, Previous employers, Level of Education, Top-tier school, and Interned.
    These are the attributes of candidates that we want to predict hiring on.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we construct our *y* vector which is assigned what we''re trying to predict,
    that is our Hired column:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This code extracts the entire Hired column and calls it `y`. Then it takes all
    of our columns for the feature data and puts them in something called `X`. This
    is a collection of all of the data and all of the feature columns, and `X` and
    `y` are the two things that our decision tree classifier needs.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'To actually create the classifier itself, two lines of code: we call `tree.DecisionTreeClassifier()`
    to create our classifier, and then we fit it to our feature data (`X`) and the
    answers (`y`)- whether or not people were hired. So, let''s go ahead and run that.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Displaying graphical data is a little bit tricky, and I don''t want to distract
    us too much with the details here, so please just consider the following boilerplate
    code. You don''t need to get into how Graph viz works here - and dot files and
    all that stuff: it''s not important to our journey right now. The code you need
    to actually display the end results of a decision tree is simply:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: So let's go ahead and run this.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what your output should now look like:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19fbf5b0-4476-46de-897e-ce3bb7a5237f.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: There we have it! How cool is that?! We have an actual flow chart here.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let me show you how to read it. At each stage, we have a decision. Remember
    most of our data which is yes or no, is going to be **0** or **1**. So, the first
    decision point becomes: is Employed? less than **0.5**? Meaning that if we have
    an employment value of 0, that is no, we''re going to go left.If employment is
    1, that is yes, we''re going to go right.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: So, were they previously employed? If not go left, if yes go right. It turns
    out that in my sample data, everyone who is currently employed actually got a
    job offer, so I can very quickly say if you are currently employed, yes, you're
    worth bringing in, we're going to follow down to the second level here.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: So, how do you interpret this? The gini score is basically a measure of entropy
    that it's using at each step. Remember as we're going down the algorithm is trying
    to minimize the amount of entropy. And the samples are the remaining number of
    samples that haven't beensectioned off by a previous decision.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: So say this person was employed. The way to read the right leaf node is the
    value column that tells you at this point we have 0 candidates that were no hires
    and 5 that were hires. So again, the way to interpret the first decision point
    is if Employed? was 1, I'm going to go to the right, meaning that they are currently
    employed, and this brings me to a world where everybody got a job offer. So, that
    means I should hire this person.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Now let's say that this person doesn't currently have a job. The next thing
    I'm going to look at is, do they have an internship. If yes, then we're at a point
    where in our training data everybody got a job offer. So, at that point, we can
    say our entropy is now 0 (`gini=0.0000`), because everyone's the same, and they
    all got an offer at that point. However, you know if we keep going down(where
    the person has not done an internship),we'll be at a point where the entropy is
    0.32\. It's getting lower and lower, that's a good thing.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Next we're going to look at how much experience they have, do they have less
    than one year of experience? And, if the case is that they do have some experience
    and they've gotten this far they're a pretty good no hire decision. We end up
    at the point where we have zero entropy but, all three remaining samples in our
    training set were no hires. We have 3 no hires and 0 hires. But, if they do have
    less experience, then they're probably fresh out of college, they still might
    be worth looking at.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: The final thing we're going to look at is whether or not they went to a Top-tier
    school, and if so, they end up being a good prediction for being a hire. If not,
    they end up being a no hire. We end up with one candidate that fell into that
    category that was a no hire and 0 that were a hire. Whereas, in the case candidates
    did go to a top tier school, we have 0 no hires and 1 hire.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: So, you can see we just keep going until we reach an entropy of 0, if at all
    possible, for every case.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning – Using a random forest
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's say we want to use a random forest, you know, we're worried that
    we might be over fitting our training data. It's actually very easy to create
    a random forest classifier of multiple decision trees.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to do that, we can use the same data that we created before. You just need
    your `*X*` and `*y*` vectors, that is the set of features and the column that
    you''re trying to predict on:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We make a random forest classifier, also available from scikit-learn, and pass
    it the number of trees we want in our forest. So, we made ten trees in our random
    forest in the code above. We then fit that to the model.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: You don't have to walk through the trees by hand, and when you're dealing with
    a random forest you can't really do that anyway. So, instead we use the `predict()`
    function on the model, that is on the classifier that we made. We pass in a list
    of all the different features for a given candidate that we want to predict employment
    for.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'If you remember this maps to these columns: Years Experience, Employed?, Previous
    employers, Level of Education, Top-tier school, and Interned; interpreted as numerical
    values. We predict the employment of an employed 10-year veteran. We also predict
    the employment of an unemployed 10-year veteran. And, sure enough, we get a result:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76760aeb-0024-4cec-8ca3-5cd9c64c2ebe.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: So, in this particular case, we ended up with a hire decision on both. But,
    what's interesting is there is a random component to that. You don't actually
    get the same result every time! More often than not, the unemployed person does
    not get a job offer, and if you keep running this you'll see that's usually the
    case. But, the random nature of bagging, of bootstrap aggregating each one of
    those trees, means you're not going to get the same result every time. So, maybe
    10 isn't quite enough trees. So, anyway, that's a good lesson to learn here!
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For an activity, if you want to go back and play with this, mess around with
    my input data. Go ahead and edit the code we've been exploring, and create an
    alternate universe where it's a topsy turvy world; for example, everyone that
    I gave a job offer to now doesn't get one and vice versa. See what that does to
    your decision tree. Just mess around with it and see what you can do and try to
    interpret the results.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: So, that's decision trees and random forests, one of the more interesting bits
    of machine learning, in my opinion. I always think it's pretty cool to just generate
    a flowchart out of thin air like that. So, hopefully you'll find that useful.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talked about random forests, that was an example of ensemble learning,
    where we're actually combining multiple models together to come up with a better
    result than any single model could come up with. So, let's learn about that in
    a little bit more depth. Let's talk about ensemble learning a little bit more.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'So, remember random forests? We had a bunch of decision trees that were using
    different subsamples of the input data, and different sets of attributes that
    it would branch on, and they all voted on the final result when you were trying
    to classify something at the end. That''s an example of ensemble learning. Another
    example: when we were talking about k-means clustering, we had the idea of maybe
    using different k-means models with different initial random centroids, and letting
    them all vote on the final result as well. That is also an example of ensemble
    learning.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Basically, the idea is that you have more than one model, and they might be
    the same kind of model or it might be different kinds of models, but you run them
    all, on your set of training data, and they all vote on the final result for whatever
    it is you're trying to predict. And oftentimes, you'll find that this ensemble
    of different models produces better results than any single model could on its
    own.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'A good example, from a few years ago, was the Netflix prize. Netflix ran a
    contest where they offered, I think it was a million dollars, to any researcher
    who could outperform their existing movie recommendation algorithm. The ones that
    won were ensemble approaches, where they actually ran multiple recommender algorithms
    at once and let them all vote on the final result. So, ensemble learning can be
    a very powerful, yet simple tool, for increasing the quality of your final results
    in machine learning. Let us now try to explore various types of ensemble learning:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '**Bootstrap aggregating or bagging:** Now, random forests use a technique called
    bagging, short for bootstrap aggregating. This means that we take random subsamples
    of our training data and feed them into different versions of the same model and
    let them all vote on the final result. If you remember, random forests took many
    different decision trees that use a different random sample of the training data
    to train on, and then they all came together in the end to vote on a final result.
    That''s bagging.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting:** Boosting is an alternate model, and the idea here is that you
    start with a model, but each subsequent model boosts the attributes that address
    the areas that were misclassified by the previous model. So, you run train/tests
    on a model, you figure out what are the attributes that it''s basically getting
    wrong, and then you boost those attributes in subsequent models - in hopes that
    those subsequent models will pay more attention to them, and get them right. So,
    that''s the general idea behind boosting. You run a model, figure out its weak
    points, amplify the focus on those weak points as you go, and keep building more
    and more models that keep refining that model, based on the weaknesses of the
    previous one.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bucket of models:** Another technique, and this is what that Netflix prize-winner
    did, is called a bucket of models, where you might have entirely different models
    that try to predict something. Maybe I''m using k-means, a decision tree, and
    regression. I can run all three of those models together on a set of training
    data and let them all vote on the final classification result when I''m trying
    to predict something. And maybe that would be better than using any one of those
    models in isolation.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stacking:** Stacking has the same idea. So, you run multiple models on the
    data, combine the results together somehow. The subtle difference here between
    bucket of models and stacking, is that you pick the model that wins. So, you''d
    run train/test, you find the model that works best for your data, and you use
    that model. By contrast, stacking will combine the results of all those models
    together, to arrive at a final result.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, there is a whole field of research on ensemble learning that tries to find
    the optimal ways of doing ensemble learning, and if you want to sound smart, usually
    that involves using the word Bayes a lot. So, there are some very advanced methods
    of doing ensemble learning but all of them have weak points, and I think this
    is yet another lesson in that we should always use the simplest technique that
    works well for us.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'Now these are all very complicated techniques that I can''t really get into
    in the scope of this book, but at the end of the day, it''s hard to outperform
    just the simple techniques that we''ve already talked about. A few of the complex
    techniques are listed here:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayes optical classifier:** In theory, there''s something called the Bayes
    Optimal Classifier that will always be the best, but it''s impractical, because
    it''s computationally prohibitive to do it.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian parameter averaging:** Many people have tried to do variations of
    the Bayes Optimal Classifier to make it more practical, like the Bayesian Parameter
    Averaging variation. But it''s still susceptible to overfitting and it''s often
    outperformed by bagging, which is the same idea behind random forests; you just
    resample the data multiple times, run different models, and let them all vote
    on the final result. Turns out that works just as well, and it''s a heck of a
    lot simpler!'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian model combination:** Finally, there''s something called Bayesian
    Model Combination that tries to solve all the shortcomings of Bayes Optimal Classifier
    and Bayesian Parameter Averaging. But, at the end of the day, it doesn''t do much
    better than just cross validating against the combination of models.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, these are very complex techniques that are very difficult to use. In
    practice, we're better off with the simpler ones that we've talked about in more
    detail. But, if you want to sound smart and use the word Bayes a lot it's good
    to be familiar with these techniques at least, and know what they are.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: So, that's ensemble learning. Again, the takeaway is that the simple techniques,
    like bootstrap aggregating, or bagging, or boosting, or stacking, or bucket of
    models, are usually the right choices. There are some much fancier techniques
    out there but they're largely theoretical. But, at least you know about them now.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: It's always a good idea to try ensemble learning out. It's been proven time
    and time again that it will produce better results than any single model, so definitely
    consider it!
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machine overview
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we're going to talk about **support vector machines** (**SVM**), which
    is a very advanced way of clustering or classifying higher dimensional data.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: So, what if you have multiple features that you want to predict from? SVM can
    be a very powerful tool for doing that, and the results can be scarily good! It's
    very complicated under the hood, but the important things are understanding when
    to use it, and how it works at a higher level. So, let's cover SVM now.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines is a fancy name for what actually is a fancy concept.
    But fortunately, it's pretty easy to use. The important thing is knowing what
    it does, and what it's good for. So, support vector machines works well for classifying
    higher-dimensional data, and by that I mean lots of different features. So, it's
    easy to use something like k-means clustering, to cluster data that has two dimensions,
    you know, maybe age on one axis and income on another. But, what if I have many,
    many different features that I'm trying to predict from. Well, support vector
    machines might be a good way of doing that.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines finds higher-dimensional support vectors across which
    to divide the data (mathematically, these support vectors define hyperplanes).
    That is, mathematically, what support vector machines can do is find higher dimensional
    support vectors (that's where it gets its name from) that define the higher-dimensional
    planes that split the data into different clusters.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Obviously the math gets pretty weird pretty quickly with all this. Fortunately,
    the `scikit-learn` package will do it all for you, without you having to actually
    get into it. Under the hood, you need to understand though that it uses something
    called the kernel trick to actually find those support vectors or hyperplanes
    that might not be apparent in lower dimensions. There are different kernels you
    can use, to do this in different ways. The main point is that SVM's are a good
    choice if you have higher- dimensional data with lots of different features, and
    there are different kernels you can use that have varying computational costs
    and might be better fits for the problem at hand.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: The important point is that SVMs employ some advanced mathematical trickery
    to cluster data, and it can handle data sets with lots of features. It's also
    fairly expensive - the "kernel trick" is the only thing that makes it possible.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: I want to point out that SVM is a supervised learning technique. So, we're actually
    going to train it on a set of training data, and we can use that to make predictions
    for future unseen data or test data. It's a little bit different than k-means
    clustering and that k-means was completely unsupervised; with a support vector
    machine, by contrast, it is training based on actual training data where you have
    the answer of the correct classification for some set of data that it can learn
    from. So, SVM's are useful for classification and clustering, if you will - but
    it's a supervised technique!
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: One example that you often see with SVMs is using something called support vector
    classification. The typical example uses the Iris dataset which is one of the
    sample datasets that comes with scikit-learn. This set is a classification of
    different flowers, different observations of different Iris flowers and their
    species. The idea is to classify these using information about the length and
    width of the petal on each flower, and the length and width of the sepal of each
    flower. (The sepal, apparently, is a little support structure underneath the petal.
    I didn't know that until now either.) You have four dimensions of attributes there;
    you have the length and width of the petal, and the length and the width of the
    sepal. You can use that to predict the species of an Iris given that information.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example of doing that with SVC: basically, we have sepal width and
    sepal length projected down to two dimensions so we can actually visualize it:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11af3028-7b21-40ac-94ae-b9db94d97655.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: With different kernels you might get different results. SVC with a linear kernel
    will produce something very much as you see in the preceding image. You can use
    polynomial kernels or fancier kernels that might project down to curves in two
    dimensions as shown in the image. You can do some pretty fancy classification
    this way.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: These have increasing computational costs, and they can produce more complex
    relationships. But again, it's a case where too much complexity can yield misleading
    results, so you need to be careful and actually use train/test when appropriate.
    Since we are doing supervised learning, you can actually do train/test and find
    the right model that works, or maybe use an ensemble approach.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: You need to arrive at the right kernel for the task at hand. For things like
    polynomial SVC, what's the right degree polynomial to use? Even things like linear
    SVC will have different parameters associated with them that you might need to
    optimize for. This will make more sense with a real example, so let's dive into
    some actual Python code and see how it works!
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Using SVM to cluster people by using scikit-learn
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's try out some support vector machines here. Fortunately, it's a lot easier
    to use than it is to understand. We're going to go back to the same example I
    used for k-means clustering, where I'm going to create some fabricated cluster
    data about ages and incomes of a hundred random people.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to go back to the k-means clustering section, you can learn more
    about kind of the idea behind this code that generates the fake data. And if you''re
    ready, please consider the following code:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Please note that because we're using supervised learning here, we not only need
    the feature data again, but we also need the actual answers for our training dataset.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: What the `createClusteredData()` function does here, is to create a bunch of
    random data for people that are clustered around `k` points, based on age and
    income, and it returns two arrays. The first array is the feature array, that
    we're calling `X`, and then we have the array of the thing we're trying to predict
    for, which we're calling `y`. A lot of times in scikit-learn when you're creating
    a model that you can predict from, those are the two inputs that it will take,
    a list of feature vectors, and the thing that you're trying to predict, that it
    can learn from. So, we'll go ahead and run that.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'So now we''re going to use the `createClusteredData()` function to create 100
    random people with 5 different clusters. We will just create a scatter plot to
    illustrate those, and see where they land up:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The following graph shows our data that we're playing with. Every time you run
    this you're going to get a different set of clusters. So, you know, I didn't actually
    use a random seed... to make life interesting.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'A couple of new things here--I''m using the `figsize` parameter on `plt.figure()`
    to actually make a larger plot. So, if you ever need to adjust the size in `matplotlib`,
    that''s how you do it. I''m using that same trick of using the color as the classification
    number that I end up with. So the number of the cluster that I started with is
    being plotted as the color of these data points. You can see, it''s a pretty challenging
    problem, there''s definitely some intermingling of clusters here:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18ecc82e-4a00-47b9-b93e-7b7de259f4ab.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
- en: Now we can use linear SVC (SVC is a form of SVM), to actually partition that
    into clusters. We're going to use SVM with a linear kernel, and with a C value
    of `1.0`. C is just an error penalty term that you can adjust; it's `1` by default.
    Normally, you won't want to mess with that, but if you're doing some sort of convergence
    on the right model using ensemble learning or train/test, that's one of the things
    you can play with. Then, we will fit that model to our feature data, and the actual
    classifications that we have for our training dataset.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: So, let's go ahead and run that. I don't want to get too much into how we're
    actually going to visualize the results here, just take it on faith that `plotPredictions()`
    is a function that can plot the classification ranges and SVC.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'It helps us visualize where different classifications come out. Basically,
    it''s creating a mesh across the entire grid, and it will plot different classifications
    from the SVC models as different colors on that grid, and then we''re going to
    plot our original data on top of that:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'So, let''s see how that works out. SVC is computationally expensive, so it
    takes a long time to run:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1342ff6-9d4a-4546-a76a-ed1092efdbb0.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
- en: You can see here that it did its best. Given that it had to draw straight lines,
    and polygonal shapes, it did a decent job of fitting to the data that we had.
    So, you know, it did miss a few - but by and large, the results are pretty good.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'SVC is actually a very powerful technique; it''s real strength is in higher
    dimensional feature data. Go ahead and play with it. By the way if you want to
    not just visualize the results, you can use the `predict()` function on the SVC
    model, just like on pretty much any model in scikit-learn, to pass in a feature
    array that you''re interested in. If I want to predict the classification for
    someone making $200,000 a year who was 40 years old, I would use the following
    code:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This would put that person in, in our case, cluster number 1:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a7954d2-6d7d-482b-a3dc-05b75deeab61.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
- en: 'If I had a someone making $50,000 here who was 65, I would use the following
    code:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This is what your output should now look like:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30e1d81a-b05c-48a1-9f17-ac864d6e9cd6.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
- en: That person would end up in cluster number 2, whatever that represents in this
    example. So, go ahead and play with it.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, linear is just one of many kernels that you can use, like I said there
    are many different kernels you can use. One of them is a polynomial model, so
    you might want to play with that. Please do go ahead and look up the documentation.
    It's good practice for you to looking at the docs. If you're going to be using
    scikit-learn in any sort of depth, there's a lot of different capabilities and
    options that you have available to you. So, go look up scikit-learn online, find
    out what the other kernels are for the SVC method, and try them out, see if you
    actually get better results or not.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: This is a little exercise, not just in playing with SVM and different kinds
    of SVC, but also in familiarizing yourself with how to learn more on your own
    about SVC. And, honestly, a very important trait of any data scientist or engineer
    is going to be the ability to go and look up information yourself when you don't
    know the answers.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: So, you know, I'm not being lazy by not telling you what those other kernels
    are, I want you to get used to the idea of having to look this stuff up on your
    own, because if you have to ask someone else about these things all the time you're
    going to get really annoying, really fast in a workplace. So, go look that up,
    play around it, see what you come up with.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: So, that's SVM/SVC, a very high power technique that you can use for classifying
    data, in supervised learning. Now you know how it works and how to use it, so
    keep that in your bag of tricks!
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw some interesting machine learning techniques. We covered
    one of the fundamental concepts behind machine learning called train/test. We
    saw how to use train/test to try to find the right degree polynomial to fit a
    given set of data. We then analyzed the difference between supervised and unsupervised
    machine learning.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: We saw how to implement a spam classifier and enable it to determine whether
    an email is spam or not using the Naive Bayes technique. We talked about k-means
    clustering, an unsupervised learning technique, which helps group data into clusters.
    We also looked at an example using scikit-learn which clustered people based on
    their income and age.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: We then went on to look at the concept of entropy and how to measure it. We
    walked through the concept of decision trees and how, given a set of training
    data, you can actually get Python to generate a flowchart for you to actually
    make a decision. We also built a system that automatically filters out resumes
    based on the information in them and predicts the hiring decision of a person.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: We learned along the way the concept of ensemble learning, and we concluded
    by talking about support vector machines, which is a very advanced way of clustering
    or classifying higher dimensional data. We then moved on to use SVM to cluster
    people using scikit-learn. In the next chapter, we'll talk about recommender systems.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
