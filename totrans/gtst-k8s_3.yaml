- en: Chapter 3. Core Concepts – Networking, Storage, and Advanced Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be covering how the Kubernetes cluster handles networking
    and how it differs from other approaches. We will be describing the three requirements
    for Kubernetes networking solutions and exploring why these are key to ease of
    operations. Further, we will take a deeper dive into services and how the Kubernetes
    proxy works on each node. Towards the end, we will take a look at storage concerns
    and how we can persist data across pods and the container life cycle. Finishing
    up, we will see a brief overview of some higher level isolation features for multitenancy.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will discuss the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced services concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistent storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Namespace limits and quotas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Networking is a vital concern for production-level operations. At a service
    level, we need a reliable way for our application components to find and communicate
    with each other. Introduce containers and clustering into the mix and things get
    more complex as we now have multiple networking namespaces to bear in mind. Communication
    and discovery now becomes a feat that must traverse container IP space, host networking,
    and sometimes even multiple data center network topologies.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes benefits here from getting its ancestry from the clustering tools
    used by Google for the past decade. Networking is one area where Google has outpaced
    the competition with one of the largest networks on the planet. Early on, Google
    built its own hardware switches and **Software-defined Networking** (**SDN**)
    to give them more control, redundancy, and efficiency in their day-to-day network
    operations¹. Many of the lessons learned from running and networking two billion
    containers per week have been distilled into Kubernetes and informed how K8s networking
    is done.
  prefs: []
  type: TYPE_NORMAL
- en: Networking in Kubernetes requires that each pod have its own IP address. Implementation
    details may vary based on the underlying infrastructure provider. However, all
    implementations must adhere to some basic rules. First and second, Kubernetes
    does not allow the use of **Network Address Translation** (**NAT**) for container-to-container
    or for container-to-node (minion) traffic. Further, the internal container IP
    address must match the IP address that is used to communicate with it.
  prefs: []
  type: TYPE_NORMAL
- en: These rules keep much of the complexity out of our networking stack and ease
    the design of the applications. Further, it eliminates the need to redesign network
    communication in legacy applications that are migrated from existing infrastructure.
    Finally, in greenfield applications, it allows for greater scale in handling hundreds,
    or even thousands, of services and application communication.
  prefs: []
  type: TYPE_NORMAL
- en: K8s achieves this pod-wide IP magic by using a **placeholder**. Remember that
    `pause` container we saw in [Chapter 1](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 1. Kubernetes and Container Operations"), *Kubernetes and Container Operations*,
    under the *Services running on the master* section. That is often referred to
    as a **pod infrastructure container**, and it has the important job of reserving
    the network resources for our application containers that will be started later
    on. Essentially, the pause container holds the networking namespace and IP address
    for the entire pod and can be used by all the containers running within.
  prefs: []
  type: TYPE_NORMAL
- en: Networking comparisons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In getting a better understanding of networking in containers, it can be instructive
    to look at other approaches to container networking.
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Docker Engine** by default uses a *bridged* networking mode. In this mode,
    the container has its own networking namespace and is then bridged via virtual
    interfaces to the host (or node in the case of K8s) network.
  prefs: []
  type: TYPE_NORMAL
- en: In the *bridged* mode, two containers can use the same IP range because they
    are completely isolated. Therefore, service communication requires some additional
    port mapping through the host side of network interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Docker also supports a *host* mode, which allows the containers to use the host
    network stack. Performance is greatly benefited since it removes a level of network
    virtualization; however, you lose the security of having an isolated network namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Docker supports a *container* mode, which shares a network namespace
    between two containers. The containers will share the namespace and IP address,
    so containers cannot use the same ports.
  prefs: []
  type: TYPE_NORMAL
- en: In all these scenarios, we are still on a single machine, and outside of a host
    mode, the container IP space is not available outside that machine. Connecting
    containers across two machines then requires **Network Address Translation** (**NAT**)
    and **port mapping** for communication.
  prefs: []
  type: TYPE_NORMAL
- en: Docker plugins (libnetwork)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to address the cross-machine communication issue, Docker has released
    new network plugins, which just moved out of experimental support as we went to
    press. This plugin allows networks to be created independent of the containers
    themselves. In this way, containers can join the same existing *networks*. Through
    the new plugin architecture, various drivers can be provided for different network
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: The first of these is the **overlay** driver. In order to coordinate across
    multiple hosts, they must all agree on the available networks and their topologies.
    The overlay driver uses a distributed key-value store to synchronize the network
    creation across multiple hosts.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that the plugin mechanism will allow a wide range of
    networking possibilities in Docker. In fact, many of the third-party options such
    as Weave are already creating their own Docker network plugins.
  prefs: []
  type: TYPE_NORMAL
- en: Weave
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Weave** provides an overlay network for Docker containers. It can be used
    as a plugin with the new Docker network plugin interface, and it is also compatible
    with Kubernetes. Like many overlay networks, many criticize the performance impact
    of the encapsulation overhead. Note that they have recently added a preview release
    with **Virtual Extensible LAN** (**VXLAN**) encapsulation support, which greatly
    improves performance. For more information, visit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://blog.weave.works/2015/06/12/weave-fast-datapath/](http://blog.weave.works/2015/06/12/weave-fast-datapath/)'
  prefs: []
  type: TYPE_NORMAL
- en: Flannel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Flannel** comes from CoreOS and is an etcd-backed overlay. Flannel gives
    a full subnet to each host/node enabling a similar pattern to the Kubernetes practice
    of a routable IP per pod or group of containers. Flannel includes an in-kernel
    VXLAN encapsulation mode for better performance and has an experimental multinetwork
    mode similar to the overlay Docker plugin. For more information, visit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/coreos/flannel](https://github.com/coreos/flannel)'
  prefs: []
  type: TYPE_NORMAL
- en: Project Calico
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Project Calico** is a layer 3-based networking model that uses the built-in
    routing functions of the Linux kernel. Routes are propagated to virtual routers
    on each host via **Border Gateway Protocol** (**BGP**). Calico can be used for
    anything from small-scale deploys to large Internet-scale installations. Because
    it works at a lower level on the network stack, there is no need for additional
    NAT, tunneling, or overlays. It can interact directly with the underlying network
    infrastructure. Additionally, it has a support for network-level ACLs to provide
    additional isolation and security. For more information visit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.projectcalico.org/](http://www.projectcalico.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: Balanced design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It's important to point out the balance Kubernetes is trying to achieve by placing
    the IP at the pod level. Using unique IP addresses at the host level is problematic
    as the number of containers grow. Ports must be used to expose services on specific
    containers and allow external communication. In addition to this, the complexity
    of running multiple services that may or may not know about each other (and their
    custom ports), and managing the port space becomes a big issue.
  prefs: []
  type: TYPE_NORMAL
- en: However, assigning an IP address to each container can be overkill. In cases
    of sizable scale, overlay networks and NATs are needed in order to address each
    container. Overlay networks add latency, and IP addresses would be taken up by
    backend services as well since they need to communicate with their frontend counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we really see an advantage in the abstractions that Kubernetes provides
    at the application and service level. If I have a web server and a database, we
    can keep them on the same pod and use a single IP address. The web server and
    database can use the local interface and standard ports to communicate, and no
    custom setup is required. Further, services on the backend are not needlessly
    exposed to other application stacks running elsewhere in the cluster (but possibly
    on the same host). Since the pod sees the same IP address that the applications
    running within it see, service discovery does not require any additional translation.
  prefs: []
  type: TYPE_NORMAL
- en: If you need the flexibility of an overlay network, you can still use an overlay
    at the pod level. Both Weave and Flannel overlays, as well as the BGP routing
    Project Calico, can be used with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: This is also very helpful in the context of scheduling the workloads. It is
    a key to have a simple and standard structure for the scheduler to match constraints
    and understand where space exists on the cluster's network at any given time.
    This is a dynamic environment with a variety of applications and tasks running,
    so any additional complexity here will have rippling effects.
  prefs: []
  type: TYPE_NORMAL
- en: There are also implications for service discovery. New services coming online
    must determine and register an IP address on which the rest of the world, or at
    least cluster, can reach them. If NAT is used, the services will need an additional
    mechanism to learn their externally facing IP.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's explore the IP strategy as it relates to Services and communication between
    containers. If you recall, in [Chapter 2](part0022_split_000.html#KVCC1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 2. Kubernetes – Core Concepts and Constructs"), *Kubernetes – Core Concepts
    and Constructs* **,** under the *Services* section, you learned that Kubernetes
    is using kube-proxy to determine the proper pod IP address and port serving each
    request. Behind the scenes, kube-proxy is actually using virtual IPs and **iptables**
    to make all this magic work.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that kube-proxy is running on every host. Its first duty is to monitor
    the API from the Kubernetes master. Any updates to services will trigger an update
    to iptables from kube-proxy. For example, when a new service is created, a virtual
    IP address is chosen and a rule in iptables is set, which will direct its traffic
    to kube-proxy via a random port. Thus, we now have a way to capture service-destined
    traffic on this node. Since kube-proxy is running on all nodes, we have cluster-wide
    resolution for the service VIP. Additionally, DNS records can point to this virtual
    IP as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a *hook* created in iptables, we still need to get the traffic
    to the servicing pods; however, the rule is only sending traffic to the service
    entry in kube-proxy at this point. Once kube-proxy receives the traffic for a
    particular service, it must then forward it to a pod in the service''s pool of
    candidates. It does this using a random port that was selected during service
    creation. Refer to the following figure (Figure 3.1) for an overview of the flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Advanced services](../images/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1\. Kube-proxy communication
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book, there are plans in the upcoming version 1.1
    to include a kube-proxy, which does not rely on service entry and uses only iptable
    rules.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is also possible to always forward traffic from the same client IP to same
    backend pod/container using the `sessionAffinity` element in your service definition.
  prefs: []
  type: TYPE_NORMAL
- en: External services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the last chapter, we saw a few service examples. For testing and demonstration
    purposes, we wanted all the services to be externally accessible. This was configured
    by the `type: LoadBalancer` element in our service definition. The `LoadBalancer`
    type creates an external load balancer on the cloud provider. We should note that
    support for external load balancers varies by provider as does the implementation.
    In our case, we are using GCE, so integration is pretty smooth. The only additional
    setup needed is to open firewall rules for the external service ports.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's dig a little deeper and do a `describe` on one of the services from the
    [Chapter 2](part0022_split_000.html#KVCC1-22fbdd9ef660435ca6bcc0309f05b1b7 "Chapter 2. Kubernetes
    – Core Concepts and Constructs"), *Kubernetes – Core Concepts and Constructs*,
    under the *More on labels* section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![External services](../images/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2\. Service description
  prefs: []
  type: TYPE_NORMAL
- en: In the output, in Figure 3.2, you'll note several key elements. Our namespace
    is set to default, **Type:** is `LoadBalancer`, and we have the external IP listed
    under **LoadBalancer Ingress:**. Further, we see **Endpoints:**, which shows us
    the IPs of the pods available to answer service requests.
  prefs: []
  type: TYPE_NORMAL
- en: Internal services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s explore the other types of services we can deploy. First, by default,
    services are internally facing only. You can specify a type of `clusterIP` to
    achieve this, but if no type is defined, `clusterIP` is the assumed type. Let''s
    take a look at an example, note the lack of the `type` element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-1*: `nodejs-service-internal.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use this listing to create the service definition file. You''ll need a healthy
    version of the `node-js` RC (*Listing 2-7*: `nodejs-health-controller-2.yaml`).
    As you can see, the selector matches on the pods named `node-js` that our RC launched
    in the last chapter. We will create the service and then list the currently running
    services with a filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Internal services](../images/00035.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3\. Internal service listing
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we have a new service, but only one IP. Further, the IP address
    is not externally accessible. We won''t be able to test the service from a web
    browser this time. However, we can use the handy `kubectl exec` command and attempt
    to connect from one of the other pods. You will need `node-js-pod` (*Listing 2-1*:
    `nodejs-pod.yaml`) running. Then, you can execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This allows us to run a `docker exec` command as if we had a shell in the `node-js-pod`
    container. It then hits the internal service URL, which forwards to any pods with
    the `node-js` label.
  prefs: []
  type: TYPE_NORMAL
- en: If all is well, you should get the raw HTML output back. So, you've successfully
    created an internal-only service. This can be useful for backend services that
    you want to make available to other containers running in your cluster, but not
    open to the world at large.
  prefs: []
  type: TYPE_NORMAL
- en: Custom load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A third type of service K8s allows is the `NodePort` type. This type allows
    us to expose a service through the host or minion on a specific port. In this
    way, we can use the IP address of any node (minion) and access our service on
    the assigned node port. Kubernetes will assign a node port by default in the range
    of 3000–32767, but you can also specify your own custom port. In the example in
    *Listing 3-2*: `nodejs-service-nodeport.yaml`, we choose port `30001` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-2*: `nodejs-service-nodeport.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, create this YAML definition file and create your service as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should have a message like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Custom load balancing](../images/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4\. New GCP firewall rule
  prefs: []
  type: TYPE_NORMAL
- en: You'll note a message about opening firewall ports. Similar to the external
    load balancer type, `NodePort` is exposing your service externally using ports
    on the nodes. This could be useful if, for example, you want to use your own load
    balancer in front of the nodes. Let's make sure that we open those ports on GCP
    before we test our new service.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the GCE VM instance console, click on the network for any of your nodes
    (minions). In my case, it was default. Under firewall rules, we can add a rule
    by clicking **Add firewall rule**. Create a rule like the one shown in Figure
    3.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Custom load balancing](../images/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5\. New GCP firewall rule
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now test our new service out, by opening a browser and using an IP address
    of any node (minion) in your cluster. The format to test the new service is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://`**`<Minoion IP Address>`**`:``<NodePort>``/`'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-node proxy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember that kube-proxy is running on all the nodes, so even if the pod is
    not running there, traffic will be given a proxy to the appropriate host. Refer
    to Figure 3.6 for a visual on how the traffic flows. A user makes a request to
    an external IP or URL. The request is serviced by **Node 1** in this case. However,
    the pod does not happen to run on this node. This is not a problem because the
    pod IP addresses are routable. So, **Kube-proxy** simply passes traffic on to
    the pod IP for this service. The network routing then completes on **Node 2**,
    where the requested application lives.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross-node proxy](../images/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6\. Cross-node traffic
  prefs: []
  type: TYPE_NORMAL
- en: Custom ports
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Services also allow you to map your traffic to different ports, then the containers
    and pods themselves expose. We will create a service that exposes `port 90` and
    forwards traffic to `port 80` on the pods. We will call the `node-js-90` pod to
    reflect the custom port number. Create the following two definition files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-3*: `nodejs-customPort-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-4*: `nodejs-customPort-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: You'll note that in the service definition, we have a `targetPort` element.
    This element tells the service the port to use for pods/containers in the pool.
    As we saw in previous examples, if you do not specify `targetPort`, it assumes
    that it's the same port as the service. Port is still used as the service port,
    but in this case, we are going to expose the service on port `90` while the containers
    serve content on port `80`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create this RC and service and open the appropriate firewall rules, as we did
    in the last example. It may take a moment for the external load balancer IP to
    propagate to the `get service` command. Once it does, you should be able to open
    and see our familiar web application in a browser using the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://`**`<external service IP>`**`:90/`'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple ports
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another custom port use case is that of multiple ports. Many applications expose
    multiple ports, such as HTTP on port `80` and port `8888` for web servers. The
    following example shows our app responding on both ports. Once again, we''ll also
    need to add a firewall rule for this port, as we did for *Listing 3-2*: `nodejs-service-nodeport.yaml`
    previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-5*: `nodejs-multicontroller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-6*: `nodejs-multiservice.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the application and container itself must be listening on both ports
    for this to work. In this example, port `8888` is used to represent a fake admin
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: If, for example, you want to listen on port 443, you would need a proper SSL
    socket listening on the server.
  prefs: []
  type: TYPE_NORMAL
- en: Migrations, multicluster, and more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you've seen so far, Kubernetes offers a high level of flexibility and customization
    to create a service abstraction around your containers running in the cluster.
    However, there may be times where you want to point to something outside your
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: An example of this would be working with legacy systems, or even applications
    running on another cluster. In the case of the former, this is a perfectly good
    strategy in order to migrate to Kubernetes and containers in general. We can begin
    to manage the service endpoints in Kubernetes while stitching the stack together
    using the K8s orchestration concepts. Additionally, we can even start bringing
    over pieces of the stack, as the frontend, one at a time as the organization refactors
    applications for microservices and/or containerization.
  prefs: []
  type: TYPE_NORMAL
- en: 'To allow access to non-pod–based applications, the services construct allows
    you to use endpoints that are outside the cluster. Kubernetes is actually creating
    an endpoint resource every time you create a service that uses selectors. The
    `endpoints` object keeps track of the pod IPs in the load balancing pool. You
    can see this by running a `get endpoints` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You'll note an entry for all the services we currently have running on our cluster.
    For most, the endpoints are just the IP of each pod running in a RC. As I mentioned,
    Kubernetes does this automatically based on the selector. As we scale the replicas
    in a controller with matching labels, Kubernetes will update the endpoints automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to create a service for something that is not a pod and therefore
    has no labels to select, we can easily do this with both a service and endpoint
    definition as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-7*: `nodejs-custom-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-8*: `nodejs-custom-endpoint.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, you'll need to replace the `<X.X.X.X>` with a real
    IP address where the new service can point. In my case, I used the public load
    balancer IP from `node-js-multiservice` we created earlier. Go ahead and create
    these resources now.
  prefs: []
  type: TYPE_NORMAL
- en: If we now run a `get endpoints` command, we will see this IP address at port
    `80` associated with the `custom-service` endpoint. Further, if we look at the
    service details, we will see the IP listed in the `Endpoints` section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We can test out this new service by opening the `custom-service` external IP
    from a browser.
  prefs: []
  type: TYPE_NORMAL
- en: Custom addressing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another option to customize services is with the `clusterIP` element. In our
    examples this far, we''ve not specified an IP address, which means that it chooses
    the internal address of the service for us. However, we can add this element and
    choose the IP address in advance with something like `clusterip: 10.0.125.105`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There may be times when you don''t want to load balance and would rather have
    DNS with *A* records for each pod. For example, software that needs to replicate
    data evenly to all nodes may rely on *A* records to distribute data. In this case,
    we can use an example like the following one and set `clusterip` to `None`. Kubernetes
    will not assign an IP address and instead only assign *A* records in DNS for each
    of the pods. If you are using DNS, the service should be available at `node-js-none`
    or `node-js-none.default.cluster.local` from within the cluster. We have the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-9*: `nodejs-headless-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test it out after you create this service with the trusty `exec` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Service discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we discussed earlier, the Kubernetes master keeps track of all service definitions
    and updates. Discovery can occur in one of three ways. The first two methods use
    Linux environment variables. There is support for the Docker link style of environment
    variables, but Kubernetes also has its own naming convention. Here is an example
    of what our `node-js` service example might look like using K8s environment variables
    (note IPs will vary):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-10*: *Service environment variables*'
  prefs: []
  type: TYPE_NORMAL
- en: Another option for discovery is through DNS. While environment variables can
    be useful when DNS is not available, it has drawbacks. The system only creates
    variables at creation time, so services that come online later will not be discovered
    or would require some additional tooling to update all the system environments.
  prefs: []
  type: TYPE_NORMAL
- en: DNS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DNS solves the issues seen with environment variables by allowing us to reference
    the services by their name. As services restart, scale out, or appear anew, the
    DNS entries will be updating and ensuring that the service name always points
    to the latest infrastructure. DNS is set up by default in most of the supported
    providers.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If DNS is supported by your provider, but not setup, you can configure the
    following variables in your default provider `config` when you create your Kubernetes
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: With DNS active, services can be accessed in one of two forms—either the service
    name itself, `<service-name>`, or a fully qualified name that includes the namespace,
    `<service-name>.<namespace-name>.cluster.local`. In our examples, it would look
    similar to `node-js-90` or `node-js-90.default.cluster.local`.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s switch gears for a moment and talk about another core concept: persistent
    storage. When you start moving from development to production, one of the most
    obvious challenges you face is the transient nature of containers themselves.
    If you recall our discussion of layered file systems in [Chapter 1](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 1. Kubernetes and Container Operations"), *Kubernetes and Container Operations*,
    the top layer is writable. (It''s also frosting, which is delicious.) However,
    when the container dies, the data goes with it. The same is true for crashed containers
    that Kubernetes restarts.'
  prefs: []
  type: TYPE_NORMAL
- en: This is where **persistent disks** (**PDs**), or volumes, come into play. A
    persistent volume that exists outside the container allows us to save our important
    data across containers outages. Further, if we have a volume at the pod level,
    data can be shared between containers in the same application stack and within
    the same pod.
  prefs: []
  type: TYPE_NORMAL
- en: Docker itself has some support for volumes, but Kubernetes gives us persistent
    storage that lasts beyond the lifetime of a single container. The volumes are
    tied to pods and live and die with those pods. Additionally, a pod can have multiple
    volumes from a variety of sources. Let's take a look at some of these sources.
  prefs: []
  type: TYPE_NORMAL
- en: Temporary disks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the easiest ways to achieve improved persistence amid container crashes
    and data sharing within a pod is to use the `emptydir` volume. This volume type
    can be used with either the storage volumes of the node machine itself or an optional
    RAM disk for higher performance.
  prefs: []
  type: TYPE_NORMAL
- en: Again, we improve our persistence beyond a single container, but when a pod
    is removed, the data will be lost. Machine reboot will also clear any data from
    RAM-type disks. There may be times when we just need some shared temporary space
    or have containers that process data and hand it off to another container before
    they die. Whatever the case, here is a quick example of using this temporary disk
    with the RAM-backed option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open your favorite editor and create a file like the one in *Listing 3-11*:
    `storage-memory.yaml` here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-11*: `storage-memory.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s probably second nature by now, but we will once again issue a `create`
    command followed by an `exec` command to see the folders in the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This will give us a bash shell in the container itself. The `ls` command shows
    us a `memory-pd` folder at the top level. We use `grep` to filter the output,
    but you can run the command without `| grep memory-pd` to see all folders.
  prefs: []
  type: TYPE_NORMAL
- en: '![Temporary disks](../images/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7\. Temporary storage inside a container
  prefs: []
  type: TYPE_NORMAL
- en: Again, this folder is quite temporary as everything is stored in the minion's
    RAM. When the node gets restarted, all the files will be erased. We will look
    at a more permanent example next.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud volumes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many companies will already have significant infrastructure running in the public
    cloud. Luckily, Kubernetes has native support for the persistent volume types
    provided by two of the most popular providers.
  prefs: []
  type: TYPE_NORMAL
- en: GCE persistent disks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's create a new **GCE persistent volume**. From the console, under **Compute**,
    go to **Disks**. On this new screen, click on the **New disk** button.
  prefs: []
  type: TYPE_NORMAL
- en: We'll be presented with a screen similar to Figure 3.8\. Choose a name for this
    volume and give it a brief description. Make sure that the zone is the same as
    the nodes in your cluster. GCE PDs can only be attached to machines in the same
    zone.
  prefs: []
  type: TYPE_NORMAL
- en: Enter `mysite-volume-1` for the **Name**. Choose a **Source type** of **None
    (blank disk)** and give `10` (10 GB) as value in **Size (GB)**. Finally, click
    on **Create**.
  prefs: []
  type: TYPE_NORMAL
- en: '![GCE persistent disks](../images/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8\. GCE new persistent disk
  prefs: []
  type: TYPE_NORMAL
- en: 'The nice thing about PDs on GCE is that they allow for mounting to multiple
    machines (nodes in our case). However, when mounting to multiple machines, the
    volume must be in read-only mode. So, let''s first mount this to a single pod,
    so we can create some files. Use *Listing 3-12*: `storage-gce.yaml` as follows
    to create a pod that will mount the disk in read/write mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-12*: `storage-gce.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: First, let's issue a `create` command followed by a describe to find out which
    node it is running on. Note the node and save the pod IP address for later. Then,
    open an SSH session into the node.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we''ve already looked at the volume from inside the running container,
    let''s access it directly from the minion node itself this time. We will run a
    `df` command to see where it is mounted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the GCE volume is mounted directly to the node itself. We can
    use the mount path listed in the output of the earlier `df` command. Use `cd`
    to change to the folder now. Then, create a new file named `index.html` with your
    favorite editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Enter a quaint message such as `Hello from my GCE PD!`. Now save the file and
    exit the editor. If you recall from *Listing 3-12*: `storage-gce.yaml`, the PD
    is mounted directly to the NGINX html directory. So, let''s test this out while
    we still have the SSH session open on the node. Do a simple `curl` command to
    the pod IP we wrote down earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: You should see **Hello from my GCE PD!** or whatever message you saved in the
    `index.html` file. In a real-world scenario, we could use the volume for an entire
    website or any other central storage. Let's take a look at running a set of load
    balanced web servers all pointing to the same volume.
  prefs: []
  type: TYPE_NORMAL
- en: First, leave the SSH session with `exit`. Before we proceed, we will need to
    remove our `test-gce` pod so that the volume can be mounted read-only across a
    number of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create a RC that will run three web servers all mounting the same
    persistent volume as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-13*: `http-pd-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also create an external service, so we can see it from outside the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-14*: `http-pd-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go ahead and create these two resources now. Wait a few moments for the external
    IP to get assigned. After this, a `describe` command will give us the IP we can
    use in a browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![GCE persistent disks](../images/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9\. K8s service with GCE PD shared across three pods
  prefs: []
  type: TYPE_NORMAL
- en: Type the IP address into a browser, and you should see your familiar `index.html`
    file show up with the text we entered previously!
  prefs: []
  type: TYPE_NORMAL
- en: AWS Elastic Block Store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: K8s also supports AWS **Elastic Block Store** (**EBS**) volumes. Like the GCE
    PDs, EBS volumes are required to be attached to an instance running in the same
    availability zone. A further limitation is that EBS can only be mounted to a single
    instance at one time.
  prefs: []
  type: TYPE_NORMAL
- en: For brevity, we will not walk through an AWS example, but a sample YAML file
    is included to get you started. Again, remember to create the EBS volume before
    your pod.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-15*: `storage-aws.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: Other PD options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes supports a variety of other types of persistent storage. A full
    list can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://kubernetes.io/v1.0/docs/user-guide/volumes.html#types-of-volumes](http://kubernetes.io/v1.0/docs/user-guide/volumes.html#types-of-volumes)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few that may be of particular interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nfs`: This type allows us to mount a **Network File Share** (**NFS**), which
    can be very useful for both persisting the data and sharing it across the infrastructure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gitrepo`: As you might have guessed, this option clones a Git repo into an
    a new and empty folder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multitenancy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes also has an additional construct for isolation at the cluster level.
    In most cases, you can run Kubernetes and never worry about namespaces; everything
    will run in the default namespace if not specified. However, in cases where you
    run multitenancy communities or want broad-scale segregation and isolation of
    the cluster resources, namespaces can be used to this end.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, Kubernetes has two namespaces: `default` and `kube-system`. `kube-system`
    is used for all the system-level containers we saw in [Chapter 1](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 1. Kubernetes and Container Operations"), *Kubernetes and Container Operations*,
    under the *Services running on the minions* section. The UI, logging, DNS, and
    so on are all run under `kube-system`. Everything else the user creates runs in
    the default namespace. However, our resource definition files can optionally specify
    a custom namespace. For the sake of experimenting, let''s take a look at how to
    build a new namespace.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll need to create a namespace definition file like the one in this
    listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-16*: `test-ns.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can go ahead and create this file with our handy `create` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create resources that use the `test` namespace. The following is
    an example of a pod using this new namespace. We have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-17*: `ns-pod.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the pod can still access services in other namespaces, it will need to
    use the long DNS form of `<service-name>.<namespace-name>.cluster.local`. For
    example, if you were to run command from inside the container in *Listing 3-17*:
    `ns-pod.yaml`, you could use `http-pd.default.cluster.local` to access the PD
    example from *Listing 3-14*: `http-pd-service.yaml`.'
  prefs: []
  type: TYPE_NORMAL
- en: Limits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s inspect our new namespace a bit more. Run the `describe` command as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Limits](../images/00042.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10\. Namespace describe
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes allows you to both limit the resources used by individual pods or
    containers and the resources used by the overall namespace using quotas. You'll
    note that there are no resource **limits** or **quotas** currently set on the
    test namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to limit the footprint of this new namespace; we can set quotas
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-18*: `quota.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that in reality, namespaces would be for larger application communities
    and would probably never have quotas this low. I am using this in order to ease
    illustration of the capability in the example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will create a quota of 3 pods, 1 RC, and 1 service for the test namespace.
    As you probably guessed, this is executed once again by our trusty `create` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have that in place, let''s use `describe` on the namespace as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Limits](../images/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11\. Namespace describe after quota is set
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll note that we now have some values listed in the quota section and the
    limits section is still blank. We also have a **Used** column, which lets us know
    how close to the limits we are at the moment. Let''s try to spin up a few pods
    using the following definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-19*: `busybox-ns.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: You'll note that we are creating four replicas of this basic pod. After using
    `create` to build this RC, run the `describe` command on the `test` namespace
    once more. You'll note that the `used` values for pods and RCs are at their max.
    However, we asked for four replicas and only see three pods in use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what''s happening with our RC. You might tempt to do that with the
    command here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: However, if you try, you'll be disparaged to see a **not found** message from
    the server. This is because we created this RC in a new namespace and `kubectl`
    assumes the default namespace if not specified. This means that we need to specify
    `--namepsace=test` with every command when we wish to access resources in the
    `test` namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can also set the current namespace by working with the context settings.
    First, we need to find our current context, which is found with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can take that context and set the namespace variable like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Now you can run the `kubectl` command without the need to specify the namespace.
    Just remember to switch back when you want to look at the resources running in
    your `default` namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the command with the namespace specified like so. If you''ve set your current
    namespace as demonstrated in the tip box, you can leave off the `--namespace`
    argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Limits](../images/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12\. Namespace quotas
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding image, the first three pods were successfully
    created, but our final one fails with the error **Limited to 3 pods**.
  prefs: []
  type: TYPE_NORMAL
- en: This is an easy way to set limits for resources partitioned out at a community
    scale. It's worth noting that you can also set quotas for CPU, memory, persistent
    volumes, and secrets. Additionally, limits work similar to quota, but they set
    the limit for each pod or container within the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We took a deeper look into networking and services in Kubernetes. You should
    now understand how networking communications are designed in K8s and feel comfortable
    accessing your services internally and externally. We saw how kube-proxy balances
    traffic both locally and across the cluster. We also looked briefly at how DNS
    and service discovery is achieved in Kubernetes. In the later portion of the chapter,
    we explored a variety of persistent storage options. We finished off with quick
    look at namespace and isolation for multitenancy.
  prefs: []
  type: TYPE_NORMAL
- en: Footnotes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ¹[http://www.wired.com/2015/06/google-reveals-secret-gear-connects-online-empire/](http://www.wired.com/2015/06/google-reveals-secret-gear-connects-online-empire/)
  prefs: []
  type: TYPE_NORMAL
