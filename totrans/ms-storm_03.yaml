- en: Storm Parallelism and Data Partitioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first two chapters, we have covered the introduction to Storm, the installation
    of Storm, and developing a sample topology. In this chapter, we are focusing on
    distribution of the topology on multiple Storm machines/nodes. This chapter covers
    the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism of topology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to configure parallelism at the code level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of stream groupings in a Storm cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guaranteed message processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tick tuple
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelism of a topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallelism means the distribution of jobs on multiple nodes/instances where
    each instance can work independently and can contribute to the processing of data.
    Let's first look at the processes/components that are responsible for the parallelism
    of a Storm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Worker process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Storm topology is executed across multiple supervisor nodes in the Storm cluster.
    Each of the nodes in the cluster can run one or more JVMs called **worker processes**,
    which are responsible for processing a part of the topology.
  prefs: []
  type: TYPE_NORMAL
- en: A worker process is specific to one of the specific topologies and can execute
    multiple components of that topology. If multiple topologies are being run at
    the same time, none of them will share any of the workers, thus providing some
    degree of isolation between topologies.
  prefs: []
  type: TYPE_NORMAL
- en: Executor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Within each worker process, there can be multiple threads executing parts of
    the topology. Each of these threads is called an **executor**. An executor can
    execute only one of the components, that is, any spout or bolt in the topology.
  prefs: []
  type: TYPE_NORMAL
- en: Each executor, being a single thread, can execute only tasks assigned to it
    serially. The number of executors defined for a spout or bolt can be changed dynamically
    while the topology is running, which means that you can easily control the degree
    of parallelism of various components in your topology.
  prefs: []
  type: TYPE_NORMAL
- en: Task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the most granular unit of task execution in Storm. Each task is an instance
    of a spout or bolt. When defining a Storm topology, you can specify the number
    of tasks for each spout and bolt. Once defined, the number of tasks cannot be
    changed for a component at runtime. Each task can be executed alone or with another
    task of the same type, or another instance of the same spout or bolt.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram depicts the relationship between a worker process, executors,
    and tasks. In the following diagram, there are two executors for each component,
    with each hosting a different number of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, as you can see, there are two executors and eight tasks defined for one
    component (each executor is hosting four tasks). If you are not getting enough
    performance out of this configuration, you can easily change the number of executors
    for the component to four or eight to increase performance and the tasks will
    be uniformly distributed between all executors of that component. The following
    diagrams show the relationship between executor, task, and worker:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Configure parallelism at the code level
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Storm provides an API to set the number of worker processes, number of executors,
    and number of tasks at the code level. The following section shows how we can
    configure parallelism at the code level.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can set the number of worker processes at the code level by using the `setNumWorkers`
    method of the `org.apache.storm.Config` class. Here is the code snippet to show
    these settings in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the previous chapter, we configured the number of workers as three. Storm
    will assign the three workers for the `SampleStormTopology` and `SampleStormClusterTopology`
    topology.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can set the number of executors at the code level by passing the `parallelism_hint`
    argument in the `setSpout(args,args,parallelism_hint)` or `setBolt(args,args,parallelism_hint)`
    methods of the `org.apache.storm.topology.TopologyBuilder` class. Here is the
    code snippet to show these settings in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the previous chapter, we set `parallelism_hint=2` for `SampleSpout` and `parallelism_hint=4`
    for `SampleBolt`. At the time of execution, Storm will assign two executors for
    `SampleSpout` and four executors for `SampleBolt`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can configure the number of tasks that can execute inside the executors.
    Here is the code snippet to show these settings in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have configured the two executors and four tasks of
    `SampleSpout`. For `SampleSpout`, Storm will assign two tasks per executor. By
    default, Storm will run one task per executor if the user does not set the number
    of tasks at the code level.
  prefs: []
  type: TYPE_NORMAL
- en: Worker process, executor, and task distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assume the numbers of worker processes set for the topology is three,
    the number of executors for `SampleSpout` is three, and the number of executors
    for `SampleBolt` is three. Also, the number of tasks for `SampleBolt`Â is to be
    six, meaning that each `SampleBolt` executor will have two tasks. The following
    diagram shows what the topology would look like in operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Rebalance the parallelism of a topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained in the previous chapter, one of the key features of Storm is that
    it allows us to modify the parallelism of a topology at runtime. The process of
    updating a topology parallelism at runtime is called **rebalance**.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to rebalance the topology:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Storm Web UI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Storm CLI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Storm Web UI was covered in the previous chapter. This section covers how
    we can rebalance the topology using the Storm CLI tool. Here are the commands
    that we need to execute on Storm CLI to rebalance the topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `rebalance` command will first deactivate the topology for the duration
    of the message timeout and then redistribute the workers evenly around the Storm
    cluster. After a few seconds or minutes, the topology will revert to the previous
    state of activation and restart the processing of input streams.
  prefs: []
  type: TYPE_NORMAL
- en: Rebalance the parallelism of a SampleStormClusterTopology topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s first check the numbers of worker processes that are running in the
    Storm cluster by running the `jps` command on the supervisor machine:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `jps` command on supervisor-1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Two worker processes are assigned to the supervisor-1 machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, run the `jps` command on supervisor-2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: One worker process is assigned to the supervisor-2 machine.
  prefs: []
  type: TYPE_NORMAL
- en: A total of three worker processes are running on the Storm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try reconfiguring `SampleStormClusterTopology` to use two worker processes,
    `SampleSpout` to use four executors, and `SampleBolt` to use four executors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Rerun the `jps` commands on the supervisor machines to view the number of worker
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `jps` command on supervisor-1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `jps` command on supervisor-2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this case, two worker processes are shown previously. The first worker process
    is assigned to supervisor-1 and the other one is assigned to supervisor-2\. The
    distribution of workers may vary depending on the number of topologies running
    on the system and the number of slots available on each supervisor. Ideally, Storm
    tries to distribute the load uniformly between all the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Different types of stream grouping in the Storm cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When defining a topology, we create a graph of computation with the number of
    bolt-processing streams. At a more granular level, each bolt executes multiple
    tasks in the topology. Thus, each task of a particular bolt will only get a subset
    of the tuples from the subscribed streams.
  prefs: []
  type: TYPE_NORMAL
- en: Stream grouping in Storm provides complete control over how this partitioning
    of tuples happens among the many tasks of a bolt subscribed to a stream. Grouping
    for a bolt can be defined on the instance of `org.apache.storm.topology.InputDeclarer`
    returned when defining bolts using the `org.apache e.storm.topology.TopologyBuilder.setBolt`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Storm supports the following types of stream groupings.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle grouping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shuffle grouping distributes tuples in a uniform, random way across the tasks.
    An equal number of tuples will be processed by each task. This grouping is ideal
    when you want to distribute your processing load uniformly across the tasks and
    where there is no requirement for any data-driven partitioning. This is one of
    the most commonly used groupings in Storm.
  prefs: []
  type: TYPE_NORMAL
- en: Field grouping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This grouping enables you to partitionÂ a stream on the basis of some of the
    fields in the tuples. For example, if you want all the tweets from a particular
    user to go to a single task, then you can partition the tweet stream using field
    grouping by username in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As a result of the field grouping being *hash (fields) % (no. of tasks)*, it
    does not guarantee that each of the tasks will get tuples to process. For example,
    if you have applied a field grouping on a field, say *X*, with only two possible
    values, *A* and *B*, and created two tasks for the bolt, then it might be possible
    that both *hash (A) % 2* and *hash (B) % 2* return equal values, which will result
    in all the tuples being routed to a single task and the other being completely
    idle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another common usage of field grouping is to join streams. Since partitioning
    happens solely on the basis of field values, and not the stream type, we can join
    two streams with any common join fields. The name of the fields needs not be the
    same. For example, in the order processing domain, we can join the `Order` stream
    and the `ItemScanned` stream to see when an order is completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Since joins on streams vary from application to application, you'll make your
    own definition of a join, say joins over a time window, that can be achieved by
    composing field groupings.
  prefs: []
  type: TYPE_NORMAL
- en: All grouping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All grouping is a special grouping that does not partition the tuples but replicates
    them to all the tasks, that is, each tuple will be sent to each of the bolt's
    tasks for processing.
  prefs: []
  type: TYPE_NORMAL
- en: One common use case of all grouping is for sending signals to bolts. For example,
    if you are doing some kind of filtering on the streams, you can pass or change
    the filter parameters to all the bolts by sending them those parameters over a
    stream that is subscribed by all the bolt's tasks with an all grouping. Another
    example is to send a reset message to all the tasks in an aggregation bolt.
  prefs: []
  type: TYPE_NORMAL
- en: Global grouping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Global grouping does not partition the stream but sends the complete stream
    to the bolt's task, the smallest ID. A general use case of this is when there
    needs to be a reduce phase in your topology where you want to combine the results
    from previous steps in the topology into a single bolt.
  prefs: []
  type: TYPE_NORMAL
- en: Global grouping might seem redundant at first, as you can achieve the same results
    by defining the parallelism for the bolt as one if you only have one input stream.
    However, when you have multiple streams of data coming through a different path,
    you might want only one of the streams to be reduced and others to be parallel
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the following topology. In this, you might want to combine
    all the tuples coming from **Bolt C** in a single **Bolt D** task, while you might
    still want parallelism for tuples coming from **Bolt E** to **Bolt D**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Direct grouping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In direct grouping, the emitter decides where each tuple will go for processing.
    For example, say we have a log stream and we want to process each log entry to
    be processed by a specific bolt task on the basis of the type of resource. In
    this case, we can use direct grouping.
  prefs: []
  type: TYPE_NORMAL
- en: Direct grouping can only be used with direct streams. To declare a stream as
    a direct stream, use the `backtype.storm.topology.OutputFieldsDeclarer.declareStream`
    method, which takes a `boolean` parameter. Once you have a direct stream to emit
    to, use `backtype.storm.task.OutputCollector.emitDirect` instead of emit methods
    to emit it. The `emitDirect` method takes a `taskId` parameter to specify the
    task. You can get the number of tasks for a component using the `backtype.storm.task.TopologyContext.getComponentTasks`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Local or shuffle grouping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the tuple source and target bolt tasks are running in the same worker, using
    this grouping will act as a shuffle grouping only between the target tasks running
    on the same worker, thus minimizing any network hops, resulting in increased performance.
  prefs: []
  type: TYPE_NORMAL
- en: If there are no target bolt tasks running on the source worker process, this
    grouping will act similar to the shuffle grouping mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: None grouping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: None grouping is used when you don't care about the way tuples are partitioned
    among various tasks. As of Storm 0.8, this is equivalent to using shuffle grouping.
  prefs: []
  type: TYPE_NORMAL
- en: Custom grouping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If none of the preceding groupings fit your use case, you can define your own
    custom grouping by implementing the `backtype.storm.grouping.CustomStreamGrouping`
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample custom grouping that partitions the stream on the basis of
    the category in the tuples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram represents the Storm groupings graphically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Guaranteed message processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a Storm topology, a single tuple being emitted by a spout can result in
    a number of tuples being generated in the later stages of the topology. For example,
    consider the following topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, **Spout A** emits a tuple **T(A)**, which is processed by **bolt B** and
    **bolt C**, which emit tuple **T(AB)** and **T(AC)** respectively. So, when all
    the tuples produced as a result of tuple **T(A)**--namely, the tuple tree **T(A)**,
    **T(AB)**, and **T(AC)**--are processed, we say that the tuple has been processed
    completely.
  prefs: []
  type: TYPE_NORMAL
- en: When some of the tuples in a tuple tree fail to process either due to some runtime
    error or a timeout that is configurable for each topology, then Storm considers
    that to be a failed tuple.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the six steps that are required by Storm to guarantee message processing:'
  prefs: []
  type: TYPE_NORMAL
- en: Tag each tuple emitted by a spout with a unique message ID. This can be done
    by using the `org.apache.storm.spout.SpoutOutputColletor.emit` method, which takes
    a `messageId` argument. Storm uses this message ID to track the state of the tuple
    tree generated by this tuple. If you use one of the emit methods that doesn't
    take a `messageId` argument, Storm will not track it for complete processing.
    When the message is processed completely, Storm will send an acknowledgement with
    the same `messageId` that was used while emitting the tuple.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A generic pattern implemented by spouts is that they read a message from a messaging
    queue, say RabbitMQ, produce the tuple into the topology for further processing,
    and then dequeue the message once it receives the acknowledgement that the tuple
    has been processed completely.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When one of the bolts in the topology needs to produce a new tuple in the course
    of processing a message, for example, **bolt B** in the preceding topology, then
    it should emit the new tuple anchored with the original tuple that it got from
    the spout. This can be done by using the overloaded emit methods in the `org.apache.storm.task.OutputCollector`
    class that takes an anchor tuple as an argument. If you are emitting multiple
    tuples from the same input tuple, then anchor each outgoing tuple.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Whenever you are done with processing a tuple in the execute method of your
    bolt, send an acknowledgment using the `org.apache.storm.task.OutputCollector.ack`
    method. When the acknowledgement reaches the emitting spout, you can safely mark
    the message as being processed and dequeue it from the message queue, if any.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, if there is some problem in processing a tuple, a failure signal
    should be sent back using the `org.apache.storm.task.OutputCollector.fail` method
    so that Storm can replay the failed message.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the general patterns of processing in Storm bolts is to process a tuple
    in, emit new tuples, and send an acknowledgement at the end of the execute method.
    Storm provides the `org.apache.storm.topology.base.BasicBasicBolt` class that
    automatically sends the acknowledgement at the end of the execute method. If you
    want to signal a failure, throw `org.apache.storm.topology.FailedException` from
    the execute method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This model results in at-least-once message processing semantics, and your application
    should be ready to handle a scenario when some of the messages will be processed
    multiple times. Storm also provides exactly-once message processing semantics,
    which we will discuss in [Chapter 5](part0102.html#318PC0-6149dd15e07b443593cc93f2eb31ee7b),
    *Trident Topology and Uses*.
  prefs: []
  type: TYPE_NORMAL
- en: Even though you can achieve some guaranteed message processing in Storm using
    the methods mentioned here, it is always a point to ponder whether you actually
    require it or not, as you can gain a large performance boost by risking some of
    the messages not being completely processed by Storm. This is a trade-off that
    you can think of when designing your application.
  prefs: []
  type: TYPE_NORMAL
- en: Tick tuple
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some use cases, a bolt needs to cache the data for a few seconds before performing
    some operation, such as cleaning the cache after every 5 seconds or inserting
    a batch of records into a database in a single request.
  prefs: []
  type: TYPE_NORMAL
- en: The tick tuple is the system-generated (Storm-generated) tuple that we can configure
    at each bolt level. The developer can configure the tick tuple at the code level
    while writing a bolt.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to overwrite the following method in the bolt to enable the tick tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have configured the tick tuple time to 10 seconds.
    Now, Storm will start generating a tick tuple after every 10 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we need to add the following code in the execute method of the bolt to
    identify the type of tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If the output of the `isTickTuple()` method is true, then the input tuple is
    a tick tuple. Otherwise, it is a normal tuple emitted by the previous bolt.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that tick tuples are sent to bolts/spouts just like regular tuples,
    which means they will be queued behind other tuples that a bolt/spout is about
    to process via its `execute()` or `nextTuple()` method, respectively. As such,
    the time interval you configure for tick tuples is, in practice, served on a best-effort
    basis. For instance, if a bolt is suffering from high execution latency--for example,
    due to being overwhelmed by the incoming rate of regular, non-tick tuples--then
    you will observe that the periodic activities implemented in the bolt will get
    triggered later than expected.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have shed some light on how we can define the parallelism
    of Storm, how we can distribute jobs between multiple nodes, and how we can distribute
    data between multiple instances of a bolt. The chapter also covered two important
    features: guaranteed message processing and the tick tuple.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are covering the Trident high-level abstraction over
    Storm. Trident is mostly used to solve the real-time transaction problem, which
    can't be solved through plain Storm.
  prefs: []
  type: TYPE_NORMAL
