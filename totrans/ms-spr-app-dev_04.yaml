- en: Chapter 4. Jobs with Spring Batch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enterprise applications often have requirements for processing bulk information
    by applying complex business rules. Some applications require automated jobs to
    run and provide large chunks of data as input for further processing. Such functions
    are always time-based jobs, which don't require any user intervention. Batch processing
    is widely used in banking and insurance domains where large sets of data are processed
    at scheduled times. A **job** is a process while a **batch job** implies a set
    of processes that run to perform a task at a scheduled time.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Spring Batch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spring Batch is itself a batch framework that is used to develop applications
    to do batch jobs. It supports batch optimization and job partitioning and is highly
    scalable, which provokes us to consider it in the development of batch applications.
  prefs: []
  type: TYPE_NORMAL
- en: Use cases for using Spring Batch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us list a few use cases where we can use Spring batch in the application:'
  prefs: []
  type: TYPE_NORMAL
- en: To send bulk mails to the user at a scheduled time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To read messages from the queue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To update transactions at a given time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To process all the received files from the user at a given time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goals of batch processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The batch processing key goal is to fulfill the following set of steps in order
    to complete the batch job:'
  prefs: []
  type: TYPE_NORMAL
- en: Locating a job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identifying the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scheduling the job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Starting the job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Processing the job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to step 2 (for fresh input).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Architecture of a batch job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s depict the basic architecture of a batch processor; we can also see
    the components involved in the batch processing. From the following diagram you
    can figure out the main components of Spring Batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture of a batch job](img/7320OS_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's now have a look at the components individually.
  prefs: []
  type: TYPE_NORMAL
- en: '`JobRepository`: This container is where we need to register our jobs or processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`JobOperator`: This is the one that triggers the registered job. It also provides
    APIs for accessing the register. It is an interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Job`: It is a process or task in the `jobRepository`. This consists of one
    more step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Step`: This actually contains the logic that needs to be executed. Each step
    consists of an `ItemReader`, `ItemProcessor`, and `ItemWriter` interface. First,
    the `ItemReader` interface reads one step at a time in a job and gives it to `ItemProcessor`
    which processes the job. For example, it might collect some data required. Then,
    the `ItemWriter` interface writes the data to the database or it might execute
    a transaction or log a message. There are two kinds of steps:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChunkStyle`: A `ChunkStyle` step has exactly one `ItemReader`, one `ItemProcessor`,
    and one `ItemWriter`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BatchLet`: In Spring, `BatchLet` is called `TaskLetStep`. `BatchLet` is a
    custom-made step that can be used for sending bulk mails or text messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know the basics of a batch, in the next section we shall see how
    to implement or use a batch.
  prefs: []
  type: TYPE_NORMAL
- en: Using an enterprise batch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have the following two options for implementing a batch:'
  prefs: []
  type: TYPE_NORMAL
- en: Using JVM and starting JVM for each job run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a batch job management application in a J2EE container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JSR-352 is the standard specification available for implementing batch processing.
    The Spring framework supports this specification to a great extent. Most JEE containers,
    such as **Glassfish**, **Jboss- JMX**, and Web Sphere are bound to support the
    JSR-352 specifications. As a developer, we can choose the Spring framework and
    deploy the batch on a J2EE container.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use restful APIs to pool the data into and out of the batch application.
    In the next section, let's use the Spring Batch framework to create a job. We
    shall first look at the dependency.
  prefs: []
  type: TYPE_NORMAL
- en: Dependency for Spring Batch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get started with Spring Batch, we need to look at the dependencies. Assuming
    that the user is familiar with the Maven application, we can look at the following
    dependencies that need to be added to the `pom.xml` file to use Spring Batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Key components of Spring Batch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can see that the key components of Spring Batch are very similar to the
    JSR specification for batch processing in Java.
  prefs: []
  type: TYPE_NORMAL
- en: '`JobRepository`: This is again a repository for jobs. But, in the Spring Batch
    framework, the core API has `JobRepository`. It provides the `create`, `update`,
    `read`, and `delete` methods for `JobLauncher`, `JobReader`, `ItemProcessor`,
    and `ItemWriter`. The class responsible for `JobRepository` in the Spring framework
    is `SimpleJobRepository`. There are two ways of storing the jobs: one in the database
    and another in memory (which will have to make use of `HashMaps`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SimpleJobRepositoryConstructor` looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`JobLauncher`: `JobLauncher` is just a simple interface used for launching
    jobs. Jobs are registered at the `jobRepository`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A `SimpleJobLauncher` class implements the `JobLauncher` interface. This class
    has a `setJobRepository` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`ItemReader`: It is an interface in `org.springframework.batch.item package`.
    ItemReader is used for providing data. The data can be from a database, XML, or
    from a flat file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation classes are expected to be stateful and will be called multiple
    times for each batch, with each call to `read()` returning a different value and
    finally returning null when all input data is exhausted. Implementation classes
    need not be thread-safe, and clients of an `ItemReader` interface need to be aware
    that this is the case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`ItemProcessor`: This is an interface that can be used for processing data
    and does the intermediate processing of data. Before it is given to `ItemWriter`,
    `ItemProcessor` can be used for implementing certain business logic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Assume that an `ItemReader` interface provides a class of type `ProductBean`
    and this class needs to be converted to type `RelatedProductsBea`n before being
    written out. An `ItemProcessor` can be written to perform the conversion. In this
    very simple example, there is a class `ProductBean`, a class `RelatedProductsBean`,
    and a class `ProductBeanProcessor` that adhere to the `ItemProcessor` interface.
    The transformation is simple, but any type of transformation could be done here.
    The `RelatedProductsBean` writer will be used to write out `RelatedProductsBean`
    objects, throwing an exception if any other type is provided. Similarly, `ProductBeanProcessor`
    will throw an exception if anything but `ProductBean` is provided.
  prefs: []
  type: TYPE_NORMAL
- en: '`ProductBeanProcessor` can then be injected into a step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`Item Writer`: This is an interface and here are its frequently used implementation
    classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `write` method defines the most essential contract of the ItemWriter interface.
    It will attempt to write out the list of items passed in as long as it is open.
    As it is expected that items will be batched together into a chunk and then the
    output given, the interface accepts a list of items rather than an item by itself.
    Once the items are written out, any flushing that may be necessary can be performed
    before returning from the `write` method. For example, if writing to a Hibernate
    DAO, multiple calls to `write` can be made, one for each item.
  prefs: []
  type: TYPE_NORMAL
- en: The writer can then call close on the hibernate session before returning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a frequently used implementation of `ItemWriter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FlatFileItemWriter`: This writes data to a file or stream. It uses a buffered
    writer to improve performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Developing a sample batch application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have covered the basics of batch processing and the components of
    Spring Batch, let's develop a simple example in which names starting with `$$`
    are recognized as nonvegetarian food items and names starting with `##` are vegetarian.
    Names that do not start with either of these characters need to be ignored. Our
    job must generate an HTML string with the font color red for nonvegetarian recipes
    and a green font color for vegetarian recipes.
  prefs: []
  type: TYPE_NORMAL
- en: You need to create a Maven project named `recipeMarker` with the previously
    mentioned dependency. Also add all the Spring Framework core dependencies. We
    shall work on the `context.xml` file. We need to configure the job repository
    and job launcher.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the `applicationContext.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You can see that we have used `MapJobRepositoryFactoryBean` to create a job
    repository. It's a `FactoryBean` that automates the creation of `SimpleJobRepository`
    using nonpersistent, in-memory **data access object** (**DAO**) implementations.
    This repository is only really intended for use in testing and rapid prototyping.
    In such settings, you might find that `ResourcelessTransactionManager` is useful
    (as long as your business logic does not use a relational database). It is not
    suited for use in multi-threaded jobs with splits, although it should be safe
    to use in a multi-threaded step.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we shall create implementation classes using the `ItemReader` and `ItemWriter`
    interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: The following is the `ItemReader` implementation class. It reads the data in
    the overridden `read()` method which returns an object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here we have `ItemProcessor`. It applies the logic of marking the recipe list
    with red and green colors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, let's write the implementation class for reading the modified data from
    the `ItemProcessor` and write it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the next step, we shall combine `ItemReader`, `ItemProcessor`, and `ItemWriter`
    into a job.
  prefs: []
  type: TYPE_NORMAL
- en: Let us create an `itemreaderprocessorwriter.xml` file. We shall pass the list
    of recipes in the XML file. We have included the `applicationContext.xml` file.
    A commit interval has been defined to say that the writer should commit after
    it writes two elements at a time. You can also observe that the step consists
    of `reader`, `writer`, and `jobRepository`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to launch the job using a command-line interface provided by
    the Spring Batch framework.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Let us create a file named `itemreaderprocessorwriter.xml`. We shall pass the
    list of recipes in the XML file. We have included the `applicationContext.xml`
    file. A commit interval has been defined to say that the writer should commit
    after it writes two elements at a time. You can also observe that the step consists
    of `reader`, `writer`, and `jobRepository`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Creating a sample batch application using the Tasklet interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s create another batch application that runs on the command line. This
    batch application prints the message. We have already spoken about Tasklet in
    the beginning of the chapter. A job compromises of steps and a step can be one
    of two types: chunk style step and Tasklet.'
  prefs: []
  type: TYPE_NORMAL
- en: We are using the `Tasklet` interface in this example. In Spring Batch, `Tasklet`
    is an interface called to perform a single task, like to clean or set up resources
    before or after a step execution. This interface comes with a method called `executeStatus`,
    which should be overridden by the class that implements it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the following example, `TaskLetImpl` implements the `Tasklet` interface.
    We have also used the `TaskLetStep` class for configuring the `JobRepository`
    in the configuration file. The public class `TaskletStep` extends `AbstractStep`.
  prefs: []
  type: TYPE_NORMAL
- en: '`TaskletStep` is a simple implementation of executing the step as a call to
    `Tasklet`, possibly repeated, and each call is surrounded by a transaction. The
    structure is therefore that of a loop with a transaction boundary inside the loop.
    The loop is controlled by the step operations (`setStepOperations(RepeatOperations)`).'
  prefs: []
  type: TYPE_NORMAL
- en: Clients can use interceptors in the step operations to intercept or listen to
    the iteration on a step-wide basis—for instance, to get a callback when the step
    is complete. Those that want callbacks at the level of an individual task can
    specify interceptors for the chunk operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the flow through the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a sample batch application using the Tasklet interface](img/7320OS_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's create a simple Java batch application project named `Chapter4-SpringBatchCommandLine`
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Maven folder structure for `Chapter4-SpringBatchCommandLine`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`src`/`main`/`java`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src`/`main`/`resources`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src`/`pom.xml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a package called `com.packt.example`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a class called `TaskletImpl`. This class implements the `Tasklet` interface
    and overrides the `execute()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Configure the `simpleJob.xml` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Place this file in the `resources` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can see that we have created three instances of the `TaskletImpl` class:
    `object1`, `object2`, and `object3`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In each instance, we are setting the message property. We are passing the object
    instance to `TaskletStep`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Configure `jobLauncher` and `JobRepository`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You can run the project with MVN Compile, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Using Spring Batch to read a CSV file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us create another batch application that reads a CSV file from a directory
    and uses `commandlinerunner` to run the job. The output is again a CSV file that
    will be available in the `output` folder.
  prefs: []
  type: TYPE_NORMAL
- en: This example is about showing the various options of the `ItemWriter` and `ItemReader`
    implementations available in the Spring Batch framework. We have used `flatFileItemReader`
    and `flatFileItemWriter` implementations available in the Spring Framework here.
  prefs: []
  type: TYPE_NORMAL
- en: We shall begin with application development and check out how these `ItemReader`
    implementation classes are used.
  prefs: []
  type: TYPE_NORMAL
- en: Create a Spring Java application using Maven and name it `SpringBatchCommandLine-Chapter4Example2`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a domain class `Employee` with two instance variables, `empId` and `name`,
    with getters and setters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Use the `ItemWriter` interface and implement a `CustomeItemWriter` class. This
    class overrides the `write` method, which is defined in the `ItemWriter` interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will observe that the `write` method accepts `List` as input. In the `write`
    method, we are just parsing the list and typecasting the list index value to the
    `Employee` object and printing it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `Main` class with `public static void main()` with a `jobrun()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us set the `bean id` to `JobRepository` in the `context.xml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `Job Read files.xml` file is present in the resource folder `*/Job Read
    files.xml`.
  prefs: []
  type: TYPE_NORMAL
- en: We have used `flatfileItemReader` and `FlatFileItemWriter`. These classes read
    the input and recreate the files in the `output` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a prototype of `FlatFileItemReader` and learn what it does in
    the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Restartable `ItemReader` reads lines from input `setResource(Resource)`. A line
    is defined by `setRecordSeparatorPolicy(RecordSeparatorPolicy)` and mapped to
    an item using `setLineMapper(LineMapper)`.
  prefs: []
  type: TYPE_NORMAL
- en: If an exception is thrown during line mapping, it is rethrown as `FlatFileParseException`,
    adding information about the problematic line and its line number.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This class is an item writer that writes data to a file or stream. The writer
    also provides a restart. The location of the output file is defined by a resource
    and must represent a writable file and use buffered writer to improve performance.
    The implementation is not thread-safe.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the file we have done the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We have configured the job with the name `readMultiFileJob`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must observe that `tasklet` has a step which is configured with the `ItemReader`
    and `ItemWriter` classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have again used `tasklet`, but we have used step as a chunk reader that accepts
    `MultiResourceReader`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To understand `MultiResourceReader`, we shall look at the prototype:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`MultiResourceReader` reads items from multiple resources sequentially. The
    resource list is given by `setResources(Resource[])`, and the actual reading is
    delegated to `setDelegate(ResourceAwareItemReaderItemStream)`. Input resources
    are ordered using `setComparator(Comparator)` to make sure that resource ordering
    is preserved between job runs in a restart scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see what step of the type chunk is about. In a chunk, a reader and
    a writer are mandatory! However, the `ItemProcessor` is optional.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a few CSV files with the name `employee*.csv`, replacing the `*` with
    a different number for each file. Each file will have two values: `employeeId`
    and `name`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The delimiter in the CSV file can also be configured in the XML, as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The values will be mapped with a **plain old java object** (**Pojo**), `Employee.java`,
    and the output is processed. The file location is passed as input to the `MultiResourceItemReader`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we shall see how we can schedule a batch job in Spring.
  prefs: []
  type: TYPE_NORMAL
- en: Spring Batch with a Spring scheduler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, let's see how we can schedule a batch in Spring Batch framework.
    We shall see how we can configure the scheduler. This is a sample `jobproduct.xml`
    file that needs to be available in the classpath. If you are working on a Maven
    project, place it in the resource folder. You need to inject `joblauncher` with
    the intervals and method name `run()` to run the job at the scheduled time.
  prefs: []
  type: TYPE_NORMAL
- en: To use the scheduler, we need to configure the `job-product.xml` file. This
    file is also used in the next section to configure the external scheduler with
    the scheduler details.
  prefs: []
  type: TYPE_NORMAL
- en: 'To schedule running the task every 600 second interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Let's use the `@Component` and `@Autowired` annotations with `MyJobScheduler.class`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Configuring Spring Batch with Quartz scheduler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Spring Batch framework provides an option to configure an external scheduler
    into the application.
  prefs: []
  type: TYPE_NORMAL
- en: Let us integrate Quartz Scheduler with the Spring Batch application. Quartz
    is an open source Java-based scheduler. We shall make this application to read
    a file, but we shall integrate the Quartz scheduler to do the scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: Create a simple Maven application with the name `SpringBatchQuartzExample`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the same `pom.xml` file as in the previous application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the Quartz JAR file to the dependencies in the `pom.xml` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add these properties:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, add these dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a file called `quartz-job.xml`. This should be present in the
    resource folder of the Maven project. To configure the batch to run every minute,
    use the following configuration in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To integrate Spring Batch with Quartz Scheduler, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '`JobQuartzLauncherDetails` is a bean that extends `QuartzJobBean`.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`QuartzJobBean` is available in the package and can be found at `org.springframework.scheduling.quartz.QuartzJobBean`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The class has setters for `JobLauncher` and `JobLocator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: To read the `JobMapDetails` from the configuration, we have created another
    method as shown in the following code. We can see that different data types are
    handled here based on the values read from the map, and `JobParametersBuilder`
    is created.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'As we know, `JobName` and `JobParamters` are required input for `JobLauncher`
    to run the job. In the preceding code snippet, we have got `JobParameters`. Next,
    we shall get `JobName` with the following code snippet using `JobExecutionContext`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '`Product.java` is a domain class that gets mapped to the values in the `.csv`
    file.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The following is the code for `CustomeItemWriter`, which writes the product
    Pojo object values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's create the `Main` class to load the `job-quartz.xml` file and run
    the batch job every 60 seconds to read the CSV file and write using `CustomItemWriter`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The Spring Batch framework uses Quartz Scheduler to run the batch job of reading
    a file, mapping the CSV values to product Pojo, and writing it using `CustomeItemWriter`.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let's create a batch that reads a file and updates a database.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spring Batch to read a file and update a MongoDB database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, let's create a batch job that reads an XML file and writes
    to a MongoDB database. Think of a scenario where we keep getting an XML file from
    a source and that this file needs to be read and updated to a database.
  prefs: []
  type: TYPE_NORMAL
- en: 'The XML file structure is as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Create a Maven Java-based project. In the `com.packt.model` package, add the
    corresponding product Pojo.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Add the same dependencies as the ones shown in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the `pom.xml` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the ORM and MongoDB database dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a file named `mongodatabase.xml` and add the following configurations
    to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Add the following configuration to the `job-product.xml` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`StaxEventItemReader`: This is a class that reads the `products.xml` file.
    We need to provide the `rootElemenent` name to this class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fragmentRootElementName`: This property accepts the string parameter which
    is the root element in the provided XML file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also need to provide the XML file name as a value to the resource property.
    The third property that needs to be passed is the `unmarshaller` reference. This
    class is available in the Spring OXM framework used for marshalling and unmarshalling
    the XML file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The `XstreamMarshaller` accepts three properties to perform the unmarshalling
    process. It accepts a map with entry key and product Pojo as values, so that in
    the XML each product record is converted as a `Product` object and is stored in
    the map. The second property is again a bean created to convert the XML to POJO.
    This is named `ProductXMLConverter`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the `ProductXMLConverter` class. This class implements the `converter`
    interface which is available in the `com.thoughtworks.xstream.converters.converter`
    package. The class overrides three methods defined in the interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '`public boolean canConvert(Class type)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`public void marshal(Object source, HierarchicalStreamWriter writer, MarshallingContext
    context)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`public Object unmarshal(HierarchicalStreamReader reader, UnmarshallingContext
    context)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we shall be doing unmarshalling here, we shall clearly implement the `unmarshall`
    method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure `MongoDBItemWriter` to write to the Pojo object in the MongoDB database
    in `job-product.xml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the batch job in the `job-product.xml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Write the `Main` class to run the batch job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load all the configuration files in the `Main` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: So, when we run the `Main` class, the job gets instantiated and will run every
    60 seconds. The job will read the XML and convert it into the Pojo `product.java`
    and will then insert it into the MongoDB database. The configurations are given
    in the MongoDB database XML file.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we shall see how we can create a multithreaded environment
    to process multiple jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spring Batch with threads to partition jobs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a Spring batch process, a single thread processes the requests sequentially.
    If we want to execute the batch job in parallel, we go in for a multithreaded
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Think of scenario where we are processing 1000 records in an employee table
    that is mapped with the `Employee` Pojo. We need to read 1000 records at a time
    and write to a CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: A job is actually portioned into multiple subjobs, and a separate thread is
    assigned to process each subjob. So, if you have 1000 records to be read, this
    will take more time when done using a single thread. When we partition 1000 records
    into 100 subrecords, we can process them using 10 different threads running at
    the same time.
  prefs: []
  type: TYPE_NORMAL
- en: We can create a simple partitioner class by implementing the `Partitioner` interface.
    This partitioner will partition 1000 jobs into 100 subjobs. You will observe that
    we provided `start_range` and `end_range` variables in the partition range.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The `ExecutionContext` object used in the `Partitioner` class works with `ItemStream`
    and acts like a wrapper around the map. We can get two kinds of execution context
    objects in Spring Batch. One execution object works at the job level and another
    works at the step level. The job level execution context is used to share data
    or information among steps.
  prefs: []
  type: TYPE_NORMAL
- en: Let's implement an `ItemProcess` class that processes the partitioned records.
    Also observe that we are using the step execution context in the following code.
    The class overrides the `process` method.
  prefs: []
  type: TYPE_NORMAL
- en: This class is used for chunk processing the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Let's configure the `job-partioner.xml` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's configure `pagingItemReader`, which acts in the same way as pagination.
    It fetches 100 records per page; this also connects with the data source using
    the JDBC information provided and executes a query to fetch a range of records,
    as specified. It will also sort the data based on the `emp_id` column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Let's write the `Main` class, which will load the configuration files, and then
    run the job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: So, with the preceding configuration and classes, multiple threads get created
    to process 100 records per thread. The records are read from the database and
    are written to the CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we shall use event listeners with Spring Batch.
  prefs: []
  type: TYPE_NORMAL
- en: Intercepting a Spring Batch job with listeners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spring Batch comes with listeners. They intercept the job execution to perform
    certain tasks. `StepListener` is a `super` class for the following mentioned listeners:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SkipListener`: One of the most common use cases for `SkipListener` is to log
    out a skipped item so that another batch process, or even human process, can be
    used to evaluate and fix the issue leading to the skip. Because there are many
    cases in which the original transaction may be rolled back, Spring Batch makes
    two guarantees:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The appropriate `skip` method (depending on when the error happened) will only
    be called once per item.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `SkipListener` will always be called just before the transaction is committed.
    This is to ensure that any transactional resources called by the listener are
    not rolled back by a failure within `ItemWriter`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChunkListener`: These listeners can be configured with a step, and if the
    step is of the type chunk-styled step, this will have both `ItemReader` and `ItemWriter`.
    The listener will intimate `ItemWriter` when `ItemReader` has completed its reading
    task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '`ItemWriterListener`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ItemReaderListener`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ItemProcessListener`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StepExecutionListener`: It represents the most generic listener for step execution.
    It allows for notification before a step is started and after it has ended, whether
    it ended normally or failed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will observe that there is listener configured for each of the `ItemReader`,
    `ItemWriter`, `ItemProcess`, and `StepExecution` interfaces and classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can have a look at how to configure listeners in the spring `batch.xml`
    file. Please have a look:'
  prefs: []
  type: TYPE_NORMAL
- en: Create classes that implement the listeners and override their methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the `PacktItemReaderListener` and `PacktItemWriterListner` listeners.
    The `IteamReadListener` interface comes with three methods to be implemented:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`beforeRead()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`afterRead()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`onReadError()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us next look at `PackItemWriterListener`. The `ItemWriter` interface comes
    with three `abstract` methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`beforeWrite`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`afterWrite`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`onWriteError`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: So far, we have seen how to create custom listeners and listener configuration
    in `spring-job` file.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's try to integrate this with a scenario where we are reading multiple
    files in a directory and deleting the files.
  prefs: []
  type: TYPE_NORMAL
- en: We shall again consider product Pojo, with `id` and `name` as the instance variables
    with getters and setters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We need to define the Pojo in the XML as a bean.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Next is the file deleting task class file. After the files are read, they need
    to be deleted from the directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at the `FileDeletingTasklet` class. This class implements the `TaskLet`
    interface. This will delete the files as per the specified directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: The bean properties need to be set in the job configuration file created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The next task would be to read the multiple files available in the directory.
    Since there are multiple resources that need to be read, we shall use a `MultiResourceReader`
    configuration in the bean.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The `flatfileItemReader` maps the CSV values to the product Pojo. So, provide
    the following configuration to the `jobs.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Then, after reading the CSV values and mapping them to Pojo from different CSV
    files, we can add the `writterListener` if we need to merge into a single CSV
    file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the `Main` class, all the beans configured in the XML file get instantiated
    for the batch job to run. The job does a chunk execution with `ItemReader` and
    `Writer`, as shown in the configuration of the `Main` class here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we have learned about listeners and configuring listeners with
    the job.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we shall see how we can do some unit testing on Spring
    Batch applications.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing Spring Batch applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s demonstrate writing a test case for Spring Batch applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a simple `Test` class called `mport static org.junit.Assert.assertEquals`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: We have to create a file called `text-context.xml` to be available in the batch
    and configure the `JobLauncher` to be available in the XML file, and for the test
    package. In the `Test` class, use the `@Test annotation` method and call the `JobLauncher`
    to execute a step. We need to use `assertEquals` to check the status of the batch
    job against the `jobExecution` status.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the chapter, we learned how to create Spring-based batch applications to
    read CSV files. We have also illustrated how Spring Batch can be used to read
    XML files. The most advanced topic was to partition the jobs and run the jobs
    into separate threads. We have also integrated Spring Batch with Quartz Scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: We have demonstrated writing simple test cases using Spring Batch. We also used
    listeners to intercept a job defined to perform certain operations and have demonstrated
    certain configurations.
  prefs: []
  type: TYPE_NORMAL
