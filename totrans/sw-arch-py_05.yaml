- en: Chapter 5. Writing Applications That Scale
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine the checkout counter of a supermarket on a Saturday evening, the usual
    rush-hour time. It is common to see long queues of people waiting to check out
    with their purchases. What could a store manager do to reduce the rush and waiting
    time?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: A typical manager would try a few approaches, including telling those manning
    the checkout counters to pick up their speed, and to try and redistribute people
    to different queues so that each queue roughly has the same waiting time. In other
    words, he would manage the current load with available resources by *optimizing
    the performance* of the existing resources.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: However, if the store has existing counters that are not in operation—and enough
    people at hand to manage them—the manager could enable those counters, and move
    people to these new counters. In other words, he would add resources to the store
    to *scale* the operation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Software systems too scale in a similar way. An existing software application
    can be scaled by adding compute resources to it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: When the system scales by either adding or making better use of resources inside
    a compute node, such as CPU or RAM, it is said to *scale vertically* or *scale
    up*. On the other hand, when a system scales by adding more compute nodes to it,
    such as a creating a load-balanced cluster of servers, it is said to *scale horizontally*
    or *scale out*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: The degree to which a software system is able to scale when compute resources
    are added is called its *scalability*. Scalability is measured in terms of how
    much the system's performance characteristics, such as throughput or latency,
    improve with respect to the addition of resources. For example, if a system doubles
    its capacity by doubling the number of servers, it is scaling linearly.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the concurrency of a system often increases its scalability. In the
    supermarket example given earlier, the manager is able to scale out his operations
    by opening additional counters. In other words, he increases the amount of concurrent
    processing done in his store. Concurrency is the amount of work that gets done
    simultaneously in a system.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we look at the different techniques of scaling a software application
    with Python.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: We will be following the approximate sketch of the topics below in our discussion
    in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Scalability and Performance
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency and Parallelism
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency in Python - Multi-threading
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thumbnail generator
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – producer/consumer architecture
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – program end condition
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – resource constraint using locks
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – resource constraint using semaphores
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Resource constraint – semaphore vs lock
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – url rate controller using conditions
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Multi-threading – Python and GIL
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency in Python – Multi-processing
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A primality checker
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Sorting disk files
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Sorting disk files – using a counter
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Sorting disk files – using multi-processing
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Multi-threading vs Multi-processing
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency in Python – Asynchronous Execution
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-emptive vs Co-operative multitasking
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Asyncio in Python
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Waiting for future – async and await
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent futures – high level concurrent processing
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency Options - how to choose ?
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel Processing libraries
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: joblib
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyMP
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fractals – The Mandelbrot Set
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Fractals – Scaling the Mandelbrot Set implementation
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Scaling for the Web
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling workflows – message queues and task queues
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Celery – a distributed task queue
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Mandelbrot Set - Using Celery
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Serving Python on the Web – WSGI
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: uWSGI – WSGI middleware on steroids
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: gunicorn – unicorn for WSGI
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: gunicorn vs uWSGI
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Scalability Architectures
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical scalability Architectures
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal scalability Architectures
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability and performance
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How do we measure the scalability of a system? Let's take an example, and see
    how this is done.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Let's say our application is a simple report generation system for employees.
    It is able to load employee data from a database, and generate a variety of reports
    in bulk, such as pay slips, tax deduction reports, employee leave reports, and
    so on.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的应用是一个简单的员工报告生成系统。它能够从数据库中加载员工数据，并批量生成各种报告，如工资单、税收扣除报告、员工请假报告等。
- en: The system is able to generate 120 reports per minute—this is the *throughput*
    or *capacity* of the system expressed as the number of successfully completed
    operations in a given unit of time. Let's say the time it takes to generate a
    report at the server side (latency) is roughly 2 seconds.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 系统能够每分钟生成120份报告——这是系统吞吐量或容量的表达，表示在给定时间内成功完成的操作数量。假设在服务器端生成报告所需的时间（延迟）大约为2秒。
- en: Let us say, the architect decides to scale up the system by doubling the RAM
    on its server
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 假设架构师决定通过在服务器上加倍RAM来扩展系统
- en: Once this is done, a test shows that the system is able to increase its throughput
    to 180 reports per minute. The latency remains the same at 2 seconds.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，测试显示系统能够将吞吐量提高到每分钟180份报告。延迟保持在2秒。
- en: 'So at this point, the system has scaled *close to linear* in terms of the memory
    added. The scalability of the system expressed in terms of throughput increase
    is as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，此时系统在增加的内存方面实现了“接近线性”的扩展。系统的可伸缩性以吞吐量增加的方式表达如下：
- en: Scalability (throughput) = *180/120 = 1.5X*
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可伸缩性（吞吐量）= 180/120 = 1.5倍
- en: 'As a second step, the architect decides to double the number of servers on
    the backend—all with the same memory. After this step, he finds that the system''s
    performance throughput has now increased to 350 reports per minute. The scalability
    achieved by this step is given as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第二步，架构师决定在后端将服务器数量加倍，所有服务器的内存相同。此步骤后，他发现系统的性能吞吐量现在增加到每分钟350份报告。此步骤实现的可伸缩性如下：
- en: Scalability (throughput) = *350/180 = 1.9X*
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 可伸缩性（吞吐量）= 350/180 = 1.9倍
- en: The system has now responded much better with a close to linear increase in
    scalability.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 系统现在以接近线性的方式做出了更好的响应，提高了可伸缩性。
- en: After further analysis, the architect finds that by rewriting the code that
    was processing reports on the server to run in multiple processes instead of a
    single process, he is able to reduce the processing time at the server, and hence,
    the latency of each request by roughly 1 second per request at peak time. The
    latency has now gone down from 2 seconds to 1 second.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 经过进一步分析，架构师发现通过重写在服务器上处理报告的代码，使其在多个进程中运行而不是单个进程，他能够在高峰时期将每个请求的处理时间减少约1秒。延迟现在从2秒降至1秒。
- en: The system's performance with respect to latency has become better by
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的性能在延迟方面已经变得更好
- en: 'Performance (latency): *X = 2/1 = 2X*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 性能（延迟）：X = 2/1 = 2倍
- en: How does this improve scalability? Since the time taken to process each request
    is lesser now, the system overall will be able to respond to similar loads at
    a faster rate than what it was able to earlier. With the exact same resources,
    the system's throughput performance, and hence, scalability has increased assuming
    other factors remain the same.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这如何改善可伸缩性？由于现在处理每个请求所需的时间更短，系统整体将能够以比之前更快的速度响应类似的负载。在其他因素保持不变的情况下，系统的吞吐性能和因此可伸缩性已经提高。
- en: 'Let''s summarize what we discussed so far as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下我们迄今讨论的内容：
- en: In the first step, the architect increased the throughput of a single system
    by scaling it up by adding extra memory as a resource, which increased the overall
    scalability of the system. In other words, he scaled the performance of a single
    system by *scaling up*, which boosted the overall performance of the whole system.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一步中，架构师通过增加额外内存作为资源来扩展单个系统的吞吐量，从而增加了系统的整体可伸缩性。换句话说，他通过“纵向扩展”来扩展单个系统的性能，从而提高了整个系统的性能。
- en: In the second step, he added more nodes to the system, and hence, its ability
    to perform work concurrently, and found that the system responded well by rewarding
    him with a near-linear scalability factor. In other words, he increased the throughput
    of the system by scaling its resource capacity. Thus, he increased scalability
    of the system by *scaling out*, that is, by adding more compute nodes.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二步中，他向系统添加了更多节点，因此系统能够同时执行工作，并发现系统以接近线性的可伸缩性因子回报他。换句话说，他通过增加计算节点来扩展系统的资源容量，从而提高了系统的可伸缩性。因此，他通过“横向扩展”来增加了系统的可伸缩性，即通过添加更多计算节点。
- en: In the third step, he made a critical fix by running a computation in more than
    one process. In other words, he increased the *concurrency* of a single system
    by dividing the computation to more than one part. He found that this increased
    the performance characteristic of the application by reducing its *latency*, potentially
    setting up the application to handle workloads better at high stress.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第三步中，他通过在多个进程中运行计算来进行关键修复。换句话说，他通过将计算分成多个部分来增加单个系统的并发性。他发现这提高了应用程序的性能特征，通过减少其延迟，潜在地使应用程序能够更好地处理高压工作负载。
- en: 'We find that there is a relation between Scalability, Performance, Concurrency,
    and Latency. This can be explained as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现可伸缩性、性能、并发性和延迟之间存在关系。这可以解释如下：
- en: When performance of one of the components in a system goes up, generally the
    performance of the overall system goes up.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当系统中某个组件的性能提高时，通常整个系统的性能也会提高。
- en: When an application scales in a single machine by increasing its concurrency,
    it has the potential to improve performance, and hence, the net scalability of
    the system in deployment.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当应用程序通过增加并发性在单台机器上扩展时，它有潜力提高性能，从而提高部署系统的净可伸缩性。
- en: When a system reduces its performance time, or its latency, at the server, it
    positively contributes to scalability.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当系统减少其性能时间或服务器端的延迟时，这对可伸缩性是积极的贡献。
- en: 'We have captured these relationships in the following table:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '| Concurrency | Latency | Performance | Scalability |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: '| High | Low | High | High |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '| High | High | Variable | Variable |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| Low | High | Poor | Poor |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: An ideal system is one that has good concurrency and low latency; such a system
    has high performance, and would respond better to scaling up and/or scaling out.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: A system with high concurrency, but also high latency, would have variable characteristics—its
    performance, and hence, scalability would be potentially very sensitive to other
    factors such as current system load, network congestion, geographical distribution
    of compute resources and requests, and so on.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: A system with low concurrency and high latency is the worst case—it would be
    difficult to scale such a system, as it has poor performance characteristics.
    The latency and concurrency issues should be addressed before the architect decides
    to scale the system horizontally or vertically.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Scalability is always described in terms of variation in performance throughput.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A system's concurrency is the degree to which the system is able to perform
    work simultaneously instead of sequentially. An application written to be concurrent
    in general, can execute more units of work in a given time than one which is written
    to be sequential or serial.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: When one makes a serial application concurrent, one makes the application better
    utilize the existing compute resources in the system—CPU and/or RAM—at a given
    time. Concurrency, in other words, is the cheapest way of making an application
    scale inside a machine in terms of the cost of compute resources.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrency can be achieved using different techniques. The common ones include
    the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '**Multithreading**: The simplest form of concurrency is to rewrite the application
    to perform parallel tasks in different threads. A thread is the simplest sequence
    of programming instructions that can be performed by a CPU. A program can consist
    of any number of threads. By distributing tasks to multiple threads, a program
    can execute more work simultaneously. All threads run inside the same process.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Multiprocessing**: Another way to concurrently scale up a program is to run
    it in multiple processes instead of a single process. Multiprocessing involves
    more overhead than multithreading in terms of message passing and shared memory.
    However, programs that perform a lot of CPU-intensive computations can benefit
    more from multiple processes than multiple threads.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Asynchronous Processing**: In this technique, operations are performed asynchronously
    with no specific ordering of tasks with respect to time. Asynchronous processing
    usually picks tasks from a queue of tasks, and schedules them to execute at a
    future time, often receiving the results in callback functions or special future
    objects. Asynchronous processing usually happens in a single thread.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are other forms of concurrent computing, but in this chapter, we will
    focus our attention on only these three.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Python, especially Python 3, has built-in support for all these types of concurrent
    computing techniques in its standard library. For example, it supports multi-threading
    via its *threading* module, and multiple processes via its *multiprocessing* module.
    Asynchronous execution support is available via the *asyncio* module. A form of
    concurrent processing that combines asynchronous execution with threads and processes
    is available via the *concurrent.futures* module.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: In the coming sections we will take a look at each of these in turn with sufficient
    examples.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NOTE: The asyncio module is available only in Python 3'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency versus parallelism
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will take a brief look at the concept of concurrency and its close cousin,
    namely parallelism.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Both concurrency and parallelism are about executing work simultaneously rather
    than sequentially. However, in concurrency, the two tasks need not be executed
    at the exact same time; instead, they just need to be scheduled to be executed
    simultaneously. Parallelism, on the other hand, requires that both the tasks execute
    together at a given moment in time.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 并发和并行都是关于同时执行工作而不是顺序执行。然而，在并发中，两个任务不需要在完全相同的时间执行；相反，它们只需要被安排同时执行。另一方面，并行性要求两个任务在给定的时间点同时执行。
- en: 'To take a real-life example, let''s say you are painting two exterior walls
    of your house. You have employed just one painter, and you find that he is taking
    a lot more time than you thought. You can solve the problem in these two ways:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 举一个现实生活的例子，假设你正在为你房子的两面外墙涂漆。你只雇用了一个画家，你发现他花的时间比你想象的要多。你可以通过以下两种方式解决问题：
- en: Instruct the painter to paint a few coats on one wall before switching to the
    next wall, and doing the same there. Assuming he is efficient, he will work on
    both the walls simultaneously (though not at the same time), and achieve the same
    degree of finish on both walls for a given time. This is a *concurrent* solution.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指示画家在切换到下一面墙之前在一面墙上涂几层，然后在另一面墙上做同样的事情。假设他很有效，他将同时（虽然不是在同一时间）在两面墙上工作，并在给定的时间内达到相同的完成度。这是一个*并发*解决方案。
- en: Employ one more painter. Instruct the first painter to paint the first wall,
    and the second painter to paint the second wall. This is a *parallel* solution.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再雇用一个画家。指示第一个画家涂第一面墙，第二个画家涂第二面墙。这是一个*并行*解决方案。
- en: Two threads are performing bytecode computations in a single core CPU do not
    exactly perform parallel computation, as the CPU can accommodate only one thread
    at a time. However, they are concurrent from a programmer's perspective, since
    the CPU scheduler performs fast switching in and out of the threads so that they
    appear to run in parallel.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 两个线程在单核CPU中执行字节码计算并不完全进行并行计算，因为CPU一次只能容纳一个线程。然而，从程序员的角度来看，它们是并发的，因为CPU调度程序快速地在线程之间进行切换，使它们看起来是并行运行的。
- en: However, on a multi-core CPU, two threads can perform parallel computations
    at any given time in its different cores. This is true parallelism.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在多核CPU上，两个线程可以在不同的核心上同时进行并行计算。这是真正的并行。
- en: Parallel computation requires that the computation resources increase at least
    linearly with respect to its scale. Concurrent computation can be achieved by
    using the techniques of multitasking, where work is scheduled and executed in
    batches, making better use of existing resources.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 并行计算要求计算资源至少与其规模成线性增长。通过使用多任务处理技术，可以实现并发计算，其中工作被安排并批量执行，更好地利用现有资源。
- en: Note
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In this chapter, we will use the term *concurrent* uniformly to indicate both
    types of execution. In some places, it may indicate concurrent processing in the
    traditional way, and in some other, it may indicate true parallel processing.
    Use the context to disambiguate.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将统一使用术语*并发*来指示两种类型的执行。在某些地方，它可能表示传统方式的并发处理，而在其他地方，它可能表示真正的并行处理。使用上下文来消除歧义。
- en: Concurrency in Python – multithreading
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python中的并发 - 多线程
- en: We will start our discussion of concurrent techniques in Python with multithreading.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从Python中的多线程开始讨论并发技术。
- en: 'Python supports multiple threads in programming via its *threading* module.
    The threading module exposes a `Thread` class, which encapsulates a thread of
    execution. Along with this, it also exposes the following synchronization primitives:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Python通过其*threading*模块支持多线程编程。线程模块公开了一个`Thread`类，它封装了一个执行线程。除此之外，它还公开了以下同步原语：
- en: A `Lock` object, which is useful for synchronized protected access to share
    resources, and its cousin `RLock`.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个`Lock`对象，用于同步保护对共享资源的访问，以及它的表兄弟`RLock`。
- en: A Condition object, which is useful for threads to synchronize while waiting
    for arbitrary conditions.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个Condition对象，用于线程在等待任意条件时进行同步。
- en: An `Event` object, which provides a basic signaling mechanism between threads.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Event`对象，它提供了线程之间的基本信号机制。'
- en: A `Semaphore` object, which allows synchronized access to limited resources.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个`Semaphore`对象，它允许对有限资源进行同步访问。
- en: A `Barrier` object, which allows a fixed set of threads to wait for each other,
    synchronize to a particular state, and proceed.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个`Barrier`对象，它允许一组固定的线程等待彼此，同步到特定状态，然后继续。
- en: Thread objects in Python can be combined with the synchronized `Queue` class
    in the queue module for implementing thread-safe producer/consumer workflows.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Python中的线程对象可以与队列模块中的同步`Queue`类结合使用，用于实现线程安全的生产者/消费者工作流程。
- en: Thumbnail generator
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩略图生成器
- en: Let us start our discussion of multi-threading in Python with the example of
    a program used to generate thumbnails of image URLs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个用于生成图像URL缩略图的程序的例子开始讨论Python中的多线程。
- en: 'In the example, we are using **Pillow**, a fork of the **Python Imaging Library**
    (**PIL**) to perform this operation:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用**Pillow**，它是**Python Imaging Library**（**PIL**）的一个分支，来执行这个操作：
- en: '[PRE0]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding code works very well for single URLs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码对单个URL非常有效。
- en: 'Let us say we want to convert five image URLs to their thumbnails:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要将五个图像URL转换为它们的缩略图：
- en: '[PRE1]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s see how such a function performs with respect to time taken in the following
    screenshot:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这样一个函数在所花费的时间方面的表现：
- en: '![Thumbnail generator](../Images/image00433.jpeg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![缩略图生成器](../Images/image00433.jpeg)'
- en: Response time of serial thumbnail converter for 5 URLs
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于5个URL的串行缩略图转换的响应时间
- en: The function took approximately 1.7 seconds per URL.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数大约每个URL花费了1.7秒的时间。
- en: 'Let us now scale the program to multiple threads so we can perform the conversions
    concurrently. Here is the rewritten code to run each conversion in its own thread:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将程序扩展到多个线程，这样我们就可以同时进行转换。以下是重写的代码，以便在每个转换中运行自己的线程：
- en: '[PRE2]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The timing that this last program now gives is shown in this screenshot:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail generator](../Images/image00434.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: Response time of threaded thumbnail converter for 5 URLs
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: With this change, the program returns in 1.76 seconds, almost equal to the time
    taken by a single URL in serial execution before. In other words, the program
    has now linearly scaled with respect to the number of threads. Note that, we had
    to make no change to the function itself to get this scalability boost.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – producer/consumer architecture
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous example, we saw a set of image URLs being processed by a thumbnail
    generator function concurrently by using multiple threads. With the use of multiple
    threads, we were able to achieve near linear scalability as compared to serial
    execution.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: However, in real life, rather than processing a fixed list of URLs, it is more
    common for the URL data to be produced by some kind of URL producer. It could
    be fetching this data from a database, a **comma separated value** (**CSV**) file
    or from a TCP socket for example.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: In such a scenario, creating one thread per URL would be a tremendous waste
    of resources. It takes a certain overhead to create a thread in the system. We
    need some way to reuse the threads we create.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'For such systems that involve a certain set of threads producing data and another
    set of threads consuming or processing data, the producer/consumer model is an
    ideal fit. Such a system has the following features:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Producers are a specialized class of workers (threads) producing the data. They
    may receive the data from a specific source(s), or generate the data themselves.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Producers add the data to a shared synchronized queue. In Python, this queue
    is provided by the `Queue` class in the aptly named `queue` module.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another set of specialized class of workers, namely consumers, wait on the queue
    to get (consume) the data. Once they get the data, they process it and produce
    the results.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The program comes to an end when the producers stop generating data and the
    consumers are starved of data. Techniques like timeouts, polling, or poison pills
    can be used to achieve this. When this happens, all threads exit, and the program
    completes.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have rewritten our thumbnail generator to a producer consumer architecture.
    The resulting code is given next. Since this is a bit detailed, we will discuss
    each class one by one.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at the imports—these are pretty self-explanatory:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next is the code for the producer class:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s analyze the producer class code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: The class is named `ThumbnailURL_Generator`. It generates the URLs (by using
    the service of a website named [http://dummyimage.com](http://dummyimage.com))
    of different sizes, foreground, and background colors. It inherits from the `threading.Thread`
    class.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It has a `run` method, which goes in a loop, generates a random image URL, and
    pushes it to the shared queue. Every time, the thread sleeps for a fixed time,
    as configured by the `sleep_time` parameter.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The class exposes a `stop` method, which sets the internal flag to `False` causing
    the loop to break and the thread to finish its processing. This can be called
    externally by another thread, typically, the main thread.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, the URL consumer class that consumes the thumbnail URLs and creates the
    thumbnails:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here''s the analysis of the consumer class:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The class is named `ThumbnailURL_Consumer`, as it consumes URLs from the queue,
    and creates thumbnail images of them.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `run` method of this class goes in a loop, gets a URL from the queue, and
    converts it to thumbnail by passing it to the `thumbnail_image` method. (Note
    that this code is exactly the same as that of the `thumbnail_image` function we
    created earlier.)
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `stop` method is very similar, checking for a stop flag every time in the
    loop, and ending once the flag has been unset.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is the main part of the code—setting up a couple of producers and consumers
    each, and running them:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is a screenshot of the program in action:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这是程序运行的屏幕截图：
- en: '![Thumbnail generator – producer/consumer architecture](../Images/image00435.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![缩略图生成器-生产者/消费者架构](../Images/image00435.jpeg)'
- en: Running the thumbnail producer/consumer program with 4 threads, 2 of each type
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用4个线程运行缩略图生产者/消费者程序，每种类型2个
- en: In the above program, since the producers keeps generating random data without
    any end, the consumers will keep consuming it without any end. Our program has
    no proper end condition.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述程序中，由于生产者不断生成随机数据而没有结束，消费者将继续消耗它而没有结束。我们的程序没有适当的结束条件。
- en: Hence this program will keep running forever till the network requests are denied
    or timed out or the disk space of the machine runs out because of thumbnails.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该程序将一直运行，直到网络请求被拒绝或超时，或者由于缩略图而使机器的磁盘空间耗尽。
- en: However, a program solving a real world problem should end in some way which
    is predictable.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，解决真实世界问题的程序应该以可预测的方式结束。
- en: This could be due to a number of external constraints
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是由于许多外部约束造成的。
- en: It could be a timeout introduced where the consumers wait for data for a certain
    maximum time, and then exit if no data is available during that time. This, for
    example, can be configured as a timeout in the `get` method of the queue.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以引入一个超时，在这种情况下，消费者等待一定的最大时间获取数据，如果在此期间没有可用数据，则退出。例如，这可以在队列的`get`方法中配置为超时。
- en: Another technique would be to signal program end after a certain number of resources
    are consumed or created. In this program, for example, it could be a fixed limit
    to the number of thumbnails created.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种技术是在消耗或创建一定数量的资源后发出程序结束信号。例如，在该程序中，可以限制创建的缩略图数量。
- en: In the following section, we will see how to enforce such resource limits by
    using threading synchronization primitives such as Locks and Semaphores.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将看到如何通过使用线程同步原语（如Locks和Semaphores）来强制执行此类资源限制。
- en: Note
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You may have observed that we start a thread using its `start` method, though
    the overridden method in the Thread subclass is `run`. This is because, in the
    parent `Thread` class, the `start` method sets up some state, and then calls the
    `run` method internally. This is the right way to call the thread's run method.
    It should never be called directly.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，我们使用`start`方法启动线程，尽管线程子类中的重写方法是`run`。这是因为在父`Thread`类中，`start`方法设置了一些状态，然后在内部调用`run`方法。这是调用线程的运行方法的正确方式。它不应该直接调用。
- en: Thumbnail generator – resource constraint using locks
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩略图生成器-使用锁的资源约束
- en: In the earlier section, we saw how to rewrite the thumbnail generator program
    moulded in the producer/consumer architecture. However, our program had a problem—it
    would run endlessly till it ran out of disk space or network bandwidth.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们看到了如何重写生产者/消费者架构中的缩略图生成器程序。然而，我们的程序有一个问题——它会无休止地运行，直到磁盘空间或网络带宽耗尽。
- en: In this section, we will see how to modify the program using a `Lock`, a synchronization
    primitive to implement a counter that will limit the number of images created
    as a way to end the program.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何使用`Lock`来修改程序，`Lock`是一种同步原语，用于实现限制创建图像数量的计数器，以结束程序。
- en: Lock objects in Python allows exclusive access by threads to a shared resource.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Python中的Lock对象允许线程对共享资源进行独占访问。
- en: 'The pseudo-code would be as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 伪代码如下：
- en: '[PRE7]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'However, Lock objects support context-managers via the with statement, so this
    is more commonly written as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Lock对象支持上下文管理器，通过`with`语句更常见地编写如下：
- en: '[PRE8]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To implement a fixed number of images per run, our code needs to be supported
    to add a counter. However, since multiple threads would check and increment this
    counter, it needs to be synchronized via a `Lock` object.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现每次运行固定数量的图像，我们的代码需要支持添加一个计数器。然而，由于多个线程将检查和增加此计数器，因此需要通过`Lock`对象进行同步。
- en: This is our first implementation of the resource counter class using Locks.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们使用Locks实现的资源计数器类的第一个实现。
- en: '[PRE9]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Since this modifies the consumer class as well, it makes sense to discuss both
    changes together. Here is the modified consumer class to accommodate the extra
    counter needed to keep track of the images:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这也修改了消费者类，因此讨论这两个更改是有意义的。这是修改后的消费者类，以适应需要跟踪图像的额外计数器：
- en: '[PRE10]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let's analyze both these classes. First the new class, `ThumbnailImageSaver`.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析这两个类。首先是新类`ThumbnailImageSaver`。
- en: This class derives from the `object`. In other words, it is not a `Thread`.
    It is not meant to be one.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个类派生自`object`。换句话说，它不是一个`Thread`。它不是一个`Thread`。
- en: It initializes a lock object and a counter dictionary in its initializer method.
    The lock is for synchronizing access to the counter by threads. It also accepts
    a `limit` parameter equal to the number of images it should save.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它在初始化方法中初始化了一个锁对象和一个计数器字典。锁用于线程同步访问计数器。它还接受一个等于应保存的图像数量的`limit`参数。
- en: The `thumbnail_image` method moves to here from the consumer class. It is called
    from a `save` method, which encloses the call in a synchronized context using
    the lock.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`thumbnail_image`方法从消费者类移动到这里。它从一个使用锁的上下文中的`save`方法调用。'
- en: The `save` method first checks if the count has crossed the configured limit;
    when this happens, the method returns `False`. Otherwise, the image is saved with
    a call to `thumbnail_image`, and the image filename is added to the counter, effectively
    incrementing the count.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`save`方法首先检查计数是否超过了配置的限制；当这种情况发生时，该方法返回`False`。否则，通过调用`thumbnail_image`保存图像，并将图像文件名添加到计数器，有效地增加计数。'
- en: Next, the modified `ThumbnailURL_Consumer` class.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是修改后的`ThumbnailURL_Consumer`类。
- en: The class's initializer is modified to accept an instance of the `ThumbnailImageSaver`
    as a `saver` argument. The rest of the arguments remain the same.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该类的初始化程序已修改为接受`ThumbnailImageSaver`的实例作为`saver`参数。其余参数保持不变。
- en: The `thumbnail_image` method no longer exists in this class, as it is moved
    to the new class.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个类中，`thumbnail_image`方法不再存在，因为它已经移动到新的类中。
- en: The `run` method is much simplified. It makes a call to the `save` method of
    the saver instance. If it returns `False`, it means the limit has been reached,
    the loop breaks, and the consumer thread exits.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`run`方法大大简化。它调用保存程序实例的`save`方法。如果返回`False`，则表示已达到限制，循环中断，消费者线程退出。'
- en: We also have modified the `__str__` method to return a unique ID per thread,
    which is set in the initializer using the `uuid` module. This helps to debug threads
    in a real-life example.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还修改了`__str__`方法，以返回每个线程的唯一ID，该ID在初始化时使用`uuid`模块设置。这有助于在实际示例中调试线程。
- en: 'The calling code also changes a bit, as it needs to set up the new object,
    and configure the consumer threads with it:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 调用代码也稍有更改，因为它需要设置新对象，并配置消费者线程：
- en: '[PRE11]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following are the main points to be noted here:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是需要注意的主要要点：
- en: We create an instance of the new `ThumbnailImageSaver` class, and pass it on
    to the consumer threads when creating them.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了新的`ThumbnailImageSaver`类的实例，并在创建消费者线程时将其传递给它们。
- en: We wait on consumers first. Note that, the main thread doesn't call `stop`,
    but `join` on them. This is because the consumers exit automatically when the
    limit is reached, so the main thread should just wait for them to stop.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先等待消费者。请注意，主线程不调用`stop`，而是对它们调用`join`。这是因为当达到限制时，消费者会自动退出，因此主线程应该等待它们停止。
- en: We stop the producers after the consumers exit—explicitly so—since they would
    otherwise keep working forever, since there is no condition for the producers
    to exit.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在消费者退出后，我们明确地停止生产者-因为否则它们将永远工作，因为没有条件让生产者退出。
- en: We use a dictionary instead of an integer as because of the nature of the data.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用字典而不是整数，因为数据的性质。
- en: Since the images are randomly generated, there is a minor chance of one image
    URL being same as another one created previously, causing the filenames to clash.
    Using a dictionary takes care of such possible duplicates.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图像是随机生成的，因此有可能一个图像URL与之前创建的另一个图像URL相同，导致文件名冲突。使用字典可以解决这种可能的重复情况。
- en: 'The following screenshot shows a run of the program with a limit of 100 images.
    Note that we can only show the last few lines of the console log, since it produces
    a lot of output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了使用100张图像限制运行程序的情况。请注意，我们只能显示控制台日志的最后几行，因为它会产生大量输出：
- en: '![Thumbnail generator – resource constraint using locks](../Images/image00436.jpeg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![缩略图生成器-使用锁的资源约束](../Images/image00436.jpeg)'
- en: Run of the thumbnail generator program with a limit of 100 images using a Lock
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 使用锁限制100张图像的缩略图生成程序的运行
- en: You can configure this program with any limit of the images, and it will always
    fetch exactly the same count—nothing more or less.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将此程序配置为任何图像限制，并且它将始终获取完全相同的数量-既不多也不少。
- en: In the next section, we will get familiarized with another synchronization primitive,
    namely *Semaphore*, and learn how to implement a resource limiting class in a
    similar way using the semaphore.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将熟悉另一个同步原语，即*信号量*，并学习如何使用信号量以类似的方式实现资源限制类。
- en: Thumbnail generator – resource constraint using semaphores
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用信号量的缩略图生成器-资源约束
- en: Locks aren't the only way to implement synchronization constraints and write
    logic on top of them such as to limit resources used/generated by a system.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 锁不是实现同步约束和在其上编写逻辑的唯一方法，例如限制系统使用/生成的资源。
- en: A `Semaphore`, one of the oldest synchronization primitives in computer science,
    is ideally suited for such use cases.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 信号量是计算机科学中最古老的同步原语之一，非常适合这种用例。
- en: 'A semaphore is initialized with a value greater than zero:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 信号量是用大于零的值初始化的：
- en: When a thread calls `acquire` on a semaphore that has a positive internal value,
    the value gets decremented by one, and the thread continues on its way.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当线程在具有正内部值的信号量上调用`acquire`时，该值会减少一个，并且线程会继续进行。
- en: When another thread calls `release` on the semaphore, the value is incremented
    by 1.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当另一个线程在信号量上调用`release`时，该值会增加1。
- en: Any thread calling `acquire` once the value has reached zero is blocked on the
    semaphore till it is woken up by another thread calling *release*.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦值达到零，任何线程调用`acquire`都会在信号量上被阻塞，直到另一个线程调用*release*唤醒它。
- en: Due to this behavior, a semaphore is perfectly suited for implementing a fixed
    limit on shared resources.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种行为，信号量非常适合在共享资源上实现固定限制。
- en: 'In the following code example, we will implement another class for resource
    limiting our thumbnail generator program, this time using a semaphore:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们将使用信号量实现另一个用于限制缩略图生成器程序资源的类：
- en: '[PRE12]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Since the new semaphore-based class keeps the exact same interface as the previous
    lock-based class—with a save method—there is no need to change any code on the
    consumer!
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于信号量的新类保持与基于锁的先前类完全相同的接口-具有保存方法-因此不需要更改任何消费者代码！
- en: Only the calling code needs to be changed.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 只有调用代码需要更改。
- en: 'This line in the previous code which initialized the `ThumbnailImageSaver`
    instance:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的代码中初始化了`ThumbnailImageSaver`实例的这一行：
- en: '[PRE13]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding line needs to be replaced with the following one:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 前一行需要替换为以下行：
- en: '[PRE14]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The rest of the code remains exactly the same.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 其余代码保持完全相同。
- en: 'Let us quickly discuss the new class using the semaphore before seeing this
    code in action:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到这段代码之前，让我们快速讨论一下使用信号量的新类：
- en: The `acquire` and `release` methods are simple wrappers over the same methods
    on the semaphore.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`acquire`和`release`方法只是对信号量上相同方法的简单包装。'
- en: We initialize the semaphore with a value equal to the image limit in the initializer.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在初始化程序中使用图像限制的值来初始化信号量。
- en: In the save method, we call the `acquire` method. If the semaphore's limit is
    reached, it will return `False`. Otherwise, the thread saves the image and returns
    `True`. In the former case, the calling thread quits.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在保存方法中，我们调用`acquire`方法。如果信号量的限制已达到，它将返回`False`。否则，线程保存图像并返回`True`。在前一种情况下，调用线程退出。
- en: Note
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The internal count attribute of this class is only there for debugging. It doesn't
    add anything to the logic of limiting images.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类的内部计数属性只用于调试。它对限制图像的逻辑没有任何添加。
- en: 'This class behaves in a way similar way to the previous one, and limits resources
    exactly. The following is an example with a limit of 200 images:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类的行为方式与前一个类似，并且确切地限制资源。以下是一个限制为200张图片的示例：
- en: '![Thumbnail generator – resource constraint using semaphores](../Images/image00437.jpeg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![使用信号量的缩略图生成器-资源约束](../Images/image00437.jpeg)'
- en: Run of the thumbnail generator program with a limit of 200 images using a Semaphore
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 使用信号量运行缩略图生成程序，限制为200张图片
- en: Resource constraint – semaphore versus lock
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源约束-信号量与锁
- en: We saw two competing versions of implementing a fixed resource constraint in
    the previous two examples—one using `Lock` and another using `Semaphore`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两个示例中，我们看到了两个实现固定资源约束的竞争版本——一个使用`Lock`，另一个使用`Semaphore`。
- en: 'The differences between the two versions are as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 两个版本之间的区别如下：
- en: The version using Lock protects all the code that modifies the resource—in this
    case, checking the counter, saving the thumbnail, and incrementing the counter—to
    make sure that there are no data inconsistencies.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用锁的版本保护了所有修改资源的代码——在这种情况下，检查计数器、保存缩略图和增加计数器——以确保没有数据不一致。
- en: The Semaphore version is implemented more like a gate—a door that is open while
    the count is below the limit, and through which any number of threads can pass,
    and that only closes when the limit is reached. In other words, it doesn't mutually
    exclude threads from calling the thumbnail saving function.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 信号量版本更像是一个门，当计数低于限制时门是打开的，任意数量的线程可以通过，只有当达到限制时才关闭。换句话说，它不会互斥地排除线程调用缩略图保存函数。
- en: Hence, the effect is that the Semaphore version would be faster than the version
    using Lock.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，信号量版本的效果将比使用锁的版本更快。
- en: How much faster? The following timing example for a run of 100 images gives
    an idea.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 有多快？以下是一个运行100张图片的计时示例。
- en: 'This screenshot shows the time it takes for the Lock version to save 100 images:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这个截图显示了使用锁版本保存100张图片所需的时间：
- en: '![Resource constraint – semaphore versus lock](../Images/image00438.jpeg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![资源约束-信号量与锁](../Images/image00438.jpeg)'
- en: Timing the run of the thumbnail generator program—the Lock version—for 100 images
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 计时缩略图生成程序的运行——锁版本——100张图片
- en: 'The following screenshot shows the time for the Semaphore version to save a
    similar number:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了使用信号量版本保存类似数量的时间：
- en: '![Resource constraint – semaphore versus lock](../Images/image00439.jpeg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![资源约束-信号量与锁](../Images/image00439.jpeg)'
- en: Timing the run of the thumbnail generator program—the Semaphore version—for
    100 images
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 计时缩略图生成程序的运行——信号量版本——100张图片
- en: By a quick calculation you can see that the semaphore version is about 4 times
    faster than the lock version for the same logic. In other words, it *scales 4
    times better*.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通过快速计算，您可以看到信号量版本比锁版本快大约4倍，逻辑相同。换句话说，它*扩展4倍*。
- en: Thumbnail generator – URL rate controller using conditions
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩略图生成器-使用条件控制URL速率
- en: In this section, we will briefly see the application of another important synchronization
    primitive in threading, namely the `Condition` object.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍线程中另一个重要的同步原语的应用，即`Condition`对象。
- en: First, we will get a real life example of using a `Condition` object. We will
    implement a throttler for our thumbnail generator to manage the rate of URL generation.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将得到一个使用`Condition`对象的现实生活示例。我们将为我们的缩略图生成器实现一个节流器，以管理URL生成的速率。
- en: 'In the producer/consumer systems in real life, the following three kinds of
    scenario can occur with respect to the rate of data production and consumption:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中的生产者/消费者系统中，关于数据生产和消费速度，可能会出现以下三种情况：
- en: Producers produce data at a faster pace than consumers can consume. This causes
    the consumers to always play catch up with the producers. Excess data by the producers
    can accumulate in the queue, which causes the queue to consume a higher memory
    and CPU usage in every loop causing the program to slow down.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生产者产生的数据速度比消费者消耗的速度快。这导致消费者总是在追赶生产者。生产者产生的多余数据可能会积累在队列中，导致队列消耗更多的内存和CPU使用率，从而使程序变慢。
- en: Consumers consume data at a faster rate than producers. This causes the consumers
    to always wait on the queue—for data. This, in itself, is not a problem as long
    as the producers don't lag too much. In the worst case, this leads to half of
    the system, that is, the consumers, remaining idle, while the other half—the producers—try
    to keep up with the demand.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 消费者以比生产者更快的速度消耗数据。这导致消费者总是在队列上等待数据。这本身并不是问题，只要生产者不落后太多。在最坏的情况下，这会导致系统的一半，即消费者，保持空闲，而另一半——生产者——试图满足需求。
- en: Both producers and consumers work at nearly the same pace keeping the queue
    size within limits. This is the ideal scenario.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生产者和消费者以几乎相同的速度工作，保持队列大小在限制范围内。这是理想的情况。
- en: 'There are many ways to solve this problem. Some of them are as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以解决这个问题。其中一些如下：
- en: '**Queue with a fixed size** – Producers would be forced to wait till data is
    consumed by a consumer once the queue size limit is reached. However this would
    almost always keeps the queue full.'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**具有固定大小的队列**——一旦队列大小限制达到，生产者将被迫等待，直到数据被消费者消耗。然而，这几乎总是使队列保持满状态。'
- en: '**Provide the workers with timeouts plus other responsibilities**: Rather than
    remain blocked on the queue, producers and/or consumers can use a timeout to wait
    on the queue. When they time out they can either sleep or perform some other responsibilities
    before coming back and waiting on the queue.'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dynamically configure the number of workers**: This is an approach where
    the worker pool size automatically increases or decreases upon demand. If one
    class of workers is ahead, the system will launch just the required number of
    workers of the opposite class to keep the balance.'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Adjust the data generation rate**: In this approach, we statically or dynamically
    adjust the data generation rate by the producers. For example, the system can
    be configured to produce data at a fixed rate, say, 50 URLs in a minute or it
    can calculate the rate of consumption by the consumers, and adjust the data production
    rate of the producers dynamically to keep things in balance.'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following example, we will implement the last approach—to limit the production
    rate of URLs to a fixed limit using `Condition` objects.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'A `Condition` object is a sophisticated synchronization primitive that comes
    with an implicit built-in lock. It can wait on an arbitrary condition till it
    becomes True. The moment the thread calls `wait` on the condition, the internal
    lock is released, but the thread itself becomes blocked:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, another thread can wake up this preceding thread by setting the condition
    to True, and then calling `notify` or `notify_all` on the condition object. At
    this point, the preceding blocked thread is woken up, and continues on its way:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here is our new class namely `ThumbnailURLController` which implements the rate
    control of URL production using a condition object.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s discuss the preceding code before we discuss the changes in the producer
    class that will make use of this class:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: The class is an instance of `Thread`, so it runs in its own thread of execution.
    It also holds a Condition object.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It has a `calc_rate` method, which calculates the rate of generation of URLs
    by keeping a counter and using timestamps.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `run` method, the rate is checked. If it's below the configured limit,
    the condition object notifies all threads waiting on it.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Most importantly, it implements a `throttle` method. This method uses the current
    rate, calculated via `calc_rate`, and uses it to throttle and adjust the sleep
    times of the producers. It mainly does these two things:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the rate is more than the configured limit, it causes the calling thread
    to wait on the condition object till the rate levels off. It also calculates an
    extra sleep time that the thread should sleep in its loop to adjust the rate to
    the required level.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the rate is less than the configured limit, then the thread needs to work
    faster and produce more data, so it calculates the sleep difference and lowers
    the sleep limit accordingly.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is the code of the producer class to incorporate the changes:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s see how this last code works:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: The class now accepts an additional controller object in its initializer. This
    is the instance of the controller class given earlier.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After putting a URL, it increments the count on the controller. Once the count
    reaches a minimum limit (set as 5 to avoid early throttling of the producers),
    it calls `throttle` on the controller, passing itself as the argument.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The calling code also needs quite a few changes. The modified code is shown
    as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The main changes here are the ones listed next:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: The controller object is created – with the exact number of producers that will
    be created. This helps the correct calculation of sleep time per thread.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The producer threads themselves are passed the instance of the controller in
    their initializer.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The controller is started as a thread before all other threads.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here is a run of the program configured with 200 images at the rate of 50 images
    per minute. We show two images of the running program's output, one at the beginning
    of the program and one towards the end.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail generator – URL rate controller using conditions](../Images/image00440.jpeg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: Starting the thumbnail program with URL rate controller—at 50 URLs per minute
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: You will find that, when the program starts, it almost immediately slows down,
    and nearly comes to a halt, since the original rate is high. What happens here
    is that the producers call on the `throttle` method, and since the rate is high,
    they all get blocked on the condition object.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: After a few seconds, the rate comes down to the prescribed limit, since no URLs
    are generated. This is detected by the controller in its loop, and it calls `notify_all`
    on the threads, waking them up.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: After a while you will see that the rate is getting settled around the set limit
    of 50 URLs per minute.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail generator – URL rate controller using conditions](../Images/image00441.jpeg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: The thumbnail program with URL rate controller 5-6 seconds after start
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Towards the end of the program, you will see that the rate has almost settled
    to the exact limit:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail generator – URL rate controller using conditions](../Images/image00442.jpeg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
- en: The thumbnail program with URL rate controller towards the end
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: We are coming towards the end of our discussion on threading primitives and
    how to use them in improving the concurrency of your programs and in implementing
    shared resource constraints and controls.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Before we conclude, we will look at an aspect of Python threads which prevents
    multi-threaded programs from making full use of the CPU in Python – namely the
    GIL or Global Interpreter Lock.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading – Python and GIL
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Python there is, a global lock that prevents multiple threads from executing
    native bytecode at once. This lock is required, since the memory management of
    CPython (the native implementation of Python) is not thread-safe.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: This lock is called **Global Interpreter Lock** or just **GIL**.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Python cannot execute bytecode operations concurrently on CPUs due to the GIL.
    Hence, Python becomes nearly unsuitable for the following cases:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: When the program depends on a number of heavy bytecode operations, which it
    wants to run concurrently
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the program uses multithreading to utilize the full power of multiple CPU
    cores on a single machine
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I/O calls and long-running operations typically occur outside the GIL. So multithreading
    is efficient in Python only when it involves some amount of I/O or such operations-
    such as image processing.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: In such cases, scaling your program to concurrently scale beyond a single process
    becomes a handy approach. Python makes this possible via its `multiprocessing`
    module, which is our next topic of discussion.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency in Python – multiprocessing
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Python standard library provides a multiprocessing module, which allows
    a programmer to write programs that scale concurrently using multiple processes
    instead of threads.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Since multi-processing scales computation across multiple processes, it effectively
    removes any issues with the GIL in Python. Programs can make use of multiple CPU
    cores efficiently using this module.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: The main class exposed by this module is the `Process` class, the analog to
    the `Thread` class in the threading module. It also provides a number of synchronization
    primitives, which are almost exact counterparts of their cousins in the threading
    module.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: We will get started by using an example using the `Pool` object provided by
    this module. It allows a function to execute in parallel over multiple inputs
    using processes.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: A primality checker
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following function is a simple checker function for primality, that is,
    whether the input number is prime or not:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following is a threaded class that uses this last function to check numbers
    from a queue for primality:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We will test it with 1,000 large prime numbers. In order to save space for
    the list represented here, what we''ve done is to take 10 of these numbers and
    multiply the list with 100:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We''ve used four threads for this test. Let''s see how the program performs,
    in the following screenshot:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '![A primality checker](../Images/image00443.jpeg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
- en: Primality checker of 1,000 numbers using a pool of 4 threads
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, here is the equivalent code using the multiprocessing `Pool` object:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The following screenshot shows its performance over the same set of numbers:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![A primality checker](../Images/image00444.jpeg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: Primality checker of 1,000 numbers using a multiprocessing Pool of 4 processes
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'We learn the following by comparing these numbers:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: The real time, that is, the wall clock time spent by the process pool version
    at 1 minute 9.6 seconds (69.6 seconds) is nearly 50% lesser than that of the thread
    pool version at 2 minute 12 seconds (132 seconds).
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, notice that the user time—that is, the time spent inside the CPU for
    user code—for the process pool version at 4 minute 22 seconds (262 seconds ) is
    nearly two times more than that of the thread pool version at 2 minutes 12 seconds
    (132 seconds).
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The real and user CPU time of the thread pool version is exactly the same at
    2 minutes 12 seconds. This is a clear indication that the threaded version was
    able to execute effectively, only in one of the CPU cores.
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This means that the process pool version was able to better make use of all
    the CPU cores, since for the 50% of the real time of the thread pool version,
    it was able to make use of the CPU time twice over.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, the real performance boost in terms of CPU time/real time for the two
    programs is as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Threaded version → 132 seconds/132 seconds = 1
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process version → 262 seconds/69.6 seconds = 3.76 ~= 4
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The real performance ratio of the process version to the threaded version is,
    hence, given as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 4/1 = 4
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: The machine on which the program was executed has a four-core CPU. This clearly
    shows that the multiprocess version of the code was able to utilize all the four
    cores of the CPU nearly equally.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: This is because the threaded version is being restricted by the GIL, whereas
    the process version has no such restriction and can freely make use of all the
    cores.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let us get on to a more involved problem—that of sorting
    disk-based files.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Sorting disk files
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you have hundreds of thousands of files on the disk, each containing
    a certain fixed number of integers in a given range. Let's say we need the files
    to be sorted and merged into a single file.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: If we decide to load all this data into memory, it will need large amounts of
    RAM. Let's do a quick calculation for a million files, each containing around
    100 integers in the range of 1 to 10,000 for a total of 1,00,000,000 or 100 million
    integers.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume each of the files is loaded as a list of integers from the disk—we
    will ignore string processing, and the like for the time being.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `sys.getsizeof`, we can get a rough calculation going:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: So, the entire data will take close to 800 MB if loaded into memory at once.
    Now this may not look like a large memory footprint at first, but the larger the
    list, the more system resources it takes to sort it in memory as one large list.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the simplest code for sorting of all the integers present in the disk
    files after loading them into memory:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This preceding code loads a certain number of files from the disk, each containing
    100 integers in the range 1 to 10,000\. It reads each file, maps it to a list
    of integers, and adds each list to a cumulative list. Finally, the list is sorted
    and written to a file.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the time taken to sort a certain number of disk files:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '| Number of files (n) | Time taken for sorting |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 17.4 seconds |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '| 10000 | 101 seconds |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: '| 100000 | 138 seconds |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| 1000000 | NA |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: As you can see, the time taken scales pretty reasonably—less than *O(n)*. However,
    this is one problem where more than the time, it is the space—in terms of memory
    and operations on it—that matters.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the machine that was used to conduct the test, an 8-GB RAM,
    4-core CPU laptop with 64-bit Linux, the test with a million numbers didn't finish.
    Instead, it caused the system to hang, so it was not completed.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Sorting disk files – using a counter
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you look at the data, you find that there is an aspect that allows us to
    treat the problem as more about space than time. This is the observation that
    the integers are in a fixed range with a maximum limit of 10,000.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Hence, instead of loading all the data as separate lists and merging them, one
    can use a data structure like a counter.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the basic idea of how this works:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a data structure—a counter, where each integer starts from 1… 10,000
    the maximum entry is initialized to zero.
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load each file and convert the data to a list. For any number found in the list,
    increment its count in the counter data structure initialized in Step 1.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, loop through the counter, and output each number with a count greater
    than zero *so many times*, and save the output to a file. The output is your merged
    and sorted single file:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In the preceding code, we use a `defaultdict` from the collections module as
    the counter. Whenever we encounter an integer, we increment its count. In the
    end, the counter is looped through, and each item is output as many times as it
    was found.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: The sort and merge happen due to the way we have converted the problem from
    one of sorting integers to one of keeping a count and outputting in a naturally
    sorted order.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the time taken for sorting of numbers against
    the size of the input – in terms of number of disk files:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '| Number of files (n) | Time taken for sorting |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 16.5 seconds |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: '| 10000 | 83 seconds |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '| 100000 | 86 seconds |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| 1000000 | 359 seconds |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: Though the performance for the smallest case – that of 1,000 files is similar
    to that for the in-memory sort, the performance becomes better as the size of
    the input increases. This code also manages to finish the sorting of a million
    files or 100 million integers - in about 5m 59s.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In timing measurements for processes that read files, there is always the effect
    of buffer caches in the kernel. You will find that running the same performance
    test successively shows a tremendous improvement, as Linux caches the contents
    of the files in its buffer cache. Hence, subsequent tests for the same input size
    should be done after clearing the buffer cache. In Linux, this can be done by
    the following command:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In our tests for successive numbers, we *don't* reset the buffer caches as shown
    before. This means that runs for higher numbers enjoy a performance boost from
    the caches created during the previous runs. However, since this is done uniformly
    for each test, the results are comparable. The cache is reset before starting
    the test suite for a specific algorithm.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm also requires much lesser memory, since for each run, the memory
    requirements are *the same* since we are using an array of integers upto MAXINT
    and just incrementing the count.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Here is the memory usage of the sort in-memory program for 100,000 files using
    the *memory_profiler*, which we have encountered in the previous chapter.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '![Sorting disk files – using a counter](../Images/image00445.jpeg)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
- en: Memory usage of in-memory sort program for an input of 100,000 files
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 'And the following screenshot shows the memory usage for the sort counter for
    the same number of files:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '![Sorting disk files – using a counter](../Images/image00446.jpeg)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
- en: Memory usage of counter sort program for an input of 100,000 files
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: The memory usage of the in-memory sort program at 465 MB is more than six times
    that of the counter sort program at 70 MB. Also note that the sorting operation
    itself takes extra memory of nearly 10 MB in the in-memory version.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Sorting disk files – using multiprocessing
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we rewrite the counter sorting program using multiple processes.
    The approach is to scale the processing input files for more than one process
    by splitting the list of file paths to a pool of processes – and planning to take
    advantage of the resulting data parallelism.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the rewrite of the code:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'It is exactly the same code as earlier with the following changes:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Instead of processing all the files as a single list, the filenames are put
    in batches, with batches equaling the size of the pool.
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use a sorter function, which accepts the list of filenames, processes them,
    and returns a dictionary with the counts.
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The counts are summed for each integer in the range from 1 to MAXINT, and so
    many numbers are written to the sorted file.
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following table shows the data for processing a different number of files
    for a pool sizes of 2 and 4 respectively:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '| Number of files (n) | Pool size | Time taken for sorting |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
- en: '| 1,000 | 2 | 18 seconds |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
- en: '| 4 | 20 seconds |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '| 10,000 | 2 | 92 seconds |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '| 4 | 77 seconds |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '| 100,000 | 2 | 96 seconds |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| 4 | 86 seconds |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| 1,000,000 | 2 | 350 seconds |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| 4 | 329 seconds |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: 'The numbers tell an interesting story:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: The multiple process version one with 4 processes (equal to number of cores
    in the machine) has better numbers overall when compared to the one with 2 processes
    and the single process one.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, the multiple-process version doesn't seem to offer much of a performance
    benefit when compared to the single-process version. The performance numbers are
    very similar and any improvement is within bounds of error and variation. For
    example, for 1 million number input the multiple process with 4 processes has
    just a 8% improvement over the single-process one.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is because the bottleneck here is the processing time it takes to load
    the files into memory – in file I/O - not the computation (sorting), as the sorting
    is just an increment in the counter. Hence the single process version is pretty
    efficient as it is able to load all the file data in the same address space. The
    multiple-process ones are able to improve this a bit by loading the files in multiple
    address spaces, but not by a lot.
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This example shows that in situations where there is not much computation done
    but the bottleneck is disk or file I/O, the impact of scaling by multi-processing
    is much lesser.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading versus multiprocessing
  id: totrans-410
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have come to the end of our discussion on multi-processing, it is
    a good time to compare and contrast the scenarios where one needs to choose between
    scaling using threads in a single process or using multiple processes in Python.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Here are some guidelines.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 'Use multithreading in the following cases:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: The program needs to maintain a lot of shared states, especially mutable ones.
    A lot of the standard data structures in Python, such as lists, dictionaries,
    and others, are thread-safe, so it costs much less to maintain a mutable shared
    state using threads than via processes.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The program needs to keep a low memory foot-print.
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The program spends a lot of time doing I/O. Since the GIL is released by threads
    doing I/O, it doesn't affect the time taken by the threads to perform I/O.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The program doesn't have a lot of data parallel operations which it can scale
    across multiple processes
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use multiprocessing in these scenarios:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'The program performs a lot of CPU-bound heavy computing: byte-code operations,
    number crunching, and the like on reasonably large inputs.'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The program has inputs which can be parallelized into chunks and whose results
    can be combined afterwards – in other words, the input of the program yields well
    to data-parallel computations.
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The program doesn't have any limitations on memory usage, and you are on a modern
    machine with a multicore CPU and large enough RAM.
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is not much shared mutable state between processes that need to be synchronized—this
    can slow down the system, and offset any benefits gained from multiple processes.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your program is not heavily dependent on I/O—file or disk I/O or socket I/O.
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Concurrecy in Python - Asynchronous Execution
  id: totrans-424
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen two different ways to perform concurrent execution using multiple
    threads and multiple processes. We saw different examples of using threads and
    their synchronization primitives. We also saw a couple of examples using multi-processing
    with slightly varied outcomes.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Apart from these two ways to do concurrent programming, another common technique
    is that of asynchronous programming or asynchronous I/O.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: In an asynchronous model of execution, tasks are picked to be executed from
    a queue of tasks by a scheduler, which executes these tasks in an interleaved
    manner. There is no guarantee that the tasks will be executed in any specific
    order. The order of execution of tasks depend upon how much processing time a
    task is willing to *yield* to another task in the queue. Put in other words, asynchronous
    execution happens through co-operative multitasking.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous execution usually happens in a single thread. This means no true
    data parallelism or true parallel execution can happen. Instead, the model only
    provides a semblance of parallelism.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: As execution happens out of order, asynchronous systems need a way to return
    the results of function execution to the callers. This usually happens with *callbacks*,
    which are functions to be called when the results are ready or using special objects
    that receive the results, often called *futures*.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Python 3 provides support for this kind of execution via its *asyncio* module
    using coroutines. Before we go on to discuss this, we will spend some time understanding
    pre-emptive multitasking versus cooperative multitasking, and how we can implement
    a simple cooperative multitasking scheduler in Python using generators.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Pre-emptive versus cooperative multitasking
  id: totrans-431
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The programs we wrote earlier using multiple threads were examples of concurrency.
    However, we didn't have to worry about how and when the operating system chose
    to run the thread—we just had to prepare the threads (or processes), provide the
    target function, and execute them. The scheduling is taken care of by the operating
    system.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: Every few ticks of the CPU clock, the operating system pre-empts a running thread,
    and replaces it with another one in a particular core. This can happen due to
    different reasons, but the programmer doesn't have to worry about the details.
    He just creates the threads, sets them up with the data they need to process,
    uses the correct synchronization primitives, and starts them. The operating system
    does the rest including switching and scheduling.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: This is how almost all modern operating systems work. It guarantees each thread
    a fair share of the execution time, all other things being equal. This is known
    as **pre-emptive multitasking**.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: There is another type of scheduling which is the opposite of pre-emptive multitasking.
    This is called as co-operative multitasking, where the operating system plays
    no role in deciding the priority and execution of competing threads or processes.
    Instead, a process or thread willingly yields control for another process or thread
    to run. Or a thread can replace another thread which is idling (sleeping) or waiting
    for I/O.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: This is the technique used in the asynchronous model of concurrent execution
    using co-routines. A function, while waiting for data, say a call on the network
    that is yet to return, can yield control for another function or task to run.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: Before we go to discuss actual co-routines using `asyncio` let us write our
    own co-operative multitasking scheduler using simple Python generators. It is
    not very difficult to do this as you can see below.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s analyze the preceding code:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: We have four functions—three generators, since they use the `yield` keyword
    to return the data, and a scheduler, which runs a certain set of tasks
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `square_mapper` function accepts an iterator, which returns integers iterating
    through it, and yields the squares of the members
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `prime_filter` function accepts a similar iterator, and filters out numbers
    that are not prime, yielding only prime numbers
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `number_generator` function acts as the input iterator to both these functions,
    providing them with an input stream of integers
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us now look at the calling code which ties all the four functions together.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here is an analysis of the calling code:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: The number generator is initialized with a count, which is received via the
    command-line argument. It is passed to the `square_mapper` function. The combined
    function is added as a task to the `tasks` list.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A similar operation is performed for the `prime_filter` function.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `scheduler` method is run by passing the task list to it, which it runs
    by iterating through a `for` loop, running each task one after another. The results
    are appended to a dictionary using the function's name as the key, and returned
    at the end of execution.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We print the last prime number's value to verify correct execution, and also
    the time taken for the scheduler to process.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see the output of our simple cooperative multitasking scheduler for
    a limit of `10`. This allows to capture all the input in a single command window,
    as seen in the following screenshot:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '![Pre-emptive versus cooperative multitasking](../Images/image00447.jpeg)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
- en: Output of the simple co-operative multitasking program example for an input
    of 10
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s analyze the output:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: The output of the `square_mapper` and `prime_filter` functions alternates on
    the console. This is because the scheduler switches between them in the `for`
    loop. Each of the functions are co-routines (generators) so they *yield* execution
    – that is the control is passed from one function to the next – and vice-versa.
    This allows both functions to run concurrently, while maintaining state and producing
    output.
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since we used generators here, they provide a natural way of generating the
    result plus yielding control in one go, using the *yield* keyword.
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The asyncio module in Python
  id: totrans-457
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `asyncio` module in Python provides support for writing concurrent, single-threaded
    programs using co-routines. It is available only in Python 3.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: 'A co-routine using the `asyncio` module is one that uses either of the following
    approaches:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: Using the `async def` statement for defining functions
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being decorated using the `@asyncio.coroutine` expression
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generator-based co-routines use the second technique, and they yield from expressions.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: Co-routines created using the first technique typically use the `await <future>`
    expression to wait for the future to be completed.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: Co-routines are scheduled for execution using an `event` loop, which connects
    the objects and schedules them as tasks. Different types of event loop are provided
    for different operating systems.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code rewrites our earlier example of a simple cooperative multitasking
    scheduler to use the `asyncio` module:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Here is how this last code works:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: The `number_generator` function is a co-routine that yields from the sub-generator
    `range(m, n+1)`, which is an iterator. This allows this co-routine to be called
    in other co-routines.
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `square_mapper` function is a co-routine of the first type using the `async
    def` keyword. It returns a list of squares using numbers from the number generator.
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `prime_filter` function is of the same type. It also uses the number generator,
    and appends prime numbers to a list and returns it.
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both co-routines yield to the other by sleeping using the *asyncio.sleep* function
    and waiting on it. This allows both co-routines to work concurrently in an interleaved
    fashion.
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is the calling code with the `event` loop and the rest of the plumbing:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here is the output of the program. Observe how the results of each of the task
    is getting printed in an interleaved fashion.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '![The asyncio module in Python](../Images/image00448.jpeg)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
- en: Result of executing the asyncio task calculating prime numbers and squares
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us analyze how the preceding code worked line by line, while following
    a top-to-bottom approach:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: We first get an asyncio event `loop` using the `factory` function `asyncio.get_event_loop`.
    This returns the default event loop implementation for the operating system.
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set up an asyncio `future` object by using the `gather` method of the module.
    This method is used to aggregate results from a set of co-routines or futures
    passed as its argument. We pass both the `prime_filter` and the `square_mapper`
    to it.
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A callback is added to the `future` object—the `print_result` function. It will
    be automatically called once the future's execution is completed.
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loop is run until the future's execution is completed. At this point the
    callback is called and it prints the result. Note how the output appears interleaved
    – as each task yields to the other one using the *sleep* function of the asyncio
    module.
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loop is closed and terminates is operation.
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Waiting for a future – async and await
  id: totrans-483
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed how one could wait for data from a future inside a co-routine using
    await. We saw an example that uses await to yield control to other co-routines.
    Let's now look at an example that waits for I/O completion on a future, which
    returns data from the web.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: For this example, you need the `aiohttp` module which provides an HTTP client
    and server to work with the asyncio module and supports futures. We also need
    the `async_timeout` module which allows timeouts on asynchronous co-routines.
    Both these modules can be installed using pip.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code—this is a co-routine that fetches a URL using a timeout and
    awaits the future, that is, the result of the operation:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The following is the calling code with the event loop:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: What are we doing in the preceding code?
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: We create an event loop and a list of URLs to be fetched. We also create an
    instance of `aiohttp ClientSession` object which is a helper for fetching URLs.
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a map of tasks by mapping the `fetch_page` function to each of the
    URLs. The session object is passed as first argument to the *fetch_page* function.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The tasks are passed to the wait method of `asyncio` with a timeout of `120`
    seconds.
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loop is run until complete. It returns two sets of futures—`done` and `pending`.
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We iterate through the future that is done, and print the response by fetching
    it using the `result` method of the `future`.
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can see the result of the operation (first few lines as many lines are
    output) in the following screenshot:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: '![Waiting for a future – async and await](../Images/image00449.jpeg)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
- en: Output of program doing an async fetch of URLs for 5 URLs
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: As you can see we are able to print the responses in terms of the a simple summary.
    How about processing the response to get more details about it such as the actual
    response text, the content length, status code, and so on?
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: The function below parses a list of *done* futures – waiting for the response
    data via *await* on the *read* method of the response*.* This returns the data
    for each response asynchronously.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The details of the `response` object—the final URL, status code, and length
    of data—are output by this method for each response before closing the response.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: We only need to add one more processing step on the list of completed responses
    for this to work.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note how we chain the co-routines together. The final link in the chain is the
    `parse_response` co-routine, which processes the list of done futures before the
    loop ends.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the output of the program:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: '![Waiting for a future – async and await](../Images/image00450.jpeg)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
- en: Output of program doing fetching and response processing of 5 URLs asynchronously
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: A lot of complex programming can be done using the `asyncio` module. One can
    wait for futures, cancel their execution, and run `asyncio` operations from multiple
    threads. A full discussion is beyond the scope of this chapter.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: We will move on to another model for executing concurrent tasks in Python, namely
    the `concurrent.futures` module.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent futures – high-level concurrent processing
  id: totrans-511
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `concurrent.futures` module provides high-level concurrent processing using
    either threads or processes, while asynchronously returning data using future
    objects.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: 'It provides an executor interface which exposes mainly two methods, which are
    as follows:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '`submit`: Submits a callable to be executed asynchronously, returning a `future`
    object representing the execution of the callable.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map`: Maps a callable to a set of iterables, scheduling the execution asynchronously
    in the `future` object. However, this method returns the results of processing
    directly instead of returning a list of futures.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two concrete implementations of the executor interface: `ThreadPoolExecutor`
    executes the callable in a pool of threads, and `ProcessPoolExecutor` does so
    in a pool of processes.'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple example of a `future` object that calculates the factorial
    of a set of integers asynchronously:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following is a detailed explanation of the preceding code:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: The `factorial` function computes the factorial of a given number iteratively
    by using `functools.reduce` and the multiplication operator
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We create an executor with two workers, and submit the numbers (from 10 to 20)
    to it via its `submit` method
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The submission is done via a dictionary comprehension, returning a dictionary
    with the future as the key and the number as the value
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We iterate through the completed futures, which have been computed, using the
    `as_completed` method of the `concurrent.futures` module
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result is printed by fetching the future's result via the `result` method
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When executed, the program prints its output, rather in order, as shown in
    the next screenshot:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: '![Concurrent futures – high-level concurrent processing](../Images/image00451.jpeg)'
  id: totrans-526
  prefs: []
  type: TYPE_IMG
- en: Output of concurrent futures factorial program
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: Disk thumbnail generator
  id: totrans-528
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our earlier discussion of threads, we used the example of the generation
    of thumbnails for random images from the Web to demonstrate how to work with threads,
    and process information.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will do something similar. Here, rather than processing
    random image URLs from the Web, we will load images from disk, and convert them
    to thumbnails using the `concurrent.futures` function.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: We will reuse our thumbnail creation function from before. On top of that, we
    will add concurrent processing.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: 'First, here are the imports:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Here is our familiar thumbnail creation function:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We will process images from a specific folder—in this case, the `Pictures`
    subdirectory of the `home` folder. To process this, we will need an iterator that
    yields image filenames. We have written one next with the help of the `os.walk`
    function:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As you can see, the preceding function is a generator.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the main calling code, which sets up an executor and runs it over the
    folder:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The preceding code uses the same technique of submitting arguments to a function
    asynchronously, saving the resultant futures in a dictionary and then processing
    the result as and when the futures are finished, in a loop.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: To change the executor to use processes, one simply needs to replace `ThreadPoolExecutor`
    with `ProcessPoolExecutor`; the rest of the code remains the same. We have provided
    a simple command-line flag, `--process`, to make this easy.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: Here is an output of a sample run of the program using both thread and process
    pools on the `~/Pictures` folder – generating around 2000+ images in roughly the
    same time.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: '![Disk thumbnail generator](../Images/image00452.jpeg)'
  id: totrans-544
  prefs: []
  type: TYPE_IMG
- en: Output of concurrent futures disk thumbnail program—using thread and process
    executor
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency options – how to choose?
  id: totrans-546
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are at the end of our discussion of concurrency techniques in Python. We
    discussed threads, processes, asynchronous I/O, and concurrent futures. Naturally,
    a question arises—when to pick what?
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: This question has been already answered for the choice between threads and processes,
    where the decision is mostly influenced by the GIL.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: Here are somewhat rough guidelines for picking your concurrency options.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: '**Concurrent futures vs Multi-processing:** Concurrent futures provide an elegant
    way to parallelize your tasks using either a thread or process pool executor.
    Hence, it is ideal if the underlying application has similar scalability metrics
    with either threads or processes, since it''s very easy to switch from one to
    the other as we''ve seen in a previous example. Concurrent futures can be chosen
    also when the result of the operation needn''t be immediately available. Concurrent
    futures is a good option when the data can be finely parallelized and the operation
    can be executed asynchronously, and when the operations involve simple callables
    without requiring complex synchronization techniques.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-processing should be chosen if the concurrent execution is more complex,
    and not just based on data parallelism, but has aspects like synchronization,
    shared memory, and so on. For example, if the program requires processes, synchronization
    primitives, and IPC, the only way to truly scale up then is to write a concurrent
    program using the primitives provided by the multiprocessing module.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: Similarly when your muti-threaded logic involves simple parallelization of data
    across multiple tasks, one can choose concurrent futures with a thread pool. However
    if there is a lot of shared state to be managed with complex thread synchronization
    objects – one has to use thread objects and switch to multiple threads using `threading`
    module to get finer control of the state.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '**Asynchronous I/O vs Threaded concurrency:** When your program doesn''t need
    true concurrency (parallelism), but is dependent more on asynchronous processing
    and callbacks, then `asyncio` is the way to go. Asyncio is a good choice when
    there are lot of waits or sleep cycles involved in the application, such as waiting
    for user input, waiting for I/O, and so on, and one needs to take advantage of
    such wait or sleep times by yielding to other tasks via co-routines. Asyncio is
    not suitable for CPU-heavy concurrent processing, or for tasks involving true
    data parallelism.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AsyncIO seems to be suitable for request-response loops- where a lot of I/O
    happens - so its good for writing web application servers which doesn't have real-time
    data requirements.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: You can use these points just listed as rough guidelines when deciding on the
    correct concurrency package for your applications.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: Parallel processing libraries
  id: totrans-556
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apart from the standard library modules that we've discussed so far, Python
    is also rich in its ecosystem of third-party libraries, which support parallel
    processing in a **symmetric multi-processing** (**SMP**) or multi-core systems.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: We will take a look at a couple of such packages, that are somewhat distinct
    and present some interesting features.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: Joblib
  id: totrans-559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`joblib` is a package that provides a wrapper over multiprocessing to execute
    code in loops in parallel. The code is written as a generator expression, and
    interpreted to execute in parallel over CPU cores using multi-processing module
    behind the scenes.'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, take the following code which calculates square roots for first
    10 numbers:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-562
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This preceding code can be converted to run on two CPU cores by the following:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here is another example: this is our primality checker that we had written
    earlier to run using multiprocessing rewritten to use the `joblib` package:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-566
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: If you execute and time the preceding code, you will find the performance metrics
    very similar to that of the version using multi-processing.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: PyMP
  id: totrans-568
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`OpenMP` is an open API, which supports shared memory multi-processing in C/C++
    and Fortran. It uses special work-sharing constructs such as pragmas (special
    instructions to compilers) indicating how to split work among threads or processes.'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following C code using the `OpenMP` API indicates that the
    array should be initialized in parallel using multiple threads:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-571
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`PyMP` is inspired by the idea behind `OpenMP`, but uses the `fork` system
    call to parallelize code executing in expressions like for loops across processes.
    For this, `PyMP` also provides support for shared data structures like lists and
    dictionaries, and also provides a wrapper for `numpy` arrays.'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: We will look at an interesting and exotic example—that of fractals—to illustrate
    how `PyMP` can be used to parallelize code and obtain performance improvement.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-574
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NOTE: The PyPI package for PyMP is named pymp-pypi so make sure you use this
    name when trying to install it via pip. Also note that it doesn''t do a good job
    of pulling its dependencies such as numpy, so these have to be installed separately.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: Fractals – the Mandelbrot set
  id: totrans-576
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is the code listing of a very popular class of complex numbers,
    which when plotted, produces very interesting fractal geometries: namely, the
    **Mandelbrot set**:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The preceding code calculates a Mandelbrot set using a certain number of `c`
    and a variable geometry (*width x height*). It is complete with argument parsing
    to produce fractal images of varying geometries, and supports different iterations.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-580
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For simplicity's sake, and for producing rather beautiful pics than what Mandelbrot
    usually does, the code takes some liberties, and uses the color scheme of a related
    fractal class, namely, Julia sets.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: How does it work ? Here is an explanation of the code .
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
- en: The `mandelbrot_calc_row` function calculates a row of the Mandelbrot set for
    a certain value of the *y* coordinate for a certain number of maximum iterations.
    The pixel color values for the entire row, from `0` to width `w` for the *x* coordinate,
    is calculated. The pixel values are put into the `Image` object that is passed
    to this function.
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `mandelbrot_calc_set` function calls the `mandelbrot_calc_row` function
    for all values of the *y* coordinate ranging from `0` to the height `h` of the
    image. An `Image` object (via the **Pillow library**) is created for the given
    geometry (*width x height*), and filled with pixel values. Finally, we save this
    image to a file, and we've got our fractal!
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Without further ado, let us see the code in action.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: Here is the image that our Mandelbrot program produces for the default number
    of iterations namely 1000.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: '![Fractals – the Mandelbrot set](../Images/image00453.jpeg)'
  id: totrans-587
  prefs: []
  type: TYPE_IMG
- en: Mandelbrot set fractal image for 1000 iterations
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: Here is the time it takes to create this image.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: '![Fractals – the Mandelbrot set](../Images/image00454.jpeg)'
  id: totrans-590
  prefs: []
  type: TYPE_IMG
- en: Timing of single process Mandelbrot program—for 1000 iterations
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: 'However if you increase the number of iterations – the single process version
    slows down quite a bit. Here is the output when we increase the number of iterations
    by 10X – for 10000 iterations.:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: '![Fractals – the Mandelbrot set](../Images/image00455.jpeg)'
  id: totrans-593
  prefs: []
  type: TYPE_IMG
- en: Timing of single process Mandelbrot program—for 10,000 iterations
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the code, we can see that there is an outer for loop in the `mandelbrot_calc_set`
    function, which sets things in motion. It calls `mandelbrot_calc_row` for each
    row of the image ranging from `0` to the height of the function, varied by the
    *y* coordinate.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: Since each invocation of the `mandelbrot_calc_row` function calculates one row
    of the image, it naturally fits into a data parallel problem, and can be parallelized
    sufficiently easily.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how to do this using PyMP.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: Fractals – Scaling the Mandelbrot set implementation
  id: totrans-598
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use `PyMP` to parallelize the outer for loop across many processes in
    a rewrite of the previous simple implementation of the Mandelbrot set, to take
    advantage of the inherent data parallelism in the solution.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: Here is the `PyMP` version of the two functions of the mandelbrot program. The
    rest of the code remains the same.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The rewrite mainly involved converting the code to one that builds the mandelbrot
    image line by line, each line of data being computed separately and in a way that
    it can be computed in parallel – in a separate process.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: In the single process version, we put the pixel values directly in the image
    in the `mandelbrot_calc_row` function. However, since the new code executes this
    function in parallel processes, we cannot modify the image data in it directly.
    Instead, the new code passes a shared dictionary to the function, and it sets
    the pixel color values in it using the location as `key` and the pixel RGB value
    as `value`.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new shared data structure—a shared dictionary—is hence added to the `mandelbrot_calc_set`
    function, which is finally iterated over, and the pixel data, filled, in the `Image`
    object, which is then saved to the final output.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use four `PyMP` parallel processes, as the machine has four CPU cores, using
    a with context and enclosing the outer for loop inside it. This causes the code
    to execute in parallel in four cores, each core calculating approximately 25%
    of the rows. The final data is written to the image in the main process.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the result timing of the `PyMP` version of the code:'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: '![Fractals – Scaling the Mandelbrot set implementation](../Images/image00456.jpeg)'
  id: totrans-607
  prefs: []
  type: TYPE_IMG
- en: Timing of parallel process mandelbrot program using PyMP—for 10000 iterations
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: The program is about 33% faster in real time. In terms of CPU usage, you can
    see that the `PyMP` version has a higher ratio of user CPU time to real CPU time,
    indicating a higher usage of the CPU by the processes than the single process
    version.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-610
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NOTE: We can write an even more efficient version of the program by avoiding
    the shared data structure image_rows which is used to keep the pixel values of
    the image. This version however uses that to show the features of PyMP. The code
    archives of this book contain two more versions of the program – one that uses
    multiprocessing and another that uses PyMP without the shared dictionary.'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the output fractal image produced by this run of the program:'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: '![Fractals – Scaling the Mandelbrot set implementation](../Images/image00457.jpeg)'
  id: totrans-613
  prefs: []
  type: TYPE_IMG
- en: Mandelbrot set fractal image for 10000 iterations using PyMP
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: You can observe that the colors are different, and this image provides more
    detail and a finer structure than the previous one due to the increased number
    of iterations.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: Scaling for the Web
  id: totrans-616
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, all the scalability and concurrency techniques we discussed were involved
    with scalability within the confines of a single server or machine—in other words,
    scaling up. In real world, applications also scale by scaling out, that is, by
    spreading their computation over multiple machines. This is how most real-world
    web applications run and scale at present.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: We will look at a few techniques, scaling out an application in terms of scaling
    communications/workflows, scaling computation, and horizontal scaling using different
    protocols.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: Scaling workflows – message queues and task queues
  id: totrans-619
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One important aspect of scalability is to reducing coupling between systems.
    When two systems are tightly coupled, they prevent each other from scaling beyond
    a certain limit.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: For example, a code written serially, where data and computation is tied into
    the same function, prevents the program from taking advantage of the existing
    resources like multiple CPU cores. When the same program is rewritten to use multiple
    threads (or processes) and a message passing system like a queue in between, we
    find it scales well to multiple CPUs. We've seen such examples aplenty in our
    concurrency discussion.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: In a much similar way, systems over the Web scale better when they are decoupled.
    The classic example is the client/server architecture of the Web itself, where
    clients interact via well-known RestFUL protocols like HTTP, with servers located
    in different places across the world.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: Message queues are systems that allow applications to communicate in a decoupled
    manner by sending messages to each other. The applications typically run in different
    machines or servers connected to the Internet, and communicate via queuing protocols.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: One can think of a message queue as a scaled-up version of the multi-threaded
    synchronized queue, with applications on different machines replacing the threads,
    and a shared, distributed queue replacing the simple in-process queue.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: Message queues carry packets of data called messages, which are delivered from
    the **Sending Applications** to the **Receiving Applications**. Most **Message
    Queue** provide **store and forward** semantics, where the message is stored on
    the queue till the receiver is available to process the message.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple schematic model of a **Message Queue**:'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling workflows – message queues and task queues](../Images/image00458.jpeg)'
  id: totrans-627
  prefs: []
  type: TYPE_IMG
- en: Schematic model of a distributed message queue
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
- en: The most popular and standardized implementation of a message queue or **message-oriented
    middleware** (**MoM**) is the **Advanced Message Queuing Protocol** (**AMQP**).
    AMQP provides features such as queuing, routing, reliable delivery, and security.
    The origins of AMQP are in the financial industry, where reliable and secure message
    delivery semantics are of critical importance.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: The most popular implementations of AMQP (version 1.0) are Apache Active MQ,
    RabbitMQ, and Apache Qpid.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ is a MoM written in Erlang. It provides libraries in many languages
    including Python. In RabbitMQ, a message is always delivered via exchanges via
    routing keys which indicate the queues to which the message should be delivered.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: We won't be discussing RabbitMQ in this section anymore, but will move on to
    a related, but slightly different, middleware with a varying focus, namely, Celery.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
- en: Celery – a distributed task queue
  id: totrans-633
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Celery is a distributed task queue written in Python, which works using distributed
    messages. Each execution unit in celery is called a **task**. A task can be executed
    concurrently on one or more servers using processes called **workers**. By default,
    celery achieves this using `multiprocessing`, but it can also use other backend
    such as gevent, for example.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
- en: Tasks can be executed synchronously or asynchronously with results available
    in the future, like objects. Also, task results can be stored in storage backend
    such as Redis, databases, or in files.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
- en: Celery differs from message queues in that the basic unit in celery is an executable
    task—a callable in Python—rather than just a message.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: Celery, however, can be made to work with message queues. In fact, the default
    broker for passing messages in celery is RabbitMQ, the popular implementation
    of AMQP. Celery can also work with Redis as the broker backend.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: Since Celery takes a task, and scales it over multiple workers; over multiple
    servers, it is suited to problems involving data parallelism as well as computational
    scaling. Celery can accept messages from a queue and distribute it over multiple
    machines as tasks for implementing a distributed e-mail delivery system, for example,
    and achieve horizontal scalability. Or, it can take a single function and perform
    parallel data computation by splitting the data over multiple processes, achieving
    parallel data processing.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will take our Mandelbrot fractal program and, rewrite
    it to work with Celery. We will try to scale the program by performing data parallelism,
    in terms of computing the rows of the Mandelbrot set over multiple celery workers—in
    a similar way to what we did with `PyMP`.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: The Mandelbrot set using Celery
  id: totrans-640
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For implementing a program to take advantage of Celery, it needs to be implemented
    as a task. This is not as difficult as it sounds. Mostly, it just involves preparing
    an instance of the celery app with a chosen broker backend, and decorating the
    callable we want to parallelize – using the special decorator `@app.task` where
    *app* is an instance of Celery.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look at this program listing step by step, since it involves a few
    new things. The software requirements for this session are as follows:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: Celery
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AMQP backend; RabbitMQ is preferred
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redis as a result storage backend
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First we will provide the listing for the Mandelbrot tasks module:'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-647
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let us analyze this preceding code:'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: We first do the imports required for celery. This requires importing the `Celery`
    class from the `celery` module.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We prepare an instance of the `Celery` class as the celery app using AMQP as
    the message broker and Redis as the result backend. The AMQP configuration will
    use whatever AMQP MoM is available on the system (in this case, it is RabbitMQ).
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a modified version of `mandelbrot_calc_row`. In the `PyMP` version,
    the `image_rows` dictionary was passed as an argument to the function. Here, the
    function calculates it locally and returns a value. We will use this return value
    at the receiving side to create our image.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We decorated the function using `@app.task`, where app is the `Celery` instance.
    This makes it ready to be executed as a celery task by the celery workers.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next is the main program, which calls the task for a range of `y` input values
    and creates the image:'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-654
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The argument parser is the same so is not reproduced here.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: 'This last bit of code introduces some new concepts in celery, so needs some
    explanation. Let us analyze the code in some detail:'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: The `mandelbrot_main` function is similar to the previous `mandelbrot_calc_set`
    function in its arguments.
  id: totrans-657
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This function sets up a group of tasks, each performing `mandelbrot_calc_row`
    execution on a given `y` input over the entire range of `y` inputs from `0` to
    the height of the image. It uses the `group` object of celery to do this. A group
    is a set of tasks which can be executed together.
  id: totrans-658
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The tasks are executed by calling the `apply_async` function on the group. This
    executes the tasks asynchronously in the background in multiple workers. We get
    an async `result` object in return—the tasks are not completed yet.
  id: totrans-659
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then wait on this result object by calling `join` on it, which returns the
    results—the rows of the image as a dictionary from each single execution of the
    `mandelbrot_calc_row` task. We loop through this, and do integer conversions for
    the values, since celery returns data as strings, and put the pixel values in
    the image.
  id: totrans-660
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the image is saved in the output file.
  id: totrans-661
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So how does celery execute the tasks? This needs the celery program to run,
    processing the tasks module with a certain number of workers. Here is how we start
    it in this case:'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mandelbrot set using Celery](../Images/image00459.jpeg)'
  id: totrans-663
  prefs: []
  type: TYPE_IMG
- en: Celery console—workers starting up with the Mandelbrot task as target
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: The command starts celery with tasks loaded from the module `mandelbrot_tasks.py`
    with a set of 4 worker processes. Since the machine has 4 CPU cores, we have chosen
    this as the concurrency.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-666
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that Celery will automatically default the workers to the number of cores
    if not specifically configured.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: The program ran under 15 seconds, twice as faster than the single-process version
    and also the `PyMP` version.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: 'If you observe the celery console, you will find a lot of messages getting
    echoed, since we configured celery with the `INFO` log level. All these are info
    messages with data on the tasks and their results:'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the result of the run for `10000` iterations.
    This performance is slightly better than that of the similar run by the `PyMP`
    version earlier, by around 20 seconds:'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mandelbrot set using Celery](../Images/image00460.jpeg)'
  id: totrans-671
  prefs: []
  type: TYPE_IMG
- en: Celery Mandelbrot program for a set of 10000 iterations.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: Celery is used in production systems in many organizations. It has plugins for
    some of the more popular Python web application frameworks. For example, celery
    supports Django out-of-the-box with some basic plumbing and configuration. There
    are also extension modules such as `django-celery-results`, which allow the programmer
    to use the Django ORM as celery results backend.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
- en: It is beyond the scope of this chapter and book to discuss this in detail so
    the reader is suggested to refer to the documentation available on this on the
    celery project website.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
- en: Serving with Python on the Web—WSGI
  id: totrans-675
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Web Server Gateway Interface** (**WSGI**) is a specification for a standard
    interface between Python web application frameworks and web servers.'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: In the early days of Python web applications, there was a problem of connecting
    web application frameworks to web servers, since there was no common standard.
    Python web applications were designed to work with one of the existing standards
    of CGI, FastCGI, or `mod_python` (Apache). This meant that an application written
    to work with one web server might not be able to work with another. In other words,
    interoperability between the uniform application and web server was missing.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: WSGI solved this problem by specifying a simple, but uniform, interface between
    servers and web application frameworks to allow for portable web application development.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
- en: 'WSGI specifies two sides: the server (or gateway) side, and the application
    or framework side. A WSGI request gets processed as follows:'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: The server side executes the application, providing it with an environment and
    a callback function
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application processes the request, and returns the response to the server
    using the provided callback function
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a schematic diagram showing the interaction between a web server and
    web application using WSGI:'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
- en: '![Serving with Python on the Web—WSGI](../Images/image00461.jpeg)'
  id: totrans-683
  prefs: []
  type: TYPE_IMG
- en: Schematic diagram showing WSGI protocol interaction
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the simplest function that is compatible with the application
    or framework side of WSGI is:'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-686
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The preceding function can be explained as follows:'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: The `environ` variable is a dictionary of environment variables passed from
    the server to the application as defined by the **Common Gateway Interface** (**CGI**)
    specification. WSGI makes a few of these environment variables mandatory in its
    specification.
  id: totrans-688
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `start_response` is a callable provided as a callback from the server side
    to the application side to start response processing on the server side. It must
    take two positional arguments. The first should be a status string with an integer
    status code, and the second, a list of (`header_name`, `header_value`), tuples
    describing the HTTP response header.
  id: totrans-689
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-690
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more details, the reader can refer to the WSGI specification v1.0.1, which
    is published on the Python language website as PEP 3333.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-692
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Python Enhancement Proposal** (**PEP**) is a design document on the Web,
    that describes a new feature or feature suggestion for Python, or provides information
    to the Python community about an existing feature. The Python community uses PEPs
    as a standard process for describing, discussing, and adopting new features and
    enhancements to the Python programming language and its standard library.'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
- en: 'WSGI middleware components are software that implement both sides of the specification,
    and hence, provide capabilities such as the following:'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing of multiple requests from a server to an application
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remote processing of requests by forwarding requests and responses over a network
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-tenancy or co-hosting of multiple servers and/or applications in the same
    process
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL-based routing of requests to different application objects
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The middleware sits in between the server and application. It forwards requests
    from server to the application and responses from application to the server.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of WSGI middleware an architect can choose from. We will
    briefly look at two of the most popular ones, namely, uWSGI and Gunicorn.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
- en: uWSGI – WSGI middleware on steroids
  id: totrans-701
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: uWSGI is an open source project and application, which aims to build a full
    stack for hosting services. The WSGI of the uWSGI project stems from the fact
    that the WSGI interface plugin for Python was the first one developed in the project.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: Apart from WSGI, the uWSGI project also supports **Perl Webserver Gateway Interface**
    (**PSGI**) for Perl web applications, and the rack web server interface for Ruby
    web applications. It also provides gateways, load balancers, and routers for requests
    and responses. The Emperor plugin of uWSGI provides management and monitoring
    of multiple uWSGI deployments of your production system across servers.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
- en: The components of uWSGI can run in preforked, threaded, asynchronous. or green-thread/co-routine
    modes.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: uWSGI also comes with a fast and in-memory caching framework, which allows the
    responses of the web applications to be stored in multiple caches on the uWSGI
    server. The cache can also be backed with a persistence store such as a file.
    Apart from a multitude of other things, uWSGI also supports virtualenv based deployments
    in Python.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
- en: uWSGI also provides a native protocol, that is used by the uWSGI server. uWSGI
    version 1.9 also adds native support for the web sockets.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a typical example of a uWSGI configuration file:'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-708
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'A typical deployment architecture with uWSGI looks like what is depicted in
    the following diagram. In this case, the web server is Nginx, and the web application
    framework is Django. uWSGI is deployed in a reverse-proxy configuration with Nginx,
    forwarding request and responses between Nginx and Django:'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: '![uWSGI – WSGI middleware on steroids](../Images/image00462.jpeg)'
  id: totrans-710
  prefs: []
  type: TYPE_IMG
- en: uWSGI deployment with Nginx and Django
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-712
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Nginx web server supports a native implementation of the uWSGI protocol
    since version 0.8.40\. There is also a proxy module support for uWSGI in Apache
    named `mod_proxy_uwsgi`.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
- en: uWSGI is an ideal choice for Python web application production deployments where
    one needs a good balance of customization with high performance and features.
    It is the swiss-army-knife of components for WSGI web application deployments.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
- en: Gunicorn – unicorn for WSGI
  id: totrans-715
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Gunicorn project is another popular WSGI middleware implementation, which
    is opensource. It uses a preforked model, and is a ported version from the unicorn
    project of Ruby. There are different worker types in Gunicorn, like uWSGI supporting
    synchronous and asynchronous handling of requests. The asynchronous workers make
    use of the `Greenlet` library which is built on top of gevent.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: There is a master process in Gunicorn that runs an event loop, processing and
    reacting to various signals. The master manages the workers, and the workers process
    the requests, and send responses.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
- en: Gunicorn versus uWSGI
  id: totrans-718
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few guidelines when choosing whether to go with Gunicorn or uWSGI
    for your Python web application deployments:'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: For simple application deployments which don't need a lot of customization,
    gunicorn is a good choice. uWSGI has a bigger learning curve when compared to
    Gunicorn, and takes a while to get used to. The defaults in Gunicorn work pretty
    well for most deployments.
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your deployment is homogenously Python, then Gunicorn is a good choice. On
    the other hand, uWSGI allows you to perform heterogeneous deployments due to its
    support for other stacks such as PSGI and Rack.
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want a more full-featured WSGI middleware, which is heavily customizable,
    then uWSGI is a safe bet. For example, uWSGI makes Python virtualenv-based deployments
    simple, whereas, Gunicorn doesn't natively support virtualenv; instead, Gunicorn
    itself has to be deployed in the virtual environment.
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since Nginx supports uWSGI natively, it is very commonly deployed along with
    Nginx on production systems. Hence, if you use Nginx, and want a full-featured
    and highly customizable WSGI middleware with caching, uWSGI is the default choice.
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With respect to performance, both Gunicorn and uWSGI score similarly on different
    benchmarks published on the Web.
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability architectures
  id: totrans-725
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed, a system can scale vertically, or horizontally, or both. In this
    section, we will briefly look at a few of the architectures that an architect
    can choose from when deploying his systems to production to take advantage of
    the scalability options.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
- en: Vertical scalability architectures
  id: totrans-727
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Vertical scalability techniques comes in the following two flavors:'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding more resources to an existing system**: This could mean adding more
    RAM to a physical or virtual machine, adding more vCPUs to a virtual machine or
    VPS, and so on. However, none of these options are dynamic, as they require stopping,
    reconfiguring, and restarting the instance.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Making better use of existing resources in the system**: We have spent a
    lot of this chapter discussing this approach. This is when an application is rewritten
    to make use of the existing resources, such as multiple CPU cores, more effectively
    by concurrency techniques such as threading, multiple processes, and/or asynchronous
    processing. This approach scales dynamically, since no new resource is added to
    the system, and hence, there is no need for a stop/start.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal scalability architectures
  id: totrans-731
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Horizontal scalability involves a number of techniques that an architect can
    add to his tool box, and pick and choose from. They include the ones listed next:'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
- en: '**Active redundancy**: This is the simplest technique of scaling out, which
    involves adding multiple, homogenous processing nodes to a system typically fronted
    with a load balancer. This is a common practice for scaling out web application
    server deployments. Multiple nodes make sure that an even if one or a few of the
    systems fail, the remaining systems continue to carry out request processing,
    ensuring no downtime for your application.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a redundant system, all the nodes are actively in operation, though only
    one or a few of them may be responding to requests at a specific time.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
- en: '**Hot standby**: A hot standby (hot spare) is a technique used to switch to
    a system that is ready to server requests, but is not active till the moment the
    main system go down. A hot spare is in many ways exactly similar to the main node(s)
    that is serving the application. In the event of a critical failure, the load
    balancer is configured to switch to the hot spare.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hot spare itself may be a set of redundant nodes instead of just a single
    node. Combining redundant systems with a hot spare ensures maximum reliability
    and failover.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-737
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A variation of a hot standby is a software standby, which provides a mode in
    the application that switches the system to a minimum **Quality of Service** (**QoS**)
    instead of offering the full feature at extreme load. An example is a web application
    that switches to the read-only mode under high loads, serving most users but not
    allowing writes.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
- en: '**Read replicas**: The response of a system that is dependent on read-heavy
    operations on a database can be improved by adding read-replicas of the database.
    Read replicas are essentially database nodes that provide hot backups (online
    backups), which constantly sync from the main database node. Read replicas, at
    a given point of time, may not be exactly consistent with the main database node,
    but they provide eventual consistency with SLA guarantees.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud service providers such as Amazon make their RDS database service available
    with a choice of read replicas. Such replicas can be distributed geographically
    closer to your active user locations to ensure less response time and failover
    in case the master node goes down, or doesn't respond.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
- en: Read replicas basically offer your system a kind of data redundancy.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
- en: '**Blue-green deployments**: This is a technique where two separate systems
    (labeled `blue` and `green` in the literature) are run side by side. At any given
    moment, only one of the systems is active and is serving requests. For example,
    blue is *active*, green is *idle*.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When preparing a new deployment, it is done on the idle system. Once the system
    is ready, the load balancer is switched to the idle system (green), and away from
    the active system (blue). At this point, green is active, and blue is idle. The
    positions are reversed again in the next switch.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green deployments, if done correctly, ensure zero to minimum downtime of
    your production applications.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
- en: '**Failure monitoring and/or restart**: A failure monitor is a system that detects
    failure of critical components—software or hardware—of your deployments, and either
    notifies you, and/or takes steps to mitigate the downtime.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, you can install a monitoring application on your server that detects
    when a critical component, say, a celery or rabbitmq server, goes down, sends
    an e-mail to the DevOps contact, and also tries to restart the daemon.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
- en: Heartbeat monitoring is another technique where a software actively sends pings
    or heartbeats to a monitoring software or hardware, which could be in the same
    machine or another server. The monitor will detect the downtime of the system
    if it fails to send the heartbeat after a certain interval, and could then inform
    and/or try to restart the component.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
- en: Nagios is an example of a common production monitoring server, usually deployed
    in a separate environment, and monitors your deployment servers. Other examples
    of system-switch monitors and restart components are **Monit** and **Supervisord**.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from these techniques, the following best practices should be followed
    when performing system deployments to ensure scalability, availability, and redundancy/failover:'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
- en: '**Cache it**: Use caches, and if possible, distributed caches, in your system
    as much as possible. Caches can be of various types. The simplest possible cache
    is caching static resources on the **content delivery network** (**CDN**) of your
    application service provider. Such a cache ensures geographic distribution of
    resources closer to your users, which reduces response, and hence, page-load times.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A second kind of cache is your application's cache, where it caches responses
    and database query results. Memcached and Redis are commonly used for these scenarios,
    and they provide distributed deployments, typically, in master/slave modes. Such
    caches should be used to load and cache most commonly requested content from your
    application with proper expiry times to ensure that the data is not too stale.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
- en: 'Effective and well-designed caches minimize system load, and avoid multiple,
    redundant operations that can artificially increase load on a system and decrease
    performance:'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
- en: '**Decouple**: As much as possible, decouple your components to take advantage
    of the shared geography of your network. For example, a message queue may be used
    to decouple components in an application that need to publish and subscribe data
    instead of using a local database or sockets in the same machine. When you decouple,
    you automatically introduce redundancy and data backup to your system, since the
    new components you add for decoupling—message queues, task queues, and distributed
    caches—typically come with their own stateful storage and clustering.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The added complexity of decoupling is the configuration of the extra systems.
    However, in this day and age, with most systems being able to perform auto configuration
    or providing simple web-based configurations, this is not an issue.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to literature for application architectures that provide effective
    decoupling, such as observer patterns, mediators, and other such middleware:'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
- en: '**Gracefully degrade**: Rather than being unable to answer a request and providing
    timeouts, arm your systems with graceful degradation behaviors. For example, a
    write-heavy web application can switch to the read-only mode under heavy load
    when it finds that the database node is not responding. Another example is when
    a system which provides heavy, JS-dependent dynamic web pages could switch to
    a similar static page under heavy loads on the server when the JS middleware is
    not responding well.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graceful degradation can be configured on the application itself, or on the
    load balancers, or both. It is a good idea to prepare your application itself
    to provide a gracefully downgraded behavior, and configure the load balancer to
    switch to that route under heavy loads.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
- en: '**Keep data close to the code:** A golden rule of performance-strong software
    is to provide data closer to where the computation is. For example, if your application
    is making 50 SQL queries to load data from a remote database for every request,
    then you are not doing this correctly.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing data close to the computation reduces data access and transport times,
    and hence, processing times, decreasing latency in your application, and making
    it more scalable.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different techniques for this: caching, as discussed earlier, is
    a favored technique. Another one is to split your database to a local and remote
    one, where most of the reads happen from the local read replica, and writes (which
    can take time) happen to a remote write master. Note that local in this sense
    may not mean the same machine, but typically, the same data center, sharing the
    same subnet if possible.'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
- en: Also, common configurations can be loaded from an on-disk database like SQLite
    or local JSON files, reducing the time it takes for preparing the application
    instances.
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
- en: Another technique is to not store any transactional state in the application
    tier or the frontend, but to move the state closer to the backend where the computation
    is. Since this makes all application server nodes equal in terms of not having
    any intermediate state, it also allows you to front them with a load-balancer,
    and provide a redundant cluster of equals, any of which can serve a given request.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
- en: '**Design according to SLAs**: It is very important for an architect to understand
    the guarantees that the application provides to its users, and design the deployment
    architecture accordingly.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CAP theorem ensures that if a network partition in a distributed system
    fails, the system can guarantee only one of consistency or availability at a given
    time. This groups distributed systems into two common types, namely, CP and AP
    systems.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
- en: Most web applications in today's world are AP. They ensure availability, but
    data is only eventually consistent, which means they will serve stale data to
    users in case one of the systems in the network partition, say the master DB node,
    fails.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand. a number of businesses such as banking, finance, and healthcare
    need to ensure consistent data even if there is a network partition failure. These
    are CP systems. The data in such systems should never be stale, so, in case of
    a choice between availability and consistent data, they will chose the latter.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
- en: The choice of software components, application architecture, and the final deployment
    architecture are influenced by these constraints. For example, an AP system can
    work with NoSQL databases which guarantee eventual consistent behavior. It can
    make better use of caches. A CP system, on the other hand, may need ACID guarantees
    provided by **Relational Database Systems** (**RDBMs**).
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-768
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reused a lot of ideas and concepts that you learned in the
    previous chapter on performance.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
- en: We started with a definition of scalability, and looked at its relation with
    other aspects like concurrency, latency, and performance. We briefly compared
    and contrasted concurrency and its close cousin parallelism.
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
- en: We then went on to discuss various concurrency techniques in Python with detailed
    examples and performance comparisons. We used a thumbnail generator with random
    URLs from the Web as an example to illustrate the various techniques of implementing
    concurrency using multi-threading in Python. You also learned and saw an example
    of the producer/consumer pattern, and using a couple of examples, learned how
    to implement resource constraints and limits using synchronization primitives.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
- en: Next we discussed how to scale applications using multi-processing and saw a
    couple of examples using the `multiprocessing` module – such as a primality checker
    which showed us the effects of `GIL` on multiple threads in Python and a disk
    file sorting program which showed the limits of multi-processing when it comes
    to scaling programs using a lot disk I/O .
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
- en: We looked at asynchronous processing as the next technique of concurrency. We
    saw a generator based co-operative multitasking scheduler and also its counterpart
    using `asyncio`. We saw couple of examples using asyncio and learned how to perform
    URL fetches using the aiohttp module asynchronously. The section on concurrent
    processing compared and contrasted concurrent futures with other options on concurrency
    in Python while sketching out a couple of examples.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
- en: We used Mandelbrot fractals as an example to show how to implement data parallel
    programs and showed an example of using `PyMP` to scale a mandelbrot fractal program
    across multiple processes and hence multiple cores.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
- en: Next we went on to discuss how to scale your programs out on the Web. We briefly
    discussed the theoretical aspect of message queues and task queues. We looked
    at celery, the Python task queue library, and rewrote the Mandelbrot program to
    scale using celery workers, and did performance comparisons.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
- en: WSGI, Python's way of serving web applications over web servers, was the next
    topic of discussion. We discussed the WSGI specification, and compared and contrasted
    two popular WSGI middleware, namely, uWSGI and Gunicorn.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
- en: Towards the end of the chapter, we discussed scalability architectures, and
    looked at the different options of scaling vertically and horizontally on the
    Web. We also discussed at some best practices an architect should follow while
    designing, implementing, and deploying distributed applications on the web for
    achieving high scalability.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we discuss the aspect of Security in software architecture
    and discuss aspects of security the architect should be aware of and strategies
    for making your applications secure.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
