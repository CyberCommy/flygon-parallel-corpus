- en: Chapter 5. Writing Applications That Scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine the checkout counter of a supermarket on a Saturday evening, the usual
    rush-hour time. It is common to see long queues of people waiting to check out
    with their purchases. What could a store manager do to reduce the rush and waiting
    time?
  prefs: []
  type: TYPE_NORMAL
- en: A typical manager would try a few approaches, including telling those manning
    the checkout counters to pick up their speed, and to try and redistribute people
    to different queues so that each queue roughly has the same waiting time. In other
    words, he would manage the current load with available resources by *optimizing
    the performance* of the existing resources.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the store has existing counters that are not in operation—and enough
    people at hand to manage them—the manager could enable those counters, and move
    people to these new counters. In other words, he would add resources to the store
    to *scale* the operation.
  prefs: []
  type: TYPE_NORMAL
- en: Software systems too scale in a similar way. An existing software application
    can be scaled by adding compute resources to it.
  prefs: []
  type: TYPE_NORMAL
- en: When the system scales by either adding or making better use of resources inside
    a compute node, such as CPU or RAM, it is said to *scale vertically* or *scale
    up*. On the other hand, when a system scales by adding more compute nodes to it,
    such as a creating a load-balanced cluster of servers, it is said to *scale horizontally*
    or *scale out*.
  prefs: []
  type: TYPE_NORMAL
- en: The degree to which a software system is able to scale when compute resources
    are added is called its *scalability*. Scalability is measured in terms of how
    much the system's performance characteristics, such as throughput or latency,
    improve with respect to the addition of resources. For example, if a system doubles
    its capacity by doubling the number of servers, it is scaling linearly.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the concurrency of a system often increases its scalability. In the
    supermarket example given earlier, the manager is able to scale out his operations
    by opening additional counters. In other words, he increases the amount of concurrent
    processing done in his store. Concurrency is the amount of work that gets done
    simultaneously in a system.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we look at the different techniques of scaling a software application
    with Python.
  prefs: []
  type: TYPE_NORMAL
- en: We will be following the approximate sketch of the topics below in our discussion
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability and Performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency and Parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency in Python - Multi-threading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thumbnail generator
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – producer/consumer architecture
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – program end condition
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – resource constraint using locks
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – resource constraint using semaphores
  prefs: []
  type: TYPE_NORMAL
- en: Resource constraint – semaphore vs lock
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – url rate controller using conditions
  prefs: []
  type: TYPE_NORMAL
- en: Multi-threading – Python and GIL
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency in Python – Multi-processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A primality checker
  prefs: []
  type: TYPE_NORMAL
- en: Sorting disk files
  prefs: []
  type: TYPE_NORMAL
- en: Sorting disk files – using a counter
  prefs: []
  type: TYPE_NORMAL
- en: Sorting disk files – using multi-processing
  prefs: []
  type: TYPE_NORMAL
- en: Multi-threading vs Multi-processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency in Python – Asynchronous Execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-emptive vs Co-operative multitasking
  prefs: []
  type: TYPE_NORMAL
- en: Asyncio in Python
  prefs: []
  type: TYPE_NORMAL
- en: Waiting for future – async and await
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent futures – high level concurrent processing
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency Options - how to choose ?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel Processing libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: joblib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyMP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fractals – The Mandelbrot Set
  prefs: []
  type: TYPE_NORMAL
- en: Fractals – Scaling the Mandelbrot Set implementation
  prefs: []
  type: TYPE_NORMAL
- en: Scaling for the Web
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling workflows – message queues and task queues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Celery – a distributed task queue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Mandelbrot Set - Using Celery
  prefs: []
  type: TYPE_NORMAL
- en: Serving Python on the Web – WSGI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: uWSGI – WSGI middleware on steroids
  prefs: []
  type: TYPE_NORMAL
- en: gunicorn – unicorn for WSGI
  prefs: []
  type: TYPE_NORMAL
- en: gunicorn vs uWSGI
  prefs: []
  type: TYPE_NORMAL
- en: Scalability Architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical scalability Architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal scalability Architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability and performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How do we measure the scalability of a system? Let's take an example, and see
    how this is done.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say our application is a simple report generation system for employees.
    It is able to load employee data from a database, and generate a variety of reports
    in bulk, such as pay slips, tax deduction reports, employee leave reports, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: The system is able to generate 120 reports per minute—this is the *throughput*
    or *capacity* of the system expressed as the number of successfully completed
    operations in a given unit of time. Let's say the time it takes to generate a
    report at the server side (latency) is roughly 2 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Let us say, the architect decides to scale up the system by doubling the RAM
    on its server
  prefs: []
  type: TYPE_NORMAL
- en: Once this is done, a test shows that the system is able to increase its throughput
    to 180 reports per minute. The latency remains the same at 2 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'So at this point, the system has scaled *close to linear* in terms of the memory
    added. The scalability of the system expressed in terms of throughput increase
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Scalability (throughput) = *180/120 = 1.5X*
  prefs: []
  type: TYPE_NORMAL
- en: 'As a second step, the architect decides to double the number of servers on
    the backend—all with the same memory. After this step, he finds that the system''s
    performance throughput has now increased to 350 reports per minute. The scalability
    achieved by this step is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Scalability (throughput) = *350/180 = 1.9X*
  prefs: []
  type: TYPE_NORMAL
- en: The system has now responded much better with a close to linear increase in
    scalability.
  prefs: []
  type: TYPE_NORMAL
- en: After further analysis, the architect finds that by rewriting the code that
    was processing reports on the server to run in multiple processes instead of a
    single process, he is able to reduce the processing time at the server, and hence,
    the latency of each request by roughly 1 second per request at peak time. The
    latency has now gone down from 2 seconds to 1 second.
  prefs: []
  type: TYPE_NORMAL
- en: The system's performance with respect to latency has become better by
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance (latency): *X = 2/1 = 2X*'
  prefs: []
  type: TYPE_NORMAL
- en: How does this improve scalability? Since the time taken to process each request
    is lesser now, the system overall will be able to respond to similar loads at
    a faster rate than what it was able to earlier. With the exact same resources,
    the system's throughput performance, and hence, scalability has increased assuming
    other factors remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s summarize what we discussed so far as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first step, the architect increased the throughput of a single system
    by scaling it up by adding extra memory as a resource, which increased the overall
    scalability of the system. In other words, he scaled the performance of a single
    system by *scaling up*, which boosted the overall performance of the whole system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the second step, he added more nodes to the system, and hence, its ability
    to perform work concurrently, and found that the system responded well by rewarding
    him with a near-linear scalability factor. In other words, he increased the throughput
    of the system by scaling its resource capacity. Thus, he increased scalability
    of the system by *scaling out*, that is, by adding more compute nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the third step, he made a critical fix by running a computation in more than
    one process. In other words, he increased the *concurrency* of a single system
    by dividing the computation to more than one part. He found that this increased
    the performance characteristic of the application by reducing its *latency*, potentially
    setting up the application to handle workloads better at high stress.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We find that there is a relation between Scalability, Performance, Concurrency,
    and Latency. This can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: When performance of one of the components in a system goes up, generally the
    performance of the overall system goes up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When an application scales in a single machine by increasing its concurrency,
    it has the potential to improve performance, and hence, the net scalability of
    the system in deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When a system reduces its performance time, or its latency, at the server, it
    positively contributes to scalability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We have captured these relationships in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Concurrency | Latency | Performance | Scalability |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| High | Low | High | High |'
  prefs: []
  type: TYPE_TB
- en: '| High | High | Variable | Variable |'
  prefs: []
  type: TYPE_TB
- en: '| Low | High | Poor | Poor |'
  prefs: []
  type: TYPE_TB
- en: An ideal system is one that has good concurrency and low latency; such a system
    has high performance, and would respond better to scaling up and/or scaling out.
  prefs: []
  type: TYPE_NORMAL
- en: A system with high concurrency, but also high latency, would have variable characteristics—its
    performance, and hence, scalability would be potentially very sensitive to other
    factors such as current system load, network congestion, geographical distribution
    of compute resources and requests, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: A system with low concurrency and high latency is the worst case—it would be
    difficult to scale such a system, as it has poor performance characteristics.
    The latency and concurrency issues should be addressed before the architect decides
    to scale the system horizontally or vertically.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability is always described in terms of variation in performance throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A system's concurrency is the degree to which the system is able to perform
    work simultaneously instead of sequentially. An application written to be concurrent
    in general, can execute more units of work in a given time than one which is written
    to be sequential or serial.
  prefs: []
  type: TYPE_NORMAL
- en: When one makes a serial application concurrent, one makes the application better
    utilize the existing compute resources in the system—CPU and/or RAM—at a given
    time. Concurrency, in other words, is the cheapest way of making an application
    scale inside a machine in terms of the cost of compute resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrency can be achieved using different techniques. The common ones include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multithreading**: The simplest form of concurrency is to rewrite the application
    to perform parallel tasks in different threads. A thread is the simplest sequence
    of programming instructions that can be performed by a CPU. A program can consist
    of any number of threads. By distributing tasks to multiple threads, a program
    can execute more work simultaneously. All threads run inside the same process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Multiprocessing**: Another way to concurrently scale up a program is to run
    it in multiple processes instead of a single process. Multiprocessing involves
    more overhead than multithreading in terms of message passing and shared memory.
    However, programs that perform a lot of CPU-intensive computations can benefit
    more from multiple processes than multiple threads.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Asynchronous Processing**: In this technique, operations are performed asynchronously
    with no specific ordering of tasks with respect to time. Asynchronous processing
    usually picks tasks from a queue of tasks, and schedules them to execute at a
    future time, often receiving the results in callback functions or special future
    objects. Asynchronous processing usually happens in a single thread.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are other forms of concurrent computing, but in this chapter, we will
    focus our attention on only these three.
  prefs: []
  type: TYPE_NORMAL
- en: Python, especially Python 3, has built-in support for all these types of concurrent
    computing techniques in its standard library. For example, it supports multi-threading
    via its *threading* module, and multiple processes via its *multiprocessing* module.
    Asynchronous execution support is available via the *asyncio* module. A form of
    concurrent processing that combines asynchronous execution with threads and processes
    is available via the *concurrent.futures* module.
  prefs: []
  type: TYPE_NORMAL
- en: In the coming sections we will take a look at each of these in turn with sufficient
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NOTE: The asyncio module is available only in Python 3'
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency versus parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will take a brief look at the concept of concurrency and its close cousin,
    namely parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Both concurrency and parallelism are about executing work simultaneously rather
    than sequentially. However, in concurrency, the two tasks need not be executed
    at the exact same time; instead, they just need to be scheduled to be executed
    simultaneously. Parallelism, on the other hand, requires that both the tasks execute
    together at a given moment in time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To take a real-life example, let''s say you are painting two exterior walls
    of your house. You have employed just one painter, and you find that he is taking
    a lot more time than you thought. You can solve the problem in these two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Instruct the painter to paint a few coats on one wall before switching to the
    next wall, and doing the same there. Assuming he is efficient, he will work on
    both the walls simultaneously (though not at the same time), and achieve the same
    degree of finish on both walls for a given time. This is a *concurrent* solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Employ one more painter. Instruct the first painter to paint the first wall,
    and the second painter to paint the second wall. This is a *parallel* solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Two threads are performing bytecode computations in a single core CPU do not
    exactly perform parallel computation, as the CPU can accommodate only one thread
    at a time. However, they are concurrent from a programmer's perspective, since
    the CPU scheduler performs fast switching in and out of the threads so that they
    appear to run in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: However, on a multi-core CPU, two threads can perform parallel computations
    at any given time in its different cores. This is true parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel computation requires that the computation resources increase at least
    linearly with respect to its scale. Concurrent computation can be achieved by
    using the techniques of multitasking, where work is scheduled and executed in
    batches, making better use of existing resources.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we will use the term *concurrent* uniformly to indicate both
    types of execution. In some places, it may indicate concurrent processing in the
    traditional way, and in some other, it may indicate true parallel processing.
    Use the context to disambiguate.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency in Python – multithreading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will start our discussion of concurrent techniques in Python with multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: 'Python supports multiple threads in programming via its *threading* module.
    The threading module exposes a `Thread` class, which encapsulates a thread of
    execution. Along with this, it also exposes the following synchronization primitives:'
  prefs: []
  type: TYPE_NORMAL
- en: A `Lock` object, which is useful for synchronized protected access to share
    resources, and its cousin `RLock`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Condition object, which is useful for threads to synchronize while waiting
    for arbitrary conditions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An `Event` object, which provides a basic signaling mechanism between threads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A `Semaphore` object, which allows synchronized access to limited resources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A `Barrier` object, which allows a fixed set of threads to wait for each other,
    synchronize to a particular state, and proceed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thread objects in Python can be combined with the synchronized `Queue` class
    in the queue module for implementing thread-safe producer/consumer workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us start our discussion of multi-threading in Python with the example of
    a program used to generate thumbnails of image URLs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example, we are using **Pillow**, a fork of the **Python Imaging Library**
    (**PIL**) to perform this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code works very well for single URLs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us say we want to convert five image URLs to their thumbnails:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how such a function performs with respect to time taken in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail generator](../Images/image00433.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Response time of serial thumbnail converter for 5 URLs
  prefs: []
  type: TYPE_NORMAL
- en: The function took approximately 1.7 seconds per URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now scale the program to multiple threads so we can perform the conversions
    concurrently. Here is the rewritten code to run each conversion in its own thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The timing that this last program now gives is shown in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail generator](../Images/image00434.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Response time of threaded thumbnail converter for 5 URLs
  prefs: []
  type: TYPE_NORMAL
- en: With this change, the program returns in 1.76 seconds, almost equal to the time
    taken by a single URL in serial execution before. In other words, the program
    has now linearly scaled with respect to the number of threads. Note that, we had
    to make no change to the function itself to get this scalability boost.
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – producer/consumer architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous example, we saw a set of image URLs being processed by a thumbnail
    generator function concurrently by using multiple threads. With the use of multiple
    threads, we were able to achieve near linear scalability as compared to serial
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: However, in real life, rather than processing a fixed list of URLs, it is more
    common for the URL data to be produced by some kind of URL producer. It could
    be fetching this data from a database, a **comma separated value** (**CSV**) file
    or from a TCP socket for example.
  prefs: []
  type: TYPE_NORMAL
- en: In such a scenario, creating one thread per URL would be a tremendous waste
    of resources. It takes a certain overhead to create a thread in the system. We
    need some way to reuse the threads we create.
  prefs: []
  type: TYPE_NORMAL
- en: 'For such systems that involve a certain set of threads producing data and another
    set of threads consuming or processing data, the producer/consumer model is an
    ideal fit. Such a system has the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: Producers are a specialized class of workers (threads) producing the data. They
    may receive the data from a specific source(s), or generate the data themselves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Producers add the data to a shared synchronized queue. In Python, this queue
    is provided by the `Queue` class in the aptly named `queue` module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another set of specialized class of workers, namely consumers, wait on the queue
    to get (consume) the data. Once they get the data, they process it and produce
    the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The program comes to an end when the producers stop generating data and the
    consumers are starved of data. Techniques like timeouts, polling, or poison pills
    can be used to achieve this. When this happens, all threads exit, and the program
    completes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have rewritten our thumbnail generator to a producer consumer architecture.
    The resulting code is given next. Since this is a bit detailed, we will discuss
    each class one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at the imports—these are pretty self-explanatory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next is the code for the producer class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s analyze the producer class code:'
  prefs: []
  type: TYPE_NORMAL
- en: The class is named `ThumbnailURL_Generator`. It generates the URLs (by using
    the service of a website named [http://dummyimage.com](http://dummyimage.com))
    of different sizes, foreground, and background colors. It inherits from the `threading.Thread`
    class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It has a `run` method, which goes in a loop, generates a random image URL, and
    pushes it to the shared queue. Every time, the thread sleeps for a fixed time,
    as configured by the `sleep_time` parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The class exposes a `stop` method, which sets the internal flag to `False` causing
    the loop to break and the thread to finish its processing. This can be called
    externally by another thread, typically, the main thread.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, the URL consumer class that consumes the thumbnail URLs and creates the
    thumbnails:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the analysis of the consumer class:'
  prefs: []
  type: TYPE_NORMAL
- en: The class is named `ThumbnailURL_Consumer`, as it consumes URLs from the queue,
    and creates thumbnail images of them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `run` method of this class goes in a loop, gets a URL from the queue, and
    converts it to thumbnail by passing it to the `thumbnail_image` method. (Note
    that this code is exactly the same as that of the `thumbnail_image` function we
    created earlier.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `stop` method is very similar, checking for a stop flag every time in the
    loop, and ending once the flag has been unset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is the main part of the code—setting up a couple of producers and consumers
    each, and running them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a screenshot of the program in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail generator – producer/consumer architecture](../Images/image00435.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Running the thumbnail producer/consumer program with 4 threads, 2 of each type
  prefs: []
  type: TYPE_NORMAL
- en: In the above program, since the producers keeps generating random data without
    any end, the consumers will keep consuming it without any end. Our program has
    no proper end condition.
  prefs: []
  type: TYPE_NORMAL
- en: Hence this program will keep running forever till the network requests are denied
    or timed out or the disk space of the machine runs out because of thumbnails.
  prefs: []
  type: TYPE_NORMAL
- en: However, a program solving a real world problem should end in some way which
    is predictable.
  prefs: []
  type: TYPE_NORMAL
- en: This could be due to a number of external constraints
  prefs: []
  type: TYPE_NORMAL
- en: It could be a timeout introduced where the consumers wait for data for a certain
    maximum time, and then exit if no data is available during that time. This, for
    example, can be configured as a timeout in the `get` method of the queue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another technique would be to signal program end after a certain number of resources
    are consumed or created. In this program, for example, it could be a fixed limit
    to the number of thumbnails created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, we will see how to enforce such resource limits by
    using threading synchronization primitives such as Locks and Semaphores.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may have observed that we start a thread using its `start` method, though
    the overridden method in the Thread subclass is `run`. This is because, in the
    parent `Thread` class, the `start` method sets up some state, and then calls the
    `run` method internally. This is the right way to call the thread's run method.
    It should never be called directly.
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – resource constraint using locks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the earlier section, we saw how to rewrite the thumbnail generator program
    moulded in the producer/consumer architecture. However, our program had a problem—it
    would run endlessly till it ran out of disk space or network bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will see how to modify the program using a `Lock`, a synchronization
    primitive to implement a counter that will limit the number of images created
    as a way to end the program.
  prefs: []
  type: TYPE_NORMAL
- en: Lock objects in Python allows exclusive access by threads to a shared resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudo-code would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'However, Lock objects support context-managers via the with statement, so this
    is more commonly written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To implement a fixed number of images per run, our code needs to be supported
    to add a counter. However, since multiple threads would check and increment this
    counter, it needs to be synchronized via a `Lock` object.
  prefs: []
  type: TYPE_NORMAL
- en: This is our first implementation of the resource counter class using Locks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this modifies the consumer class as well, it makes sense to discuss both
    changes together. Here is the modified consumer class to accommodate the extra
    counter needed to keep track of the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let's analyze both these classes. First the new class, `ThumbnailImageSaver`.
  prefs: []
  type: TYPE_NORMAL
- en: This class derives from the `object`. In other words, it is not a `Thread`.
    It is not meant to be one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It initializes a lock object and a counter dictionary in its initializer method.
    The lock is for synchronizing access to the counter by threads. It also accepts
    a `limit` parameter equal to the number of images it should save.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `thumbnail_image` method moves to here from the consumer class. It is called
    from a `save` method, which encloses the call in a synchronized context using
    the lock.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `save` method first checks if the count has crossed the configured limit;
    when this happens, the method returns `False`. Otherwise, the image is saved with
    a call to `thumbnail_image`, and the image filename is added to the counter, effectively
    incrementing the count.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the modified `ThumbnailURL_Consumer` class.
  prefs: []
  type: TYPE_NORMAL
- en: The class's initializer is modified to accept an instance of the `ThumbnailImageSaver`
    as a `saver` argument. The rest of the arguments remain the same.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `thumbnail_image` method no longer exists in this class, as it is moved
    to the new class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `run` method is much simplified. It makes a call to the `save` method of
    the saver instance. If it returns `False`, it means the limit has been reached,
    the loop breaks, and the consumer thread exits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also have modified the `__str__` method to return a unique ID per thread,
    which is set in the initializer using the `uuid` module. This helps to debug threads
    in a real-life example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The calling code also changes a bit, as it needs to set up the new object,
    and configure the consumer threads with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the main points to be noted here:'
  prefs: []
  type: TYPE_NORMAL
- en: We create an instance of the new `ThumbnailImageSaver` class, and pass it on
    to the consumer threads when creating them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We wait on consumers first. Note that, the main thread doesn't call `stop`,
    but `join` on them. This is because the consumers exit automatically when the
    limit is reached, so the main thread should just wait for them to stop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We stop the producers after the consumers exit—explicitly so—since they would
    otherwise keep working forever, since there is no condition for the producers
    to exit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use a dictionary instead of an integer as because of the nature of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Since the images are randomly generated, there is a minor chance of one image
    URL being same as another one created previously, causing the filenames to clash.
    Using a dictionary takes care of such possible duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows a run of the program with a limit of 100 images.
    Note that we can only show the last few lines of the console log, since it produces
    a lot of output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail generator – resource constraint using locks](../Images/image00436.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Run of the thumbnail generator program with a limit of 100 images using a Lock
  prefs: []
  type: TYPE_NORMAL
- en: You can configure this program with any limit of the images, and it will always
    fetch exactly the same count—nothing more or less.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will get familiarized with another synchronization primitive,
    namely *Semaphore*, and learn how to implement a resource limiting class in a
    similar way using the semaphore.
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – resource constraint using semaphores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Locks aren't the only way to implement synchronization constraints and write
    logic on top of them such as to limit resources used/generated by a system.
  prefs: []
  type: TYPE_NORMAL
- en: A `Semaphore`, one of the oldest synchronization primitives in computer science,
    is ideally suited for such use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'A semaphore is initialized with a value greater than zero:'
  prefs: []
  type: TYPE_NORMAL
- en: When a thread calls `acquire` on a semaphore that has a positive internal value,
    the value gets decremented by one, and the thread continues on its way.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When another thread calls `release` on the semaphore, the value is incremented
    by 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any thread calling `acquire` once the value has reached zero is blocked on the
    semaphore till it is woken up by another thread calling *release*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Due to this behavior, a semaphore is perfectly suited for implementing a fixed
    limit on shared resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code example, we will implement another class for resource
    limiting our thumbnail generator program, this time using a semaphore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Since the new semaphore-based class keeps the exact same interface as the previous
    lock-based class—with a save method—there is no need to change any code on the
    consumer!
  prefs: []
  type: TYPE_NORMAL
- en: Only the calling code needs to be changed.
  prefs: []
  type: TYPE_NORMAL
- en: 'This line in the previous code which initialized the `ThumbnailImageSaver`
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line needs to be replaced with the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the code remains exactly the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us quickly discuss the new class using the semaphore before seeing this
    code in action:'
  prefs: []
  type: TYPE_NORMAL
- en: The `acquire` and `release` methods are simple wrappers over the same methods
    on the semaphore.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We initialize the semaphore with a value equal to the image limit in the initializer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the save method, we call the `acquire` method. If the semaphore's limit is
    reached, it will return `False`. Otherwise, the thread saves the image and returns
    `True`. In the former case, the calling thread quits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The internal count attribute of this class is only there for debugging. It doesn't
    add anything to the logic of limiting images.
  prefs: []
  type: TYPE_NORMAL
- en: 'This class behaves in a way similar way to the previous one, and limits resources
    exactly. The following is an example with a limit of 200 images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail generator – resource constraint using semaphores](../Images/image00437.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Run of the thumbnail generator program with a limit of 200 images using a Semaphore
  prefs: []
  type: TYPE_NORMAL
- en: Resource constraint – semaphore versus lock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We saw two competing versions of implementing a fixed resource constraint in
    the previous two examples—one using `Lock` and another using `Semaphore`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The differences between the two versions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The version using Lock protects all the code that modifies the resource—in this
    case, checking the counter, saving the thumbnail, and incrementing the counter—to
    make sure that there are no data inconsistencies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Semaphore version is implemented more like a gate—a door that is open while
    the count is below the limit, and through which any number of threads can pass,
    and that only closes when the limit is reached. In other words, it doesn't mutually
    exclude threads from calling the thumbnail saving function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hence, the effect is that the Semaphore version would be faster than the version
    using Lock.
  prefs: []
  type: TYPE_NORMAL
- en: How much faster? The following timing example for a run of 100 images gives
    an idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'This screenshot shows the time it takes for the Lock version to save 100 images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Resource constraint – semaphore versus lock](../Images/image00438.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Timing the run of the thumbnail generator program—the Lock version—for 100 images
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the time for the Semaphore version to save a
    similar number:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Resource constraint – semaphore versus lock](../Images/image00439.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Timing the run of the thumbnail generator program—the Semaphore version—for
    100 images
  prefs: []
  type: TYPE_NORMAL
- en: By a quick calculation you can see that the semaphore version is about 4 times
    faster than the lock version for the same logic. In other words, it *scales 4
    times better*.
  prefs: []
  type: TYPE_NORMAL
- en: Thumbnail generator – URL rate controller using conditions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will briefly see the application of another important synchronization
    primitive in threading, namely the `Condition` object.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will get a real life example of using a `Condition` object. We will
    implement a throttler for our thumbnail generator to manage the rate of URL generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the producer/consumer systems in real life, the following three kinds of
    scenario can occur with respect to the rate of data production and consumption:'
  prefs: []
  type: TYPE_NORMAL
- en: Producers produce data at a faster pace than consumers can consume. This causes
    the consumers to always play catch up with the producers. Excess data by the producers
    can accumulate in the queue, which causes the queue to consume a higher memory
    and CPU usage in every loop causing the program to slow down.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consumers consume data at a faster rate than producers. This causes the consumers
    to always wait on the queue—for data. This, in itself, is not a problem as long
    as the producers don't lag too much. In the worst case, this leads to half of
    the system, that is, the consumers, remaining idle, while the other half—the producers—try
    to keep up with the demand.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both producers and consumers work at nearly the same pace keeping the queue
    size within limits. This is the ideal scenario.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are many ways to solve this problem. Some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Queue with a fixed size** – Producers would be forced to wait till data is
    consumed by a consumer once the queue size limit is reached. However this would
    almost always keeps the queue full.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Provide the workers with timeouts plus other responsibilities**: Rather than
    remain blocked on the queue, producers and/or consumers can use a timeout to wait
    on the queue. When they time out they can either sleep or perform some other responsibilities
    before coming back and waiting on the queue.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dynamically configure the number of workers**: This is an approach where
    the worker pool size automatically increases or decreases upon demand. If one
    class of workers is ahead, the system will launch just the required number of
    workers of the opposite class to keep the balance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Adjust the data generation rate**: In this approach, we statically or dynamically
    adjust the data generation rate by the producers. For example, the system can
    be configured to produce data at a fixed rate, say, 50 URLs in a minute or it
    can calculate the rate of consumption by the consumers, and adjust the data production
    rate of the producers dynamically to keep things in balance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following example, we will implement the last approach—to limit the production
    rate of URLs to a fixed limit using `Condition` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `Condition` object is a sophisticated synchronization primitive that comes
    with an implicit built-in lock. It can wait on an arbitrary condition till it
    becomes True. The moment the thread calls `wait` on the condition, the internal
    lock is released, but the thread itself becomes blocked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, another thread can wake up this preceding thread by setting the condition
    to True, and then calling `notify` or `notify_all` on the condition object. At
    this point, the preceding blocked thread is woken up, and continues on its way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here is our new class namely `ThumbnailURLController` which implements the rate
    control of URL production using a condition object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s discuss the preceding code before we discuss the changes in the producer
    class that will make use of this class:'
  prefs: []
  type: TYPE_NORMAL
- en: The class is an instance of `Thread`, so it runs in its own thread of execution.
    It also holds a Condition object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It has a `calc_rate` method, which calculates the rate of generation of URLs
    by keeping a counter and using timestamps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `run` method, the rate is checked. If it's below the configured limit,
    the condition object notifies all threads waiting on it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Most importantly, it implements a `throttle` method. This method uses the current
    rate, calculated via `calc_rate`, and uses it to throttle and adjust the sleep
    times of the producers. It mainly does these two things:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the rate is more than the configured limit, it causes the calling thread
    to wait on the condition object till the rate levels off. It also calculates an
    extra sleep time that the thread should sleep in its loop to adjust the rate to
    the required level.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the rate is less than the configured limit, then the thread needs to work
    faster and produce more data, so it calculates the sleep difference and lowers
    the sleep limit accordingly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is the code of the producer class to incorporate the changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how this last code works:'
  prefs: []
  type: TYPE_NORMAL
- en: The class now accepts an additional controller object in its initializer. This
    is the instance of the controller class given earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After putting a URL, it increments the count on the controller. Once the count
    reaches a minimum limit (set as 5 to avoid early throttling of the producers),
    it calls `throttle` on the controller, passing itself as the argument.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The calling code also needs quite a few changes. The modified code is shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The main changes here are the ones listed next:'
  prefs: []
  type: TYPE_NORMAL
- en: The controller object is created – with the exact number of producers that will
    be created. This helps the correct calculation of sleep time per thread.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The producer threads themselves are passed the instance of the controller in
    their initializer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The controller is started as a thread before all other threads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here is a run of the program configured with 200 images at the rate of 50 images
    per minute. We show two images of the running program's output, one at the beginning
    of the program and one towards the end.
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail generator – URL rate controller using conditions](../Images/image00440.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Starting the thumbnail program with URL rate controller—at 50 URLs per minute
  prefs: []
  type: TYPE_NORMAL
- en: You will find that, when the program starts, it almost immediately slows down,
    and nearly comes to a halt, since the original rate is high. What happens here
    is that the producers call on the `throttle` method, and since the rate is high,
    they all get blocked on the condition object.
  prefs: []
  type: TYPE_NORMAL
- en: After a few seconds, the rate comes down to the prescribed limit, since no URLs
    are generated. This is detected by the controller in its loop, and it calls `notify_all`
    on the threads, waking them up.
  prefs: []
  type: TYPE_NORMAL
- en: After a while you will see that the rate is getting settled around the set limit
    of 50 URLs per minute.
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail generator – URL rate controller using conditions](../Images/image00441.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The thumbnail program with URL rate controller 5-6 seconds after start
  prefs: []
  type: TYPE_NORMAL
- en: 'Towards the end of the program, you will see that the rate has almost settled
    to the exact limit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thumbnail generator – URL rate controller using conditions](../Images/image00442.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The thumbnail program with URL rate controller towards the end
  prefs: []
  type: TYPE_NORMAL
- en: We are coming towards the end of our discussion on threading primitives and
    how to use them in improving the concurrency of your programs and in implementing
    shared resource constraints and controls.
  prefs: []
  type: TYPE_NORMAL
- en: Before we conclude, we will look at an aspect of Python threads which prevents
    multi-threaded programs from making full use of the CPU in Python – namely the
    GIL or Global Interpreter Lock.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading – Python and GIL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Python there is, a global lock that prevents multiple threads from executing
    native bytecode at once. This lock is required, since the memory management of
    CPython (the native implementation of Python) is not thread-safe.
  prefs: []
  type: TYPE_NORMAL
- en: This lock is called **Global Interpreter Lock** or just **GIL**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Python cannot execute bytecode operations concurrently on CPUs due to the GIL.
    Hence, Python becomes nearly unsuitable for the following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: When the program depends on a number of heavy bytecode operations, which it
    wants to run concurrently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the program uses multithreading to utilize the full power of multiple CPU
    cores on a single machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I/O calls and long-running operations typically occur outside the GIL. So multithreading
    is efficient in Python only when it involves some amount of I/O or such operations-
    such as image processing.
  prefs: []
  type: TYPE_NORMAL
- en: In such cases, scaling your program to concurrently scale beyond a single process
    becomes a handy approach. Python makes this possible via its `multiprocessing`
    module, which is our next topic of discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency in Python – multiprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Python standard library provides a multiprocessing module, which allows
    a programmer to write programs that scale concurrently using multiple processes
    instead of threads.
  prefs: []
  type: TYPE_NORMAL
- en: Since multi-processing scales computation across multiple processes, it effectively
    removes any issues with the GIL in Python. Programs can make use of multiple CPU
    cores efficiently using this module.
  prefs: []
  type: TYPE_NORMAL
- en: The main class exposed by this module is the `Process` class, the analog to
    the `Thread` class in the threading module. It also provides a number of synchronization
    primitives, which are almost exact counterparts of their cousins in the threading
    module.
  prefs: []
  type: TYPE_NORMAL
- en: We will get started by using an example using the `Pool` object provided by
    this module. It allows a function to execute in parallel over multiple inputs
    using processes.
  prefs: []
  type: TYPE_NORMAL
- en: A primality checker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following function is a simple checker function for primality, that is,
    whether the input number is prime or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a threaded class that uses this last function to check numbers
    from a queue for primality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We will test it with 1,000 large prime numbers. In order to save space for
    the list represented here, what we''ve done is to take 10 of these numbers and
    multiply the list with 100:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve used four threads for this test. Let''s see how the program performs,
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A primality checker](../Images/image00443.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Primality checker of 1,000 numbers using a pool of 4 threads
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, here is the equivalent code using the multiprocessing `Pool` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows its performance over the same set of numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A primality checker](../Images/image00444.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Primality checker of 1,000 numbers using a multiprocessing Pool of 4 processes
  prefs: []
  type: TYPE_NORMAL
- en: 'We learn the following by comparing these numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: The real time, that is, the wall clock time spent by the process pool version
    at 1 minute 9.6 seconds (69.6 seconds) is nearly 50% lesser than that of the thread
    pool version at 2 minute 12 seconds (132 seconds).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, notice that the user time—that is, the time spent inside the CPU for
    user code—for the process pool version at 4 minute 22 seconds (262 seconds ) is
    nearly two times more than that of the thread pool version at 2 minutes 12 seconds
    (132 seconds).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The real and user CPU time of the thread pool version is exactly the same at
    2 minutes 12 seconds. This is a clear indication that the threaded version was
    able to execute effectively, only in one of the CPU cores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This means that the process pool version was able to better make use of all
    the CPU cores, since for the 50% of the real time of the thread pool version,
    it was able to make use of the CPU time twice over.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, the real performance boost in terms of CPU time/real time for the two
    programs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Threaded version → 132 seconds/132 seconds = 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process version → 262 seconds/69.6 seconds = 3.76 ~= 4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The real performance ratio of the process version to the threaded version is,
    hence, given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 4/1 = 4
  prefs: []
  type: TYPE_NORMAL
- en: The machine on which the program was executed has a four-core CPU. This clearly
    shows that the multiprocess version of the code was able to utilize all the four
    cores of the CPU nearly equally.
  prefs: []
  type: TYPE_NORMAL
- en: This is because the threaded version is being restricted by the GIL, whereas
    the process version has no such restriction and can freely make use of all the
    cores.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let us get on to a more involved problem—that of sorting
    disk-based files.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting disk files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you have hundreds of thousands of files on the disk, each containing
    a certain fixed number of integers in a given range. Let's say we need the files
    to be sorted and merged into a single file.
  prefs: []
  type: TYPE_NORMAL
- en: If we decide to load all this data into memory, it will need large amounts of
    RAM. Let's do a quick calculation for a million files, each containing around
    100 integers in the range of 1 to 10,000 for a total of 1,00,000,000 or 100 million
    integers.
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume each of the files is loaded as a list of integers from the disk—we
    will ignore string processing, and the like for the time being.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `sys.getsizeof`, we can get a rough calculation going:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: So, the entire data will take close to 800 MB if loaded into memory at once.
    Now this may not look like a large memory footprint at first, but the larger the
    list, the more system resources it takes to sort it in memory as one large list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the simplest code for sorting of all the integers present in the disk
    files after loading them into memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This preceding code loads a certain number of files from the disk, each containing
    100 integers in the range 1 to 10,000\. It reads each file, maps it to a list
    of integers, and adds each list to a cumulative list. Finally, the list is sorted
    and written to a file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the time taken to sort a certain number of disk files:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Number of files (n) | Time taken for sorting |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 17.4 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 10000 | 101 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 100000 | 138 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 1000000 | NA |'
  prefs: []
  type: TYPE_TB
- en: As you can see, the time taken scales pretty reasonably—less than *O(n)*. However,
    this is one problem where more than the time, it is the space—in terms of memory
    and operations on it—that matters.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the machine that was used to conduct the test, an 8-GB RAM,
    4-core CPU laptop with 64-bit Linux, the test with a million numbers didn't finish.
    Instead, it caused the system to hang, so it was not completed.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting disk files – using a counter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you look at the data, you find that there is an aspect that allows us to
    treat the problem as more about space than time. This is the observation that
    the integers are in a fixed range with a maximum limit of 10,000.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, instead of loading all the data as separate lists and merging them, one
    can use a data structure like a counter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the basic idea of how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a data structure—a counter, where each integer starts from 1… 10,000
    the maximum entry is initialized to zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load each file and convert the data to a list. For any number found in the list,
    increment its count in the counter data structure initialized in Step 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, loop through the counter, and output each number with a count greater
    than zero *so many times*, and save the output to a file. The output is your merged
    and sorted single file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we use a `defaultdict` from the collections module as
    the counter. Whenever we encounter an integer, we increment its count. In the
    end, the counter is looped through, and each item is output as many times as it
    was found.
  prefs: []
  type: TYPE_NORMAL
- en: The sort and merge happen due to the way we have converted the problem from
    one of sorting integers to one of keeping a count and outputting in a naturally
    sorted order.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the time taken for sorting of numbers against
    the size of the input – in terms of number of disk files:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Number of files (n) | Time taken for sorting |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 16.5 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 10000 | 83 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 100000 | 86 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 1000000 | 359 seconds |'
  prefs: []
  type: TYPE_TB
- en: Though the performance for the smallest case – that of 1,000 files is similar
    to that for the in-memory sort, the performance becomes better as the size of
    the input increases. This code also manages to finish the sorting of a million
    files or 100 million integers - in about 5m 59s.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In timing measurements for processes that read files, there is always the effect
    of buffer caches in the kernel. You will find that running the same performance
    test successively shows a tremendous improvement, as Linux caches the contents
    of the files in its buffer cache. Hence, subsequent tests for the same input size
    should be done after clearing the buffer cache. In Linux, this can be done by
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In our tests for successive numbers, we *don't* reset the buffer caches as shown
    before. This means that runs for higher numbers enjoy a performance boost from
    the caches created during the previous runs. However, since this is done uniformly
    for each test, the results are comparable. The cache is reset before starting
    the test suite for a specific algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm also requires much lesser memory, since for each run, the memory
    requirements are *the same* since we are using an array of integers upto MAXINT
    and just incrementing the count.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the memory usage of the sort in-memory program for 100,000 files using
    the *memory_profiler*, which we have encountered in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sorting disk files – using a counter](../Images/image00445.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Memory usage of in-memory sort program for an input of 100,000 files
  prefs: []
  type: TYPE_NORMAL
- en: 'And the following screenshot shows the memory usage for the sort counter for
    the same number of files:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sorting disk files – using a counter](../Images/image00446.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Memory usage of counter sort program for an input of 100,000 files
  prefs: []
  type: TYPE_NORMAL
- en: The memory usage of the in-memory sort program at 465 MB is more than six times
    that of the counter sort program at 70 MB. Also note that the sorting operation
    itself takes extra memory of nearly 10 MB in the in-memory version.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting disk files – using multiprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we rewrite the counter sorting program using multiple processes.
    The approach is to scale the processing input files for more than one process
    by splitting the list of file paths to a pool of processes – and planning to take
    advantage of the resulting data parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the rewrite of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'It is exactly the same code as earlier with the following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of processing all the files as a single list, the filenames are put
    in batches, with batches equaling the size of the pool.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use a sorter function, which accepts the list of filenames, processes them,
    and returns a dictionary with the counts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The counts are summed for each integer in the range from 1 to MAXINT, and so
    many numbers are written to the sorted file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following table shows the data for processing a different number of files
    for a pool sizes of 2 and 4 respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Number of files (n) | Pool size | Time taken for sorting |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1,000 | 2 | 18 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 20 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 10,000 | 2 | 92 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 77 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 100,000 | 2 | 96 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 86 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 1,000,000 | 2 | 350 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 329 seconds |'
  prefs: []
  type: TYPE_TB
- en: 'The numbers tell an interesting story:'
  prefs: []
  type: TYPE_NORMAL
- en: The multiple process version one with 4 processes (equal to number of cores
    in the machine) has better numbers overall when compared to the one with 2 processes
    and the single process one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, the multiple-process version doesn't seem to offer much of a performance
    benefit when compared to the single-process version. The performance numbers are
    very similar and any improvement is within bounds of error and variation. For
    example, for 1 million number input the multiple process with 4 processes has
    just a 8% improvement over the single-process one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is because the bottleneck here is the processing time it takes to load
    the files into memory – in file I/O - not the computation (sorting), as the sorting
    is just an increment in the counter. Hence the single process version is pretty
    efficient as it is able to load all the file data in the same address space. The
    multiple-process ones are able to improve this a bit by loading the files in multiple
    address spaces, but not by a lot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This example shows that in situations where there is not much computation done
    but the bottleneck is disk or file I/O, the impact of scaling by multi-processing
    is much lesser.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading versus multiprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have come to the end of our discussion on multi-processing, it is
    a good time to compare and contrast the scenarios where one needs to choose between
    scaling using threads in a single process or using multiple processes in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use multithreading in the following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: The program needs to maintain a lot of shared states, especially mutable ones.
    A lot of the standard data structures in Python, such as lists, dictionaries,
    and others, are thread-safe, so it costs much less to maintain a mutable shared
    state using threads than via processes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The program needs to keep a low memory foot-print.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The program spends a lot of time doing I/O. Since the GIL is released by threads
    doing I/O, it doesn't affect the time taken by the threads to perform I/O.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The program doesn't have a lot of data parallel operations which it can scale
    across multiple processes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use multiprocessing in these scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The program performs a lot of CPU-bound heavy computing: byte-code operations,
    number crunching, and the like on reasonably large inputs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The program has inputs which can be parallelized into chunks and whose results
    can be combined afterwards – in other words, the input of the program yields well
    to data-parallel computations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The program doesn't have any limitations on memory usage, and you are on a modern
    machine with a multicore CPU and large enough RAM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is not much shared mutable state between processes that need to be synchronized—this
    can slow down the system, and offset any benefits gained from multiple processes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your program is not heavily dependent on I/O—file or disk I/O or socket I/O.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Concurrecy in Python - Asynchronous Execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen two different ways to perform concurrent execution using multiple
    threads and multiple processes. We saw different examples of using threads and
    their synchronization primitives. We also saw a couple of examples using multi-processing
    with slightly varied outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from these two ways to do concurrent programming, another common technique
    is that of asynchronous programming or asynchronous I/O.
  prefs: []
  type: TYPE_NORMAL
- en: In an asynchronous model of execution, tasks are picked to be executed from
    a queue of tasks by a scheduler, which executes these tasks in an interleaved
    manner. There is no guarantee that the tasks will be executed in any specific
    order. The order of execution of tasks depend upon how much processing time a
    task is willing to *yield* to another task in the queue. Put in other words, asynchronous
    execution happens through co-operative multitasking.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous execution usually happens in a single thread. This means no true
    data parallelism or true parallel execution can happen. Instead, the model only
    provides a semblance of parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: As execution happens out of order, asynchronous systems need a way to return
    the results of function execution to the callers. This usually happens with *callbacks*,
    which are functions to be called when the results are ready or using special objects
    that receive the results, often called *futures*.
  prefs: []
  type: TYPE_NORMAL
- en: Python 3 provides support for this kind of execution via its *asyncio* module
    using coroutines. Before we go on to discuss this, we will spend some time understanding
    pre-emptive multitasking versus cooperative multitasking, and how we can implement
    a simple cooperative multitasking scheduler in Python using generators.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-emptive versus cooperative multitasking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The programs we wrote earlier using multiple threads were examples of concurrency.
    However, we didn't have to worry about how and when the operating system chose
    to run the thread—we just had to prepare the threads (or processes), provide the
    target function, and execute them. The scheduling is taken care of by the operating
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Every few ticks of the CPU clock, the operating system pre-empts a running thread,
    and replaces it with another one in a particular core. This can happen due to
    different reasons, but the programmer doesn't have to worry about the details.
    He just creates the threads, sets them up with the data they need to process,
    uses the correct synchronization primitives, and starts them. The operating system
    does the rest including switching and scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: This is how almost all modern operating systems work. It guarantees each thread
    a fair share of the execution time, all other things being equal. This is known
    as **pre-emptive multitasking**.
  prefs: []
  type: TYPE_NORMAL
- en: There is another type of scheduling which is the opposite of pre-emptive multitasking.
    This is called as co-operative multitasking, where the operating system plays
    no role in deciding the priority and execution of competing threads or processes.
    Instead, a process or thread willingly yields control for another process or thread
    to run. Or a thread can replace another thread which is idling (sleeping) or waiting
    for I/O.
  prefs: []
  type: TYPE_NORMAL
- en: This is the technique used in the asynchronous model of concurrent execution
    using co-routines. A function, while waiting for data, say a call on the network
    that is yet to return, can yield control for another function or task to run.
  prefs: []
  type: TYPE_NORMAL
- en: Before we go to discuss actual co-routines using `asyncio` let us write our
    own co-operative multitasking scheduler using simple Python generators. It is
    not very difficult to do this as you can see below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s analyze the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: We have four functions—three generators, since they use the `yield` keyword
    to return the data, and a scheduler, which runs a certain set of tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `square_mapper` function accepts an iterator, which returns integers iterating
    through it, and yields the squares of the members
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `prime_filter` function accepts a similar iterator, and filters out numbers
    that are not prime, yielding only prime numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `number_generator` function acts as the input iterator to both these functions,
    providing them with an input stream of integers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us now look at the calling code which ties all the four functions together.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an analysis of the calling code:'
  prefs: []
  type: TYPE_NORMAL
- en: The number generator is initialized with a count, which is received via the
    command-line argument. It is passed to the `square_mapper` function. The combined
    function is added as a task to the `tasks` list.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A similar operation is performed for the `prime_filter` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `scheduler` method is run by passing the task list to it, which it runs
    by iterating through a `for` loop, running each task one after another. The results
    are appended to a dictionary using the function's name as the key, and returned
    at the end of execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We print the last prime number's value to verify correct execution, and also
    the time taken for the scheduler to process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see the output of our simple cooperative multitasking scheduler for
    a limit of `10`. This allows to capture all the input in a single command window,
    as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pre-emptive versus cooperative multitasking](../Images/image00447.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of the simple co-operative multitasking program example for an input
    of 10
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s analyze the output:'
  prefs: []
  type: TYPE_NORMAL
- en: The output of the `square_mapper` and `prime_filter` functions alternates on
    the console. This is because the scheduler switches between them in the `for`
    loop. Each of the functions are co-routines (generators) so they *yield* execution
    – that is the control is passed from one function to the next – and vice-versa.
    This allows both functions to run concurrently, while maintaining state and producing
    output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since we used generators here, they provide a natural way of generating the
    result plus yielding control in one go, using the *yield* keyword.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The asyncio module in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `asyncio` module in Python provides support for writing concurrent, single-threaded
    programs using co-routines. It is available only in Python 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'A co-routine using the `asyncio` module is one that uses either of the following
    approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the `async def` statement for defining functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being decorated using the `@asyncio.coroutine` expression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generator-based co-routines use the second technique, and they yield from expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Co-routines created using the first technique typically use the `await <future>`
    expression to wait for the future to be completed.
  prefs: []
  type: TYPE_NORMAL
- en: Co-routines are scheduled for execution using an `event` loop, which connects
    the objects and schedules them as tasks. Different types of event loop are provided
    for different operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code rewrites our earlier example of a simple cooperative multitasking
    scheduler to use the `asyncio` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is how this last code works:'
  prefs: []
  type: TYPE_NORMAL
- en: The `number_generator` function is a co-routine that yields from the sub-generator
    `range(m, n+1)`, which is an iterator. This allows this co-routine to be called
    in other co-routines.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `square_mapper` function is a co-routine of the first type using the `async
    def` keyword. It returns a list of squares using numbers from the number generator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `prime_filter` function is of the same type. It also uses the number generator,
    and appends prime numbers to a list and returns it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both co-routines yield to the other by sleeping using the *asyncio.sleep* function
    and waiting on it. This allows both co-routines to work concurrently in an interleaved
    fashion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is the calling code with the `event` loop and the rest of the plumbing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Here is the output of the program. Observe how the results of each of the task
    is getting printed in an interleaved fashion.
  prefs: []
  type: TYPE_NORMAL
- en: '![The asyncio module in Python](../Images/image00448.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Result of executing the asyncio task calculating prime numbers and squares
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us analyze how the preceding code worked line by line, while following
    a top-to-bottom approach:'
  prefs: []
  type: TYPE_NORMAL
- en: We first get an asyncio event `loop` using the `factory` function `asyncio.get_event_loop`.
    This returns the default event loop implementation for the operating system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set up an asyncio `future` object by using the `gather` method of the module.
    This method is used to aggregate results from a set of co-routines or futures
    passed as its argument. We pass both the `prime_filter` and the `square_mapper`
    to it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A callback is added to the `future` object—the `print_result` function. It will
    be automatically called once the future's execution is completed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loop is run until the future's execution is completed. At this point the
    callback is called and it prints the result. Note how the output appears interleaved
    – as each task yields to the other one using the *sleep* function of the asyncio
    module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loop is closed and terminates is operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Waiting for a future – async and await
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed how one could wait for data from a future inside a co-routine using
    await. We saw an example that uses await to yield control to other co-routines.
    Let's now look at an example that waits for I/O completion on a future, which
    returns data from the web.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, you need the `aiohttp` module which provides an HTTP client
    and server to work with the asyncio module and supports futures. We also need
    the `async_timeout` module which allows timeouts on asynchronous co-routines.
    Both these modules can be installed using pip.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code—this is a co-routine that fetches a URL using a timeout and
    awaits the future, that is, the result of the operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the calling code with the event loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: What are we doing in the preceding code?
  prefs: []
  type: TYPE_NORMAL
- en: We create an event loop and a list of URLs to be fetched. We also create an
    instance of `aiohttp ClientSession` object which is a helper for fetching URLs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a map of tasks by mapping the `fetch_page` function to each of the
    URLs. The session object is passed as first argument to the *fetch_page* function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The tasks are passed to the wait method of `asyncio` with a timeout of `120`
    seconds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loop is run until complete. It returns two sets of futures—`done` and `pending`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We iterate through the future that is done, and print the response by fetching
    it using the `result` method of the `future`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can see the result of the operation (first few lines as many lines are
    output) in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Waiting for a future – async and await](../Images/image00449.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of program doing an async fetch of URLs for 5 URLs
  prefs: []
  type: TYPE_NORMAL
- en: As you can see we are able to print the responses in terms of the a simple summary.
    How about processing the response to get more details about it such as the actual
    response text, the content length, status code, and so on?
  prefs: []
  type: TYPE_NORMAL
- en: The function below parses a list of *done* futures – waiting for the response
    data via *await* on the *read* method of the response*.* This returns the data
    for each response asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The details of the `response` object—the final URL, status code, and length
    of data—are output by this method for each response before closing the response.
  prefs: []
  type: TYPE_NORMAL
- en: We only need to add one more processing step on the list of completed responses
    for this to work.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Note how we chain the co-routines together. The final link in the chain is the
    `parse_response` co-routine, which processes the list of done futures before the
    loop ends.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the output of the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Waiting for a future – async and await](../Images/image00450.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of program doing fetching and response processing of 5 URLs asynchronously
  prefs: []
  type: TYPE_NORMAL
- en: A lot of complex programming can be done using the `asyncio` module. One can
    wait for futures, cancel their execution, and run `asyncio` operations from multiple
    threads. A full discussion is beyond the scope of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We will move on to another model for executing concurrent tasks in Python, namely
    the `concurrent.futures` module.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent futures – high-level concurrent processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `concurrent.futures` module provides high-level concurrent processing using
    either threads or processes, while asynchronously returning data using future
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'It provides an executor interface which exposes mainly two methods, which are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`submit`: Submits a callable to be executed asynchronously, returning a `future`
    object representing the execution of the callable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map`: Maps a callable to a set of iterables, scheduling the execution asynchronously
    in the `future` object. However, this method returns the results of processing
    directly instead of returning a list of futures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two concrete implementations of the executor interface: `ThreadPoolExecutor`
    executes the callable in a pool of threads, and `ProcessPoolExecutor` does so
    in a pool of processes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple example of a `future` object that calculates the factorial
    of a set of integers asynchronously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a detailed explanation of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: The `factorial` function computes the factorial of a given number iteratively
    by using `functools.reduce` and the multiplication operator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We create an executor with two workers, and submit the numbers (from 10 to 20)
    to it via its `submit` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The submission is done via a dictionary comprehension, returning a dictionary
    with the future as the key and the number as the value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We iterate through the completed futures, which have been computed, using the
    `as_completed` method of the `concurrent.futures` module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result is printed by fetching the future's result via the `result` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When executed, the program prints its output, rather in order, as shown in
    the next screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Concurrent futures – high-level concurrent processing](../Images/image00451.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of concurrent futures factorial program
  prefs: []
  type: TYPE_NORMAL
- en: Disk thumbnail generator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our earlier discussion of threads, we used the example of the generation
    of thumbnails for random images from the Web to demonstrate how to work with threads,
    and process information.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will do something similar. Here, rather than processing
    random image URLs from the Web, we will load images from disk, and convert them
    to thumbnails using the `concurrent.futures` function.
  prefs: []
  type: TYPE_NORMAL
- en: We will reuse our thumbnail creation function from before. On top of that, we
    will add concurrent processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, here are the imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is our familiar thumbnail creation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We will process images from a specific folder—in this case, the `Pictures`
    subdirectory of the `home` folder. To process this, we will need an iterator that
    yields image filenames. We have written one next with the help of the `os.walk`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the preceding function is a generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the main calling code, which sets up an executor and runs it over the
    folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code uses the same technique of submitting arguments to a function
    asynchronously, saving the resultant futures in a dictionary and then processing
    the result as and when the futures are finished, in a loop.
  prefs: []
  type: TYPE_NORMAL
- en: To change the executor to use processes, one simply needs to replace `ThreadPoolExecutor`
    with `ProcessPoolExecutor`; the rest of the code remains the same. We have provided
    a simple command-line flag, `--process`, to make this easy.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an output of a sample run of the program using both thread and process
    pools on the `~/Pictures` folder – generating around 2000+ images in roughly the
    same time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Disk thumbnail generator](../Images/image00452.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of concurrent futures disk thumbnail program—using thread and process
    executor
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency options – how to choose?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are at the end of our discussion of concurrency techniques in Python. We
    discussed threads, processes, asynchronous I/O, and concurrent futures. Naturally,
    a question arises—when to pick what?
  prefs: []
  type: TYPE_NORMAL
- en: This question has been already answered for the choice between threads and processes,
    where the decision is mostly influenced by the GIL.
  prefs: []
  type: TYPE_NORMAL
- en: Here are somewhat rough guidelines for picking your concurrency options.
  prefs: []
  type: TYPE_NORMAL
- en: '**Concurrent futures vs Multi-processing:** Concurrent futures provide an elegant
    way to parallelize your tasks using either a thread or process pool executor.
    Hence, it is ideal if the underlying application has similar scalability metrics
    with either threads or processes, since it''s very easy to switch from one to
    the other as we''ve seen in a previous example. Concurrent futures can be chosen
    also when the result of the operation needn''t be immediately available. Concurrent
    futures is a good option when the data can be finely parallelized and the operation
    can be executed asynchronously, and when the operations involve simple callables
    without requiring complex synchronization techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-processing should be chosen if the concurrent execution is more complex,
    and not just based on data parallelism, but has aspects like synchronization,
    shared memory, and so on. For example, if the program requires processes, synchronization
    primitives, and IPC, the only way to truly scale up then is to write a concurrent
    program using the primitives provided by the multiprocessing module.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly when your muti-threaded logic involves simple parallelization of data
    across multiple tasks, one can choose concurrent futures with a thread pool. However
    if there is a lot of shared state to be managed with complex thread synchronization
    objects – one has to use thread objects and switch to multiple threads using `threading`
    module to get finer control of the state.
  prefs: []
  type: TYPE_NORMAL
- en: '**Asynchronous I/O vs Threaded concurrency:** When your program doesn''t need
    true concurrency (parallelism), but is dependent more on asynchronous processing
    and callbacks, then `asyncio` is the way to go. Asyncio is a good choice when
    there are lot of waits or sleep cycles involved in the application, such as waiting
    for user input, waiting for I/O, and so on, and one needs to take advantage of
    such wait or sleep times by yielding to other tasks via co-routines. Asyncio is
    not suitable for CPU-heavy concurrent processing, or for tasks involving true
    data parallelism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AsyncIO seems to be suitable for request-response loops- where a lot of I/O
    happens - so its good for writing web application servers which doesn't have real-time
    data requirements.
  prefs: []
  type: TYPE_NORMAL
- en: You can use these points just listed as rough guidelines when deciding on the
    correct concurrency package for your applications.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel processing libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apart from the standard library modules that we've discussed so far, Python
    is also rich in its ecosystem of third-party libraries, which support parallel
    processing in a **symmetric multi-processing** (**SMP**) or multi-core systems.
  prefs: []
  type: TYPE_NORMAL
- en: We will take a look at a couple of such packages, that are somewhat distinct
    and present some interesting features.
  prefs: []
  type: TYPE_NORMAL
- en: Joblib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`joblib` is a package that provides a wrapper over multiprocessing to execute
    code in loops in parallel. The code is written as a generator expression, and
    interpreted to execute in parallel over CPU cores using multi-processing module
    behind the scenes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, take the following code which calculates square roots for first
    10 numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This preceding code can be converted to run on two CPU cores by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is another example: this is our primality checker that we had written
    earlier to run using multiprocessing rewritten to use the `joblib` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: If you execute and time the preceding code, you will find the performance metrics
    very similar to that of the version using multi-processing.
  prefs: []
  type: TYPE_NORMAL
- en: PyMP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`OpenMP` is an open API, which supports shared memory multi-processing in C/C++
    and Fortran. It uses special work-sharing constructs such as pragmas (special
    instructions to compilers) indicating how to split work among threads or processes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following C code using the `OpenMP` API indicates that the
    array should be initialized in parallel using multiple threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`PyMP` is inspired by the idea behind `OpenMP`, but uses the `fork` system
    call to parallelize code executing in expressions like for loops across processes.
    For this, `PyMP` also provides support for shared data structures like lists and
    dictionaries, and also provides a wrapper for `numpy` arrays.'
  prefs: []
  type: TYPE_NORMAL
- en: We will look at an interesting and exotic example—that of fractals—to illustrate
    how `PyMP` can be used to parallelize code and obtain performance improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NOTE: The PyPI package for PyMP is named pymp-pypi so make sure you use this
    name when trying to install it via pip. Also note that it doesn''t do a good job
    of pulling its dependencies such as numpy, so these have to be installed separately.'
  prefs: []
  type: TYPE_NORMAL
- en: Fractals – the Mandelbrot set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is the code listing of a very popular class of complex numbers,
    which when plotted, produces very interesting fractal geometries: namely, the
    **Mandelbrot set**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code calculates a Mandelbrot set using a certain number of `c`
    and a variable geometry (*width x height*). It is complete with argument parsing
    to produce fractal images of varying geometries, and supports different iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For simplicity's sake, and for producing rather beautiful pics than what Mandelbrot
    usually does, the code takes some liberties, and uses the color scheme of a related
    fractal class, namely, Julia sets.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work ? Here is an explanation of the code .
  prefs: []
  type: TYPE_NORMAL
- en: The `mandelbrot_calc_row` function calculates a row of the Mandelbrot set for
    a certain value of the *y* coordinate for a certain number of maximum iterations.
    The pixel color values for the entire row, from `0` to width `w` for the *x* coordinate,
    is calculated. The pixel values are put into the `Image` object that is passed
    to this function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `mandelbrot_calc_set` function calls the `mandelbrot_calc_row` function
    for all values of the *y* coordinate ranging from `0` to the height `h` of the
    image. An `Image` object (via the **Pillow library**) is created for the given
    geometry (*width x height*), and filled with pixel values. Finally, we save this
    image to a file, and we've got our fractal!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Without further ado, let us see the code in action.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the image that our Mandelbrot program produces for the default number
    of iterations namely 1000.
  prefs: []
  type: TYPE_NORMAL
- en: '![Fractals – the Mandelbrot set](../Images/image00453.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Mandelbrot set fractal image for 1000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: Here is the time it takes to create this image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Fractals – the Mandelbrot set](../Images/image00454.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Timing of single process Mandelbrot program—for 1000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: 'However if you increase the number of iterations – the single process version
    slows down quite a bit. Here is the output when we increase the number of iterations
    by 10X – for 10000 iterations.:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fractals – the Mandelbrot set](../Images/image00455.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Timing of single process Mandelbrot program—for 10,000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the code, we can see that there is an outer for loop in the `mandelbrot_calc_set`
    function, which sets things in motion. It calls `mandelbrot_calc_row` for each
    row of the image ranging from `0` to the height of the function, varied by the
    *y* coordinate.
  prefs: []
  type: TYPE_NORMAL
- en: Since each invocation of the `mandelbrot_calc_row` function calculates one row
    of the image, it naturally fits into a data parallel problem, and can be parallelized
    sufficiently easily.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how to do this using PyMP.
  prefs: []
  type: TYPE_NORMAL
- en: Fractals – Scaling the Mandelbrot set implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use `PyMP` to parallelize the outer for loop across many processes in
    a rewrite of the previous simple implementation of the Mandelbrot set, to take
    advantage of the inherent data parallelism in the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the `PyMP` version of the two functions of the mandelbrot program. The
    rest of the code remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The rewrite mainly involved converting the code to one that builds the mandelbrot
    image line by line, each line of data being computed separately and in a way that
    it can be computed in parallel – in a separate process.
  prefs: []
  type: TYPE_NORMAL
- en: In the single process version, we put the pixel values directly in the image
    in the `mandelbrot_calc_row` function. However, since the new code executes this
    function in parallel processes, we cannot modify the image data in it directly.
    Instead, the new code passes a shared dictionary to the function, and it sets
    the pixel color values in it using the location as `key` and the pixel RGB value
    as `value`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new shared data structure—a shared dictionary—is hence added to the `mandelbrot_calc_set`
    function, which is finally iterated over, and the pixel data, filled, in the `Image`
    object, which is then saved to the final output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use four `PyMP` parallel processes, as the machine has four CPU cores, using
    a with context and enclosing the outer for loop inside it. This causes the code
    to execute in parallel in four cores, each core calculating approximately 25%
    of the rows. The final data is written to the image in the main process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the result timing of the `PyMP` version of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fractals – Scaling the Mandelbrot set implementation](../Images/image00456.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Timing of parallel process mandelbrot program using PyMP—for 10000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: The program is about 33% faster in real time. In terms of CPU usage, you can
    see that the `PyMP` version has a higher ratio of user CPU time to real CPU time,
    indicating a higher usage of the CPU by the processes than the single process
    version.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NOTE: We can write an even more efficient version of the program by avoiding
    the shared data structure image_rows which is used to keep the pixel values of
    the image. This version however uses that to show the features of PyMP. The code
    archives of this book contain two more versions of the program – one that uses
    multiprocessing and another that uses PyMP without the shared dictionary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the output fractal image produced by this run of the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fractals – Scaling the Mandelbrot set implementation](../Images/image00457.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Mandelbrot set fractal image for 10000 iterations using PyMP
  prefs: []
  type: TYPE_NORMAL
- en: You can observe that the colors are different, and this image provides more
    detail and a finer structure than the previous one due to the increased number
    of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling for the Web
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, all the scalability and concurrency techniques we discussed were involved
    with scalability within the confines of a single server or machine—in other words,
    scaling up. In real world, applications also scale by scaling out, that is, by
    spreading their computation over multiple machines. This is how most real-world
    web applications run and scale at present.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at a few techniques, scaling out an application in terms of scaling
    communications/workflows, scaling computation, and horizontal scaling using different
    protocols.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling workflows – message queues and task queues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One important aspect of scalability is to reducing coupling between systems.
    When two systems are tightly coupled, they prevent each other from scaling beyond
    a certain limit.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a code written serially, where data and computation is tied into
    the same function, prevents the program from taking advantage of the existing
    resources like multiple CPU cores. When the same program is rewritten to use multiple
    threads (or processes) and a message passing system like a queue in between, we
    find it scales well to multiple CPUs. We've seen such examples aplenty in our
    concurrency discussion.
  prefs: []
  type: TYPE_NORMAL
- en: In a much similar way, systems over the Web scale better when they are decoupled.
    The classic example is the client/server architecture of the Web itself, where
    clients interact via well-known RestFUL protocols like HTTP, with servers located
    in different places across the world.
  prefs: []
  type: TYPE_NORMAL
- en: Message queues are systems that allow applications to communicate in a decoupled
    manner by sending messages to each other. The applications typically run in different
    machines or servers connected to the Internet, and communicate via queuing protocols.
  prefs: []
  type: TYPE_NORMAL
- en: One can think of a message queue as a scaled-up version of the multi-threaded
    synchronized queue, with applications on different machines replacing the threads,
    and a shared, distributed queue replacing the simple in-process queue.
  prefs: []
  type: TYPE_NORMAL
- en: Message queues carry packets of data called messages, which are delivered from
    the **Sending Applications** to the **Receiving Applications**. Most **Message
    Queue** provide **store and forward** semantics, where the message is stored on
    the queue till the receiver is available to process the message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple schematic model of a **Message Queue**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling workflows – message queues and task queues](../Images/image00458.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Schematic model of a distributed message queue
  prefs: []
  type: TYPE_NORMAL
- en: The most popular and standardized implementation of a message queue or **message-oriented
    middleware** (**MoM**) is the **Advanced Message Queuing Protocol** (**AMQP**).
    AMQP provides features such as queuing, routing, reliable delivery, and security.
    The origins of AMQP are in the financial industry, where reliable and secure message
    delivery semantics are of critical importance.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular implementations of AMQP (version 1.0) are Apache Active MQ,
    RabbitMQ, and Apache Qpid.
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ is a MoM written in Erlang. It provides libraries in many languages
    including Python. In RabbitMQ, a message is always delivered via exchanges via
    routing keys which indicate the queues to which the message should be delivered.
  prefs: []
  type: TYPE_NORMAL
- en: We won't be discussing RabbitMQ in this section anymore, but will move on to
    a related, but slightly different, middleware with a varying focus, namely, Celery.
  prefs: []
  type: TYPE_NORMAL
- en: Celery – a distributed task queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Celery is a distributed task queue written in Python, which works using distributed
    messages. Each execution unit in celery is called a **task**. A task can be executed
    concurrently on one or more servers using processes called **workers**. By default,
    celery achieves this using `multiprocessing`, but it can also use other backend
    such as gevent, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks can be executed synchronously or asynchronously with results available
    in the future, like objects. Also, task results can be stored in storage backend
    such as Redis, databases, or in files.
  prefs: []
  type: TYPE_NORMAL
- en: Celery differs from message queues in that the basic unit in celery is an executable
    task—a callable in Python—rather than just a message.
  prefs: []
  type: TYPE_NORMAL
- en: Celery, however, can be made to work with message queues. In fact, the default
    broker for passing messages in celery is RabbitMQ, the popular implementation
    of AMQP. Celery can also work with Redis as the broker backend.
  prefs: []
  type: TYPE_NORMAL
- en: Since Celery takes a task, and scales it over multiple workers; over multiple
    servers, it is suited to problems involving data parallelism as well as computational
    scaling. Celery can accept messages from a queue and distribute it over multiple
    machines as tasks for implementing a distributed e-mail delivery system, for example,
    and achieve horizontal scalability. Or, it can take a single function and perform
    parallel data computation by splitting the data over multiple processes, achieving
    parallel data processing.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will take our Mandelbrot fractal program and, rewrite
    it to work with Celery. We will try to scale the program by performing data parallelism,
    in terms of computing the rows of the Mandelbrot set over multiple celery workers—in
    a similar way to what we did with `PyMP`.
  prefs: []
  type: TYPE_NORMAL
- en: The Mandelbrot set using Celery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For implementing a program to take advantage of Celery, it needs to be implemented
    as a task. This is not as difficult as it sounds. Mostly, it just involves preparing
    an instance of the celery app with a chosen broker backend, and decorating the
    callable we want to parallelize – using the special decorator `@app.task` where
    *app* is an instance of Celery.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look at this program listing step by step, since it involves a few
    new things. The software requirements for this session are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Celery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AMQP backend; RabbitMQ is preferred
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redis as a result storage backend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First we will provide the listing for the Mandelbrot tasks module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us analyze this preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: We first do the imports required for celery. This requires importing the `Celery`
    class from the `celery` module.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We prepare an instance of the `Celery` class as the celery app using AMQP as
    the message broker and Redis as the result backend. The AMQP configuration will
    use whatever AMQP MoM is available on the system (in this case, it is RabbitMQ).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a modified version of `mandelbrot_calc_row`. In the `PyMP` version,
    the `image_rows` dictionary was passed as an argument to the function. Here, the
    function calculates it locally and returns a value. We will use this return value
    at the receiving side to create our image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We decorated the function using `@app.task`, where app is the `Celery` instance.
    This makes it ready to be executed as a celery task by the celery workers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next is the main program, which calls the task for a range of `y` input values
    and creates the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The argument parser is the same so is not reproduced here.
  prefs: []
  type: TYPE_NORMAL
- en: 'This last bit of code introduces some new concepts in celery, so needs some
    explanation. Let us analyze the code in some detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The `mandelbrot_main` function is similar to the previous `mandelbrot_calc_set`
    function in its arguments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This function sets up a group of tasks, each performing `mandelbrot_calc_row`
    execution on a given `y` input over the entire range of `y` inputs from `0` to
    the height of the image. It uses the `group` object of celery to do this. A group
    is a set of tasks which can be executed together.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The tasks are executed by calling the `apply_async` function on the group. This
    executes the tasks asynchronously in the background in multiple workers. We get
    an async `result` object in return—the tasks are not completed yet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then wait on this result object by calling `join` on it, which returns the
    results—the rows of the image as a dictionary from each single execution of the
    `mandelbrot_calc_row` task. We loop through this, and do integer conversions for
    the values, since celery returns data as strings, and put the pixel values in
    the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the image is saved in the output file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So how does celery execute the tasks? This needs the celery program to run,
    processing the tasks module with a certain number of workers. Here is how we start
    it in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mandelbrot set using Celery](../Images/image00459.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Celery console—workers starting up with the Mandelbrot task as target
  prefs: []
  type: TYPE_NORMAL
- en: The command starts celery with tasks loaded from the module `mandelbrot_tasks.py`
    with a set of 4 worker processes. Since the machine has 4 CPU cores, we have chosen
    this as the concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that Celery will automatically default the workers to the number of cores
    if not specifically configured.
  prefs: []
  type: TYPE_NORMAL
- en: The program ran under 15 seconds, twice as faster than the single-process version
    and also the `PyMP` version.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you observe the celery console, you will find a lot of messages getting
    echoed, since we configured celery with the `INFO` log level. All these are info
    messages with data on the tasks and their results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the result of the run for `10000` iterations.
    This performance is slightly better than that of the similar run by the `PyMP`
    version earlier, by around 20 seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mandelbrot set using Celery](../Images/image00460.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Celery Mandelbrot program for a set of 10000 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Celery is used in production systems in many organizations. It has plugins for
    some of the more popular Python web application frameworks. For example, celery
    supports Django out-of-the-box with some basic plumbing and configuration. There
    are also extension modules such as `django-celery-results`, which allow the programmer
    to use the Django ORM as celery results backend.
  prefs: []
  type: TYPE_NORMAL
- en: It is beyond the scope of this chapter and book to discuss this in detail so
    the reader is suggested to refer to the documentation available on this on the
    celery project website.
  prefs: []
  type: TYPE_NORMAL
- en: Serving with Python on the Web—WSGI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Web Server Gateway Interface** (**WSGI**) is a specification for a standard
    interface between Python web application frameworks and web servers.'
  prefs: []
  type: TYPE_NORMAL
- en: In the early days of Python web applications, there was a problem of connecting
    web application frameworks to web servers, since there was no common standard.
    Python web applications were designed to work with one of the existing standards
    of CGI, FastCGI, or `mod_python` (Apache). This meant that an application written
    to work with one web server might not be able to work with another. In other words,
    interoperability between the uniform application and web server was missing.
  prefs: []
  type: TYPE_NORMAL
- en: WSGI solved this problem by specifying a simple, but uniform, interface between
    servers and web application frameworks to allow for portable web application development.
  prefs: []
  type: TYPE_NORMAL
- en: 'WSGI specifies two sides: the server (or gateway) side, and the application
    or framework side. A WSGI request gets processed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The server side executes the application, providing it with an environment and
    a callback function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application processes the request, and returns the response to the server
    using the provided callback function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a schematic diagram showing the interaction between a web server and
    web application using WSGI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Serving with Python on the Web—WSGI](../Images/image00461.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Schematic diagram showing WSGI protocol interaction
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the simplest function that is compatible with the application
    or framework side of WSGI is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `environ` variable is a dictionary of environment variables passed from
    the server to the application as defined by the **Common Gateway Interface** (**CGI**)
    specification. WSGI makes a few of these environment variables mandatory in its
    specification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `start_response` is a callable provided as a callback from the server side
    to the application side to start response processing on the server side. It must
    take two positional arguments. The first should be a status string with an integer
    status code, and the second, a list of (`header_name`, `header_value`), tuples
    describing the HTTP response header.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more details, the reader can refer to the WSGI specification v1.0.1, which
    is published on the Python language website as PEP 3333.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Python Enhancement Proposal** (**PEP**) is a design document on the Web,
    that describes a new feature or feature suggestion for Python, or provides information
    to the Python community about an existing feature. The Python community uses PEPs
    as a standard process for describing, discussing, and adopting new features and
    enhancements to the Python programming language and its standard library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'WSGI middleware components are software that implement both sides of the specification,
    and hence, provide capabilities such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing of multiple requests from a server to an application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remote processing of requests by forwarding requests and responses over a network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-tenancy or co-hosting of multiple servers and/or applications in the same
    process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL-based routing of requests to different application objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The middleware sits in between the server and application. It forwards requests
    from server to the application and responses from application to the server.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of WSGI middleware an architect can choose from. We will
    briefly look at two of the most popular ones, namely, uWSGI and Gunicorn.
  prefs: []
  type: TYPE_NORMAL
- en: uWSGI – WSGI middleware on steroids
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: uWSGI is an open source project and application, which aims to build a full
    stack for hosting services. The WSGI of the uWSGI project stems from the fact
    that the WSGI interface plugin for Python was the first one developed in the project.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from WSGI, the uWSGI project also supports **Perl Webserver Gateway Interface**
    (**PSGI**) for Perl web applications, and the rack web server interface for Ruby
    web applications. It also provides gateways, load balancers, and routers for requests
    and responses. The Emperor plugin of uWSGI provides management and monitoring
    of multiple uWSGI deployments of your production system across servers.
  prefs: []
  type: TYPE_NORMAL
- en: The components of uWSGI can run in preforked, threaded, asynchronous. or green-thread/co-routine
    modes.
  prefs: []
  type: TYPE_NORMAL
- en: uWSGI also comes with a fast and in-memory caching framework, which allows the
    responses of the web applications to be stored in multiple caches on the uWSGI
    server. The cache can also be backed with a persistence store such as a file.
    Apart from a multitude of other things, uWSGI also supports virtualenv based deployments
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: uWSGI also provides a native protocol, that is used by the uWSGI server. uWSGI
    version 1.9 also adds native support for the web sockets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a typical example of a uWSGI configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'A typical deployment architecture with uWSGI looks like what is depicted in
    the following diagram. In this case, the web server is Nginx, and the web application
    framework is Django. uWSGI is deployed in a reverse-proxy configuration with Nginx,
    forwarding request and responses between Nginx and Django:'
  prefs: []
  type: TYPE_NORMAL
- en: '![uWSGI – WSGI middleware on steroids](../Images/image00462.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: uWSGI deployment with Nginx and Django
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Nginx web server supports a native implementation of the uWSGI protocol
    since version 0.8.40\. There is also a proxy module support for uWSGI in Apache
    named `mod_proxy_uwsgi`.
  prefs: []
  type: TYPE_NORMAL
- en: uWSGI is an ideal choice for Python web application production deployments where
    one needs a good balance of customization with high performance and features.
    It is the swiss-army-knife of components for WSGI web application deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Gunicorn – unicorn for WSGI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Gunicorn project is another popular WSGI middleware implementation, which
    is opensource. It uses a preforked model, and is a ported version from the unicorn
    project of Ruby. There are different worker types in Gunicorn, like uWSGI supporting
    synchronous and asynchronous handling of requests. The asynchronous workers make
    use of the `Greenlet` library which is built on top of gevent.
  prefs: []
  type: TYPE_NORMAL
- en: There is a master process in Gunicorn that runs an event loop, processing and
    reacting to various signals. The master manages the workers, and the workers process
    the requests, and send responses.
  prefs: []
  type: TYPE_NORMAL
- en: Gunicorn versus uWSGI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few guidelines when choosing whether to go with Gunicorn or uWSGI
    for your Python web application deployments:'
  prefs: []
  type: TYPE_NORMAL
- en: For simple application deployments which don't need a lot of customization,
    gunicorn is a good choice. uWSGI has a bigger learning curve when compared to
    Gunicorn, and takes a while to get used to. The defaults in Gunicorn work pretty
    well for most deployments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your deployment is homogenously Python, then Gunicorn is a good choice. On
    the other hand, uWSGI allows you to perform heterogeneous deployments due to its
    support for other stacks such as PSGI and Rack.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want a more full-featured WSGI middleware, which is heavily customizable,
    then uWSGI is a safe bet. For example, uWSGI makes Python virtualenv-based deployments
    simple, whereas, Gunicorn doesn't natively support virtualenv; instead, Gunicorn
    itself has to be deployed in the virtual environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since Nginx supports uWSGI natively, it is very commonly deployed along with
    Nginx on production systems. Hence, if you use Nginx, and want a full-featured
    and highly customizable WSGI middleware with caching, uWSGI is the default choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With respect to performance, both Gunicorn and uWSGI score similarly on different
    benchmarks published on the Web.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed, a system can scale vertically, or horizontally, or both. In this
    section, we will briefly look at a few of the architectures that an architect
    can choose from when deploying his systems to production to take advantage of
    the scalability options.
  prefs: []
  type: TYPE_NORMAL
- en: Vertical scalability architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Vertical scalability techniques comes in the following two flavors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding more resources to an existing system**: This could mean adding more
    RAM to a physical or virtual machine, adding more vCPUs to a virtual machine or
    VPS, and so on. However, none of these options are dynamic, as they require stopping,
    reconfiguring, and restarting the instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Making better use of existing resources in the system**: We have spent a
    lot of this chapter discussing this approach. This is when an application is rewritten
    to make use of the existing resources, such as multiple CPU cores, more effectively
    by concurrency techniques such as threading, multiple processes, and/or asynchronous
    processing. This approach scales dynamically, since no new resource is added to
    the system, and hence, there is no need for a stop/start.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal scalability architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Horizontal scalability involves a number of techniques that an architect can
    add to his tool box, and pick and choose from. They include the ones listed next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Active redundancy**: This is the simplest technique of scaling out, which
    involves adding multiple, homogenous processing nodes to a system typically fronted
    with a load balancer. This is a common practice for scaling out web application
    server deployments. Multiple nodes make sure that an even if one or a few of the
    systems fail, the remaining systems continue to carry out request processing,
    ensuring no downtime for your application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a redundant system, all the nodes are actively in operation, though only
    one or a few of them may be responding to requests at a specific time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hot standby**: A hot standby (hot spare) is a technique used to switch to
    a system that is ready to server requests, but is not active till the moment the
    main system go down. A hot spare is in many ways exactly similar to the main node(s)
    that is serving the application. In the event of a critical failure, the load
    balancer is configured to switch to the hot spare.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hot spare itself may be a set of redundant nodes instead of just a single
    node. Combining redundant systems with a hot spare ensures maximum reliability
    and failover.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A variation of a hot standby is a software standby, which provides a mode in
    the application that switches the system to a minimum **Quality of Service** (**QoS**)
    instead of offering the full feature at extreme load. An example is a web application
    that switches to the read-only mode under high loads, serving most users but not
    allowing writes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Read replicas**: The response of a system that is dependent on read-heavy
    operations on a database can be improved by adding read-replicas of the database.
    Read replicas are essentially database nodes that provide hot backups (online
    backups), which constantly sync from the main database node. Read replicas, at
    a given point of time, may not be exactly consistent with the main database node,
    but they provide eventual consistency with SLA guarantees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud service providers such as Amazon make their RDS database service available
    with a choice of read replicas. Such replicas can be distributed geographically
    closer to your active user locations to ensure less response time and failover
    in case the master node goes down, or doesn't respond.
  prefs: []
  type: TYPE_NORMAL
- en: Read replicas basically offer your system a kind of data redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: '**Blue-green deployments**: This is a technique where two separate systems
    (labeled `blue` and `green` in the literature) are run side by side. At any given
    moment, only one of the systems is active and is serving requests. For example,
    blue is *active*, green is *idle*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When preparing a new deployment, it is done on the idle system. Once the system
    is ready, the load balancer is switched to the idle system (green), and away from
    the active system (blue). At this point, green is active, and blue is idle. The
    positions are reversed again in the next switch.
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green deployments, if done correctly, ensure zero to minimum downtime of
    your production applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Failure monitoring and/or restart**: A failure monitor is a system that detects
    failure of critical components—software or hardware—of your deployments, and either
    notifies you, and/or takes steps to mitigate the downtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, you can install a monitoring application on your server that detects
    when a critical component, say, a celery or rabbitmq server, goes down, sends
    an e-mail to the DevOps contact, and also tries to restart the daemon.
  prefs: []
  type: TYPE_NORMAL
- en: Heartbeat monitoring is another technique where a software actively sends pings
    or heartbeats to a monitoring software or hardware, which could be in the same
    machine or another server. The monitor will detect the downtime of the system
    if it fails to send the heartbeat after a certain interval, and could then inform
    and/or try to restart the component.
  prefs: []
  type: TYPE_NORMAL
- en: Nagios is an example of a common production monitoring server, usually deployed
    in a separate environment, and monitors your deployment servers. Other examples
    of system-switch monitors and restart components are **Monit** and **Supervisord**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from these techniques, the following best practices should be followed
    when performing system deployments to ensure scalability, availability, and redundancy/failover:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cache it**: Use caches, and if possible, distributed caches, in your system
    as much as possible. Caches can be of various types. The simplest possible cache
    is caching static resources on the **content delivery network** (**CDN**) of your
    application service provider. Such a cache ensures geographic distribution of
    resources closer to your users, which reduces response, and hence, page-load times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A second kind of cache is your application's cache, where it caches responses
    and database query results. Memcached and Redis are commonly used for these scenarios,
    and they provide distributed deployments, typically, in master/slave modes. Such
    caches should be used to load and cache most commonly requested content from your
    application with proper expiry times to ensure that the data is not too stale.
  prefs: []
  type: TYPE_NORMAL
- en: 'Effective and well-designed caches minimize system load, and avoid multiple,
    redundant operations that can artificially increase load on a system and decrease
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decouple**: As much as possible, decouple your components to take advantage
    of the shared geography of your network. For example, a message queue may be used
    to decouple components in an application that need to publish and subscribe data
    instead of using a local database or sockets in the same machine. When you decouple,
    you automatically introduce redundancy and data backup to your system, since the
    new components you add for decoupling—message queues, task queues, and distributed
    caches—typically come with their own stateful storage and clustering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The added complexity of decoupling is the configuration of the extra systems.
    However, in this day and age, with most systems being able to perform auto configuration
    or providing simple web-based configurations, this is not an issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to literature for application architectures that provide effective
    decoupling, such as observer patterns, mediators, and other such middleware:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gracefully degrade**: Rather than being unable to answer a request and providing
    timeouts, arm your systems with graceful degradation behaviors. For example, a
    write-heavy web application can switch to the read-only mode under heavy load
    when it finds that the database node is not responding. Another example is when
    a system which provides heavy, JS-dependent dynamic web pages could switch to
    a similar static page under heavy loads on the server when the JS middleware is
    not responding well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graceful degradation can be configured on the application itself, or on the
    load balancers, or both. It is a good idea to prepare your application itself
    to provide a gracefully downgraded behavior, and configure the load balancer to
    switch to that route under heavy loads.
  prefs: []
  type: TYPE_NORMAL
- en: '**Keep data close to the code:** A golden rule of performance-strong software
    is to provide data closer to where the computation is. For example, if your application
    is making 50 SQL queries to load data from a remote database for every request,
    then you are not doing this correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing data close to the computation reduces data access and transport times,
    and hence, processing times, decreasing latency in your application, and making
    it more scalable.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different techniques for this: caching, as discussed earlier, is
    a favored technique. Another one is to split your database to a local and remote
    one, where most of the reads happen from the local read replica, and writes (which
    can take time) happen to a remote write master. Note that local in this sense
    may not mean the same machine, but typically, the same data center, sharing the
    same subnet if possible.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, common configurations can be loaded from an on-disk database like SQLite
    or local JSON files, reducing the time it takes for preparing the application
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: Another technique is to not store any transactional state in the application
    tier or the frontend, but to move the state closer to the backend where the computation
    is. Since this makes all application server nodes equal in terms of not having
    any intermediate state, it also allows you to front them with a load-balancer,
    and provide a redundant cluster of equals, any of which can serve a given request.
  prefs: []
  type: TYPE_NORMAL
- en: '**Design according to SLAs**: It is very important for an architect to understand
    the guarantees that the application provides to its users, and design the deployment
    architecture accordingly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CAP theorem ensures that if a network partition in a distributed system
    fails, the system can guarantee only one of consistency or availability at a given
    time. This groups distributed systems into two common types, namely, CP and AP
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Most web applications in today's world are AP. They ensure availability, but
    data is only eventually consistent, which means they will serve stale data to
    users in case one of the systems in the network partition, say the master DB node,
    fails.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand. a number of businesses such as banking, finance, and healthcare
    need to ensure consistent data even if there is a network partition failure. These
    are CP systems. The data in such systems should never be stale, so, in case of
    a choice between availability and consistent data, they will chose the latter.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of software components, application architecture, and the final deployment
    architecture are influenced by these constraints. For example, an AP system can
    work with NoSQL databases which guarantee eventual consistent behavior. It can
    make better use of caches. A CP system, on the other hand, may need ACID guarantees
    provided by **Relational Database Systems** (**RDBMs**).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reused a lot of ideas and concepts that you learned in the
    previous chapter on performance.
  prefs: []
  type: TYPE_NORMAL
- en: We started with a definition of scalability, and looked at its relation with
    other aspects like concurrency, latency, and performance. We briefly compared
    and contrasted concurrency and its close cousin parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: We then went on to discuss various concurrency techniques in Python with detailed
    examples and performance comparisons. We used a thumbnail generator with random
    URLs from the Web as an example to illustrate the various techniques of implementing
    concurrency using multi-threading in Python. You also learned and saw an example
    of the producer/consumer pattern, and using a couple of examples, learned how
    to implement resource constraints and limits using synchronization primitives.
  prefs: []
  type: TYPE_NORMAL
- en: Next we discussed how to scale applications using multi-processing and saw a
    couple of examples using the `multiprocessing` module – such as a primality checker
    which showed us the effects of `GIL` on multiple threads in Python and a disk
    file sorting program which showed the limits of multi-processing when it comes
    to scaling programs using a lot disk I/O .
  prefs: []
  type: TYPE_NORMAL
- en: We looked at asynchronous processing as the next technique of concurrency. We
    saw a generator based co-operative multitasking scheduler and also its counterpart
    using `asyncio`. We saw couple of examples using asyncio and learned how to perform
    URL fetches using the aiohttp module asynchronously. The section on concurrent
    processing compared and contrasted concurrent futures with other options on concurrency
    in Python while sketching out a couple of examples.
  prefs: []
  type: TYPE_NORMAL
- en: We used Mandelbrot fractals as an example to show how to implement data parallel
    programs and showed an example of using `PyMP` to scale a mandelbrot fractal program
    across multiple processes and hence multiple cores.
  prefs: []
  type: TYPE_NORMAL
- en: Next we went on to discuss how to scale your programs out on the Web. We briefly
    discussed the theoretical aspect of message queues and task queues. We looked
    at celery, the Python task queue library, and rewrote the Mandelbrot program to
    scale using celery workers, and did performance comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: WSGI, Python's way of serving web applications over web servers, was the next
    topic of discussion. We discussed the WSGI specification, and compared and contrasted
    two popular WSGI middleware, namely, uWSGI and Gunicorn.
  prefs: []
  type: TYPE_NORMAL
- en: Towards the end of the chapter, we discussed scalability architectures, and
    looked at the different options of scaling vertically and horizontally on the
    Web. We also discussed at some best practices an architect should follow while
    designing, implementing, and deploying distributed applications on the web for
    achieving high scalability.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we discuss the aspect of Security in software architecture
    and discuss aspects of security the architect should be aware of and strategies
    for making your applications secure.
  prefs: []
  type: TYPE_NORMAL
