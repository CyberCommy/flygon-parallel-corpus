- en: Chapter 4. CPU Profiling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Profiling is boring, but it's a good form of software analysis where you measure
    resource usage. This usage is measured over time and sometimes under specific
    workloads. Resources can mean anything the application is using, be it memory,
    disk, network, or processor. More specifically, CPU profiling allows you to analyze
    how and how much your functions use the processor. You can also analyze the opposite—the
    non-usage of the processor, or the idle time.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Node.js is not primarily meant for continuous CPU-intensive tasks, and sometimes,
    for profiling, it is important to identify the methods of the intensive task that
    are holding to the processor and keeping other tasks from performing better. You
    may find huge call stacks continuously occupying the processor or repetitive and
    recursive tasks not ending as you expected. There are several techniques, such
    as splitting and scheduling tasks instead of continuously running them as they
    block the event loop.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: You may ask why these tasks are so horrible. The answer is simple; Node.js runs
    around an event loop, which means that when your code ends a specific task, the
    loop restarts and pending events get dispatched. If your code does not end, the
    rest of the application will be kept in standby until the task finishes. You need
    to be able to split a big task into smaller ones for your application to perform
    well.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The main goal of an application should be to use the least resources possible,
    so using the least processor time possible would be ideal. This is equivalent
    to be running most of the time idle in the main thread. This is when the call
    stack is the smallest possible. From a basic Node.js perspective, that should
    be level zero.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: When profiling the processor, we usually take samples of the call stack at a
    certain frequency and analyze how the stack changes (increases or decreases) over
    the sampling period. If you use profilers from the operating system, you'll have
    more items in the stack than you probably expect, as you'll get internal calls
    of Node.js and V8.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'In the chapter, the following topics will be covered:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: The I/O library
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fibonacci
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flame graphs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling alternatives
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The I/O library
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The library used by Node.js to be able to perform asynchronous I/O operations
    across multiple platform environments is **libuv**. This is an open source library.
    Actually, It is used by platforms to provide similar functionality to other languages
    such as Luvit and Lua. **Libuv** is a cross-platform library that uses the best
    possible methods for each platform to achieve the best I/O performance and still
    exposes a common API.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'This library is responsible for network tasks (TCP and UDP sockets), DNS requests,
    filesystem operations, and much more. It''s the library that accesses files, lists
    directory contents, listens for socket connections, and executes child processes.
    The following image shows how Node.js uses V8 and libuv at the same level:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![The I/O library](img/4183_04_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: 'You can see that libuv does not depend on V8 to interact with I/O. It''s a
    C library with its own thread pool. This thread pool is designed to be fast and
    avoid creating and destroying task threads too often, as they''re very expensive.
    The library handles many I/O tasks from the network to the filesystem. It''s responsible
    for Node.js exposing `fs`, `net`, `dns`, and many more APIs. During an event loop,
    your code can request I/O data. This is processed, and when ready (that is, all
    or part of your request is ready for you), it triggers an event that will hopefully
    be handled in the next event loop. The following image describes how the thread
    pool works. Your code runs in the event loop (green), libuv runs in separate threads
    (blue) and triggers events to your code (orange) that get triggered before each
    cycle:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![The I/O library](img/4183_04_02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: This means that if you request a file's content and start performing a lot of
    intensive operations, it doesn't affect the file operation since it's done outside
    your scope. So, although Node.js is single threaded, there are several operations
    that are done in separate threads (from a pool). This is important to remember
    as we profile our code so as to differentiate what a Node.js bottleneck, a libuv
    (I/O) bottleneck, and just a system bottleneck are.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Fibonacci
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s dive into an example. Take it with a grain of salt. It''s actually a
    very common and criticized example, involving the Fibonacci sequence. Let''s create
    a simple HTTP server file called `fib.js` that will answer every request with
    a response based on the sum of the numbers of a Fibonacci sequence of a specific
    length. There are no dependencies here, just plain Node.js. Additionally, we''ll
    use the `ab` command (Apache Benchmark) to make a few requests to our server.
    If you have a Debian-based machine, you just need to install `apache2-utils` to
    be able to use this command:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As you can see, the `fibonacci` function is recursive (as it should be), and
    is called every time a new request comes in. It should not be a surprise to see
    that this won''t perform that well. Let''s start it and tell V8 that we want a
    profile log:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now let''s benchmark it with just 10 requests with two concurrency connections.
    The following output has been truncated for clearer understanding:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can see that it took 2 seconds for each request (half a request per second).
    That doesn't look good, does it? Let's stop the server. You should see an `isolate*.log`
    file in the same folder. You can open it with V8 Tick Processor. There's an online
    version ([http://v8.googlecode.com/svn/trunk/tools/tick-processor.html](http://v8.googlecode.com/svn/trunk/tools/tick-processor.html)),
    if you want; or if you have the node source as I do, you will find it in `deps/v8/tools/tick-processor.html`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![Fibonacci](img/4183_04_03.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: 'Click on **Choose File** and pick your log. The tool will chew like process,
    throw like return output similar to the following. Once more, some of the output
    has been removed:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Our `fibonacci` function is really using our processor all the time. You can
    notice the recursive pattern in the `Bottom up (heavy) profile` section. You can
    see different levels (indentations) because of the recursiveness of the function.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that when running your own test, you should restrict running the
    server to only the time of the benchmark (as in this example). If you leave the
    server running more than that, the use of your function will get mixed with the
    idle time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: In our example, it's not easy or even better to split the code because the operation
    is really simple (adding two numbers). In other use cases, you may be able to
    optimize some operations by modifying your code using, for example, some of the
    techniques shown in [Chapter 2](ch02.html "Chapter 2. Development Patterns"),
    *Development Patterns*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way of improving performance in this case is by using a technique called
    **memoizing**. What this does is wrap a function and cache its return value based
    on the arguments. This means that a function, for a specific set of parameters,
    will only be called once, and then the return value will be cached and used repeatedly.
    Of course, this does not apply to every situation. Let''s try it on our server:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: There are modules that help you achieve this result. In our case, we're adding
    a `memoizing` function and actually overwriting the function with itself—`memoized`.
    This is important because the function calls itself recursively, and so it really
    needs to be overwritten.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'This will cache every call to it, so only the first `fibonacci(40)` call will
    not use the cache. Moreover, since the function calls itself with *n-1* and *n-2*,
    half of the calls will be cached, so the first call will be even faster. Running
    an `ab` test will get you very different results:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This is much better at more than 250 requests per second. This is obviously
    a bad benchmark because if you increase the number of requests to a couple of
    thousands, the number will be even better (a couple of thousands). If you use
    V8 Tick Processor, you will notice that the function call is no longer there:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is obviously a bad and very simple example. Every application is different
    and analyzing it will involve knowing more about it. Using development platforms
    helps centralize your knowledge of the subject and helps you improve more easily
    overtime.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![Fibonacci](img/4183_04_04.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: Flame graphs
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flame graphs are a visualization technique used to profile an application and
    rapidly and more precisely spot the most frequently used functions. These graphs
    replace or complement the previous log text output, as they give a more pleasant
    and simple way of profiling.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: A flame graph is composed of several stacked blocks, each representing a function
    call. It usually shows usage times horizontally (not necessarily in an order).
    When a function is called by another function, the first function is displayed
    on top of the former one. Using this rule, you can figure out that the blocks
    at the top will definitely be smaller (horizontally) than the ones at the bottom.
    This creates a graph that visually resembles a flame. Moreover, the blocks normally
    use warm colors (such as red and orange), so the graph really looks like flames.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: These can be used with different objectives, such as seeing memory usage and
    leaks. For example, you can create a flame graph to see how the CPU is being used
    (A busy CPU is one that is working hard, nonstop. An idle CPU is one that is doing
    nothing). Another good use is to see when your application is idle and I/O is
    very slow compared to CPU and memory, it's normal when applications block (stop)
    waiting for a file from disk or from the network. This is called off-CPU. This
    is better seen in cold colors (blue and green). A mix of the two CPU flame graphs
    can also give you a good understanding of how your application behaves.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating flame graphs is not easy on Node.js yet, and it depends on your system.
    Since V8 has `perf_events` support ([https://codereview.chromium.org/70013002](https://codereview.chromium.org/70013002)),
    I currently find it much easier to do it on a Linux box using the `perf` command
    and `perf_events`, but you have alternatives, such as DTrace ([http://www.brendangregg.com/flamegraphs.html](http://www.brendangregg.com/flamegraphs.html)).
    Let''s try it right now. Get yourself an Ubuntu machine (or a virtual machine)
    and install some dependencies. Note that some of them depend on your current kernel
    version:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now let''s run our node server telling V8 that we want the `perf_events` output.
    This time, let''s run it in the background so that we can see its PID more easily,
    and run `perf` afterwards:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'There''s the PID we need—`30462`. Then let''s run `perf` to collect events
    for about a minute. The command will not return until it finishes (listening for
    events for a minute), so you need to open another console to run the benchmark
    command:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We're telling perf to record events with a frequency of `99` Hz for the `30462`
    process, enabling call graphs (`-g`), and do this for `60` seconds. After that
    time, this command should end. The first version of code is so slow that will
    take longer than 60 seconds to finish so the user can stop it after a minute.
    The second version is much faster and there's no need to do it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'You can look at the directory and notice that there''s a `perf.data` file.
    Now we need to tell `perf` to read this file and display the trace output. We''ll
    use it and convert it into a flame graph. For this, we''ll use a stack trace visualizer
    created by Brendan Gregg. This output will be converted into an SVG file. You
    can then open it in your favorite browser. First let''s get this stack output:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let''s download the stack trace visualizer and use it to convert this file.
    You''ll need `git` to get this command:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You should now have a `stack01.svg` file that you can interact with. You can
    click on a horizontal block to zoom into that level or click on the lowest block
    to reset zoom. For the first version of your server, you should get something
    similar to this graph:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![Flame graphs](img/4183_04_05.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'You can clearly see the recursive pattern that is pushing the flames higher.
    There''s an initial big flame and there are others next to it. If you click on
    the base of the second flame, you''ll see something similar to the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![Flame graphs](img/4183_04_06.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: Now you can clearly see your processor being exhausted by this inefficient and
    recursive function. When inspecting the flame graph, take a look at the bottom
    line. It will display the information we saw in the initial outputs of the log
    processor, such as usage percentage.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are using the second server version, we''ll need to increase the benchmark
    load if we want to see anything useful. Try creating the flame graph for the second
    server version using the following steps:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now open this new SVG in your browser and see how the flames are thinner. This
    means that although the stack size may be large, the duration of that stack size
    is short. Something similar to this is more normal:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![Flame graphs](img/4183_04_07.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: At the bottom of the graph, you'll always see `node` or `main` as Node.js spends
    most of the time on the main thread. On top of the node or main, you'll see other
    lines. Every stacked line means a call by the line below. As you reach the top
    of the flame, you'll start seeing actual JavaScript code. You'll find many calls
    to the internal functions of Node.js related to events and the `libuv` tasks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a rule of thumb, a flame graph with a huge and wide flame means excessive
    CPU usage. A flame graph with high but thinner flames means low CPU usage.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Profiling alternatives
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are other alternatives for profiling your application's processor usage
    depending on the operating system. You can try DTrace if you use a supported system.
    I won't recommend using it just yet on a Linux box. Moreover, if you're not using
    an Illumos-based system, you might just forget it, at least for Node.js. Linux
    has more call stack debugging tools that you can use to log stacks and then produce
    a flame graph.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Node.js has profiling modules and even call stack trace modules, but I recommend
    that you avoid debugging them at the language level and go for the operating system
    level. It's much faster, is less intrusive to your code, and usually gives you
    a bigger picture of the behavior, or bad behavior, that you're trying to profile.
    Remember that the system is not just your application and there can be other factors
    outside your stack scope that influence your performance.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: You can use flame graphs for other types of data. For example, you can trace
    device I/O `calls` or `syscalls`. You can filter a trace to specific function
    calls to see when and for how long a function is used. You can trace memory allocations,
    and instead of gathering allocation calls, you can gather the allocation size
    in bytes. There are many uses for this type of graph, as it can be really handy
    for visually analyzing your application behavior.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In environments seen nowadays, it's very important to be able to profile an
    application to identify bottlenecks, especially at the processor and memory levels.
    Systems are complex and divided into several layers, so analyzing processor usage
    using call stacks can be really hard without some tools and visualization techniques,
    such as flame graphs. Overall, you should focus on your code quality before going
    for profiling.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: As you saw in our example, a simple and effective solution for our server was
    to cache the results. Caching is a very important technique and is usually crucial
    in balancing resource usage. Normally, you have available memory and it's better
    to cache a result for a small period of time than to process it every time, specially
    when the result is imutable.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在我们的示例中看到的，对于我们的服务器来说，一个简单而有效的解决方案是缓存结果。缓存是一种非常重要的技术，通常在平衡资源使用方面至关重要。通常情况下，您有可用的内存，最好将结果缓存一小段时间，而不是每次都处理它，特别是当结果是不可变的时候。
- en: Next, we'll look at how you should use and store your data and how, when, and
    for how long you should cache it. We'll take a look at the pros and cons of some
    cache methodologies so that you can be more prepared to choose your own path to
    making your application the most performant application possible.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看您应该如何使用和存储数据，以及何时以及多长时间应该对其进行缓存。我们将研究一些缓存方法论的利弊，这样您就可以更好地选择自己的路径，使您的应用程序成为可能的最高性能应用程序。
- en: 'Prepared for Bentham Chang, Safari ID bentham@gmail.com User number: 2843974
    © 2015 Safari Books Online, LLC. This download file is made available for personal
    use only and is subject to the Terms of Service. Any other use requires prior
    written consent from the copyright owner. Unauthorized use, reproduction and/or
    distribution are strictly prohibited and violate applicable laws. All rights reserved.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为Bentham Chang准备，Safari ID为bentham@gmail.com 用户编号：2843974 © 2015 Safari Books
    Online，LLC。此下载文件仅供个人使用，并受到服务条款的约束。任何其他用途均需版权所有者的事先书面同意。未经授权的使用、复制和/或分发严格禁止，并违反适用法律。保留所有权利。
