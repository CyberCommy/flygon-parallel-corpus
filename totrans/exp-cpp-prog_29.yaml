- en: Parallelism and Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Automatically parallelizing code that uses standard algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting a program to sleep for specific amounts of time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting and stopping threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing exception-safe shared locking with `std::unique_lock` and `std::shared_lock`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding deadlocks with `std::scoped_lock`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronizing concurrent `std::cout` use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Safely postponing initialization with `std::call_once`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pushing the execution of tasks into the background using `std::async`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the producer/consumer idiom with `std::condition_variable`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the multiple producers/consumers idiom with `std::condition_variable`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelizing the ASCII Mandelbrot renderer using `std::async`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a tiny automatic parallelization library with `std::future`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before C++11, C++ didn't have much support for parallelization. This does not
    mean that starting, controlling, stopping, and synchronizing threads was not possible,
    but it was necessary to use operating system-specific libraries because threads
    are inherently operating system-related.
  prefs: []
  type: TYPE_NORMAL
- en: With C++11, we got `std::thread`, which enables basic portable thread control
    across all operating systems. For synchronizing threads, C++11 also introduced
    mutex classes and comfortable RAII-style lock wrappers. In addition to that, `std::condition_variable`
    allows for flexible event notification between threads.
  prefs: []
  type: TYPE_NORMAL
- en: Some other really interesting additions are `std::async` and `std::future`--we
    can now wrap arbitrary normal functions into `std::async` calls in order to execute
    them asynchronously in the background. Such wrapped functions return `std::future`
    objects that promise to contain the result of the function later, so we can do
    something else before we wait for its arrival.
  prefs: []
  type: TYPE_NORMAL
- en: Another actually enormous improvement to the STL are *execution policies*, which
    can be added to 69 of the already *existing* algorithms. This addition means that
    we can just add a single execution policy argument to the existing standard algorithm
    calls in our old programs and get parallelization without complex rewrites.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will go through all these additions in order to learn the
    most important things about them. Afterward, we'll have enough oversight of the
    parallelization support in the C++17 STL. We do not cover all the details, but
    the most important ones. The overview gained from this book helps in quickly understanding
    the rest of the parallel programming mechanisms, which you can always look up
    in the C++ 17 STL documentation online.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this chapter contains two bonus recipes. In one recipe, we will parallelize
    the Mandelbrot ASCII renderer from [Chapter 23](897bfd02-6f27-4b2c-b44d-052811364259.xhtml),
    *Advance Use of STL Algorithms*, with only minimal changes. In the last recipe,
    we will implement a tiny library that helps parallelizing complex tasks implicitly
    and automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Automatically parallelizing code that uses standard algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'C++17 came with one really *major* extension for parallelism: e*xecution policies*
    for standard algorithms. Sixty nine algorithms were extended to accept execution
    policies in order to run parallel on multiple cores, and even with enabled vectorization.'
  prefs: []
  type: TYPE_NORMAL
- en: For the user, this means that if we already use STL algorithms everywhere, we
    get a nice parallelization bonus for free. We can *easily* give our applications
    subsequent parallelization by simply adding a single execution policy argument
    to our existing STL algorithm calls.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will implement a simple program (with a not too serious use
    case scenario) that lines up multiple STL algorithm calls. While using these,
    we will see how easy it is to use C++17 execution policies in order to let them
    run multithreaded. In the last subsections of this section, we will have a closer
    look at the different execution policies.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will write a program that uses some standard algorithms.
    The program itself is more of an example of how real-life scenarios can look than
    doing actual real-life work situation. While using these standard algorithms,
    we are embedding execution policies in order to speed the code up:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to include some headers and declare that we use the `std` namespace.
    The `execution` header is a new one; it came with C++17:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Just for the sake of the example, we''ll declare a predicate function that
    tells whether a number is odd. We will use it later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s first define a large vector in our main function. We will fill it with
    a lot of data so that it takes some time to do calculations on it. The execution
    speed of this code will vary *a lot*, depending on the computer this code is executed
    on. Smaller/larger vector sizes might be better on different computers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to get a lot of random data for the vector, let''s instantiate a random
    number generator along with a distribution and pack them up in a callable object.
    If this looks strange to you, please first have a look at the recipes that deal
    with random number generators and distributions in [Chapter 25](bfd75fad-20a0-40bc-a783-12b3281a00bf.xhtml),
    *Utility Classes*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use the `std::generate` algorithm to fill the vector with random
    data. There is a new C++17 version of this algorithm, which can take a new kind
    of argument: an execution policy. We put in `std::par` here, which allows for
    automatic parallelization of this code. By doing this, we allow for multiple threads
    to start filling the vector together, which reduces the execution time if the
    computer has more than one CPU, which is usually the case with modern computers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `std::sort` method should also already be familiar. The C++17 version does
    also support an additional argument defining the execution policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The same applies to `std::reverse`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then we use `std::count_if` to count all the odd numbers in the vector. And
    we can even parallelize that by just adding an execution policy again!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This whole program did not do any *real* scientific work, as we were just going
    to have a look on how to parallelize standard algorithms, but let''s print something
    in the end:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running the program gives us the following output. At this point,
    it is interesting to see how the execution speed differs when using the algorithms
    without an execution policy compared with all the other execution policies. Doing
    this is left as an exercise for the reader. Try it; the available execution policies
    are `seq`, `par`, and `par_vec`. We should get different execution times for each
    of them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Especially since this recipe did not distract us with any complicated real-life
    problem solution, we were able to fully concentrate on the standard library function
    calls. It is pretty obvious that the their parallelized versions are hardly different
    from the classic sequential ones. They only differ by *one additional* argument,
    which is the *execution policy*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the invocations and answer three central questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Which STL algorithms can we parallelize this way?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sixty nine of the existing STL algorithms were upgraded to support parallelism
    in the C++17 standard, and there are seven new ones that also support parallelism.
    While such an upgrade might be pretty invasive for the implementation, not much
    has changed in terms of their interface--they all got an additional `ExecutionPolicy&&
    policy` argument, and that's it. This does *not* mean that we *always* have to
    provide an execution policy argument. It is just that they *additionally* support
    accepting an execution policy as their first argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the 69 upgraded standard algorithms. There are also the seven new
    ones that support execution policies from the beginning (highlighted in *bold*):'
  prefs: []
  type: TYPE_NORMAL
- en: '| `std::adjacent_difference` `std::adjacent_find`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::all_of`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::any_of`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::copy`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::copy_if`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::copy_n`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::count`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::count_if`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::equal` `**std::exclusive_scan**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::fill`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::fill_n`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::find`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::find_end`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::find_first_of`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::find_if`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::find_if_not` `**std::for_each**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**std::for_each_n**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::generate_n`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::includes` `**std::inclusive_scan**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::inner_product` | `std::inplace_merge` `std::is_heap` `std::is_heap_until`
    `std::is_partitioned`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::is_sorted`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::is_sorted_until`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::lexicographical_compare`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::max_element`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::merge`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::min_element`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::minmax_element`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::mismatch`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::move`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::none_of`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::nth_element`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::partial_sort`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::partial_sort_copy`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::partition`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::partition_copy`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::remove`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::remove_copy`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::remove_copy_if`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::remove_if`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::replace`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::replace_copy`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::replace_copy_if` | `std::replace_if` `std::reverse`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::reverse_copy`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::rotate`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::rotate_copy`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::search`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::search_n`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::set_difference`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::set_intersection`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::set_symmetric_difference`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::set_union`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::sort`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::stable_partition`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::stable_sort`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::swap_ranges`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::transform`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**std::transform_exclusive_scan**` `**std::transform_inclusive_scan**` `**std::transform_reduce**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::uninitialized_copy`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::uninitialized_copy_n`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::uninitialized_fill`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::uninitialized_fill_n`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::unique`'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::unique_copy` |'
  prefs: []
  type: TYPE_NORMAL
- en: Having these algorithms upgraded is great news! The more our old programs utilize
    STL algorithms, the easier we can add parallelism to them retroactively. Note
    that this does *not* mean that such changes make every program automatically *N*
    times faster because multiprogramming is quite a bit more complex than that.
  prefs: []
  type: TYPE_NORMAL
- en: However, instead of designing our own complicated parallel algorithms using
    `std::thread`, `std::async`, or by including external libraries, we can now parallelize
    standard tasks in a very elegant, operating system-independent way.
  prefs: []
  type: TYPE_NORMAL
- en: How do those execution policies work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The execution policy tells which strategy we allow for the automatic parallelization
    of our standard algorithm calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following three policy types exist in the `std::execution` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Policy** | **Meaning** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `sequenced_policy` | The algorithm has to be executed in a sequential form
    similar to the original algorithm without an execution policy. The globally available
    instance has the name `std::execution::seq`. |'
  prefs: []
  type: TYPE_TB
- en: '| `parallel_policy` | The algorithm may be executed with multiple threads that
    share the work in a parallel fashion. The globally available instance has the
    name `std::execution::par`. |'
  prefs: []
  type: TYPE_TB
- en: '| `parallel_unsequenced_policy` | The algorithm may be executed with multiple
    threads sharing the work. In addition to that, it is permissible to vectorize
    the code. In this case, container access can be interleaved between threads and
    also within the same thread due to vectorization. The globally available instance
    has the name `std::execution::par_unseq`. |'
  prefs: []
  type: TYPE_TB
- en: 'The execution policies imply specific constraints for us. The stricter the
    specific constraints, the more parallelization strategy measures we can allow:'
  prefs: []
  type: TYPE_NORMAL
- en: All element access functions used by the parallelized algorithm *must not* cause
    *deadlocks* or *data races*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of parallelism and vectorization, all the access functions *must
    not* use any kind of blocking synchronization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As long as we comply with these rules, we should be free from bugs introduced
    by using the parallel versions of the STL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Note that just using parallel STL algorithms correctly does not always lead
    to guaranteed speedup. Depending on the problem we try to solve, the problem size,
    and the efficiency of our data structures and other access methods, measurable
    speedup will vary very much or not occur at all. *Multiprogramming is still hard.*
  prefs: []
  type: TYPE_NORMAL
- en: What does vectorization mean?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vectorization is a feature that both the CPU and the compiler need to support.
    Let''s have a quick glance at a simple example to briefly understand what vectorization
    is and how it works. Imagine we want to sum up numbers from a very large vector.
    A plain implementation of this task can look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The compiler will eventually generate a loop from the `accumulate` call, which
    could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Proceeding from this point, with vectorization allowed and enabled, the compiler
    could then produce the following code. The loop does four accumulation steps in
    one loop step and also does four times fewer iterations. For the sake of simplicity,
    the example does not deal with the remainder if the vector does not contain `N
    * 4` elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Why should it do this? Many CPUs provide instructions that can perform mathematical
    operations such as `sum += v[i] + v[i+1] + v[i + 2] + v[i + 3];` in just *one
    step*. Pressing as *many* mathematical operations into as *few* instructions as
    possible is the target because this speeds up the program.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic vectorization is hard because the compiler needs to understand our
    program to some degree in order to make our program faster but without tampering
    with its *correctness*. At least, we can help the compiler by using standard algorithms
    as often as possible because those are easier to grasp for the compiler than complicated
    handcrafted loops with complex data flow dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Putting a program to sleep for specific amounts of time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A nice and simple possibility to control threads came with C++11\. It introduced
    the `this_thread` namespace, which includes functions that affect only the caller
    thread. It contains two different functions that allow putting a thread to sleep
    for a certain amount of time, so we do not need to use any external or operating
    system-dependent libraries for such tasks any longer.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we concentrate on how to suspend threads for a certain amount
    of time, or how to put them to *sleep*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will write a short program that just puts the main thread to sleep for certain
    amounts of time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first include all the needed headers and declare that we''ll use the
    `std` and `chrono_literals` namespaces. The `chrono_literals` namespace contains
    handy abbreviations for creating time-span values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s immediately put the main thread to sleep for 5 seconds and 300 milliseconds.
    Thanks to `chrono_literals`, we can express this in a very readable format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The last sleep statement was `relative`. We can also express `absolute` sleep
    requests. Let''s sleep until the point in time, which is *now* plus `3` seconds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Before quitting the program, let''s print something else to signal the end
    of the second sleep period:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running the program yields the following results. Linux, Mac,
    and other UNIX-like operating systems provide the `time` command, which accepts
    another command in order to execute it and stop the time it takes. Running our
    program with `time` shows that it ran `8.32` seconds, which is roughly the `5.3`
    and `3` seconds we let our program sleep. When running the program, it is possible
    to count the time between the arrival of the printed lines on the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `sleep_for` and `sleep_until` functions have been added to C++11 and reside
    in the `std::this_thread` namespace. They block the current thread (not the whole
    process or program) for a specific amount of time. A thread does not consume CPU
    time while it is blocked. It is just put into an inactive state by the operating
    system. The operating system does, of course, remind itself of waking the thread
    up again. The best thing about this is that we do not need to care which operating
    system our program runs on because the STL abstracts this detail away from us.
  prefs: []
  type: TYPE_NORMAL
- en: The `this_thread::sleep_for` function accepts a `chrono::duration` value. In
    the simplest case, this is just `1s` or `5s + 300ms`, just like in our example
    code. In order to get such nice literals for time spans, we need to declare `using
    namespace std::chrono_literals;`.
  prefs: []
  type: TYPE_NORMAL
- en: The `this_thread::sleep_until` function accepts a `chrono::time_point` instead
    of a time span. This is comfortable if we wish to put the thread to sleep until
    some specific wall clock time.
  prefs: []
  type: TYPE_NORMAL
- en: The timing for waking up is only as accurate as the operating system allows.
    This will be generally accurate *enough* with most operating systems, but it might
    become difficult if some application needs nanosecond-granularity.
  prefs: []
  type: TYPE_NORMAL
- en: Another possibility to put a thread to sleep for a short time is `this_thread::yield`.
    It accepts *no* arguments, which means that we cannot know for how long the execution
    of a thread is placed back. The reason is that this function does not really implement
    the notion of sleeping or parking a thread. It just tells the operating system
    in a cooperative way that it can reschedule any other thread of any other process.
    If there are none, then the thread will be executed again immediately. For this
    reason, `yield` is often less useful than just sleeping for a minimal, but specified,
    amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: Starting and stopping threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another addition that came with C++11 is the `std::thread` class. It provides
    a clean and simple way to start and stop threads, without any need for external
    libraries or to know how the operating system implements this. It's all just included
    in the STL.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will implement a program that starts and stops threads. There
    are some minor details to know what to do with threads once they are started,
    so we will go through these too.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start multiple threads and see how our program behaves when we unleash
    multiple processor cores to execute parts of its code at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, we need to include only two headers and then we declare that we use
    the `std` and `chrono_literals` namespaces:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to start a thread, we need to be able to tell what code should be
    executed by it. So, let''s define a function that can be executed. Functions are
    natural potential entry points for threads. The example function accepts an argument,
    `i`, which acts as the thread ID. This way we can tell which print line came from
    which thread later. Additionally, we use the thread ID to let all threads wait
    for different amounts of time, so we can be sure that they do not try to use `cout`
    at exactly the same time. If they did, that would garble the output. Another recipe
    in this chapter deals specifically with this problem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In the main function, we can, just out of curiosity, print how many threads
    can be run at the same time, using `std::thread::hardware_concurrency`. This depends
    on how many cores the machine really has and how many cores are supported by the
    STL implementation. This means that this might be a different number on every
    other computer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now finally start threads. With different IDs for each one, we start
    three threads. When instantiating a thread with an expression such as `thread
    t {f, x}`, this leads to a call of `f(x)` by the new thread. This ,way we can
    give the `thread_with_param` functions different arguments for each thread:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Since these threads are freely running, we need to stop them again when they
    are done with their work. We do this using the `join` function. It will *block*
    the calling thread until the thread we try to join returns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'An alternative to joining is *detaching*. If we do not call `join` or detach,
    the whole application will be terminated with a lot of smoke and noise as soon
    as the destructor of the `thread` object is executed. By calling `detach`, we
    tell `thread` that we really want to let thread number 3 to continue running,
    even after its `thread` instance is destructed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Before quitting the main function and the whole program, we print another message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running the code shows the following output. We can see that
    my machine has eight CPU cores. Then, we see the *hello* messages from all the
    threads, but the *bye* messages only from the two threads we actually joined.
    Thread 3 is still in its waiting period of 3 seconds, but the whole program does
    already terminate after the second thread has finished waiting for 2 seconds.
    This way, we cannot see the bye message from thread 3 because it was simply killed
    without any chance for completion (and without noise):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Starting and stopping threads is a very simple thing to do. Multiprogramming
    starts to be complicated where threads need to work together (sharing resources,
    waiting for each other, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to start a thread, we first need some function that will be executed
    by it. The function does not need to be special, as a thread could execute practically
    every function. Let''s pin down a minimal example program that starts a thread
    and waits for its completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The constructor call of `std::thread` accepts a function pointer or a callable
    object, followed by arguments that should be used with the function call. It is,
    of course, also possible to start a thread on a function that doesn't accept any
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: If the system has multiple CPU cores, then the threads can run parallel *and*
    concurrently. What is the difference between parallel and concurrent? If the computer
    has only one CPU core, then there can be a lot of threads that run in parallel
    but never concurrently because one CPU core can only run one thread at a time.
    The threads are then run in an interleaved way where every thread is executed
    for some parts of a second, then paused, and then the next thread gets a time
    slice (for human users, this looks like they run at the same time). If they do
    not need to share a CPU core, then they can run concurrently, as in *really at
    the same time*.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have absolutely *no control* over the following details:'
  prefs: []
  type: TYPE_NORMAL
- en: The *order* in which the threads are interleaved when sharing a CPU core.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *priority* of a thread, or which one is more important than the other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fact that threads are really *distributed* among all the CPU cores or if
    the operating system just pins them to the same core. It is indeed *possible*
    that all our threads run on only a single core, although the machine has more
    than 100 cores.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most operating systems provide possibilities to control also these facets of
    multiprogramming, but such features are, at this point, *not* included in the
    STL.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we can start and stop threads and tell them when to work on what and
    when to pause. That should be enough for a large class of applications. What we
    did in this section was we started three additional threads. Afterward, we *joined*
    most of them and *detached* the last one. Let''s summarize in a simple diagram
    what happened:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b848126-e59e-4cc1-baa0-6f31962879a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Reading the diagram from top to the bottom, it shows one point in time where
    we split the program workflow to four threads in total. We started three additional
    threads that did something (namely waiting and printing), but after starting the
    threads, the main thread executing the main function remained without work.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever a thread has finished executing the function it was started with, it
    will return from this function. The standard library then does some tidy up work
    that results in the thread being removed from the operating system's schedule,
    and maybe in its destruction, but we do not need to worry about it.
  prefs: []
  type: TYPE_NORMAL
- en: The only thing we *need* to worry about is *joining*. When a thread calls function
    `x.join()` on another `thread` object, it is put to sleep until thread `x` returns.
    Note that we are out of luck if the thread is trapped in an endless loop! If we
    want a thread to continue living until it decides to terminate itself, we can
    call `x.detach()`. After doing so, we have no external control over the thread
    any longer. No matter what we decide--we *must* always *join* or *detach* threads.
    If we don't do one of the two, the destructor of the `thread` object will call
    `std::terminate()`, which leads to an abrupt application shutdown.
  prefs: []
  type: TYPE_NORMAL
- en: The moment when our main function returns, the whole application is, of course,
    terminated. However, at the same time, our detached thread, `t3`, was still sleeping
    before printing its *bye* message to the terminal. The operating system didn't
    care--it just terminated our whole program without waiting for that thread to
    finish. This is something we need to consider. If that additional thread had to
    complete something important, we would have to make the main function *wait* for
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Performing exception safe shared locking with std::unique_lock and std::shared_lock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since the operation of threads is a heavily operating system support-related
    thing and the STL provides good operating system-agnostic interfaces for that,
    it is also wise to provide STL support for *synchronization* between threads.
    This way, we can not only start and stop threads without external libraries but
    also synchronize them with abstractions from a single unified library: the STL.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will have a look at STL mutex classes and RAII lock abstractions.
    While we play around with some of them in our concrete recipe implementation,
    we will also get an overview of more synchronization helpers that the STL provides.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to write a program that uses an `std::shared_mutex` instance in
    its *exclusive* and *shared* modes and to see what that means. Additionally, we
    do not call the lock and unlock functions ourselves but do the locking with automatic
    unlocking using RAII helpers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to include all necessary headers. Because we use STL functions
    and data structures all the time together with time literals, we declare that
    we use the `std` and `chrono_literal` namespaces:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The whole program revolves around one shared mutex, so let''s define a global
    instance for the sake of simplicity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to use the `std::shared_lock` and `std::unique_lock` RAII helpers.
    In order to make their names appear less clumsy, we define short type aliases
    for them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Before beginning with the main function, we define two helper functions that
    both try to lock the mutex in *exclusive* mode. This function here will instantiate
    a `unique_lock` instance on the shared mutex. The second constructor argument
    `defer_lock` tells the object to keep the lock unlocked. Otherwise, its constructor
    would try to lock the mutex and then block until it succeeds. Then we call `try_lock`
    on the `exclusive_lock` object. This call will return immediately and its boolean
    return value tells us if it got the lock or if the mutex was locked already somewhere
    else:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The other helper function tries to lock the mutex in exclusive mode, too. It
    blocks until it gets the lock. Then we simulate some error case by throwing an
    exception (which carries just a plain integer number instead of a more complex
    exception object). Although this leads to an immediate exit of the context in
    which we hold a locked mutex, the mutex will cleanly be released again. That is
    because the destructor of `unique_lock` will release the lock in any case by design:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now to the main function. First, we open up another scope and instantiate a
    `shared_lock` instance. Its constructor immediately locks the mutex in `shared`
    mode. We will see what this means in the next steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we open yet another scope and instantiate a second `shared_lock` instance
    on the same mutex. We have two `shared_lock` instances now, and they both hold
    a shared lock on the mutex. In fact, we could instantiate arbitrarily many `shared_lock`
    instances on the same mutex. Then we call `print_exclusive`, which tries to lock
    the mutex in *exclusive* mode. This will not succeed because it is locked in *shared*
    mode already:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'After leaving the latest scope, the destructor of the `shared_lock` `sl2` releases
    its shared lock on the mutex. The `print_exclusive` function will again fail because
    the mutex is still in shared lock mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'After leaving also the other scope, all `shared_lock` objects are destroyed,
    and the mutex is in unlocked state again. *Now* we can finally lock the mutex
    in exclusive mode. Let''s do this by calling `exclusive_throw` and then `print_exclusive`.
    Remember that we throw an exception in `exclusive_throw`. But because `unique_lock`
    is an RAII object that gives us exception safety, the mutex will be unlocked again
    no matter how we return from `exclusive_throw`. This way `print_exclusive` will
    not block on an erroneously still locked mutex:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running the code yields the following output. The first two lines
    show that we got the two shared lock instances. Then the `print_exclusive` function
    fails to lock the mutex in exclusive mode. After leaving the inner scope and unlocking
    the second shared lock, the `print_exclusive` function still fails. After leaving
    the other scope too, which finally released the mutex again, `exclusive_throw`
    and `print_exclusive` are finally able to lock the mutex:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When looking at the C++ documentation, it is at first a little confusing that
    there are different mutex classes and RAII lock-helpers. Before looking at our
    concrete code sample, let us summarize what the STL has available for us.
  prefs: []
  type: TYPE_NORMAL
- en: Mutex classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term mutex stands for **mut**ual **ex**clusion. In order to prevent that
    concurrently running threads alter the same object in a non-orchestrated way that
    might lead to data corruption, we can use mutex objects. The STL provides different
    mutex classes with different specialties. They all have in common that they have
    a `lock` and an `unlock` method.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever a thread is the first one to call `lock()` on a mutex that was not
    locked before, it owns the mutex. At this point, other threads will block on their
    `lock` calls, until the first thread calls `unlock` again. `std::mutex` can do
    exactly this.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many different mutex classes in the STL:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type name** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `mutex` | Standard mutex with a `lock` and an `unlock` method. Provides an
    additional nonblocking `try_lock` method. |'
  prefs: []
  type: TYPE_TB
- en: '| `timed_mutex` | Same as mutex, but provides additional `try_lock_for` and
    `try_lock_until` methods that allow for *timing out* instead of blocking forever.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `recursive_mutex` | Same as `mutex`, but if a thread locked an instance of
    it already, it can call `lock` multiple times on the same mutex object without
    blocking. It is released after the owning thread called `unlock` as often as it
    called `lock`. |'
  prefs: []
  type: TYPE_TB
- en: '| `recursive_timed_mutex` | Provides the features of both `timed_mutex` and
    `recursive_mutex`. |'
  prefs: []
  type: TYPE_TB
- en: '| `shared_mutex` | This mutex is special in that regard, that it can be locked
    in *exclusive* mode and in *shared* mode. In exclusive mode, it shows the same
    behavior as the standard mutex class. If a thread locks it in shared mode, it
    is possible for other threads to lock it in shared mode, too. It will then be
    unlocked as soon as the last shared mode lock owner releases it. While a lock
    is locked in shared mode, it is not possible to obtain exclusive ownership. This
    is very similar to the behavior of `shared_ptr`, only that it does not manage
    memory, but lock ownership. |'
  prefs: []
  type: TYPE_TB
- en: '| `shared_timed_mutex` | Combines the features of `shared_mutex` and `timed_mutex`
    for both exclusive and shared mode. |'
  prefs: []
  type: TYPE_TB
- en: Lock classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everything is nice and easy as long as threads do just lock a mutex, access
    some concurrence protected object and unlock the mutex again. As soon as a forgetful
    programmer misses to unlock a mutex somewhere after locking it, or an exception
    is thrown while a mutex is still locked, things look ugly pretty quick. In the
    best case, the program just hangs immediately and the missing unlock call is identified
    quickly. Such bugs, however, are very similar to memory leaks, which also occur
    when there are missing explicit `delete` calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'When regarding memory management, we have `unique_ptr`, `shared_ptr` and `weak_ptr`.
    Those helpers provide very convenient ways to avoid memory leaks. Such helpers
    exist for mutexes, too. The simplest one is `std::lock_guard`. It can be used
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '`lock_guard` element''s constructor accepts a mutex, on which it calls `lock`
    immediately. The whole constructor call will block until it obtains the lock on
    the mutex. Upon destruction, it unlocks the mutex again. This way it is hard to
    get the `lock`/`unlock` cycle wrong because it happens automatically.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The C++17 STL provides the following different RAII lock-helpers. They all
    accept a template argument that shall be of the same type as the mutex (although,
    since C++17, the compiler can deduce that type itself):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `lock_guard` | This class provides nothing else than a constructor and a
    destructor, which `lock` and `unlock` a mutex. |'
  prefs: []
  type: TYPE_TB
- en: '| `scoped_lock` | Similar to `lock_guard`, but supports arbitrarily many mutexes
    in its constructor. Will release them in opposite order in its destructor. |'
  prefs: []
  type: TYPE_TB
- en: '| `unique_lock` | Locks a mutex in exclusive mode. The constructor also accepts
    arguments that instruct it to timeout instead of blocking forever on the lock
    call. It is also possible to not lock the mutex at all, or to assume that it is
    locked already, or to only *try* locking the mutex. Additional methods allow to
    lock and unlock the mutex during the `unique_lock` lock''s lifetime. |'
  prefs: []
  type: TYPE_TB
- en: '| `shared_lock` | Same as `unique_lock`, but all operations are applied on
    the mutex in shared mode. |'
  prefs: []
  type: TYPE_TB
- en: While `lock_guard` and `scoped_lock` have dead-simple interfaces that only consist
    of constructor and destructor, `unique_lock` and `shared_lock` are more complicated,
    but also more versatile. We will see in later recipes of this chapter, how else
    they can be used if not for plain simple lock regions.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get back to the recipe code now. Although we only ran the code in single
    thread context, we have seen how it is meant to use the lock helpers. The `shrd_lck`
    type alias stands for `shared_lock<shared_mutex>` and allows us to lock an instance
    multiple times in shared mode. As long as `sl1` and `sl2` exist, no `print_exclusive`
    call is able to lock the mutex in exclusive mode. This is still simple.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s get to the exclusively locking functions that came later in the
    main function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: One important detail is that after returning from `exclusive_throw`, the `print_exclusive`
    function is able to lock the mutex again, although `exclusive_throw` did not exit
    cleanly due to the exception it throws.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have another look at `print_exclusive` because it used a strange constructor
    call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We did not only provide `shared_mut` but also `defer_lock` as constructor arguments
    for `unique_lock` in this procedure. `defer_lock` is an empty global object that
    can be used to select a different constructor of `unique_lock` that simply does
    not lock the mutex. By doing so, we are able to call `l.try_lock()` later, which
    does not block. In case the mutex is locked already, we can do something else.
    If it was indeed possible to get the lock, we still have the destructor tidying
    up after us.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding deadlocks with std::scoped_lock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If deadlocks had occurred in road traffic, they would have looked like the
    following situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e02fbc8d-bc97-496b-b388-f0ecbea0e5e5.png)'
  prefs: []
  type: TYPE_IMG
- en: In order to get the traffic flow going again, we either need a large crane that
    randomly picks one car from the center of the street intersection and removes
    it. If that is not possible, then we need enough drivers to be cooperative. The
    deadlock can be solved by all drivers in one direction driving several meters
    backwards, making space for the other drivers to continue.
  prefs: []
  type: TYPE_NORMAL
- en: In multithreaded programs, such situations, of course, need to be avoided strictly
    by the programmer. It is however too easy to fail in that regard when the program
    is really complex.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we are going to write code which intentionally provokes a deadlock
    situation. Then we will see how to write code that acquires the same resources
    that led the other code into a deadlock, but use the new STL lock class `std::scoped_lock`
    that came with C++17, in order to avoid this mistake.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code of this section contains two pairs of functions that ought to be executed
    by concurrent threads, and that acquire two resources in form of mutexes. One
    pair provokes a deadlock and the other avoids it. In the main function, we are
    going to try them out:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first include all needed headers and declare that we use namespace `std`
    and `chrono_literals`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we instantiate two mutex objects which we need in order to run into a
    deadlock:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In order to provoke a deadlock with two resources, we need two functions. One
    function tries to lock mutex A and then mutex B, while the other function will
    do that in the opposite order. By letting both functions sleep a bit between the
    locks, we can make sure that this code blocks forever on a deadlock. (This is
    for demonstration purposes. A program without some sleep lines might run successfully
    without a deadlock sometimes if we start it repeatedly.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that we do not use the `''n''` character in order to print a line break,
    but we use `endl`. `endl` does not only perform a line break but also flushes
    the stream buffer of `cout`, so we can be sure that prints are not bunched up
    and postponed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'As promised in the last step, `deadlock_func_2` looks exactly same as `deadlock_func_1`,
    but it locks mutex A and B in the opposite order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we write a deadlock-free variant of those two functions we just implemented.
    They use the class `scoped_lock`, which locks all mutexes we provide as constructor
    arguments. Its destructor unlocks them again. While locking the mutexes, it internally
    applies a deadlock avoidance strategy for us. Note that both functions still use
    mutex A and B in opposite order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'In the main function, we will go through two scenarios. First, we use the *sane*
    functions in multithreaded context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we use the deadlock-provoking functions that do not utilize any deadlock
    avoidance strategy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Compiling and running the program yields the following output. The first two
    lines show that the *sane* locking function scenario works and both functions
    return without blocking forever. The other two functions run into a deadlock.
    We can tell that this is a deadlock because we see the print lines that tell that
    the individual threads try to lock mutexes A and B and then wait *forever*. Both
    do not reach the point where they successfully locked both mutexes. We can let
    this program run for hours, days, and years, and *nothing* will happen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This application needs to be killed from outside, for example by pressing the
    keys *Ctrl* + *C*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By implementing code that willfully causes a deadlock, we've seen how quick
    such an unwanted scenario can happen. In a large project, where multiple programmers
    write code that needs to share a common set of mutex-protected resources, all
    programmers need to comply with the *same order* when locking and unlocking mutexes.
    While such strategies or rules are really easy to follow, they are also easy to
    forget. Another term for this problem is *lock order inversion*.
  prefs: []
  type: TYPE_NORMAL
- en: '`scoped_lock` is a real help in such situations. It came with C++17 and works
    the same way as `lock_guard` and `unique_lock` work: its constructor performs
    the locking, and its destructor the unlocking of a mutex. `scoped_lock`''s specialty
    is that it can do this with *multiple* mutexes.'
  prefs: []
  type: TYPE_NORMAL
- en: '`scoped_lock` uses the `std::lock` function, which applies a special algorithm
    that performs a series of `try_lock` calls on all the mutexes provided, in order
    to prevent deadlocking. Therefore it is perfectly safe to use `scoped_lock` or
    call `std::lock` on the same set of locks, but in different orders.'
  prefs: []
  type: TYPE_NORMAL
- en: Synchronizing concurrent std::cout use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One inconvenience in multithreaded programs is that we must practically secure
    *every* data structure they modify, with mutexes or other measures that protect
    from uncontrolled concurrent modification.
  prefs: []
  type: TYPE_NORMAL
- en: One data structure that is typically used very often for printing is `std::cout`.
    If multiple threads access `cout` concurrently, then the output will appear in
    interesting mixed patterns on the terminal. In order to prevent this, we would
    need to write our own function that prints in a concurrency-safe fashion.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to learn how to provide a `cout` wrapper that consists of minimal
    code itself and that is as comfortable to use as `cout`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to implement a program that prints to the terminal
    concurrently from many threads. In order to prevent garbling of the messages due
    to concurrency, we implement a little helper class that synchronizes printing
    between threads:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, the includes come first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we implement our helper class, which we call `pcout`. The `p` stands for
    *parallel* because it works in a synchronized way for parallel contexts. The idea
    is that `pcout` publicly inherits from `stringstream`. This way we can use `operator<<`
    on instances of it. As soon as a `pcout` instance is destroyed, its destructor
    locks a mutex and then prints the content of the `stringstream` buffer. We will
    see how to use it in the next step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s write two functions that can be executed by additional threads.
    Both accept a thread ID as arguments. Then, their only difference is that the
    first one simply uses `cout` for printing. The other one looks nearly identical,
    but instead of using `cout` directly, it instantiates `pcout`. This instance is
    a temporary object that lives only exactly for this line of code. After all `operator<<`
    calls have been executed, the internal string stream is filled with what we want
    to print. Then `pcout` instance''s destructor is called. We have seen what the
    destructor does: it locks a specific mutex all `pcout` instances share along and
    prints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try it out. First, we are going to use `print_cout`, which just uses
    `cout` for printing. We start 10 threads which concurrently print their strings
    and wait until they finish:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we do the same thing with the `print_pcout` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running the program yields the following result. As we see, the
    first 10 prints are completely garbled. This is how it can look like when `cout`
    is used concurrently without locking. The last 10 lines of the program are the
    `print_pcout` lines which do not show any signs of garbling. We can see that they
    are printed from different threads because their order appears randomized every
    time when we run the program again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c9208c3b-6c0c-4187-aaba-dd51820c5c5e.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ok, we've built this *"cout wrapper"* that automatically serializes concurrent
    printing attempts. How does it work?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do the same steps our `pcout` helper does in a manual manner without
    any magic. First, it instantiates a string stream and accepts the input we feed
    into it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Then it locks a globally available mutex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'In this locked scope, it accesses the content of string stream `ss`, prints
    it, and releases the mutex again by leaving the scope. The `cout.flush()` line
    tells the stream object to print to the terminal immediately. Without this line,
    a program might run faster because multiple printed lines can be bunched up and
    printed in a single run later. In our recipes, we will like to see all output
    lines immediately, so we use the `flush` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Ok, this is simple enough but tedious to write if we have to to the same thing
    again and again. We can shorten down the `stringstream` instantiation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: This instantiates a string stream object, feeds everything we want to print
    into it and then destructs it again. The lifetime of the string stream is reduced
    to just this line. Afterward, we cannot print it any longer, because we cannot
    access it. Which code is the last that is able to access the stream's content?
    It is the destructor of `stringstream`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We cannot modify `stringstream` instance''s member methods, but we can extend
    them by wrapping our own type around it via inheritance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: This class *is still* a string stream and we can use it like any other string
    stream. The only difference is that it will lock a mutex and print its own buffer
    using `cout`.
  prefs: []
  type: TYPE_NORMAL
- en: We also moved the `cout_mutex` object into struct `pcout` as a static instance
    so we have both bundled in one place.
  prefs: []
  type: TYPE_NORMAL
- en: Safely postponing initialization with std::call_once
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes we have specific code sections that can be run in parallel context
    by multiple threads with the obligation that some *setup code* must be executed
    exactly once before executing the actual functions. A simple solution is to just
    execute the existing setup function before the program enters a state from which
    parallel code can be executed from time to time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The drawbacks of such an approach are the following ones:'
  prefs: []
  type: TYPE_NORMAL
- en: If the parallel function comes from a library, the user must not forget to call
    the setup function. That does not make the library easier to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the setup function is expensive in some way, and it might not even need to
    be executed in case the parallel functions that need this setup are not even always
    used, then we need code that decides when/if to run it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will have a look at `std::call_once`, which is a helper function
    that solves this problem for us in a simple to use and elegant implicit way.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to write a program that starts multiple threads with exactly the
    same code. Although they are programmed to execute exactly the same code, our
    example setup function will only be called once:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to include all the necessary headers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to use `std::call_once` later. In order to use it, we need an
    instance of `once_flag` somewhere. It is needed for the synchronization of all
    threads that use `call_once` on a specific function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The function which must be only executed once is the following one. It just
    prints a single exclamation mark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'All threads will execute the print function. The first thing we do is calling
    the function `once_print` through the function `std::call_once`. `call_once` needs
    the variable `callflag` we defined before. It will use it to orchestrate the threads:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Ok, let''s now start 10 threads which all use the `print` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running yields the following output. First, we see the exclamation
    mark from the `once_print` function. Then we see all thread IDs. `call_once` did
    not only make sure that `once_print` was only called once. Additionally, it synchronized
    all threads, so that no ID is printed *before* `once_print` was executed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`std:call_once` works like a barrier. It maintains access to a function (or
    a callable object). The first thread to reach it gets to execute the function.
    Until it has finished, any other thread that reaches the `call_once` line is blocked.
    After the first thread returns from the function, all other threads are released,
    too.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to organize this little choreography, a variable is needed from which
    the other threads can determine if they must wait and when they are released again.
    This is what our variable `once_flag callflag;` is for. Every `call_once` line
    also needs a `once_flag` instance as the argument prepending the function that
    shall be called only once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another nice detail is: If it happens, that the thread which is selected to
    execute the function in `call_once` *fails* because some *exception* is thrown,
    then the next thread is allowed to execute the function again. This happens in
    the hope that it will not throw an exception the next time.'
  prefs: []
  type: TYPE_NORMAL
- en: Pushing the execution of tasks into the background using std::async
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Whenever we want some code to be executed in the background, we can simply
    start a new thread that executes this code. While this happens, we can do something
    else and then wait for the result. It''s simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'But then the inconvenience starts: `t.join()` does not give us the return value
    of `my_function`. In order to get at that, we need to write a function that calls
    `my_function` and stores its return value in some variable that is also accessible
    for the first thread in which we started the new thread. If such situations occur
    repeatedly, then this represents quite a bunch of boilerplate code we have to
    write again and again.'
  prefs: []
  type: TYPE_NORMAL
- en: Since C++11, we have `std::async` which can do exactly this job for us and not
    only that. In this recipe, we are going to write a simple program that does multiple
    things at the same time using asynchronous function calls. As `std::async` is
    a bit more powerful than that alone, we will have a closer look at its different
    facets.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to implement a program that does multiple different things concurrently
    but instead of explicitly starting threads, we use `std::async` and `std::future`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we include all necessary headers and declare that we use the `std` namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We implement three functions which have nothing to do with parallelism but
    do interesting tasks. The first function accepts a string and creates a histogram
    of all characters occurring within that string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The second function does also accept a string and returns a sorted copy of
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The third one counts how many vowels exist within the string it accepts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'In the main function, we read the whole standard input into a string. In order
    to not segment the input into words, we deactivate `ios::skipws`. This way we
    get one large string, no matter how much white space the input contains. We use
    `pop_back` on the resulting string afterward because we got one string terminating
    `''''` character too much this way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s get the return values from all the functions we implemented before.
    In order to speed the execution up for very long input, we launch them *asynchronously*.
    The `std::async` function accepts a policy, a function, and arguments for that
    function. We call `histogram`, `sorted`, and `vowels` with `launch::async` as
    a policy (we will see later what that means). All functions get the same input
    string as arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The `async` calls return immediately because they do not actually execute our
    functions. Instead, they set up synchronization structures which will obtain the
    results of the function calls later. The results are now being calculated concurrently
    by additional threads. In the meantime, we are free to do whatever we want, as
    we can pick up those values later. The return values `hist`, `sorted_str` and
    `vowel_count` are of the types the functions `histogram`, `sorted`, and `vowels`
    return, but they were wrapped in a `future` type by `std::async`. Objects of this
    type express that they will contain their values at some point in time. By using
    `.get()` on all of them, we can make the main function block until the values
    arrive, and then use them for printing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running the code looks like the following. We use a short example
    string that does not really make it worth being parallelized, but for the sake
    of this example, the code is nevertheless executed concurrently. Additionally,
    the overall structure of the program did not change much compared to a naive sequential
    version of it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we would not have used `std::async` the serial unparallelized code could
    have looked as simple as that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The only thing we did in order to parallelize the code was the following. We
    wrapped the three function calls into `async(launch::async, ...)` calls. This
    way these three functions are not executed by the main thread we are currently
    running in. Instead, `async` starts new threads and lets them execute the functions
    concurrently. This way we get to execute only the overhead of starting another
    thread and can continue with the next line of code, while all the work happens
    in the background:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: While `histogram` for example, returns us a map instance, `async(..., histogram,
    ...)` does return us a map that was wrapped in a `future` object before. This
    `future` object is kind of an empty *placeholder* until the thread that executes
    the `histogram` function returns. The resulting map is then placed into the `future`
    object so we can finally access it. The `get` function then gives us access to
    the encapsulated result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at another minimal example. Consider the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of writing the preceding code, we can also do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s basically it. Executing tasks in the background might have never been
    easier in standard C++. There is still one thing left to resolve: What does `launch::async`
    mean? `launch::async` is a flag that defines the launch policy. There are two
    policy flags which allow for three constellations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Policy choice** | **Meaning** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `launch::async` | The function is guaranteed to be executed by another thread.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `launch::deferred` | The function is executed by the same thread, but later
    (*lazy evaluation*). Execution then happens when `get` or `wait` is called on
    the future. If *none* of both happens, the function is not called *at all*. |'
  prefs: []
  type: TYPE_TB
- en: '| `launch::async &#124; launch::deferred` | Having both flags set, the STL''s
    `async` implementation is free to choose which policy shall be followed. This
    is the default choice if no policy is provided. |'
  prefs: []
  type: TYPE_TB
- en: By just calling `async(f, 1, 2, 3)` without a policy argument, we automatically
    select *both* policies. The implementation of `async` is then free to choose which
    policy to employ. This means that we cannot be *sure* that another thread is started
    at all, or if the execution is just deferred in the current thread.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is indeed one last thing we should know about. Suppose, we write code
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: This might have the motivation of executing functions `f` and `g` (we do not
    care about their return values in this example) in concurrent threads and then
    doing different things at the same time. While running such code, we will notice
    that the code *blocks* on this calls, which is most probably not what we want.
  prefs: []
  type: TYPE_NORMAL
- en: 'So why does it block? Isn''t `async` all about nonblocking asynchronous calls?
    Yes it is, but there is one special peculiarity: if a future was obtained from
    an async call with the `launch::async` policy, then its destructor performs a
    *blocking wait*.'
  prefs: []
  type: TYPE_NORMAL
- en: This means that *both* the async calls from this short example are blocking
    because the lifetime of the futures they return ends in the same line! We can
    fix this by capturing their return values in variables with a longer lifetime.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the producer/consumer idiom with std::condition_variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we are going to implement a typical producer/consumer program
    with multiple threads. The general idea is that there is one thread that produces
    items and puts them into a queue. Then there is another thread that consumes such
    items. If there is nothing to produce, the producer thread sleeps. If there is
    no item in the queue to consume, the consumer sleeps.
  prefs: []
  type: TYPE_NORMAL
- en: Since the queue that both threads have access to is also modified by both whenever
    an item is produced or consumed, it needs to be protected by a mutex.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing to consider is: What does the consumer do if there is no item
    in the queue? Does it poll the queue every second until it sees new items? That
    is not necessary because we can let the consumer wait for wakeup *events* that
    are triggered by the producer, whenever there are new items.'
  prefs: []
  type: TYPE_NORMAL
- en: C++11 provides a nice data structure called `std::condition_variable` for this
    kind of events. In this recipe, we are going to implement a simple producer/consumer
    app that takes advantage of this.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to implement a simple producer/consumer program which runs a single
    producer of values in its own thread, as well as a single consumer thread in another
    thread:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to perform all the needed includes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'We instantiate a queue of simple numeric values and call it `q`. The producer
    will push values into it, and the consumer will take values out of it. In order
    to synchronize both, we need a mutex. In addition to that, we instantiate a `condition_variable`
    `cv`. The variable `finished` will be the producer''s way to tell the consumer
    that no more values will follow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s first implement the producer function. It accepts an argument `items`
    which limits the maximum number of items for production. In a simple loop, it
    will sleep 100 milliseconds for every item, which simulates some computational
    *complexity*. Then we lock the mutex that synchronizes access to the queue. After
    successful production and insertion to the queue, we call `cv.notify_all()`. This
    function wakes the consumer up. We will see later at the consumer side how this
    works:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'After having produced all items, we lock the mutex again because we are going
    to change to set the `finished` bit. Then we call `cv.notify_all()` again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can implement the consumer function. It takes no arguments because it
    will blindly consume until the queue runs empty. In a loop that is executed as
    long as `finished` is not set, it will first lock the mutex that protects both
    the queue and the `finished` flag. As soon as it has the lock, it calls `cv.wait`
    with the lock and a lambda expression as arguments. The lambda expression is a
    predicate that tells if the producer thread is still alive and if there is anything
    to consume in the queue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cv.wait` call unlocks the lock and waits until the condition described
    by the predicate function holds. Then, it locks the mutex again and consumes everything
    from the queue until it appears empty. If the producer is still alive, it will
    iterate through the loop again. Otherwise, it will terminate because `finished`
    is set, which is the producer''s way to signal that there are no further items
    being produced:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'In the main function, we start a producer thread which produces 10 items, and
    a consumer thread. Then we wait until their completion and terminate the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running the program yields the following output. When the program
    is executed, we can see that there is some time (100 milliseconds) between each
    line, because the production of items takes some time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we simply started two threads. The first thread produces items
    and puts them into a queue. The other takes items out of the queue. Whenever one
    of those threads touches the queue in any way, it locks the common mutex `mut`
    which is accessible for both. This way we made sure that it cannot happen that
    both threads manipulate the queue's state at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the queue and the mutex, we declared generally four variables that
    were involved in the producer-consumer thing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: The variable `finished` is easy to explain. It was set to `true` when the producer
    finished producing its fixed amount of items. When the consumer sees that this
    variable is `true`, it consumes the last items in the queue and stops consuming.
    But what is the `condition_variable` `cv` for? We used `cv` in two different contexts.
    One of the contexts was *waiting for a specific condition*, and the other was
    *signaling that condition*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The consumer side that waits for a specific condition looks like this. The
    consumer thread loops over a block that first locks mutex `mut` in a `unique_lock`.
    Then it calls `cv.wait`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'This code is *somewhat* equivalent to the following alternative code. We will
    elaborate soon why it is not really the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that we generally first acquire the lock and then check what scenario
    we have:'
  prefs: []
  type: TYPE_NORMAL
- en: Are there items to consume? Then keep the lock, consume, release the lock, and
    start over.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Else, if there are *no consumable items* but the producer is still *alive,*
    release the mutex to give the producer a chance of adding items to the queue.
    Then, try to lock it again in hope that the situation changes and we get to see
    situation 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The real reason why the `cv.wait` line is not equivalent to the `while (q.empty()
    && ... )` construct is, that we cannot simply loop over a `l.unlock(); l.lock();`
    cycle. If the producer thread is inactive for some time, then this would lead
    to continuous locking and unlocking of the mutex, which makes no sense because
    it needlessly burns CPU cycles.
  prefs: []
  type: TYPE_NORMAL
- en: An expression like `cv.wait(lock, predicate)` will wait until `predicate()`
    returns `true`. But it does not do this by continuously unlocking and locking
    `lock`. In order to wake a thread up that blocks on the `wait` call of a `condition_variable`
    object, another thread has to call the `notify_one()` or `notify_all()` method
    on the same object. Only then the waiting thread(s) is/are kicked out of their
    sleep in order to check if `predicate()` holds.
  prefs: []
  type: TYPE_NORMAL
- en: The nice thing about the `wait` call checking the predicate is that if there
    is a *spurious* wakeup call, the thread will go to sleep immediately again. This
    means that it does not really harm the program flow (but maybe the performance)
    if we have too many notify calls.
  prefs: []
  type: TYPE_NORMAL
- en: On the producer side, we just called `cv.notify_all()` after the producer inserted
    an item to the queue and after it produced its last item and set the `finished`
    flag to `true`. This was enough to direct the consumer.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the multiple producers/consumers idiom with std::condition_variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s pick up the producer/consumer problem from the last recipe and make
    it a bit more complicated: We make *multiple* producers produce items and *multiple*
    consumers consume them. In addition to that, we define that the queue shall not
    exceed a maximum size.'
  prefs: []
  type: TYPE_NORMAL
- en: This way not only the consumers have to sleep from time to time if there are
    no items in the queue, but also the producers have to sleep from time to time
    when there are *enough* items in the queue.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to see how to solve this problem with multiple `std::condition_variable`
    objects and will also use them in slightly different ways than in the last recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to implement a program just like in the recipe
    before, but this time with multiple producers and multiple consumers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to include all needed headers and we declare that we use namespace
    `std` and `chrono_literals`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we implement the synchronized printing helper from the other recipe in
    this chapter because we are going to do a lot of concurrent printing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'All producers write values into the same queue and all consumers will also
    take values out of this queue. In addition to that queue, we need a mutex that
    protects both the queue and a flag that can tell if the production was stopped
    at some point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to employ two different `condition_variables` in this program.
    In the single producer/consumer recipe, we had a `condition_variable` telling
    that there are new items in the queue. In this case, we make it a bit more complicated.
    We want the producers to produce until the queue contains a certain *stock amount*
    of items. If that stock amount is reached, they shall *sleep*. This way the `go_consume`
    variable can be used to wake up consumers which then, in turn, can wake up the
    producers with the `go_produce` variable again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'The producer function accepts a producer ID number, a total number of items
    to produce and a stock limit as arguments. It then enters its own production loop.
    There, it first locks the queue''s mutex and unlocks it again in the `go_produce.wait`
    call. It waits for the condition that the queue size is below the `stock` threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'After the producer was woken up, it produces an item and pushes it into the
    queue. The queue value is calculated from the expression `id * 100 + i`. This
    way we can later see which producer produced it because the hundreds in the number
    are the producer ID. We also print the production event to the terminal. The format
    of the printing may look strange, but it will align nicely with the consumer output
    in the terminal later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'After production, we can wake up sleeping consumers. A sleeping period of 90
    milliseconds simulates that producing items takes some time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Now to the consumer function that only accepts a consumer ID as an argument.
    It shall continue waiting for items if the production has not stopped, or the
    queue is not empty. If the queue is empty, but the production has not stopped,
    then it is possible that there might be new items soon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'After locking the queue mutex, we unlock it again in order to wait on the `go_consume`
    event variable. The lambda expression argument describes that we want to return
    from the wait call when the queue contains items. The second argument `1s` tells
    that we do not want to wait forever. If it takes longer than 1 second, we want
    to drop out of the wait function. We can distinguish if the `wait_for` function
    returned because the predicate condition holds, or if we dropped out of it because
    of a timeout because it will return `false` in case of the timeout. If there are
    new items in the queue, we consume them and print this event to the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'After item consumption, we notify the producers and sleep for 130 milliseconds
    to simulate that consuming items is also time-consuming:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'In the main function, we instantiate a vector for worker threads and another
    for consumer threads:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we spawn three producer threads and five consumer threads:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'We first let the producer threads finish. As soon as all of them have returned,
    we set the `production_stopped` flag, which will lead the consumers to finish,
    too. We need to collect those and then we can quit the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running the program leads to the following output. The output
    is very long, which is why it is truncated here. We can see that the producers
    go to sleep from time to time, and let the consumers eat up some items until they
    finally produce again. It is interesting to alter the wait times for producers/consumers,
    as well as manipulating the number of producers/consumers and stock items because
    this completely changes the output patterns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe is an extension of the preceding recipe. Instead of synchronizing
    only one producer with one consumer, we implemented a program that synchronizes
    `M` producers with `N` consumers. On top of that, not only the consumers go to
    sleep if there are no items for them left, but also the producers go to sleep
    as soon as the item queue becomes *too long*.
  prefs: []
  type: TYPE_NORMAL
- en: When multiple consumers wait for the same queue to fill up, then this would
    generally also work with the consumer code from the one producer/one consumer
    scenario. As long as only one thread locks the mutex that protects the queue and
    then takes items out of it, the code is safe. It does not matter how many threads
    are waiting for the lock at the same time. The same applies to the producers,
    as in both scenarios the only important thing is that the queue is never accessed
    by more than one thread at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'So what makes this program really more complex than just running the one producer/one
    consumer example with more threads is the fact that we make the producer threads
    stop as soon as the item queue length reached a certain threshold. In order to
    meet that requirement, we implemented two different signals with their own `condition_variable`:'
  prefs: []
  type: TYPE_NORMAL
- en: The `go_produce` signals the event that the queue is not completely filled to
    the maximum and the producers may fill it up again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `go_consume` signals the event that the queue reached its maximum length
    and consumers are free to consume items again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This way producers fill items into the queue and signal the `go_consume` event
    to the consuming threads, which wait on the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'The producers, on the other hand, wait on the following line until they are
    allowed to produce again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'One interesting detail is that we do not let consumers wait *forever*. In the
    `go_consume.wait_for` call, we additionally added a timeout argument of 1 second.
    This is the exit mechanism for consumers: if the queue is empty for longer than
    a second, maybe there are no active producers any longer.'
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of simplicity, the code tries to keep the queue length *always
    at the maximum*. A more sophisticated program could let the consumer threads push
    a wake-up notification, *only* if the queue has only *half* the size of its maximum
    length. This way producers would be woken up before the queue runs empty again,
    but not unnecessarily earlier when there are still enough items in the queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'One situation that `condition_variable` solves elegantly for us is the following:
    If a consumer fires the `go_produce` notification, there might be a horde of producers
    racing to produce the next item. If only one item is missing, then there will
    only be one producer producing it. If all producers would always produce an item
    as soon as the `go_produce` event is fired, we would often see the case that the
    queue is filled above its allowed maximum.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's imagine the situation that we have `(max - 1)` items in the queue and
    want one new item produced so that the queue is filled up again. No matter if
    a consumer thread calls `go_produce.notify_one()` (which would wake up only one
    waiting thread) or `go_produce.notify_all()` (which wakes up *all* waiting threads),
    we have the guarantee that only one producer thread will exit the `go_produce.wait`
    call, because, for all other producer threads, the `q.size() < stock` wait condition
    doesn't hold any longer as soon as they get the mutex after being woken up.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing the ASCII Mandelbrot renderer using std::async
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember the *ASCII Mandelbrot renderer* from [Chapter 23](897bfd02-6f27-4b2c-b44d-052811364259.xhtml),
    *Advanced Use of STL algorithms*? In this recipe, we will make it use threads
    in order to speed its calculation time a bit up.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will modify the line in the original program that limits the number
    of iterations for every selected coordinate. This will make the program *slower*
    and its results *more accurate* than we can actually display on the terminal,
    but then we have a nice example target for parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will apply minor modifications to the program and see how the whole
    program runs faster. After those modifications, the program runs with `std::async`
    and `std::future`. In order to fully understand this recipe, it is crucial to
    understand the original program.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we take the ASCII Mandelbrot fractal renderer that we implemented
    in [Chapter 23](897bfd02-6f27-4b2c-b44d-052811364259.xhtml), *Advanced Use of
    STL Algorithms*. First, we are going to make the calculation take much more time
    by incrementing the calculation limit. Then we get some speedup by doing only
    four little changes to the program in order to parallelize it:'
  prefs: []
  type: TYPE_NORMAL
- en: In order to follow the steps, it is best to just copy the whole program from
    the other recipe. Then follow the instructions in the following steps in order
    to do all needed adjustments. All differences from the original program are highlighted
    in *bold*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first change is an additional header, `<future>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'The `scaler` and `scaled_cmplx` functions don''t need any change:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'In the function `mandelbrot_iterations`, we are just going to increment the
    number of iterations in order to make the program a bit more computation-heavy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we have a part of the main function that does not need any change again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `to_iteration_count` function, we do not call `mandelbrot_iterations(x_to_xy(x))`
    directly any longer, but make the call asynchronous using `std::async`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Before the last change, the function `to_iteration_count` returned us the number
    of iterations a specific coordinate needs for the Mandelbrot algorithm to converge.
    Now it returns a `future` variable that will contain the same value later because
    it is computed asynchronously. Because of this, we need a vector that holds all
    the future values, so let''s just add one. The output iterator we provide `transform`
    as the third argument must be the begin iterator of the new output vector `r`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'The `accumulate` call which did all the printing for us doesn''t get `size_t`
    values as its second argument any longer, but `future<size_t>` values. We need
    to adapt it to this type (if we had used `auto&` as its type from the beginning
    then this would not even be necessary), and then we need to call `x.get()` where
    we just accessed `x` before, in order to wait for the value to arrive:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: Compiling and running gives us the same output as before. The only interesting
    difference is the execution speed. If we increase the number of iterations for
    the original version of the program, too, then the parallelized version should
    compute faster. On my computer with four CPU cores with hyperthreading (which
    results in 8 virtual cores), I get different results with GCC and clang. The best
    speedup is `5.3`, and the worst is `3.8`. The results will also vary across machines,
    of course.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is crucial to understand the whole program first because then it is clear
    that all the CPU-intense work happens in one line of code in the main function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: The vector `v` contains all the indices that are mapped to complex coordinates,
    which are then in turn iterated over with the Mandelbrot algorithm. The result
    of each iteration is saved in vector `r`.
  prefs: []
  type: TYPE_NORMAL
- en: In the original program, this is the single line which consumes all the processing
    time for calculating the fractal image. All code that precedes it is just set
    up work and all code that follows it is just for printing. This means that parallelizing
    this line is key to more performance.
  prefs: []
  type: TYPE_NORMAL
- en: One possible approach to parallelizing this is to break up the whole linear
    range from `begin(v)` to `end(v)` into chunks of the same size and distribute
    them evenly across all cores. This way all cores would share the amount of work.
    If we used the parallel version of `std::transform` with a parallel execution
    policy, this would exactly be the case. Unfortunately, this is not the right strategy
    for *this* problem, because every single point in the Mandelbrot set shows a very
    individual number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Our approach here is to make every single vector item which represents an individually
    printed character cell on the terminal later an asynchronously calculated `future`
    value. As source and target vector are `w * h` items large, which means `100 *
    40` in our case, we have a vector of 4000 future values that are calculated asynchronously.
    If our system had 4000 CPU cores, then this would mean that we start 4000 threads
    that do the Mandelbrot iteration really concurrently. On a normal system with
    fewer cores, the CPUs will just process one asynchronous item after the other
    per core.
  prefs: []
  type: TYPE_NORMAL
- en: While the `transform` call with the asynchronized version of `to_iteration_count`
    itself does *no calculation* but setting up of threads and future objects, it
    returns practically immediately. The original version of the program blocked at
    this point because the iterations took so long.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parallelized version of the program does of course block *somewhere*, too.
    The function that prints all our values on the terminal must access the results
    from within the futures. In order to do that, it calls `x.get()` on all the values.
    And this is the trick: while it waits for the first value to be printed, a lot
    of other values are calculated at the same time. So if the `get()` call of the
    first future returns, the next future might be ready for printing already too!'
  prefs: []
  type: TYPE_NORMAL
- en: In case `w * h` results in much larger numbers, there will be some measurable
    overhead in creating and synchronizing all these futures. In this case, the overhead
    is not too significant. On my laptop with an Intel i7 processor with 4 *hyperthreading*
    capable cores (which results in eight virtual cores), the parallel version of
    this program ran more than 3-5 times faster compared to the original program.
    The ideal parallelization would make it indeed 8 times faster. Of course, this
    speedup will vary between different computers, because it depends on a lot of
    factors.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a tiny automatic parallelization library with std::future
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most complex tasks can be broken down into subtasks. From all subtasks, we can
    draw an **directed acyclic graph** (**DAG**) that describes which subtask depends
    on what other subtasks in order to finish the higher level task. Let us, for example,
    imagine that we want to produce the string `"foo bar foo bar this that "`, and
    we can only do this by creating single words and concatenate those with other
    words, or with themselves. Let's say this functionality is provided by three primitive
    functions `create`, `concat`, and `twice`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking this into account, we can draw the following DAG that visualizes the
    dependencies between them in order to get the final result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0648ebd7-03e5-4c9c-8ee2-8ed58cb25773.png)'
  prefs: []
  type: TYPE_IMG
- en: When implementing this in code, it is clear that everything can be implemented
    in a serial manner on one CPU core. Alternatively, all subtasks that depend on
    no other subtasks or other subtasks that already have been finished, can be executed
    *concurrently* on multiple CPU cores.
  prefs: []
  type: TYPE_NORMAL
- en: It might perhaps seem tedious to write such code, even with `std::async` because
    the dependencies between the subtasks need to be modeled. In this recipe, we will
    implement two little library helper functions that help to transform the normal
    functions `create`, `concat`, and `twice` to functions that work asynchronously.
    With those, we will find a really elegant way to set up the dependency graph.
    During execution, the graph will parallelize itself in a *seemingly intelligent*
    way in order to calculate the result as fast as possible.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to implement some functions that simulate computation-intensive
    tasks that depend on each other, and let them run as parallel as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first include all the necessary headers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to synchronize concurrent access to `cout`, so let''s use the synchronization
    helper from the other recipe in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s implement three functions which transform strings. The first function
    shall create an `std::string` object from a C-string. We let it sleep for 3 seconds
    to simulate that string creation is computation-heavy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'The next function accepts two string objects as arguments and returns their
    concatenation. We give it 5-second wait time to simulate that this is a time-consuming
    task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'The last computation-heavy function accepts a string and concatenates it with
    itself. It shall take 3 seconds to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'We could now already use those functions in a serial program, but we want to
    get some elegant automatic parallelization. So let''s implement some helpers for
    this. *Attention please*, the following three functions look really complicated.
    `asynchronize` accepts a function `f` and returns a callable object that captures
    it. We can call this callable object with any number of arguments, and then it
    will capture those together with `f` in another callable object which it returns
    to us. This last callable object can be called without arguments. It does then
    call `f` asynchronously with all the arguments it captures:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'The next function will be used by the function we declare in the next step
    afterward. It accepts a function `f`, and captures it in a callable object that
    it returns. That object can be called with a number of future objects. It will
    then call `.get()` on all the futures, apply `f` to them and return its result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'The last helper function does also accept a function `f`. It returns a callable
    object that captures `f`. That callable object can be called with any number of
    callable objects as arguments, which it returns captured together with `f` in
    another callable object. That final callable object can then be called without
    arguments. It does then call all the callable objects that are captured in the
    `xs...` pack. These return futures which need to be unwrapped with `fut_unwrap`.
    The future-unwrapping and actual application of the real function `f` on the real
    values from the futures does again happen asynchronously using `std::async`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'Ok, that was maybe kind of a crazy ride that was slightly reminiscent of the
    movie *"Inception"* because of the lambda expressions that return lambda expressions.
    We will have a very detailed look at this voodoo-code later. Now let''s take the
    functions `create`, `concat`, and `twice` and make them asynchronous. The function
    `async_adapter` makes a completely normal function wait for future arguments and
    return a future result. It is kind of a translating wrapper from the synchronous
    to the asynchronous world. We apply it to `concat` and `twice`. We must use `asynchronize`
    on `create` because it shall return a future, but we will feed it with real values
    instead of futures. The task dependency chain must begin with `create` calls:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have automatically parallelizing functions that have the same names
    as the original synchronous ones, but with a `p`-prefix. Let us now set up a complex
    example dependency tree. First, we create the strings `"foo "` and `"bar "`, which
    we immediately concatenate to `"foo bar "`. This string is then concatenated with
    itself using `twice`. Then we create the strings `"this "` and `"that "`, which
    we concatenate to `"this that "`. Finally, we concatenate the results to `"foo
    bar foo bar this that "`. The result shall be saved in the variable `callable`.
    Then finally call `callable().get()` in order to start the computation and wait
    for its return value, in order to also print that. No computation is done before
    we call `callable()`, and after this call, all the magic starts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running the program shows that all the `create` calls are performed
    at the same time, and then the other calls are performed. It looks as if they
    were scheduled intelligently. The whole program runs for 16 seconds. If the steps
    were not performed in parallel, it would take 30 seconds to complete. Note that
    we need a system with at least four CPU cores to be able to perform all `create`
    calls at the same time. If the system had fewer CPU cores, then some calls would
    have to share CPUs which would of course then consume more time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A plain serial version of this program without any `async` and `future` magic
    would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: In this recipe, we wrote the helper functions `async_adapter` and `asynchronize`
    that helped us create new functions from `create`, `concat`, and `twice`. We called
    those new asynchronous functions `pcreate`, `pconcat`, and `ptwice`.
  prefs: []
  type: TYPE_NORMAL
- en: Let us first ignore the complexity of the implementation of `async_adapter`
    and `asynchronize`, in order to first have a look what this got us.
  prefs: []
  type: TYPE_NORMAL
- en: 'The serial version looks similar to this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'The parallelized version looks similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: Okay, now we get at the complicated part. The type of the parallelized result
    is not `string`, but a callable object that returns a `future<string>` on which
    we can call `get()`. This might indeed look crazy at first.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how and *why* did we exactly end up with callable objects that return futures?
    The problem with our `create`, `concat`, and `twice` methods is, that they are
    *slow*. (okay, we made them artificially slow, because we tried to model real
    life tasks that consume a lot of CPU time). But we identified that the dependency
    tree which describes the data flow has independent parts that could be executed
    in parallel. Let''s have a look at two example schedules:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8745f882-ba89-481e-bfcd-06203d12370f.png)'
  prefs: []
  type: TYPE_IMG
- en: On the left side, we see a *single core* schedule. All the function calls have
    to be done one after each other because we have only a single CPU. That means,
    that when `create` costs 3 seconds, `concat` costs 5 seconds and `twice` costs
    3 seconds, it will take 30 seconds to get the end result.
  prefs: []
  type: TYPE_NORMAL
- en: On the right side, we see a *parallel schedule* where as much is done in parallel
    as the dependencies between the function calls allow. In an ideal world with four
    cores, we can create all substrings at the same time, then concatenate them and
    so on. The minimal time to get the result with an optimal parallel schedule is
    16 seconds. We cannot go faster if we cannot make the function calls themselves
    faster. With just four CPU cores we can achieve this execution time. We measurably
    achieved the optimal schedule. How did it work?
  prefs: []
  type: TYPE_NORMAL
- en: 'We could naively write the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: This is a good start for `a`, `b`, `c`, and `d`, which represent the four substrings
    to begin with. These are created asynchronously in the background. Unfortunately,
    this code blocks on the line where we initialize `e`. In order to concatenate
    `a` and `b`, we need to call `get()` on both of them, which *blocks* until these
    values are *ready*. Obviously, this is not a good idea, because the parallelization
    stops being parallel on the first `get()` call. We need a better strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, so let us roll out the complicated helper functions we wrote. The first
    one is `asynchronize`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'When we have a function `int f(int, int)` then we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '`f2` is our asynchronous version of `f`. It can be called with the same arguments
    like `f`, because it *mimics* `f`. Then it returns a callable object, which we
    save in `f3`. `f3` now captures `f` and the arguments `1, 2`, but it did not call
    anything yet. This is just about the capturing.'
  prefs: []
  type: TYPE_NORMAL
- en: When we call `f3()` now, then we finally get a future, because `f3()` does the
    `async(launch::async, **f, 1, 2**);` call! In that sense, the semantic meaning
    of `f3` is "*Take the captured function and the arguments, and throw them together
    into `std::async`.*".
  prefs: []
  type: TYPE_NORMAL
- en: 'The inner lambda expression that does not accept any arguments gives us an
    indirection. With it, we can set up work for parallel dispatch but do not have
    to call anything that blocks, *yet*. We follow the same principle in the much
    more complicated function `async_adapter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: This function does also first return a function that mimics `f` because it accepts
    the same arguments. Then that function returns a callable object that again accepts
    no arguments. And then that callable object finally differs from the other helper
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does the `async(launch::async, fut_unwrap(f), xs()...);` line mean? The
    `xs()...` part means, that all arguments that are saved in pack `xs` are assumed
    to be callable objects (like the ones we are creating all the time!), and so they
    are all called without arguments. Those callable objects that we are producing
    all the time themselves produce future values, on which we can call `get()`. This
    is where `fut_unwrap` comes into play:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '`fut_unwrap` just transforms a function `f` into a function object that accepts
    a range of arguments. This function object does then call `.get()` on *all* of
    them and then finally forwards them to `f`.'
  prefs: []
  type: TYPE_NORMAL
- en: Take your time to digest all of this. When we used this in our main function,
    then the `auto result (pconcat(...));` call chain did just construct a large callable
    object that contains all functions and all arguments. No `async` call was done
    at this point yet. Then, when we called `result()`, we *unleashed a little avalanche*
    of `async` and `.get()` calls that come just in the right order to not block each
    other. In fact, no `get()` call happens before not all `async` calls have been
    dispatched.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we can finally call `.get()` on the future value that `result()`
    returned, and there we have our final string.
  prefs: []
  type: TYPE_NORMAL
