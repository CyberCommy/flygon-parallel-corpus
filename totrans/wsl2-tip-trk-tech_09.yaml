- en: '*Chapter 7*: Working with Containers in WSL'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Containers are a hot topic as a way of packaging and managing applications.
    While there are both Windows and Linux flavors of containers, since this is a
    book about WSL, we will focus on Linux containers and Docker containers in particular.
    If you want to learn about Windows containers, this link is a good starting point:
    [https://docs.microsoft.com/virtualization/windowscontainers/](https://docs.microsoft.com/virtualization/windowscontainers/)'
  prefs: []
  type: TYPE_NORMAL
- en: After covering what a container is and getting Docker installed, this chapter
    will guide you through running a prebuilt Docker container before taking you through
    how to build a container image for your own application using a Python web application
    as an example. After creating the container image, you will take a quick tour
    of some key components of Kubernetes and then see how to use these components
    to host the containerized application inside Kubernetes, all running in WSL.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and using Docker with WSL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a container with Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and running a web application in Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing orchestrators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Kubernetes in WSL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a web application in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll start the chapter by exploring what a container is.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containers provide a way of packaging up an application and its dependencies.
    This description might feel a bit like a **virtual machine** (**VM**), where you
    have a file system that you can install application binaries in and then run later.
    When you run a container, however, it feels more like a process, both in the speed
    with which it starts and the amount of memory it consumes. Under the covers, containers
    are a set of processes that are isolated through the use of features such as **Linux
    namespaces** and **control groups** (**cgroups**), to make it seem like those
    processes are running in their own environment (including with their own file
    system). Containers share the kernel with the host operating system so are less
    isolated than VMs, but for many purposes, this isolation is sufficient, and this
    sharing of host resources enables the low memory consumption and rapid start up
    time that containers can achieve.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to container execution, Docker also makes it easy to define what
    makes up a container (referred to as a container image) and to publish container
    images in a registry where they can be consumed by other users.
  prefs: []
  type: TYPE_NORMAL
- en: We will see this in action a little later in the chapter, but first, let's get
    Docker installed.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and using Docker with WSL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The traditional approach to running Docker on a Windows machine is to use Docker
    Desktop (https://www.docker.com/products/docker-desktop), which will create and
    manage a Linux VM for you and run the Docker service as a daemon in that VM. The
    downside of this is that the VM takes time to start up and has to pre-allocate
    enough memory to accommodate running various containers for you.
  prefs: []
  type: TYPE_NORMAL
- en: With WSL2, it became possible to install and run the standard Linux Docker daemon
    inside a WSL **distribution (distro)**. This had the benefits of starting up more
    quickly and consuming a smaller amount of memory on startup, and only increasing
    the memory consumption when you run containers. The downside was that you had
    to install and manage the daemon yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there is now a third option, which is to install Docker Desktop
    and enable the WSL backend. With this approach, you keep the convenience of Docker
    Desktop from an installation and management perspective. The difference is that
    Docker Desktop runs the daemon in WSL for you, giving you the improvements to
    start up time and memory usage without losing ease of use.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, download and install Docker Desktop from https://www.docker.com/products/docker-desktop.
    When installed, right-click on the Docker icon in your system icon tray and choose
    **Settings**. You will see the following screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – A screenshot of the Docker settings showing the WSL 2 option'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.1_B16412.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – A screenshot of the Docker settings showing the WSL 2 option
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows the **Use the WSL 2 based engine** option. Ensure
    this option is ticked to configure Docker Desktop to run under WSL 2 rather than
    a traditional VM.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can choose which distros Docker Desktop integrates with from the **Resources**
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – A screenshot of the Docker settings for WSL integration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.2_B16412.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – A screenshot of the Docker settings for WSL integration
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding screenshot, you can control which distros you
    want Docker Desktop to integrate with. When you choose to integrate with a WSL
    distro, the socket for the Docker daemon is made available to that distro and
    the docker **command-line interface** (**CLI**) is added for you. Select all the
    distros you want to be able to use Docker from and click **Apply & Restart**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once Docker has restarted, you will be able to use the `docker` CLI to interact
    with Docker from any of the selected WSL distros, for example, `docker info`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This snippet shows some of the output from running `docker info` and you can
    see that the server is running on `linux` with a kernel of `4.19.104-microsoft-standard`,
    which is the same as the WSL kernel version on my machine (you can check this
    on your machine by running `uname -r` from your WSL distro).
  prefs: []
  type: TYPE_NORMAL
- en: More information on installing and configuring Docker Desktop with WSL can be
    found in the Docker documentation at https://docs.docker.com/docker-for-windows/wsl/.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have Docker installed, let's get started by running a container.
  prefs: []
  type: TYPE_NORMAL
- en: Running a container with Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As was mentioned earlier, Docker gives us a standardized way of packaging up
    a container image. These container images can be shared through Docker registries,
    and Docker Hub (https://hub.docker.com/) is a commonly used registry for publicly
    available images. In this section, we will run a container with the `nginx` web
    server using the `docker run -d --name docker-nginx -p 8080:80 nginx` command
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The last part of the command we just ran tells Docker what container image
    we want to run (`nginx`). This snippet of output shows that Docker didn''t find
    the `nginx` image locally, so it has started to pull it (that is, download it)
    from Docker Hub. Container images consist of a number of layers (we''ll discuss
    this more later in the chapter) and in the output, one layer already exists and
    another is being downloaded. The `docker` CLI keeps updating the output as the
    download progresses, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When Docker has finished pulling the image, you will see something similar
    to the preceding output, which confirms that Docker has pulled the image and prints
    the ID of the container it created (`336ab5bed2d5…`). At this point, we can run
    `docker ps` to list the running containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This output shows a single container running and we can see that the container
    ID `336ab5bed2d5` value matches the start of the container ID output from `docker
    run`. By default, `docker ps` outputs the short form of the container ID, whereas
    `docker run` outputs the full container ID value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s return to the command we used to run a container: `docker run -d --name
    docker-nginx -p 8080:80 nginx`. This has various parts to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-d` tells Docker to run this container detached from our terminal, that is,
    to run it in the background.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--name` tells Docker to use a specific name, `docker-nginx`, for the container
    rather than generating a random one. This name can also be seen in the `docker
    ps` output and can be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-p` allows us to map ports on the host to ports inside the running container.
    The format is `<host port>:<container port>`, so in the case of `8080:80`, we
    have mapped port `8080` on our host to port `80` inside the container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The final argument is the name of the image to run: `nginx`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since port `80` is the default port that `nginx` serves content on and we have
    mapped port `8080` to that container port, we can open our web browser to `http://localhost:8080`,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – A screenshot of the browser showing nginx output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.3_B16412.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – A screenshot of the browser showing nginx output
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows the output from nginx in a web browser. At this
    point, we have used a single command (`docker run`) to download and run nginx
    in a Docker container. Container resources have a level of isolation, which means
    that the port `80` that nginx is serving traffic on inside the container isn't
    visible externally, so we mapped that to port `8080` outside the container. Since
    we're running Docker Desktop with the WSL 2 backend, that port `8080` is actually
    exposed on the WSL 2 VM, but thanks to the magic we saw in [*Chapter 4*](B16412_04_Final_JC_ePub.xhtml#_idTextAnchor047),
    *Windows to Linux Interoperability*, in the *Accessing Linux web applications
    from Windows* section, we can access that at `http://localhost:8080` from Windows.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we leave the container running, it will continue to consume resources, so
    let''s stop and delete it before we move on, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this output, you can see `docker stop docker-nginx`, which stops the running
    container. At this point, it is no longer consuming memory or CPU, but it still
    exists and references the image that was used to create it, which prevents that
    image from being deleted. So, after stopping the container, we use `docker rm
    docker-nginx` to delete it. To free up disk space, we can also clean up the `nginx`
    image by running `docker image rm nginx:latest`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've seen how to run a container, let's build our own container image
    to run.
  prefs: []
  type: TYPE_NORMAL
- en: Building and running a web application in Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will build a Docker container image that packages a Python
    web application. This container image will include the web application and all
    its dependencies so that it can be run on a machine that has the Docker daemon
    installed.
  prefs: []
  type: TYPE_NORMAL
- en: To follow along with this example, make sure that you have the code for the
    book (from [https://github.com/PacktPublishing/Windows-Subsystem-for-Linux-2-WSL-2-Tips-Tricks-and-Techniques](https://github.com/PacktPublishing/Windows-Subsystem-for-Linux-2-WSL-2-Tips-Tricks-and-Techniques))
    cloned in a Linux distro and then open a terminal and navigate to the `chapter-07/01-docker-web-app`
    folder, which contains the sample application we will use here. Check the `README.md`
    file for instructions on installing the dependencies needed to run the application.
  prefs: []
  type: TYPE_NORMAL
- en: The sample application is built on the **Flask** web framework for Python (https://github.com/pallets/flask)
    and uses the **Gunicorn HTTP server** to host the application (https://gunicorn.org/).
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep the focus of the chapter on Docker containers, the application has
    a single code file, `app.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As the code shows, there is a single endpoint for the home page defined, which
    returns a message showing the hostname for the machine where the web server is
    running.
  prefs: []
  type: TYPE_NORMAL
- en: 'The application can be run using `gunicorn --bind 0.0.0.0:5000 app:app` and
    we can open `http://localhost:5000` in our web browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – A screenshot showing the sample app in a web browser'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.4_B16412.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.4 – A screenshot showing the sample app in a web browser
  prefs: []
  type: TYPE_NORMAL
- en: In this screenshot, you can see the response from the sample application, showing
    the hostname (`wfhome`) that the app is running on.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have seen the sample application in action, we will start looking
    at how to package it as a container image.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Dockerfiles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To build an image, we need to be able to describe to Docker what the image
    should contain, and for this, we will use a `Dockerfile`. A `Dockerfile` contains
    a series of commands for Docker to execute in order to build a container image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This Dockerfile contains a number of commands. Let''s look at them:'
  prefs: []
  type: TYPE_NORMAL
- en: The `FROM` command specifies the base image that Docker should use, in other
    words, the starting content for our container image. Any applications and packages
    installed in the base image become part of the image that we build on top of it.
    Here, we have specified the `python:3.8-slim-buster` image, which provides an
    image based on `python:3.8-buster` image, which includes a number of common packages
    in the image, but this makes the base image larger. Since this application only
    uses a few packages, we are using the `slim` variant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EXPOSE` indicates that we want to expose a port (`5000` in this case, as that
    is the port that the web application will listen on).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use the `ADD` command to add content to the container image. The first parameter
    to `ADD` specifies the content to add from the `host` folder, and the second parameter
    specifies where to place it in the container image. Here, we are adding `requirements.txt`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `RUN` command is used to perform a `pip install` operation using the `requirements.txt`
    file that we just added to the image with the help of the `ADD` command.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WORKDIR` is used to set the working directory in the container to `/app`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ADD` is used again to copy the full application contents into the `/app` directory.
    We''ll discuss why the application files have been copied in with two separate
    `ADD` commands in the next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, the `CMD` command specifies what command will be executed when a container
    is run from this image. Here, we specify the same `gunicorn` command that we just
    used to run the web application locally.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a `Dockerfile`, let's take a look at using it to build an image.
  prefs: []
  type: TYPE_NORMAL
- en: Building the image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To build a container image, we will use the `docker build` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have used the `-t` switch to specify that the resulting image should
    be tagged as `simple-python-app`. This is the name of the image that we can use
    later to run a container from the image. Finally, we tell Docker what directory
    to use as the build context, and here, we used `.` to indicate the current directory.
    The build context specifies what content is packaged up and passed to the Docker
    daemon to use for building the image – when you `ADD` a file to the `Dockerfile`,
    it is copied from the build context.
  prefs: []
  type: TYPE_NORMAL
- en: The output from this command is quite long, so rather than including it in full,
    we'll take a look at a few key pieces.
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial output is from the `FROM` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, you can see that Docker has determined that it doesn't have the base image
    locally, so has pulled it from Docker Hub, just like when we ran the `nginx` image
    previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'A little further down the output, we can see that `pip install` has been executed
    to install the application requirements in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding snippet, you can see the output of `pip install` as it installs
    `flask` and `gunicorn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the output, we see a couple of success messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The first of these success messages gives the ID of the image that we just
    created (`747c4a9481d8`), and the second shows that it has been tagged using the
    tag we specified (`simple-python-app`). To see the Docker images on your local
    machine, we can run `docker image ls`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In this output, we can see the `simple-python-app` image we just built. Now
    that we have built a container image, we are ready to run it!
  prefs: []
  type: TYPE_NORMAL
- en: Running the image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we saw previously, we can run the container with the `docker run` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, you can see that we are running the `simple-python-app` image as a container
    named `chapter-07-example` and have exposed port `5000`. The command output shows
    the ID of the container that we just started.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the container running, we can open `http://localhost:5000` in a web browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – A screenshot showing the containerized sample app in the web
    browser'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.5_B16412.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.5 – A screenshot showing the containerized sample app in the web browser
  prefs: []
  type: TYPE_NORMAL
- en: In this screenshot, we can see the output from the sample app. Notice that the
    hostname it has output matches the start of the container ID in the output from
    the `docker run` command. When the isolated environment for a container is created,
    the hostname is set to the short form of the container ID.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an initial version of the container built and running, let's
    take a look at modifying the application and rebuilding the image.
  prefs: []
  type: TYPE_NORMAL
- en: Rebuilding the image with changes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When developing an application, we will make changes to the source code. To
    simulate this, make a simple change to the message in `app.py` (for example, change
    `Hello from` to `Coming to you from`). Once we have made this change, we can rebuild
    the container image using the same `docker build` command we used previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The output this time is a little different. Aside from the base image not being
    pulled (because we already have the base image downloaded), you might also note
    a number of lines with `---> Using cache`. When Docker runs the commands in the
    `Dockerfile`, each line (with a couple of exceptions) creates a new container
    image and the subsequent commands build upon that image just like we build on
    top of the base image. These images are often referred to as layers due to the
    way they build upon each other. When building an image, if Docker determines that
    the files used in a command match the previously built layer, then it will reuse
    that layer and indicate this with the `---> Using cache` output. If the files
    don't match, then Docker runs the command and invalidates the cache for any later
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This layer caching is why we split out `requirements.txt` from the main application
    content in the `Dockerfile` for the application: installing the requirements is
    typically a slow operation and, generally, the rest of the application files change
    more frequently. Splitting out the requirements and performing `pip install` before
    copying the application code ensures that the layer caching works with us as we
    develop the application.'
  prefs: []
  type: TYPE_NORMAL
- en: We've seen a range of Docker commands here; if you want to explore further (including
    how to push an image to a registry), take a look at the *Docker 101 tutorial*
    at https://www.docker.com/101-tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we've seen how to build container images and how to run containers,
    whether our own images or those from Docker Hub. We've also seen how layer caching
    can speed up the development cycle. These are all foundational steps and, in the
    next section, we'll start to take a look at orchestrators, which are the next
    layer up for building systems using containers.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing orchestrators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we saw how we can use the capabilities of Docker to
    easily package our application as a container image and run it. If we push our
    image to a Docker registry, then it becomes simple to pull and run that application
    from any machine with Docker installed. Larger systems, however, are made up of
    many such components and we will likely want to distribute these across a number
    of Docker hosts. This allows us to adapt to a changing load on the system by increasing
    or decreasing the number of instances of a component container that are running.
    The way to get these features with a containerized system is to use an orchestrator.
    Orchestrators provide other features, such as automatically restarting failed
    containers, running containers on a different host if a host fails, and a stable
    way to communicate with containers as they potentially restart and move between
    hosts.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of container orchestrators, such as **Kubernetes**, **Docker
    Swarm**, and **Mesosphere DC/OS** (built on Apache Mesos with Marathon). These
    orchestrators all provide slightly different features and ways of implementing
    the requirements we just described. Kubernetes has become a very popular orchestrator,
    and all the major cloud vendors have a Kubernetes offering (it even has support
    in Docker Enterprise and Mesosphere DC/OS). We will spend the rest of this chapter
    looking at how to create a Kubernetes development environment in WSL and run an
    application on it.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Kubernetes in WSL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is no shortage of options for installing Kubernetes, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Kind ([https://kind.sigs.k8s.io/](https://kind.sigs.k8s.io/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minikube ([https://kubernetes.io/docs/tasks/tools/install-minikube/](https://kubernetes.io/docs/tasks/tools/install-minikube/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MicroK8s (https://microk8s.io/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k3s ([https://k3s.io/](https://k3s.io/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first of these, Kind, stands for Kubernetes in Docker and was designed for
    testing Kubernetes. As long as your build tool can run Docker containers, it can
    be a good option as a way to run Kubernetes as part of your integration tests
    in your automated builds. By default, Kind will create a single-node Kubernetes
    cluster but you can configure it to run multi-node clusters, where each node is
    run as a separate container *(*we will see how to use Kind in[*Chapter 10*](B16412_10_Final_JC_ePub.xhtml#_idTextAnchor125)*,
    Visual Studio Code and Containers* in the *Working with Kubernetes in dev container*
    section*)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this chapter, however, we will use the built-in Kubernetes capabilities
    in Docker Desktop, which provides a convenient way to enable a Kubernetes cluster
    that is managed for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – A screenshot showing Kubernetes enabled in Docker Desktop'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.6_B16412.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 – A screenshot showing Kubernetes enabled in Docker Desktop
  prefs: []
  type: TYPE_NORMAL
- en: In this screenshot, you can see the **Kubernetes** page of the Docker Desktop
    settings, with the **Enable Kubernetes** option. By ticking this option and clicking
    **Apply & Restart**, Docker Desktop will install a Kubernetes cluster for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as we''ve been using the `docker` CLI to interact with Docker, Kubernetes
    has its own CLI, `kubectl`. We can use `kubectl` to check that we are able to
    connect to the Kubernetes cluster that Docker Desktop has created for us with
    the `kubectl cluster-info` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This output shows that `kubectl` has successfully connected to the Kubernetes
    cluster at `kubernetes.docker.internal`, indicating that we're using the *Docker
    Desktop Kubernetes integration*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a Kubernetes cluster running, let's look at running an application
    in it.
  prefs: []
  type: TYPE_NORMAL
- en: Running a web application in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes introduces a few new terms, the first of these is a pod. **Pods**
    are the way to run a container in Kubernetes. When we ask Kubernetes to run a
    pod, we specify some details, such as the image we want it to run. Orchestrators
    such as Kubernetes are designed to enable us to run multiple components as part
    of a system, including being able to scale out the number of instances of components.
    To help serve this goal, Kubernetes adds another concept called **deployments**.
    Deployments are built on pods and allow us to specify how many instances of the
    corresponding pod we want Kubernetes to run, and this value can be changed dynamically,
    enabling us to scale out (and in) our application.
  prefs: []
  type: TYPE_NORMAL
- en: We'll take a look at creating a deployment in a moment, but first, we need to
    create a new tag for our sample application. When we built the Docker image previously,
    we used the `simple-python-app` tag. Each tag has one or more associated versions
    and since we didn't specify the version, it is assumed to be `simple-python-app:latest`.
    When working with Kubernetes, using the *latest* image version means that Kubernetes
    will try to pull the image from a registry, even if it has the image locally.
    Since we haven't pushed our image to a registry, this will fail. We could rebuild
    the image, specifying `simple-python-app:v1` as the image name, but since we have
    already built the image, we can also create a new tagged version of our image
    by running `docker tag simple-python-app:latest simple-python-app:v1`. Now we
    have two tags referring to the same image, but by using the `simple-python-app:v1`
    tag, Kubernetes will only attempt to pull the image if it doesn't exist locally
    already. With our new tag in place, let's start deploying our application to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step of deploying our sample application to Kubernetes is to create
    a deployment object in Kubernetes. Using the versioned tag for our container image,
    we can use `kubectl` to create a deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This output shows the creation of a deployment called `chapter-07-example`
    running the `simple-python-app:v1` image. After creating the deployment, it shows
    `kubectl get deployments` used to list the deployments and get summary information
    about the state of the deployment. Here, `1/1` in the `READY` column indicates
    that the deployment is configured to have one instance of the pod running and
    that it is available. If the application running in our pod crashes, Kubernetes
    will (by default) automatically restart it for us. We can run `kubectl get pods`
    to see the pod that the deployment has created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In this output, we can see that the pod has been created with a name starting
    with the deployment name followed by a random suffix.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, one benefit of using a deployment over a pod is the
    ability to scale it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, we see the `kubectl scale` command being used on the `chapter-07-example`
    deployment to set the number of replicas to two, in other words, to scale the
    deployment to two pods. After scaling, we run `kubectl get pods` again and can
    see that we have a second pod created.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with kubectl, you can improve your productivity by enabling bash
    completion. To configure this, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '`echo ''source <(kubectl completion bash)'' >>~/.bashrc`'
  prefs: []
  type: TYPE_NORMAL
- en: This adds kubectl bash completion to your `.bashrc` file, so you will need to
    restart Bash to enable it (for full details see [https://kubernetes.io/docs/tasks/tools/install-kubectl/#optional-kubectl-configurations](https://kubernetes.io/docs/tasks/tools/install-kubectl/#optional-kubectl-configurations)),
  prefs: []
  type: TYPE_NORMAL
- en: 'With this change, you can now type the following (press the *Tab* key in place
    of `<TAB>`):'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl sc<TAB> dep<TAB> chap<TAB> --re<TAB>2`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The end result of this with bash completion is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl scale deployment chapter-07-example --replicas=2`'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this saves time entering commands and supports completion for
    both commands (such as `scale`) and resource names (`chapter-07-example`).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the application deployed, let's look at how to access it.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a service
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we want to be able to access the web application running as the `chapter-07-example`
    deployment. Since we can have instances of the web application running across
    pods, we need a way to access the set of pods. For this purpose, Kubernetes has
    a concept called `kubectl expose` to create a service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, we run `kubectl expose`, instructing Kubernetes to create a service for
    our `chapter-07-example` deployment. We specify `NodePort` as the service type,
    which makes the service available on any node in the cluster, and pass `5000`
    as the port that the service targets to match the port that our web application
    is listening on. Next, we run `kubectl get services`, which shows the new `chapter-07-example`
    service. Under the `PORT(S)` column, we can see `5000:30123/TCP`, indicating that
    the service is listening on port `30123` and will forward traffic to port `5000`
    on the pods in the deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to the way Docker Desktop sets up the networking for the Kubernetes
    cluster (and the WSL forwarding of `localhost` from Windows to WSL), we can open
    `http://localhost:30123` in a web browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – A screenshot showing the Kubernetes web application in the browser'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.7_B16412.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.7 – A screenshot showing the Kubernetes web application in the browser
  prefs: []
  type: TYPE_NORMAL
- en: This screenshot shows the web application loaded in a browser and the hostname
    that it displays matches one of the pod names we saw when we listed the pods after
    scaling the deployment. If you refresh the page a few times, you will see the
    name changes between the pod names we created after scaling the deployment, showing
    that the Kubernetes service we created is distributing the traffic between the
    pods.
  prefs: []
  type: TYPE_NORMAL
- en: We have been interactively running `kubectl` commands to create deployments
    and services, but a powerful aspect of Kubernetes is its support for declarative
    deployments. Kubernetes allows you to define objects such as deployments and services
    in files written in the `YAML` format. In this way, you can specify multiple aspects
    of your system and then pass the set of `YAML` files to Kubernetes in one go and
    Kubernetes will create them all. You can later update the *YAML* specification
    and pass it to Kubernetes, and it will reconcile the differences in the specification
    to apply your changes. An example of this is in the code accompanying the book
    in the `chapter-07/02-deploy-to-kubernetes` folder (refer to the `README.md` file
    in the folder for instructions on how to deploy).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we've taken a look at how to deploy our web application packaged
    as a container image using a Kubernetes deployment. We saw how this creates pods
    for us and allows us to dynamically scale the number of pods that we have running.
    We also saw how we can use Kubernetes to create a service that distributes traffic
    across the pods in our deployment. This service gives a logical abstraction over
    the pods in the deployment and handles scaling the deployment as well as pods
    that have restarted (for example, if it has crashed). This gives a good starting
    point for working with Kubernetes, and if you want to take it further, Kubernetes
    has a great interactive tutorial at [https://kubernetes.io/docs/tutorials/kubernetes-basics/](https://kubernetes.io/docs/tutorials/kubernetes-basics/).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested in digging deeper into using *Docker* or *Kubernetes*
    for building applications, the following links give a good starting point (with
    further links to other content):'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.docker.com/develop/](https://docs.docker.com/develop/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you''ve been introduced to containers and have seen how they
    enable an application and its dependencies to be packaged together to enable it
    to be run simply on a machine with the Docker daemon running. We discussed Docker
    registries as a way of sharing images, including the commonly used public registry:
    `docker` CLI and used this to run the `nginx` image from Docker Hub, with Docker
    automatically pulling the image to the local machine from Docker Hub.'
  prefs: []
  type: TYPE_NORMAL
- en: After running the `nginx` image, you saw how to build an image from a custom
    web application using steps defined in a `Dockerfile`. You saw how Docker builds
    image layers for steps in the `Dockerfile` and reuses them in subsequent builds
    if files haven't changed, and also how this can be used to improve subsequent
    build times by carefully structuring the `Dockerfile` so that the most commonly
    changing content is added in later steps.
  prefs: []
  type: TYPE_NORMAL
- en: After looking at how to work with Docker, you were introduced to the concept
    of container orchestrators, before taking a look at Kubernetes. With Kubernetes,
    you saw how you can use different types of resources, such as pods, deployments,
    and services, to deploy an application. You saw how Kubernetes deployments build
    on pods to allow you to easily scale the number of instances of the pod running
    with a single command, and how to use Kubernetes services to provide an easy and
    consistent way to address the pods in a deployment independent of the scaling.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will turn our attention more directly to WSL, where
    a knowledge of building and working with containers will prove useful.
  prefs: []
  type: TYPE_NORMAL
