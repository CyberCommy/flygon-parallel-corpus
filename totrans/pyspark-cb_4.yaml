- en: Preparing Data for Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover how to clean up your data and prepare it for
    modeling. You will learn the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Handling duplicates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling missing observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring descriptive statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing correlations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawing histograms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing interactions between features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a thorough understanding of how RDDs and DataFrames work and
    what they can do, we can start preparing ourselves and our data for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Someone famous (Albert Einstein) once said (paraphrasing):'
  prefs: []
  type: TYPE_NORMAL
- en: '"The universe and the problems with any dataset are infinite, and I am not
    sure about the former."'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding is of course a joke. However, any dataset you work with, be it
    acquired at work, found online, collected yourself, or obtained through any other
    means, is dirty until proven otherwise; you should not trust it, you should not
    play with it, you should not even look at it until such time that you have proven
    to yourself that it is sufficiently clean (there is no such thing as totally clean).
  prefs: []
  type: TYPE_NORMAL
- en: 'What problems can your dataset have? Well, to name a few:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Duplicated observations**: These arise through systemic and operator''s faults'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing observations**: These can emerge due to sensor problems, respondents''
    unwillingness to provide an answer to a question, or simply some data corruption'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aanomalous observations**: Observations that, when you look at them, stand
    out when compared with the rest of the dataset or a population'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoding**: Text fields that are not normalized (for example, words are not
    stemmed or use synonyms), in different languages, or you can encounter gibberish
    text input, and date and date time fields may not encoded the same way'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Untrustworthy answers (true especially for surveys)**: When respondents lie
    for any reason; this type of dirty data is much harder to work with and clean
    up'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, your data might be plagued by thousands upon thousands of traps
    that are just waiting for you to fall for them. Cleaning up the data and getting
    familiar with it is what we (as data scientists) do 80% of the time (the remaining
    20% we spend building models and complaining about cleaning data). So fasten your
    seatbelt and prepare for *a bumpy ride* that is necessary for us to trust the
    data that we have and get familiar with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will work with a small dataset of `22` records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Throughout the subsequent recipes, we will clean up the preceding dataset and
    learn a little bit more about it.
  prefs: []
  type: TYPE_NORMAL
- en: Handling duplicates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Duplicates show up in data for many reasons, but sometimes it's really hard
    to spot them. In this recipe, we will show you how to spot the most common ones
    and handle them using Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. If you
    do not have one, you might want to go back to [Chapter 1](part0026.html#OPEK0-dc04965c02e747b9b9a057725c821827),
    *Installing and Configuring Spark,* and follow the recipes you will find there.
  prefs: []
  type: TYPE_NORMAL
- en: We will work on the dataset from the introduction. All the code that you will
    need in this chapter can be found in the GitHub repository we set up for the book: [http://bit.ly/2ArlBck](http://bit.ly/2ArlBck).
    Go to `Chapter04` and open the [4.Preparing data for modeling.ipynb](https://github.com/drabastomek/PySparkCookbook/blob/devel/Chapter04/4.Preparing%20data%20for%20modeling.ipynb) notebook.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A duplicate is a record in your dataset that appears more than once. It is
    an exact copy. Spark DataFrames have a convenience method to remove the duplicated
    rows, the `.dropDuplicates()` transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check whether any rows are duplicated, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If any are duplicates, remove them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should know this one by now, but the `.count()` method counts how many
    rows there are in our DataFrame. The second command checks how many distinct rows
    we have. Execute these two commands on our `dirty_data`. DataFrame produces `(22,
    21)` as the result. So, we now know that we have two records in our dataset that
    are exact copies of each other. Let''s see which ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let's unpack what's happening here. First, we use the `.groupby(...)` method
    to define what columns to use for the aggregation; in this example, we essentially
    use all of them as we want to find all the distinct combinations of all the columns
    in our dataset. Next, we count how many times such a combination of values occurs
    using the `.count()` method; the method adds the `count` column to our dataset.
    Using the `.filter(...)` method, we select all the rows that occur in our dataset
    more than once and print them to the screen using the `.show()` action.
  prefs: []
  type: TYPE_NORMAL
- en: 'This produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So, the row with `Id` equal to `16` is the duplicated one. So, let's drop it
    using the `.dropDuplicates(...)` method. Finally, running the `full_removed.count()`
    command confirms that we now have 21 records.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, there's more to it, as you might imagine. There are still some records
    that are duplicated in our `full_removed` DataFrame. Let's have a closer look.
  prefs: []
  type: TYPE_NORMAL
- en: Only IDs differ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you collect data over time, you might record the same data but with different
    IDs. Let''s check whether our DataFrame has any such records. The following snippet
    will help you do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like before, we first group by all the columns but we exclude the `''Id''`
    column, then count how many records we get given from this grouping, and finally
    we extract those with `''count > 1''` and show them on the screen. After running
    the preceding code, here''s what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00079.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we have four records with different IDs but that are the same
    cars: the `BMW` `440i Coupe` and the `Hyundai` `G80 AWD`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We could also check the counts, like before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `.dropDuplicates(...)` method can handle such situations easily. All we
    need to do is to pass to the `subset` parameter a list of all the columns we want
    it to consider while searching for the duplicates. Here''s how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, we select all the columns but the `''Id''` columns to define which
    columns to use to determine the duplicates. If we now count the total number of
    rows in the `id_removed` DataFrame, we should get `19`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00080.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: And that's precisely what we got!
  prefs: []
  type: TYPE_NORMAL
- en: ID collisions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might also assume that if there are two records with the same ID, they are
    duplicated. Well, while this might be true, we would have already removed them
    by now when dropping the records based on all the columns. Thus, at this point,
    any duplicated IDs are more likely collisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Duplicated IDs might arise for a multitude of reasons: an instrumentation error
    or insufficient data structure to store the IDs, or if the IDs represent some
    hash function of the record elements, there might be collisions arising from the
    choice of the hash function. These are just a few of the reasons why you might
    have duplicated IDs but the records are not really duplicated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check whether this is true for our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this example, instead of subsetting records and then counting the records,
    then counting the distinct records, we will use the `.agg(...)` method. To this
    end, we first import all the functions from the `pyspark.sql.functions` module.
  prefs: []
  type: TYPE_NORMAL
- en: For a list of all the functions available in `pyspark.sql.functions`, please
    refer to [https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions).
  prefs: []
  type: TYPE_NORMAL
- en: 'The two functions we''ll use will allow us to do the counting in one go: the
    `.count(...)` method counts all the records with non-null values in the specified
    column, while the `.countDistinct(...)` returns a count of distinct values in
    such a column. The `.alias(...)` method allows us to specify a friendly name for
    the columns resulting from the counting. Here''s what we get after counting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00081.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'OK, so we have two records with the same IDs. Again, let''s check which IDs
    are duplicated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, we first group by the values in the `''Id''` column, and then show
    all the records with a `count` greater than `1`. Here''s what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00082.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Well, it looks like we have two records with `''Id == 3''`. Let''s check whether
    they''re the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00083.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: These are definitely not the same records but they share the same ID. In this
    situation, we can create a new ID that will be unique (we have already made sure
    we do not have other duplicates in our dataset). PySpark's SQL functions module
    offers a `.monotonically_increasing_id()` method that creates a unique stream
    of IDs.
  prefs: []
  type: TYPE_NORMAL
- en: The `.monotonically_increasing_id()`—generated ID is guaranteed to be unique
    as long as your data lives in less than one billion partitions and with less than
    eight billion records in each. That's a pretty big number.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a snippet that will create and replace our ID column with a unique
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We are creating the ID column first and then selecting all the other columns
    except the original `''Id''` column. Here''s what the new IDs look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00084.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The numbers are definitely unique. We are now ready to handle the other problems
    in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing observations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Missing observations are pretty much the second-most-common issue in datasets.
    These arise for many reasons, as we have already alluded to in the introduction.
    In this recipe, we will learn how to deal with them.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. Also,
    we will be working off of the `new_id` DataFrame we created in the previous recipe,
    so we assume you have followed the steps to remove the duplicated records.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since our data has two dimensions (rows and columns), we need to check the
    percentage of data missing in each row and each column to make a determination
    of what to keep, what to drop, and what to (potentially) impute:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate how many missing observations there are in a row, use the following
    snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To calculate how much data is missing in each column, use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Let's walk through these step by step.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now take a look at how to handle missing observations in rows and columns
    in detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Missing observations per row
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To calculate how much data is missing from a row, it is easier to work with
    RDDs as we can loop through each element of an RDD''s record and count how many
    values are missing. Thus, the first thing we do is we access `.rdd` within our
    `new_id` DataFrame. Using the `.map(...)` transformation, we loop through each
    row, extract `''Id''`, and count how many times an element is missing using the `sum([c
    == None for c in row])` expression. The outcome of these operations is an RDD
    of elements that each has two values: the ID of the row and the count of missing
    values.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we only select those that have more than one missing value and `.collect()`
    those records on the driver. We then create a simple DataFrame, `.orderBy(...)`,
    by the count of missing values in a descending order and show the records.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00085.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, one of the records has five out of eight values missing. Let''s
    see that record:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows that one of the `Mercedes-Benz` records has most of
    its values missing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we can drop the whole observation as there isn''t really much value contained
    in this record. To achieve this goal, we can use the `.dropna(...)` method of
    DataFrames: `merc_out = new_id.dropna(thresh=4)`.'
  prefs: []
  type: TYPE_NORMAL
- en: If you use `.dropna()` without passing any parameters, any record that has a
    missing value will be removed.
  prefs: []
  type: TYPE_NORMAL
- en: We specify `thresh=4`, so we only remove the records that have a minimum of
    four non-missing values; our record has only three useful pieces of information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s confirm: running `new_id.count(), merc_out.count()` produces `(19, 18)`,
    so yes, indeed, we removed one of the records. Did we really remove the `Mercedes-Benz`
    one? Let''s check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00087.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Missing observations per column
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We also need to check whether there are columns with a particularly low incidence
    of useful information. There's a lot of things happening in the code we presented,
    so let's unpack it step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the inner list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We loop through all the columns in the `merc_out` DataFrame and count how many
    non-missing values we find in each column. We then divide it by the total count
    of all the rows and subtract this from 1 so we get the percentage of missing values.
  prefs: []
  type: TYPE_NORMAL
- en: We imported `pyspark.sql.functions` as `fn` earlier in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: However, what we're actually doing here is not really calculating anything.
    The way Python stores this information, at this time, is just as a list of objects,
    or pointers, to certain operations. Only after we pass the list to the `.agg(...)`
    method does it get translated into PySpark's internal execution graph (which only
    gets executed when we call the `.collect()` action).
  prefs: []
  type: TYPE_NORMAL
- en: The `.agg(...)` method accepts a set of parameters, not as a list object, but
    as a comma-separated list of parameters. Therefore, instead of passing the list
    itself to the `.agg(...)` method, we included `'*'` in front of the list, which
    unfolds each element of our list and passes it like a parameter to our method.
  prefs: []
  type: TYPE_NORMAL
- en: The `.collect()` method will return a list of one element—a `Row` object with
    aggregated information. We can transform `Row` into a dictionary using the `.asDict()`
    method and then extract all the `items` from it. This will result in a list of
    tuples, where the first element is the column name (we used the `.alias(...)`
    method to append `'_miss'` to each column) and the second element is the percentage
    of missing observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'While looping through the elements of the sorted list, we just print them to
    the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00088.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Well, it looks like most of the information in the `MSRP` column is missing.
    Thus, we can drop it, as it will not bring us any useful information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We still have two columns with some missing information. Let's do something
    about them.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PySpark allows you to impute the missing observations. You can either pass a
    value that every `null` or `None` in your data will be replaced with, or you can
    pass a dictionary with different values for each column with missing observations.
    In this example, we will use the latter approach and will specify a ratio between
    the fuel economy and displacement, and between the number of cylinders and displacement.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create our dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are effectively calculating our multipliers. In order to replace the
    missing values in the fuel economy, we will use the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00089.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the number of cylinders, we will use the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00090.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Our preceding code uses these two formulas to calculate the multiplier for each
    row and then takes the average of these.
  prefs: []
  type: TYPE_NORMAL
- en: This is not going to be totally accurate but given the data we have, it should
    be accurate enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we also present yet another way of creating a dictionary out of your
    (small!) Spark DataFrame: use the `.toPandas()` method to convert the Spark DataFrame
    to a pandas DataFrame. The DataFrame of pandas has a `.to_dict(...)` method that
    will allow you to convert our data to a dictionary. The `''records''` parameter
    instructs the method to convert each row to a dictionary where the key is the
    column name with the corresponding record value.'
  prefs: []
  type: TYPE_NORMAL
- en: Check out this link to read more about the `.to_dict(...)` method: [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our resulting dictionary looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00091.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s use it now to impute our missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: First, we convert our original data so it also reflects the ratios we specified
    earlier. Next, we use the multipliers dictionary to fill in the missing values,
    and finally we revert the columns to their original state.
  prefs: []
  type: TYPE_NORMAL
- en: Note that each time we use the `.withColumn(...)` method, we overwrite the original
    column names.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting DataFrame looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00092.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the resulting values for the cylinders and the fuel economy
    are not totally accurate but still are arguably better than replacing them with
    some predefined value.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check out PySpark's documentation on the missing observation methods: [https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Observations that differ greatly from the rest of the observations, that is,
    they are located in the long tail(s) of the data distribution, are outliers. In
    this recipe, we will learn how to locate and handle the outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. Also,
    we will be working off of the `imputed` DataFrame we created in the previous recipe,
    so we assume you have followed the steps to handle missing observations.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with a popular definition of an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'A point, ![](img/00093.jpeg), that meets the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00094.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Is not considered an outlier; any point outside this range is. In the preceding
    equation, *Q¹* is the first quartile (25^(th) percentile), *Q³* is the third quartile,
    and *IQR* is the **interquartile range** and is defined as the difference between *Q³* and *Q¹* :
    IQR= *Q³-Q¹*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To flag the outliers, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s calculate our ranges first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we flag the outliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will only be looking at the numerical variables: the displacement, cylinders,
    and the fuel economy.'
  prefs: []
  type: TYPE_NORMAL
- en: We loop through all these features and calculate the first and third quartiles
    using the `.approxQuantile(...)` method. The method takes the feature (column)
    name as its first parameter, the float (or list of floats) of quartiles to calculate
    as the second parameter, and the third parameter specifies the relative target
    precision (setting this value to 0 will find exact quantiles but it can be very
    expensive).
  prefs: []
  type: TYPE_NORMAL
- en: 'The method returns a list of two (in our case) values: *Q¹* and *Q³*. We then
    calculate the interquartile range and append the `(feature_name, [lower_bound,
    upper_bound])` tuple to the `cut_off_point` list. After converting to a dictionary,
    our cut-off points are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00095.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, now we can use these to flag our outlying observations. We will only select
    the ID columns and then loop through our features to check whether they fall outside
    of our calculated bounds. Here''s what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00096.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we have two outliers in the fuel economy column. Let''s check the records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we join our `imputed` DataFrame with the `outliers` one and then we
    filter on the `FuelEconomy_o` flag to select our outlying records only. Finally,
    we just extract the most relevant columns to show:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00097.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So we have `SPARK ACTIV` and `CAMRY HYBRID LE` as the outliers. `SPARK ACTIV`
    became an outlier due to our imputation logic, as we had to impute its fuel economy
    values; given that its engine's displacement is 1.4 liters, our logic didn't work
    out well. Well, there are other ways you can impute the values. The Camry, being
    a hybrid, is definitely an outlier in a dataset dominated by large and turbo-charged
    engines; it should not surprise us to see it here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Trying to build a machine learning model based on data with outliers can lead
    to some untrustworthy results or a model that does not generalize well, so we
    normally remove these from our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check out this website for more information about outliers: [http://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm](http://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring descriptive statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Descriptive statistics are the most fundamental measures you can calculate on
    your data. In this recipe, we will learn how easy it is to get familiar with our
    dataset in PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. Also,
    we will be working off of the `no_outliers` DataFrame we created in the *Handling
    outliers* recipe so we assume you have followed the steps to handle duplicates,
    missing observations, and outliers.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Calculating the descriptive statistics for your data is extremely easy in PySpark.
    Here''s how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: That's it!
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preceding code barely needs an explanation. The `.describe(...)` method
    takes a list of columns you want to calculate the descriptive statistics on and
    returns a DataFrame with basic descriptive statistics: count, mean, standard deviation,
    minimum value, and maximum value.'
  prefs: []
  type: TYPE_NORMAL
- en: You can specify both numeric and string columns as input parameters to `.describe(...)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what we get from running the `.describe(...)` method on our `features`
    list of columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00098.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As expected, we have `16` records in total. Our dataset seems to be skewed (used
    here as a loose term, not in statistical terms) toward larger engines as the mean
    displacement is `3.44` liters with six cylinders. Fuel economy, for such sizable
    engines, seems to be decent, though, at 19 MPG.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you do not pass a list of columns to calculate the descriptive statistics
    over, PySpark will return the statistics for each and every column in your DataFrame.
    Check out the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'It will result in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00099.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, even the string columns got their descriptive statistics which
    are, however, fairly questionable to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive statistics for aggregated columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes you want to calculate some descriptive statistics within a group
    of values. In this example, we will calculate some basic stats for cars with different
    numbers of cylinders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: First, we select our `features` list of columns so we reduce the number of data
    we need to analyze. Next, we aggregate our data over the cylinders column and
    use the (already familiar) `.agg(...)` method to calculate the count, mean, and
    standard deviation over fuel economy and displacement.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are more aggregation functions available in the `pyspark.sql.functions`
    module: `avg(...)`, `count(...)`, `countDistinct(...)`, `first(...)`, `kurtosis(...)`,
    `max(...)`, `mean(...)`, `min(...)`, `skewness(...)`, `stddev_pop(...)`, `stddev_samp(...)`,
    `sum(...)`, `sumDistinct(...)`, `var_pop(...)`, `var_samp(...)`, and `variance(...)`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the resulting table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00100.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can read two things from this table:'
  prefs: []
  type: TYPE_NORMAL
- en: Our imputation method is truly inaccurate, so next time we should come up with
    a better method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MPG_avg` for six cylinder cars is higher than for four cylinder cars and it
    would be suspicious. This is why you should be getting intimate with your data,
    as you can then spot such hidden traps in your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What to do with such finding goes beyond the scope of this book. But, the point
    is that this is why a data scientist would spend 80% of their time cleaning the
    data and getting familiar with it, so the model that is built with such data can
    be relied on.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many other statistics you can calculate on your data that we did not
    cover here (but that PySpark will allow you to calculate). For a more comprehensive
    overview, we suggest you check out this website: [https://www.socialresearchmethods.net/kb/statdesc.php](https://www.socialresearchmethods.net/kb/statdesc.php).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing correlations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Features correlated with the outcome are desirable, but those that are also
    correlated among themselves can make the model unstable. In this recipe, we will
    show you how to calculate correlations between features.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. Also,
    we will be working off of the `no_outliers` DataFrame we created in the *Handling
    outliers* recipe, so we assume you have followed the steps to handle duplicates,
    missing observations, and outliers.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To calculate the correlations between two features, all you have to do is to
    provide their names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: That's it!
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.corr(...)` method takes two parameters, the names of the two features
    you want to calculate the correlation coefficient between.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, only the Pearson correlation coefficient is available.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding command will produce a correlation coefficient equal to `0.938`
    for our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to calculate a correlation matrix, you need to do this somewhat
    manually. Here''s our solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is effectively looping through the list of our `features`
    and computing the pair-wise correlations between them to fill the upper-triangular
    portion of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: We introduced the `features` list in the *Handling outliers *recipe earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The calculated coefficient is then appended to the `temp` list which, in return,
    gets added to the `corr` list. Finally, we create the correlations DataFrame.
    Here''s what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00101.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the only strong correlation is between `Displacement` and `Cylinders`
    and this, of course, comes as no surprise. `FuelEconomy` is not really correlated
    with the displacement as there are other factors that affect `FuelEconomy`, such
    as drag and weight of the car. However, if you were trying to predict, for example,
    maximum speed and assuming (and it is a fair assumption to make) that both `Displacement`
    and `Cylinders` would be highly positively correlated with the maximum speed,
    then you should only use one of them.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing histograms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Histograms are the easiest way to visually *inspect* the distribution of your
    data. In this recipe, we will show you how to do this in PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. Also,
    we will be working off of the `no_outliers` DataFrame we created in the *Handling
    outliers* recipe, so we assume you have followed the steps to handle duplicates,
    missing observations, and outliers.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two ways to produce histograms in PySpark:'
  prefs: []
  type: TYPE_NORMAL
- en: Select feature you want to visualize, `.collect()` it on the driver, and then
    use the matplotlib's native `.hist(...)` method to draw the histogram
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the counts in each histogram bin in PySpark and only return the counts
    to the driver for visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The former solution will work for small datasets (such as ours in this chapter)
    but it will break your driver if the data is too big. Moreover, there''s a good
    reason why we distribute the data so we can do the computations in parallel instead
    of in a single thread. Thus, in this recipe, we will only show you the second
    solution. Here''s the snippet that does all the calculations for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding code is pretty self-explanatory. First, we select the feature
    of interest (in our case, the fuel economy).
  prefs: []
  type: TYPE_NORMAL
- en: The Spark DataFrames do not have a native histogram method, so that's why we
    switch to the underlying RDD.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we flatten our results into a long list (instead of a `Row` object) and
    use the `.histogram(...)` method to calculate our histogram.
  prefs: []
  type: TYPE_NORMAL
- en: The `.histogram(...)` method accepts either an integer that would specify the
    number of buckets to allocate our data to or a list with a specified bucket limit.
  prefs: []
  type: TYPE_NORMAL
- en: Check out PySpark's documentation on the `.histogram(...)` at [https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.histogram](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.histogram).
  prefs: []
  type: TYPE_NORMAL
- en: 'The method returns a tuple of two elements: the first element is a list of
    bin bounds, and the other element is the counts of elements in the corresponding
    bins. Here''s what this looks like for our fuel economy feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that we specified that we want the `.histogram(...)` method to bucketize
    our data into five bins, but there are six elements in the first list. However,
    we still have five buckets in our dataset: *[8.97, 12.38), [ 12.38, 15.78), [15.78,
    19.19), [19.19, 22.59)*, and *[22.59, 26.0)*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We cannot create any plots in PySpark natively without going through a lot
    of setting up (see, for example, this: [https://plot.ly/python/apache-spark/](https://plot.ly/python/apache-spark/)).
    The easier way is to prepare a DataFrame with our data and use some *magic* (well,
    sparkmagics, but it still counts!) locally on the driver.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to extract our data and create a temporary `histogram_MPG` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a two-column DataFrame where the first column contains the bin lower
    bound and the second column contains the corresponding count. The `.registerTempTable(...)`
    method (as the name suggests) registers a temporary table so we can actually use
    it with the `%%sql` magic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command selects all the records from our temporary `histogram_MPG`
    table and outputs it to the locally-accessible `hist_MPG` variable; the `-q` switch
    is there so nothing gets printed out to the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'With `hist_MPG` locally accessible, we can now use it to produce our plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`%%local` executes whatever is located in that notebook cell in local mode.
    First, we import the `matplotlib` library and specify that it produces the plots
    inline within the notebook instead of popping up a new window each time a plot
    is produced. `plt.style.use(...)` changes the styles of our charts.'
  prefs: []
  type: TYPE_NORMAL
- en: For a full list of available styles, check out [https://matplotlib.org/devdocs/gallery/style_sheets/style_sheets_reference.html](https://matplotlib.org/devdocs/gallery/style_sheets/style_sheets_reference.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a figure and add a subplot to it that we will be drawing in.
    Finally, we use the `.bar(...)` method to plot our histogram and set the title.
    Here''s what the chart looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00103.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: That's it!
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Matplotlib is not the only library we can use to plot histograms. Bokeh (available
    at [https://bokeh.pydata.org/en/latest/](https://bokeh.pydata.org/en/latest/))
    is another powerful plotting library, built on top of `D3.js`, which allows you
    to interactively *play* with your charts.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the gallery of examples at [https://bokeh.pydata.org/en/latest/docs/gallery.html](https://bokeh.pydata.org/en/latest/docs/gallery.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how you plot with Bokeh:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we load all the necessary components of Bokeh; the `output_notebook()`
    method makes sure that we produce the chart inline in the notebook instead of
    opening a new window each time. Next, we produce the labels to put on our chart.
    Then, we define our figure: the `x_range` parameter specifies the number of points
    on the *x* axis and the `plot_height` sets the height of our plot. Finally, we
    use the `.vbar(...)` method to draw the bars of our histogram; the `x` parameter
    is the labels to put on our plot, and the `top` parameter specifies the counts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00104.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It's the same information, but you can interact with this chart in your browser.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you want to further customize your histograms, here is a page that might
    be useful: [https://plot.ly/matplotlib/histograms/](https://plot.ly/matplotlib/histograms/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing interactions between features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Plotting the interactions between features can further your understanding of
    not only the distribution of your data, but also how the features relate to each
    other. In this recipe, we will show you how to create scatter plots from your
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. Also,
    we will be working off of the `no_outliers` DataFrame we created in the *Handling
    outliers* recipe, so we assume you have followed the steps to handle duplicates,
    missing observations, and outliers.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once again, we will select our data from the DataFrame and expose it locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we select the two features we want to learn more about to see how they
    interact with each other; in our case they are the displacement and cylinders
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Our example here is small so we can work with all our data. However, in the
    real world, you should sample your data first before attempting to plot billions
    of data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'After registering the temp table, we use the `%%sql` magic to select all the
    data from the `scatter` table and expose it locally as a `scatter_source`. Now,
    we can start plotting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: First, we load the Matplotlib library and set it up.
  prefs: []
  type: TYPE_NORMAL
- en: See the *Drawing histograms* recipe for a more detailed explanation of what
    these Matplotlib commands do.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a figure and add a subplot to it. Then, we draw a scatter plot
    using our data; the *x* axis will represent the number of cylinders and the *y*
    axis will represent the displacement. Finally, we set the axes labels and the
    chart title.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what the final result looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00105.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can create an interactive version of the preceding chart using `bokeh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: First, we create the canvas, the figure we will be plotting on. Next, we set
    our labels. Finally, we use the `.circle(...)` method to plot the dots on the
    canvas.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final result looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00106.jpeg)'
  prefs: []
  type: TYPE_IMG
