- en: Scalable Multi-GPU Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have concentrated on getting optimal performance on a single GPU.
    Dense nodes with multiple GPUs have become a pressing need for upcoming supercomputers,
    especially since the ExaFLOP (a quintillion operations per sec) system is becoming
    a reality. GPU architecture is energy-efficient and hence, in recent years, systems
    with GPUs have taken the majority of the top spots in the Green500 list ([https://www.top500.org/green500](https://www.top500.org/green500)). 
    In Green500's November 2018 list, seven out of the top 10 systems were based on
    the NVIDIA GPU.
  prefs: []
  type: TYPE_NORMAL
- en: The DGX system from NVIDIA now has 16 V100 32 GB in one server. With the help
    of unified memory and interconnect technologies such as NVLink and NvSwitch, developers
    can see all GPUs as one big GPU with 512 GB memory (16 GPU * 32 GB each). In this
    chapter, we will go into the details of writing CUDA code and make use of CUDA-aware
    libraries to efficiently get scalability in a multi-GPU environment within and
    across nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Solving a linear equation using Gaussian elimination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPUDirect peer to peer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief introduction to MPI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPUDirect RDMA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional tricks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Linux PC with a modern NVIDIA GPU (Pascal architecture onward) is required
    for this chapter, with all necessary GPU drivers and the CUDA Toolkit (10.0 onward)
    installed. If you are unsure of your GPU's architecture, please visit NVIDIA GPU's
    site ([https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus))
    and confirm your GPU's architecture. This chapter's code is also available on
    GitHub at [https://github.com/PacktPublishing/Learn-CUDA-Programming](https://github.com/PacktPublishing/Learn-CUDA-Programming).
  prefs: []
  type: TYPE_NORMAL
- en: The sample code examples in this chapter have been developed and tested with
    CUDA version 10.1\. However, it is recommended you use the latest version (CUDA)
    or a higher one.
  prefs: []
  type: TYPE_NORMAL
- en: Since this chapter needs to showcase multi-GPU interactions, we will need a
    minimum of two GPUs of the same type and architecture. Also, note that some features,
    such as GPUDirect RDMA and NVLink, are only supported on Tesla cards of NVIDIA.
    If you don't have a Tesla card such as the Tesla P100 or Tesla V100, don't be
    disheartened. You can safely ignore some of these features. There will be a change
    in performance numbers compared to what we show here, but the same code will work
    as-is.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at an example of the popular Gaussian algorithm
    to solve a series of linear equations to demonstrate how to write multi-GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Solving a linear equation using Gaussian elimination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate the usage of multiple GPUs within and across nodes, we will
    start with some sequential code and then convert it into multiple GPUs within
    and across nodes. We will be solving a linear system of equations containing *M*
    equations and *N* unknowns. The equation can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '***A × x = b***'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *A* is a matrix with *M* rows and *N* columns, *x* is a column vector
    (also referred to as a solution vector) with *N* rows, and *b* is also a column
    vector with *M* rows. Finding a solution vector involves computing vector *x*
    when *A* and *b* are given. One of the standard methods for solving a linear system
    of equations is Gaussian elimination. In Gaussian elimination, first matrix *A*
    is reduced to either the upper or lower triangular matrix by performing elementary
    row transformations. Then, the resulting triangular system of equations is solved
    by using the back substitution step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following pseudocode explains the steps that are involved in solving the
    linear equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at an example in order to understand the algorithm. Let''s
    say the system of equations is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ce99d49-d0fa-45b2-9185-fd05c2242e1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we will try to set the baseline system, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare your GPU application. This code can be found in the `06_multigpu/gaussian` folder
    in this book's GitHub repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compile your application with the `nvcc` compiler, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding steps compile and run two versions of the code that are present
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The CPU code, which runs sequentially
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CUDA code, which runs on a single GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's take a look at the hotspots in the single GPU implementation of Gaussian
    elimination.
  prefs: []
  type: TYPE_NORMAL
- en: Single GPU hotspot analysis of Gaussian elimination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's try to understand and profile sequential and single GPU code to set a
    baseline. Over this baseline, we will enhance and add support for running on multi-GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequential CPU code**: The following code shows the extracted code of sequential
    implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Visually, the operation that takes place is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b938a13f-62d4-416d-a911-ad1be7c4db3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, in this Gaussian elimination, the number of rows is equal to the number
    of equations and the number of columns is equal to the number of unknowns. The
    **pr** row shown in the preceding diagram is the pivot row and will be used to
    reduce other rows using the pivot element.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first observation that we can make is that we are operating on an augmented
    matrix to merge the *A* matrix with the *b* vector. Hence, the size of unknowns
    is *N+1* as an augmented matrix has the last column as the *b* vector. Creating
    an augmented matrix helps us work on just one data structure, that is, a matrix.
    You can profile this code using the following command. The profiling results will
    show you that the `guassian_elimination_cpu()` function takes the most time to
    complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**CUDA single GPU code**: After going through the previous chapters, we expect
    you to have familiarized yourself with how to write optimal GPU code and hence
    we will not go into the details of the single GPU implementation. The following
    extract shows that, in a single GPU implementation, the three steps are known
    as three kernels for finding *N* unknowns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`findPivotRowAndMultipliers<<<...>>>`: The kernel finds the pivot row and multiplier,
    which should be used for row elimination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`extractPivotRow<<<>>>`: The kernel extracts the pivot row, which is then used
    to perform row elimination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rowElimination<<<>>>`: This is the final kernel call, and does the row elimination
    in parallel on the GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code snippet shows the three kernels called iteratively after
    the data has been copied to the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The focus of this chapter is on how this single GPU implementation can be enhanced
    to support multiple GPUs. However, to fill in the missing pieces from the GPU
    implementation, we need to make some optimization changes to the single GPU implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The performance of the Gaussian elimination algorithm is heavily influenced
    by the memory access pattern. Basically, it depends on how the AB matrix is stored:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the pivot row prefers the column-major format as it provides coalesced
    access if the matrix is stored in a column major format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, extracting a pivot row prefers the row-major format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No matter how we store the *AB* matrix, one coalesced and one strided/non-coalesced
    access to memory is unavoidable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The column major format is also beneficial for row elimination kernels and hence,
    for our Gaussian elimination kernel, we decided to store the transpose of the
    AB matrix instead of AB. The AB matrix gets transposed once, at the beginning
    of the code in the `transposeMatrixAB()` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will enable multi-GPU P2P access and split the work
    among multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: GPUDirect peer to peer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The GPUDirect technology was created to allow high-bandwidth, low-latency communication
    between GPUs within and across different nodes. This technology was introduced
    to eliminate CPU overheads when one GPU needs to communicate with another. GPUDirect
    can be classified into the following major categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Peer to peer (P2P) transfer between GPU**: AllowsCUDA programs to use high-speed
    **Direct Memory Transfer** (**DMA**) to copy data between two GPUs in the same
    system. It also allows optimized access to the memory of other GPUs within the
    same system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accelerated communication between network and storage**: This technology
    helps with direct access to CUDA memory from third-party devices such as InfiniBand
    network adapters or storage. It eliminates unnecessary memory copies and CPU overhead
    and hence reduces the latency of transfer and access. This feature is supported
    from CUDA 3.1 onward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPUDirect for video**: This technology optimizes pipelines for frame-based
    video devices. It allows low-latency communication with OpenGL, DirectX, or CUDA
    and is supported from CUDA 4.2 onward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Remote Direct Memory Access (RDMA)**: This feature allows direct communication
    between GPUs across a cluster. This feature is supported from CUDA 5.0 and later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will be converting our sequential code to make use of the
    P2P feature of GPUDirect so that it can run on multiple GPUs within the same system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GPUDirect P2P feature allows the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPUDirect transfers**: `cudaMemcpy()` initiates a DMA copy from GPU 1''s
    memory to GPU 2''s memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Direct access**: GPU 1 can read or write GPU 2''s memory (load/store).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates these features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/937e7aae-d492-4615-a17f-8e2a3e2f48bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To understand the advantage of P2P, it is necessary to understand the PCIe
    bus specification. This was created with the intention of optimally communicating
    through interconnects such as InfiniBand to other nodes. This is different when
    we want to optimally send and receive data from individual GPUs. The following
    is a sample PCIe topology where eight GPUs are being connected to various CPUs
    and NIC/InfiniBand cards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33fbb94b-f6e0-4951-8285-c0e03c6c8bf4.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, P2P transfer is allowed between GPU0 and GPU1 as they
    are both situated in the same PCIe switch. However, GPU0 and GPU4 cannot perform
    a P2P transfer as PCIe P2P communication is not supported between two **I/O Hubs** (**IOHs**). The
    IOH does not support non-contiguous bytes from PCI Express for remote peer-to-peer
    MMIO transactions. The nature of the QPI link connecting the two CPUs ensures
    that a direct P2P copy between GPU memory is not possible if the GPUs reside on
    different PCIe domains. Thus, a copy from the memory of GPU0 to the memory of
    GPU4 requires copying over the PCIe link to the memory attached to CPU0, then
    transferring it over the QPI link to CPU1 and over the PCIe again to GPU4\. As
    you can imagine, this process adds a significant amount of overhead in terms of
    both latency and bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows another system where GPUs are connected to each
    other via an NVLink interconnect that supports P2P transfers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88e4972a-9fab-4b7c-9643-8b4b16c550fb.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows a sample NVLink topology resulting in an eight-cube
    mesh where each GPU is connected to another GPU with a max 1 hop.
  prefs: []
  type: TYPE_NORMAL
- en: 'The more important query would be, *How can we figure out this topology and
    which GPUs support P2P transfer?* Fortunately, there are tools for this. `nvidia-smi`
    is one such tool that gets installed as part of the NVIDIA driver''s installation.
    The following screenshot shows the output of running `nvidia-smi` on the NVIDIA
    DGX server whose network topology is shown in the preceding diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/806e629d-8aa4-4dc4-a453-5f803bb512e5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot represents the result of the `nvidia-smi topo -m` command
    being run on the DGX system, which has 8 GPUs. As you can see, any GPU that is
    connected to another GPU via SMP interconnect (`QPI`/`UPI`) cannot perform P2P
    transfer. For example, `GPU0` will not be able to do P2P with `GPU5`, `GPU6`,
    and `GPU7`. Another way is to figure out this transfer via CUDA APIs, which we
    will be using to convert our code in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood the system topology, we can start converting our
    application into multiple GPUs in a single node/server.
  prefs: []
  type: TYPE_NORMAL
- en: Single node – multi-GPU Gaussian elimination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Prepare your multi-GPU application. This code can be found at `06_multigpu/gaussian` in
    this book''s GitHub repository. Compile your application with the `nvcc` compiler,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Going from a single-to multi-GPU implementation, the three kernels we defined
    in the previous subsection will be used as-is. However, the linear system is split
    into a number of parts equal to the number of GPUs. These parts are distributed
    one part per GPU. Each GPU is responsible for performing the operation on the
    part that''s been assigned to that GPU. The matrix is split column-wise. This
    means each GPU gets an equal number of consecutive columns from all the rows.
    The kernel for finding the pivot is launched on the GPU that holds the column
    containing the pivot element. The row index of the pivot element is broadcasted
    to other GPUs. The extracted pivot row and row elimination kernels are launched
    on all the GPUs, with each GPU working on its own part of the matrix. The following
    diagram shows the rows being split among multiple GPUs and how the pivot row needs
    to be broadcasted to the rest of the processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97345168-60cd-4663-9836-2f391027e26d.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram represents the division of work across multiple GPUs.
    Currently, the pivot row belongs to **GPU1** and is responsible for broadcasting
    the pivot row to other GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to understand these code changes, as well as the CUDA API that was
    used to enable the P2P feature:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable P2P access between the supported GPUs. The following code shows the
    first step in this:s enabling P2P access between GPUs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The key APIs that were used in the preceding code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cudaDeviceCanAccessPeer()`: Checks if the current GPU can do P2P access to
    the GPU whose ID is sent as a parameter'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cudaDeviceEnablePeerAccess()`: If `cudaDeviceCanAccessPeer()` returns `True`,
    enable P2P access'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Split and transfer the content to the respective GPUs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The key API that was used in the preceding code is `cudaSetDevice()`. This sets
    the current context to the GPU ID that was passed as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the pivot row and broadcast it via P2P:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The API that's used to broadcast the transfer to GPUs is `cudaMemcpyPeer()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the pivot row and perform row elimination:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we are still reusing the same kernels. The only difference is
    that we use the `cudaSetDevice()` API to tell the CUDA runtime which GPU the kernel
    should be launched on. Note that `cudaSetDevice()` is a costly call, especially
    on older generation GPUs. Therefore, it is advised that you call the for loop
    for `nGPUs` in parallel on the CPU by making use of `OpenMP`/`OpenACC` or any
    other threading mechanism on the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the data back from the respective CPU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: These five steps complete the exercise of converting a single GPU implementation
    into a multiple GPU on a single node.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA samples that get shipped as part of CUDA's installation include some sample
    code that tests P2P bandwidth performance. It can be found in the `samples/1_Utilities/p2pBandwidthLatencyTest`
    folder. It is advised that you run this application on your system so that you
    understand the P2P bandwidth and latency of your system.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've achieved multi-GPU implementation on a single node, we'll change
    gear and run this code on multiple GPUs. But before converting our code into multiple
    GPUs, we will provide a short primer on MPI programming, which is primarily used
    for internode communication.
  prefs: []
  type: TYPE_NORMAL
- en: Brief introduction to MPI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Message Passing Interface** (**MPI**) standard is a message-passing library
    standard and has become the industry standard for writing message-passing programs
    on HPC platforms. Basically, MPI is used for message passing across multiple MPI
    processes. MPI processes that communicate with each other may reside on the same
    node or across multiple nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example Hello World MPI program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the general steps that are involved in the MPI program are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We include the header file, `mpi.h`, which includes the declaration of all MPI
    API calls.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We initialize the MPI environment by calling `MPI_Init` and passing the executable
    arguments to it. After this statement, multiple MPI ranks are created and start
    executing in parallel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All MPI processes work in parallel and communicate with each other using message-passing
    APIs such as `MPI_Send()`, `MPI_Recv()`, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we terminate the MPI environment by calling `MPI_Finalize()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can compile this code using different MPI implementation libraries such
    as OpenMPI, MVPICH, Intel MPI, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We are making use of the `mpicc` compiler to compile our code. `mpicc` is basically
    is a wrapper script that internally expands the compilation instructions to include
    the paths to the relevant libraries and header files. Also, running an MPI executable
    requires it to be passed as an argument to `mpirun`. `mpirun` is a wrapper that
    helps set up the environment across multiple nodes where the application is supposed
    to be executed. The `-n 4` argument says that we want to run four processes and
    that these processes will run on nodes with the hostname stored in the file hosts
    list.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, our goal is to integrate GPU kernels with MPI to make it run
    across multiple MPI processes. However, we will not be covering the details of
    MPI programming. Those of you who are not familiar with MPI programming should
    take a look at [https://computing.llnl.gov/tutorials/mpi/](https://computing.llnl.gov/tutorials/mpi/)
    to understand distributed parallel programming before moving on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: GPUDirect RDMA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a cluster environment, we would like to make use of GPUs across multiple
    nodes. We will allow our parallel solver to integrate CUDA code with MPI to utilize
    multi-level parallelism on multi-node, multi-GPU systems. A CUDA-aware MPI is
    used to leverage GPUDirect RDMA for optimized inter-node communication.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPUDirect RDMA allows direct communication between GPUs across a cluster. It
    was first supported by CUDA 5.0 with the Kepler GPU card. In the following diagram,
    we can see the GPUDirect RDMA, that is, **GPU 2** in **Server 1** communicating
    directly with **GPU 1** in **Server 2**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38bb750c-b2e8-44c6-9293-69582300c70e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The only theoretical requirement for GPUDirect RDMA to work is that the **Network
    Card** and **GPU** share the same root complex. The path between the GPU and a
    third-party device such as a network adapter decides whether RDMA is supported
    or not. Let''s revisit the output of the `nvidia-smi topo -m` command on the DGX
    system that we ran in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5cca0b09-e6e3-48c2-8d73-d4603bfc5ba7.png)'
  prefs: []
  type: TYPE_IMG
- en: If we look at the `GPU4` row, it shows that the `GPU4` to `mlx5_2` connection
    type is `PIX` (traversal via PCIe switch). We can also see that the `GPU4` to
    `mlx_5_0` connection type is `SYS` (traversal via `QPI`). This means that `GPU4`
    can perform RDMA transfers via Mellanox InfiniBand Adapter `mlx_5_2` but not if
    the transfer needs to happen from `mlx_5_0` as `QPI` does not allow RDMA protocols.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA-aware MPI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the latest versions of the MPI libraries support the GPUDirect feature. MPI
    libraries that support for NVIDIA GPUDirect and **Unified Virtual Addressing**
    (**UVA**) enable the following:'
  prefs: []
  type: TYPE_NORMAL
- en: MPI can transfer the API to copy data directly to/from GPU memory (RDMA).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MPI library can also differentiate between device memory and host memory
    without any hints from the user and hence it becomes transparent to the MPI programmer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The programmer's productivity increases as less application code needs to be
    changed for data transfers across multiple MPI ranks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, CPU memory and GPU memory are different. Without a
    CUDA-aware MPI, the developer can only pass pointers pointing to CPU/host memory
    to MPI calls. The following code is an example of using non-CUDA-aware MPI calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'With a CUDA-aware MPI library, this is not necessary; the GPU buffers can be
    directly passed to MPI, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, for Open MPI, CUDA-aware support exists in the Open MPI 1.7 series
    and later. To enable this feature, the Open MPI library needs to be configured
    with CUDA support at the time of compilation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Having a CUDA-aware MPI does not mean that the GPUDirect RDMA is always used.
    The GPUDirect feature is used if the data transfer happens between the network
    card and the GPU that share the same root complex. Nonetheless, even if RDMA support
    is not enabled, having a CUDA-aware MPI makes the application more efficient by
    making use of features such as message transfers, which can be pipelined as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0de7272-eae9-4914-aee2-400f73f125bf.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows the CUDA-aware MPI with GPUDirect versus the CUDA-aware
    MPI without GPUDirect. Both calls are from the CUDA-aware MPI, but the left-hand
    side is with GPUDirect transfer and the right-hand side is without GPUDirect transfer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-GPUDirect transfer has the following stages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Node 1: Transfer from GPU1 to host memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Node 1: Transfer from host memory to the network adaptor staging area'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Network: Transfer over the network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Node 2: Transfer from the network staging area to host memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Node 2: Transfer from host memory to GPU memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If GPUDirect RDMA is supported, the transfer from the GPU happens directly over
    the network and the additional copies involving host memory are all removed.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have grasped this concept, let's start converting the code to enable
    Multi-GPU support using CUDA-aware MPI programming.
  prefs: []
  type: TYPE_NORMAL
- en: Multinode – multi-GPU Gaussian elimination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Prepare your GPU application. This code can be found at `06_multigpu/gaussian` in
    this book''s GitHub repository. Compile and run your application with the `nvcc`
    compiler, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We are using `mpicc` instead of `nvcc` to compile the MPI program. We run the
    executable using the `mpirun` command instead of running the compiled executable
    directly. The results that you will see in this section are the output of running
    on the DGX system with 8 V100 on the same system. We make use of the 8 max MPI
    process as we map 1 MPI process per GPU. To understand how to map multiple MPI
    processes onto the same GPU, please read the *MPS* subsection later in the chapter. For
    this exercise, we have used Open MPI 1.10, which has been compiled to support
    CUDA as described in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps that are involved in the multi-GPU implementation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Rank 0 of the MPI process generates data for the linear system (matrices A,
    B).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The transposed augmented matrix (AB^T) is split row-wise by the root among the
    MPI processes using `MPI_Scatterv()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Each MPI process computes on its part of the input in parallel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Processing the three kernels happens on the GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The consensus of the pivot is achieved after the `findPivot` operation using
    `MPI_Send()`/`Recv()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reduced **transposed augmented matrix** (**ABT**) is gathered on the root
    using `MPI_Gatherv()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The root performs back substitution to compute solution X.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The extracted sample Gaussian code which showcases the preceding code is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s add multi-GPU support:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Set the CUDA device per MPI rank**:In Open MPI, you can get the local rank
    of the MPI process by making use of `MPI_COMM_TYPE_SHARED` as a parameter to `MPI_Comm_split_type`, as
    shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the local rank, each MPI process uses it to set the current
    GPU by using `cudaSetDevice()`, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/316a4f63-8ca7-4540-a2c2-47b2dcc0e122.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Split and distribute the inputs to different MPI processes using `MPI_Scatter`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform Gaussian elimination on the GPU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Before performing any operation, the current GPU is set based on local rank.
    Then, the pivot row is extracted by the process which is responsible for that
    row, followed by the pivot row being broadcasted to all the other MPI ranks, which
    we use for elimination.
  prefs: []
  type: TYPE_NORMAL
- en: The overall performance of the transfer time can be improved by making use of
    asynchronous MPI calls instead of using broadcast APIs such as `MPI_Bcast`. In
    fact, the use of a broadcast API is discouraged; it should be replaced with `MPI_Isend`
    and `MPI_Irecv`, which are asynchronous versions that can achieve the same functionality.
    Please note that making the calls asynchronous adds complexity to other aspects
    such as debugging. Due to this, the user needs to write additional code to send
    and receive data.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provides the best coding practices when it comes to adding GPU
    support to an existing MPI program and should not be considered an expert guide
    on the best programming practices for MPI programming.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streams act in a FIFO manner, where the sequence of operations is executed in
    the order of when they were issued. Requests that are made from the host code
    are put into First-In-First-Out queues. Queues are read and processed asynchronously
    by the driver, and the device driver ensures that the commands in a queue are
    processed in sequence. For example, memory copies end before kernel launch, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: The general idea of using multiple streams is that CUDA operations that are
    fired in different streams may run concurrently. This can result in multiple kernels
    overlapping or overlapping memory copies within the kernel execution.
  prefs: []
  type: TYPE_NORMAL
- en: To understand CUDA streams, we will be looking at two applications. The first
    application is a simple vector addition code with added streams so that it can
    overlap data transfers with kernel execution. The second application is of an
    image merging application, which will also be used in [Chapter 9](9335adec-9dd0-4f6f-8eea-9ce4ca8912e5.xhtml),
    *GPU Programming Using OpenACC*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, configure your environment according to the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare your GPU application. As an example, we will be merging two images.
    This code can be found in the `06_multi-gpu/streams` folder in this book's GitHub
    repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compile your application with the `nvcc` compiler as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding commands will create two binaries named `vector_addition` and
    `merging_multi_gpu`. As you might have observed, we are using additional arguments
    in our code. Let''s understand them in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--default-stream per-thread`: This flag tells the compiler to parse the OpenACC
    directives provided in the code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-Xcompiler -fopenmp -lgomp`: This flag tells `nvcc` to pass these additional
    flags to the CPU compiler underneath to compile the CPU part of the code. In this
    case, we are asking the compiler to add OpenMP-related libraries to our application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will divide this section into two parts. Application 1 and application 2
    demonstrate using streams in single and multiple GPUs, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Application 1 – using multiple streams to overlap data transfers with kernel
    execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps that we need to follow to overlap data transfers with kernel execution
    or to launch multiple kernels concurrently are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Declare the host memory to be pinned, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are making use of the `cudaMallocHost()` API to allocate vectors as
    pinned memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `Stream` object, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here, we make use of the `cudaStreamCreateWithFlags()` API, passing `cudaStreamNonBlocking`
    as the flag to make this stream non-blocking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Call the CUDA kernel and memory copies with the `stream` flag, as shown in
    the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, instead of performing the vector addition in one shot by copying
    the whole array once, instead we chunk the array into segments and copy the segments
    asynchronously. Kernel execution is also done asynchronously in the respective
    streams.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run this code through Visual Profiler, we can see the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccb79f18-189a-4f8c-84dc-6af4ada0e01f.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding profiler screenshot shows that the blue bars (which are basically
    `vector_addition` kernels) overlap the memory copies. Since we created four streams
    in our code, there are four streams in the profiler as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every GPU has two memory copy engines. One is responsible for the host to device
    transfer while the other is responsible for the device to host transfer. Hence,
    the two memory copies, which happen in opposite directions, can be overlapped.
    Also, the memory copies can be overlapped with the compute kernels. This can result
    in *n*-way concurrency, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4eedad6-44a2-4498-8647-246244ec6999.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Every GPU architecture comes with certain constraints and rules based on which
    we will see these overlaps at execution time. In general, the following are some
    guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA operations must be in different, non-0 streams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cudaMemcpyAsync` with the host should be pinned using `cudaMallocHost()` or
    `cudaHostAlloc()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sufficient resources must be available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cudaMemcpyAsyncs` in different directions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Device resources (SMEM, registers, blocks, and so on) to launch multiple concurrent
    kernels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application 2 – using multiple streams to run kernels on multiple devices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run kernels and overlap memory transfers across multiple devices, the steps
    that we followed previously remain the same, except for one additional step: setting
    the CUDA device to create the stream. Let''s have a look at the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create streams equal to the number of CUDA devices present in the system, as
    shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We make use of the `cudaGetDeviceCount()` API to get the number of CUDA devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the stream in the respective device, as shown in the following code
    snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We are launching OpenMP threads equal to the number of CUDA devices so that
    each CPU thread can create its own CUDA stream for its respective devices. Each
    CPU thread executes `cudaSetDevice()` to set the current GPU based on its ID and
    then creates the stream for that device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch the kernel and memory copies in that stream, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output after running the code in the profiler can be seen in the following
    screenshot, which represents the Visual Profiler''s timeline view. This shows
    an overlapping memory copy for one GPU with the kernel execution of the other
    GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfdd5f05-a117-4032-8736-123f1f2e9cc3.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we ran this code on the multi-GPU system with four V100s. The
    memory copies and kernels in the different GPUs overlap each other. In this code,
    we demonstrated making use of OpenMP to call CUDA kernels in parallel on different
    devices. This can also be done by making use of MPI to launch multiple processes
    that utilize different GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will take a look at some additional topics that can
    improve the performance of multi-GPU applications and help developers profile
    and debug their code.
  prefs: []
  type: TYPE_NORMAL
- en: Additional tricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover some additional topics that will help us understand
    the additional characteristics of the multi-GPU system.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking an existing system with an InfiniBand network card
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Different benchmarks are available for testing the RDMA feature. One such benchmark
    for the InfiniBand adapter can be found at [https://www.openfabrics.org/](https://www.openfabrics.org/).
    You can test your bandwidth by executing the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can run the following commands to test the bandwidth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: NVIDIA Collective Communication Library (NCCL)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The NCCL provides an implementation of communication primitives that are commonly
    used in domains such as deep learning. NCCL 1.0 started with the implementation
    of communication primitives across multiple GPUs within the same node and evolved
    to support multiple GPUs in multiple nodes. Some key features of the NCCL library
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Supports calls from multiple threads and multiple processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports the multiple ring and tree topology for better bus utilization within
    and across nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports InfiniBand inter-node communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source package is free to download from GitHub ([https://github.com/nvidia/nccl](https://github.com/nvidia/nccl))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NCCL can be scaled up to 24,000 GPUs, well below the 300 microsecond latency.
    Note that NCCL has proven to be a really useful and handy library for deep learning
    frameworks but has its limitation when used for HPC applications as it does not
    support point-to-point communication. NCCL supports collective operations, which
    are used in deep learning applications such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AllReduce`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AllGather`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReduceScatter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Reduce`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Broadcast`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All NCCL calls run as CUDA kernels for faster access to GPU memory. It makes
    use of fewer threads that are implemented as one block. This ends up running only
    on one GPU SM and hence does not affect the utilization of other GPUs. Let''s
    have a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, NCCL calls are simple and can be called with ease.
  prefs: []
  type: TYPE_NORMAL
- en: Collective communication acceleration using NCCL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **NVIDIA Collective Communication Library **(**NCCL**) provides a performance-optimized
    collective of communication primitives for multiple NVIDIA GPUs. In this section,
    we will see how this library works and how we can benefit from using it.
  prefs: []
  type: TYPE_NORMAL
- en: 'It isn''t difficult to find deep learning models that use multiple GPUs to
    train the network. Since two GPUs compute the neural network in parallel, we can
    easily imagine that this technique will increase training performance along with
    the GPU numbers. Unfortunately, the world is not that simple. The gradients should
    be shared across multiple GPUs and the weight update procedure in one GPU should
    wait for the others'' gradients to update its weights. This is the general procedure
    of deep learning training with multiple GPUs, and is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/914037ea-4291-4208-a867-6281cff0b742.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Collective communication has many types: all-reduce, broadcast, reduce, all
    gather, reduce scatter, and so on. In deep learning, each GPU collects another
    GPU''s data while it transfers its own to the other GPUs. Therefore, we can determine
    that deep learning needs all types of reducing style communication in their communication.'
  prefs: []
  type: TYPE_NORMAL
- en: In the HPC community, collective communication, including all-reduce, is quite
    a common topic. Communication between the processor from inter- and intra-nodes
    was a challenging but crucial issue because it's directly related to scalability.
    As we mentioned in [Chapter 6](ba3092b0-9a57-4137-8ec9-229253c98552.xhtml), *Scalable
    Multi-GPU Programming*, in the *Multiple GPU programming* section, it requires
    a lot of consideration to communicate with each GPU. The developer should design
    and implement collective communication in GPUs, even though MPI already supports
    such communication patterns.
  prefs: []
  type: TYPE_NORMAL
- en: NCCL provides such a collective that's aware of the GPU topology configuration.
    By using a variety of grouping and communication commands, you can apply the required
    communication task.
  prefs: []
  type: TYPE_NORMAL
- en: One prerequisite is that your system needs to have more than one GPU because
    NCCL is a communication library that works with multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps cover how to call `ncclAllReduce()` as a test and measure
    the system''s GPU network bandwidth. The sample code is implemented in `04_nccl`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define a type that will contain, send, and receive, a buffer and `cudaStream` for
    each GPU device, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'At the beginning of the application, we need to prepare some handles so that
    we can control multiple GPUs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will create a buffer, assuming that we have data. For each device,
    we will initialize each device''s items, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Before starting the NCCL communication, we need to initialize the GPU devices
    so that they are aware of their rank across the GPU group. Since we will be testing
    the bandwidth with a single process, we are safe to call a function that initializes
    all the devices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: If we are testing the bandwidth with multiple processes, we need to call `ncclCommInitRank()`.
    We will need to provide GPU IDs for counting the process IDs and GPU ranks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we can complete the all-reduce operations with NCCL. The following code
    is an example implementation of `ncclAllReduce`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: For each device, we need to trigger the traffic. For this, we need to start
    and close NCCL group communication. Now, we have implemented some test code that
    uses `ncclAllReduce()`. Let's cover how NCCL works by micro-benchmarking our system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s test this code on a multi-GPU system, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the performance that was measured using four V100
    32G GPUs in DGX Station. The blue line denotes the NVLink-based bandwidth, while
    the orange line denotes PCIe-based bandwidth, which it does by setting `NCCL_P2P_DISABLE=1
    ./ncd` and turning off peer-to-peer GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7576c92f-ae93-41d2-bd31-8943edd22b8c.png)'
  prefs: []
  type: TYPE_IMG
- en: This NCCL test can be impacted by the system's configuration. This means that
    the result can vary, depending on your system's GPU topology.
  prefs: []
  type: TYPE_NORMAL
- en: 'This shows the difference between PCI express-based and NVLINK-based all-reduce performance.
    We can see its communication using `nvprof`. The following screenshot shows NCCL''s
    all-reduce communication in DGX Station via NCCL 2.3.7:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8acf126f-830f-4d1a-bb3b-58ff6ab62223.png)'
  prefs: []
  type: TYPE_IMG
- en: NCCL is getting faster and faster. By introducing new GPU interconnects with
    NVLink and NVSwitch, our experience with NCCL is increasing, so much so that we
    can achieve scalable performance.
  prefs: []
  type: TYPE_NORMAL
- en: The following link provides a discussion about NCCL: [https://developer.nvidia.com/gtc/2019/video/S9656/video](https://developer.nvidia.com/gtc/2019/video/S9656/video).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered different approaches to multi-GPU programming. With
    the help of an example Gaussian elimination, we saw how a single GPU application
    workload can be split across multiple GPUs, first into a single node and then
    into multiple nodes. We saw how system topology plays an important role in making
    use of features such as P2P transfer and GPUDirect RDMA. We also saw how multiple
    CUDA streams can be used to overlap communication and data transfer among multiple
    GPUs. We also briefly covered some additional topics that can help CUDA programmers
    optimize code such as MPS and the use of `nvprof` to profile multi-GPU applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at common patterns that appear in most HPC
    applications and how to implement them in GPUs.
  prefs: []
  type: TYPE_NORMAL
