- en: 3\. Python's Statistical Toolbox
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about the three main libraries in Python
    that help us facilitate various tasks in our statistics/machine learning projects.
    This chapter, in turn, initiates the formal topic of statistics and its relevant
    concepts. While it contains a number of theoretical discussion points, we will
    also employ intuitive examples and hands-on coding activities to help facilitate
    understanding. What we learn in this chapter will then prepare us for later statistics-related
    chapters in this workshop.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand the fundamental concepts in
    statistics and statistical methods. You'll also be able to carry out various statistics-related
    tasks using Python tools and libraries, and will have had an overview of a number
    of advanced statistics libraries in Python, such as statsmodels and PyMC3.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned how to use the Python language, especially three of
    its core libraries—NumPy, pandas, and Matplotlib, for statistics and data science.
    However, in order to fully take advantage of these tools, we will need to have
    a solid theoretical understanding of statistics itself. By knowing the idea behind
    statistical tests and techniques, we will be able to utilize the tools that Python
    offers more effectively.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: It is true that in statistics and machine learning, libraries in Python offer
    great options—from data cleaning/processing to modeling and making inferences.
    However, a fundamental understanding of statistics is still required so that we
    can make initial decisions regarding what kinds of techniques should be used in
    our process, depending on the data we have.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: As such, in this chapter, we will learn about core concepts in statistics such
    as , inference, sampling, variables, and so on. We will also be introduced to
    a wide range of Python tools that can help facilitate more advanced statistical
    techniques and needs. All of this will be demonstrated with hands-on discussions
    and examples.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: An Overview of Statistics
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will briefly discuss the goal of the overarching field of
    statistics and talk about some of its fundamental ideas. This conversation will
    set the context for the subsequent topics in this chapter and this book.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally speaking, statistics is all about working with data, be it processing,
    analyzing, or drawing a conclusion from the data we have. In the context of a
    given dataset, statistics has two main goals: describing the data, and drawing
    conclusions from it. These goals coincide with the two main categories of statistics
    — descriptive statistics and inferential statistics — respectively.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'In descriptiv**e statistics**, questions are asked about the general characteristics
    of a dataset: What is the average amount? What is the difference between the maximum
    and the minimum? What value appears the most? And so forth. The answers to these
    questions help us get an idea of what the dataset in question constitutes and
    what the subject of the dataset is. We saw brief examples of this in the previous
    chapter.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'In **inferential statistics**, the goal is to go a step further: after extracting
    appropriate insights from a given dataset, we''d like to use that information
    and infer on unknown data. One example of this is making predictions for the future
    from observed data. This is typically done via various statistical and machine
    learning models, each of which is only applicable to certain types of data. This
    is why it is highly important to understand what types of data there are in statistics,
    which are described in the next section.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Overall, statistics can be thought of as a field that studies data, which is
    why it is the foundation for data science and machine learning. Using statistics,
    we can understand the state of the world using our sometimes-limited datasets,
    and from there make appropriate and actionable decisions, made from the data-driven
    knowledge that we obtain. This is why statistics is used ubiquitously in various
    fields of study, from sciences to social sciences, and sometimes even the humanities,
    when there are analytical elements involved in the research.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'With that said, let''s begin our first technical topic of this chapter: distinguishing
    between data types.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Types of Data in Statistics
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In statistics, there are two main types of data: categorical data and numerical
    data. Depending on which type an attribute or a variable in your dataset belongs
    to, its data processing, modeling, analysis, and visualization techniques might
    differ. In this section, we will explain the details of these two main data types
    and discuss relevant points for each of them, which are summarized in the following
    table:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Data type comparison'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.1: Data type comparison'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: For the rest of this section, we will go into more detail about each of the
    preceding comparisons, starting with categorical data in the next subsection.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Categorical Data
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When an attribute or a variable is categorical, the possible values it can take
    belong to a predetermined and fixed set of values. For example, in a weather-related
    dataset, you might have an attribute to describe the overall weather for each
    day, in which case that attribute might be among a list of discrete values such
    as `"sunny"`, `"windy"`, `"cloudy"`, `"rain"`, and so on. A cell in this attribute
    column *must* take on one of these possible values; a cell cannot contain, for
    example, a number or an unrelated string like `"apple"`. Another term for this
    type of data is *nominal data*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of the nature of the data, in most cases, there is no ordinal relationship
    between the possible values of a categorical attribute. For example, there is
    no comparison operation that can be applied to the weather-related data we described
    previously: `"sunny"` is neither greater than or less than `"windy"`, and so on.
    This is to be contrasted with numerical data, which, although we haven''t discussed
    it yet, expresses clear ordinality.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: On the topic of differences between data types, let's now go through a number
    of points to keep in mind when working with categorical data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: If an unknown variable that is a categorical attribute is to be modeled using
    a probability distribution, a categorical distribution will be required. Such
    a distribution describes the probability that the variable is one out of *K* predefined
    possible categories. Luckily for us, most of the modeling will be done in the
    backend of various statistical/machine learning models when we call them from
    their respective libraries, so we don't have to worry about the problem of modeling
    right now.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: In terms of data processing, an encoding scheme is typically used to *convert*
    the categorical values in an attribute to numerical, machine-interpretable values.
    As such, string values, which are highly common in categorical data, cannot be
    fed to a number of models that only take in numerical data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, some tend to use the simple encoding of assigning each possible
    value with a positive integer and replacing them with their respective numerical
    value. Consider the following sample dataset (stored in the variable named `weather_df`):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output will be as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, you could potentially call the `map()` method on the `weather` attribute
    and pass in the dictionary `{''windy'': 0, ''cloudy'': 1, ''sunny'': 2, ''rain'':
    3}` (the `map()` method simply applies the mapping defined by the dictionary on
    the attribute) to encode the categorical attribute like so:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This DataFrame object will now hold the following data:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We see that the categorical column `weather` has been successfully converted
    to numerical data in `weather_encoded` via a one-to-one mapping. However, this
    technique can be potentially dangerous: the new attribute implicitly places an
    order on its data. Since *0 < 1 < 2 < 3*, we are inadvertently imposing the same
    ordering on the original categorical data; this is especially dangerous if the
    model we are using specifically interprets that as truly numerical data.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the reason why we must be careful when transforming our categorical
    attributes into a numerical form. We have actually already discussed a certain
    technique that is able to convert categorical data without imposing a numerical
    relationship in the previous chapter: one-hot encoding. In this technique, we
    create a new attribute for every unique value in a categorical attribute. Then,
    for each row in the dataset, we place a `1` in a newly created attribute if that
    row has the corresponding value in the original categorical attribute and `0`
    in the other new attributes.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet reiterates how we can implement one-hot encoding
    with pandas and what effect it will have on our current sample weather dataset:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will produce the following output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Among the various descriptive statistics that we will discuss later in this
    chapter, the mode — the value that appears the most — is typically the only statistic
    that can be used on categorical data. As a consequence of this, when there are
    values missing from a categorical attribute in our dataset and we'd like to fill
    them with a central tendency statistic, a concept we will define later on in this
    chapter, the mode is the only one that should be considered.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: In terms of making predictions, if a categorical attribute is the target of
    our machine learning pipeline (as in, if we want to predict a categorical attribute),
    classification models are needed. As opposed to regression models, which make
    predictions on numerical, continuous data, classification models, or classifiers
    for short, keep in mind the possible values their target attribute can take and
    only predict among those values. Thus, when deciding which machine learning model(s)
    you should train on your dataset to predict categorical data, make sure to only
    use classifiers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: The last big difference between categorical data and numerical data is in visualization
    techniques. A number of visualization techniques were discussed in the previous
    chapter that are applicable for categorical data, two of the most common of which
    are bar graphs (including stacked and grouped bar graphs) and pie charts.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: These types of visualization focus on the portion of the whole dataset each
    unique value takes up.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, with the preceding weather dataset, we can create a pie chart
    using the following code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will create the following visualization:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: Pie chart for weather data'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_02.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.2: Pie chart for weather data'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: We can see that in the whole dataset, the value `'sunny'` occurs 40 percent
    of the time, while each of the other values occurs 20 percent of the time.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'We have so far covered most of the biggest theoretical differences between
    a categorical attribute and a numerical attribute, which we will discuss in the
    next section. However, before moving on, there is another subtype of the categorical
    data type that should be mentioned: binary data.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: A binary attribute, whose values can only be `True` and `False`, is a categorical
    attribute whose set of possible values contains the two Boolean values mentioned.
    Since Boolean values can be easily interpreted by machine learning and mathematical
    models, there is usually not a need to convert a binary attribute into any other
    form.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, binary attributes that are not originally in the Boolean form should
    be converted into `True` and `False` values. We encountered an example of this
    in the sample student dataset in the previous chapter:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, the column `''sex''` is a categorical attribute whose values can either
    be `''female''` or `''male''`. So instead, what we can do to make this data more
    machine-friendly (while ensuring no information will be lost or added in) is to
    *binarize* the attribute, which we have done via the following code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，列`'sex'`是一个分类属性，其值可以是`'female'`或`'male'`。因此，为了使这些数据更适合机器处理（同时确保不会丢失或添加任何信息），我们可以对属性进行*二值化*，我们已经通过以下代码完成了这一步骤：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: Since the newly created column `'female_flag'` contains all the information
    from the column `'sex'` and only that, we can simply drop the latter from our
    dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于新创建的列`'female_flag'`包含了来自列`'sex'`的所有信息，而且只有这些信息，我们可以简单地从数据集中删除后者。
- en: Aside from that, binary attributes can be treated as categorical data in any
    other way (processing, making predictions, and visualization).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，二进制属性可以以任何其他方式（处理、预测和可视化）处理为分类数据。
- en: Let's now apply what we have discussed so far in the following exercise.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在将我们迄今讨论的内容应用到以下练习中。
- en: 'Exercise 3.01: Visualizing Weather Percentages'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习3.01：可视化天气百分比
- en: 'In this exercise, we are given a sample dataset that includes the weather in
    a specific city across five days. This dataset can be downloaded from [https://packt.live/2Ar29RG](https://packt.live/2Ar29RG).
    We aim to visualize the categorical information in this dataset to examine the
    percentages of different types of weather using the visualization techniques for
    categorical data that we have discussed so far:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们得到了一个样本数据集，其中包括特定城市在五天内的天气情况。这个数据集可以从[https://packt.live/2Ar29RG](https://packt.live/2Ar29RG)下载。我们的目标是使用迄今为止讨论的分类数据可视化技术，来可视化这个数据集中的分类信息，以检查不同类型天气的百分比：
- en: 'In a new Jupyter notebook, import pandas, Matplotlib, and seaborn and use pandas
    to read in the aforementioned dataset:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个新的Jupyter笔记本中，导入pandas、Matplotlib和seaborn，并使用pandas读取上述数据集：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'When the first five rows of this dataset are printed out, you should see the
    following output:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当打印出此数据集的前五行时，您应该看到以下输出：
- en: '![Figure 3.3: The weather dataset'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.3：天气数据集'
- en: '](image/B15968_03_03.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15968_03_03.jpg)'
- en: 'Figure 3.3: The weather dataset'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：天气数据集
- en: As you can see, each row of this dataset tells us what the weather was on a
    given day in a given city. For example, on day `0`, it was sunny in `St Louis`
    while it was `cloudy` in `New York`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，此数据集的每一行告诉我们在给定城市的给定日期的天气情况。例如，在`0`号那天，`St Louis`是晴天，而`New York`是`多云`。
- en: 'In the next code cell in the notebook, compute the counts (the numbers of occurrences)
    for all the weather types in our dataset and visualize that information using
    the `plot.bar()` method:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中的下一个代码单元中，计算数据集中所有天气类型的计数（发生次数），并使用`plot.bar()`方法可视化该信息：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This code will produce the following output:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将产生以下输出：
- en: '![Figure 3.4: Counts of weather types'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.4：天气类型的计数'
- en: '](image/B15968_03_04.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15968_03_04.jpg)'
- en: 'Figure 3.4: Counts of weather types'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：天气类型的计数
- en: 'Visualize the same information we have in the previous step as a pie chart
    using the `plot.pie(autopct=''%1.1f%%'')` method:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plot.pie(autopct='%1.1f%%')`方法将与上一步相同的信息可视化为饼图：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This code will produce the following output:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将产生以下输出：
- en: '![Figure 3.5: Counts of weather types'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.5：天气类型的计数'
- en: '](image/B15968_03_05.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15968_03_05.jpg)'
- en: 'Figure 3.5: Counts of weather types'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：天气类型的计数
- en: 'Now, we would like to visualize these counts of weather types, together with
    the information on what percentage each weather type accounts for in each city.
    First, this information can be computed using the `groupby()` method, as follows:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们想要可视化这些天气类型的计数，以及每种天气类型在每个城市中所占百分比的信息。首先，可以使用`groupby()`方法计算这些信息，如下所示：
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We see that this object contains the information that we wanted. For example,
    looking at the `cloudy` row in the table, we see that the weather type `cloudy`
    occurs three times in New York and three times in St Louis. There are multiple
    places where we have `NaN` values, which denote non-occurrences.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到这个对象包含了我们想要的信息。例如，看看表中`cloudy`行，我们可以看到`cloudy`天气类型在纽约出现了三次，在圣路易斯也出现了三次。我们有多个地方有`NaN`值，表示没有发生。
- en: 'We finally visualize the table we have in the previous step as a stacked bar
    plot:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将上一步中的表可视化为堆叠条形图：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will produce the following plot:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下图表：
- en: '![Figure 3.6: Counts of weather types with respect to cities'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.6：天气类型的计数与城市相关'
- en: '](image/B15968_03_06.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15968_03_06.jpg)'
- en: 'Figure 3.6: Counts of weather types with respect to cities'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6：天气类型的计数与城市相关
- en: Throughout this exercise, we have put our knowledge regarding categorical data
    into practice to visualize various types of counts computed from a sample weather dataset.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个练习过程中，我们已经将关于分类数据的知识付诸实践，以可视化从样本天气数据集中计算出的各种计数类型。
- en: Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: To access the source code for this specific section, please refer to [https://packt.live/2ArQAtw](https://packt.live/2ArQAtw).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本节的源代码，请参阅[https://packt.live/2ArQAtw](https://packt.live/2ArQAtw)。
- en: You can also run this example online at [https://packt.live/3gkIWAw](https://packt.live/3gkIWAw).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在[https://packt.live/3gkIWAw](https://packt.live/3gkIWAw)上在线运行此示例。
- en: 'With that, let''s move on to the second main type of data: numerical data.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，让我们继续讨论第二种主要类型的数据：数值数据。
- en: Numerical Data
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数值数据
- en: The term proves to be intuitive in helping us understand what type of data this
    is. A numerical attribute should contain numerical and continuous values or real
    numbers. The values belonging to a numerical attribute can have a specific range;
    for example, they can be positive, negative, or between 0 and 1\. However, an
    attribute being numerical implies that its data can take any value within its
    given range. This is notably different from values in a categorical attribute,
    which only belong to a given discrete set of values.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many examples of numerical data: the height of the members of a population,
    the weight of the students in a school, the price of houses that are for sale
    in certain areas, the average speed of track-and-field athletes, and so on. As
    long as the data can be represented as real-valued numbers, it is most likely
    numerical data.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Given its nature, numerical data is vastly different from categorical data.
    In the following text, we will lay out some of the most important differences
    with respect to statistics and machine learning that we should keep in mind.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: As opposed to a few probability distributions that can be used to model categorical
    data, there are numerous probability distributions for numerical data. These include
    the normal distribution (also known as the bell curve distribution), the uniform
    distribution, the exponential distribution, the Student's t distribution, and
    many more. Each of these probability distributions is designed to model specific
    types of data. For example, the normal distribution is typically used to model
    quantities with linear growth such as age, height, or students' test scores, while
    the exponential distribution models the amount of time between the occurrences
    of a given event.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: It is important, therefore, to research what specific probability distribution
    is suitable for the numerical attribute that you are attempting to model. An appropriate
    distribution will allow for coherent analysis as well as accurate predictions;
    on the other hand, an unsuitable choice of probability distribution might lead
    to unintuitive and incorrect conclusions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: On another topic, many processing techniques can be applied to numerical data.
    Two of the most common of these include scaling and normalization.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Scaling involves adding and/or multiplying all the values in a numerical attribute
    by a fixed quantity to scale the range of the original data to another range.
    This method is used when statistical and machine learning models can only handle
    values within a given range (for example, positive numbers or numbers between
    0 and 1 can be processed and analyzed more easily).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most commonly used scaling techniques is the min-max scaling method,
    which is explained by the following formula, where *a* and *b* are positive numbers:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7: Formula for min-max scaling'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_07.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.7: Formula for min-max scaling'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '*X''* and *X* denote the data after and before the transformation, while *X*max
    and *X*min denote the maximum and minimum values within the data, respectively.
    It can be mathematically proven that the output of the formula is always greater
    than *a* and less than *b*, but we don''t need to go over that here. We will come
    back to this scaling method again in our next exercise.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: As for normalization, even though this term is sometimes used interchangeably
    with *scaling*, it denotes the process of specifically scaling a numerical attribute
    to the normalized form with respect to its probability distribution. The goal
    is for us to obtain a transformed dataset that nicely follows the shape of the
    probability distribution we have chosen.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, say the data we have in a numerical attribute follows a normal
    distribution with a mean of `4` and a standard deviation of `10`. The following
    code randomly generates that data synthetically and visualizes it:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This produces the following plot:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8: Histogram for normally distributed data'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_08.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.8: Histogram for normally distributed data'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Now, say you have a model that assumes the standard form of the normal distribution
    for this data, where the mean is `0` and the standard deviation is `1`, and if
    the input data is not in this form, the model will have difficulty learning from
    it. Therefore, you'd like to somehow transform the preceding data into this standard
    form, without sacrificing the true pattern (specifically the general shape) of
    the data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can apply the normalization technique for normally distributed data,
    in which we subtract the true mean from the data points and divide the result
    by the true standard deviation. This scaling process is more generally known as
    a standard scaler. Since the preceding data is already a NumPy array, we can take
    advantage of vectorization and perform the normalization as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This code will generate the histogram for our newly transformed data, which
    is shown here:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9: Histogram for normalized data'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_09.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.9: Histogram for normalized data'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: We see that while the data has been successfully shifted to the range we want,
    now it centers around `0` and most of the data lies between `-3` and `3`, which
    is the standard form of the normal distribution, but the general shape of the
    data has not been altered. In other words, the relative differences between the
    data points have not been changed.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'On an additional note, in practice, when the true mean and/or the true standard
    deviation are not available, we can approximate those statistics with the sample
    mean and standard deviation as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: With a large number of samples, these two statistics offer a good approximation
    that can be further used for this type of transformation. With that, we can now
    feed this normalized data to our statistical and machine learning models for further
    analysis.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of the mean and the standard deviation, those two statistics are usually
    used to describe numerical data. To fill in missing values in a numerical attribute,
    central tendency measures such as the mean and the median are typically used.
    In some special cases such as a time-series dataset, you can use more complex
    missing value imputation techniques such as interpolation, where we estimate the
    missing value to be somewhere *in between* the ones immediately before and after
    it in a sequence.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: When we'd like to train a predictive model to target a numerical attribute,
    regression models are used. Instead of making predictions on which possible categorical
    values an entry can take like a classifier, a regression model looks for a reasonable
    prediction across a continuous numerical range. As such, similar to what we have
    discussed, we must take care to only apply regression models on datasets whose
    target values are numerical attributes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in terms of visualizing numerical data, we have seen a wide range of
    visualization techniques that we can use. Immediately before this, we saw histograms
    being used to describe the distribution of a numerical attribute, which tells
    us how the data is dispersed along its range.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: In addition, line graphs and scatter plots are generally good tools to visualize
    patterns of an attribute with respect to other attributes. (For example, we plotted
    the PDF of various probability distributions as line graphs.) Lastly, we also
    saw a heatmap being used to visualize a two-dimensional structure, which can be
    applied to represent correlations between numerical attributes in a dataset.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on with our next topic of discussion, let's performa quick exercise
    on the concept of scaling/normalization. Again, one of the most popular scaling/normalization
    methods is called *Min-Max scaling*, which allows us to transform all values in
    a numerical attribute into any arbitrary range *[a, b]*. We will explore this
    method next.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.02: Min-Max Scaling'
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will write a function that facilitates the process of
    applying Min-Max scaling to a numerical attribute. The function should take in
    three parameters: `data`, `a`, and `b`. While `data` should be a NumPy array or
    a pandas `Series` object, `a` and `b` should be real-valued positive numbers denoting
    the endpoints of the numerical range that `data` should be transformed into.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Referring back to the formula included in the *Numerical Data* section, Min-Max
    scaling is given by the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10: Formula for min-max scaling'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_10.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.10: Formula for min-max scaling'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the steps that need to be followed to meet our goal:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Jupyter notebook and in its first code cell, import the libraries
    that we will be using for this exercise, as follows:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the dataset that we will be using, the first column is named `''Column 1''`
    and contains 1,000 samples from a normal distribution with a mean of 4 and a standard
    deviation of 10\. The second column is named `''Column 2''` and contains 1,000
    samples from a uniform distribution from 1 to 2\. The third column is named `''Column
    3''` and contains 1,000 samples from a Beta distribution with parameters 2 and
    5\. In the next code cell, read in the `''data.csv''` file, which we generated
    for you beforehand (and which can be found at [https://packt.live/2YTrdKt](https://packt.live/2YTrdKt)),
    as a `DataFrame` object using pandas and print out the first five rows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You should see the following numbers:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In the next cell, write a function named `min_max_scale()` that takes in three
    parameters: `data`, `a`, and `b`. As mentioned, `data` should be an array of values
    in an attribute of a dataset, while `a` and `b` specify the range that the input
    data is to be transformed into.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Given the (implicit) requirement we have about `data` (a NumPy array or a pandas
    `Series` object—both of which can utilize vectorization), implement the scaling
    function with vectorized operations:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We will consider the data in the `''Column 1''` attribute first. To observe
    the effect that this function will have on our data, let''s first visualize the
    distribution of what we currently have:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This code will generate a plot that is similar to the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11: Histogram of unscaled data'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_11.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.11: Histogram of unscaled data'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, use the same `plt.hist()` function to visualize the returned value of
    the `min_max_scale()` function when called on `df[''Column 1'']` to scale that
    data to the range `[-3, 3]`:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This will produce the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12: Histogram of scaled data'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_12.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.12: Histogram of scaled data'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: We see that while the general shape of the data distribution remains the same,
    the range of the data has been effectively changed to be from `-3` to `3`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Go through the same process (visualizing the data before and after scaling
    with histograms) for the `''Column 2''` attribute. First, we visualize the original
    data:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now we visualize the scaled data, which should be scaled to the range `[0,
    1]`:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The second block of code should produce a graph similar to the following:![Figure
    3.13: Histogram of scaled data'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B15968_03_13.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.13: Histogram of scaled data'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Go through the same process (visualizing the data before and after the scaling
    with histograms) for the `''Column 3''` attribute. First, we visualize the original data:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now we visualize the scaled data, which should be scaled to the range `[10, 20]`:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The second block of code should produce a graph similar to the following:![Figure
    3.14: Histogram of scaled data'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B15968_03_14.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.14: Histogram of scaled data'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we have considered the concept of scaling/normalization for
    numerical data in more detail. We have also revisited the `plt.hist()` function
    as a method to visualize the distribution of numerical data.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2VDw3JP](https://packt.live/2VDw3JP).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅[https://packt.live/2VDw3JP](https://packt.live/2VDw3JP)。
- en: You can also run this example online at [https://packt.live/3ggiPdO](https://packt.live/3ggiPdO).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/3ggiPdO](https://packt.live/3ggiPdO)上在线运行此示例。
- en: The exercise concludes the topic of numerical data in this chapter. Together
    with categorical data, it makes up most of the data types that you might see in
    a given dataset. However, there is actually another data type in addition to these
    two, which is less common, as we will discuss in the next section.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习结束了本章关于数值数据的讨论。连同分类数据一起，它构成了您可能在给定数据集中看到的大多数数据类型。然而，实际上除了这两种数据类型之外，还有另一种数据类型，这种数据类型较少见，我们将在下一节中讨论。
- en: Ordinal Data
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序数数据
- en: Ordinal data is somewhat of a combination of categorical data (values in an
    ordinal attribute belonging to a specific given set) and numerical data (where
    the values are numbers—this fact implies an ordered relationship between them).
    The most common examples of ordinal data are letter scores (`"A"`, `"B"`, `"C"`,
    `"D"`, and `"E"`), integer ratings (for example, on a scale of 1 to 10), or quality
    ranking (for example, `"excellent"`, `"okay"`, and `"bad"`, where `"excellent"`
    implies a higher level of quality than `"okay"`, which in itself is better than
    `"bad"`).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 序数数据在某种程度上是分类数据（序数属性中的值属于特定给定集合）和数值数据（其中值为数字——这一事实意味着它们之间存在有序关系）的组合。序数数据的最常见示例是字母分数（“A”，“B”，“C”，“D”和“E”），整数评分（例如，在1到10的范围内），或者质量排名（例如，“优秀”，“好”，和“差”，其中“优秀”意味着比“好”更高的质量级别，而“好”本身又比“差”更好）。
- en: Since entries in an ordinal attribute can only take on one out of a specific
    set of values, *categorical probability distributions* should be used to model
    this type of data. For the same reason, missing values in an ordinal attribute
    can be filled out using the mode of the attribute, and visualization techniques
    for categorical data can be applied to ordinal data as well.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于序数属性中的条目只能取特定一组值中的一个，应该使用*分类概率分布*来对这种类型的数据进行建模。出于同样的原因，序数属性中的缺失值可以使用属性的众数来填充，分类数据的可视化技术也可以应用于序数数据。
- en: However, other processes might prove different from what we have discussed for
    categorical data. In terms of data processing, you could potentially assign a
    one-to-one mapping between each ordinal value and a numerical value/range.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，其他过程可能与我们讨论的分类数据有所不同。在数据处理方面，您可能会为每个序数值分配一个一对一的映射，以及一个数字值/范围。
- en: In the letter score example, it is commonly the case that the grade `"A"` corresponds
    to the range `[90, 100]` in the raw score, and other letter grades have their
    own continuous ranges as well. In the quality ranking example, `"excellent"`,
    `"okay"`, and `"bad"` can be mapped to 10, 5, and 0, respectively, as an example;
    however, this type of transformation is undesirable unless the degree of difference
    in quality between the values can be quantified.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在字母分数的例子中，通常情况下，等级“A”对应于原始分数的范围[90, 100]，其他字母等级也有它们自己的连续范围。在质量排名的例子中，“优秀”，“好”和“差”可以分别映射为10，5和0，但是，除非可以量化值之间的质量差异程度，否则这种转换是不可取的。
- en: In terms of fitting a machine learning model to the data and having it predict
    unseen values of an ordinal attribute, classifiers should be used for this task.
    Furthermore, since ranking is a unique task that constitutes many different learning
    structures, considerable effort has been dedicated to *machine-learning ranking*,
    where models are designed and trained specifically to predict ranking data.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在将机器学习模型拟合到数据并让其预测序数属性的未见值方面，应该使用分类器来执行此任务。此外，由于排名是构成许多不同学习结构的独特任务，已经付出了相当大的努力来进行*机器学习排名*，其中专门设计和训练模型以预测排名数据。
- en: 'This discussion concludes the topic of data types in statistics and machine
    learning. Overall, we have learned that there are two main data types commonly
    seen in datasets: categorical and numerical data. Depending on which type your
    data belongs to, you will need to employ different data processing, machine learning,
    and visualization techniques.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这个讨论结束了统计学和机器学习中的数据类型主题。总的来说，我们已经了解到数据集中常见的两种主要数据类型：分类和数值数据。根据您的数据属于哪种类型，您将需要使用不同的数据处理、机器学习和可视化技术。
- en: In the next section, we will talk about descriptive statistics and how they
    can be computed in Python.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论描述统计以及如何在Python中进行计算。
- en: Descriptive Statistics
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 描述统计
- en: As mentioned before, descriptive statistics and inferential statistics are the
    two main categories in the field of statistics. With descriptive statistics, our
    goal is to compute specific quantities that can convey important information about—or
    in other words, describe—our data.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，描述统计和推断统计是统计学领域的两个主要类别。通过描述统计，我们的目标是计算特定的数量，可以传达关于我们的数据的重要信息，或者换句话说，描述我们的数据。
- en: 'From within descriptive statistics, there are two main subcategories: central
    tendency statistics and dispersion statistics. The actual terms are suggestive
    of their respective meaning: **central tendency statistics** are responsible for
    describing the *center* of the distribution of the given data, while **dispersion
    statistics** convey information about the spread or range of the data away from
    its center.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述统计中，有两个主要的子类别：中心趋势统计和离散统计。实际术语暗示了它们各自的含义：中心趋势统计负责描述给定数据的*中心*，而离散统计传达有关数据远离其中心的传播或范围的信息。
- en: One of the clearest examples of this distinction is from the familiar normal
    distribution, whose statistics include a mean and a standard deviation. The mean,
    which is calculated to be the average of all the values from the probability distribution,
    is suitable for estimating the center of the distribution. In its standard form,
    as we have seen, the normal distribution has a mean of 0, indicating that its
    data revolves around point 0 on the axis.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: The standard deviation, on the other hand, represents how much the data points
    vary from the mean. Without going into much detail, in a normal distribution,
    it is calculated to be the mean distance from the mean of the distribution. A
    low-valued standard deviation indicates that the data does not deviate too much
    from the mean, while a high-valued standard deviation implies that the individual
    data points are quite different from the mean.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, these types of statistics and their characteristics can be summarized
    in the following table:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15: Types of descriptive statistics'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_15.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.15: Types of descriptive statistics'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: There are also other, more specialized descriptive statistics, such as skewness,
    which measures the asymmetry of the data distribution, or kurtosis, which measures
    the sharpness of the distribution peak. However, these are not as commonly used
    as the ones we listed previously, and therefore will not be covered in this chapter.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we will start discussing each of the preceding statistics
    in more depth, starting with central tendency measures.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Central Tendency
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Formally, the three commonly used central tendency statistics are the mean,
    the median, and the mode. The **median** is defined as the middlemost value when
    all the data points are ordered along the axis. The **mode**, as we have mentioned
    before, is the value that occurs the most. Due to their characteristics, the mean
    and the median are only applicable for numerical data, while the mode is often
    used on categorical data.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: All three of these statistics capture the concept of central tendency well by
    representing the center of a dataset in different ways. This is also why they
    are often used as replacements for missing values in an attribute. As such, with
    a missing numerical value, you can choose either the mean or the median as a potential
    replacement, while the mode could be used if a categorical attribute contains
    missing values.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: In particular, it is actually not arbitrary that the mean is often used to fill
    in missing values in a numerical attribute. If we were to fit a probability distribution
    to the given numerical attribute, the mean of that attribute would actually be
    the sample mean, an estimation of the true population mean. Another term for the
    population mean is the expected value of an unknown value within that population,
    which, in other words, is what we should expect an arbitrary value from that population
    to be.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: This is why the mean, or the expectation of a value from the corresponding distribution,
    should be used to fill in missing values in certain cases. While it is not exactly
    the case for the median, a somewhat similar argument can be made for its role
    in replacing missing numerical values. The mode, on the other hand, is a good
    estimation for missing categorical values, being the most commonly occurring value
    in an attribute.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Dispersion
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Different from central tendency statistics, dispersion statistics, again, attempt
    to quantify how much variation there is in a dataset. Some common dispersion statistics
    are the standard deviation, the range (the difference between the maximum and
    the minimum), and quartiles.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: The standard deviation, as we have mentioned, calculates the difference between
    each data point and the mean of a numerical attribute, squares them, takes their
    average, and finally takes the square root of the result. The further away the
    individual data points are from the mean, the larger this quantity gets, and vice
    versa. This is why it is a good indicator of how dispersed a dataset is.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: The range—the distance between the maximum and the minimum, or the 0- and 100-percent
    quartiles—is another, simpler way to describe the level of dispersion of a dataset.
    However, because of its simplicity, sometimes this statistic does not convey as
    much information as the standard deviation or the quartiles.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: A quartile is defined to be a threshold below which a specific portion of a
    given dataset falls. For example, the median, the middlemost value of a numerical
    dataset, is the 50-percent quartile for that dataset, as (roughly) half of the
    dataset is less than that number. Similarly, we can compute common quartile quantities
    such as the 5-, 25-, 75-, and 95-percent quartiles. These quartiles are arguably
    more informative in terms of quantifying how dispersed our data is than the range,
    as they can account for different distributions of the data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the *interquartile range*, another common dispersion statistic,
    is defined to be the difference between the 25- and 75-percent quartiles of a
    dataset.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed the concepts of central tendency statistics and dispersion
    statistics. Let's go through a quick exercise to reinforce some of these important ideas.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.03: Visualizing Probability Density Functions'
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In *Exercise 2.04*, *Visualization of Probability Distributions* of *Chapter
    2*, *Python''s Main Tools for Statistics*, we considered the task of comparing
    the PDF of a probability distribution against the histogram of its sampled data.
    Here, we will implement an extension of that program, where we also visualize
    various descriptive statistics for each of these distributions:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first cell of a new Jupyter notebook, import NumPy and Matplotlib:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In a new cell, randomly generate 1,000 samples from the normal distribution
    using `np.random.normal()`. Compute the mean, median, and the 25- and 75-percent
    quartiles descriptive statistics as follows:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the next cell, visualize the samples using a histogram. We will also indicate
    where the various descriptive statistics are by drawing vertical lines—a red vertical
    line at the mean point, a black one at the median, a blue line at each of the
    quartiles:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Note here that we are combining the specification of the `label` argument in
    various plotting function calls and the `plt.legend()` function. This will help
    us create a legend with appropriate labels, as can be seen here:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16: Descriptive statistics for a normal distribution'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_16.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.16: Descriptive statistics for a normal distribution'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing is of interest here: the mean and the median almost coincide on the
    x axis. This is one of the many mathematically convenient features of a normal
    distribution that is not found in many other distributions: its mean is equal
    to both its median and its mode.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the same process to a Beta distribution with parameters `2` and `5`,
    as follows:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This should generate a graph similar to the following:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17: Descriptive statistics for a Beta distribution'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_17.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.17: Descriptive statistics for a Beta distribution'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the same process to a Gamma distribution with parameter `5`, as follows:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This should generate a graph similar to the following:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18: Descriptive statistics for a Gamma distribution'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_18.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.18: Descriptive statistics for a Gamma distribution'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: With this exercise, we have learned how to compute various descriptive statistics
    of a dataset using NumPy and visualize them in a histogram.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2YTobpm](https://packt.live/2YTobpm).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2CZf26h](https://packt.live/2CZf26h).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: In addition to computing descriptive statistics, Python also offers other additional
    methods to describe data, which we will discuss in the next section.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Python-Related Descriptive Statistics
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we will examine two intermediate methods for describing data. The first
    is the `describe()` method, to be called on a `DataFrame` object. From the official
    documentation (which can be found at [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)),
    the function "generate(s) descriptive statistics that summarize the central tendency,
    dispersion, and shape of a dataset's distribution, excluding `NaN` values."
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the effect of this method in action. First, we will create a sample
    dataset with a numerical attribute, a categorical attribute, and an ordinal one,
    as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, if we were to call the `describe()` method on the `df` variable, a tabular
    summary would be generated:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As you can see, each row in the printed output denotes a different descriptive
    statistic about each attribute in our dataset: the number of values (`count`),
    mean, standard deviation, and various quartiles. Since both the `numerical` and
    `ordinal` attributes were interpreted as numerical data (given the data they contain),
    `describe()` only generates these reports for them by default. The `categorical`
    column, on the other hand, was excluded. To force the reports to apply to all
    columns, we can specify the `include` argument as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This forces the method to compute other statistics that apply for categorical
    data, such as the number of unique values (`unique`), the mode (`top`), and the
    count/frequency of the mode (`freq`). As we have discussed, most of the descriptive
    statistics for numerical data do not apply for categorical data and vice versa,
    which is why `NaN` values are used in the preceding reports to indicate such a
    non-application.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the `describe()` method from pandas offers a quick way to summarize
    and obtain an overview of a dataset and its attributes. This especially comes
    in handy during exploratory data analysis tasks, where we'd like to broadly explore
    a new dataset that we are not familiar with yet.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: The second descriptive statistics-related method that is supported by Python
    is the visualization of boxplots. Obviously, a boxplot is a visualization technique
    that is not unique to the language itself, but Python, specifically its seaborn
    library, provides a rather convenient API, the `sns.boxplot()` function, to facilitate
    the process.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Theoretically, a boxplot is another method to visualize the distribution of
    a numerical dataset. It, again, can be generated with the `sns.boxplot()` function:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This code will produce a graph roughly similar to the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19: Boxplot using seaborn'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_19.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.19: Boxplot using seaborn'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding boxplot, the blue box in the middle denotes the interquartile
    range of the input data (from the 25- to 75-percent quartile). The vertical line
    in the middle of the box is the median, while the two thresholds on the left and
    right but outside of the box denote the minimum and maximum of the input data,
    respectively.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the minimum is calculated to be the 25-percent
    quartile *minus* the interquartile range multiplied by 1.5, and the maximum the
    75-percent quartile *plus* the interquartile range also multiplied by 1.5\. It
    is common practice to consider any number outside of this range between the minimum
    and the maximum to be outliers, visualized as black dots in the preceding graph.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: In essence, a boxplot can represent the statistics computed by the `describe()`
    function from pandas visually. What sets this function from seaborn apart from
    other visualization tools is the ease in creating multiple boxplots given a criterion
    provided by seaborn.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see this in this next example, where we extend the sample dataset to
    `1000` rows with random data generation:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Here, the `'numerical'` attribute contains random draws from the standard normal
    distribution, the `'categorical'` attribute contains values randomly chosen from
    the list `['a', 'b', 'c']`, while `'ordinal'` also contains values randomly chosen
    from a list, `[1, 2, 3, 4, 5]`.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Our goal with this dataset is to generate a slightly more complex boxplot visualization—a
    boxplot representing the distribution of the data in `'numerical'` for the different
    values in `'categorical'`. The general process is to split the dataset into different
    groups, each corresponding to the unique value in `'categorical'`, and for each
    group, we'd like to generate a boxplot using the respective data in the `'numerical'`
    attribute.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'However, with seaborn, we can streamline this process by specifying the `x`
    and `y` arguments for the `sns.boxplot()` function. Specifically, we will have
    our *x* axis contain the different unique values in `''categorical''` and the
    *y* axis represent the data in `''numerical''` with the following code:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This will generate the following plot:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20: Multi-boxplot using seaborn'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_20.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.20: Multi-boxplot using seaborn'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'The visualization contains what we wanted to display: the distribution of the
    data in the `''numerical''` attribute, represented as boxplots and separated by
    the unique values in the `''categorical''` attribute. Considering the unique values
    in `''ordinal''`, we can apply the same process as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This will generate the following graph:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21: Multi-boxplot using seaborn'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_21.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.21: Multi-boxplot using seaborn'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, this method of visualization is ideal when we'd like to
    analyze the differences in the distribution of a numerical attribute with respect
    to categorical or ordinal data.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'And that concludes the topic of descriptive statistics in this chapter. In
    the next section, we will talk about the other category of statistics: inferential
    statistics.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Inferential Statistics
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike descriptive statistics, where our goal is to describe various characteristics
    of a dataset using specific quantities, with inferential statistics, we'd like
    to perform a particular statistical modeling process on our dataset so that we
    can *infer* further information, either about the dataset itself or even about
    unseen data points that are from the same population.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will go through a number of different methods of inferential
    statistics. From these discussions, we will see that each method is designed for
    specific data and situations, and it is the responsibility of the statistician
    or machine learning engineer to appropriately apply them.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'The first method that we will discuss is one of the most fundamental in classical
    statistics: t-tests.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: T-Tests
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, t-tests (also known as Student's t-tests) are used to compare two
    mean (average) statistics and conclude whether they are different enough from
    each other. The main application of a t-test is comparing the effect of an event
    (for example, an experimental drug, an exercise routine, and so on) on a population
    against a controlled group. If the means are different enough (we call this statistically
    significant), then we have good reason to believe in the effect of the given event.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main types of t-tests in statistics: independent samples t-tests
    (used to compare the means of two independent samples), paired sample t-tests
    (used to compare the means of the same group at different times), and one-sample
    t-tests (used to compare the mean of one group with a predetermined mean).'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: The general workflow of a t-test is to first declare the null hypothesis that
    the two means are indeed equal and then consider the output of the t-test, which
    is the corresponding p-value. If the p-value is larger than a fixed threshold
    (usually, 0.05 is chosen), then we cannot reject the null hypothesis. If, on the
    other hand, the p-value is lower than the threshold, we can reject the null hypothesis,
    implying that the two means are different. We see that this is an inferential
    statistics method as, from it, we can *infer* a fact about our data; in this case,
    it is whether the two means we are interested in are different from each other.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: We will not go into the theoretical details of these tests; instead, we will
    see how we can simply take advantage of the API offered in Python, or specifically
    the SciPy library. We used this library in the last chapter, so if you are not
    yet familiar with the tool, be sure to head back to *Chapter 2*, *Python's Main
    Tools for Statistics* to see how it can be installed in your environment.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Let's design our sample experiment. Say we have two arrays of numbers, each
    was drawn from an unknown distribution, and we'd like to find out whether their
    respective means are equal to each other. Thus, we have our null hypothesis that
    the means of these two arrays are equal, which can be rejected if the p-value
    of our t-test is less than 0.05.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate the synthetic data for this example, we will use `20` samples from
    the standard form of the normal distribution (a mean of `0`, and a standard deviation
    of `1`) for the first array, and another `20` samples from a normal distribution
    with a mean of `0.2` and a standard deviation of `1` for the second array:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'To quickly visualize this dataset, we can use the `plt.hist()` function as
    follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This generates the following plot (note that your own output might be different):'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22: Histogram of sample data for a t-test'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_22.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.22: Histogram of sample data for a t-test'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will call the `ttest_ind()` function from the `scipy.stats` package.
    This function facilitates an independent samples t-test and will return an object
    having an attribute named `pvalue`; this attribute contains the p-value that will
    help us decide whether to reject our null hypothesis or not:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: With this result, we do not reject our null hypothesis. Again, your p-value
    might be different from the preceding output, but chances are it is not lower
    than 0.05 either. Our final conclusion here is that we don't have enough evidence
    to say that the means of our two arrays are different (even though they were actually
    generated from two normal distributions with different means).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s repeat this experiment, but this time we have significantly more data—each
    array now contains 1,000 numbers:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The histogram now looks like the following:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.23: Histogram of sample data for a t-test'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_23.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.23: Histogram of sample data for a t-test'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the t-test again, we see that this time, we obtain a different result:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This p-value is a lot lower than 0.05, thus rejecting the null hypothesis and
    giving us enough evidence to say that the two arrays have different means.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: These two experiments demonstrated a phenomenon we should keep in mind. In the
    first experiment, our p-value wasn't low enough for us to reject the null hypothesis,
    even though our data was indeed generated from two distributions with different
    means. In the second experiment, with more data, the t-test was more conclusive
    in terms of differentiating the two means.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: In essence, with only 20 samples in each array, the first t-test didn't have
    a high enough level of confidence to output a lower p-value, even if the two means
    were indeed different. With 1,000 samples, this difference was more consistent
    and robust so that the second t-test was able to positively output a lower p-value.
    In general, many other statistical methods will similarly prove to be more conclusive
    as more data is used as input.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: We have looked at an example of the independent samples t-test as a method of
    inferential statistics to test for the degree of difference between the averages
    of two given populations. Overall, the `scipy.stats` package offers a wide range
    of statistical tests that take care of all of the computation in the background
    and only return the final test output. This follows the general philosophy of
    the Python language, keeping the API at a high level so that users can take advantage
    of complex methodologies in a flexible and convenient manner.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: More details on what is available in the `scipy.stats` package can be found
    in its official documentation at [https://docs.scipy.org/doc/scipy-0.15.1/reference/tutorial/stats.html](https://docs.scipy.org/doc/scipy-0.15.1/reference/tutorial/stats.html).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the most commonly used tests that can be called from the package include:
    t-tests or ANOVAs for differences in means; normality testing to ascertain whether
    samples have been drawn from a normal distribution; and computation of the Bayesian
    credible intervals for the mean and standard deviation of a sample population.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving away from the `scipy.stats` package, we have seen that the pandas library
    also supports a wide range of statistical functionalities, especially with its
    convenient `describe()` method. In the next section, we will look into the second
    inferential statistics method: the correlation matrix of a dataset.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Correlation Matrix
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A correlation matrix is a two-dimensional table containing correlation coefficients
    between each pair of attributes of a given dataset. A correlation coefficient
    between two attributes quantifies their level of linear correlation, or in other
    words, how similarly they behave in a linear fashion. A correlation coefficient
    lies in the range between -1 and +1, where +1 denotes perfect linear correlation,
    0 denotes no correlation, and -1 denotes perfect negative correlation.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: If two attributes have a high linear correlation, then when one increases, the
    other tends to increase by the same amount multiplied by a constant. In other
    words, if we were to plot the data in the two attributes on a scatter plot, the
    individual points would tend to follow a line with a positive slope. For two attributes
    having no correlation, the best-fit line tends to be horizontal, and two attributes
    having a negative correlation are represented by a line with a negative slope.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: The correlation between two attributes can, in a way, tell us how much information
    is shared among the attributes. We can infer from two correlated attributes, either
    positively or negatively, that there is some underlying relationship between them.
    This is the idea behind the correlation matrix as an inferential statistics tool.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: In some machine learning models, it is recommended that if we have highly correlated
    features, we should only leave one in the dataset before feeding it to the models.
    In most cases, having another attribute that is highly correlated to one that
    a model has been trained on does not improve its performance; what's more, in
    some situations, correlated features can even mislead our models and steer their
    predictions in the wrong direction.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: This is to say that the correlation coefficient between two data attributes,
    and thus the correlation matrix of the dataset, is an important statistical object
    for us to consider. Let's see this in a quick example.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we have a dataset of three attributes, `''x''`, `''y''`, and `''z''`. The
    data in `''x''` and `''z''` is randomly generated in an independent way, so there
    should be no correlation between them. On the other hand, we will generate `''y''`
    as the data in `''x''` multiplied by 2 and add in some random noise. This can
    be done with the following code, which creates a dataset with 500 entries:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'From here, the correlation matrix (which, again, contains correlation coefficients
    of every pair of attributes in our dataset) can be easily computed with the `corr()` method:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output is as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We see that this is a 3 x 3 matrix, as there are three attributes in the calling
    `DataFrame` object. Each number denotes the correlation between the row and the
    column attributes. One effect of this representation is that we have all of the
    diagonal values in the matrix as 1, as each attribute is perfectly correlated
    to itself.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'What''s more interesting to us is the correlation between different attributes:
    as `''z''` was generated independently of `''x''` (and therefore `''y''`), the
    values in the `''z''` row and column are relatively close to 0\. In contrast to
    this, the correlation between `''x''` and `''y''` is quite close to 1, as one
    was generated to be roughly two times the other.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, it is common to visually represent the correlation matrix with
    a heatmap. This is because when we have a large number of attributes in our dataset,
    a heatmap will help us identify the regions that correspond to highly correlated
    attributes more efficiently. The visualization of a heatmap can be done using
    the `sns.heatmap()` function from the seaborn library:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The `annot=True` argument specifies that the values in the matrix should be
    printed out in each cell of the heatmap.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'The code will produce the following:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24: Heatmap representing a correlation matrix'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_24.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.24: Heatmap representing a correlation matrix'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: In this case, while visually inspecting a correlation matrix heatmap, we can
    focus on the bright regions, aside from the diagonal cells, to identify highly
    correlated attributes. If there were negatively correlated attributes in a dataset
    (which we don't have in our current example), those could be detected with dark
    regions as well.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the correlation matrix of a given dataset can be a useful tool for
    us to understand the relationship between the different attributes of that dataset.
    We will see an example of this in the upcoming exercise.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.04: Identifying and Testing Equality of Means'
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will practice the two inferential statistics methods to
    analyze a synthetic dataset that we have generated for you. The dataset can be
    downloaded from the GitHub repository at [https://packt.live/3ghKkDS](https://packt.live/3ghKkDS).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Here, our goal is to first identify which attributes in this dataset are correlated
    with each other and then apply a t-test to determine whether any pair of attributes
    have the same mean.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'With that said, let''s get started:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'In a new Jupyter notebook, import `pandas`, `matplotlib`, `seaborn`, and the
    `ttest_ind()` method from the `stats` module from SciPy:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Read in the dataset that you have downloaded. The first five rows should look
    like the following:![Figure 3.25: Reading the first five rows of the dataset'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B15968_03_25.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.25: Reading the first five rows of the dataset'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next code cell, use seaborn to generate the heatmap that represents
    the correlation matrix for this dataset. From the visualization, identify the
    pair of attributes that are correlated with each other the most:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This code should produce the following visualization:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.26: Correlation matrix for the dataset'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_26.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.26: Correlation matrix for the dataset'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'From this output, we see that attributes `''x''` and `''y''` have a correlation
    coefficient that is quite high: `0.94`.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this `jointplot()` method in seaborn, create a combined plot with two
    elements: a scatter plot on a two-dimensional plane where the coordinates of the
    points correspond to the individual values in `''x''` and `''y''` respectively,
    and two histograms representing the distributions of those values. Observe the
    output and decide whether the two distributions have the same mean:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This will produce the following output:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.27: Combined plot of correlated attributes'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_27.jpg)'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.27: Combined plot of correlated attributes'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: From this visualization, it is not clear whether the two attributes have the
    same mean or not.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of using a visualization, run a t-test with 0.05 level of significance
    to decide whether the two attributes have the same mean:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This command will have the following output:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: This p-value is indeed lower than 0.05, allowing us to reject the null hypothesis
    that the two distributions have the same mean, even though they are highly correlated.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we applied the two inferential statistics methods that we
    have learned in this section to analyze a pair of correlated attributes in a dataset.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31Au1hc](https://packt.live/31Au1hc).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2YTt7L7](https://packt.live/2YTt7L7).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final section on the topic of inferential statistics, we will
    discuss the process of using statistical and machine learning models as a method
    of making inferences using statistics.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Statistical and Machine Learning Models
  id: totrans-389
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modeling a given dataset using a mathematical or machine learning model, which
    in itself is capable of generalizing any potential patterns and trends in the
    dataset to unseen data points, is another form of inferential statistics. Machine
    learning itself is arguably one of the fastest-growing fields in computer science.
    However, most machine learning models actually leverage mathematical and statistical
    theories, which is why the two fields are heavily connected. In this section,
    we will consider the process of training a model on a given dataset and how Python
    can help facilitate that process.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that a machine learning model does not actually learn
    in the same sense that humans do. Most of the time, a model attempts to solve
    an optimization problem that minimizes its training error, which represents how
    well it can process the pattern within the training data, with the hope that the
    model can generalize well on unseen data that is drawn from the same distributions
    as the training data.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: For example, a linear regression model generates the line of best fit that passes
    through all the data points in a given dataset. In the model definition, this
    line corresponds to the line that has the minimal sum of distances to the individual
    data points, and by solving the optimization problem of minimizing the sum of
    distances, a linear regression model is able to output that best-fitted line.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Overall, each machine learning algorithm models the data and therefore the optimization
    problem in a different way, each suitable for specific settings. However, different
    levels of abstraction built into the Python language allow us to skip through
    these details and apply different machine learning models at a high level. All
    we need to keep in mind is that statistical and machine learning models are another
    method of inferential statistics where we are able to make predictions on unseen
    data, given the pattern represented in a training dataset.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we are given the task of training a model on the sample dataset we
    have in the previous section, where the learning features are `'x'` and `'z'`,
    and our prediction target is `'y'`. That is, our model should learn any potential
    relationship between `'x'` or `'z'` and `'y'`, and from there know how to predict
    unseen values of `'y'` from the data in `'x'` and `'z'`.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'Since `''y''` is a numerical attribute, we will need a regression model, as
    opposed to a classifier, to train on our data. Here, we will use one of the most
    commonly used regressors in statistics and machine learning: linear regression.
    For that, we will require the scikit-learn library, one of the most—if not the
    most—popular predictive data analysis tools in Python.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'To install scikit-learn, run the following `pip` command:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'You can also use the `conda` command to install it:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now, we import the linear regression model and fit it to our training data:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'In general, the `fit()` method, called by a machine learning model object,
    takes in two arguments: the independent features (that is, the features that will
    be used to make predictions), which in this case are `''x''` and `''z''`, and
    the dependent feature or the prediction target (that is, the attribute that we''d
    like to make predictions on), which in this case is `''y''`.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: This `fit()` method will initiate the training process of the model on the given
    data. Depending on the complexity of the model as well as the size of the training
    data, this process might take a significant amount of time. For a linear regression,
    however, the training process should be relatively fast.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Once our model has finished training, we can look at its various statistics.
    What statistics are available depends on the specific model being used; for a
    linear regression, it is common for us to consider the coefficients. A regression
    coefficient is an estimate of the linear relationship between an independent feature
    and the prediction target. In essence, the regression coefficients are what the
    linear regression model estimates for the slope of the best-fit line for a specific
    predictor variable, `'x'` or `'z'` in our case, and the feature we'd like to predict—`'y'`.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: 'These statistics can be accessed as follows:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'This will give us the following output:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Again, the output from your own experiment might not be exactly the same as
    the preceding. However, there is a clear trend to these coefficients: the first
    coefficient (denoting the estimated linear relationship between `''x''` and `''y''`)
    is approximately 2, while the second (denoting the estimated linear relationship
    between `''z''` and `''y''`) is close to 0.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 'This result is quite consistent with what we did to generate this dataset:
    `''y''` was generated to be roughly equal to the elements in `''x''` multiplied
    by 2, while `''z''` was independently generated. By looking at these regression
    coefficients, we can obtain information about which features are the best (linear)
    predictors for our prediction target. Some consider these types of statistics
    to be explainability/interpretability statistics, as they give us insights regarding
    how the prediction process was done.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'What''s more interesting to us is the process of making predictions on unseen
    data. This can be done by calling the `predict()` method on the model object like
    so:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The output will be as follows:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Here, we pass to the `predict()` method any data structure that can represent
    a two-dimensional table (in the preceding code, we used a nested list, but in
    theory, you could also use a two-dimensional NumPy array or a pandas `DataFrame`
    object). This table needs to have its number of columns equal to the number of
    independent features in the training data; in this case, we have two (`'x'` and
    `'z'`), so each sub-list in `[[1, 2], [2, 3]]` has two elements.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: From the predictions produced by the model, we see that when `'x'` is equal
    to 1 and `'z'` is equal to 2 (our first test case), the corresponding prediction
    is roughly 2\. This is consistent with the fact that the coefficient for `'x'`
    is approximately 2 and the one for `'z'` is close to 0\. The same goes for the
    second test case.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: 'And that is an example of how a machine learning model can be used to make
    predictions on data. Overall, the scikit-learn library offers a wide range of
    models for different types of problems: classification, regression, clustering,
    dimensionality reduction, and so on. The API among the models is consistent with
    the `fit()` and `predict()` methods, as we have seen. This allows a greater degree
    of flexibility and streamlining.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: An important concept in machine learning is model selection. Not all models
    are created equal; some models, due to their design or characteristics, are better
    suited to a given dataset than others. This is why model selection is an important
    phase in the whole machine learning pipeline. After collecting and preparing a
    training dataset, machine learning engineers typically feed the dataset to a number
    of different models, and some models might be excluded from the process due to
    poor performance.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: We will see a demonstration of this in the following exercise, where we are
    introduced to the process of model selection.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.05: Model Selection'
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will go through a sample model selection procedure, where
    we attempt to fit three different models to a synthetic dataset and consider their performance:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first code cell of a new Jupyter notebook, import the following tools:'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Note
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: We are not yet familiar with some of the tools, but they will be explained to
    us as we go through this exercise.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Now, we'd like to create a synthetic dataset of points lying on a two-dimensional
    plane. Each of these points belongs to a specific group, and points belonging
    to the same group should revolve around a common center point.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'This synthetic data can be generated using the `make_blobs` function that we
    have imported from the `sklearn.datasets` package:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: As we can see, this function takes in an argument named `n_samples`, which specifies
    the number of data points that should be produced. The `centers` argument, on
    the other hand, specifies the total number of groups that the individual points
    belong to and their respective coordinates. In this case, we have three groups
    of points centering around `(-2, 2)`, `(0, 0)`, and `(2, 2)` respectively.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, by specifying the `random_state` argument as `0`, we ensure that the
    same data is generated every time we rerun this notebook. As we mentioned in *Chapter
    1*, *Fundamentals of Python*, this is good practice in terms of reproducibility.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our goal here is to train various models on this data so that when fed a new
    list of points, the model can decide which group each point should belong to with
    high accuracy.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: This function returns a tuple of two objects that we are assigning to the variables
    `X` and `y`, respectively. The first element in the tuple contains the independent
    features of the dataset; in this case, they are the *x* and *y* coordinates of
    the points. The second tuple element is our prediction target, the index of the
    group each point belongs to. The convention is to store the independent features
    in a matrix named `X`, and the prediction targets in a vector named `y`, as we
    are doing.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: 'Print out these variables to see what we are dealing with. Type `X` as the
    input:'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'This will give the following output:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Now, type `y` as the input:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This will give the following output:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Now, in a new code cell, we''d like to visualize this dataset using a scatter
    plot:'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: We use the first attribute in our dataset as the *x* coordinates and the second
    as the *y* coordinates for the points in the scatter plot. We can also quickly
    specify that points belonging to the same group should have the same color by
    passing our prediction target `y` to argument `c`.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: 'This code cell will produce the following scatter plot:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.28: Scatter plot for a machine learning problem'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15968_03_28.jpg)'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.28: Scatter plot for a machine learning problem'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: The most common strategy of a model selection process is to first split our
    data into a training dataset and a test/validation dataset. The training dataset
    is used to train the machine learning models we'd like to use, and the test dataset
    is used to validate the performance of those models.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'The `train_test_split()` function from the `sklearn.model_selection` package
    facilitates the process of splitting our dataset into the training and test datasets.
    In the next code cell, enter the following code:'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'As we can see, this function returns a tuple of four objects, which we are
    assigning to the four preceding variables: `X_train` contains the data in the
    independent features for the training dataset, while `X_test` contains the data
    of the same features for the test dataset, and the equivalent goes for `y_train`
    and `y_test`.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: 'We can inspect how the split was done by considering the shape of our training dataset:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: By default, the training dataset is randomly selected from 75 percent of the
    input data, and the test dataset is the remaining data, randomly shuffled. This
    is demonstrated by the preceding output, where we have 7,500 entries in our training
    dataset from the original data with 10,000 entries.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next code cell, we will initialize the machine learning models that
    we have imported without specifying any hyperparameters (more on this later):'
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Next, we will loop through each of them, train them on our training dataset,
    and finally compute their accuracy on the test dataset using the `accuracy_score`
    function, which compares the values stored in `y_test` and the predictions generated
    by our models in `y_pred`:'
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Again, the `fit()` method is used to train each model on `X_train` and `y_train`,
    while `predict()` is used to have the models make predictions on `X_test`. This
    will produce an output similar to the following:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: From here, we see that the `SVC` model performed the best, which is somewhat
    expected as it is the most complex model out of the three used. In an actual model
    selection process, you might incorporate more tasks, such as cross-validation,
    to ensure that the model you select in the end is the best option.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: And that is the end of our model selection exercise. Through the exercise, we
    have familiarized ourselves with the general procedure of working with a scikit-learn
    model. As we have seen, the fit/predict API is consistent across all models implemented
    in the library, which leads to a high level of flexibility and convenience for
    Python programmers.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: This exercise also concludes the general topic of inferential statistics.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2BowiBI](https://packt.live/2BowiBI).
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3dQdZ5h](https://packt.live/3dQdZ5h).
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final section of this chapter, we will iterate a number of other
    libraries that can support various specific statistical procedures.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: Python's Other Statistics Tools
  id: totrans-468
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we considered Python''s three main libraries, which
    make up the majority of a common data science/scientific computing pipeline: NumPy
    for multi-dimensional matrix computation, pandas for tabular data manipulation,
    and Matplotlib for data visualization.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: Along the way, we have also discussed a number of supporting tools that complement
    those three libraries well; they are seaborn for the implementation of complex
    visualizations, SciPy for statistical and scientific computing capability, and
    scikit-learn for advanced data analysis needs.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: Needless to say, there are also other tools and libraries that, even though
    they did not fit into our discussions well, offer unique and powerful capabilities
    for particular tasks in scientific computing. In this section, we will briefly
    consider some of them so that we can gain a comprehensive understanding of what
    Python tools are available for which specific tasks.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: 'These tools include:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '**statsmodels**: This library was originally part of SciPy''s overarching ecosystem
    but ultimately split off into its own project. The library offers a wide range
    of statistical tests and analysis techniques, models, and plotting functionalities,
    all grouped into one comprehensive tool with a consistent API, including time-series
    analysis capabilities, which its predecessor SciPy somewhat lacks.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main website for statsmodels can be found here: [http://www.statsmodels.org/stable/index.html](http://www.statsmodels.org/stable/index.html).'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '**PyMC3**: In a subfield of statistics called Bayesian statistics, there are
    many unique concepts and procedures that can offer powerful capabilities in modeling
    and making predictions that are not well supported by the libraries that we have considered.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In PyMC3, Bayesian statistical modeling and probabilistic programming techniques
    are implemented to make up its own ecosystem with plotting, testing, and diagnostic
    capabilities, making it arguably the most popular probabilistic programming tool,
    not just for Python users but for all scientific computing engineers.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: More information on how to get started with PyMC3 can be found on its home page,
    at [https://docs.pymc.io/](https://docs.pymc.io/).
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: '**SymPy**: Moving away from statistics and machine learning, if you are looking
    for a Python library that supports symbolic mathematics, SymPy is most likely
    your best bet. The library covers a wide range of core mathematical subfields
    such as algebra, calculus, discrete math, geometry, and physics-related applications.
    SymPy is also known to have quite a simple API and extensible source code, making
    it a popular choice for users looking for a symbolic math library in Python.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can learn more about SymPy from its website at [https://www.sympy.org/en/index.html](https://www.sympy.org/en/index.html).
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '**Bokeh**: Our last entry on this list is a visualization library. Unlike Matplotlib
    or seaborn, Bokeh is a visualization tool specifically designed for interactivity
    and web browsing. Bokeh is typically the go-to tool for visualization engineers
    who need to process a large amount of data in Python but would like to generate
    interactive reports/dashboards as web applications.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To read the official documentation and see the gallery of some of its examples,
    you can visit the main website at [https://docs.bokeh.org/en/latest/index.html](https://docs.bokeh.org/en/latest/index.html).
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: These libraries offer great support to their respective subfields of statistics
    and mathematics. Again, it is also always possible to find other tools that fit
    your specific needs. One of the biggest advantages of using a programming language
    as popular as Python is the fact that many developers are working to develop new
    tools and libraries every day for all purposes and needs. The libraries we have
    discussed so far will help us achieve most of the basic tasks in statistical computing
    and modeling, and from there we can incorporate other more advanced tools to extend
    our projects further.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: Before we close out this chapter, we will go through an activity as a way to
    reinforce some of the important concepts that we have learned so far.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3.01: Revisiting the Communities and Crimes Dataset'
  id: totrans-484
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, we will once again consider the *Communities and Crimes*
    dataset that we analyzed in the previous chapter. This time, we will apply the
    concepts we have learned in this chapter to gain additional insights from this
    dataset:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: In the same directory that you stored the dataset in, create a new Jupyter notebook.
    Alternatively, you can download the dataset again at [https://packt.live/2CWXPdD](https://packt.live/2CWXPdD).
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the first code cell, import the libraries that we will be using: `numpy`,
    `pandas`, `matplotlib`, and `seaborn`.'
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we did in the previous chapter, read in the dataset and print out its first
    five rows.
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace every `'?'` character with a `nan` object from NumPy.
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Focus on the following columns: `''population''` (which includes the total
    population count of a given region), `''agePct12t21''`, `''agePct12t29''`, `''agePct16t24''`,
    and `''agePct65up''`, each of which includes the percentage of different age groups
    in that population.'
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the code that creates new columns in our dataset that contain the actual
    number of people in these age groups. These should be the product of the data
    in the column `'population'` and each of the age percentage columns.
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `groupby()` method from pandas to compute the total number of people
    in different age groups for each state.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call the `describe()` method on our dataset to print out its various descriptive statistics.
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Focus on the `'burglPerPop'`, `'larcPerPop'`, `'autoTheftPerPop'`, `'arsonsPerPop'`,
    and `'nonViolPerPop'` columns, each of which describes the number of various crimes
    (burglary, larceny, auto theft, arson, and non-violent crimes) committed per 100,000
    people.
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the distribution of the data in each of these columns in a boxplot
    while having all the boxplots in a single visualization. From the plot, identify
    which type of crime out of the five is the most common and which is the least
    common.
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Focus on the `'PctPopUnderPov'`, `'PctLess9thGrade'`, `'PctUnemployed'`, `'ViolentCrimesPerPop'`,
    and `'nonViolPerPop'` columns. The first three describe the percentage of the
    population in a given region that falls into the corresponding categories (percentages
    of people living under the poverty level, over 25 years old with less than a ninth-grade
    education, and in the labor force but unemployed). The last two give us the number
    of violent and non-violent crimes per 100,000 people.
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the appropriate statistical object and visualize it accordingly to answer
    this question. Identify the pair of columns that correlate with each other the most.
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 659.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-500
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter formalized various introductory concepts in statistics and machine
    learning, including different types of data (categorical, numerical, and ordinal),
    and the different sub-categories of statistics (descriptive statistics and inferential
    statistics). During our discussions, we also introduced relevant Python libraries
    and tools that can help facilitate procedures corresponding to the topics covered.
    Finally, we briefly touched on a number of other Python libraries, such as statsmodels,
    PyMC3, and Bokeh, that can serve more complex and advanced purposes in statistics
    and data analysis.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will begin a new part of the book looking at mathematics-heavy
    topics such as sequences, vectors, complex numbers, and matrices. Specifically,
    in the next chapter, we will take a deep dive into functions and algebraic equations.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: PSQ66
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: WRC42
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
