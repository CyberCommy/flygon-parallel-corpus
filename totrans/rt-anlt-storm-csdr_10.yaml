- en: Chapter 10. Advance Concepts in Storm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a Trident topology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Trident API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples and illustrations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will learn about transactional topologies and the Trident
    API. We will also explore the aspects of micro-batching and its implementation
    in Storm topology.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Trident topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Trident gives a batching edge to the Storm computation. It lets developers use
    the abstracted layer for computations over the Storm framework, giving the advantage
    of stateful processing with high throughput for distributed queries.
  prefs: []
  type: TYPE_NORMAL
- en: Well the architecture of Trident is the same as Storm; it's built on top of
    Storm to abstract a layer that adds the functionality of micro-batching and execution
    of SQL-like functions on top of Storm.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of analogy, one can say that Trident is a lot like Pig for batch
    processing in terms of concept. It has support for joins, aggregates, grouping,
    filters, functions, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Trident has basic batch processing features such as consistent processing and
    execution of process logic over the tuples exactly once.
  prefs: []
  type: TYPE_NORMAL
- en: Now to understand Trident and its working; let's look at a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example we have picked up would achieve the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Word count over the stream of sentences (a standard Storm word count kind of
    topology)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A query implementation to get the sum of counts for a set of listed words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for dissection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have made sure about continuous input stream let''s look at the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This metadata information is stored for small batches wherein the batch size
    is a variant based on the speed of incoming tuples; it could be few hundred to
    millions of tuples based on the event **transactions per second** (**tps**).
  prefs: []
  type: TYPE_NORMAL
- en: Now my spout reads and emits the stream into the field labeled as `sentence`.
    In the next line, we will split the sentence into words; that's the very same
    functionality that we deployed in our earlier reference to the `wordCount` topology.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code context capturing the working of the `split` functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, we created a DRPC stream using `myTridentTopology`
    and over and above it, we have a function named `word`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We split the argument stream into its constituent words; for example, my argument,
    `storm trident topology`, is split into individual words such as `storm`, `trident`,
    and `topology`*   Then the incoming stream is grouped by `word`*   Next, the state-query-operator
    is used to query the Trident-state-object that was generated by the first part
    of the topology:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State query takes in the word counts computed by an earlier section of the topology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It then executes the function as specified as part of the DRPC request to query
    the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, my topology is executing the `MapGet` function on the query to
    get the count of each word; the DRPC stream, in our case, is grouped in exactly
    the same manner as the `TridentState` in the preceding section of the topology.
    This arrangement guarantees that all my word count queries for each word are directed
    to the same Trident state partition of the `TridentState` object that would manage
    the updates for the word.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FilterNull` ensures that the words that don''t have a count are filtered out*   The
    sum aggregator then sums all the counts to get the results, which are automatically
    returned back to the awaiting client'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having understood the execution as per the developer-written code, let's take
    a look at what's boilerplate to Trident and what happens automatically behind
    the scenes when this framework executes.
  prefs: []
  type: TYPE_NORMAL
- en: We have two operations in our Trident word count topology that read from or
    write to state—`persistentAggregate` and `stateQuery`. Trident employs the capability
    to batch these operations automatically to that state. So for instance, the current
    processing requires 10 reads and writes to the database; Trident would automatically
    batch them together as one read and one write. This gets you performance and ease
    of computation where the optimization is handled by the framework.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trident aggregators are other highly efficient and optimized components of the
    framework. They don't work by the rule to transfer all the tuples to one machine
    and then aggregate, instead they optimize the computation by executing partial
    aggregations wherever possible and then transfer the results over the network,
    thus saving on network latency. The approach employed here is similar to combiners
    of the MapReduce world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Trident API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Trident API supports five broad categories of operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Operations for manipulations of partitioning local data without network transfer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operations related to the repartitioning of the stream (involves the transfer
    of stream data over the network)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data aggregation over the stream (this operation do the network transfer as
    a part of operation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping over a field in the stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merge and join
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local partition manipulation operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, these operations are locally operative over the batch
    on each node and no network traffic is involved for it. The following functions
    fall under this category.
  prefs: []
  type: TYPE_NORMAL
- en: Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This operation takes single input value and emits zero or more tuples as the
    output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of these function operations is appended to the end of the original
    tuple and emitted to the stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In cases where the function is such that no output tuple is emitted, the framework
    filters the input tuple too, while in other cases the input tuple is duplicated
    for each of the output tuples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s illustrate how this works with an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the next assumption, the input stream in the variable called `myTridentStream`
    has the following fields `["a", "b", "c" ]` and the tuples on the stream are depicted
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output expected here is as per the function it should return `["a", "b",
    "c", "d"]`, so for the preceding tuples in the stream I would get the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Filters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Filters are no misnomers; their execution is exactly the same as their name
    suggests: they help us decide whether or not we have to keep a tuple or not—they
    do exactly what filters do, that is, remove what is not required as per a given
    criteria.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the following snippet to see a working illustration of
    filter functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the sample tuples on the input stream with the fields as `[
    "a" , "b" , "c"]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We execute or call the function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: partitionAggregate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `partitionAggregate` function on each of the partitions over a set of tuples
    clubbed together as a batch. There is a behavioral difference between this function;
    compared to local functions that we have executed so far, this one emits a single
    output tuple for the stream on input tuples.
  prefs: []
  type: TYPE_NORMAL
- en: The following are other functions that can be used for various aggregates that
    can be executed over this framework.
  prefs: []
  type: TYPE_NORMAL
- en: Sum aggregate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here is how the call is made to the sum aggregator function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assume the input stream has the `["a", "b"]` fields, and the following
    are the tuples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: CombinerAggregator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The implementation of this interface provided by the Trident API returns a single
    tuple with a single field as an output; internally, it executes an init function
    on each input tuple and then after that it combines the values until only one
    value is left, which is returned as an output. If the combiner functions encounter
    a partition that doesn't have any value, "0" is emitted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the interface definition and its contracts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the implementation for the count functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The biggest advantage these `CombinerAggregators` functions have over the `partitionAggregate`
    function is that it's a more efficient and optimized approach as it proceeds by
    performing partial aggregations before the transfer of results over the network.
  prefs: []
  type: TYPE_NORMAL
- en: ReducerAggregator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As the name suggests, this function produces an `init` value and then iterates
    over every tuple in the input stream to produce an output comprising of a single
    field and a single tuple.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the interface contract for the `ReducerAggregate` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the implementation of this interface for count functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Aggregator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An `Aggregator` function is the most commonly used and versatile aggregator
    function. It has the ability to emit one or more tuples, and each can have any
    number of fields. They have the following interface signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The execution pattern is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `init` method is a predecessor to processing of every batch. It's called
    before the processing of each batch. On completion, it returns an object holding
    the state representation of the batch, and this is passed on to the subsequent
    aggregate and complete methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike the `init` method, the `aggregate` method is called once for every tuple
    in the batch partition. This method can store the state, and can emit the results
    depending upon functionality requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complete method is like a postprocessor; it's executed at the end, when
    the batch partition has been completely processed by the aggregate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the implementation of the count as an aggregator function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Numerous times we run into implementations requiring multiple aggregators to
    be executing simultaneously. In such cases, the concept of chaining comes in handy.
    Thanks to this functionality in the Trident API, we can build an execution chain
    of aggregators to be executed over batches of incoming stream tuples. Here is
    an example of these kinds of chains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The execution of this chain would run the specified `sum` and `count` aggregator
    functions on each partition. The output would be a single tuple, with two fields
    holding the values of `sum` and `count`.
  prefs: []
  type: TYPE_NORMAL
- en: Operations related to stream repartitioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, these stream repartitioning operations are related to
    the execution of functions to change the tuple partitions across the tasks. These
    operations involve network traffic and the results redistribute the stream, and
    can result in changes to an overall partitioning strategy thus impacting a number
    of partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the repartitioning functions provided by the Trident API:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Shuffle`: This executes a rebalance kind of functionality and it employs a
    random round robin algorithm for an even redistribution of tuples across the partitions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Broadcast`: This does what the name suggests; it broadcasts and transmits
    each tuple to every target partition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`partitionBy`: This function works on hashing and mod on a set of specified
    fields so that the same fields are always moved to the same partitions. As an
    analogy, one can assume that the functioning of this is similar to the fields
    grouping that we learned about initially in Storm groupings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global`: This is identical to the global grouping of streams in a Storm, and
    in this case, the same partition is chosen for all the batches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batchGlobal`: All tuples in a batch are sent to the same partition (so they
    kind of stick together), but different batches can be delivered to different partitions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data aggregations over the streams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Storm''s Trident framework provides two kinds of operations for performing
    aggregations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`aggregate`: We have covered this in an earlier section, and it works in isolated
    partitions without involving network traffic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`persistentAggregate`: This performs aggregate across partitions, but the difference
    is that it stores the results in a source of state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping over a field in a stream
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Grouping operations work in analogy to group by the operations in a relational
    model with the only differential being that the ones in the Storm framework execute
    over a stream of tuples from the input source.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand this more closely with the help of the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Grouping over a field in a stream](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: These operations in the Storm Trident run over a stream of tuples of several
    different partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Merge and join
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The merges and joins APIs provide interfaces for merging and joining various
    streams together. This is possible using a variety of ways provided as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Merge`: As the name suggests, `merge` merges two or more streams together
    and emits the merged stream as the output field of the first stream:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`Join`: This operation works as the traditional SQL `join` function, but with
    the difference that it applies to small batches instead of entire infinite streams
    coming out of the spout'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, consider a join function where Stream 1 has fields such as `["key",
    "val1", "val2"]` and Stream 2 has `["x", "val1"]`, and from these functions we
    execute the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As a result, Stream 1 and Stream 2 would be joined using `key` and `x`, wherein
    `key` would join the field for Stream 1 and `x` would join the field for Stream
    2.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output tuples emitted from the join would have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The list of all the join fields; in our case, it would be `key` from Stream
    1 and `x` from Stream 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of all the fields that are not join fields from all the streams involved
    in the join operation in the same order as they are passed to the `join` operation.
    In our case, it's `a` and `b` respectively for `val1` and `val2` of Stream 1,
    and `c` for `val1` from Stream 2 (note that this step also removes the ambiguity
    of field names if any ambiguity is present within the stream, in our case `val1`
    field was ambiguous between both the streams).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When operations like join happen on streams that are being fed in the topology
    from different spouts, the framework ensures that the spouts are synchronized
    with respect to batch emission, so that every join computation can include tuples
    from a batch of each spout.
  prefs: []
  type: TYPE_NORMAL
- en: Examples and illustrations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the other out-of-the-box and popular implementations of Trident is reach
    topology, which is a pure DRPC topology that finds the reach of a URL on demand.
    Let's first understand some of the jargon before we delve deeper.
  prefs: []
  type: TYPE_NORMAL
- en: Reach is basically a sum total of the count of Twitter users exposed to a URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reach computation is a multistep process that can be attained by the following
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Get all the users who have ever tweeted a URL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fetch the follower tree of each of these users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assemble the huge follower sets fetched previously
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Count the set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well, looking at the skeletal algorithm entailed previously, you can make out
    that it is beyond the capability of a single machine and we'd need a distributed
    compute engine to achieve it. It's an ideal candidate of the Storm Trident framework,
    as you have the capability to execute highly parallel computations at each step
    across the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Our Trident reach topology would be sucking data from two large data banks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bank A is the URL to the originator bank, wherein all the URLs would be stored
    along with the name of the user who had tweeted them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bank B is the user follower bank; this data bank will have a user to follow
    the mapping for all Twitter users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The topology would be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding topology, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a `TridentState` object for both data banks (URL to the originator Bank
    A and users to follow Bank B).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `newStaticState` method is used for the instantiation of state objects for
    data banks; we have the capability to run the DRPC queries over the source states
    created earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In execution, when the reach of a URL is to be computed, we perform a query
    using the Trident state for data bank A to fetch the list of all the users who
    have ever tweeted with this URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `ExpandList` function creates and emits one tuple for each of the tweeters
    of the URL in query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we fetch the follower of each tweeter fetched previously. This step needs
    the highest degree of parallelism, thus we use shuffle grouping here for even
    load distribution across all instances of the bolt. In our reach topology, this
    is the most intense compute step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we have the list of followers of the tweeter of the URL, we execute an
    operation analog to filter unique followers only.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We arrive at unique followers by grouping them together and then using the `one`
    aggregator. The latter simply emits `1` for each group and in the next step all
    these are counted together to arrive at the reach.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we count the followers (unique) thus arriving at the reach of the URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Quiz time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Q.1\. State whether the following statements are true or false:'
  prefs: []
  type: TYPE_NORMAL
- en: DRPC is a stateless, Storm processing mechanism.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If a tuple fails to execute in a Trident topology, the entire batch is replayed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Trident lets the user implement windowing functions over streaming data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggregators are more efficient then partitioned Aggregators.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Q.2\. Fill in the blanks:'
  prefs: []
  type: TYPE_NORMAL
- en: _______________ is the distributed version of RPC.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: _______________ is the basic micro-batching framework over Storm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ___________________functions are used to remove tuples based on certain
    criteria or conditions from the stream batches.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Q.3\. Create a Trident topology to find the tweeters who have the maximum number
    of tweets in the last 5 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have pretty much covered everything about Storm and its
    advanced concepts with giving you the change to get hands-on with the Trident
    and DRPC topologies. You learned about Trident and its need and application, the
    DRPC topologies, and the various functions available in the Trident API.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore other technology components that go hand
    in hand with Storm and are necessary for building end-to-end solutions with Storm.
    We will touch upon areas of distributed caches and **Complex Event Processing**
    (**CEP**) with memcache and Esper in conjunction with Storm.
  prefs: []
  type: TYPE_NORMAL
