- en: Integration Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have reviewed how to run your code and describe your services in
    Kubernetes. We also looked into how to utilize additional tools to get information
    about how your code is running on each pod and on aggregate. This chapter builds
    on this to look at how to use Kubernetes to validate your code, with examples
    of different ways to accomplish that testing, and suggestions for how to leverage
    Kubernetes for validation testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Topics for this chapter include:'
  prefs: []
  type: TYPE_NORMAL
- en: Testing strategies using Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple validation with Bats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example – testing code with Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example – testing code with Node.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous integration with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing strategies using Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a wide variety of kinds of tests that are used during development
    and validation, in software engineering. Across this taxonomy, there are some
    kinds of tests that utilize the strengths of Kubernetes extremely well. The terms
    associated with testing can be vague and confusing, so for clarity, we will briefly
    review the terms I''ll use and the differences between these kinds of tests. There
    are many more variants of these themes that I haven''t detailed here, but for
    the purpose of describing where Kubernetes is most effective, this list is sufficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unit tests**: Unit tests are the *lowest level* of testing; this focuses
    on the interfaces, implementations, and modules within your application. Unit
    testing frequently implies isolated testing of just the component that the test
    focuses on. These tests are generally intended to be very fast, easily run on
    a developer''s system directly, and often with no access to external services
    upon which the relevant code may depend. These tests often do not deal with state
    or persistence, and primarily focus on business logic and interface validation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Functional tests**: Functional tests are the next step up from unit tests,
    implying that code the libraries are used against their underlying systems without
    fakes, mocks, or other layers that would otherwise pretend to act like the remote
    dependencies. These kinds of tests are usually applied to a subset of a service,
    testing and validating a full service, and using immediate dependencies (often
    a database or persistence store). Functional tests often imply a validation of
    state in persistence stores, and how that changes with the operation of your code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration tests**: Integration tests assemble all the required parts of
    your software together and validate the individual components as well as the components,
    working together and their interactions. The state of the system is often defined
    or set as a key setup in integration tests, and because state is represented and
    validated within the system, tests have a tendency to be ordered and more linear,
    often using composed interactions to validate the code and how it works (and how
    it fails).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a blurry line between functional tests and integration tests, with
    the former typically being focused on a subset of your overall service being validated,
    and the latter representing either large portions or your service or the whole
    system at once.
  prefs: []
  type: TYPE_NORMAL
- en: '**End-to-end tests**: Where integration tests can mean testing a portion of
    a system, end-to-end tests are specific to meaning testing and validating the
    whole of a system and all its dependencies. Frequently, end-to-end tests and integration
    tests are used synonymously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance tests**: Where the previous terms focused on the scope of validation
    between code and any underlying dependencies it interacts with, performance tests
    focus on the type of validation more than the scope. These tests intend to measure
    the efficiency or utilization of code and services; how much CPU and memory they
    utilize, and how fast they respond with a given set of underlying resources. Rather
    than focusing on the correctness of code, they focus on validation of scale and
    underlying service needs. Performance tests frequently require dependent systems
    to be not only operational, but fully resourced to provide accurate results, and
    run with some measure of expectation isolation so that external resource constraints
    don''t artificially limit the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactive/exploratory tests**: Interactive tests, sometimes also known
    as exploratory tests, are again not so much a term about scope as much as implying
    an intent. These kinds of tests typically require at least a portion of the system
    operational, and often imply the whole system is operational if not scaled to
    support high levels of requests. These tests focus on letting people interact
    with the system without any predefined or structured stream of events, and this
    same setup is often used for acceptance validation or refinement of other kinds
    of tests as a means of validating the tests themselves.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing resources needed for testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we walk through this taxonomy of tests, the compute resources and time needed
    to run the tests typically grow and become more significant. Depending on the
    scope of the software being developed, it is quite possible to require more resources
    than can be accommodated in a single machine. And where lower-level tests can
    often be optimized to leverage all possible resources within a computer, the serialized
    nature of end-to-end tests tends to be less efficient and more time consuming
    in the validation process.
  prefs: []
  type: TYPE_NORMAL
- en: As you establish your tests, you need to be aware of the size of the compute
    resources that are needed to validate your software. This can correspond to the
    process of determining how much memory and CPU a given pod needs, and extends
    to needing to be aware of all the resources for all the dependencies based on
    what you are testing and wanting to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: Where we have been using Minikube for most of these examples, modern development
    and dependencies can easily exceed the amount of resources you can give to Minikube
    for its single-node cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Using Kubernetes in your testing is most effective where you want to set up
    and use large portions of your environment that correspond to integration testing
    and testing scenarios that expect a complete system with all dependencies operational.
  prefs: []
  type: TYPE_NORMAL
- en: You can, of course, use Kubernetes in the process of development where you run
    tests such as unit tests, or perhaps functional tests, within Kubernetes, although
    you may find far more benefit from leveraging Kubernetes when you are focusing
    more at integration, end-to-end, and those later portions of the taxonomy outlined
    previously.
  prefs: []
  type: TYPE_NORMAL
- en: Because Kubernetes excels at describing a desired state for services and keeping
    them running, it can be very effectively used where you want to set up large portions
    or many services interacting together. Additionally, where you expect tests to
    take more time and resources, Kubernetes is a good fit, as it requires you to
    lock down the code into discrete, versioned containers, which can also take significant
    time and processing.
  prefs: []
  type: TYPE_NORMAL
- en: Patterns of using Kubernetes with testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are quite a number of ways to use Kubernetes for testing, and one of the
    first things you need to determine is where you're running the system being tested,
    and where you are running the tests that will validate that system.
  prefs: []
  type: TYPE_NORMAL
- en: Tests local and system-under-test in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most common pattern, especially while developing tests, is to run tests
    from your development machine against your code running within Kubernetes. After
    you create the tests, the same pattern can be employed to run your tests against
    a Kubernetes cluster housing your code from a continuous integration service.
    When you are starting out with development, you may be able to run all of this
    on your local development machine, with Minikube. In general, this pattern is
    an excellent way to get started, and solves the problem of getting feedback from
    the tests by running the tests where you want to get the feedback—either on your
    own development system, or from a CI system acting on your behalf.
  prefs: []
  type: TYPE_NORMAL
- en: Tests local and system-under-test in Kubernetes namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If, or perhaps when, the system you are testing exceeds what Minikube can support,
    the common solution is to start using a remote cluster, either managed by you,
    your IT team, or a cloud provider. As you start to use remote compute to do this,
    sharing and isolation become important, particularly with tests that are dependent
    on the state of the system, where the control of that state is critical to understanding
    if the validation is correct or not. Kubernetes has good isolation in general,
    and taking advantage of how namespaces work can make the setup of your code and
    testing it significantly easier. You can leverage namespaces by running the related
    pods and services all in a single namespace, and consistently referencing between
    them by taking advantage of the short DNS names for each service. This can be
    visualized as a stack, where you can deploy many of these effectively in parallel. In
    the earlier pattern, and in most of the examples in this book, we have used the
    default namespace, but all of the commands can include a namespace as an option
    by simply adding on `-n <namespace>` to the kubectl commands.
  prefs: []
  type: TYPE_NORMAL
- en: Namespaces support quotas for a variety of resources, and you will want to see
    what is defined and validate that you have sufficient allowances set up. Especially
    in shared environments, using Quota to put a cap on consumption is common.
  prefs: []
  type: TYPE_NORMAL
- en: Tests in Kubernetes and system-under-test in Kubernetes namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A variation of the theme is to also package and run your tests within Kubernetes—either
    in the same namespace, or in a separate namespace from your system under test.
    This is slower than running the tests locally, as it requires you to package your
    tests into a container, just as you do with your code. The trade-off is having
    a very consistent means of running those tests and interacting with the system-under-test.
  prefs: []
  type: TYPE_NORMAL
- en: If you work in a very diverse development environment where everyone has a slightly
    different setup, then this pattern can consolidate the testing so that everyone
    has the same experience. Additionally, where local tests need to access the remote
    Kubernetes through exposed services (such as NodePort using Minikube, or perhaps
    `LoadBalancer` on a provider), you can simplify that access by using the service
    names, either within the same namespace or using the longer service names with
    the namespace included.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge with running tests within Kubernetes is getting the results.
    While it is quite possible to collect the results and post them to a remote location,
    this pattern is not common. A more frequent resolution when using this pattern
    is to have a cluster dedicated to testing that also includes some continuous integration
    infrastructure, either as a part of the cluster, or alongside and with dedicated
    access to the cluster, that then runs the tests and captures the results as a
    part of the test automation. We will look at continuous integration in more depth
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Simple validation with Bats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A fairly common desire is to simply get everything deployed and make a few queries
    to validate that the resulting system is operational. As you do these operations,
    they are frequently captured in either Makefiles or shell scripts as simple programs
    to validate a baseline of functionality. Several years ago, a system called Bats,
    which stands for Bash Automated Testing System, was developed to make it slightly
    more convenient to run tests using shell scripts.
  prefs: []
  type: TYPE_NORMAL
- en: There are several examples of using Bats to test systems deployed in Kubernetes.
    The tests are generally straightforward and easy to read, and it is easy to extend
    and use. You can find more information on Bats at its GitHub home [https://github.com/sstephenson/bats](https://github.com/sstephenson/bats).
    You may see Bats used in some Kubernetes-related projects as well, for simple
    validation.
  prefs: []
  type: TYPE_NORMAL
- en: Bitnami has set up an example GitHub repository to use as a starting point that
    uses Bats and Minikube, and was designed to also work with external CI systems
    such as Travis.CI. You can find the example at [https://github.com/bitnami/kubernetes-travis](https://github.com/bitnami/kubernetes-travis).
  prefs: []
  type: TYPE_NORMAL
- en: If you leverage Bats, you will want to have helper scripts to set up your deployments
    and wait until the relevant deployments are reporting ready, or fail the tests
    at setup. In the Bitnami example, the scripts `cluster_common.bash` and `libtest.bash`
    have these helper functions. If you want to use this path, you can start with
    the files from their repository and update and extend them to match your needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The integration tests start by loading the libraries and creating a local cluster,
    followed by deploying the system under test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`deploy-stacks.bats` can be represented as a Bats test, and in the Bitnami
    example it validates that Kubernetes tools are all defined locally and then encapsulates
    the deployment itself as a test:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is from the example at [https://github.com/bitnami/kubernetes-travis/blob/master/tests/deploy-stack.bats](https://github.com/bitnami/kubernetes-travis/blob/master/tests/deploy-stack.bats):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The script `deploy.sh` is set up to either delete or create and load manifests
    just as we have been doing earlier in this book, by using the `kubectl create`,
    `kubectl delete`, or `kubectl apply` commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once that is complete, the integration test continues with getting access to
    the cluster. In the Bitnami example, they are using Kubernetes Ingress to consistently
    access the cluster, and have scripts set up to capture and return the IP address
    and URL path for accessing the underlying system through `Ingress`. You could
    also utilize `kubectl port-forward` or `kubectl proxy`, as we showed earlier in
    the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once that is set, then the integration tests are invoked again using Bats,
    and the exit code from that whole process is captured and used to reflect whether
    the tests succeeded or failed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: While this is easy to get started with, programming in bash quickly becomes
    its own speciality, and while basic bash usage is frequent and easily understood,
    some of the more complex helpers within that example can take some digging to
    fully understand.
  prefs: []
  type: TYPE_NORMAL
- en: If you are having trouble with a shell script, then a common debugging solution
    is to add `set -x` near the top of the script. Within bash, this turns on a command
    echo, so that all the commands within the script are echoed to standard out so
    that you can see what is happening.
  prefs: []
  type: TYPE_NORMAL
- en: A good pattern to follow is to write tests in a language that you are familiar
    with. You can frequently leverage the testing frameworks of those languages to
    help you. You still may want to use shell scripts like the Bitnami example to
    set up and deploy your code to your cluster, and use the logic and structure of
    a language you are more comfortable with for the tests.
  prefs: []
  type: TYPE_NORMAL
- en: Example – integration testing with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the case of Python, the example code here uses PyTest as a test framework.
    The example code can be found on GitHub, in the 0.7.0 branch of the repository
    [https://github.com/kubernetes-for-developers/kfd-flask/](https://github.com/kubernetes-for-developers/kfd-flask/).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the example using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the example, I changed the code structure to move all the Python code for
    the application itself under the `src` directory, following the recommended pattern
    from PyTest. If you have not used PyTest before, reviewing their best practices
    at [https://docs.pytest.org/en/latest/goodpractices.html](https://docs.pytest.org/en/latest/goodpractices.html)
    is very worthwhile.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you view the code or download it, you will also notice a new file, `test-dependencies.txt`,
    which defines a number of dependencies specific to testing. Python does not have
    a manifest where they separate out dependencies for production from ones used
    during development or testing, so I separated the dependencies out myself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The actual integration tests are housed under the directory `e2e_tests`, primarily
    as a pattern that would let you have a local directory for any unit or functional
    tests you wanted to create during normal development.
  prefs: []
  type: TYPE_NORMAL
- en: The pattern that I'm using in this example is leveraging our code in Kubernetes
    and accessing it external to the cluster, leveraging Minikube. The same pattern
    can work nicely with a cluster hosted in AWS, Google, or Azure if your environment
    requires more resources than you have available on your local development machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `README` file in `e2e_tests` shows an example of how to run the tests.
    I take advantage of `pip` and `virtualenv` to set up a local environment, install
    the dependencies, and then use PyTest to run the tests directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run through these tests, you should see output akin to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'PyTest includes a large number of plugins, including a means to export the
    test results in JUnit XML format. You can get such a report created by PyTest
    by invoking it with the `--junitxml` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The code within these tests leverages the examples we have built so far: our
    deployment YAML and the images we have made with the code in the repository. The
    tests do a simple verification that the cluster is available and healthy (and
    that we can communicate with it), and then uses `kubectl` to deploy our code.
    It then waits until the code is deployed, with a max timeout defined, before continuing
    on to interact with the service and get a simple response.'
  prefs: []
  type: TYPE_NORMAL
- en: This example is primarily aimed at showing you how you can interact with a remote
    Kubernetes cluster, including using the `python-kubernetes` client library.
  prefs: []
  type: TYPE_NORMAL
- en: PyTest and pytest-dependency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTest is foremost a unit testing framework. Unit testing frameworks generally
    have different needs from integration tests, and fortunately PyTest has a means
    to allow a developer to specify that one test needs to run and complete before
    another. This is done with the `pytest-dependency` plugin. Within the code, you
    will see some of the test cases annotated with dependency markers. To use this
    plugin, you define which tests can be dependency targets, and for any test that
    needs to run after it, you define the tests upon which they depend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The test checks to verify that the cluster is accessible and responding as
    healthy. This test doesn''t depend on any others, so it has just the basic annotation,
    and tests further down will specify that this test needs to have completed before
    they will run, with this annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This can make the test annotations quite verbose, but allows you to explicitly
    define the ordering of execution. By default, most unit testing frameworks do
    not guarantee a specific order or execution, which can be critical when you are
    testing a system that includes state and changes to that state—exactly what we
    do with integration testing.
  prefs: []
  type: TYPE_NORMAL
- en: PyTest fixtures and the python-kubernetes client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding example also leverages a simple text fixture to provide us an
    instance of a Python Kubernetes client to interact with the cluster. The Python
    client can be awkward to use, since it is generated from an OpenAPI specification,
    and has class setups for each API endpoint, of which there are several. In particular,
    as sections of the Kubernetes API evolve through alpha, beta, and final release
    stages, these API endpoints will move, which means the client code you're using
    may need to change as you upgrade the version of the Kubernetes cluster with which
    you are interacting.
  prefs: []
  type: TYPE_NORMAL
- en: The `python-kubernetes` client does come with readily available source code
    and a generated index to all the methods, which I recommend having handy if you're
    going to use the client. The code is housed at [https://github.com/kubernetes-client/python](https://github.com/kubernetes-client/python),
    and the releases are stored in branches. The release that I was using was 5.0,
    which is paired with Kubernetes version 1.9 and supports earlier versions. The
    `README` that includes all the documentation for the OpenAPI-generated methods
    is available at [https://github.com/kubernetes-client/python/blob/release-5.0/kubernetes/README.md](https://github.com/kubernetes-client/python/blob/release-5.0/kubernetes/README.md).
  prefs: []
  type: TYPE_NORMAL
- en: 'A PyTest fixture sets up the client for the other tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the client loads the locally available `kubeconfig` for access
    to the cluster. Depending on your development environment, you may want to investigate
    alternatives to authenticating to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'While it is possible to do the deployment with the python-kubernetes client,
    the example shows how to use the local `kubectl` command line to interact with
    the cluster as well. In this case, it''s significantly fewer lines than defining
    the full definition of what you want deployed in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If you want to leverage other tools to deploy your code, this kind of mechanism
    can be invaluable, and is always a useful fallback in writing integration tests.
    Also note that this test depends on the test we mentioned earlier, forcing it
    to run after the cluster health validation test.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that debugging these commands when the system fails can be more difficult,
    because so much is happening outside of the actual test with commands like these.
    You will want to be aware of what process is invoking the test, its permissions
    relative to your environment, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: Waiting for state changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Following the deployment, we expect the deployment and services to all become
    active, but this does not happen instantaneously. Depending on your environment,
    it may happen quite quickly, or fairly slowly. The bane of integration tests is
    not being able to know when something has completed, and working around the issue
    with an invocation of `sleep()` to wait a little longer. In this example, we explicitly
    check the status rather than just waiting an arbitrary amount of time and hoping
    the system is ready to go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This example has a maximum timeout of `300` seconds for the deployment to become
    active, and includes a short delay in requesting the status of the environment
    before it will continue. Should the overall timeout be exceeded, the test will
    report a failure—and by using `pytest-dependency`, all the following tests that
    depend on this will not be run—short circuiting the testing process to report
    the failure.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last two tests highlight two ways of interacting with the code running within
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first example expects something to be set up and running that provides
    access to the cluster outside of the test, and simply uses the Python `requests`
    library to make an HTTP request directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s an incredibly basic test and is fairly fragile. It uses a PyTest fixture
    defined earlier in the code to set up an invocation of `kubectl proxy` to provide
    access to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: While this generally works, it's harder to track down issues when things fail,
    and the fixture mechanism wasn't entirely reliable in setting up (and tearing
    down) the proxy invocation in a forked shell command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second example uses the python-kubernetes client to access the service
    through a series of methods that allow you to easily invoke HTTP requests through
    the proxy that is included with Kubernetes. The client configuration takes care
    of the authentication to the cluster, and you access the code through the proxy
    by leveraging the client directly rather than using an external proxy to provide
    access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This mechanism is great if you don''t need to fiddle with headers or otherwise
    get complicated with your HTTP requests, which is more accessible when using a
    general Python client such as `requests`. There are a whole series of methods
    that support a variety of HTTP/REST style calls all prefixed with `proxy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`proxy_get`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxy_delete`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxy_head`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxy_options`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxy_patch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '``proxy_put``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of these is mapped to the following endpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '`namespaced_pod`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`namespaced_pod_with_path`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`namespaced_service`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`namespaced_service_with_path`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This gives you some flexibility in standard REST commands to send to either
    a pod directly, or to a service endpoint. The `with_path` option allows you to
    define the specific URI with which you're interacting on the pod or service.
  prefs: []
  type: TYPE_NORMAL
- en: Example – integration testing with Node.js
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Node.js example uses mocha, chai, supertest, and the JavaScript kubernetes
    client in much the same fashion as the Python example. The example code can be
    found on GitHub, in the 0.7.0 branch of the repository at [https://github.com/kubernetes-for-developers/kfd-nodejs/](https://github.com/kubernetes-for-developers/kfd-nodejs/).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the example using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'I took advantage of Node.js''s mechanism to have development dependencies separate
    from production dependencies, and added most of these dependencies into `package.json`.
    I also went ahead and set up a simple unit test directly in a `test` directory,
    and a separate integration test in an `e2e-tests` directory. I also set up the
    commands so that you can run these tests through `npm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'For the unit tests, the code runs locally and takes advantage of `supertest`
    to access everything within a JavaScript runtime on your local machine. This doesn''t
    account for any remote services or systems (such as interacting with endpoints
    that depend on Redis):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Within the `e2e_tests` directory,  there is an analog of the Python tests,
    which validates the cluster is operational, sets up the deployment, and then accesses
    that code. This can be invoked with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoking the tests would show you something akin to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Node.js tests and dependencies with mocha and chai
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The test code itself is at `e2e_tests/integration_test.js`, and I leverage mocha
    and chai to lay out the tests in a BDD-style structure. A convenient side effect
    of the BDD structure with mocha and chai is that tests can be wrapped by `describe`
    and `it`, which structure how the tests get run. Anything within a `describe`
    block doesn't have a guaranteed ordering, but you can nest `describe` blocks to
    get the structure you want.
  prefs: []
  type: TYPE_NORMAL
- en: Validating the cluster health
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The JavaScript Kubernetes client is generated in much the same fashion as the
    Python client, from the OpenAPI definition and mapped to the releases of Kubernetes.
    You can find the client at [https://github.com/kubernetes-client/javascript](https://github.com/kubernetes-client/javascript),
    although this repository doesn''t have the same level of generated documentation
    as the Python client. Instead, the developers have gone to some length to reflect
    the types in TypeScript with the client, which results in editors and IDEs being
    able to do some level of automatic code completion as you are writing your tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The nesting of the code can make indenting and tracking at the right level quite
    tricky, so the test code leverages promises where it can to simplify the callback
    structures. The preceding example uses a Kubernetes client that automatically
    grabs credentials from the environment in which it's run, a feature of several
    of these clients, so be aware of it if you wish to arrange specific access.
  prefs: []
  type: TYPE_NORMAL
- en: Where the Python client had a method, `list_component_status`, the JavaScript
    pattern scrunches the names together with CamelCase formatting, so the same call
    here is `listComponentStatus`. The result is then passed in a promise, and we
    iterate through the various elements to verify that the cluster components are
    all reporting healthy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example leaves in some commented out code that inspects the objects that
    were returned. With little external documentation, I found it convenient to see
    what was returned while developing the tests, and the common trick is to use the
    `util.inspect` function and log the results to `STDOUT`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Deploying with kubectl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Following the Python example, I used `kubectl` on the command line to deploy
    the code, invoking it from the integration test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This particular bit of code is dependent on where you have this test case and
    its relative directory to the deploy directory where the manifests are stored,
    and like the preceding example it uses promises to chain the validation of the
    execution of the invocation.
  prefs: []
  type: TYPE_NORMAL
- en: Waiting for the pods to become available
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The process of waiting and retrying was significantly more tricky with Node.js,
    promises, and callbacks. In this case, I leveraged a capability of the mocha test
    library to allow a test to be retried and manipulate the overall timeout for a
    section of the test structure to get the same end result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: By returning promises in the tests, every one of the tests is already asynchronous
    with a preset timeout that mocha provides of `20` seconds. Within each `describe`,
    you can tweak how mocha runs the tests—in this case, setting the overall timeout
    to five minutes and asserting that the test can be retried up to `30` times. To
    slow down the checking iterations, I also included a timeout promise that simply
    introduces a five-second delay before invoking the check of the cluster to get
    the pod health.
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with the deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code to interact with the deployment is simpler than the Python example,
    utilizing the Kubernetes client and the proxy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this branch, I changed the code running from a stateful set to a deployment,
    as getting proxy access to the headless endpoints proved complicated. The stateful
    sets can be easily accessed from within the cluster via DNS, but mapping to external
    didn't appear to be easily supported in the current client code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the Python code, there''s a matrix of calls to make REST style requests
    through the client:'
  prefs: []
  type: TYPE_NORMAL
- en: '`proxyGET`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxyDELETE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxyHEAD`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxyOPTIONS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxyPATCH`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxyPUT`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And each is mapped to endpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '`namespacedPod`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`namespacedPodWithPath`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`namespacedService`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`namespacedServiceWithPath`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This gives you some flexibility in standard REST commands to send to either
    a pod directly or to a service endpoint. Like the Python code, the `withPath`
    option allows you to define the specific URI with which you're interacting on
    the pod or service.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re writing these tests in an editor such as Visual Studio Code, code
    completion will help provide some of the details that are otherwise missing from
    the documentation. The following is an example of code completion showing the
    `method` options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6ccc784d-e1e6-4d88-8c5f-7a1265ae5400.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And when you choose a method, the TypeScriptannotations are also available
    to show you what options the JavaScript methods expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b9556730-dee8-4b4b-ae6a-a9b11dbd364e.png)'
  prefs: []
  type: TYPE_IMG
- en: Continuous integration with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have integration tests, getting something operational to validate those
    tests is very important. If you don't run the tests, they're effectively useless—so
    having a means of consistently invoking the tests while you're doing development
    is important. It is fairly common to see continuous integration do a lot of the
    automated lifting for development.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of options available to development teams to help you with
    continuous integration, or even its more advanced cousin, continuous deployment.
    The following tools are an overview of what was available at the time of writing,
    and in use by developers working with their code in containers and/or in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Travis.CI**: Travis.CI ([https://travis-ci.org/](https://travis-ci.org/))
    is a hosted continuous integration service, and it is quite popular as the company
    offers free service with an easy means of plugging into GitHub for public and
    open source repositories. Quite a number of open source projects leverage Travis.CI
    to do basic testing validation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Drone.IO**: Drone.IO ([https://drone.io/](https://drone.io/)) is a hosted
    or local option for continuous integration that is also open source software itself,
    hosted at [https://github.com/drone/drone](https://github.com/drone/drone). Drone
    has an extensive plugin library, including a plugin for Helm ([https://github.com/ipedrazas/drone-helm](https://github.com/ipedrazas/drone-helm)),
    which has made it attractive to some development teams who are using Helm to deploy
    their software.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gitlab**: Gitlab ([https://about.gitlab.com/](https://about.gitlab.com/))
    is an opensource source control solution that includes continuous integration.
    Like Drone, it can be leveraged in your local environment, or you can use the
    hosted version. Where the previous options were agnostic to the source control
    mechanism, Gitlab CI is tightly bound to Gitlab, effectively making it useful
    only if you''re also willing to use Gitlab.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jenkins**: Jenkins ([https://jenkins.io/](https://jenkins.io/)) is the grandaddy
    of the CI solutions, originally known as Hudson, and it is used extensively in
    a wide variety of environments. A hosted version of Jenkins is available through
    some providers, but it is primarily a opensource solution that you''re expected
    to deploy and manage yourself. It has an amazing (perhaps overwhelming) number
    of plugins and options available to it, and notably a Kubernetes plugin ([https://github.com/jenkinsci/kubernetes-plugin](https://github.com/jenkinsci/kubernetes-plugin))
    that will let a Jenkins instance run its tests within a Kubernetes cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concourse**: Concourse ([https://concourse-ci.org/](https://concourse-ci.org/)),
    like Jenkins, is an open source project rather than a hosted solution, built in
    the CloudFoundry project and focusing on pipelines for deployment as a first-class
    concept (it''s relatively new to some older projects such as Jenkins). Like Drone,
    it is set up to be a continuous delivery pipeline and an integral part of your
    development process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example – using Minikube with Travis.CI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The example earlier that showed using Bats to run a test was created by the
    team at Bitnami, and they also leveraged that same example repository for building
    and deploying code into an instance of Minikube hosted and run on Travis.CI. Their
    example repository is online at [https://github.com/bitnami/kubernetes-travis](https://github.com/bitnami/kubernetes-travis),
    and it installs Minikube as well as additional tooling to build and deploy into
    a small Kubernetes instance.
  prefs: []
  type: TYPE_NORMAL
- en: Travis.CI is configured through a `.travis.yml` file, and the documentation
    for how to configure it and what options are available is hosted online at [https://docs.travis-ci.com](https://docs.travis-ci.com).
    Travis.CI, by default, will attempt to understand what language is being used,
    and orient its build scripts to the language, focusing primarily on running builds
    for every pull request and merge into a repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Node.js example added a sample `.travis.yml` that sets up and runs the
    current integration test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The key `language`, which is set to `nodejs` in our example, defines a large
    part of how Travis will run. We define which versions of Node.js are used (`lts/*`)
    and by default the system would use `npm` , running `npm test` to validate our
    build. That would run our unit tests, but wouldn't invoke our integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: You can extend what happens before the testing, and what is used by the tests,
    by manipulating the values under the keys `before_script` and `script`. In the
    preceding example, we preload `minikube` and `kubectl` by downloading them from
    their released locations, and the preceding example then starts Minikube and waits
    until the command `kubectl get nodes` returns with a positive result.
  prefs: []
  type: TYPE_NORMAL
- en: 'By adding `npm run integration` under the key script, we override the default
    Node.js behavior and instead run our integration test. When the example was developed,
    updates were pushed to the 0.7.0 branch, which was open as a pull request to the
    main repository. The results of those updates were published into the hosted solution,
    available at [https://travis-ci.org/kubernetes-for-developers/kfd-nodejs](https://travis-ci.org/kubernetes-for-developers/kfd-nodejs).
    For example, the following is a build page showing a successful build:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cfdc6e56-9507-4163-a667-4afa57807b67.png)'
  prefs: []
  type: TYPE_IMG
- en: Next steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This example build does not do the entire process from source to container to
    deployment. Instead, it relies on a pre-built image with a tag that is set in
    the deployment manifests, managed in source control. Travis.CI does include the
    capability of building an image using Docker, and has documentation on how to
    leverage Docker to test a single container by itself at [https://docs.travis-ci.com/user/docker/](https://docs.travis-ci.com/user/docker/).
  prefs: []
  type: TYPE_NORMAL
- en: Travis also has the capability of storing credentials to build and push Docker
    images into an image repository, and recently added the capability of building
    in stages so that you can pipeline in a container build and then utilize it in
    your integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: You need to update the Kubernetes declarations to use the image in question,
    and this example doesn't show that process. The common pattern for enabling this
    sort of functionality involves templating the manifests that we have stored in
    the deploy directory in our examples, and rendering them out with specific variables
    passed in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Helm ([https://docs.helm.sh/](https://docs.helm.sh/)) is one way to accomplish
    this need: instead of having a `deploy` directory with the manifests, we might
    have a `charts` directory, and write up the manifests as templates. Helm uses
    a `values` file, which can be created as needed, to provide the variables that
    are used to render out the templates, and after creating a Docker image with a
    tag, that same tag value could be added to the `values` file and used for deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: Another option is a newer project called ksonnet ([https://ksonnet.io](https://ksonnet.io)),
    which builds on an opensource library, [http://jsonnet.org/](http://jsonnet.org/),
    to make a composable template-style language available that builds on prototypes
    for Kubernetes. ksonnet is relatively new and is still getting established. Using
    Helm, you utilize Go templates and need to have some familiarity with that format
    while creating the chart. ksonnet has its own style of writing the templates,
    and you can find a tutorial and examples at the project's site: [https://ksonnet.io/tour/welcome](https://ksonnet.io/tour/welcome).
  prefs: []
  type: TYPE_NORMAL
- en: Example – using Jenkins and the Kubernetes plugin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While not a hosted solution, Jenkins is one of the most frequently used continuous
    integration tools available. It is very simple to get an instance of Jenkins operational
    on a Kubernetes cluster, and thanks to a Kubernetes-specific plugin, it can also
    do all of its builds within a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: One of the quickest ways to install Jenkins in this fashion is to use Helm.
    The default Helm repository includes a maintained chart for running Jenkins, along
    with the configuration to use the Jenkins Kubernetes plugin. The chart that we
    will use is available on GitHub at [https://github.com/kubernetes/charts/tree/master/stable/jenkins](https://github.com/kubernetes/charts/tree/master/stable/jenkins).
    You can also get more details on the Jenkins Kubernetes plugin that gets installed
    with that chart at [https://wiki.jenkins.io/display/JENKINS/Kubernetes+Plugin](https://wiki.jenkins.io/display/JENKINS/Kubernetes+Plugin).
  prefs: []
  type: TYPE_NORMAL
- en: Installing Jenkins using Helm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, I'm going to walk through how you can set up and install Jenkins
    into a Minikube cluster on your local machine in order to experiment with it.
    You can use a very similar process to install into any Kubernetes cluster, but
    you will need to make some modifications depending on your target cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't have Helm already installed on your laptop, you can install it
    by following the instructions at the project's website: [https://docs.helm.sh/using_helm/#installing-helm](https://docs.helm.sh/using_helm/#installing-helm).
    Once you have the command-line client on your local system, you can bootstrap
    the rest.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step will be to install Helm onto your cluster and update the repositories.
    This is accomplished by running two commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be very minimal, showing something akin to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Tiller, as it mentions, is the server-side component of Helm, and it is responsible
    for coordinating the installations invoked from the `helm` command-line tool.
    By default, `helm init` will install Tiller into the `kube-system` namespace,
    so you can see it in your cluster using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Once it''s in the state `Running`, it is a good idea to get the latest repository
    index loaded. It comes with a number of charts already installed, but the charts
    do get updated fairly regularly, and this will ensure that you have the latest
    charts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The update process is usually pretty fast, returning something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The `"stable" chart repository` that it's mentioning is the one hosted on GitHub
    at the Kubernetes project: [https://github.com/kubernetes/charts](https://github.com/kubernetes/charts).
    Within that repository, there is a `stable` directory that includes all the charts.
    If you use the command `helm search`, it will display a list of charts and the
    relevant versions, which matches with the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `helm search jenkins` command will show you the target we''ll be
    using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note that charts have a chart version as well as a reported *app version*. Many
    charts wrap existing opensource projects, and the charts are maintained separately
    from the systems they deploy. Charts within the `stable` repository at the Kubernetes
    project strive to be examples of how to build charts as well as to be useful to
    the community at large. In this case, the chart version is `0.14.1`, and it is
    reported to be deploying Jenkins version `2.73`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get more details on the specific chart using the `helm inspect` command,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This will show you a large amount of output, starting with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The very top is the information that goes into the chart repository index and
    what is used to provide the results from the `helm search` commands, and the section
    after that is the configuration options that the chart supports.
  prefs: []
  type: TYPE_NORMAL
- en: Most charts strive to have and use good defaults, but expect that you might
    provide overridden values where appropriate. In the case of deploying Jenkins
    into Minikube, we will want to do just that, as the default `values.yaml` that
    the chart uses expects to use a `LoadBalancer`, which Minikube doesn't support.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the full details of `values.yaml` in the extended output of `helm
    inspect`. Before you install anything with Helm, it is a good idea to see what
    it is doing on your behalf, and what values it offers for configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a small `yaml` file to override just one of the defaults: `Master.ServiceType`.
    If you scan through the output of the `helm inspect` command, you will see a reference
    to changing this to install on Minikube.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `jenkins.yaml` file with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can see what Helm will create when we ask it to install, using the
    options `--dry-run` and `--debug` to get detailed output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Running this command will dump a lot of information to your Terminal screen,
    the rendered manifests of everything that Helm would install on your behalf. You
    can see the deployments, secrets, configmaps, and services.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start the installation process by running that exact same command,
    minus the `--dry-run` and `--debug` options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This will provide you with a list of all the Kubernetes objects that it has
    created, and then some notes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The notes that are generated are rendered as a template and generally provide
    instructions for how to access the service. You can always get this same information
    repeated to you using the `helm status` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we invoked Helm, we named this `release j` to keep it short and simple.
    To get information about the current state of this release, use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a fairly large installation and it will take a while to install. You
    can watch the events that roll out from this installation using a command such
    as `kubectl get events -w`. This will update events as the deployment progresses,
    with output looking something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Once the deployment is fully available, you can start to access it using the
    instructions in the notes.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Jenkins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The chart and images together make some secrets as the deployment is progressing
    to hold things such as the password to access Jenkins. The notes include a command
    to use to get this password from Kubernetes and display it on your Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Run that command and copy the output, as we''ll need it to log in to your instance
    of Jenkins. The next commands tell you how to get a URL to access Jenkins. You
    can use those commands to get the information and open a browser to access Jenkins.
    If you deployed this to Minikube, you can also use Minikube to open a browser
    window for the relevant service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The first page will provide you with a request for credentials. Use `admin`
    as the username and the password that you read in the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e3fafc03-9858-4270-bfa6-e33828d6519e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, logging in should provide you with administrative access to Jenkins:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1cdf14f1-5fdf-48be-ad3b-4fbf3b4cd0bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Updating Jenkins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you connect, and in the preceding example, you may see a menu item in
    red with a number. This is how Jenkins alerts you to things that you should consider
    immediately updating. I highly recommend that you click on the number and review
    what it''s presenting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/51ee8772-f058-4a34-b1ca-41d41c6928bf.png)'
  prefs: []
  type: TYPE_IMG
- en: While the charts and base images are maintained, updates or considerations that
    can't be determined in advance can become available. Plugins to Jenkins, in particular,
    can get updated, and Jenkins reviews the existing plugins for possible updates.
    You can click on the buttons on this page to run the updates, restart Jenkins,
    or learn more about its suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: The Jenkins chart includes a `persistent-volume-claim` where plugin updates
    are stored, so unless you disabled it, you can safely load updates to the Jenkins
    plugins and tell it to restart itself to have those plugin updates take effect.
  prefs: []
  type: TYPE_NORMAL
- en: Example pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the benefits to this install is that jobs you create can run pipelines
    that completely build and run within the Kubernetes cluster. Pipelines can be
    defined as something you build using the tooling within Jenkins, you can enter
    them directly, or you can load them from source control.
  prefs: []
  type: TYPE_NORMAL
- en: The example code for the Python/Flask application has a basic Jenkinsfile to
    show you how this can work. The Jenkinsfile was added to the 0.7.0 branch, and
    you can view it online at [https://github.com/kubernetes-for-developers/kfd-flask/blob/0.7.0/Jenkinsfile](https://github.com/kubernetes-for-developers/kfd-flask/blob/0.7.0/Jenkinsfile).
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline is set up to be used from source control, build a Docker image,
    and interact with Kubernetes. The example does not push the image to a repository
    or deploy the image, following the same pattern as the previous Travis.CI example.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable this example in your instance of Jenkins, you will want to navigate
    to the front page of Jenkins and select New Item. From there, select Multibranch
    Pipeline and name the job `kfd-flask-pipeline`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6f374ec3-fe39-4455-b37f-6ea631365d5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once created, the critical item to enter is the location of content from source
    control. You can enter `https://github.com/kubernetes-for-developers/kfd-flask`
    to use this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5e672215-e2bc-44c9-90e6-b3bee02439b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Save the configuration, and it should build the example, reaching out to GitHub,
    getting the pipeline, and then configuring it and running it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Loading the various images can take quite a bit of time, and once it is complete
    the results will be available in Jenkins:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bad25e3c-e8af-4fe0-a602-092d2ca7d17e.png)'
  prefs: []
  type: TYPE_IMG
- en: In the pipeline example, it checks out from source control, builds a new Docker
    image with a tag name based on the branch and `git commit`, and then later interacts
    with Kubernetes to show you the list of current pods active in the cluster in
    which its running.
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins has the same needs as our Travis.CI example, such as changing the manifests
    to run a complete sequence, and this is something you can solve by using Helm
    or perhaps ksonnet to build onto the preceding example.
  prefs: []
  type: TYPE_NORMAL
- en: Next steps with pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The full extent of what you can do with Jenkins pipelines is beyond what we
    can cover here, but the full documentation for both pipelines and the Kubernetes
    plugin additions is available online:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://jenkins.io/doc/book/pipeline/syntax/](https://jenkins.io/doc/book/pipeline/syntax/)
    provides documentation on the pipeline syntax, how to write pipelines, and what
    options are built in by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/jenkinsci/kubernetes-plugin](https://github.com/jenkinsci/kubernetes-plugin)
    offers details on what the Jenkins Kubernetes plugin does and how it operates,
    as well as including examples of how to use it with some sample pipelines at its
    GitHub repo: [https://github.com/jenkinsci/kubernetes-plugin/tree/master/examples](https://github.com/jenkinsci/kubernetes-plugin/tree/master/examples).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The general Jenkins documentation is very extensive, available at [https://jenkins.io/doc/](https://jenkins.io/doc/),
    along with more details about how to create and use a Jenkinsfile at [https://jenkins.io/doc/book/pipeline/jenkinsfile/](https://jenkins.io/doc/book/pipeline/jenkinsfile/).
    A significant benefit to using a Jenkinsfile is that the declaration for what
    your pipelines should do can be stored alongside your code in source control.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://jenkins.io/doc/pipeline/steps/credentials-binding/](https://jenkins.io/doc/pipeline/steps/credentials-binding/)
    details one way to expose secrets and credentials so that you can use them within
    pipelines, for example, to push image updates to DockerHub, Quay, or your own
    private image repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we delved into how to use Kubernetes when testing your code.
    We looked at the patterns you might explore with integration testing. We pointed
    at a simple example of using shell scripts to run integration tests within Kubernetes,
    and then dove more deeply into examples using Python and Node.js that run integration
    tests using Kubernetes. Finally, we wrapped up the chapter with an overview of
    options that are readily available for continuous integration that can use a cluster,
    and explored two options: using Travis.CI as a hosted solution and how to use
    Jenkins on your own Kubernetes cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how we can pull together multiple pieces
    that we have explored and show how to benchmark your code running on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
