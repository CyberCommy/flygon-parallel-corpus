- en: Testing Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software is the most complex thing humans create. Most programmers can't write
    10 lines of code without any errors occurring. Now, take this common knowledge
    and consider what it takes to write a distributed system made of tens, hundreds,
    or thousands of interacting components that have been designed and implemented
    by large teams using lots of third-party dependencies, lots of data-driven logic,
    and lots of configuration. Over time, many of the original architects and engineers
    that built the system might have left the organization or moved to a different
    role. Requirements change, new technologies are reintroduced, and better practices
    are discovered. The system must evolve to meet all of these changes.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom line is that you have zero chance of building a working non-trivial
    system without rigorous testing. Proper tests are the skeleton that ensures that
    your system works as expected and immediately identifies problems when you introduce
    a breaking change before it makes in into production. A microservices-based architecture
    introduces some unique challenges for testing since many of the workflows touch
    upon multiple microservices and it may be difficult to control the test conditions
    across all the relevant microservices and data stores. Kubernetes introduces its
    own testing challenges since it does so much under the covers, which takes more
    work to create predictable and repeatable tests.
  prefs: []
  type: TYPE_NORMAL
- en: We will demonstrate all these types of tests within Delinkcious. In particular,
    we will focus on local testing with Kubernetes. Then, we'll discuss the important
    issue of isolation, which allows us to run end-to-end tests without impacting
    our production environments. Finally, we'll see how to deal with data-intensive
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local testing with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End to end testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code is split between two Git repositories:'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code samples here: [https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter10](https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter10)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find the updated Delinkcious application here: [https://github.com/the-gigi/delinkcious/releases/tag/v0.8](https://github.com/the-gigi/delinkcious/releases/tag/v0.8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unit testing is the easiest type of testing to incorporate into your codebase,
    yet it brings a lot of value. When I say it's the easiest, I take it for granted
    that you can use best practices such as proper abstraction, separation of concerns,
    dependency injection, and so on. There is nothing easy about trying to test a
    spaghetti codebase!
  prefs: []
  type: TYPE_NORMAL
- en: Let's talk briefly about unit testing in Go, the Ginkgo test framework, and
    then review some unit tests in Delinkcious.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing with Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go is a modern language and recognizes the importance of testing. Go encourages
    that for each `foo.go` file you have, to have `foo_test.go`. It also provides
    the testing package, and the Go tool has a `test` command. Let's look at a simple
    example. Here is a `foo.go` file that contains the `safeDivide()` function. This
    function divides integers and returns a result and an error.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the denominator is non-zero, it returns no error, but if the denominator
    is zero, it returns a `division by zero` error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that Go division uses integer division when both operands are integers.
    This is done so that the result of dividing two integers is always the whole part
    (the fractional part is discarded). For example, 6/4 returns 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a Go unit test in a file called `foo_test.go` that tests both non-zero
    and zero denominators and uses the `testing` package. Each `test` function accepts
    a pointer to the `testing.T` object. When a test fails, it calls the `Errorf()`
    method of the `T` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to run the tests, we can use the `go test -v` command. It is part of the
    standard Go tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Nice – all the tests pass. We can also see how long it took to run the tests.
    Let''s introduce an intentional bug. Now, `safeDivide` subtracts instead of divides:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We only expect the divide by zero test to pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We got exactly what we expected.
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot more to the `testing` package. The `T` object has additional
    methods you can use. There are facilities for benchmarks and for common setups.
    However, overall, due to the ergonomics of the testing package, it's not ideal
    to have call methods on the `T` object. It can also be difficult to manage a complex
    and hierarchical set of tests using the `testing` package without additional tooling
    on top of it. This is exactly where Ginkgo comes into the picture. Let's get to
    know Ginkgo. Delinkcious uses Ginkgo for its unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing with Ginkgo and Gomega
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ginkgo ([https://github.com/onsi/ginkgo](https://github.com/onsi/ginkgo)) is
    a **b****ehavior-driven development** (**BDD**) testing framework. It still uses
    the testing package under the covers, but allows you to write tests using a much
    nicer syntax. It also pairs well with Gomega ([https://github.com/onsi/gomega](https://github.com/onsi/gomega)),
    which is an excellent assertions library. Here is what you get with Ginkgo and
    Gomega:'
  prefs: []
  type: TYPE_NORMAL
- en: Write BDD-style tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arbitrary nested blocks (`Describe`, `Context`, `When`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good setup/teardown support (`BeforeEach`, `AfterEach`, `BeforeSuite`, `AfterSuite`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on one test only or match by regex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skip tests by regex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with coverage and benchmarking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see how Delinkcious uses Ginkgo and Gomega for its unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: Delinkcious unit testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll use `LinkManager` from the `link_manager` package as an example. It
    has pretty sophisticated interactions: it allows you to manage a data store, hit
    another microservice (social graph service), trigger a serverless function (link
    checker), and respond to link check events. This sounds like a very diverse set
    of dependencies, but as you''ll see, by designing for testability, it is possible
    to achieve a high level of testing without too much complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: Designing for testability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Proper testing starts a long time before you write your test. Even if you practice
    **test-driven design** (**TDD**) and you write your tests before the implementation,
    you still need to design the interface of the code you want to test before you
    write the test (otherwise what functions or methods will the test invoke?). With
    Delinkcious, we took a very deliberate approach with abstractions, layers, and
    separation of concerns. All our hard work is going to pay off now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at `LinkManager` and just consider its dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `LinkManager` depends on the Delinkcious object model abstract
    package, `link_checker_events`, and standard Go packages.  `LinkManager` doesn't
    depend on the implementation of any other Delinkcious component or on any third-party
    dependency. During testing, we can provide alternative (mock) implementations
    for all the dependencies and have total control of the test environment and the
    result. We'll see how we can go about doing this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The art of mocking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ideally, an object should have all its dependencies injected when it is created.
    Let''s look at the `NewLinkManager()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This is almost the ideal situation. We get interfaces to the link store, social
    graph manager, and to the event sink. However, there are two dependencies that
    are not injected here: `link_checker_events` and the built-in `net/http` package.
    Let''s start with mocking the link store, the social graph manager, and the link
    manager event sink, and then consider the more difficult cases.'
  prefs: []
  type: TYPE_NORMAL
- en: '`LinkStore` is an interface that''s defined internally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `pkg/link_manager/mock_social_graph_manager.go` file, we can find a
    mock social graph manager that implements `om.SocialGraphManager` and always returns
    the followers that were provided to the `newMockSocialGraphManager()` function
    from the `GetFollowers()` method. This is a great way to reuse the same mock for
    different tests that require different canned responses from `GetFollowers()`.
    The reason the other methods just return nil is that they are not called by `LinkManager`,
    so there''s no need to provide an actual response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The event sink is a little different. We are interested in verifying that when
    various operations, such as `AddLink()`, are called, `LinkManager` properly notifies
    the event sink. In order to do that, we can create a test event sink that implements
    the `om.LinkManagerEvents` interface and keeps track of events coming its way.
    Here is the code in the `pkg/link_manager/test_event_sink.go` file. The `testEventSink`
    struct keeps a map for each event type, where the keys are username and the values
    are a list of links. It updates these maps in response to the various events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now that we've got our mocks in place, let's create the Ginkgo test suite.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping your test suite
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ginkgo builds on top of Go''s testing package, which is convenient because
    you can run your Ginkgo tests with just `go test`, although Ginkgo also provides
    a CLI called Ginkgo with more options. To bootstrap a test suite for a package,
    run the `ginkgo bootstrap` command. It will generate a file called `<package>_suite_test.go`.
    The file wires up all the Ginkgo tests to the standard Go testing, and also imports
    the `ginkgo` and `gomega` packages. Here is the test suite file for the `link_manager`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: With the test suite file in place, we can start writing some unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the LinkManager unit tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the test for getting and adding links. There is a lot going
    on there. This is all in the `pkg/link_manager/in_memory_link_manager_test.go` file.
    First, let''s set the scene by importing `ginkgo`, `gomega`, and the `delinkcious`
    object model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The Ginkgo `Describe` block describes all the tests in the file and defines
    variables that will be used by multiple tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `BeforeEach()` function is called before each test. It creates a fresh
    mock social graph manager with `liat` as the only follower, a new event sink,
    and initializes the new `LinkManager` with these dependencies, as well as an in-memory
    link store, thus utilizing the dependency injection practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the actual test. Note the BDD style of defining tests that read like
    English, *It should add and get link*. Let''s break it down piece by piece; first,
    the test makes sure that there are no existing links for the `"gigi"` user by
    calling `GetLinks()` and asserting that the result is empty by using Gomega''s
    `Ω` operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The next part is about adding a link and just making sure that no errors occurred:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the test calls `GetLinks()` and expects the link that was just added to
    be returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the test makes sure that the event sink recorded the `OnLinkAdded()`
    call for `follower "liat"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a pretty typical unit test that performs the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Controls the test environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mocks dependencies (social graph manager)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides recording placeholders for outgoing interactions (test event sink records
    link manager events)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executes the code under test (get links and add links)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verifies the responses (no links at first; one link is returned after it is
    added)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verifies any outgoing interactions (the event sink received the `OnLinkAdded()`
    event)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We didn't test error cases here, but it's easy to add. You add bad inputs and
    check the code under the test that returned the expected error.
  prefs: []
  type: TYPE_NORMAL
- en: Should you test everything?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The answer is no! Testing provides a lot of value, but has costs too. The marginal
    value of adding tests is decreasing. Testing *everything* is difficult, if not
    impossible. Considering that testing takes time to develop, it can slow down changes
    to the system (you need to update the tests), and the tests might need to change
    when dependencies change. Testing also takes time and resources to run, which
    can slow down the edit-test-deploy cycle. Also, tests can have bugs too. Finding
    the sweet spot of how much testing you need is a judgement call.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests are very valuable, but they are not enough. This is especially true
    for microservice-based architectures where there are a lot of small components
    that may work independently, but fail to work together to accomplish the goals
    of the system. This is where integration tests come in.
  prefs: []
  type: TYPE_NORMAL
- en: Integration testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Integration testing is a test that includes multiple components that interact
    with each other. Integration tests means testing complete subsystems without or
    very little mocking. Delinkcious has several integration tests focused on particular
    services. These tests are not automated Go tests. They don''t use Ginkgo or the
    standard Go testing. They are executable programs that panic on error. These programs
    are designed to test cross-service interaction and how a service integrates with
    third-party components such as actual data stores. For example, the  `link_manager_e2e`
    test performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Starts the social graph service and the link service as local processes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Starts a Postgres DB in a Docker container
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Runs the test against the link service
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verifies the results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s see how it all plays out. The list of imports includes the Postgres
    Golang driver (`lib/pq`), several Delinkcious packages, and a couple of standard
    Go packages (`context`, `log`, and `os`). Note that `pq` is imported as a dash.
    This means that the `pq` name is unavailable. The reason to import a library in
    such an unnamed mode is that it just needs to run some initialization code and
    is not accessed externally. Specifically, `pq` registers a Go driver with the
    standard Go `database/sql` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at some of the functions that are used to set up the test environments,
    starting with initializing the database.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a test database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `initDB()` function calls the `RunLocalDB()` function by passing the name
    of the database (`link_manager`). This is important because if you''re starting
    from fresh, it needs to create the database too. Then, to make sure that the test
    always runs from scratch, it deletes the `tags` and `links` tables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Running services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The test has two separate functions to run the services. These functions are
    very similar. They set environment variables and call the `RunService()` function,
    which we will dive into soon. Both services depend on the value of the `PORT`
    environment variable, and it needs to be different for each of the services. This
    means that it is imperative that we launch the services sequentially and not in
    parallel. Otherwise, a service might end up listening on the wrong port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Running the actual test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `main()` function is the driver of the entire test. It turns on the mutual
    authentication between the link manager and the social graph manager, initializes
    the database, and runs the services (as long as the `RUN_XXX_SERVICE` environment
    variable is `true`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s ready to actually run the test. It uses the link manager client to
    connect to port `8080` on the localhost, which is where the link service is running.
    Then, it calls the `GetLinks()` method, prints the result (should be empty), adds
    a link by calling `AddLink()`, calls `GetLinks()` again, and prints the results
    (should be one link):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This integration test is not automated. It is designed for interactive use where
    the developer can run and debug individual services. If an error occurs, it immediately
    bails out. The results of each operation are simply printed to the screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the test checks the `UpdateLink()` and `DeleteLink()` operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The fact that the test is conducted through the link manager client library
    ensures that the entire chain is working from client to service to dependent services
    and their data stores.
  prefs: []
  type: TYPE_NORMAL
- en: Let's review some test helper functions, which are very useful when we are trying
    to test and debug complex interactions between microservices locally.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing database test helpers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into the code, let's consider what we want to accomplish. We want
    a local empty database to be created. We want to launch it as a Docker container,
    but only if it's not running already. In order to do that, we need to check whether
    a Docker container is running already, if we should restart it, or if we should
    run a new one. Then, we will try to connect to the target database and create
    it if it doesn't exist. The service will be responsible for creating the schema
    if needed because the generic DB utilities know nothing about the database schema
    of specific services.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `db_util.go` file in the `db_util` package contains all the helpers functions.
    First, let''s review the imports that include the standard Go `database/sql` package
    and squirrel – a fluent-style Go library to generate SQL (but not an ORM). The
    Postgres driver library – `pq` – is imported as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `dbParams` struct contains the information that''s needed to connect to
    the database, and the `defaultDbParams()` function is convenient for getting a
    struct that''s populated with default values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'You can call the `connectToDB()` function by passing the information from the
    `dbParams` struct. If everything goes OK, you''ll get back a handle to the database
    (`*sql.DB`) that you can then use to access the database later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'With all the preliminaries out of the way, let''s see how the `RunLocalDB()`
    function works. First, it runs a `docker ps -f name=postgres` command, which lists
    the running Docker containers named `postgres` (there can only be one):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If the output is empty, it means there is no such container running, so it
    tries to restart the container in case it has stopped. If that fails too, it just
    runs a new container of the `postgres:alpine` image, exposing the standard `5432`
    port to the local host. Note the `-z` flag. It tells Docker to run the container
    in detached (non-blocking) mode, which allows the function to continue. If it
    fails to run the new container for any reason, it gives up and returns an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we are running a Postgres DB running in a container. We can
    use the `defaultDBParams()` function and call the `EnsureDB()` function, which
    we will examine next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To ensure that the DB is ready, we need to connect to the Postgres DB of the
    postgres instance. Each postgres instance has several built-in databases, including
    the `postgres` database. The Postgres DB of the postgres instance can be used
    to get information and metadata about the instance. In particular, we can query
    the `pg_database` table to check if the target database exists. If it doesn''t
    exist, we can create it by executing the `CREATE database <db name>` command.
    Finally, we connect to the target database and return its handle. As usual, if
    anything goes wrong, we return an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: That was a deep dive into automatically setting up a database for local tests.
    It's very handy in many situations, even beyond microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing service test helpers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at some of the helper functions for testing services. The `test_util`
    package is very basic and uses Go standard packages as dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: It provides an error checking function and two functions to run and stop services.
  prefs: []
  type: TYPE_NORMAL
- en: Checking errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the annoying things about Go is the explicit error checking you have
    to do all time. The following snippet is very common; we call a function that
    returns a result and an error, check the error, and if it''s not nil, we do something
    (often, we just return):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Check()` function makes this a little more concise by deciding that it
    will just panic and exit the program (or the current Go routine). This is an acceptable
    choice in a testing scenario where you want to bail out once any failure is encountered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous snippet can be shortened to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: If you have code that needs to check many errors, then these small savings accumulate.
  prefs: []
  type: TYPE_NORMAL
- en: Running a local service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most important helper functions is `RunService()`.  Microservices
    often depend on other microservices. When testing a service, the test code often
    needs to run the dependent services. Here, the code builds a Go service in its
    `target` directory and executes it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Running a service is important, but cleaning up at the end of the test by stopping
    all the services that were started by the test is important too.
  prefs: []
  type: TYPE_NORMAL
- en: Stopping a local service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Stopping a service is as simple as calling the `Done()` method of the context.
    It can be used to signal completion to any code that uses contexts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there is a lot of work involved in running Delinkcious, or even
    just a few parts of Delinkcious locally without the help of Kubernetes. When Delinkcious
    is running, it's great for debugging and troubleshooting, but creating and maintaining
    this setup is tedious and error-prone.
  prefs: []
  type: TYPE_NORMAL
- en: Also, even if all the integration tests work, they don't fully replicate the
    Kubernetes cluster, and there may be many failure modes that are not captured.
    Let's see how we can do local testing with Kubernetes itself.
  prefs: []
  type: TYPE_NORMAL
- en: Local testing with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the hallmarks of Kubernetes is that the same cluster can run anywhere.
    For real-world systems, it's not always trivial if you use services that are not
    available locally or are prohibitively slow or expensive to access locally. The
    trick is to find a good spot between high fidelity and convenience.
  prefs: []
  type: TYPE_NORMAL
- en: Let's write a smoke test that takes Delinkcious through the primary workflow
    of getting links, adding links, and checking their status.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a smoke test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Delinkcious smoke test is not an automated one. It can be, but it will require
    special setup to make it work in the CI/CD environment. For real-world production
    systems, I highly recommend that you have an automated smoke test (and other tests,
    too).
  prefs: []
  type: TYPE_NORMAL
- en: The code is in the `cmd/smoke_test` directory and consists of a single file, `smoke.go`.
    It exercises Delinkcious though the REST API that's exposed by the API gateway.
    We could write this test in any language because there is no client library. I
    chose to use Go for consistency and to highlight how to consume a raw REST API
    from Go, working directly with URLs, query strings, and JSON payload serialization.
    I also used the Delinkcious object model link as a convenient serialization target.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test expects a local Minikube cluster where Delinkcious is installed to
    be up and running. Here is the flow of the test:'
  prefs: []
  type: TYPE_NORMAL
- en: Delete our test link to start fresh.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get links (and print them).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a test link.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get links again (the new link should have a *pending* status).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait a couple of seconds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get links one more time (the new link should have a *valid* status now).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This simple smoke test goes through a significant portion of Delinkcious functionality,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Hitting the API gateway for multiple endpoints (GET links, POST new link, DELETE
    link).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verifying the caller identity (via access token).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The API gateway will forward the requests to the link manager service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The link manager service will trigger the link checker serverless function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The link checker will notify the link manager via NATS about the status of the
    new link.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Later, we can extend the test to create social relationships, which will involve
    the social graph manager, as well as checking the news service. This will establish
    a comprehensive end-to-end test. For smoke test purposes, the aforementioned workflow
    is just fine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the list of imports, which includes a lot of standard Go
    libraries, as well as the Delinkcious `object_model` (for the `Link` struct) package
    and the `test_util` package (for the `Check()` function). We could easily avoid
    these dependencies, but they are familiar and convenient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The next part defines a few variables. `delinkciousUrl` will be initialized
    later. `delinkciousToken` should be available in the environment, and `httpClient`
    is the standard Go HTTP client that we will use to call the Delinkcious REST API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preliminaries out of the way, we can focus on the test itself. It
    is surprisingly simple and looks pretty much like the high-level description of
    the smoke test. It gets the Delinkcious URL from Minikube using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, it calls the `DeleteLink()`, `GetLinks()`, and `AddLink()` functions,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The `GetLinks()` function constructs the proper URL, creates a new HTTP request,
    adds the authentication token as a header (as required by the API gateway social
    login authentication), and hits the `/links` endpoint. When the response comes
    back, it checks the status code and bails out if there was an error. Otherwise,
    it deserializes the response''s body into the `om.GetLinksResult` struct and prints
    the links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The `addLink()` function is very similar except that it uses the POST method
    and just checks that the response has an OK status. The function takes a URL and
    a title and constructs a URL (including encoding the query string) to comply with
    the API gateway specification. If the status is not OK, it will use the contents
    of the body as an error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Great! Now, let's see the test in action.
  prefs: []
  type: TYPE_NORMAL
- en: Running the test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before running the test, we should export `DELINKCIOUS_TOKEN` and make sure
    that Minikube is running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the test, we just type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are printed to the console. There was already one invalid link,
    that is, `http://gg.com`. Then, the test added the new link, that is, `https://github.com/the-gigi`.
    The new link''s status was initially pending and then, after a couple of seconds
    when the link check succeeded, it became valid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Telepresence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Telepresence ([https://www.telepresence.io/](https://www.telepresence.io/))
    is a special tool. It lets you run a service locally as if it''s running inside
    your Kubernetes cluster. Why is that interesting? Consider the smoke test we just
    implemented. If we detect a failure, we would like to do the following three things:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the root cause.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fix it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify that the fix works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we discovered the failure only when running the smoke test on our Kubernetes
    cluster, it is probably a failure that is not detected by our local unit tests.
    The normal way to find the root cause (other than reviewing the code offline)
    is to add a bunch of logging statements, add experimental debug code, comment
    out irrelevant sections and deploy the modified code, rerun the smoke test, and
    try to get a sense about what's broken.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploying modified code to a Kubernetes cluster typically involves the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the code
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pushing the modified code to a Git repository (pollute your Git history with
    changes that are only used for debugging)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building an image (often requires running various tests)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pushing the new image to an image registry
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploying the new image to the cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process is cumbersome and doesn't encourage ad hoc exploration and quick
    edit-debug-fix cycles. There are tools we will explore in [Chapter 11](ba776b0b-35e6-4fbd-9450-78b155daa743.xhtml), *Deploying
    Microservices*, that can skip pushing to the Git repository and automatically
    building your images for you, but the image is still built and deployed to the
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: With Telepresence, you just make changes to the code locally, and Telepresence
    makes sure that your local service becomes a full-fledged member of your cluster.
    It sees the same environment and Kubernetes resources, it can communicate with
    other services though the internal network, and for all intents and purposes it
    is part of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Telepresence accomplishes this by installing a proxy inside the cluster that
    reaches out and talks to your local service. This is pretty ingenious. Let's install
    Telepresence and start playing with it.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Telepresence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The installation of Telepresence requires the FUSE filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can install Telepresence itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Running a local link service via Telepresence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's run the link manager service locally via Telepresence. First, to demonstrate
    that it is really the local service that is running, we can modify the service
    code. For example, we can print a message when getting links, that is, `"****
    Local link service here! calling GetLinks() ****"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add it to the `GetLinks` endpoint in `svc/link_service/service/transport.go`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build the local link service (with flags recommended by Telepresence)
    and swap the `link-manager` deployment with the local service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that Telepresence requires `sudo` privileges when you swap a deployment
    for the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Modifying your local network (via `sshuttle` and `pf/iptables`) for the `vpn-tcp`
    method that's used for Go programs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the `docker` command (for some configurations on Linux)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mounting the remote filesystem for access in a Docker container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To test our new changes, let''s run the `smoke` test again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the our local service output, we can see that it was indeed invoked
    when the `smoke` test ran:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may recall, the smoke test exercises the API gateway in the cluster,
    so the fact that our local service was invoked shows that it is indeed running
    in the cluster. One interesting fact is that the output of our local service is
    NOT captured by Kubernetes logs. If we search the logs, we find nothing. The following
    command generates no output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's see what it takes to attach the GoLand debugger to the running local
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Attaching to the local link service with GoLand for live debugging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the holy grail of debugging! We will be connecting to our local link
    service using the GoLand interactive debugger while it''s running as part of the
    Kubernetes cluster. It doesn''t get better than that. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: First, follow the instructions here to get ready for attaching to a local Go
    process with GoLand: [https://blog.jetbrains.com/go/2019/02/06/debugging-with-goland-getting-started/#debugging-a-running-application-on-the-local-machine](https://blog.jetbrains.com/go/2019/02/06/debugging-with-goland-getting-started/#debugging-a-running-application-on-the-local-machine).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, click the Run | Attach to Process menu option in GoLand, which will bring
    the following dialog box:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/131918fe-55b0-4ec6-a233-0dd4fd1bfc8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Unfortunately, when GoLand attaches to the process (successfully), Telepresence
    mistakenly thinks that the local service has exited and tears down the tunnel
    to the Kubernetes cluster and its own control process.
  prefs: []
  type: TYPE_NORMAL
- en: The local link service keeps running, but it's not connected to the cluster
    anymore. I opened a GitHub issue for the Telepresence team: [https://github.com/telepresenceio/telepresence/issues/1003](https://github.com/telepresenceio/telepresence/issues/1003).
  prefs: []
  type: TYPE_NORMAL
- en: I later contacted the Telepresence developers, dived into the code, and contributed
    a fix that was merged recently.
  prefs: []
  type: TYPE_NORMAL
- en: See the following PR (Adding support for attaching a debugger to processes under
    Telepresence): [https://github.com/telepresenceio/telepresence/pull/1005](https://github.com/telepresenceio/telepresence/pull/1005).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re using VS Code for Go programming, you can try your luck by following
    the information here: [https://github.com/Microsoft/vscode-go/wiki/Debugging-Go-code-using-VS-Code](https://github.com/Microsoft/vscode-go/wiki/Debugging-Go-code-using-VS-Code).'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have written a standalone smoke test and used Telepresence to be
    able to debug locally services that are part of our Kubernetes cluster. It doesn't
    get any better for interactive development. The next section will deal with test
    isolation.
  prefs: []
  type: TYPE_NORMAL
- en: Isolating tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Isolation is a key topic with tests. The core idea is that, in general, your
    tests should be isolated from your production environment, or even isolated from
    other shared environments. If tests are not isolated, then changes the tests make
    can impact these environments and vice versa (external changes to these environments
    can break tests that make assumptions). Another level of isolation is between
    tests. If your tests run in parallel and make changes to the same resources, then
    various race conditions can occur and tests can interfere with each other and
    cause false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: This can happen if tests don't run in parallel, but neglecting to clean up test
    A can make changes that break test B. Another case where isolation can help is
    when multiple teams or developers want to test incompatible changes. If two developers
    make incompatible changes to a shared environment, at least one of them will experience
    failures. There are various levels of isolation and they often have inverse relation
    to cost – more isolated tests are more expensive to set up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following isolation approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Test clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test namespaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross namespace/ cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster-level isolation is the highest form of isolation. You run your tests
    in clusters that are totally independent of your production cluster. The challenge
    with this approach is how to keep your test cluster/clusters in sync with your
    production cluster. On the software side, this may not be too difficult with a
    good CI/CD system, but populating and migrating data is often pretty complicated.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two forms of test clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: Each developer gets their own cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dedicated clusters for performing system tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster per developer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating a cluster per developer is the ultimate level of isolation. The developer
    doesn''t have to worry about breaking other people''s code or being impacted by
    other people''s code. However, there are some significant downsides for this approach,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It is often too expensive to provision a full-fledged cluster for each developer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The provisioned cluster often doesn't have high fidelity with the production
    system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will generally still need another integration environment to reconcile changes
    from multiple teams/developers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With Kubernetes, it may be possible to utilize Minikube as a local cluster per
    developer and avoid many of the downsides.
  prefs: []
  type: TYPE_NORMAL
- en: Dedicated clusters for system tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating dedicated clusters for system tests is a great way to consolidate changes
    and test them one more time before deploying to production. The test cluster can
    run more rigorous tests, depend on external resources, and interact with third-party
    services. Such test clusters are expensive resources, and you must manage them
    carefully.
  prefs: []
  type: TYPE_NORMAL
- en: Test namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Test namespaces are a lightweight form of isolation. They can run side-by-side
    next to the production system and reuse some of the resources of the production
    environment (for example, the control plane). It can be much easier to sync data,
    and on Kubernetes, in particular, writing a custom controller to sync and audit
    the test namespace against the production namespace is a good option.
  prefs: []
  type: TYPE_NORMAL
- en: The downside of test namespaces is the reduced level of isolation. By default,
    services in different namespaces can still talk to each other. If your system
    is already using multiple namespaces, then you have to be extremely careful to
    keep tests isolated from production.
  prefs: []
  type: TYPE_NORMAL
- en: Writing multi-tenant systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-tenant systems are systems where totally isolated entities share the same
    physical or virtual resources. Kubernetes namespaces provide several mechanisms
    to support this. You can define network policies that prevent connectivity between
    namespaces (except for interaction with the Kubernetes API server). You can define
    resource quotas and limits per namespace to prevent rogue namespaces from hogging
    all the cluster resources. If your system is already set up for multi-tenancy,
    you can treat a test namespace as just another tenant.
  prefs: []
  type: TYPE_NORMAL
- en: Cross namespace/cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, your system is deployed into multiple coordinated namespaces or even
    multiple clusters. Under these circumstances, you'll need to pay more attention
    on how to design tests that mimic the same architecture, yet be careful that tests
    don't interact with production namespaces or clusters.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: End-to-end tests are very important for complex distributed systems. The smoke
    test we wrote for Delinkcious is one example of an end-to-end test, but there
    are several other categories. End-to-end tests often run against a dedicated environment
    such as a staging environment, but in some cases they run against the production
    environment itself (with a lot of attention). Since end-to-end tests typically
    take a long time to run and may be slow and expansive to set up, it is not common
    to run them for every commit. Instead, it is common to run them periodically (every
    night, every weekend, or every month) or ad hoc (for example, before an important
    release). There are several categories of end-to-end tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will explore some of the most important categories in the following sections,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Acceptance testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acceptance testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Acceptance testing is a form of testing that verifies that the system behaves
    as expected. It is up to the system stakeholder to decide what is considered as
    acceptable. It could be as simple as a smoke test or as elaborate as testing all
    the possible paths through the code, all failure modes, and all side effects (for
    example, which messages are written to log files). One of the main benefits of
    a good battery of acceptance tests is that it is a forcing function for describing
    your system in terms that make sense for non-engineer stakeholders, such as product
    managers and top management. The ideal situation (I've never seen in it practice)
    is that business stakeholders will be able to write and maintain acceptance tests
    themselves.
  prefs: []
  type: TYPE_NORMAL
- en: This is close in spirit to visual programming. I personally believe that all
    automated testing should be written and maintained by the developers, but your
    mileage may vary. Delinkcious currently exposes just a REST API and doesn't have
    a user facing web application. Most systems these days have web applications that
    become the acceptance test boundary. It is common to run acceptance tests in the
    browser. There are many good frameworks. If you prefer to stay with Go, Agouti
    ([https://agouti.org/](https://agouti.org/)) is a great choice. It has tight integration
    with Ginkgo and Gomega and can drive the browser though PhantomJS, Selenium, or
    ChromeDriver.
  prefs: []
  type: TYPE_NORMAL
- en: Regression testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression testing is a good option when you just want to make sure that the
    new system doesn't deviate from the behavior of the current system. If you have
    comprehensive acceptance tests, then you just have to make sure that the new version
    of your system passes all the acceptance tests, just like the previous version
    did. However, if your acceptance tests coverage is lacking, you can get some form
    of confidence by bombarding the current system and the new system with the same
    inputs and verify that the outputs are identical. This can be done with fuzz testing
    too, where you generate random inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Performance testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Performance testing is a large topic. Here, the goal is to measure the performance
    of the system and not the correctness of its responses. That being said, errors
    can significantly influence performance. Consider the following error handling
    options:'
  prefs: []
  type: TYPE_NORMAL
- en: Return immediately when an error is encountered
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retry five times and sleep for one second between tries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, given these two strategies, consider a request that usually takes about
    two seconds to process. A large number of errors for this request on a naive performance
    test will increase performance when using the first strategy (because requests
    will not be processed and return immediately), but will reduce performance when
    using the second strategy (requests will be retried for five seconds before failing).
  prefs: []
  type: TYPE_NORMAL
- en: Microservices architectures often utilize asynchronous processing, queues, and
    other mechanisms that can make it challenging to measure the actual performance
    of the system. In addition, a lot of networking calls are involved, which might
    be volatile.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, performance is not just about response time. It can include CPU
    and memory utilization, a number of external API calls, access to network storage,
    and so on. Performance is also tightly related to availability and cost. In a
    complex cloud-native distributed system, performance tests can often inform and
    guide architectural decisions.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, end-to-end testing is quite a complicated issue and must be
    considered with great care, because both the value and the costs of end-to-end
    tests are not trivial. One of the most difficult resources to manage with end-to-end
    tests is the test data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at some of the approaches for managing test data, their pros,
    and their cons.
  prefs: []
  type: TYPE_NORMAL
- en: Managing test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With Kubernetes, it is relatively easy to deploy a lot of software, including
    software made of many components, as in typical microservice architectures. However,
    data is much less dynamic. There are different ways to generate and maintain test
    data. Different tactics of test data management are appropriate for different
    types of end-to-end tests. Let's look into synthetic data, manual test data, and
    production snapshots.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Synthetic data is test data that you generate programmatically. The pros and
    cons are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to control and update because it is generated programmatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to create bad data to test error handling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to create a lot of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have to write code to generate it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can get out of sync with the format of actual data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Manual test data is similar to synthetic data, but you create it manually. The
    pros and cons are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimate control, including verifying what the out should be
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be based on example data and tweaked lightly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to start quickly (no need to write and maintain code)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No need to filter or deanonymize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tedious and error-prone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficult to generate a lot of test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficult to generate related data across multiple microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have to manually update when the data format changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Production snapshot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A production snapshot is literally recording real data and using it to populate
    your test system. The pros and cons are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High fidelity to real data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recollection ensures test data is always in sync with production data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Need to filter and anonymize sensitive data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data might not support all testing scenarios (for example, error handling)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Might be difficult to collect all relevant data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we covered the topic of testing and its various flavors: unit
    testing, integration testing, and all kinds of end-to-end testing. We also dived
    deep into how Delinkcious tests are structured. We explored the link manager unit
    tests, added a new smoke test, and introduced Telepresence for expediting the
    edit-test-debug life cycle against a real Kubernetes cluster while modifying the
    code locally.'
  prefs: []
  type: TYPE_NORMAL
- en: That being said, testing is a spectrum that has costs, and just blindly adding
    more and more tests doesn't make your system better or  higher quality. There
    are many important trade-offs between quantity and quality of tests, such as the
    time it takes to develop and maintain the tests, the time and resources it takes
    to run the tests, and the number and complexity of problems that tests detect
    early. You should have enough context to make those tough decisions for your system
    and choose the testing strategies that will work best for you.
  prefs: []
  type: TYPE_NORMAL
- en: It's also important to remember that testing evolves with the system, and the
    level of testing often has to be ratcheted up when the stakes are higher, even
    for the same organization. If you're a hobbyist developer that has a Beta product
    out there with a few users that just play with it at home, you may not be as rigorous
    in your testing (unless it saves you development time). However, as your company
    grows and gathers more users that use your product for mission-critical applications,
    the impact of problems in your code might require much more stringent testing.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at various deployment use cases and situations
    for Delinkcious. Kubernetes and its ecosystem provides many interesting options
    and tools. We will consider both robust deployment to production as well as quick
    developer-focused scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following references for more information regarding what
    was covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Go Programming Language Package testing**: [https://golang.org/pkg/testing/](https://golang.org/pkg/testing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ginkgo**: [http://onsi.github.io/ginkgo/](http://onsi.github.io/ginkgo/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gomega**: [http://onsi.github.io/gomega/](http://onsi.github.io/gomega/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agouti**: [https://agouti.org/](https://agouti.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Telepresence**: [https://telepresence.io](https://telepresence.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
