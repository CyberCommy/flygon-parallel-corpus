- en: Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have explored various tools and techniques regarding web scraping via
    the use of the Python programming language.
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping, or web harvesting, is done in order to extract and collect data
    from websites. Web scraping comes in handy in terms of model development, which
    requires data to be collected on the fly that's true, relevant to the topic, and
    accurate. This is desirable as it takes less time compared to implementing datasets. The
    data that's collected is stored in various formats, such as JSON, CSV, XML, and
    more, is written to databases for later use, and is also made available online
    as datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Websites also provide web APIs with a user interface to interact with information
    on the web. This data can be used for research, analysis, marketing, **machine
    learning** (**ML**) models, information building, knowledge discovery, and more
    in the field of computer science, management, medicine, and more. We can also
    perform analysis on the data that's obtained through APIs and publicly, or freely,
    available datasets and generate an outcome, but this process isn't classed as
    web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about topics that are applicable to collected
    or scraped data and learn about some advanced concepts that are worth knowing
    about from an information and career perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: Managing scraped data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis and visualization using pandas and matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data mining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What's next?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A web browser (Google Chrome or Mozilla Firefox) is required. We will be using
    the following Python libraries in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`csv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`json`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If these libraries don't exist in your current Python setup, refer to [Chapter
    2](b9919ebf-2d5c-4721-aa76-5c1378262473.xhtml), *Python and the Web – Using urllib
    and Requests*, in the *Setting things up* section, for instructions on installing
    them and setting them up.
  prefs: []
  type: TYPE_NORMAL
- en: The code files for this chapter are available in this book's GitHub repository: [https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter10](https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter10).
  prefs: []
  type: TYPE_NORMAL
- en: Managing scraped data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore some tools and learn more about handling and
    managing the data that we have scraped or extracted from certain websites.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data that''s collected from websites using scraping scripts is known as raw
    data. This data might require some additional tasks to be performed on top of
    it before it can be processed further so that we can gain an insight on it. Therefore,
    raw data should be verified and processed (if required), which can be done by
    doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cleaning**: As the name suggests, this step is used to remove unwanted pieces
    of information, such as space and whitespace characters, and unwanted portions
    of text. The following code shows some relevant steps that were used in examples
    in previous chapters, such as [Chapter 9](5b32d118-ec78-4189-80e8-c88974ba9082.xhtml),
    *Using Regex to Extract Data*, and [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml),
    *Using LXML, XPath, and CSS Selectors*. Functions such as `sub()` (that is, `re.sub()`),
    `strip()`, and `replace()` are used in many places and can also be used for the
    purpose of cleaning:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Formatting**: This step is used to obtain the desired format from the data.
    For example, we might require fixed decimal places in the price that''s received,
    we may need to convert or round up large floating values into fixed decimal places,
    split large strings into smaller units, and more, and then write them to datasets.
    There may also be cases where decimal numbers or integers are extracted as strings
    and need to be formatted. Normally, converting data types and presenting data
    is considered formatting:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: These additional steps can also be performed within the scripts while we are
    extracting particular data, and has been done in the examples we've looked at
    throughout the book. In many cases, cleaning and formatting works together, or
    is done side by side.
  prefs: []
  type: TYPE_NORMAL
- en: Writing to files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have needed to extract lines of data throughout this book. You may have
    noticed that, in most of these examples, we used a dataset (a Python list object
    that was used to collect data) that was appended with various fields in a Python
    list, as shown in the following code (collected from various examples of this
    book):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With the availability of such a dataset, we can write this information to external
    files, as well as to the database. Before we write the dataset to the files, column
    names that describe the data from the dataset are needed. Consider the following
    code, where `keys` is a separate list containing a string title, that is, the
    name of the columns to the respective list item appended to the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s consider the following example, which contains `colNames` with the column
    to be used, and `dataSet` with the cleaned and formatted data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will write the preceding `dataSet` to the CSV file. The first line of
    the CSV file should always contain the column names. In this case, we will use
    `colNames` for the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will result in the `bookdetails.csv` file, which has the
    following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, let''s create a JSON file with `colNames` and `dataSets`. JSON is
    similar to Python dictionary, where each data or value possesses a key; that is,
    it exists in a key-value pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, `finalDataSet` is formed by appending data from `dataSet` and
    by using the `zip()` Python function. `zip()` combines each individual element
    from the list. This zipped object is then converted into a Python dictionary.
    For example, consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, with the available `finalDataSet`, we can dump or add the data to a JSON
    file using the `dump()` function from the `json` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will result in the `bookdetails.json` file. Its content
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we have covered the basic steps for managing raw data. The
    files we have obtained can be shared and exchanged easily across various independent
    systems, used as models for ML, and can be imported as data sources in applications.
    Furthermore, we can also use **Database Management Systems** (**DBMS**) such as
    MySQL, PostgreSQL, and more to store data and execute **Structured Query Language** (**SQL**)
    using the necessary Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis and visualization using pandas and matplotlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be exploring a few basic concepts with regard to analyzing
    data using pandas and plotting general charts using matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: pandas is one of the most popular data analysis libraries in recent times. Data
    analysis and visualization are major tasks and can be performed with the help
    of pandas and other libraries such as matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: For more details and documentation on pandas and matplotlib, please visit their
    official sites at [https://pandas.pydata.org/](https://pandas.pydata.org/) and [https://matplotlib.org/](https://matplotlib.org/).
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas is also termed and used as a raw spreadsheet. It supports mathematical,
    statistical and query-type statements, and allows you to read from and write to
    various files. It is also popular among developers and analysts since it has easy
    functions and properties available that can help you handle data that''s in a
    row and column structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6a141fdd-e7fa-4ce0-9e71-7120be45a431.png)'
  prefs: []
  type: TYPE_IMG
- en: Exploring pandas using the Python IDE
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will be reading data from the `bookdetails.csv` file and
    conducting analysis and visualization using the file''s data. Let''s import the
    libraries that are required, that is, pandas and `matplotlib.pyplot`. We will
    be using the  `pd` and `plt` aliases, respectively, and reading the data from
    the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the `read_csv()` function reads the content from a CSV file and
    generates a DataFrame object. pandas also supports various data files via the
    use of functions such as `read_html()`, `read_excel()`, `read_json()`, and `read_sql_table()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, `dataSet` is an object of the pandas DataFrame. The DataFrame represents
    a two-dimensional tabular structure with rows, columns, and indexes. Query-level
    analysis, conditional statements, filtering, grouping, and more are supported
    by DataFrames against data in rows and columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The following screenshot displays the content that's now available in `dataSet`*:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a640b1d4-77ce-4ee3-8188-d698f08bfe09.png)'
  prefs: []
  type: TYPE_IMG
- en: Dataset contents from a CSV file
  prefs: []
  type: TYPE_NORMAL
- en: 'Row indexes are also shown, all of which start with `0` (zero). The general
    statistical output can be obtained by using the `describe()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, by default, `describe()` selects the columns that are applicable
    to statistical functions and returns calculations with the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`count`: Number of rows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean`: Average value for the related column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min`: Minimum value found'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max`: Maximum value found'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std`: Calculated standard deviation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`25%`: Returns the 25^(th) percentile'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`50%`: Returns the 50^(th) percentile'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`75%`: Returns the 75^(th) percentile'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following code, we are selecting an individual column called `Price`
    as `price_group`*.* All of the columns from the dataset can be listed using `dataSet.columns`.
    Multiple columns can be selected by using the following `dataSet[[''Price'',''Rating'']]`
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows the individual data for the `Price` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'pandas DataFrames also accept conditions or filtering actions being used on
    columns. As you can see, the filter is applied to `Rating` for values that are `>=4.0`,
    and only `Title` and `Price` are going to be returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, string-based filters can also be applied. `Stock`, which contains
    the `Out` text, is filtered, and the output returns all the columns that satisfy
    the `Out` text. The `contains()` function accepts regular expressions and strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `between()` function is supplied with values that refer to `Rating` to
    filter and return `Title` of the books:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we have the `price_group` data, we can call the `plot()` function on
    the data with the help of the `show()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will generate a line chart with default properties, such
    as colors and legend placements, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/88032012-24fe-458b-bce4-311805f499a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Default line chart for the Price column
  prefs: []
  type: TYPE_NORMAL
- en: We can also change the kind of chart, that is, line, bar, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Visit matplotlib at [https://matplotlib.org/gallery/index.html](https://matplotlib.org/gallery/index.html) to
    find out more about various functional chart types and their additional associated
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, `kind=''bar''` overwrites the default line type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following bar chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/53896187-3e46-4cf5-8b19-32d0fb0272e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Bar chart for the Price column
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have used a basic chart type with a single column. In the following
    code, we are plotting a bar chart with the `Price` and `Rating` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1b71ada2-29dd-4b1c-909f-6153340ceef6.png)'
  prefs: []
  type: TYPE_IMG
- en: Bar chart with Price and Rating columns
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have successfully plotted line and bar charts. The following code
    plots a pie chart for the first six items from the `Price` column and labels them
    with the first six `Title` available from `dataSet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The values from `Price` are used as legends. We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/48838041-1f20-43c6-8ed4-55baf156f677.png)'
  prefs: []
  type: TYPE_IMG
- en: Pie chart with Price and Title column data
  prefs: []
  type: TYPE_NORMAL
- en: There's a lot more to explore in terms of using pandas and matplotlib. In this
    section, we have displayed the basic features that are available from both libraries.
    Now, we will look at ML.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML is a branch of **artificial intelligence** (**AI**) that deals with the study
    of mathematical and statistical algorithms to process and develop an automated
    system that can learn from data with minimal human involvement. ML predictions
    and decision-making models are dependent on data. Web scraping is one of the resources
    that makes data available to ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, many recommendation engines implement ML in order to serve the marketing
    ads and recommendations such as Google AdSense and AdWords in real time. The process
    that's implemented in ML is similar to that of data mining and predictive modeling.
    Both of these concepts seek patterns while skimming through data and modifying
    the program's actions as per the requirements. Therefore, ML is a handy tool when
    it comes to exploring the field of business, marketing, retail, stock prices,
    video surveillance, face recognition, medical diagnosis, weather prediction, online
    customer support, online fraud detection, and more.
  prefs: []
  type: TYPE_NORMAL
- en: With new and improved ML algorithms, data capture methods, and faster computer
    and networking, the field of ML is accelerating.
  prefs: []
  type: TYPE_NORMAL
- en: ML and AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI is a broad spectrum that covers a wide range of topics, such as neural networks,
    expert systems, robotics, fuzzy logic, and more. ML is a subset of AI. It explores
    the idea of building a machine that learns on its own, thus surpassing the need
    for constant speculation. Therefore, ML has led to a major breakthrough for achieving
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: ML incorporates the use of several algorithms, thus allowing software to provide
    accurate results. Making a useful prediction from a set of parsed data is what
    the concept of ML aims to do. The foremost benefit of ML is that it can tirelessly
    learn and predict without the need for a hardcoded software regime. Training includes
    feeding huge datasets as input. This allows an algorithm to learn, process, and
    make predictions, which are provided as output.
  prefs: []
  type: TYPE_NORMAL
- en: Several important parameters are employed when measuring the potential of any
    model. Accuracy is one of them, and is an important parameter in measuring the
    success of any developed model. In ML, 80% accuracy is a success. If the model
    has 80% accuracy, then we are saving 80% of our time and increasing productivity.
    However, it is not always the best metric for accessing classification models
    if the data is unbalanced.
  prefs: []
  type: TYPE_NORMAL
- en: In general, accuracy is termed as an intuitive measure. While employing accuracy,
    equal cost is assigned to false positives and false negatives. For imbalanced
    data (such as 94% falling in one instance and 6% in other), there are many great
    ways to decrease the cost; make a vague prediction that every instance belongs
    to the majority class, prove that the overall accuracy is 94%, and complete the
    task. In the same line, problems arise if what we are talking about, such as a
    disease, is rare and lethal. The cost of failing to properly examine the disease
    of a sick person is higher than the cost of pushing a healthy individual to more
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, there are no best metrics. It is common for two people to choose
    different metrics to reach their goal.
  prefs: []
  type: TYPE_NORMAL
- en: Python and ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Dutch programmer (Guido Van Rossum) launched Python as his side project but
    did not realize that it would accelerate his height of success. Python is widely
    adapted among developers when it comes to speedy prototyping. It is gaining popularity
    among all the ML tools that are available for its readability, versatility, and
    easiness.
  prefs: []
  type: TYPE_NORMAL
- en: As ML engineers, computer vision engineers, data scientists, or data engineers,
    we have to juggle with the ideas of linear algebra and calculus, which often get
    complex once we dive deeper. However, Python comes to the rescue with its quick
    implementation, thus bypassing the hurdle of the maximum effort. Quick validation
    of this idea makes the Python programming language more desirable.
  prefs: []
  type: TYPE_NORMAL
- en: Data is everything for ML. Raw data is unstructured, large, incomplete, and
    has missing values. Data cleaning is one of the most crucial steps of ML so that
    we can move on with our data. There are many essential libraries available in
    Python that make the implementation of ML simpler. Various open source repositories
    in Python help bring changes to the existing method. Web scraping is one of these
    methods that deals with data that exists on the web, which it then processes further
    as input to ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the most common and widely used libraries that are
    worth looking at if we decide to work with Python and ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '**scikit-learn**: Used for working with classical ML algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NumPy (numerical Python)**: Designed to work for scientific computing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SciPy**: Contains modules for linear algebra, optimization, integration,
    and statistics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pandas**: Used for data aggregation, manipulation, and visualization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**matplotlib** and **Seaborn**: For data visualization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bokeh** and **Plotly**: For interactive visualization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow **and **Theano**: Used for deep learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Beautiful Soup, LXML, PyQuery **and **Scrapy**: Used to withdraw data from
    HTML and XML documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we have a basic understanding of Python, these libraries can be imported
    and implemented. Alternatively, we can also apply these functionalities from scratch,
    which is what most developers do.
  prefs: []
  type: TYPE_NORMAL
- en: 'Python requires less writing and debugging in terms of code, which saves time
    compared to other programming languages. This is exactly what AI and ML programmers
    want: a focus on understanding the architectural aspect rather than spending all
    of their time on debugging. Thus, Python can be easily handled by people with
    less knowledge in programming due to syntax that provides human-level readability.'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from Python, there are several other tools for ML, such as Microsoft Excel,
    SAS, MATLAB, and R. These tools are often overlooked due to a lack of adequate
    community services and because they are incapable of handling large datasets. MATLAB
    also provides sophisticated libraries and packages for image processing and analysis.
    In comparison to Python, the execution time is moderate and the functionality
    is limited to prototyping, not deployment.
  prefs: []
  type: TYPE_NORMAL
- en: R is another tool that's used for statistical analysis. Python performs data
    manipulation by providing various development tools that can be collaborated with
    other systems. However, R only works on a particular form of dataset, and so the
    predefined functions require the predefined input. R provides a primitive ground
    to the data, which Python allows us to explore the data.
  prefs: []
  type: TYPE_NORMAL
- en: Types of ML algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In general, there are three types of ML algorithms, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised learning:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unsupervised learning:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Supervised learning is about observing or directing the execution of something.
    The input that's given to the model is the prediction we want to make. The labeled
    data is the explicit prediction given for the particular instances of the input.
    Supervised learning requires labeled data, which requires some expertise. However,
    these conditions are not always met. We don't always posses the labeled dataset.
    For example, fraud prediction is one of the rapidly unfolding fields where the
    attacker is constantly looking for available exploits. These new attacks can't
    possibly be maintained under a dataset with labelled attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, the mapping functions of the input to the output can be expressed
    as *Y = f(X)*. Here, *Y* is the output variable and *X* is the input variable.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification determines or categorizes a model based on its attributes, and is
    the process of identifying the genre to which a new observation belongs to, as
    per the membership category, which is known in advance. It is a technique of determining
    which class a dependent variable belongs to based on one or more independent variables.
    The output variable in the classification problem is either a group or a category.
  prefs: []
  type: TYPE_NORMAL
- en: Some examples include, credit scoring (differentiating between high risk and
    low risk based on earning and saving), medical diagnosis (predicting the risk
    of disease), web advertising (predicting whether a user will click on advertisements
    or not), and more.
  prefs: []
  type: TYPE_NORMAL
- en: The ability of the classification model can be determined by using model evaluation
    procedures and model evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model evaluation procedures**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model evaluation procedures help you find out how well a model will adapt to
    the sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training and testing the data**: The training data is used to train the model
    so that it fits the parameter. The testing data is a masked dataset for which
    a prediction has to be made.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Train and test split**: Typically, when the data is separated, most of the
    data is used for training, whereas a small portion of the data is used for testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-fold cross**-**validation**: K-train and test splits are created and averaged
    together. The process runs k-times slower than train and test splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model evaluation metrics**'
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation metrics are employed to quantify the performance of the model.
    The following metrics can be implemented in order to measure the ability of a
    classification predictive model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation metrics are managed with the help of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Confusion matrix**: This is a 2 x2 matrix, also known as an error matrix.
    It helps picture the performance of an algorithm – typically a supervised learning
    one—with the help of classification accuracy, classification error, sensitivity,
    precision measures, and predictions. The choice of metrics depends on the business
    objective. Hence, it is necessary to identify whether false positives or false
    negatives can be reduced based on the requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logistic regression**: Logistic regression is a statistical model that aids
    in analyzing the dataset. It has several independent variables that are responsible
    for determining the output. The output is measured with diploid variables (involving
    two possible outcomes). The aim of logistic regression is to find the best-fitting
    model to describe the relationship between diploid variables (dependent variables)
    and a set of independent variables (predictors). Hence, it is also known as a
    predictive learning model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Naives Bayes**: This works on the concept of conditional probability, as
    given by Bayes theorem. Bayes theorem calculates the conditional probability of
    an event based on the prior knowledge that might be in relation to the event.
    This approach is widely used in face recognition, medical diagnosis, news classification,
    and more. The Naives Bayes classifier is based on Bayes theorem, where the conditional
    probability of *A* given *B* can be calculated as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**Decision tree**: A decision tree is a type of supervised learning model where
    the final outcome can be viewed in the form of a tree. The decision tree includes
    leaf nodes, decision nodes, and the root node. The decision node has two or more
    branches, whereas the leaf node represents the classification or decision. The
    decision tree breaks down the dataset further into smaller subsets, thus incrementally
    developing the associated tree. It is simple to understand and can easily handle
    categorical and numerical datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random forest algorithm**: This algorithm is a supervised ML algorithm that
    is easy to use and provides great results, even without hyperparameter tuning.
    Due to its simplicity, it can be used for both regression and classification tasks.
    It can handle larger sets of data in order to maintain missing values. This algorithm
    is also considered the best at performing classification-related tasks compared
    to regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neural network**: Although we already have linear and classification algorithms,
    a neural network is the state of art technique for many ML problems. A neural
    network is comprised of units, namely neurons, which are arranged into layers.
    They are responsible for the conversion of an input vector into some output. Each
    unit takes an input, applies a function, and passes the output to the next layer. Usually,
    nonlinear functions are applied to this algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support Vector Machine (SVM) algorithm**: The SVM learning algorithm is a
    supervised ML model. It is used for both classification and regression analysis,
    and is widely known as a constrained optimization problem. SVM can be made more
    powerful using the kernel trick (linear, radial basis function, polynomial, and
    sigmoid). However, the limitations of the SVM approach lies in the selection of
    the kernel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression is a statistical measurement that aids in estimating the relationship
    among variables. In general, classification focuses on the prediction of a label,
    whereas regression focuses on the prediction of a quantity. Regression is used
    in finance, investing, and other disciplines by managers to value their assets.
    In the same line, it attempts to determine the strength of the relationship between
    dependent variables and a series of other changing variables (independent variables);
    for example, the relationship between commodity prices and the businesses dealing
    in those commodities.
  prefs: []
  type: TYPE_NORMAL
- en: The regression model has two major characteristics. The output variable in the
    regression problem is a real value or quantitative in nature. The creation of
    the model takes past data into consideration. Mathematically, a predictive model
    maps the input variable (*X*) to the continuous output variable (*Y*). A continuous
    output variable is an integer or floating-point value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ability of the regression predictive model can be measured by calculating
    the **root mean square error** (**RMSE**). For example, in total, the regression
    prediction model made two predictions, that is, 1.5 and 3.3, where the expected
    values are 1.0 and 3.0\. Therefore, RMSE can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning is a class of ML techniques in which the data that's given
    as input isn't labeled. Moreover, only the input variables (*X*) are given, with
    no correspondence to the output variables (*Y*). In unsupervised learning, the
    algorithms are left in solitude to learn and explore on their own, with no real
    early expectations. This absence of labeling teaches us about the reconstruction
    of input data either using representation or embedding. It is beneficial when
    it comes to data mining and feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning allows you to discover hidden trends and patterns. Some
    real-world examples are predicting or understanding handwritten digits, nano camera
    fabrication technology, Planck quantum spectrum, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, unsupervised learning has an input value (*X*) with no corresponding
    output value. In comparison to supervised learning, the task processing of unsupervised
    learning is quite complex. The implementation of unsupervised learning can be
    found in automatic or self-driving cars, facial recognition programs, expert systems,
    bioinformatics, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Association and clustering are two parts of unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Association
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a technique that's used to discover new patterns in huge datasets. Association
    is deliberated to identify strong rules from a dataset based on the degree of
    newsworthiness. During prolonged analysis of the data, more new rules are generated.
  prefs: []
  type: TYPE_NORMAL
- en: The association rule is largely employed in market basket analysis. This technique
    helps to determine the strength of association between the pairs of the product
    purchased and the frequency of cooccurrence in the observations.
  prefs: []
  type: TYPE_NORMAL
- en: Market basket analysis is one of the modeling techniques that's used by retailers
    to uncover associations between items. The theory elaborates around the fact that
    if we buy some items, we are more likely to buy similar items.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, it is represented as *P(A|B)*, where a person who buys *A* also
    buys *B*. It can also be written as if *{A},* then *{B}*. In other words, if there
    is a probability of A to occur, then there is also a probability of B to occur
    as well. For example, *P(milk | bread ) = 0.7*.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster is the assembly of an object belonging to the same label, treated as
    one. Clustering is the technique of grouping an object to its corresponding category.
    This includes sorting several objects into their particular groups, where the
    capacity of association is at its maximum if it belongs to the same group, or
    minimum, otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular clustering algorithms is the k-means clustering algorithm.
    This algorithm demands the predefined value of k. K represents the number of clusters
    we want to divide data into. The real performance is obtained when the cluster
    is hyperspherical, such as circles in a 2D space or spheres in a 3D space.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of clustering is that it helps you figure out the distinct,
    useful feature from the data and that it is flexible to changes.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is a part of ML that deals with taking necessary action
    in order to increase the reward for a particular situation. It employs several
    pieces of software and machines in order to find the best possible path for a
    specific situation.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is different from supervised learning. In supervised
    learning, training data is provided with a label, based on which it is trained.
    In the case of reinforcement learning, the reinforcement agent makes the decision
    to resolve the task that's been assigned to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of reinforcement learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Positive reinforcement**: Maximizes performance and sustains changes for
    a longer duration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Negative reinforcement**: Minimizes performance and sustains change for a
    shorter duration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of discovering hidden or predictive information from large datasets
    or databases is known as data mining. Data mining is a form of analysis that's
    conducted on data to discover new patterns and facts. These facts are used to
    discover knowledge and is also considered as a step toward **knowledge discovery
    in databases** (**KDD**).
  prefs: []
  type: TYPE_NORMAL
- en: Various processes and steps from AI, ML, statistics, database management systems,
    and more are often combined to search for the new pattern. With growing volumes
    of data and ML algorithms, there is always a tendency of finding new or hidden
    facts in the database. Facts and patterns that are found or searched for are then
    used to predict a certain outcome, and can also be applied in many fields, such
    as statistics, data visualization, marketing, management, medical, decision making
    systems, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis and data mining are often compared or talked about in tandem.
    Data mining is considered a part of the data analysis process. We will need some
    predefined hypotheses while working with data analysis since it's the process
    of organizing data to develop models and determine some insights. In terms of
    applied practices, data mining is mainly conducted on structured data, whereas
    data analysis can be done on structured, unstructured, or semi-structured data.
  prefs: []
  type: TYPE_NORMAL
- en: Data mining is based on scientific and mathematical methods, whereas data analysis
    uses analytics models and intelligence systems. When looking from a distance,
    both data analysis and data mining are subsets of data science, where data mining
    implements predictive algorithms to discover patterns and data analysis implements
    activities to gain some insights from datasets.
  prefs: []
  type: TYPE_NORMAL
- en: A major benefit of data mining is being able to process huge volumes of data
    in a short amount of time. It can also be implemented across new or existing platforms,
    predict hidden patterns or help in discovering them, help in decision-making,
    knowledge discovery, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks of data mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In general, data mining tasks are segregated into two types, also known as
    data mining analytics or data mining modeling. Both can be further categorized,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Predictive:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Descriptive:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This uses statistical analysis and turns data into valuable information. It
    predicts the probable future outcome of occurring situations. Prediction-related
    techniques that generate output by analyzing current and historical facts fall
    under this model.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is one of the most common mining techniques and classifies and categorizes
    samples before processing them to find facts. For more information on the classification
    and model evaluation procedure, please refer to the *Types of ML algorithms* section.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This technique is used to predict, forecast, and analyze information trends
    and the relationship between variables. For more information on regression, please
    refer to the *Types of ML algorithms* section.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This technique analyzes past events and predicts the possible missing or future
    values by using references from other data mining techniques such as clustering,
    classification, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Also known as the preliminary stage of data processing, it uses business intelligence
    and many other systems. This form of analytics is limited since it only analyzes
    past data and normally provides information about things that have already happened.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is a technique that's used to identify data that's similar to each
    other. For more information on clustering, please refer to the *Types of ML algorithms* section.
  prefs: []
  type: TYPE_NORMAL
- en: Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This provides a more compact representation of the dataset and includes visualization
    and report generation. Most management reporting regarding sales and marketing
    use this technique.
  prefs: []
  type: TYPE_NORMAL
- en: Association rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For more information on association, please refer to the *Types of ML algorithms* section.
  prefs: []
  type: TYPE_NORMAL
- en: What's next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web scraping is dynamic, demanding, and also a challenging task. We need to
    obey the legal perspective, which is presented on a website's **Terms of Services**
    (**ToS**) and Privacy Policy before carrying this task forward. Python programming,
    with its supportive nature, easy syntax, short and readable code formation, and
    the availability of libraries and tools is one of the best languages to be used
    in web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, the challenges are there and general scripts might not be able to fulfill
    the demand that exists. Sometimes, a scraping task might be for a huge volume,
    and personal PCs or laptops won''t be a place worth implementing when you consider
    time, machine resources, and more. There are a number of features and procedures
    that can make a scraping task more complicated and challenging. Let''s go over
    some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: The adoption of growing web-based security measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic loading of data and the involvement of scripting languages makes scraping
    complex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Presence of CAPTCHA, which can be found at [http://www.captcha.net/](http://www.captcha.net/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blocking a user's IP address (for simultaneous requests)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blocking requests from certain parts of the world (using and switching proxies
    might help)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For such cases, we can get help from organizations who are doing scraping-related
    work. These organizations can help us with our demand of data by charging certain
    fees and providing us with a web interface where we can process our demand. Such
    companies may be searched for in Google as `Web Scraping Services` or `Web Scraping
    Softwares`. There's also various browser-based extensions available that can be
    found by searching for `Scraping Extensions`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored and learned about the basic concepts regarding
    data management by using files, analysis, and visualization using pandas and matplotlib.
    We also introduced ML and data mining, and we also explored some related resources
    that can be helpful for further learning and career development.
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter, we come to the end of the book! Web scraping is a broad topic
    that is related directly or indirectly to a number of technologies and development
    techniques. Throughout this book, we have learned about numerous concepts in this
    domain by using the Python programming language. We can also explore more of the
    topics related to web scraping like ML, Data mining, Web scraping, AI and Python
    programming. These topics are worth exploring from a knowledge and career perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Artificial Intelligence: A Modern Approach*, at [http://aima.cs.berkeley.edu/](http://aima.cs.berkeley.edu/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine Learning*, at [http://www.cs.cmu.edu/~tom/mlbook.html](http://www.cs.cmu.edu/~tom/mlbook.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Mining and Analysis*, *Fundamental Concepts and Algorithms*, at [http://www.dataminingbook.info/pmwiki.php](http://www.dataminingbook.info/pmwiki.php)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Python data analysis library, at [https://pandas.pydata.org](https://pandas.pydata.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'matplotlib: Python plotting, at [https://matplotlib.org](https://matplotlib.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: File handling (Python), at [https://www.w3schools.com/python/python_file_handling.asp](https://www.w3schools.com/python/python_file_handling.asp)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introduction to Information Retrieval*, at [https://nlp.stanford.edu/IR-book/](https://nlp.stanford.edu/IR-book/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQLite, at [https://www.sqlite.org/index.html](https://www.sqlite.org/index.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MySQL, at [https://www.mysql.com/](https://www.mysql.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PostgreSQL, at [https://www.postgresql.org/](https://www.postgresql.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CAPTCHA, at [http://www.captcha.net/](http://www.captcha.net/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Overview of the KDD Process*, at [http://www2.cs.uregina.ca/~dbd/cs831/notes/kdd/1_kdd.html](http://www2.cs.uregina.ca/~dbd/cs831/notes/kdd/1_kdd.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
