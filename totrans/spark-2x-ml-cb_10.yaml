- en: Building Machine Learning Systems with Decision Tree and Ensemble Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting and preparing real-world medical data for exploring Decision Trees and
    Ensemble models in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a classification system with Decision Trees in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving regression problems with Decision Trees in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a classification system with Random Forest Trees in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving regression problems with Random Forest Trees in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a classification system with Gradient Boosted Trees (GBT) in Spark
    2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving regression problems with Gradient Boosted Trees (GBT) in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are one of the oldest and more widely used methods of machine
    learning in commerce. What makes them popular is not only their ability to deal
    with more complex partitioning and segmentation (they are more flexible than linear
    models) but also their ability to explain how we arrived at a solution and as
    to "why" the outcome is predicated or classified as a class/label.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark provides a good mix of decision tree based algorithms fully capable
    of taking advantage of parallelism in Spark. The implementation ranges from the
    straight forward Single Decision Tree (the CART type algorithm) to Ensemble Trees,
    such as Random Forest Trees and **GBT** (**Gradient Boosted Tree**). They all
    have both the variant flavors to facilitate classification (for example, categorical,
    such as height = short/tall) or regression (for example, continuous, such as height
    = 2.5 meters).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts a mind map that shows Spark ML library coverage
    of decision tree algorithms, as at the time of writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00242.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A quick way to think about the decision tree algorithm is as a smart partitioning
    algorithm that tries to minimize a loss function (for example, L2 or least square)
    as it partitions the ranges to come up with a segmented space which are best fitted
    decision boundaries to the data. The algorithm gets more sophisticated through
    the application of sampling the data and trying a combination of features to assemble
    a more complex ensemble model in which each learner (partial sample or feature
    combination) gets to vote toward the final outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts a simplified version in which a simple binary
    tree (stumping) is trained to classify the data into segments belonging to two
    different colors (for example, healthy patient/sick patient). The figure depicts
    a simple algorithm that just breaks the x/y feature space to one-half every time
    it establishes a decision boundary (hence classifying) while minimizing the number
    of errors (for example, a L2 least square measure):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00243.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following figure provides a corresponding tree so we can visualize the
    algorithm (in this case, a simple divide and conquer) against the proposed segmentation
    space. What makes decision tree algorithms popular is their ability to show their
    classification result in a language that can easily be communicated to a business
    user without much math:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00244.gif)'
  prefs: []
  type: TYPE_IMG
- en: A decision tree in Spark is a parallel algorithm designed to fit and grow a
    single tree into a dataset that can be categorical (classification) or continuous
    (regression). It is a greedy algorithm based on stumping (binary split, and so
    on) that partitions the solution space recursively while attempting to select
    the best split among all possible splits using Information Gain Maximization (entropy
    based).
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The other way to look at Spark's offering for decision trees is to think of
    the algorithm as belonging to two camps. The first camp, which we saw earlier
    in the introduction, concerns itself with single trees that attempt to find various
    techniques to find the best single tree for the dataset. While this is OK for
    a lot of datasets, the greedy nature of the algorithm can lead to unintended consequences,
    such as overfitting and going too deep to be able to capture all the boundaries
    within the training data (that is, it is over optimized).
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome the overfitting problem and to increase accuracy and the quality
    of predictions, Spark has implemented two classes of ensemble decision tree models
    that attempt to create many imperfect learners that either see a subset of data
    (sampling with or without substitution) and/or a subset of features. While each
    individual tree is less accurate, the collection of the trees'' assembled votes
    (or the average probability in the case of continuous variables) and the resultant
    averaging is much more accurate than any individual tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random Forest**: This method creates many trees in parallel and then votes/averages
    the outcome to minimize the overfitting problem prone in single tree algorithms.
    They are capable of capturing non-linearity and feature interaction without any
    scaling. They should be seriously considered at least as one of the first toolsets
    used to dissect the data and understand its makeup. The following figure provides
    a visual guideline for this implementation in Spark:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00245.gif)'
  prefs: []
  type: TYPE_IMG
- en: '**Gradient Boosted Trees**:This method is another ensemble model in which an
    average of many trees (even though they are less perfect) improves the accuracy
    and quality of the prediction. They differ from Random Forest in that they build
    one tree at a time and each tree tries to learn from the shortcomings of the previous
    tree by minimizing the loss function. They are similar to the concept of gradient
    descent, but they use the minimization (similar to gradient) to select and improve
    the next tree (they walk in the direction of the tree which creates the best accuracy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The three options for the loss function are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Log loss**: Negative likelihood for classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L2**: Least square for regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L1**: Absolute error for regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure provides an easy-to-use visualization reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00246.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The main packages for Decision Trees in Spark are in ML and are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Measures of impurity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With all machine learning algorithms, we are trying to minimize a set of cost
    functions which help us to select the best move. Spark uses three possible selections
    for maximization functions. The following figure depicts the alternatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00247.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'In this section, we will discuss each of the three possible alternatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Information gain**:Loosely speaking, this measures the level of impurity
    in a group based on the concept of entropy--see the Shannon information theory
    and then as later suggested by Quinlan in his ID3 algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The calculation of entropy is shown in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00248.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Information gain helps us to select an attribute in each feature vector space
    that can best help to separate the classes from each other. We use this attribute
    to decide how to order the attributes (thus, affecting the decision boundaries)
    in the nodes of a given tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts the calculation visually for easy understanding.
    In the first step, we want to select an attribute so that we maximize the IG (information
    gain) in the root or parent node, then build our child nodes for each value of
    the selected attribute (their associated vectors). We keep repeating the algorithm
    recursively untill we can no longer see any gains:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00249.gif)'
  prefs: []
  type: TYPE_IMG
- en: '**Gini Index: **This attempts to improve the IG (information gain) by isolating
    the classes so that the largest class is separated from the population. The Gini
    Index is a bit different to entropy, in that you try to have a 50/50 split and
    then apply further splits to infer the solution. It is meant to reflect the effect
    of one variable and it does not extend its reach to multi-attribute states. It
    uses a simple frequency count against the population. Use Gini for higher-dimensional
    and more noisy data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Gini Impurity where you have complex multi-dimensional data and you are
    trying to dissect a simple signal from the set.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, use information gain (or any entropy-based system) where
    you have a cleaner and low dimensional dataset, but you are looking for a more
    complex (in terms of accuracy and quality) dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00250.gif)'
  prefs: []
  type: TYPE_IMG
- en: '**Variance**: The variance is used to signal the regression model for the tree
    algorithm. In short, we still try to minimize an L2 function, but the difference
    is that here we seek to minimize the distance-squared of the observation and mean
    of the node (segment) being considered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure depicts a simplified version for visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00251.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Spark Model Evaluation Tools for evaluation classification and regression
    with tree models are as listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The confusion matrix is a table that is used to describe the performance of
    a classification model, out of the test dataset that the true values are known.
    The confusion matrix itself is relatively simple; it is a 2 x 2 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | **Prediction** | ** Value** |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual** | Yes | True Positive (TP) | False Negative (FN) |'
  prefs: []
  type: TYPE_TB
- en: '| **Value** | No | False Positive (FP) | True Negative (TN) |'
  prefs: []
  type: TYPE_TB
- en: 'For our cancer dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positives (TP):** Those are cases we predicted yes, and they did have
    breast cancer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative (TN):** Those are cases we predicted no, and they didn''t have
    breast cancer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positives (FP):** We predicted yes, but they didn''t have breast cancer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negatives (FN):** We predicted no, but they did have breast cancer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A good classification system should match the reality closely with good TP and
    TN values, while having fewer FP and FN values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, the following terms are also used as markers for a classification
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: The correctness ratio of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(TP + TN)/Total*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error**: Overall, the percentage that the model is wrong:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(FP+FN)/Total*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also equals to 1 - Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the Spark Machine Learning Library, there is a utility class to handle the
    calculation for the aforementioned common matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will use the utility class in the following sample code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, for the regression algorithm, **Mean Squared Error (MSE),** or average
    of the squares of the errors, is well utilized as a key parameter for the measurement
    of a model. In the Spark Machine Learning Library, there is also a utility class
    for it and it will provide the key indicator of a regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Documentation for the Spark Matrix Evaluator can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics)
    and [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics)[.](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics)
  prefs: []
  type: TYPE_NORMAL
- en: Getting and preparing real-world medical data for exploring Decision Trees and
    Ensemble models in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset used depicts a real-life application of Decision Trees in machine
    learning. We used a cancer dataset to predict what makes a patient's case malignant
    or not. To explore the real power of decision trees, we use a medical dataset
    that exhibits real life non-linearity with a complex error surface.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Wisconsin Breast Cancer** dataset was obtained from the University of
    Wisconsin Hospital from Dr. William H Wolberg. The dataset was gained periodically
    as Dr. Wolberg reported his clinical cases.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset can be retrieved from multiple sources, and is available directly
    from the University of California Irvine's web server [http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data](http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data)
  prefs: []
  type: TYPE_NORMAL
- en: The data is also available from the University of Wisconsin's web server [ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer1/datacum](ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer1/datacum)
  prefs: []
  type: TYPE_NORMAL
- en: The dataset currently contains clinical cases from 1989 to 1991\. It has 699
    instances, with 458 classified as benign tumors and 241 as malignant cases. Each
    instance is described by nine attributes with an integer value in the range of
    1 to 10 and a binary class label. Out of the 699 instances, there are 16 instances
    that are missing some attributes.
  prefs: []
  type: TYPE_NORMAL
- en: We will remove these 16 instances from the memory and process the rest (in total,
    683 instances) for the model calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sample raw data looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The attribute information is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **#** | **Attribute** | **Domain** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Sample code number | ID number |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Clump Thickness | 1 - 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Uniformity of Cell Size | 1 - 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Uniformity of Cell Shape | 1 - 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Marginal Adhesion | 1 - 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Single Epithelial Cell Size | 1 - 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Bare Nuclei | 1 - 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Bland Chromatin | 1 - 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Normal Nucleoli | 1 - 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Mitoses | 1 - 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | Class | (2 for benign, 4 for Malignant) |'
  prefs: []
  type: TYPE_TB
- en: 'If presented in the correct columns, it will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID Number** | **Clump Thickness** | **Uniformity of Cell Size** | **Uniformity
    of Cell Shape** | **Marginal Adhesion** | **Single Epithelial Cell Size** | **Bare
    Nucleoli** | **Bland Chromatin** | **Normal Nucleoli** | **Mitoses** | **Class**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1000025 | 5 | 1 | 1 | 1 | 2 | 1 | 3 | 1 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1002945 | 5 | 4 | 4 | 5 | 7 | 10 | 3 | 2 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1015425 | 3 | 1 | 1 | 1 | 2 | 2 | 3 | 1 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1016277 | 6 | 8 | 8 | 1 | 3 | 4 | 3 | 7 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1017023 | 4 | 1 | 1 | 3 | 2 | 1 | 3 | 1 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1017122 | 8 | 10 | 10 | 8 | 7 | 10 | 9 | 7 | 1 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 1018099 | 1 | 1 | 1 | 1 | 2 | 10 | 3 | 1 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1018561 | 2 | 1 | 2 | 1 | 2 | 1 | 3 | 1 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1033078 | 2 | 1 | 1 | 1 | 2 | 1 | 1 | 1 | 5 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1033078 | 4 | 2 | 1 | 1 | 2 | 1 | 2 | 1 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1035283 | 1 | 1 | 1 | 1 | 1 | 1 | 3 | 1 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1036172 | 2 | 1 | 1 | 1 | 2 | 1 | 2 | 1 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1041801 | 5 | 3 | 3 | 3 | 2 | 3 | 4 | 4 | 1 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 1043999 | 1 | 1 | 1 | 1 | 2 | 3 | 3 | 1 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1044572 | 8 | 7 | 5 | 10 | 7 | 9 | 5 | 5 | 4 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Wisconsin Breast Cancer dataset is widely used in the machine learning community.
    The dataset contains limited attributes and most of them are discrete numbers.
    It's very easy to apply a classification algorithm and regression model to the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: More than 20 research papers and publications already cite this dataset, and
    it is available publicly and very easy to use.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset has the multivariate datatype, where attributes are integers, and
    the number of attributes are only 10\. This makes it one of the typical datasets
    for classification and regression analysis for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Building a classification system with Decision Trees in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the breast cancer data and use classifications to
    demonstrate the Decision Tree implantation in Spark. We will use the IG and Gini
    to show how to use the facilities already provided by Spark to avoid redundant
    coding. This recipe attempts to fit a single tree using a binary classification
    to train and predict the label (benign (0.0) and malignant (1.0)) for the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and the Spark session so we can have access to
    the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We read in the original raw data file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We pre-process the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: First, we trim the line and remove any empty spaces. Once the line is ready
    for the next step, we remove the line if it's empty, or if it contains missing
    values ("?"). After this step, the 16 rows with missing data will be removed from
    the dataset in the memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then read the comma separated values into RDD. Since the first column in
    the dataset only contains the instance''s ID number, it is better to remove this
    column from the real calculation. We slice it out with the following command,
    which will remove the first column from the RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We then put the rest of the numbers into a dense vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the Wisconsin Breast Cancer dataset''s classifier is either benign cases
    (last column value = 2) or malignant cases (last column value = 4), we convert
    the preceding value using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'So the benign case 2 is converted to 0, and the malignant case value 4 is converted
    to 1, which will make the later calculations much easier. We then put the preceding
    row into a `Labeled Points`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We verify the raw data count and process the data count:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'And you will see the following on the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We split the whole dataset into training data (70%) and test data (30%) randomly.
    Please note that the random split will generate around 211 test datasets. It is
    approximately but NOT exactly 30% of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a metrics calculation function, which utilizes the Spark `MulticlassMetrics`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This function will read in the model and test dataset, and create a metric which
    contains the confusion matrix mentioned earlier. It will contain the model accuracy,
    which is one of the indicators for the classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define an evaluate function, which can take some tunable parameters for
    the Decision Tree model, and do the training for the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The evaluate function will read in several parameters, including the impurity
    type (Gini or Entropy for the model) and generate the metrics for evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We set the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Since we only have benign (0.0) and malignant (1.0), we put numClasses as 2\.
    The other parameters are tunable, and some of them are algorithm stop criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate the Gini impurity first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We evaluate the Entropy impurity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset is a bit more complex than usual, but apart from some extra steps,
    parsing it remains the same as other recipes presented in previous chapters. The
    parsing takes the data in its raw form and turns it into an intermediate format
    which will end up as a LabelPoint data structure which is common in Spark ML schemes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We use `DecisionTree.trainClassifier()` to train the classifier tree on the
    training set. We follow that by examining the various impurity and confusion matrix
    measurements to demonstrate how to measure the effectiveness of a tree model.
  prefs: []
  type: TYPE_NORMAL
- en: The reader is encouraged to look at the output and consult additional machine
    learning books to understand the concept of the confusion matrix and impurity
    measurement to master Decision Trees and variations in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To visualize it better, we included a sample decision tree work flow in Spark
    which will read the data into Spark first. In our case, we create the RDD from
    the file. We then split the dataset into training data and test data using a random
    sampling function.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the dataset is split, we use the training dataset to train the model,
    followed by test data to test the accuracy of the model. A good model should have
    a meaningful accuracy value (close to 1). The following figure depicts the workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00252.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A sample tree was generated based on the Wisconsin Breast Cancer dataset. The
    red spot represents malignant cases, and the blue ones the benign cases. We can
    examine the tree visually in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00253.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for the constructor can be found at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.DecisionTree](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.DecisionTree) and [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.model.DecisionTreeModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.model.DecisionTreeModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for the Spark Matrix Evaluator can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving Regression problems with Decision Trees in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to the previous recipe, we will use the `DecisionTree()` class to train
    and predict an outcome using a regression tree model. To refresh all these models
    is a variation on **CART** (**Classification and Regression Tree**), which comes
    in two modes. In this recipe, we explore the regression API for the decision tree
    implementation in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and Spark session so we can have access to the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We read in the original raw data file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We pre-process the dataset (see the preceding code for details):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We verify the raw data count and process the data count:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'And you will see the following on the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We split the whole dataset into training data (70%) and test data (30%) sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a metrics calculation function, which utilizes the Spark `RegressionMetrics`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We evaluate the Gini impurity first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the same dataset, but this time we use a Decision Tree to solve the
    regression problem with the data. Noteworthy is the creation of a metrics calculation
    function, which utilizes the Spark `RegressionMetrics()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We then proceed to perform the actual regression using `DecisionTree.trainRegressor()`
    and obtain the impurity measurement (GINI). We then proceed to output the actual
    regression, which is a series of decision nodes/branches and the value used to
    make a decision at the given branch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for the constructor can be found in the following URLs [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.DecisionTree](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.DecisionTree) and [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.model.DecisionTreeModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.model.DecisionTreeModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for the Spark Matrix Evaluator can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a classification system with Random Forest Trees in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore Random Forest implementation in Spark. We will
    use the Random Forest technique to solve a discrete classification problem. We
    found random forest implementation very fast due to Spark's exploitation of parallelism
    (growing many trees at once). We also do not need to worry too much about the
    hyper-parameters and technically we can get away with just setting the number
    of trees.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and Spark session so we can have access to the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We read in the original raw data file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We pre-process the dataset (see the preceding session for details):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We verify the raw data count and process the data count:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'And you will see the following in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We split the whole dataset into training data (70%) and test data (30%) randomly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a metrics calculation function, which utilizes the Spark `MulticlassMetrics`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This function will read in the model and the test dataset, and create metrics
    that contain the confusion matrix mentioned earlier. It will contain model accuracy,
    which is one of the indicators for the classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define an evaluate function, which can take some tunable parameters for
    the Random Forest model, and do the training for the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The evaluate function will read in several parameters, including the impurity
    type (Gini or Entropy for the model) and generate the metrics for evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We set the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We evaluate the Gini impurity first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We evaluate the Entropy impurity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data is the same as the data in the previous recipe, but we use Random
    Forest and the Multi metrics API to solve the classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RandomForest.trainClassifier()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MulticlassMetrics()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have a lot of options with Random Forest Trees that we can adjust to get
    the right edges for classifying complex surfaces. Some of the parameters are listed
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Noteworthy is the confusion matrix in this recipe. The confusion matrix is
    obtained via the `MulticlassMetrics()` API call. To interpret the preceding confusion
    metrics, accuracy is equal to (118+ 59)/ 182 for all test cases, and error is
    equal to 1 -accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for the constructor can be found in the following URLs [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.RandomForest$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.RandomForest%24) [and ](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.RandomForest%24)[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.model.RandomForestModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.model.RandomForestModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for the Spark Matrix Evaluator can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving regression problems with Random Forest Trees in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is similar to the previous recipes, but we use Random Forest Trees to
    solve a regression problem (continuous). The following parameter is used to direct
    the algorithm to apply regression rather than classification. We again limit the
    number of classes to two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages from Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We read in the original raw data file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We pre-process the dataset (see the preceding session for details):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We split the whole dataset into training data (70%) and test data (30%) randomly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'And you will see the following on the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a metrics calculation function, which utilizes the Spark `RegressionMetrics`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the dataset and Random Forest Tree to solve a regression problem with
    the data. The mechanics of parsing and separating remains the same, but we use
    the following two APIs to do the tree regression and evaluate the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RandomForest.trainRegressor()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RegressionMetrics()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Noteworthy is the definition of the `getMetrics()` function to utilize the
    `RegressionMetrics()` facility in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We also set the impurity value to "variance" so we can use the variance for
    measuring errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for the constructor can be found at the following URLs [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.RandomForest$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.RandomForest%24) and [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.model.RandomForestModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.model.RandomForestModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for Spark Matrix Evaluator: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a classification system with Gradient Boosted Trees (GBT) in Spark
    2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore the Gradient Boosted Tree (GBT) classification
    implementation in Spark. The GBT requires more care with hyper-parameters and
    several tries before deciding the final outcome. One must remember that it is
    completely OK to grow shorter trees if using GBT.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and Spark session so we can have access to the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'We read in the original raw data file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We pre-process the dataset (see the preceding session for details):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'We split the whole dataset into training data (70%) and test data (30%) randomly.
    Please note that the random split will generate around 211 test datasets. It''s
    approximately but NOT exactly 30% of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'And you will see the on the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a metrics calculation function, which utilizes the Spark `MulticlassMetrics`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'We define an evaluate function, which can take some tunable parameters for
    the Gradient Boosted Trees model, and do the training for the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'We evaluate the model using the preceding Strategy parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: How it works....
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We skip the data ingestion and parsing since it is similar to previous recipes,
    but what is different is how we set up the parameters, especially the use of "classification"
    as a parameter that we pass into `BoostingStrategy.defaultParams()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'We also use the `evaluate()` function to evaluate the parameters by looking
    at impurity and the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to remember that the GBT is a multi-generational algorithm with
    the twist that we grow one tree at the time, learn from our mistakes, and then
    build the next tree in an iterative way.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for the constructor can be found at the following URLs [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.GradientBoostedTrees](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.GradientBoostedTrees),
     [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.configuration.BoostingStrategy](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.configuration.BoostingStrategy) and [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.model.GradientBoostedTreesModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.model.GradientBoostedTreesModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for the Spark Matrix Evaluator can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving regression problems with Gradient Boosted Trees (GBT) in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe is similar to the GBT classification problem, but we will use regression
    instead. We will use `BoostingStrategy.defaultParams()` to direct the GBT to use
    regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '``package spark.ml.cookbook.chapter10``.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages for the Spark context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'We read in the original raw data file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'We pre-process the dataset (see the preceding session for details):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'We split the whole dataset into training data (70%) and test data (30%) randomly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'And you will see the following in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a metrics calculation function, which utilizes the Spark `RegressionMetrics`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'We evaluate the model using the preceding Strategy parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We used the same GBT tree as the previous recipe, but we adjusted the parameters
    to direct the GBT API to perform regression as opposed to classification. It is
    noteworthy to compare the following code with the previous recipe. "Regression"
    is used to direct the GBT to perform regression on the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the following API to train and evaluate the metrics from the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '`GradientBoostedTrees.train()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getMetrics()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following snippet shows a typical output needed to examine the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GBT can capture non-linearity and variable interaction in the same manner as
    Random Forest and can deal with multi-class labels as well.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for the constructor can be found at the following URLs: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.GradientBoostedTrees](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.GradientBoostedTrees), [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.configuration.BoostingStrategy](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.configuration.BoostingStrategy),
    and [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.model.GradientBoostedTreesModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.tree.model.GradientBoostedTreesModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for the Spark Matrix Evaluator can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
