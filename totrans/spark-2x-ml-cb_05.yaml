- en: Practical Machine Learning with Regression and Classification in Spark 2.0 -
    Part I
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a linear regression line to data the old-fashioned way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalized linear regression in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression API with Lasso and L-BFGS in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression API with Lasso and auto optimization selection in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression API with ridge regression and auto optimization selection
    in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isotonic regression in Apache Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilayer perceptron classifier in Apache Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One versus Rest classifier (One-vs-All) in Apache Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Survival regression - parametric AFT model in Apache Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter, along with the next chapter, covers the fundamental techniques
    for regression and classification available in Spark 2.0 ML and MLlib library.
    Spark 2.0 highlights a new direction by moving the RDD-based regressions (see
    the next chapter) to maintenance mode while emphasizing **Linear Regression**
    and **Generalized Regression** going forward.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, the new API design favors parameterization of elastic net to
    produce the ridge versus Lasso regression and everything in between, as opposed
    to a named API (for example, `LassoWithSGD`). The new API approach is a much cleaner
    design and forces you to learn elastic net and its power when it comes to feature
    engineering that remains an art in data science. We provide adequate examples,
    variations, and notes to guide you through the complexities in these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts the regression and classification coverage (part
    1) in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00104.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: First, you will learn how to implement linear regression using algebraic equations
    via just Scala code and RDDs from scratch to get an insight for the math and why
    we need an iterative optimization method to estimate the solution for a large
    system of regressions. Second, we explore the **generalized linear model** (**GLM**)
    and its various statistical distribution families and link functions while stressing
    its limitation to 4,096 parameters only in the current implementation. Third,
    we tackle the **linear regression model** (**LRM**) and how to use the elastic
    net parameterization to mix and match L1 and L2 penalty functions to achieve logistic,
    ridge, Lasso, and everything in between. We also explore the solver (that is,
    optimizer) method and how to set it to use L-BFGS optimization, auto optimizer
    selection, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: After exploring the GLM and linear regression recipes, we proceed to provide
    recipes for more exotic regression/classification methods such as isotonic regression,
    multilayer perceptron (that is, form of neuron net), One-vs-Rest, and survival
    regression to demonstrate Spark 2.0's power and completeness to deal with cases
    that are not addressed by linear techniques. With the increased risks in the financial
    world in the early 21^(st) century and new advancements in genome, Spark 2.0 also
    pulls together four important methods (isotonic regression, multilayer perceptron,
    One-vs-Rest, and survival regression or parametric ATF) in an easy to use machine
    learning library. The parametric ATF method at scale should be of particular interest
    to financial, data scientist, or actuarial professionals alike.
  prefs: []
  type: TYPE_NORMAL
- en: Even though some of these methods such as `LinearRegression()` API, have theoretically
    been available since 1.3x+, it is important to note that Spark 2.0 pulls all of
    them together in an easy-to-use and maintainable API (that is, backward compatibility)
    in a glmnet R-like manner as they move the RDD-based regression API into maintenance
    mode. The L-BFGS optimizer and normal equations take a front seat while SGD is
    available in RDD-based APIs for backward compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'Elastic net is the preferred method that can not only deal with L1 (Lasso regression)
    and L2 (ridge regression) in absolute terms prefered method for regularization,
    but also provide a dial-like mechanism that enables the user to fine-tune the
    penalty function (parameter shrinkage versus selection). While we recall using
    the elastic net function in 1.4.2, Spark 2.0 pulls it all together without the
    need to deal with each individual API for parameter tuning (important when selecting
    a model dynamically based on the latest data). As we start diving into the recipes,
    we strongly encourage the user to explore various parameter settings `setElasticNetParam()`
    and `setSolver()` configurations to master these powerful APIs. It is important
    not to mix the penalty function `setElasticNetParam(value: Double)` (L1 , L2,
    OLs, elastic net: linearly mixed L1/L2), which are regular or model penalty schemes
    with optimization (normal, L-BFGS, auto, and so on) techniques that are related
    to cost function optimization techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: It is critical to note that the RDD-based regressions are still very important
    since there are a lot of current ML implementation systems that rely heavily on
    the previous API regime and its SGD optimizer. Please see the next chapter for
    complete treatment with teaching notes covering RDD-based regressions.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a linear regression line to data the old fashioned way
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we use RDDs and a closed form formula to code a simple linear
    equation from scratch. The reason we use this as the first recipe is to demonstrate
    that you can always implement any given statistical learning algorithm via the
    RDDs to achieve computational scale using Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for `SparkSession` to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a `SparkSession` specifying configurations with the builder pattern
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Set output level to `ERROR` to reduce Spark''s output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We create two arrays representing the dependent (that is, `y`) and an independent
    variable (that is, `x`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We use `sc.parallelize(x)` to transform the two arrays to RDDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we demonstrate the `zip()` method of an RDD that creates pairs
    of dependent/independent tuples *(y,x)* from the two RDDs. We introduce this function
    since you must often learn to work with pairs in machine learning algorithms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To make sure we understand the `zip()` functionality, let''s take a look at
    the output, but make sure you include the `collect()` or some other form of action
    to make sure the data is presented in order. If we do not use an action method,
    the output from RDDs will be random:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00105.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'This is an important step that demonstrates how to iterate, access, and compute
    on each individual member of the pair. In order to compute the regression line,
    we need to compute sum, product, and averages (that is, *sum(x)*, *sum(y)*, and
    *sum (x * y)*). The `map(_._1).sum()` function is a mechanism in which the RDD
    pairs are iterated upon, but only the first elements are considered:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step we continue with computing the averages of individual RDD''s pair
    member along with their product. These individual computations (that is, *mean(x)*,
    *mean(y)*, and *mean(x*y)*), along with mean squared, will be used to compute
    the slope and intercept of the regression line. While we could have computed the
    mean from the previous statistics in the previous steps manually, we should make
    sure we are familiar with methods that are available intrinsically via an RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the final step, in which we compute the mean of `x` and `y` squared:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We print the statistic for completeness:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We compute the `numerator` and `denominator` for the formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We finally compute the slope of the regression line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We now calculate the intercept and print. If you do not want the intercept
    (intercept to be set to `0`), then the formula for the slope needs to be slightly
    modified. You can look for more details in other sources such as the internet
    and find the required equation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the slope and intercept, we write the regression line equation as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We declared two Scala arrays, parallelized them into two RDDs that are separate
    vectors of `x()` and `y()`. We then used the `zip()` method from the RDD API to
    produce a paired (that is, zipped) RDD. It results in an RDD in which each member
    is an *(x , y)* pair. We then proceed to calculate the mean, sum, and so on, and
    apply the closed form formula as described to find the intercept and slope for
    the regression line.
  prefs: []
  type: TYPE_NORMAL
- en: In Spark 2.0, the alternative would have been to use the GLM API out of the
    box. It is worth mentioning that the maximum number of parameters for a closed
    normal form scheme supported by GLM is limited to 4,096.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used a closed form formula to demonstrate that a regression line associated
    with a set of numbers *(Y1, X1), ..., (Yn, Xn)* is simply the line that minimizes
    the sum of the square errors. In a simple regression equation, the line is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Slope of the regression line ![](img/00106.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offset of the regression line ![](img/00107.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The equation for the regression line ![](img/00108.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A regression line is simply the best fit line that minimizes the sum of the
    square error. For a set of points (dependent variable, independent variable),
    there are many lines that can pass through these points and capture the general
    linear relationship, but only one of those lines is the line that minimizes all
    the errors from such a fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the example, we presented the line *Y = 1.21 + .9153145 * X*. Shown in
    the following figure is such a line and we computed the slope and the offset with
    a closed form formula. The linear model depicted by the linear equation of a line
    represents our best linear model (*slope=.915345*, *intercept= 1.21*) for the
    given data using closed form formulas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00109.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The data points plotted in the preceding figure are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It should be noted that not all regression forms have a closed form formula
    or become very inefficient (that is, impractical) with a large number of parameters
    on large datasets - this is the reason we use optimization techniques such as
    SGD or L-BFGS.
  prefs: []
  type: TYPE_NORMAL
- en: It is critical to recall from the previous recipes that you should make sure
    you cache any RDD or data structure associated with machine learning algorithms
    to avoid lazy instantiation due to the way Spark optimizes and maintains lineage
    (that is, lazy instantiation).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We recommend a book from Stanford University, which can be downloaded from
    the following site for free. It is a classic and a must-read whether you are a
    new or advanced practitioner in the field:'
  prefs: []
  type: TYPE_NORMAL
- en: The Elements of Statistical Learning, Data Mining, Inference, and Prediction,
    Second Edition, by Hastie, Tibshirani, and Friedman (2009). Springer-Verlag ([http://web.stanford.edu/~hastie/ElemStatLearn/](http://web.stanford.edu/~hastie/ElemStatLearn/)).
  prefs: []
  type: TYPE_NORMAL
- en: Generalized linear regression in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe covers the **generalized regression model** (**GLM**) implementation
    in Spark 2.0\. There is a great parallel between this `GeneralizedLinearRegression`
    in Spark 2.0 and `glmnet` implementation in R. This API is a welcome addition
    that allows you to select and set both distribution family (for example, Gaussian)
    and link functions (for example, inverse log) with a coherent and well-designed
    API.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use a housing dataset from the UCI machine library depository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the entire dataset from the following URLs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset is comprised of 14 columns with the first 13 columns being the independent
    variables (that is, features) that try to explain the median price (that is, last
    column) of an owner-occupied house in Boston, USA.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have chosen and cleaned the first eight columns as features. We use the
    first 200 rows to train and predict the median price:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CRIM**: Per capita crime rate by town'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ZN**: Proportion of residential land zoned for lots over 25,000 sq.ft'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**INDUS**: Proportion of non-retail business acres per town'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CHAS**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NOX**: Nitric oxide concentration (parts per 10 million)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RM**: Average number of rooms per dwelling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AGE**: Proportion of owner-occupied units built prior to 1940'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please use the `housing8.csv` file and make sure you move it to the following
    directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for `SparkSession` to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a `SparkSession` specifying configurations to gain access to the
    Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to import implicits for data conversion routines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we load housing data into a dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s parse the housing data and convert it into label points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now display the loaded data using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we configure a generalized linear regression algorithm for generating
    a new model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Feel free to experiment with different parameters for better fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'We fit the model to the housing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we retrieve the summary data to judge the accuracy of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print out the summary statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the program by stopping the `SparkSession`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we showed a generalized linear regression algorithm in action.
    We began by loading and parsing a CSV file into a dataset. Next, we created a
    generalized linear regression algorithm and generated a new model by passing our
    dataset to the `fit()` method. Once the fit operation was completed, we retrieved
    summary statistics from the model and displayed computed values to reconcile accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we explored fitting the data with a *Gaussian* distribution
    and *Identity*, but there are many more configurations that we can use to solve
    a specific regression fit, which are explained in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The GLM in Spark 2.0 is a general-purpose regression model that can support
    many configurations. We are impressed with the number of families available as
    of the initial release of Spark 2.0.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note as of Spark 2.0.2:'
  prefs: []
  type: TYPE_NORMAL
- en: The maximum number of parameters for a regression is currently limited to 4,096
    max.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only optimization (that is, solver) currently supported is **iteratively
    reweighted least squares** (**IRLS**), which is also the default solve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you set the solver to *auto*, it defaults to IRLS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `setRegParam()` sets the regularization parameter for L2 regularization.
    The regularization term is *0.5 * regParam * L2norm(coefficients)^2* per Spark
    2.0 documentation - make sure you understand the implications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are not sure how to handle the distribution fitting, we highly recommend
    one of our favorite books, *Handbook of Fitting Statistical Distributions with
    R*, that has served us well when modeling agricultural commodities such as CBOT
    Wheat, which has a reverse volatility smile curve (very different from equities).
  prefs: []
  type: TYPE_NORMAL
- en: 'Configuration and available options are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Be sure to experiment with different families and link functions to make sure
    your assumption of the underlying distribution is correct.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for `GeneralizedLinearRegression()` is available at the following
    link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.GeneralizedLinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.GeneralizedLinearRegression)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the important API calls within `GeneralizedLinearRegression`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`def **setFamily**(value: String): GeneralizedLinearRegression.this.type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`def **setLink**(value: String): GeneralizedLinearRegression.this.type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`def **setMaxIter**(value: Int): GeneralizedLinearRegression.this.type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`def **setRegParam**(value: Double): GeneralizedLinearRegression.this.type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`def **setSolver**(value: String): GeneralizedLinearRegression.this.type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`def **setFitIntercept**(value: Boolean): GeneralizedLinearRegression.this.type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The solver is currently IRLS; a quick reference can be found at the following
    link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For complete understanding of the new approach with GLM and linear regression
    in Spark 2.0+, please be sure to consult and understand CRAN glmnet implementation
    in R:'
  prefs: []
  type: TYPE_NORMAL
- en: Main page is available at [https://cran.r-project.org/web/packages/glmnet/index.html](https://cran.r-project.org/web/packages/glmnet/index.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User guide is available at [https://cran.r-project.org/web/packages/glmnet/glmnet.pdf](https://cran.r-project.org/web/packages/glmnet/glmnet.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression API with Lasso and L-BFGS in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will demonstrate the use of Spark 2.0's `LinearRegression()`
    API to showcase a unified/parameterized API to tackle the linear regression in
    a comprehensive way capable of extension without backward-compatibility issues
    of an RDD-based named API. We show how to use the `setSolver()` to set the optimization
    method to first-order memory-efficient L-BFGS, which can deal with numerous amount
    of parameters (that is, especially in sparse configuration) with ease.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, the `.setSolver()` is set to `lbgfs`, which makes the L-BFGS
    (see RDD-based regression for more detail) the selected optimization method. The
    `.setElasticNetParam()` is not set, so the default of `0` remains in effect, which
    makes this a Lasso regression.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use a housing dataset from the UCI machine library depository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the entire data set from the following URLs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset is comprised of 14 columns with the first 13 columns being independent
    variables (that is, features) that try to explain the median price (that is, last
    column) of an owner-occupied house in Boston, USA.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have chosen and cleaned the first eight columns as features. We use the
    first 200 rows to train and predict the median price:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CRIM**: Per capita crime rate by town'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ZN**: Proportion of residential land zoned for lots over 25,000 sq.ft.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**INDUS**: Proportion of non-retail business acres per town'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CHAS**: Charles River dummy variable (`= 1` if tract bounds river; `0` otherwise)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NOX**: Nitric oxide concentration (parts per 10 million)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RM**: Average number of rooms per dwelling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AGE**: Proportion of owner-occupied units built prior to 1940'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please use the `housing8.csv` file and make sure you move it to the following
    directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the `SparkSession` to gain access to the
    cluster and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a `SparkSession` specifying configurations to gain access to the
    Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to import implicits for data conversion routines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we load the housing data into a dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s parse the housing data and convert it into label points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now display the loaded data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we configure a linear regression algorithm for generating a model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we fit the model to the housing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we retrieve summary data to reconcile the accuracy of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print out the summary statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the program by stopping the `SparkSession`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we use the housing data again to demonstrate the Spark 2.0 `LinearRegression()` API
    using the L-BFGS optimization option. We read the file in, parse the data, and
    select specific columns for the regression. We keep the recipe short by accepting
    default parameters, but set number of iterations (for convergence to a solution)
    and optimization method to `lbfgs` before running the `.fit()` method. We then
    proceed to output a couple of quick metrics (that is, MSE and RMSE) for demonstration
    only. We show how to implement/compute these metrics yourself with RDD. Using
    Spark 2.0 native facilities/metrics and RDDs-based regression recipes, we show
    how Spark can do these metrics out of the box now, which is testimony to how far
    we have come from Spark 1.0.1!
  prefs: []
  type: TYPE_NORMAL
- en: Using Newton's optimization technique (for example, `lbfgs`) for small number
    of columns is an overkill, which will be demo later in this book to enable the
    readers to use these recipes on large datasets in real-world settings (for example,
    typical cancer/genome data readily available from sources mentioned in [Chapter
    1](part0027.html#PNV60-4d291c9fed174a6992fd24938c2f9c77), *Practical Machine Learning
    with Spark Using Scala*).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elastic net (contributed by DB Tsai and others) and evangelized by Alpine Labs
    showed up in our radar starting with Spark 1.4 and 1.5, which is now the de facto
    technique in Spark 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: To level set, elastic net is a linear combination of L1 and L2 penalty. It can
    be modeled conceptually as a dial that can decide how much of L1 and how much
    of L2 to include in the penalty (Shrinkage versus Selection).
  prefs: []
  type: TYPE_NORMAL
- en: We want to stress that we can now select between the type of regression via
    the parameter setting rather than named APIs. This is an important departure from
    RDD-based APIs (that is, now in maintenance mode) that we demonstrate later in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The following table provides a quick cheat sheet for setting parameters to select
    between Lasso, Ridge, OLS, and elastic net.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please see the following table `setElasticNetParam(value: Double)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Regression Type** | **Penalty** | **Parameter** |'
  prefs: []
  type: TYPE_TB
- en: '| Lasso | L1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Ridge | L2 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Elastic net | L1 + L2 | 0.0 < alpha < 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| OLS | Ordinary least square | None |'
  prefs: []
  type: TYPE_TB
- en: 'It is crucial to understand how regularization is controlled via an elastic
    net parameter (corresponding to Alpha) described in this following brief treatment:'
  prefs: []
  type: TYPE_NORMAL
- en: Simple: [https://en.wikipedia.org/wiki/Elastic_net_regularization](https://en.wikipedia.org/wiki/Elastic_net_regularization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complete: [http://www.stat.purdue.edu/~tlzhang/mathstat/ElasticNet.pdf](http://www.stat.purdue.edu/~tlzhang/mathstat/ElasticNet.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using genome data: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3232376/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3232376/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `LinearRegression()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be sure to consult the actual source code since it extends the *Regressor* itself: [https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/ml/regression/LinearRegression.scala](https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/ml/regression/LinearRegression.scala)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of the important API calls within `LinearRegression`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`def setElasticNetParam(value: Double): LinearRegression.this.type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`def **setRegParam**(value: Double): LinearRegression.this.type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`def **setSolver**(value: String): LinearRegression.this.type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`def **setMaxIter**(value: Int): LinearRegression.this.type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`def **setFitIntercept**(value: Boolean): LinearRegression.this.type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An important aspect of Spark ML is its simple yet exceptionally powerful API
    set that allows scaling to billions of rows at very little extra effort by the
    developer on an existing cluster. You will be surprised the scale at which Lasso
    can be used to discover relevant feature sets while L-BFGS optimization (that
    does not require a direct hessian matrix presence) can handle a tremendous number
    of features with ease. The details of the `updater` implementation for LBFGS in
    Spark 2.0 source code is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover the optimizations that relate to these ML algorithms in a follow-up
    chapter due to their complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression API with Lasso and 'auto' optimization selection in Spark
    2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we build on the previous recipe `LinearRegression` by selecting
    LASSO regression explicitly via the `setElasticNetParam(0.0)` while letting Spark
    2.0 pick the optimization on its own using `setSolver('auto')`*. *We remind again
    that the RDD-based regression API is now in maintenance mode and this is the preferred
    method going forward.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use a housing data set from the UCI machine library depository..
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the entire data set from the following URLs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset is comprised of 14 columns with the first 13 columns being the independent
    variables (that is, features) that try to explain the median price (that is, last
    column) of an owner-occupied house in Boston, USA.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have chosen and cleaned the first eight columns as features. We use the
    first 200 rows to train and predict the median price:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CRIM:** Per capita crime rate by town'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ZN:** Proportion of residential land zoned for lots over 25,000 sq.ft.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**INDUS:** Proportion of non-retail business acres per town'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CHAS:** Charles River dummy variable (`= 1` if tract bounds river; `0` otherwise)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NOX:** Nitric oxide concentration (parts per 10 million)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RM:** Average number of rooms per dwelling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AGE**: Proportion of owner-occupied units built prior to 1940'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please use the `housing8.csv` file and make sure you move it to the following
    directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the `SparkSession` to gain access to the
    cluster and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a `SparkSession` specifying configurations to gain access to the
    Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to import implicits for data conversion routines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we load the housing data into a dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s parse the housing data and convert it into label points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now display the loaded data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00113.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we configure a linear regression algorithm for generating a model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we fit the model to the housing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we retrieve summary data to reconcile the accuracy of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print out the summary statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the program by stopping the `SparkSession`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We read the housing data and load selected columns and use them to predict
    the price of a housing unit. We use the following code snippet to select the regression
    as LASSO and let Spark pick up the optimization on its own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: We change the `setMaxIter()` to `1000` for demonstration purposes. The default
    setting is `100` out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While Spark does a very nice implementation of L-BFGS, please see the following
    links for a quick understanding of BFGS and its inner workings as it relates to
    this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: Simple treatment of BFGS: [https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm](https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the *Journal of Machine Learning Research* and also nice limited-memory
    BGFS treatment from a mathematical programming viewpoint: [http://www.jmlr.org/papers/volume14/hennig13a/hennig13a.pdf](http://www.jmlr.org/papers/volume14/hennig13a/hennig13a.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also see the RDD-based regression recipes for more details on LBGFS. The following
    links provide implementation details if you need to understand the BFGS techniques.
  prefs: []
  type: TYPE_NORMAL
- en: This implementation in C language helps us to develop a solid understanding
    of the first-order optimization at code level: [http://www.chokkan.org/software/liblbfgs/](http://www.chokkan.org/software/liblbfgs/)
  prefs: []
  type: TYPE_NORMAL
- en: The *cctbx* also provides good implementation details if you need to see more: [http://cctbx.sourceforge.net](http://cctbx.sourceforge.net)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pretty good treatment from Harvard University on L-BFGS in R: [https://cran.r-project.org/web/packages/lbfgs/vignettes/Vignette.pdf](https://cran.r-project.org/web/packages/lbfgs/vignettes/Vignette.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `LinearRegression()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Documentation for BFGS and L-BFGS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm](https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Limited-memory_BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression API with ridge regression and 'auto' optimization selection
    in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we implement ridge regression using the `LinearRegression` interface.
    We use the elastic net parameter to set the appropriate value to a full L2 penalty,
    which in turn selects the ridge regression accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use a housing data set from the UCI machine library depository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the entire data set from the following URLs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset is comprised of 14 columns with the first 13 columns being the independent
    variables (that is, features) that try to explain the median price (that is, last
    column) of an owner-occupied house in Boston, USA.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have chosen and cleaned the first eight columns as features. We use the
    first 200 rows to train and predict the median price:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CRIM**: Per capita crime rate by town'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ZN**: Proportion of residential land zoned for lots over 25,000 sq.ft.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**INDUS**: Proportion of non-retail business acres per town'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CHAS**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NOX**: Nitric oxide concentration (parts per 10 million)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RM**: Average number of rooms per dwelling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AGE**: Proportion of owner-occupied units built prior to 1940'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please use the `housing8.csv` file and make sure you move it to the following
    directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for `SparkSession` to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a `SparkSession` specifying configurations to gain access to the
    Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to import implicits for data conversion routines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we load the housing data into a dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s parse the housing data and convert it into label points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now display the loaded data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we configure a linear regression algorithm for generating a model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we fit the model to the housing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we retrieve the summary data to reconcile the accuracy of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print out the summary statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the program by stopping the `SparkSession`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We loaded the data by reading the housing data and loading the appropriate
    columns. We then proceeded to set the parameters that will force the `LinearRegression()`
    to perform a Ridge regression while keeping the optimization to ''auto''. The
    following code shows how the linear regression API can be used to set the desired
    type of regression to ridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: We then used `.fit()` to fit the model to the data. We finally used `.summary`
    to extract the model summary and print the MSE and RMSE for the model.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make sure we are clear on the difference between ridge and Lasso regression,
    we must first highlight the difference between parameter shrinkage (that is, we
    squash the weight using a square root function, but never set it to zero) and
    feature engineering or parameter selection (that is, we shrink the parameters
    all the way to `0`, thereby causing some of the parameters to disappear altogether
    from the model):'
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression: [https://en.wikipedia.org/wiki/Tikhonov_regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lasso regression: [https://en.wikipedia.org/wiki/Lasso_(statistics)](https://en.wikipedia.org/wiki/Lasso_(statistics))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elastic net - Stanford University: [http://web.stanford.edu/~hastie/TALKS/enet_talk.pdf](http://web.stanford.edu/~hastie/TALKS/enet_talk.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation on Linear regression: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression)
  prefs: []
  type: TYPE_NORMAL
- en: Isotonic regression in Apache Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate the `IsotonicRegression()` function in Spark
    2.0\. The isotonic or monotonic regression is used when order is expected in the
    data and we want to fit an increasing ordered line (that is, manifest itself as
    a step function) to a series of observations. The terms **isotonic regression**
    (**IR**) and **monotonic regression** (**MR**) are synonymous in literature and
    can be used interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: In short, what we are trying to do with the `IsotonicRegression()` recipe is
    to provide a better fit versus some of the shortcomings of Naive Bayes and SVM.
    While they are both powerful classifiers, Naive Bayes lacks a good estimate of
    P (C | X) and **Support Vector Machines** (**SVM**) at best provides only a proxy
    (can use hyperplane distance), which is not an accurate estimator in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go to the website to download the file and save the file into the data path
    mentioned in the following code blocks. We use the famous Iris data and fit a
    step line to the observation. We use the Iris data in `LIBSVM` format from the
    library to demonstrate the IR.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The filename we choose is `iris.scale.txt` [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale).
  prefs: []
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for `SparkSession` to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a `SparkSession` specifying configurations with the builder pattern
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'We then read the data file in, print out the data schema, and display the data
    in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We then split the data set to training and test set in a ratio of *0.7:0.3*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the `IsotonicRegression` object and fit it in the training
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we print out the model boundary and predictions in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'We let the model transform the test data and display the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00116.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the `SparkSession`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we explored the features of the Isotonic Regress model. We
    first read the dataset file into Spark in a `libsvm` format. We then split the
    data (*70/30*) and proceeded. Next, we displayed the DataFrame in the console
    by calling the `.show()` function. We then created the `IsotonicRegression()`
    object and let the model run for itself by calling the `fit(data)` function. In
    this recipe, we kept it simple and did not change any default parameters, but
    the readers should experiment and use the JChart package to graph the line and
    see the effect on the increasing and stepped line result in action.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we displayed the model boundary and predictions in the console and
    used the model to transform the test dataset and displayed the result DataFrame
    in the console with the prediction field included. All Spark ML algorithms are
    sensitive to hyper parameter value. While there are no hard and fast rules for
    setting these parameters, a good amount of experimentation using the scientific
    method is required before going to production.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered a good number of model evaluation facilities provided by Spark
    in previous chapters and have discussed evaluation metrics throughout the book
    without being redundant. Spark provides the following model evaluation methods.
    A developer must pick and choose the specific evaluation metric facility based
    on the type of algorithm being evaluated (for example, discrete, continuous, binary,
    multiclass, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: We will cover the evaluation metrics individually using recipes, but please
    see the following link for Spark's model evaluation coverage: [http://spark.apache.org/docs/latest/mllib-evaluation-metrics.html](http://spark.apache.org/docs/latest/mllib-evaluation-metrics.html).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Spark 2.0 implementation has the following restrictions at the time of
    writing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Only single feature (that is, univariate) algorithms are supported:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation is currently set to **parallelized pool adjacent violators
    algorithm** (**PAVA**):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As of Spark 2.1.0, it is a univariate monotonic implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See CRAN implementation like Spark 2.0: [https://cran.r-project.org/web/packages/isotone/vignettes/isotone.pdf](https://cran.r-project.org/web/packages/isotone/vignettes/isotone.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See UCLA paper (PAVA): [http://gifi.stat.ucla.edu/janspubs/2009/reports/deleeuw_hornik_mair_R_09.pdf](http://gifi.stat.ucla.edu/janspubs/2009/reports/deleeuw_hornik_mair_R_09.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See University of Wisconsin: [https://www.biostat.wisc.edu/sites/default/files/tr_116.pdf](https://www.biostat.wisc.edu/sites/default/files/tr_116.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Documentation for Isotonic regression:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/ml-classification-regression.html#isotonic-regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#isotonic-regression)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.IsotonicRegression](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.IsotonicRegression)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.IsotonicRegressionModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.IsotonicRegressionModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'More information about Isotonic Regression can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Isotonic_regression](https://en.wikipedia.org/wiki/Isotonic_regression)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The isotonic regression line ends up being a step function as opposed to a
    linear regression, which is a straight line. The following figure (source: Wikipedia)
    provides a good reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00117.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Multilayer perceptron classifier in Apache Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore Spark's 2.0 **multilayer perceptron classifier**
    (**MLPC**), which is another name for feed-forward neural networks. We use the
    iris data set to predict a binary outcome for the feature vectors that describes
    the input. The key point to remember is that, even though the name sounds a bit
    complicated, at its core the MLP is just a non-linear classifier for data that
    cannot be separated via a simple linear line or hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Go to the `LIBSVM` Data: Classification (Multi-class) Repository and download
    the file from the following URL: [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for `SparkSession` to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a `SparkSession` specifying configurations to gain access to the
    Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin by loading the `libsvm` formatted data file into memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Now display the loaded data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From the console, this is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00118.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we utilize the Datasets `randomSplit` method to divide data into two
    buckets with allocations of 80% and 20% for each bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'The `randomSplit` method returns an array with two sets of data, the training
    set being the 80% portion and testing set being the 20% portion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we configure the multilayer perceptron classifier with an input layer
    of four nodes, a hidden layer of five nodes, and a four-node layer for output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '**Blocksize**: Block size for stacking input data in matrices to speed up the
    computation. This is more of an efficiency parameter with a recommended size between
    `10` and `1000`. This parameter concerns the total amount of data that is shoved
    into a partition for efficiency reasons.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MaxIter**: Maximum number of iterations to run the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seed**: Set the seed for weight initialization if weights are not set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following two lines from the Spark source code on GitHub reveals the default
    set within the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: To understand the parameters and seeding better, see the MLP source code at [https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala).
  prefs: []
  type: TYPE_NORMAL
- en: 'We generate a model by invoking the fit method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we put the trained model to use transforming the test data and displaying
    the predicted results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will be displayed in console like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00119.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we extract predictions and labels from the results and pass them to
    a Multi Class Classification Evaluator to generate an accuracy value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the program by stopping the `SparkSession`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we demonstrated usage of a multilayer perceptron classifier.
    We began by loading the classic Iris dataset in `libsvm` format. Next, we split
    the dataset with a ratio of 80% for training set data and 20% for test set data.
    In our definition phase, we configured the multilayer perceptron classifier with
    an input layer of four nodes, a hidden layer of five nodes, and a four-node layer
    for output. We generated a trained model by invoking the `fit()` method, and then
    produced predictions utilizing the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we retrieved predictions and labels, passing them to the multi-class
    classification evaluator that computes an accuracy value.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple visual inspection of predicted versus actual without much experimentation
    and fitting seems very impressive and acts a testimony as to why neural networks
    (much different than the early 1990s version) are back in favor. They do a great
    job in capturing non-linear surfaces. Here are some examples of non-linear surfaces
    (source: Graphing Calculator 4 on Mac App Store).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows a 2D depiction of a sample non-linear case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00120.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The following figure shows a 3D depiction of a sample non-linear case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00121.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Generally speaking, a neural network is defined first by a code sample as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: That defines the physical configuration of the network. In this case, we have
    a *4 x 5 x 4* MLP, meaning four input layers, five hidden layers, and four output
    layers. The `BlockSize` is set to 110 by using the `setBlockSize(110)` method
    for demonstration purposes, but 128 is the default out of the box. It is important
    to have a good random function to initialize the weights, which in this case is
    the current system time `setSeed(System.*currentTimeMillis*()`. The `setMaxIter(145)`
    is the maximum number of iterations used by the `setSolver()` method, which is
    `l-bfgs` solver by default.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Multilayer Perceptron** (**MLP**) or **Feed Forward Network** (**FFN**) is
    usually the first type of neuron network that one comes to understand before graduating
    to **Restricted Boltzmann Machines** (**RBM**) and **Recurrent Neural Network**
    (**RRN**) that are common in deep learning. While MLP technically can be configured/referred
    to as deep network, one must investigate a bit and understand as to why it is
    considered a first step (only) in a journey toward deep learning networks.'
  prefs: []
  type: TYPE_NORMAL
- en: In Spark's 2.0 implementation, sigmoid function (non-linear activation) is used
    in a deep stackable network configuration (more than three layer) to map the output
    to a `Softmax` function to create a non-trivial mapping surface that can capture
    extreme non-linear behavior of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Spark uses a sigmoid function to achieve non-linear mapping in a stackable configuration
    via an easy-to-use API. The following image depicts a Sigmoid function and its
    graph using a graphing calculator software on Mac
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00122.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for Spark's 2.0 MLP: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.MultilayerPerceptronClassifier](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.MultilayerPerceptronClassifier)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a quick introduction to MLP, see the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Multilayer_perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Perceptron](https://en.wikipedia.org/wiki/Perceptron)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.di.ubi.pt/~lfbaa/pubs/NN2008.pdf](http://www.di.ubi.pt/~lfbaa/pubs/NN2008.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See Spark MLP source code at [https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Classic papers needed to understand deep belief networks (absolute minimum)
    and its contrast to a simple MLP:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep belief networks: [https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacked auto encoders: [http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf](http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparse representation: [http://www.cs.nyu.edu/~ranzato/publications/ranzato-nips06.pdf](http://www.cs.nyu.edu/~ranzato/publications/ranzato-nips06.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the important API calls within `MultilayerPerceptronClassifier`**:**
  prefs: []
  type: TYPE_NORMAL
- en: 'The `BlockSize` by default is set to 128 - you should only start adjusting
    this parameter when you feel you have mastered the MLP in full:'
  prefs: []
  type: TYPE_NORMAL
- en: '`def **setLayers**(value: Array[Int]): MultilayerPerceptronClassifier.this.type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`def **setFeaturesCol**(value: String): MultilayerPerceptronClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`def **setLabelCol**(value: String): MultilayerPerceptronClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '``def **setSeed**(value: Long): MultilayerPerceptronClassifier.this.type``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`def **setBlockSize**(value: Int): MultilayerPerceptronClassifier.this.type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`def **setSolver**(value: String): MultilayerPerceptronClassifier.this.type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-vs-Rest classifier (One-vs-All) in Apache Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate One-vs-Rest in Apache Spark 2.0\. What we are
    trying to achieve with the `OneVsRest()` classifier is to make a binary logistic
    regression to work for a multi-class / multi-label classification problem. The
    recipe is a two-step approach in which we first configure a `LogisticRegression()`
    object and then use it in a `OneVsRest()` classifier to solve a multi-class classification
    problem using logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Go to the `LIBSVM` Data: Classification (Multi-class) Repository and download
    the file: [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the `SparkSession` to gain access to the
    cluster and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a `SparkSession` specifying configurations constructing an entry
    point to the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin by loading into memory the `libsvm` formatted data file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Now display the loaded data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00123.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we utilize the datasets `randomSplit` method splitting the dataset with
    a ratio of 80% training data and 20% test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s configure a logistics regression algorithm to use as a classifier for
    the One-vs-Rest algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the one versus rest object passing our newly created logistic
    regression object as an argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'We generated the model by invoking the fit method on our one-vs-rest object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will use the trained model to generate predictions for the test data
    and display the results:![](img/00124.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we pass predictions to the Multi Class Classification Evaluator to
    generate an accuracy value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the program by stopping the `SparkSession`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we demonstrated usage of the One-vs-Rest classifier. We began
    by loading the classic Iris dataset in `libsvm` format. Next, we split the dataset
    with a ratio of 80% for a training dataset and 20% for a test dataset. We draw
    the users'' attention to how we use system time for randomness in a split as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'The algorithm can be best described as a three-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first configure the regression object without having to have a base logistic
    model at hand so it can be fed into our classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next step, we feed the configured regression model into our classifier
    and call the `fit()` function to finish the job accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: We generate a trained model and transform the test data by way of the model.
    Finally, we pass predictions to the multi class classification evaluator, generating
    an accuracy value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The typical usage of this algorithm is for tagging and bagging different news
    items of interest about a person into various categories (such as friendly versus
    hostile, lukewarm versus elated, and so on). Another usage in medical billing
    could be the classification of patient diagnostic into different medical codes
    used for automated billing and revenue cycle maximization.
  prefs: []
  type: TYPE_NORMAL
- en: 'One-vs-Rest: As shown in the following figure, this solves an *n*-label classification
    problem by binary logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00125.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark''s 2.0 documentation for `OneVsRest()` can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.OneVsRest](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.OneVsRest)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to visualize this is, to assess for a given binary classifier can
    we break down an n-class input into *N* number of logistic regressions and then
    pick the one that describes the data the best. There are numerous examples of
    this classifier using Scikit Learn library in Python as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html#sklearn.multiclass.OneVsOneClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html#sklearn.multiclass.OneVsOneClassifier)'
  prefs: []
  type: TYPE_NORMAL
- en: 'But we recommend you do a quick scan of the actual Scala source code (less
    than 400 lines only) at GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/ml/classification/OneVsRest.scala](https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/ml/classification/OneVsRest.scala)'
  prefs: []
  type: TYPE_NORMAL
- en: Survival regression – parametric AFT model in Apache Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore Spark 2.0's implementation for Survival regression,
    which is not the typical proportional hazard model, but the **Accelerated Failure
    Time** (**AFT**) model instead. This is an important distinction that should be
    kept in mind while running this recipe otherwise the results would not make sense.
  prefs: []
  type: TYPE_NORMAL
- en: The survival regression analysis considers itself with models of *time to an
    event* nature, which are common in medicine, insurance, and anytime survivability
    of the subject is of interest. One of my coauthors happen to be a fully trained
    medical doctor (in addition to being a computer scientist), so we use a real dataset
    HMO-HIM+ study from a well-respected book in the field so we can obtain a reasonable
    output.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, we are using this technique to do drought modeling at scale to predict
    price impact on agricultural commodities in long-range time frames and forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Go to the UCLA website to download the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://stats.idre.ucla.edu/stat/r/examples/asa/hmohiv.csv](https://stats.idre.ucla.edu/stat/r/examples/asa/hmohiv.csv)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset we used is the actual data that is in the book *Applied Survival
    Analysis: Regression Modeling of Time to Event Data* by David W Hosmer and Stanley
    Lemeshow (1999). The data came from a HMO-HIM+ study and the data contains the
    following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00126.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for `SparkSession` to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a `SparkSession` specifying configurations with the builder pattern
    thus making an entry point available for the Spark Cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: We then read the `csv` file in, skipping the first line (header).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note: there are multiple ways to read in the `csv` files to a Spark DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'We convert the field from string to double. We are only interested in the ID,
    time, age, and censor fields. The four fields then form a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: The new `features` fields is a vector composed of `time` and `age` fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we display the DataFrame in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console, this is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00127.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now we will create the `AFTSurvivalRegression` object, and set the parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The quantile probabilities are set to 0.3 and 0.6 for this particular recipe.
    The values depict the boundaries of quantile, which are numerical vectors of probabilities
    with values in the range of *0.0* to *1.0* [*0.0,1.0*]. For example, using (0.25,
    0.5, 0.75) for quantile probability vector is a common theme.
  prefs: []
  type: TYPE_NORMAL
- en: The quantiles column name is set to *quantiles*.
  prefs: []
  type: TYPE_NORMAL
- en: In the following code, we create the `AFTSurvivalRegression()` object and set
    the column name and quantile probability vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code from Spark''s source code on GitHub shows default values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: To understand parameterization and seeding, *Spark Source Code for Survival
    Regression* can be referenced on GitHub at [https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'We let the model run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the model data into the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output will be seen in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the preceding model to transform the dataset and display the result
    in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output will be seen in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00128.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the `SparkSession`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We explored features of the **Accelerated Failure time** (**AFT**) model. We
    first read the dataset file into Spark using the `sparkContext.textFile()`. There
    are multiple ways to read a `csv` format file. We just picked one showing more
    detailed steps.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we filtered out the head row, and converted the interested fields from
    string to double, and then converted the double dataset into a new DataFrame with
    a new `features` field.
  prefs: []
  type: TYPE_NORMAL
- en: We then created the `AFTSurvivalRegression` object and set the quantile parameters,
    and let the model run for itself by calling the `fit(data)` function.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we displayed the model summary and used the model to transform the
    dataset and displayed the resulting DataFrame with prediction and quantiles fields
    included.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark implementation of survival regression (`AFTSurvivalRegression`)*:*
  prefs: []
  type: TYPE_NORMAL
- en: '**Model**: Accelerated Failure Time (AFT).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parametric:** Using Weibull distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization:** Spark chooses AFT because it is easier to parallelize and
    views the problem as a convex optimization problem with L-BFGS being the method
    of choice as optimization method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**R/SparkR users:** When fitting `AFTSurvivalRegressionModel` without intercept
    on dataset with constant nonzero column, Spark MLlib outputs zero coefficients
    for constant nonzero columns. This behavior is different from R `survival::survreg`.
    (from Spark 2.0.2 documentation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should think of the outcome as the time until the occurrence of an event
    of interest occurs, such as occurrence of a disease, winning, losing, time to
    mortgage default, marriage, divorce, landing a job after graduation, and so on.
    What is unique about these models is that the *time event* is a duration and does
    not necessarily have an explanatory variable (that is, it is just a duration in
    days, months, or years).
  prefs: []
  type: TYPE_NORMAL
- en: 'The reasons you might use survival models as opposed to simple regression (that
    is, tempting) are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Need to model the outcome variable as a time event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Censoring - not all the data is known or used (common when using long-range
    commodity data from past centuries)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-normality distributed outcome - often the case with time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It might or might not be a case of multivariate analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While there are two approaches to survival regression as outlined here, at
    the time of writing, Spark 2.0 only supports the AFT model and not the most-talked-about
    version, which is the proportional hazard model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Proportional hazard model** (**PH**):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proportionality assumed over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiplying a constant by covariance over consideration time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Cox Proportional Hazard Model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hx(y) = h0(y)*g(X)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Accelerated Time Failure (ATF) **- **Spark 2.0 implementation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proportionality assumption can be assumed or is violated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The constant value multiplied by covariance to get the regression coefficient
    values may be:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accelerated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decelerated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Allows for stages in unfolding of the regression:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stages of disease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stages of survivability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Yx * g(X) = Y0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sx(y) = S0(yg(X))*'
  prefs: []
  type: TYPE_NORMAL
- en: '*where,*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y*: survival time,'
  prefs: []
  type: TYPE_NORMAL
- en: '*X*: covariate vector,'
  prefs: []
  type: TYPE_NORMAL
- en: '*hx(y)*: the hazard function,'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sx(y)*: the survival function of *Y* given *X*,'
  prefs: []
  type: TYPE_NORMAL
- en: '*Yx*: *Y* given *X*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parametric modeling - underlying distribution of time variable:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exponential
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weibull - Spark 2.0 implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log Logistic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gamma
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also - very popular in R - we have used these two packages:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Library(survival): Standard Survival Analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Library(eha): For AFT modeling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Documentation for `SurvivalRegression` is available at the following URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegressionModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegressionModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/ml-classification-regression.html#survival-regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#survival-regression)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The original format of the `HMOHIV` data set can be found at - connect as guest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ftp://ftp.wiley.com/public/sci_tech_med/survival](ftp://ftp.wiley.com/public/sci_tech_med/survival)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A deep and complete comparison of Proportional versus AFT (Spark 2.0) hazard
    models can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://ecommons.usask.ca/bitstream/handle/10388/etd-03302009-140638/JiezhiQiThesis.pdf](https://ecommons.usask.ca/bitstream/handle/10388/etd-03302009-140638/JiezhiQiThesis.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'End-to-end real-world medical study with charts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.researchgate.net/profile/Richard_Kay2/publication/254087561_On_the_Use_of_the_Accelerated_Failure_Time_Model_as_an_Alternative_to_the_Proportional_Hazards_Model_in_the_Treatment_of_Time_to_Event_Data_A_Case_Study_in_Influenza/links/548ed67e0cf225bf66a710ce.pdf](https://www.researchgate.net/profile/Richard_Kay2/publication/254087561_On_the_Use_of_the_Accelerated_Failure_Time_Model_as_an_Alternative_to_the_Proportional_Hazards_Model_in_the_Treatment_of_Time_to_Event_Data_A_Case_Study_in_Influenza/links/548ed67e0cf225bf66a710ce.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for AFT survival implementation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegression](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegression)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegressionModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegressionModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "Spark source code for survival regression can be referenced on GitHub: [https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala\uFEFF\
    ](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala)"
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
