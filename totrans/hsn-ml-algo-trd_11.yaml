- en: Gradient Boosting Machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升机
- en: In the previous chapter, we learned about how random forests improve the predictions
    made by individual decision trees by combining them into an ensemble that reduces
    the high variance of individual trees. Random forests use bagging, which is short
    for bootstrap aggregation, to introduce random elements into the process of growing
    individual trees.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了随机森林如何通过将它们组合成一个集成来改善单个决策树的预测，从而减少了单个树的高方差。随机森林使用装袋（bagging）来引入随机元素到生长单个树的过程中。
- en: More specifically, bagging draws samples from the data with replacement so that
    each tree is trained on a different but equal-sized random subset of the data
    (with some observations repeating). Random forests also randomly select a subset
    of the features so that both the rows and the columns of the data that are used
    to train each tree are random versions of the original data. The ensemble then
    generates predictions by averaging over the outputs of the individual trees.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，装袋从数据中有放回地抽取样本，使得每棵树都在数据的不同但大小相等的随机子集上进行训练（有些观测重复）。随机森林还随机选择一部分特征，以便用于训练每棵树的数据的行和列都是原始数据的随机版本。然后集成通过对单个树的输出进行平均来生成预测。
- en: Individual trees are usually grown deep to ensure low bias while relying on
    the randomized training process to produce different, uncorrelated prediction
    errors that have a lower variance when aggregated than individual tree predictions.
    In other words, the randomized training aims to decorrelate or diversify the errors
    made by the individual trees so that the ensemble is much less susceptible to
    overfitting, has lower variance, and generalizes better to new data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，单个树会生长得很深，以确保低偏差，同时依靠随机化的训练过程产生不同的、不相关的预测误差，这些误差在聚合时比单个树的预测具有更低的方差。换句话说，随机化的训练旨在使单个树所产生的误差不相关或多样化，以便集成对过拟合更不敏感，具有更低的方差，并且对新数据具有更好的泛化能力。
- en: In this chapter, we will explore boosting, an alternative **machine learning**
    (**ML**) algorithm for ensembles of decision trees that often produces even better
    results. The key difference is that boosting modifies the data that is used to
    train each tree based on the cumulative errors made by the model before adding
    the new tree. In contrast to random forests which train many trees independently
    from each other using different versions of the training set, boosting proceeds
    sequentially using reweighted versions of the data. State-of-the-art boosting
    implementations also adopt the randomization strategies of random forests.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨提升，这是一种替代的机器学习（ML）算法，用于决策树集成，通常可以产生更好的结果。关键区别在于，提升根据模型在添加新树之前累积的错误修改用于训练每棵树的数据。与随机森林不同，随机森林使用不同版本的训练集独立地训练许多树，提升是顺序进行的，使用数据的重新加权版本。现代提升实现也采用了随机森林的随机化策略。
- en: 'In this chapter, we will see how boosting has evolved into one of the most
    successful ML algorithms over the last three decades. At the time of writing,
    it has come to dominate machine learning competitions for structured data (as
    opposed to high-dimensional images or speech, for example, where the relationship
    between the input and output is more complex, and deep learning excels at). More
    specifically, in this chapter we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到提升如何在过去三十年中发展成为最成功的ML算法之一。在撰写本文时，它已经成为结构化数据的机器学习竞赛的主导者（与高维图像或语音等复杂输入输出关系更复杂的领域相比，深度学习在这些领域表现出色）。更具体地说，在本章中，我们将涵盖以下主题：
- en: How boosting works, and how it compares to bagging
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升的工作原理，以及与装袋的比较
- en: How boosting has evolved from adaptive to gradient boosting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升如何从自适应提升发展为梯度提升
- en: How to use and tune AdaBoost and gradient boosting models with sklearn
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用和调整AdaBoost和梯度提升模型与sklearn
- en: How state-of-the-art GBM implementations dramatically speed up computation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代GBM实现如何显著加速计算
- en: How to prevent overfitting of gradient boosting models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何防止梯度提升模型的过拟合
- en: How to build, tune, and evaluate gradient boosting models on large datasets
    using `xgboost`, `lightgbm`, and `catboost`
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用`xgboost`、`lightgbm`和`catboost`在大型数据集上构建、调整和评估梯度提升模型
- en: How to interpret and gain insights from gradient boosting models
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何解释和从梯度提升模型中获得见解
- en: Adaptive boosting
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自适应提升
- en: Like bagging, boosting is an ensemble learning algorithm that combines base
    learners (typically decision trees) into an ensemble. Boosting was initially developed
    for classification problems, but can also be used for regression, and has been
    called one of the most potent learning ideas introduced in the last 20 years (as
    described in *Elements of Statistical Learning* by Trevor Hastie, et al.; see
    GitHub for links to references). Like bagging, it is a general method or metamethod
    that can be applied to many statistical learning models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与装袋类似，提升是一种集成学习算法，它将基本学习器（通常是决策树）组合成一个集成。提升最初是为分类问题开发的，但也可以用于回归，并且被称为过去20年中引入的最有效的学习思想之一（如Trevor
    Hastie等人在《统计学习的要素》中所述；请参阅GitHub获取参考链接）。与装袋类似，它是一种通用方法或元方法，可以应用于许多统计学习模型。
- en: 'The motivation for the development of boosting was to find a method to combine
    the outputs of many *weak* models (a predictor is called weak when it performs
    just slightly better than random guessing) into a more powerful, that is, boosted
    joint prediction. In general, boosting learns an additive hypothesis, *H[M]*, of
    a form similar to linear regression. However, now each of the *m= 1,..., M* elements
    of the summation is a weak base learner, called *h[t]* that itself requires training. The
    following formula summarizes the approach:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0751f227-28f7-43f7-a472-99acd461c675.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: As discussed in the last chapter, bagging trains base learners on different
    random samples of the training data. Boosting, in contrast, proceeds sequentially
    by training the base learners on data that is repeatedly modified to reflect the
    cumulative learning results. The goal is to ensure that the next base learner
    compensates for the shortcomings of the current ensemble. We will see in this
    chapter that boosting algorithms differ in how they define shortcomings. The ensemble
    makes predictions using a weighted average of the predictions of the weak models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The first boosting algorithm that came with a mathematical proof that it enhances
    the performance of weak learners was developed by Robert Schapire and Yoav Freund
    around 1990\. In 1997, a practical solution for classification problems emerged
    in the form of the **adaptive boosting** (**AdaBoost**) algorithm, which won the
    Göedel Prize in 2003\. About another five years later, this algorithm was extended
    to arbitrary objective functions when Leo Breiman (who invented random forests)
    connected the approach to gradient descent, and Jerome Friedman came up with gradient
    boosting in 1999\. Numerous optimized implementations, such as XGBoost, LightGBM,
    and CatBoost, have emerged in recent years and firmly established gradient boosting
    as the go-to solution for structured data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will briefly introduce AdaBoost and then focus
    on the gradient boosting model, as well as several state-of-the-art implementations
    of this very powerful and flexible algorithm.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The AdaBoost algorithm
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AdaBoost was the first boosting algorithm to iteratively adapt to the cumulative
    learning progress when fitting an additional ensemble member. In particular, AdaBoost changed
    the weights on the training data to reflect the cumulative errors of the current
    ensemble on the training set before fitting a new weak learner. AdaBoost was the
    most accurate classification algorithm at the time, and Leo Breiman referred to
    it as the best off-the-shelf classifier in the world at the 1996 NIPS conference.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm had a very significant impact on ML because it provided theoretical
    performance guarantees. These guarantees only require sufficient data and a weak
    learner that reliably predicts just better than a random guess. As a result of
    this adaptive method that learns in stages, the development of an accurate ML
    model no longer required accurate performance over the entire feature space. Instead,
    the design of a model could focus on finding weak learners that just outperformed
    a coin flip.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost is a significant departure from bagging, which builds ensembles on
    very deep trees to reduce bias. AdaBoost, in contrast, grows shallow trees as
    weak learners, often producing superior accuracy with stumps—that is, trees formed
    by a single split. The algorithm starts with an equal-weighted training set and
    then successively alters the sample distribution. After each iteration, AdaBoost
    increases the weights of incorrectly classified observations and reduces the weights
    of correctly predicted samples so that subsequent weak learners focus more on
    particularly difficult cases. Once trained, the new decision tree is incorporated
    into the ensemble with a weight that reflects its contribution to reducing the
    training error.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'The AdaBoost algorithm for an ensemble of base learners, *h[m](x)*, *m=1, ...,
    M*, that predict discrete classes, *y ∈ [-1, 1]*, and *N* training observations
    can be summarized as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 用于预测离散类别的基学习器集成*h[m](x)*的AdaBoost算法，*m=1, ..., M*，以及*N*个训练观测可以总结如下：
- en: Initialize sample weights *w[i]=1/N* for observations *i=1, ..., N*.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为观测值*w[i]=1/N*初始化样本权重*w[i]=1/N*。
- en: 'For each base classifier *h[m]*, *m=1, ..., M*, do the following:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个基分类器*h[m]*，*m=1, ..., M*，执行以下操作：
- en: Fit *h[m](x)* to the training data, weighted by *w[i]*.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据*w[i]*对训练数据进行加权的*h[m](x)*。
- en: Compute the base learner's weighted error rate *ε*[*m* ]on the training set.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算训练集上基学习器的加权错误率*ε*[*m* ]。
- en: 'Compute the base learner''s ensemble weight *α[m]* as a function of its error
    rate, as shown in the following formula:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据其错误率计算基学习器的集成权重*α[m]*，如下面的公式所示：
- en: '![](img/89d3eb5e-ac52-4d92-8d82-131a361a9c52.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89d3eb5e-ac52-4d92-8d82-131a361a9c52.png)'
- en: Update the weights for misclassified samples according to *w[i ]* exp(α[m]**)*.
  id: totrans-31
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据*w[i ]* exp(α[m]**)*更新误分类样本的权重。
- en: 'Predict the positive class when the weighted sum of the ensemble members is
    positive, and negative otherwise, as shown in the following formula:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据下面的公式，当集成成员的加权和为正时，预测正类，否则预测负类：
- en: '![](img/0f542da7-c4a9-4ad8-8fe4-fa5c75c14c8b.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f542da7-c4a9-4ad8-8fe4-fa5c75c14c8b.png)'
- en: AdaBoost has many practical advantages, including ease of implementation and
    fast computation, and it can be combined with any method for identifying weak
    learners. Apart from the size of the ensemble, there are no hyperparameters that
    require tuning. AdaBoost is also useful for identifying outliers because the samples
    that receive the highest weights are those that are consistently misclassified
    and inherently ambiguous, which is also typical for outliers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost具有许多实际优势，包括易于实现和快速计算，它可以与任何识别弱学习器的方法结合使用。除了集成的大小之外，没有需要调整的超参数。AdaBoost还用于识别异常值，因为接收最高权重的样本通常是一直被错误分类和本质上模糊的样本，这对于异常值也是典型的。
- en: On the other hand, the performance of AdaBoost on a given dataset depends on
    the ability of the weak learner to adequately capture the relationship between
    features and outcome. As the theory suggests, boosting will not perform well when
    there is insufficient data, or when the complexity of the ensemble members is
    not a good match for the complexity of the data. It can also be susceptible to
    noise in the data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，AdaBoost在给定数据集上的性能取决于弱学习器充分捕捉特征和结果之间关系的能力。正如理论所建议的，当数据不足或者集成成员的复杂性与数据的复杂性不匹配时，增强学习的表现会不佳。它也可能受到数据中的噪声的影响。
- en: AdaBoost with sklearn
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用sklearn的AdaBoost
- en: As part of its ensemble module, sklearn provides an `AdaBoostClassifier` implementation
    that supports two or more classes. The code examples for this section are in the
    notebook `gbm_baseline` that compares the performance of various algorithms with
    a dummy classifier that always predicts the most frequent class.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作为其集成模块的一部分，sklearn提供了一个支持两个或更多类的`AdaBoostClassifier`实现。本节的代码示例在笔记本`gbm_baseline`中，该笔记本比较了各种算法的性能，其中包括一个总是预测最频繁类别的虚拟分类器。
- en: 'We need to first define a `base_estimator` as a template for all ensemble members
    and then configure the ensemble itself. We''ll use the default `DecisionTreeClassifier`
    with `max_depth=1`—that is, a stump with a single split. The complexity of the
    `base_estimator` is a key tuning parameter because it depends on the nature of
    the data. As demonstrated in the previous chapter, changes to `max_depth` should
    be combined with appropriate regularization constraints using adjustments to,
    for example, `min_samples_split`, as shown in the following code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要首先定义一个`base_estimator`作为所有集成成员的模板，然后配置集成本身。我们将使用默认的`DecisionTreeClassifier`，`max_depth=1`——也就是一个只有一个分裂的树桩。`base_estimator`的复杂性是一个关键的调参参数，因为它取决于数据的性质。正如前一章所示，对`max_depth`的更改应该与适当的正则化约束相结合，例如通过调整`min_samples_split`，如下面的代码所示：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the second step, we''ll design the ensemble. The `n_estimators` parameter controls
    the number of weak learners and the `learning_rate` determines the contribution
    of each weak learner, as shown in the following code. By default, weak learners
    are decision tree stumps:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，我们将设计集成。`n_estimators`参数控制弱学习器的数量，`learning_rate`确定每个弱学习器的贡献，如下面的代码所示。默认情况下，弱学习器是决策树树桩：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The main tuning parameters that are responsible for good results are `n_estimators` and
    the base estimator complexity because the depth of the tree controls the extent
    of the interaction among the features.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 负责良好结果的主要调参参数是`n_estimators`和基学习器的复杂性，因为树的深度控制了特征之间的相互作用程度。
- en: 'We will cross-validate the AdaBoost ensemble using a custom 12-fold rolling
    time-series split to predict 1 month ahead for the last 12 months in the sample,
    using all available prior data for training, as shown in the following code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用自定义的12折滚动时间序列拆分来交叉验证AdaBoost集成，以预测样本中最后12个月的未来1个月，使用所有可用的先前数据进行训练，如下面的代码所示：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result shows a weighted test accuracy of 0.62, a test AUC of 0.6665, and
    a negative log loss of -0.6923, as well as a test F1 score of 0.5876, as shown
    in the following screenshot:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示加权测试准确率为0.62，测试AUC为0.6665，负对数损失为-0.6923，以及测试F1分数为0.5876，如下面的截图所示：
- en: '![](img/7dfaa5e9-0dc5-446d-a654-d5beadb37d9a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7dfaa5e9-0dc5-446d-a654-d5beadb37d9a.png)'
- en: See the companion notebook for additional details on the code to cross-validate
    and process the results.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有关交叉验证和处理结果的代码的更多细节，请参阅配套笔记本。
- en: Gradient boosting machines
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升机
- en: 'AdaBoost can also be interpreted as a stagewise forward approach to minimizing
    an exponential loss function for a binary *y* ∈ [-1, 1] at each iteration *m*
    to identify a new base learner *h[m]* with the corresponding weight *α[m]* to
    be added to the ensemble, as shown in the following formula:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48da6fef-cff2-47f5-9f7a-209a9d29cf19.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: This interpretation of the AdaBoost algorithm was only discovered several years
    after its publication. It views AdaBoost as a coordinate-based gradient descent
    algorithm that minimizes a particular loss function, namely exponential loss.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting leverages this insight and applies the boosting method to
    a much wider range of loss functions. The method enables the design of machine
    learning algorithms to solve any regression, classification, or ranking problem
    as long as it can be formulated using a loss function that is differentiable and
    thus has a gradient. The flexibility to customize this general method to many
    specific prediction tasks is essential to boosting's popularity.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: The main idea behind the resulting **Gradient Boosting Machines** (**GBM**)
    algorithm is the training of the base learners to learn the negative gradient
    of the current loss function of the ensemble. As a result, each addition to the
    ensemble directly contributes to reducing the overall training error given the
    errors made by prior ensemble members. Since each new member represents a new
    function of the data, gradient boosting is also said to optimize over the functions
    *h[m]* in an additive fashion.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, the algorithm successively fits weak learners *h[m]*, such as decision
    trees, to the negative gradient of the loss function that is evaluated for the
    current ensemble, as shown in the following formula:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43567de6-058e-4ab5-bc86-ba51981362d9.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: In other words, at a given iteration *m*, the algorithm computes the gradient
    of the current loss for each observation and then fits a regression tree to these
    pseudo-residuals. In a second step, it identifies an optimal constant prediction
    for each terminal node that minimizes the incremental loss that results from adding
    this new learner to the ensemble.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'This differs from standalone decision trees and random forests, where the prediction
    depends on the outcome values of the training samples present in the relevant
    terminal or leaf node: their average, in the case of regression, or the frequency
    of the positive class for binary classification. The focus on the gradient of
    the loss function also implies that gradient boosting uses regression trees to
    learn both regression and classification rules since the gradient is always a
    continuous function.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'The final ensemble model makes predictions based on the weighted sum of the
    predictions of the individual decision trees, each of which has been trained to
    minimize the ensemble loss given the prior prediction for a given set of feature
    values, as shown in the following diagram:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9aff1b85-dd69-4c79-bf6c-f453a808db13.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: Gradient boosting trees have demonstrated state-of-the-art performance on many
    classification, regression, and ranking benchmarks. They are probably the most
    popular ensemble learning algorithm both as a standalone predictor in a diverse
    set of machine learning competitions, as well as in real-world production pipelines,
    for example, to predict click-through rates for online ads.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: The success of gradient boosting is based on its ability to learn complex functional
    relationships in an incremental fashion. The flexibility of this algorithm requires
    the careful management of the risk of overfitting by tuning hyperparameters that
    constrain the model's inherent tendency to learn noise as opposed to the signal
    in the training data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: We will introduce the key mechanisms to control the complexity of a gradient
    boosting tree model, and then illustrate model tuning using the sklearn implementation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: How to train and tune GBM models
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The two key drivers of gradient boosting performance are the size of the ensemble
    and the complexity of its constituent decision trees.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度增强性能的两个关键驱动因素是集成的大小和其组成决策树的复杂性。
- en: 'The control of complexity for decision trees aims to avoid learning highly
    specific rules that typically imply a very small number of samples in leaf nodes.
    We covered the most effective constraints used to limit the ability of a decision
    tree to overfit to the training data in the previous chapter. They include requiring:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的复杂性控制旨在避免学习高度特定的规则，这通常意味着叶节点中的样本数量非常少。我们在上一章中介绍了用于限制决策树过拟合训练数据能力的最有效约束。它们包括要求：
- en: A minimum number of samples to either split a node or accept it as a terminal
    node, or
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要么分割一个节点或接受它作为终端节点的最小样本数，或
- en: A minimum improvement in node quality as measured by the purity or entropy or
    mean square error, in the case of regression.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点质量的最小改进，由纯度或熵或均方误差来衡量，在回归的情况下。
- en: In addition to directly controlling the size of the ensemble, there are various
    regularization techniques, such as shrinkage, that we encountered in the context
    of the Ridge and Lasso linear regression models in [Chapter 7](0cf85bb4-8b3f-4f83-b004-f980f348028b.xhtml),
    *Linear Models*. Furthermore, the randomization techniques used in the context
    of random forests are also commonly applied to gradient boosting machines.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了直接控制集成的大小之外，还有各种正则化技术，例如我们在[第7章](0cf85bb4-8b3f-4f83-b004-f980f348028b.xhtml)中在岭回归和Lasso线性回归模型的上下文中遇到的技术。此外，在随机森林的上下文中使用的随机化技术也经常应用于梯度增强机器。
- en: Ensemble size and early stopping
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成大小和早停止
- en: Each boosting iteration aims to reduce the training loss so that for a large
    ensemble, the training error can potentially become very small, increasing the
    risk of overfitting and poor performance on unseen data. Cross-validation is the
    best approach to find the optimal ensemble size that minimizes the generalization
    error because it depends on the application and the available data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 每次增强迭代的目标是减少训练损失，以便对于一个大的集成，训练误差可能变得非常小，增加过拟合和在未见数据上表现不佳的风险。交叉验证是找到最小化泛化误差的最佳集成大小的最佳方法，因为它取决于应用程序和可用数据。
- en: Since the ensemble size needs to be specified before training, it is useful
    to monitor the performance on the validation set and abort the training process
    when, for a given number of iterations, the validation error no longer decreases.
    This technique is called early stopping and frequently used for models that require
    a large number of iterations and are prone to overfitting, including deep neural
    networks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于集成大小需要在训练之前指定，因此有必要监控验证集上的性能，并在给定迭代次数时，验证误差不再减少时中止训练过程。这种技术称为早停止，经常用于需要大量迭代且容易过拟合的模型，包括深度神经网络。
- en: Keep in mind that using early stopping with the same validation set for a large
    number of trials will also lead to overfitting, just to the particular validation
    set rather than the training set. It is best to avoid running a large number of
    experiments when developing a trading strategy as the risk of false discoveries
    increases significantly. In any case, keep a hold-out set to obtain an unbiased
    estimate of the generalization error.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，对于大量试验使用相同的验证集进行早停止也会导致过拟合，只是过拟合到特定的验证集而不是训练集。在开发交易策略时最好避免运行大量实验，因为假发现的风险显著增加。无论如何，保留一部分数据用于获取泛化误差的无偏估计。
- en: Shrinkage and learning rate
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收缩和学习率
- en: Shrinkage techniques apply a penalty for increased model complexity to the model's
    loss function. For boosting ensembles, shrinkage can be applied by scaling the
    contribution of each new ensemble member down by a factor between 0 and 1\. This
    factor is called the learning rate of the boosting ensemble. Reducing the learning
    rate increases shrinkage because it lowers the contribution of each new decision
    tree to the ensemble.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 收缩技术通过将惩罚应用于模型的损失函数来对模型复杂性增加。对于增强集成，可以通过将每个新集成成员的贡献按0到1之间的因子进行缩放来应用收缩。这个因子被称为增强集成的学习率。减小学习率会增加收缩，因为它降低了每个新决策树对集成的贡献。
- en: The learning rate has the opposite effect of the ensemble size, which tends
    to increase for lower learning rates. Lower learning rates coupled with larger
    ensembles have been found to reduce the test error, in particular for regression
    and probability estimation. Large numbers of iterations are computationally more
    expensive but often feasible with fast state-of-the-art implementations as long
    as the individual trees remain shallow. Depending on the implementation, you can
    also use adaptive learning rates that adjust to the number of iterations, typically
    lowering the impact of trees added later in the process. We will see some examples
    later in this chapter.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率具有与集成大小相反的效果，对于较低的学习率，集成大小倾向于增加。较低的学习率与更大的集成结合使用已被发现可以减少测试误差，特别是对于回归和概率估计。大量迭代在计算上更昂贵，但通常可以通过快速的最新实现来实现，只要个别树保持浅层。根据实现的不同，您还可以使用自适应学习率，它会根据迭代次数调整，通常降低后期添加的树的影响。我们将在本章的后面看到一些例子。
- en: Subsampling and stochastic gradient boosting
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 子采样和随机梯度增强
- en: As discussed in detail in the previous chapter, bootstrap averaging (bagging)
    improves the performance of an otherwise noisy classifier.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在前一章中详细讨论的，自举平均（bagging）提高了原本嘈杂分类器的性能。
- en: Stochastic gradient boosting uses sampling without replacement at each iteration to
    grow the next tree on a subset of the training samples. The benefit is both lower
    computational effort and often better accuracy, but subsampling should be combined
    with shrinkage.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度增强在每次迭代时使用无替换的抽样来在训练样本的子集上生长下一棵树。好处是既降低了计算量，又通常提高了准确性，但子采样应该与收缩结合使用。
- en: As you can see, the number of hyperparameters keeps increasing, driving up the
    number of potential combinations, which in turn increases the risk of false positives
    when choosing the best model from a large number of parameter trials on a limited
    amount of training data. The best approach is to proceed sequentially and select
    parameter values individually or using combinations of subsets of low cardinality.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，超参数的数量不断增加，导致潜在组合的数量增加，进而增加了在有限的训练数据上从大量参数试验中选择最佳模型时出现假阳性的风险。最佳方法是按顺序进行，并逐个选择参数值，或者使用低基数子集的组合。
- en: How to use gradient boosting with sklearn
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用sklearn进行梯度提升
- en: The ensemble module of sklearn contains an implementation of gradient boosting
    trees for regression and classification, both binary and multiclass. The following `GradientBoostingClassifier`
    initialization code illustrates the key tuning parameters that we previously introduced,
    in addition to those that we are familiar with from looking at standalone decision
    tree models. The notebook `gbm_tuning_with_sklearn` contains the code examples
    for this section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`的集成模块包含了用于回归和分类的梯度提升树的实现，包括二元和多类分类。以下`GradientBoostingClassifier`初始化代码展示了我们之前介绍的关键调整参数，以及我们从独立决策树模型中熟悉的参数。笔记本`gbm_tuning_with_sklearn`包含了本节的代码示例。'
- en: 'The available loss functions include the exponential loss that leads to the
    AdaBoost algorithm and the deviance that corresponds to the logistic regression
    for probabilistic outputs. The `friedman_mse` node quality measure is a variation
    on the mean squared error that includes an improvement score (see GitHub references
    for links to original papers), as shown in the following code:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的损失函数包括导致AdaBoost算法的指数损失和对应于概率输出的逻辑回归的偏差。`friedman_mse`节点质量度量是均方误差的变化，包括改进分数（请参阅GitHub参考链接到原始论文），如下代码所示：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similar to `AdaBoostClassifier`, this model cannot handle missing values. We''ll
    again use 12-fold cross-validation to obtain errors for classifying the directional
    return for rolling 1 month holding periods, as shown in the following code:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与`AdaBoostClassifier`类似，该模型无法处理缺失值。我们将再次使用12折交叉验证，以获取滚动1个月持有期的方向性收益分类的错误，如下代码所示：
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will parse and plot the result to find a slight improvement—using default
    parameter values—over the `AdaBoostClassifier`, as shown in the following screenshot:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将解析和绘制结果，以找到与`AdaBoostClassifier`相比略有改善的结果，如下截图所示：
- en: '![](img/16b1fe7c-5a2c-44be-a37b-583e0b0d7afc.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16b1fe7c-5a2c-44be-a37b-583e0b0d7afc.png)'
- en: How to tune parameters with GridSearchCV
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用`GridSearchCV`调整参数
- en: 'The `GridSearchCV` class in the `model_selection` module facilitates the systematic
    evaluation of all combinations of the hyperparameter values that we would like
    to test. In the following code, we will illustrate this functionality for seven
    tuning parameters that when defined will result in a total of 2⁴ x 3² x 4 = 576
    different model configurations:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`model_selection`模块中的`GridSearchCV`类便于对我们想要测试的超参数值的所有组合进行系统评估。在下面的代码中，我们将为七个调整参数演示这种功能，当定义时将导致总共2⁴
    x 3² x 4 = 576种不同的模型配置：'
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `.fit()` method executes the cross-validation using the custom `OneStepTimeSeriesSplit`
    and the `roc_auc` score to evaluate the 12-folds. Sklearn lets us persist the
    result as it would for any other model using the `joblib` pickle implementation,
    as shown in the following code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`.fit()`方法使用自定义的`OneStepTimeSeriesSplit`和`roc_auc`分数执行交叉验证来评估12折。Sklearn允许我们像对待任何其他模型一样持久化结果，使用`joblib`
    pickle实现，如下代码所示：'
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `GridSearchCV` object has several additional attributes after completion
    that we can access after loading the pickled result to learn which hyperparameter
    combination performed best and its average cross-validation AUC score, which results
    in a modest improvement over the default values. This is shown in the following
    code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV`对象在完成后有几个额外的属性，我们可以在加载pickled结果后访问，以了解哪种超参数组合表现最佳以及其平均交叉验证AUC分数，这比默认值略有改善。如下代码所示：'
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameter impact on test scores
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数对测试分数的影响
- en: The `GridSearchCV` result stores the average cross-validation scores so that
    we can analyze how different hyperparameter settings affect the outcome.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV`结果存储了平均交叉验证分数，这样我们就可以分析不同超参数设置如何影响结果。'
- en: 'The six `seaborn` swarm plots in the left-hand panel of the below chart show
    the distribution of AUC test scores for all parameter values. In this case, the
    highest AUC  test scores required a low `learning_rate` and a large value for `max_features`.
    Some parameter settings, such as a low `learning_rate`, produce a wide range of
    outcomes that depend on the complementary settings of other parameters. Other
    parameters are compatible with high scores for all settings use in the experiment:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下图左侧面板中的六个`seaborn` swarm图显示了所有参数值的AUC测试分数分布。在这种情况下，最高的AUC测试分数需要低`learning_rate`和大的`max_features`值。一些参数设置，比如低`learning_rate`，会产生一系列取决于其他参数互补设置的结果。其他参数与实验中使用的所有设置兼容，可以获得高分：
- en: '![](img/b4cbf385-72cf-4ce4-a880-4cfc240163d7.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4cbf385-72cf-4ce4-a880-4cfc240163d7.png)'
- en: 'We will now explore how hyperparameter settings jointly affect the mean cross-validation
    score. To gain insight into how parameter settings interact, we can train a `DecisionTreeRegressor`
    with the mean test score as the outcome and the parameter settings, encoded as categorical
    variables in one-hot or dummy format (see the notebook for details). The tree
    structure highlights that using all features (`max_features_1`), a low `learning_rate`,
    and a `max_depth` over three led to the best results, as shown in the following
    diagram:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99276663-6052-4441-b59a-39f01f1bc6fe.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: The bar chart in the right-hand panel of the first chart in this section displays
    the influence of the hyperparameter settings in producing different outcomes,
    measured by their feature importance for a decision tree that is grown to its
    maximum depth. Naturally, the features that appear near the top of the tree also
    accumulate the highest importance scores.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: How to test on the holdout set
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we would like to evaluate the best model''s performance on the holdout
    set that we excluded from the `GridSearchCV` exercise. It contains the last six
    months of the sample period (through February 2018; see the notebook for details).
    We obtain a generalization performance estimate based on the AUC score of `0.6622`
    using the following code:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The downside of the sklearn gradient boosting implementation is the limited
    speed of computation which makes it difficult to try out different hyperparameter
    settings quickly. In the next section, we will see that several optimized implementations
    have emerged over the last few years that significantly reduce the time required
    to train even large-scale models, and have greatly contributed to a broader scope
    for applications of this highly effective algorithm.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Fast scalable GBM implementations
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Over the last few years, several new gradient boosting implementations have
    used various innovations that accelerate training, improve resource efficiency,
    and allow the algorithm to scale to very large datasets. The new implementations
    and their sources are as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost (extreme gradient boosting), started in 2014 by Tianqi Chen at the University
    of Washington
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LightGBM, first released in January 2017, by Microsoft
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CatBoost, first released in April 2017 by Yandex
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These innovations address specific challenges of training a gradient boosting
    model (see this chapter''s `README` on GitHub for detailed references). The XGBoost implementation
    was the first new implementation to gain popularity: among the 29 winning solutions
    published by Kaggle in 2015, 17 solutions used XGBoost. Eight of these solely
    relied on XGBoost, while the others combined XGBoost with neural networks.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: We will first introduce the key innovations that have emerged over time and
    subsequently converged (so that most features are available for all implementations)
    before illustrating their implementation.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: How algorithmic innovations drive performance
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forests can be trained in parallel by growing individual trees on independent
    bootstrap samples. In contrast, the sequential approach of gradient boosting slows
    down training, which in turn complicates experimentation with a large number of
    hyperparameters that need to be adapted to the nature of the task and the dataset.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: To expand the ensemble by a tree, the training algorithm incrementally minimizes
    the prediction error with respect to the negative gradient of the ensemble's loss
    function, similar to a conventional gradient descent optimizer. Hence, the computational
    cost during training is proportional to the time it takes to evaluate the impact
    of potential split points for each feature on the decision tree's fit to the current
    gradient.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Second-order loss function approximation
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most important algorithmic innovations lower the cost of evaluating the
    loss function by using approximations that rely on second-order derivatives, resembling
    Newton's method to find stationary points. As a result, scoring potential splits
    during greedy tree expansion is faster relative to using the full loss function.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned previously, a gradient boosting model is trained in an incremental
    manner with the goal of minimizing the combination of the prediction error and
    the regularization penalty for the ensemble *H[M]*.Denoting the prediction of
    the outcome *y[i]* by the ensemble after step *m* as *ŷ[i]*^((*m*)), *l* as a
    differentiable convex loss function that measures the difference between the outcome
    and the prediction, and Ω as a penalty that increases with the complexity of the
    ensemble *H[M]*, the incremental hypothesis *h[m]* aims to minimize the following
    objective:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b510aee-4afd-43d9-b3c0-420662cbd92a.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: 'The regularization penalty helps to avoid overfitting by favoring the selection
    of a model that uses simple and predictive regression trees. In the case of XGBoost,
    for example, the penalty for a regression tree *h* depends on the number of leaves
    per tree *T*, the regression tree scores for each terminal node *w*, and the hyperparameters γ
    and λ. This is summarized in the following formula:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0958570b-42a4-4892-be18-ebd5d1fcf297.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, at each step, the algorithm greedily adds the hypothesis *h[m]*
    that most improves the regularized objective. The second-order approximation of
    a loss function, based on a Taylor expansion, speeds up the evaluation of the
    objective, as summarized in the following formula:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2814899-392e-4acd-8f31-afb6ab63148c.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: 'Here, *g[i]* is the first-order gradient of the loss function before adding
    the new learner for a given feature value, and *h[i] *is the corresponding second-order
    gradient (or Hessian) value, as shown in the following formulas:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9171f3bb-111d-4e63-8a3e-ff9390df102d.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: The XGBoost algorithm was the first open-source algorithm to leverage this approximation
    of the loss function to compute the optimal leave scores for a given tree structure
    and the corresponding value of the loss function. The score consists of the ratio
    of the sums of the gradient and Hessian for the samples in a terminal node. It
    uses this value to score the information gain that would result from a split,
    similar to the node impurity measures we saw in the previous chapter, but applicable
    to arbitrary loss functions (see the references on GitHub for the detailed derivation).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Simplified split-finding algorithms
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The gradient boosting implementation by sklearn finds the optimal split that
    enumerates all options for continuous features. This precise greedy algorithm
    is computationally very demanding because it must first sort the data by feature
    values before scoring the potentially very large number of split options and making
    a decision. This approach faces challenges when the data does not fit in memory
    or when training in a distributed setting on multiple machines.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: An approximate split-finding algorithm reduces the number of split points by
    assigning feature values to a user-determined set of bins, which can also greatly
    reduce the memory requirements during training because only a single split needs
    to be stored for each bin. XGBoost introduced a quantile sketch algorithm that
    was also able to divide weighted training samples into percentile bins to achieve
    a uniform distribution. XGBoost also introduced the ability to handle sparse data
    caused by missing values, frequent zero-gradient statistics, and one-hot encoding,
    and can also learn an optimal default direction for a given split. As a result,
    the algorithm only needs to evaluate non-missing values.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, LightGBM uses **gradient-based one-side sampling** (**GOSS**) to
    exclude a significant proportion of samples with small gradients, and only uses
    the remainder to estimate the information gain and select a split value accordingly.
    Samples with larger gradients require more training and tend to contribute more
    to the information gain. LightGBM also uses exclusive feature bundling to combine features
    that are mutually exclusive, in that they rarely take nonzero values simultaneously,
    to reduce the number of features. As a result, LightGBM was the fastest implementation
    when released.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Depth-wise versus leaf-wise growth
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LightGBM differs from XGBoost and CatBoost in how it prioritizes which nodes
    to split. LightGBM decides on splits leaf-wise, i.e., it splits the leaf node
    that maximizes the information gain, even when this leads to unbalanced trees.
    In contrast, XGBoost and CatBoost expand all nodes depth-wise and first split
    all nodes at a given depth before adding more levels. The two approaches expand
    nodes in a different order and will produce different results except for complete
    trees. The following diagram illustrates the two approaches:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f953b310-1ecc-45cd-9804-0b76aeaae036.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: LightGBM's leaf-wise splits tend to increase model complexity and may speed
    up convergence, but also increase the risk of overfitting. A tree grown depth-wise
    with *n* levels has up to *2*^(*n* )terminal nodes, whereas a leaf-wise tree with *2^n* leaves
    can have significantly more levels and contain correspondingly fewer samples in
    some leaves. Hence, tuning LightGBM's `num_leaves` setting requires extra caution,
    and the library allows us to control `max_depth` at the same time to avoid undue
    node imbalance. More recent versions of LightGBM also offer depth-wise tree growth.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: GPU-based training
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All new implementations support training and prediction on one or more GPUs
    to achieve significant speedups. They are compatible with current CUDA-enabled
    GPUs. Installation requirements vary and are evolving quickly. The XGBoost and
    CatBoost implementations work for several current versions, but LightGBM may require
    local compilation (see GitHub for links to the relevant documentation).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: The speedups depend on the library and the type of the data, and range from
    low, single-digit multiples to factors of several dozen. Activation of the GPU
    only requires the change of a task parameter and no other hyperparameter modifications.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: DART – dropout for trees
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2015, Rashmi and Gilad-Bachrach proposed a new model to train gradient boosting
    trees that aimed to address a problem they labeled over-specialization: trees
    added during later iterations tend only to affect the prediction of a few instances
    while making a minor contribution regarding the remaining instances. However,
    the model's out-of-sample performance can suffer, and it may become over-sensitive
    to the contributions of a small number of trees added earlier in the process.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: The new algorithms employ dropouts which have been successfully used for learning
    more accurate deep neural networks where dropouts mute a random fraction of the
    neural connections during the learning process. As a result, nodes in higher layers
    cannot rely on a few connections to pass the information needed for the prediction.
    This method has made a significant contribution to the success of deep neural
    networks for many tasks and has also been used with other learning techniques,
    such as logistic regression, to mute a random share of the features. Random forests
    and stochastic gradient boosting also drop out a random subset of features.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: DART operates at the level of trees and mutes complete trees as opposed to individual
    features. The goal is for trees in the ensemble generated using DART to contribute
    more evenly towards the final prediction. In some cases, this has been shown to
    produce more accurate predictions for ranking, regression, and classification
    tasks. The approach was first implemented in LightGBM and is also available for
    XGBoost.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Treatment of categorical features
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CatBoost and LightGBM implementations handle categorical variables directly
    without the need for dummy encoding.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The CatBoost implementation (which is named for its treatment of categorical
    features) includes several options to handle such features, in addition to automatic
    one-hot encoding, and assigns either the categories of individual features or
    combinations of categories for several features to numerical values. In other
    words, CatBoost can create new categorical features from combinations of existing
    features. The numerical values associated with the category levels of individual
    features or combinations of features depend on their relationship with the outcome
    value. In the classification case, this is related to the probability of observing
    the positive class, computed cumulatively over the sample, based on a prior, and
    with a smoothing factor. See the documentation for more detailed numerical examples.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The LightGBM implementation groups the levels of the categorical features to
    maximize homogeneity (or minimize variance) within groups with respect to the
    outcome values.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The XGBoost implementation does not handle categorical features directly and
    requires one-hot (or dummy) encoding.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Additional features and optimizations
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XGBoost optimized computation in several respects to enable multithreading by
    keeping data in memory in compressed column blocks, where each column is sorted
    by the corresponding feature value. XGBoost computes this input data layout once
    before training and reuses it throughout to amortize the additional up-front cost.
    The search for split statistics over columns becomes a linear scan when using
    quantiles that can be done in parallel with easy support for column subsampling.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: The subsequently released LightGBM and CatBoost libraries built on these innovations,
    and LightGBM further accelerated training through optimized threading and reduced
    memory usage. Because of their open source nature, libraries have tended to converge
    over time.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost also supports monotonicity constraints. These constraints ensure that
    the values for a given feature are only positively or negatively related to the
    outcome over its entire range. They are useful to incorporate external assumptions
    about the model that are known to be true.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: How to use XGBoost, LightGBM, and CatBoost
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XGBoost, LightGBM, and CatBoost offer interfaces for multiple languages, including
    Python, and have both a sklearn interface that is compatible with other sklearn
    features, such as `GridSearchCV` and their own methods to train and predict gradient
    boosting models. The `gbm_baseline.ipynb` notebook illustrates the use of the
    sklearn interface for each implementation. The library methods are often better
    documented and are also easy to use, so we'll use them to illustrate the use of
    these models.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The process entails the creation of library-specific data formats, the tuning
    of various hyperparameters, and the evaluation of results that we will describe
    in the following sections. The accompanying notebook contains the `gbm_tuning.py`,
    `gbm_utils.py` and, `gbm_params.py` files that jointly provide the following functionalities and
    have produced the corresponding results.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: How to create binary data formats
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All libraries have their own data format to precompute feature statistics to
    accelerate the search for split points, as described previously. These can also
    be persisted to accelerate the start of subsequent training.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code constructs binary train and validation datasets for each
    model to be used with the `OneStepTimeSeriesSplit`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The available options vary slightly:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '`xgboost` allows the use of all available threads'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lightgbm` explicitly aligns the quantiles that are created for the validation
    set with the training set'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `catboost` implementation needs feature columns identified using indices
    rather than labels
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to tune hyperparameters
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The numerous hyperparameters are listed in `gbm_params.py`. Each library has
    parameter settings to:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Specify the overall objectives and learning algorithm
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design the base learners
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply various regularization techniques
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handle early stopping during training
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling the use of GPU or parallelization on CPU
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The documentation for each library details the various parameters that may refer
    to the same concept, but which have different names across libraries. The GitHub
    repository contains links to a site that highlights the corresponding parameters
    for `xgboost` and `lightgbm`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Objectives and loss functions
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The libraries support several boosting algorithms, including gradient boosting
    for trees and linear base learners, as well as DART for LightGBM and XGBoost.
    LightGBM also supports the GOSS algorithm which we described previously, as well
    as random forests.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: The appeal of gradient boosting consists of the efficient support of arbitrary
    differentiable loss functions and each library offers various options for regression,
    classification, and ranking tasks. In addition to the chosen loss function, additional
    evaluation metrics can be used to monitor performance during training and cross-validation.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Learning parameters
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient boosting models typically use decision trees to capture feature interaction,
    and the size of individual trees is the most important tuning parameter. XGBoost
    and CatBoost set the `max_depth` default to 6\. In contrast, LightGBM uses a default
    `num_leaves` value of 31, which corresponds to five levels for a balanced tree,
    but imposes no constraints on the number of levels. To avoid overfitting, `num_leaves`
    should be lower than *2^(max_depth)*. For example, for a well-performing `max_depth`
    value of 7, you would set `num_leaves` to 70–80 rather than 2⁷=128, or directly
    constrain `max_depth`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: The number of trees or boosting iterations defines the overall size of the ensemble.
    All libraries support `early_stopping` to abort training once the loss functions
    register no further improvements during a given number of iterations. As a result,
    it is usually best to set a large number of iterations and stop training based
    on the predictive performance on a validation set.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All libraries implement the regularization strategies for base learners, such
    as minimum values for the number of samples or the minimum information gain required
    for splits and leaf nodes.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: They also support regularization at the ensemble level using shrinkage via a
    learning rate that constrains the contribution of new trees. It is also possible
    to implement an adaptive learning rate via callback functions that lower the learning
    rate as the training progresses, as has been successfully used in the context
    of neural networks. Furthermore, the gradient boosting loss function can be regularized
    using *L1* or *L2*, regularization similar to the Ridge and Lasso linear regression
    models by modifying Ω(*h[m]*) or by increasing the penalty γ for adding more trees, as
    described previously.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The libraries also allow for the use of bagging or column subsampling to randomize
    tree growth for random forests and decorrelate prediction errors to reduce overall
    variance. The quantization of features for approximate split finding adds larger
    bins as an additional option to protect against overfitting.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Randomized grid search
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To explore the hyperparameter space, we specify values for key parameters that
    we would like to test in combination. The sklearn library supports `RandomizedSearchCV` to
    cross-validate a subset of parameter combinations that are sampled randomly from
    specified distributions. We will implement a custom version that allows us to
    leverage early stopping while monitoring the current best-performing combinations
    so we can abort the search process once satisfied with the result rather than
    specifying a set number of iterations beforehand.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we specify a parameter grid according to each library''s parameters
    as before, generate all combinations using the built-in Cartesian `product` generator
    provided by the `itertools` library, and randomly `shuffle` the result. In the
    case of LightGBM, we automatically set `max_depth` as a function of the current
    `num_leaves` value, as shown in the following code:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We then execute cross-validation as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `run_cv` function implements cross-validation for all three libraries.
    For the `light_gbm` example, the process looks as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `train()` method also produces validation scores that are stored in the
    `scores` dictionary. When early stopping takes effect, the last iteration is also
    the best score. See the full implementation on GitHub for additional details.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: How to evaluate the results
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using a GPU, we can train a model in a few minutes and evaluate several hundred
    parameter combinations in a matter of hours, which would take many days using
    the sklearn implementation. For the LightGBM model, we explore both a factor version
    that uses the libraries' ability to handle categorical variables and a dummy version
    that uses one-hot encoding.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The results are available in the `model_tuning.h5` HDF5 store. The model evaluation
    code samples are in the `eval_results.ipynb` notebook.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation results across models
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When comparing average cross-validation AUC across the four test runs with
    the three libraries, we find that CatBoost produces a slightly higher AUC score
    for the top-performing model, while also producing the widest dispersion of outcomes,
    as shown in the following graph:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9acfd13-bad6-4969-8a22-b15d1678ac06.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: 'The top-performing CatBoost model uses the following parameters (see notebook
    for detail):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth` of 12 and `max_bin` of 128'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_ctr_complexity` of 2, which limits the number of combinations of categorical
    features'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`one_hot_max_size` of 2, which excludes binary features from the assignment
    of numerical variables'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_strength` different from 0 to randomize the evaluation of splits'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training is a bit slower compared to LightGBM and XGBoost (all use the GPU)
    at an average of 230 seconds per model.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'A more detailed look at the top-performing models for the LightGBM and XGBoost
    models shows that the LightGBM Factors model achieves nearly as good a performance
    as the other two models with much lower model complexity. It only consists on
    average of 41 trees up to three levels deep with no more than eight leaves each,
    while also using regularization in the form of `min_gain_to_split`. It overfits
    significantly less on the training set, with a train AUC only slightly above the
    validation AUC. It also trains much faster, taking only 18 seconds per model because
    of its lower complexity. In practice, this model would be preferable since it
    is more likely to produce good out-of-sample performance. The details are shown
    in the following table:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LightGBM dummies** | **XGBoost dummies** | **LightGBM factors** |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| Validation AUC | 68.57% | 68.36% | 68.32% |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| Train AUC | 82.35% | 79.81% | 72.12% |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| `learning_rate` | 0.1 | 0.1 | 0.3 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| `max_depth` | 13 | 9 | 3 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| `num_leaves` | 8192 |  | 8 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| `colsample_bytree` | 0.8 | 1 | 1 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| `min_gain_to_split` | 0 | 1 | 0 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| Rounds | 44.42 | 59.17 | 41.00 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| Time | 86.55 | 85.37 | 18.78 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: 'The following plot shows the effect of different `max_depth` settings on the
    validation score for the LightGBM and XGBoost models: shallower trees produce
    a wider range of outcomes and need to be combined with appropriate learning rates
    and regularization settings to produce the strong result shown in the preceding
    table:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8ccc54a-3cb5-4f16-8ac5-e7c0763679ca.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: 'Instead of a `DecisionTreeRegressor` as shown previously, we can also use linear
    regression to evaluate the statistical significance of different features concerning
    the validation AUC score. For the LightGBM Dummy model, where the regression explains
    68% of the variation in outcomes, we find that only the `min_gain_to_split` regularization
    parameter was not significant, as shown in the following screenshot:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b04533f-fb2a-4f43-9267-3eb1c10d0fce.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: In practice, gaining deeper insights into how the models arrive at predictions
    is extremely important, in particular for investment strategies where decision
    makers often require plausible explanations.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: How to interpret GBM results
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding why a model predicts a certain outcome is very important for several
    reasons, including trust, actionability, accountability, and debugging. Insights
    into the nonlinear relationship between features and the outcome uncovered by
    the model, as well as interactions among features, are also of value when the
    goal is to learn more about the underlying drivers of the phenomenon under study.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: A common approach to gaining insights into the predictions made by tree ensemble
    methods, such as gradient boosting or random forest models, is to attribute feature
    importance values to each input variable. These feature importance values can
    be computed on an individual basis for a single prediction or globally for an
    entire dataset (that is, for all samples) to gain a higher-level perspective on
    how the model makes predictions.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three primary ways to compute **g****lobal feature importance** values:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '**Gain**: This classic approach introduced by Leo Breiman in 1984 uses the
    total reduction of loss or impurity contributed by all splits for a given feature.
    The motivation is largely heuristic, but it is a commonly used method to select
    features.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Split count**: This is an alternative approach that counts how often a feature
    is used to make a split decision, based on the selection of features for this
    purpose based on the resultant information gain.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Permutation**: This approach randomly permutes the feature values in a test
    set and measures how much the model''s error changes, assuming that an important
    feature should create a large increase in the prediction error. Different permutation
    choices lead to alternative implementations of this basic approach.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Individualized feature importance values that compute the relevance of features
    for a single prediction are less common because available model-agnostic explanation
    methods are much slower than tree-specific methods.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'All gradient boosting implementations provide feature-importance scores after
    training as a model attribute. The XGBoost library provides five versions, as
    shown in the following list:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '`total_gain` and `gain` as its average per split'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_cover` as the number of samples per split when a feature was used'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight` as the split count from preceding values'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These values are available using the trained model''s `.get_score()` method
    with the corresponding `importance_type` parameter. For the best performing XGBoost
    model, the results are as follows (the *total* measures have a correlation of
    0.8, as do `cover` and `total_cover`):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d12d54b5-9bf1-4028-b0c3-0bad6902c658.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: While the indicators for different months and years dominate, the most recent
    1 month return is the second-most important feature from a `total_gain` perspective,
    and is used frequently according to the `weight` measure, but produces low average
    gains as it is applied to relatively few instances on average (see the notebook
    for implementation details).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Partial dependence plots
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the summary contribution of individual features to the model's
    prediction, partial dependence plots visualize the relationship between the target
    variable and a set of features. The nonlinear nature of gradient boosting trees
    causes this relationship to depends on the values of all other features. Hence,
    we will marginalize these features out. By doing so, we can interpret the partial
    dependence as the expected target response.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize partial dependence only for individual features or feature
    pairs. The latter results in contour plots that show how combinations of feature
    values produce different predicted probabilities, as shown in the following code:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After some additional formatting (see the companion notebook), we obtain the
    following plot:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba89c98e-9f72-4e8d-a595-3ac2e89483f5.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: 'The lower-right plot shows the dependence of the probability of a positive
    return over the next month given the range of values for lagged 1-month and 3-month
    returns after eliminating outliers at the [1%, 99%] percentiles. The `month_9` variable is
    a dummy variable, hence the step-function-like plot. We can also visualize the
    dependency in 3D, as shown in the following code:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This produces the following 3D plot of the partial dependence of the 1-month
    return direction on lagged 1-month and 3-months returns:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/227d7528-588a-4af8-8be9-5e8971f7304c.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: SHapley Additive exPlanations
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the 2017 NIPS conference, Scott Lundberg and Su-In Lee from the University
    of Washington presented a new and more accurate approach to explaining the contribution
    of individual features to the output of tree ensemble models called **SHapley
    Additive exPlanations**, or **SHAP** values.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: This new algorithm departs from the observation that feature-attribution methods
    for tree ensembles, such as the ones we looked at earlier, are inconsistent—that
    is, a change in a model that increases the impact of a feature on the output can
    lower the importance values for this feature (see the references on GitHub for
    detailed illustrations of this).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: SHAP values unify ideas from collaborative game theory and local explanations,
    and have been shown to be theoretically optimal, consistent, and locally accurate
    based on expectations. Most importantly, Lundberg and Lee have developed an algorithm
    that manages to reduce the complexity of computing these model-agnostic, additive
    feature-attribution methods from *O*(*TLD^M*) to *O*(*TLD*²), where *T* and *M*
    are the number of trees and features, respectively, and *D* and *L* are the maximum
    depth and number of leaves across the trees. This important innovation permits
    the explanation of predictions from previously intractable models with thousands
    of trees and features in a fraction of a second. An open source implementation
    became available in late 2017 and is compatible with XGBoost, LightGBM, CatBoost,
    and sklearn tree models.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Shapley values originated in game theory as a technique for assigning a value
    to each player in a collaborative game that reflects their contribution to the
    team's success. SHAP values are an adaptation of the game theory concept to tree-based
    models and are calculated for each feature and each sample. They measure how a
    feature contributes to the model output for a given observation. For this reason,
    SHAP values provide differentiated insights into how the impact of a feature varies
    across samples, which is important given the role of interaction effects in these
    nonlinear models.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: How to summarize SHAP values by feature
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get a high-level overview of the feature importance across a number of samples,
    there are two ways to plot the SHAP values: a simple average across all samples
    that resembles the global feature-importance measures computed previously (as
    shown in the left-hand panel of the following screenshot), or a scatter graph
    to display the impact of every feature for every sample (as shown in the right-hand
    panel of the following screenshot). They are very straightforward to produce using
    a trained model of a compatible library and matching input data, as shown in the
    following code:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The scatter plot on the right of the following screenshot sorts features by
    their total SHAP values across all samples, and then shows how each feature impacts
    the model output as measured by the SHAP value as a function of the feature''s
    value, represented by its color, where red represents high and blue represents
    low values relative to the feature''s range:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6666557-7466-4c75-9365-9631133abbf8.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: How to use force plots to explain a prediction
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following force plot shows the cumulative impact of various features and
    their values on the model output, which in this case was 0.6, quite a bit higher
    than the base value of 0.13 (the average model output over the provided dataset).
    Features highlighted in red increase the output. The month being October is the
    most important feature and increases the output from 0.338 to 0.537, whereas the
    year being 2017 reduces the output.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, we obtain a detailed breakdown of how the model arrived at a specific
    prediction, as shown in the following image:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6352fa7-b7b2-403a-8d22-c7a3b866a7a0.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: 'We can also compute force plots for numerous data points or predictions at
    a time and use a clustered visualization to gain insights into how prevalent certain
    influence patterns are across the dataset. The following plot shows the force
    plots for the first 1,000 observations rotated by 90 degrees, stacked horizontally,
    and ordered by the impact of different features on the outcome for the given observation.
    The implementation uses hierarchical agglomerative clustering of data points on
    the feature SHAP values to identify these patterns, and displays the result interactively
    for exploratory analysis (see the notebook), as shown in the following code:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This produces the following output:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee376d94-c6bc-4f46-90f2-dbf758860722.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: How to analyze feature interaction
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lastly, SHAP values allow us to gain additional insights into the interaction
    effects between different features by separating these interactions from the main
    effects. The `shap.dependence_plot`  can be defined as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It displays how different values for 1-month returns (on the *x* axis) affect
    the outcome (SHAP value on the *y* axis), differentiated by 3-month returns:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f5b68f1-0ccc-48a4-a0a0-f10c71f91121.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: SHAP values provide granular feature attribution at the level of each individual prediction,
    and enable much richer inspection of complex models through (interactive) visualization.
    The SHAP summary scatterplot displayed at the beginning of this section offers
    much more differentiated insights than a global feature-importance bar chart.
    Force plots of individual clustered predictions allow for more detailed analysis,
    while SHAP dependence plots capture interaction effects and, as a result, provide
    more accurate and detailed results than partial dependence plots.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: The limitations of SHAP values, as with any current feature-importance measure,
    concern the attribution of the influence of variables that are highly correlated
    because their similar impact could be broken down in arbitrary ways.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the gradient boosting algorithm, which is used
    to build ensembles in a sequential manner, adding a shallow decision tree that
    only uses a very small number of features to improve on the predictions that have
    been made. We saw how gradient boosting trees can be very flexibly applied to
    a broad range of loss functions and offer many opportunities to tune the model
    to a given dataset and learning task.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Recent implementations have greatly facilitated the use of gradient boosting
    by accelerating the training process and offering more consistent and detailed
    insights into the importance of features and the drivers of individual predictions.
    In the next chapter, we will turn to Bayesian approaches to ML.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
