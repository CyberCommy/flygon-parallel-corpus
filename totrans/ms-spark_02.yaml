- en: Chapter 2. Apache Spark MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLlib is the machine learning library that is provided with Apache Spark, the
    in memory cluster based open source data processing system. In this chapter, I
    will examine the functionality, provided within the MLlib library in terms of
    areas such as regression, classification, and neural processing. I will examine
    the theory behind each algorithm before providing working examples that tackle
    real problems. The example code and documentation on the web can be sparse and
    confusing. I will take a step-by-step approach in describing how the following
    algorithms can be used, and what they are capable of doing:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification with Naïve Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering with K-Means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural processing with ANN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having decided to learn about Apache Spark, I am assuming that you are familiar
    with Hadoop. Before I proceed, I will explain a little about my environment. My
    Hadoop cluster is installed on a set of Centos 6.5 Linux 64 bit servers. The following
    section will describe the architecture in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The environment configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before delving into the Apache Spark modules, I wanted to explain the structure
    and version of Hadoop and Spark clusters that I will use in this book. I will
    be using the Cloudera CDH 5.1.3 version of Hadoop for storage and I will be using
    two versions of Spark: 1.0 and 1.3 in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: The earlier version is compatible with Cloudera software, and has been tested
    and packaged by them. It is installed as a set of Linux services from the Cloudera
    repository using the yum command. Because I want to examine the Neural Net technology
    that has not been released yet, I will also download and run the development version
    of Spark 1.3 from GitHub. This will be explained later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following diagram explains the structure of the small Hadoop cluster that
    I will use in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture](img/B01989_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The previous diagram shows a five-node Hadoop cluster with a NameNode called
    **hc2nn**, and DataNodes **hc2r1m1** to **hc2r1m4**. It also shows an Apache Spark
    cluster with a master node and four slave nodes. The Hadoop cluster provides the
    physical Centos 6 Linux machines while the Spark cluster runs on the same hosts.
    For instance, the Spark master server runs on the Hadoop Name Node machine **hc2nn**,
    whereas the Spark **slave1** worker runs on the host **hc2r1m1**.
  prefs: []
  type: TYPE_NORMAL
- en: The Linux server naming standard used higher up should be explained. For instance
    the Hadoop NameNode server is called hc2nn. The **h** in this server name means
    Hadoop, the **c** means cluster, and the **nn** means NameNode. So, hc2nn means
    Hadoop cluster 2 NameNode. Similarly, for the server hc2r1m1, the h means Hadoop
    the **c** means cluster the **r** means rack and the **m** means machine. So,
    the name stands for Hadoop cluster 2 rack 1 machine 1\. In a large Hadoop cluster,
    the machines will be organized into racks, so this naming standard means that
    the servers will be easy to locate.
  prefs: []
  type: TYPE_NORMAL
- en: You can arrange your Spark and Hadoop clusters as you see fit, they don't need
    to be on the same hosts. For the purpose of writing this book, I have limited
    machines available so it makes sense to co-locate the Hadoop and Spark clusters.
    You can use entirely separate machines for each cluster, as long as Spark is able
    to access Hadoop (if you want to use it for distributed storage).
  prefs: []
  type: TYPE_NORMAL
- en: Remember that although Spark is used for the speed of its in-memory distributed
    processing, it doesn't provide storage. You can use the Host file system to read
    and write your data, but if your data volumes are big enough to be described as
    big data, then it makes sense to use a distributed storage system like Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: Remember also that Apache Spark may only be the processing step in your **ETL**
    (**Extract**, **Transform**, **Load**) chain. It doesn't provide the rich tool
    set that the Hadoop ecosystem contains. You may still need Nutch/Gora/Solr for
    data acquisition; Sqoop and Flume for moving data; Oozie for scheduling; and HBase,
    or Hive for storage. The point that I am making is that although Apache Spark
    is a very powerful processing system, it should be considered a part of the wider
    Hadoop ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Having described the environment that will be used in this chapter, I will move
    on to describe the functionality of the Apache Spark **MLlib** (**Machine Learning
    library**).
  prefs: []
  type: TYPE_NORMAL
- en: The development environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Scala language will be used for coding samples in this book. This is because
    as a scripting language, it produces less code than Java. It can also be used
    for the Spark shell, as well as compiled with Apache Spark applications. I will
    be using the sbt tool to compile the Scala code, which I have installed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For convenience while writing this book, I have used the generic Linux account
    called **hadoop** on the Hadoop NameNode server `hc2nn`. As the previous commands
    show that I need to install `sbt` as the root account, which I have accessed via
    `su` (switch user). I have then downloaded the `sbt.rpm` file, to the `/tmp` directory,
    from the web-based server called `repo.scala-sbt.org` using `wget`. Finally, I
    have installed the `rpm` file using the `rpm` command with the options `i` for
    install, `v` for verify, and `h` to print the hash marks while the package is
    being installed.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Downloading the example code**'
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all the Packt Publishing books you have purchased. If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have developed all of the Scala code for Apache Spark, in this chapter, on
    the Linux server `hc2nn`, using the Linux hadoop account. I have placed each set
    of code within a sub directory under `/home/hadoop/spark`. For instance, the following
    sbt structure diagram shows that the MLlib Naïve Bayes code is stored within a
    subdirectory called `nbayes`, under the `spark` directory. What the diagram also
    shows is that the Scala code is developed within a subdirectory structure named
    `src/main/scala`, under the `nbayes` directory. The files called `bayes1.scala`
    and `convert.scala` contain the Naïve Bayes code that will be used in the next
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The development environment](img/B01989_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `bayes.sbt` file is a configuration file used by the sbt tool, which describes
    how to compile the Scala files within the `Scala` directory (also note that if
    you were developing in Java, you would use a path of the form `nbayes/src/main/java`).
    The contents of the `bayes.sbt` file are shown next. The `pwd` and `cat` Linux
    commands remind you of the file location, and they also remind you to dump the
    file contents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The name, version, and `scalaVersion` options set the details of the project,
    and the version of Scala to be used. The `libraryDependencies` options define
    where the Hadoop and Spark libraries can be located. In this case, CDH5 has been
    installed using the Cloudera parcels, and the packages libraries can be located
    in the standard locations, that is, `/usr/lib/hadoop` for Hadoop and `/usr/lib/spark`
    for Spark. The resolver''s option specifies the location for the Cloudera repository
    for other dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The Scala nbayes project code can be compiled from the `nbayes` sub directory
    using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sbt compile` command is used to compile the code into classes. The classes
    are then placed in the `nbayes/target/scala-2.10/classes` directory. The compiled
    classes can be packaged into a JAR file with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `sbt package` command will create a JAR file under the directory `nbayes/target/scala-2.10`.
    As the example in the *sbt structure diagram* shows the JAR file named `naive-bayes_2.10-1.0.jar`
    has been created after a successful compile and package. This JAR file, and the
    classes that it contains, can then be used in a `spark-submit` command. This will
    be described later as the functionality in the Apache Spark MLlib module is explored.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, when describing the environment used for this book, I wanted to touch
    on the approach to installing and running Apache Spark. I won't elaborate on the
    Hadoop CDH5 install, except to say that I installed it using the Cloudera parcels.
    However, I manually installed version 1.0 of Apache Spark from the Cloudera repository,
    using the Linux `yum` commands. I installed the service-based packages, because
    I wanted the flexibility that would enable me to install multiple versions of
    Spark as services from Cloudera, as I needed.
  prefs: []
  type: TYPE_NORMAL
- en: When preparing a CDH Hadoop release, Cloudera takes the code that has been developed
    by the Apache Spark team, and the code released by the Apache Bigtop project.
    They perform an integration test so that it is guaranteed to work as a code stack.
    They also reorganize the code and binaries into services and parcels. This means
    that libraries, logs, and binaries can be located in defined locations under Linux,
    that is, `/var/log/spark`, `/usr/lib/spark`. It also means that, in the case of
    services, the components can be installed using the Linux `yum` command, and managed
    via the Linux `service` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although, in the case of the Neural Network code described later in this chapter,
    a different approach was used. This is how Apache Spark 1.0 was installed for
    use with Hadoop CDH5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step is to ensure that a Cloudera repository file exists under the
    `/etc/yum.repos.d` directory, on the server `hc2nn` and all of the other Hadoop
    cluster servers. The file is called `cloudera-cdh5.repo`, and specifies where
    the yum command can locate software for the Hadoop CDH5 cluster. On all the Hadoop
    cluster nodes, I use the Linux yum command, as root, to install the Apache Spark
    components core, master, worker, history-server, and python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives me the flexibility to configure Spark in any way that I want in
    the future. Note that I have installed the master component on all the nodes,
    even though I only plan to use it from the Name Node at this time. Now, the Spark
    install needs to be configured on all the nodes. The configuration files are stored
    under `/etc/spark/conf`. The first thing to do, will be to set up a `slaves` file,
    which specifies on which hosts Spark will run it''s worker components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the contents of the `slaves` file above Spark, it will
    run four workers on the Hadoop CDH5 cluster, Data Nodes, from `hc2r1m1` to `hc2r1m4`.
    Next, it will alter the contents of the `spark-env.sh` file to specify the Spark
    environment options. The `SPARK_MASTER_IP` values are defined as the full server
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The web user interface port numbers are specified for the master and worker
    processes, as well as the operational port numbers. The Spark service can then
    be started as root from the Name Node server. I use the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This starts the Spark worker service on all of the slaves, and the master and
    history server on the Name Node `hc2nn`. So now, the Spark user interface can
    be accessed using the `http://hc2nn:18080` URL.
  prefs: []
  type: TYPE_NORMAL
- en: The following figure shows an example of the Spark 1.0 master web user interface.
    It shows details about the Spark install, the workers, and the applications that
    are running or completed. The statuses of the master and workers are given. In
    this case, all are alive. Memory used and availability is given in total and by
    worker. Although, there are no applications running at the moment, each worker
    link can be selected to view the executor processes' running on each worker node,
    as the work volume for each application run is spread across the spark cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Note also the Spark URL, `spark://hc2nn.semtech-solutions.co.nz:7077`, will
    be used when running the Spark applications like `spark-shell` and `spark-submit`.
    Using this URL, it is possible to ensure that the shell or application is run
    against this Spark cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing Spark](img/B01989_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This gives a quick overview of the Apache Spark installation using services,
    its configuration, how to start it, and how to monitor it. Now, it is time to
    tackle the first of the MLlib functional areas, which is classification using
    the Naïve Bayes algorithm. The use of Spark will become clearer as Scala scripts
    are developed, and the resulting applications are monitored.
  prefs: []
  type: TYPE_NORMAL
- en: Classification with Naïve Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will provide a working example of the Apache Spark MLlib Naïve
    Bayes algorithm. It will describe the theory behind the algorithm, and will provide
    a step-by-step example in Scala to show how the algorithm may be used.
  prefs: []
  type: TYPE_NORMAL
- en: Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to use the Naïve Bayes algorithm to classify a data set, the data
    must be linearly divisible, that is, the classes within the data must be linearly
    divisible by class boundaries. The following figure visually explains this with
    three data sets, and two class boundaries shown via the dotted lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Theory](img/B01989_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Naïve Bayes assumes that the features (or dimensions) within a data set are
    independent of one another, that is, they have no effect on each other. An example
    for Naïve Bayes is supplied with the help of Hernan Amiune at [http://hernan.amiune.com/](http://hernan.amiune.com/).
    The following example considers the classification of emails as spam. If you have
    100 e-mails then perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Thus, convert this example into probabilities, so that a Naïve Bayes equation
    can be created.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'So, what is the probability that an e-mail that contains the word buy is spam?
    Well, this would be written as **P (Spam|Buy)**. Naïve Bayes says that it is described
    by the equation in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Theory](img/B01989_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, using the previous percentage figures, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This means that it is 92 percent more likely that an e-mail that contains the
    word buy is spam. That was a look at the theory; now, it's time to try a real
    world example using the Apache Spark MLlib Naïve Bayes algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step is to choose some data that will be used for classification.
    I have chosen some data from the UK government data web site, available at: [http://data.gov.uk/dataset/road-accidents-safety-data](http://data.gov.uk/dataset/road-accidents-safety-data).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data set is called "Road Safety - Digital Breath Test Data 2013," which
    downloads a zipped text file called `DigitalBreathTestData2013.txt`. This file
    contains around half a million rows. The data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to classify the data, I have modified both the column layout, and
    the number of columns. I have simply used Excel to give the data volume. However,
    if my data size had been in the big data range, I would have had to use Scala,
    or perhaps a tool like Apache Pig. As the following commands show, the data now
    resides on HDFS, in the directory named `/data/spark/nbayes`. The file name is
    called `DigitalBreathTestData2013- MALE2.csv`. Also, the line count from the Linux
    `wc` command shows that there are 467,000 rows. Finally, the following data sample
    shows that I have selected the columns: Gender, Reason, WeekType, TimeBand, BreathAlcohol,
    and AgeBand to classify. I will try and classify on the Gender column using the
    other columns as features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The Apache Spark MLlib classification functions use a data structure called
    `LabeledPoint`, which is a general purpose data representation defined at: [http://spark.apache.org/docs/1.0.0/api/scala/index.html#org.apache.spark.mllib.regression.LabeledPoint](http://spark.apache.org/docs/1.0.0/api/scala/index.html#org.apache.spark.mllib.regression.LabeledPoint).'
  prefs: []
  type: TYPE_NORMAL
- en: This structure only accepts Double values, which means the text values in the
    previous data need to be classified numerically. Luckily, all of the columns in
    the data will convert to numeric categories, and I have provided two programs
    in the software package with this book, under the directory `chapter2\naive bayes`
    to do just that. The first is called `convTestData.pl`, and is a Perl script to
    convert the previous text file into Linux. The second file, which will be examined
    here is called `convert.scala`. It takes the contents of the `DigitalBreathTestData2013-
    MALE2.csv` file and converts each record into a Double vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The directory structure and files for an sbt Scala-based development environment
    have already been described earlier. I am developing my Scala code on the Linux
    server `hc2nn` using the Linux account hadoop. Next, the Linux `pwd` and `ls`
    commands show my top level `nbayes` development directory with the `bayes.sbt`
    configuration file, whose contents have already been examined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The Scala code to run the Naïve Bayes example is shown next, in the `src/main/scala`
    subdirectory, under the `nbayes` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will examine the `bayes1.scala` file later, but first, the text-based data
    on HDFS must be converted into the numeric Double values. This is where the `convert.scala`
    file is used. The code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'These lines import classes for Spark context, the connection to the Apache
    Spark cluster, and the Spark configuration. The object that is being created is
    called `convert1`. It is an application, as it extends the class `App`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The next line creates a function called `enumerateCsvRecord`. It has a parameter
    called `colData`, which is an array of strings, and returns a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The function then enumerates the text values in each column, so for an instance,
    `Male` becomes `0`. These numeric values are stored in values like `colVal1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'A comma separated string called `lineString` is created from the numeric column
    values, and is then returned. The function closes with the final brace character`}`.
    Note that the data line created next starts with a label value at column one,
    and is followed by a vector, which represents the data. The vector is space separated
    while the label is separated from the vector by a comma. Using these two separator
    types allows me to process both: the label and the vector in two simple steps
    later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The main script defines the HDFS server name and path. It defines the input
    file, and the output path in terms of these values. It uses the Spark URL and
    application name to create a new configuration. It then creates a new context
    or connection to Spark using these details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The CSV-based raw data file is loaded from HDFS using the Spark context `textFile`
    method. Then, a data row count is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The CSV raw data is passed line by line to the `enumerateCsvRecord` function.
    The returned string-based numeric data is stored in the `enumRddData` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the number of records in the `enumRddData` variable is printed, and
    the enumerated data is saved to HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to run this script as an application against Spark, it must be compiled.
    This is carried out with the `sbt` package command, which also compiles the code.
    The following command was run from the `nbayes` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This causes the compiled classes that are created to be packaged into a JAR
    library, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The application `convert1` can now be run against Spark using the application
    name, the Spark URL, and the full path to the JAR file that was created. Some
    extra parameters specify memory and maximum cores that are supposed to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates a data directory on HDFS called the `/data/spark/nbayes/` followed
    by the result, which contains part files, containing the processed data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following HDFS `cat` command, I have concatenated the part file data
    into a file called `DigitalBreathTestData2013-MALE2a.csv`. I have then examined
    the top five lines of the file using the `head` command to show that it is numeric.
    Finally, I have loaded it into HDFS with the `put` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following HDFS `ls` command now shows the numeric data file stored on HDFS,
    in the `nbayes` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the data has been converted into a numeric form, it can be processed
    with the MLlib Naïve Bayes algorithm; this is what the Scala file `bayes1.scala`
    does. This file imports the same configuration and context classes as before.
    It also imports MLlib classes for Naïve Bayes, vectors, and the LabeledPoint structure.
    The application class that is created this time is called `bayes1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, the HDFS data file is defined, and a Spark context is created as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The raw CSV data is loaded and split by the separator characters. The first
    column becomes the label (`Male/Female`) that the data will be classified upon.
    The final columns separated by spaces become the classification features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is then randomly divided into training (70%) and testing (30%) data
    sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The Naïve Bayes MLlib function can now be trained using the previous training
    set. The trained Naïve Bayes model, held in the variable `nbTrained`, can then
    be used to predict the `Male/Female` result labels against the testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that all of the data already contained labels, the original and predicted
    labels for the test data can be compared. An accuracy figure can then be computed
    to determine how accurate the predictions were, by comparing the original labels
    with the prediction values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'So this explains the Scala Naïve Bayes code example. It''s now time to run
    the compiled `bayes1` application using `spark-submit`, and to determine the classification
    accuracy. The parameters are the same. It''s just the class name that has changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting accuracy given by the Spark cluster is just `43` percent, which
    seems to imply that this data is not suitable for Naïve Bayes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next example, I will use K-Means to try and determine what clusters
    exist within the data. Remember, Naïve Bayes needs the data classes to be linearly
    divisible along the class boundaries. With K-Means, it will be possible to determine
    both: the membership and centroid location of the clusters within the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering with K-Means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This example will use the same test data from the previous example, but will
    attempt to find clusters in the data using the MLlib K-Means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The K-Means algorithm iteratively attempts to determine clusters within the
    test data by minimizing the distance between the mean value of cluster center
    vectors, and the new candidate cluster member vectors. The following equation
    assumes data set members that range from **X1** to **Xn**; it also assumes **K**
    cluster sets that range from **S1** to **Sk** where **K <= n**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Theory](img/B01989_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: K-Means in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Again, the K-Means MLlib functionality uses the LabeledPoint structure to process
    its data and so, it needs numeric input data. As the same data from the last section
    is being reused, I will not re-explain the data conversion. The only change that
    has been made in data terms, in this section, is that processing under HDFS will
    now take place under the `/data/spark/kmeans/` directory**.** Also, the conversion
    Scala script for the K-Means example produces a record that is all comma separated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The development and processing for the K-Means example has taken place under
    the `/home/hadoop/spark/kmeans` directory, to separate the work from other development.
    The sbt configuration file is now called `kmeans.sbt`, and is identical to the
    last example, except for the project name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The code for this section can be found in the software package under `chapter2\K-Means`.
    So, looking at the code for `kmeans1.scala`, which is stored under `kmeans/src/main/scala`,
    some similar actions occur. The import statements refer to Spark context and configuration.
    This time, however, the K-Means functionality is also being imported from MLlib.
    Also, the application class name has been changed for this example to `kmeans1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The same actions are being taken as the last example to define the data file—define
    the Spark configuration and create a Spark context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the CSV data is loaded from the data file, and is split by comma characters
    into the variable `VectorData`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'A K-Means object is initialized, and the parameters are set to define the number
    of clusters, and the maximum number of iterations to determine them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Some default values are defined for initialization mode, the number of runs,
    and Epsilon, which I needed for the K-Means call, but did not vary for processing.
    Finally, these parameters were set against the K-Means object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'I cached the training vector data to improve the performance, and trained the
    K-Means object using the Vector Data to create a trained K-Means model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'I have computed the K-Means cost, the number of input data rows, and output
    the results via print line statements. The cost value indicates how tightly the
    clusters are packed, and how separated clusters are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, I have used the K-Means Model to print the cluster centers as vectors
    for each of the three clusters that were computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, I have used the K-Means Model `predict` function to create a list
    of cluster membership predictions. I have then counted these predictions by value
    to give a count of the data points in each cluster. This shows which clusters
    are bigger, and if there really are three clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'So, in order to run this application, it must be compiled and packaged from
    the `kmeans` subdirectory as the Linux `pwd` command shows here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this packaging is successful, I check HDFS to ensure that the test data
    is ready. As in the last example, I converted my data to numeric form using the
    `convert.scala` file, provided in the software package. I will process the data
    file `DigitalBreathTestData2013-MALE2a.csv` in the HDFS directory /`data/spark/kmeans`
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The `spark-submit` tool is used to run the K-Means application. The only change
    in this command, as shown here, is that the class is now `kmeans1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the Spark cluster run is shown to be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous output shows the input data volume, which looks correct, plus
    it also shows the K-Means cost value. Next comes the three vectors, which describe
    the data cluster centers with the correct number of dimensions. Remember that
    these cluster centroid vectors will have the same number of columns as the original
    vector data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Finally, cluster membership is given for clusters 1 to 3 with cluster 1 (index
    0) having the largest membership at `407,539` member vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: So, these two examples show how data can be classified and clustered using Naïve
    Bayes and K-Means. But what if I want to classify images or more complex patterns,
    and use a black box approach to classification? The next section examines Spark-based
    classification using **ANN's**, or **Artificial Neural Network's**. In order to
    do this, I need to download the latest Spark code, and build a server for Spark
    1.3, as it has not yet been formally released (at the time of writing).
  prefs: []
  type: TYPE_NORMAL
- en: ANN – Artificial Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to examine the **ANN** (Artificial Neural Network) functionality in
    Apache Spark, I will need to obtain the latest source code from the GitHub website.
    The ANN functionality has been developed by Bert Greevenbosch ([http://www.bertgreevenbosch.nl/](http://www.bertgreevenbosch.nl/)),
    and is set to be released in Apache Spark 1.3\. At the time of writing the current
    Spark release is 1.2.1, and CDH 5.x ships with Spark 1.0\. So, in order to examine
    this unreleased ANN functionality, the source code will need to be sourced and
    built into a Spark server. This is what I will do after explaining a little on
    the theory behind ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following figure shows a simple biological neuron to the left. The neuron
    has dendrites that receive signals from other neurons. A cell body controls activation,
    and an axon carries an electrical impulse to the dendrites of other neurons. The
    artificial neuron to the right has a series of weighted inputs: a summing function
    that groups the inputs, and a firing mechanism (**F(Net)**), which decides whether
    the inputs have reached a threshold, and if so, the neuron will fire:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Theory](img/B01989_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Neural networks are tolerant of noisy images and distortion, and so are useful
    when a black box classification method is needed for potentially degraded images.
    The next area to consider is the summation function for the neuron inputs. The
    following diagram shows the summation function called **Net** for neuron **i**.
    The connections between the neurons that have the weighting values, contain the
    stored knowledge of the network. Generally, a network will have an input layer,
    an output layer, and a number of hidden layers. A neuron will fire if the sum
    of its inputs exceeds a threshold.
  prefs: []
  type: TYPE_NORMAL
- en: '![Theory](img/B01989_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the previous equation, the diagram and the key show that the input values
    from a pattern **P** are passed to neurons in the input layer of a network. These
    values become the input layer neuron activation values; they are a special case.
    The inputs to neuron **i** are the sum of the weighting value for neuron connection
    **i-j**, multiplied by the activation from neuron **j**. The activation at neuron
    **j** (if it is not an input layer neuron) is given by **F(Net)**, the squashing
    function, which will be described next.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simulated neuron needs a firing mechanism, which decides whether the inputs
    to the neuron have reached a threshold. And then, it fires to create the activation
    value for that neuron. This firing or squashing function can be described by the
    generalized sigmoid function shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Theory](img/B01989_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This function has two constants: **A** and **B**; **B** affects the shape of
    the activation curve as shown in the previous graph. The bigger the value, the
    more similar a function becomes to an on/off step. The value of **A** sets a minimum
    for the returned activation. In the previous graph it is zero.'
  prefs: []
  type: TYPE_NORMAL
- en: So, this provides a mechanism for simulating a neuron, creating weighting matrices
    as the neuron connections, and managing the neuron activation. But how are the
    networks organized? The next diagram shows a suggested neuron architecture—the
    neural network has an input layer of neurons, an output layer, and one or more
    hidden layers. All neurons in each layer are connected to each neuron in the adjacent
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Theory](img/B01989_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: During the training, activation passes from the input layer through the network
    to the output layer. Then, the error or difference between the expected or actual
    output causes error deltas to be passed back through the network, altering the
    weighting matrix values. Once the desired output layer vector is achieved, then
    the knowledge is stored in the weighting matrices, and the network can be further
    trained or used for classification.
  prefs: []
  type: TYPE_NORMAL
- en: So, the theory behind neural networks has been described in terms of back propagation.
    Now is the time to obtain the development version of the Apache Spark code, and
    build the Spark server, so that the ANN Scala code can be run.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Spark server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I would not normally advise that Apache Spark code be downloaded and used before
    it has been released by Spark, or packaged by Cloudera (for use with CDH), but
    the desire to examine ANN functionality, along with the time scale allowed for
    this book, mean that I need to do so. I extracted the full Spark code tree from
    this path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'I stored this code on the Linux server `hc2nn`, under the directory `/home/hadoop/spark/spark`.
    I then obtained the ANN code from Bert Greevenbosch''s GitHub development area:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ANNClassifier.scala` file contains the public functions that will be called.
    The `ArtificialNeuralNetwork.scala` file contains the private MLlib ANN functions
    that `ANNClassifier.scala` calls. I already have Java open JDK installed on my
    server, so the next step is to set up the `spark-env.sh` environment configuration
    file under `/home/hadoop/spark/spark/conf`. My file looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The `SPARK_MASTER_IP` variable tells the cluster which server is the master.
    The port variables define the master, the worker web, and the operating port values.
    There are some log and JAR file paths defined, as well as `JAVA_HOME` and the
    local server IP address. Details for building Spark with Apache Maven can be found
    at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The slaves file in the same directory will be set up as before with the names
    of the four workers servers from `hc2r1m1` to `hc2r1m4`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to build using Apache Maven, I had to install `mvn` on to my Linux
    server `hc2nn`, where I will run the Spark build. I did this as the root user,
    obtaining a Maven repository file by first using `wget`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Then, checking that the new repository file is in place with `ls` long listing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Then Maven can be installed using the Linux `yum` command, the examples below
    show the install command and a check via `ls` that the `mvn` command exists.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The commands that I have used to build the Spark source tree are shown here
    along with the successful output. First, the environment is set up, and then the
    build is started with the `mvn` command. Options are added to build for Hadoop
    2.3/yarn, and the tests are skipped. The build uses the `clean` and `package`
    options to remove the old build files each time, and then create JAR files. Finally,
    the build output is copied via the `tee` command to a file named `build.log`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The actual build command that you use will depend upon whether you have Hadoop,
    and the version of it. Check the previous *building spark* for details, the build
    takes around 40 minutes on my servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that this build will be packaged and copied to the other servers in the
    Spark cluster, it is important that all the servers use the same version of Java,
    else errors such as these will occur:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that the source tree has been built, it now needs to be bundled up and
    released to each of the servers in the Spark cluster. Given that these servers
    are also the members of the CDH cluster, and have password-less SSH access set
    up, I can use the `scp` command to release the built software. The following commands
    show the spark directory under the `/home/hadoop/spark` path being packaged into
    a tar file called `spark_bld.tar`. The Linux `scp` command is then used to copy
    the tar file to each slave server; the following example shows `hc2r1m1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the tarred Spark build is on the slave node, it needs to be unpacked.
    The following command shows the process for the server `hc2r1m1`. The tar file
    is unpacked to the same directory as the build server `hc2nn`, that is, `/home/hadoop/spark`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the build has been run successfully, and the built code has been released
    to the slave servers, the built version of Spark can be started from the master
    server **hc2nn**. Note that I have chosen different port numbers from the Spark
    version 1.0, installed on these servers. Also note that I will start Spark as
    root, because the Spark 1.0 install is managed as Linux services under the root
    account. As the two installs will share facilities like logging and `.pid` file
    locations, root user will ensure access. This is the script that I have used to
    start Apache Spark 1.3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'It executes the `spark-env.sh` file to set up the environment, and then uses
    the scripts in the Spark `sbin` directory to start the services. It starts the
    master and the history server first on `hc2nn`, and then it starts the slaves.
    I added a delay before starting the slaves, as I found that they were trying to
    connect to the master before it was ready. The Spark 1.3 web user interface can
    now be accessed via this URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The Spark URL, which allows applications to connect to Spark is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: As defined by the port numbers in the spark environment configuration file,
    Spark is now available to be used with ANN functionality. The next section will
    present the ANN Scala scripts and data to show how this Spark-based functionality
    can be used.
  prefs: []
  type: TYPE_NORMAL
- en: ANN in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to begin ANN training, test data is needed. Given that this type of
    classification method is supposed to be good at classifying distorted or noisy
    images, I have decided to attempt to classify the images here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ANN in practice](img/B01989_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'They are hand-crafted text files that contain shaped blocks, created from the
    characters 1 and 0\. When they are stored on HDFS, the carriage return characters
    are removed, so that the image is presented as a single line vector. So, the ANN
    will be classifying a series of shape images, and then it will be tested against
    the same images with noise added to determine whether the classification will
    still work. There are six training images, and they will each be given an arbitrary
    training label from 0.1 to 0.6\. So, if the ANN is presented with a closed square,
    it should return a label of 0.1\. The following image shows an example of a testing
    image with noise added. The noise, created by adding extra zero (0) characters
    within the image, has been highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ANN in practice](img/B01989_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Because the Apache Spark server has changed from the previous examples, and
    the Spark library locations have also changed, the `sbt` configuration file used
    for compiling the example ANN Scala code must also be changed. As before, the
    ANN code is being developed using the Linux hadoop account in a subdirectory called
    `spark/ann`. The `ann.sbt` file exists within the `ann` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The contents of the `ann.sbt` file have been changed to use full paths of JAR
    library files for the Spark dependencies. This is because the new Apache Spark
    code for build 1.3 now resides under `/home/hadoop/spark/spark`. Also, the project
    name has been changed to `A N N`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the previous examples, the actual Scala code to be compiled exists in
    a subdirectory named `src/main/scala` as shown next. I have created two Scala
    programs. The first trains using the input data, and then tests the ANN model
    with the same input data. The second tests the trained model with noisy data,
    to the test distorted data classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'I will examine the first Scala file entirely, and then I will just show the
    extra features of the second file, as the two examples are very similar up to
    the point of training the ANN. The code examples shown here can be found in the
    software package provided with this book, under the path `chapter2\ANN`. So, to
    examine the first Scala example, the import statements are similar to the previous
    examples. The Spark context, configuration, vectors, and `LabeledPoint` are being
    imported. The RDD class for RDD processing is being imported this time, along
    with the new ANN class `ANNClassifier`. Note that the `MLlib/classification` routines
    widely use the `LabeledPoint` structure for input data, which will contain the
    features and labels that are supposed to be trained against:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The application class in this example has been called `testann1`. The HDFS
    files to be processed have been defined in terms of the HDFS server, path, and
    file name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The Spark context has been created with the URL for the Spark instance, which
    now has a different port number—`8077`. The application name is `ANN 1`. This
    will appear on the Spark web UI when the application is run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The HDFS-based input training and test data files are loaded. The values on
    each line are split by space characters, and the numeric values have been converted
    into Doubles. The variables that contain this data are then stored in an array
    called inputs. At the same time, an array called outputs is created, containing
    the labels from 0.1 to 0.6\. These values will be used to classify the input patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The input and output data, representing the input data features and labels,
    are then combined and converted into a `LabeledPoint` structure. Finally, the
    data is parallelized in order to partition it for the optimal parallel processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Variables are created to define the hidden layer topology of the ANN. In this
    case, I have chosen to have two hidden layers, each with 100 neurons. The maximum
    numbers of iterations are defined, as well as a batch size (six patterns) and
    convergence tolerance. The tolerance refers to how big the training error can
    get before we can consider training to have worked. Then, an ANN model is created
    using these configuration parameters and the input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to test the trained ANN model, the same input training data is used
    as testing data used to obtain prediction labels. First, an input data variable
    is created called `rPredictData`. Then, the data is partitioned and finally, the
    predictions are obtained using the trained ANN model. For this model to work,
    it must output the labels 0.1 to 0.6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The label predictions are printed, and the script closes with a closing bracket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'So, in order to run this code sample, it must first be compiled and packaged.
    By now, you must be familiar with the `sbt` command, executed from the `ann` sub
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'The `spark-submit` command is then used from within the new `spark/spark` path
    using the new Spark-based URL at port 8077 to run the application `testann1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'By checking the Apache Spark web URL at `http://hc2nn.semtech-solutions.co.nz:19080/`,
    it is now possible to see the application running. The following figure shows
    the application **ANN 1** running, as well as the previous completed executions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ANN in practice](img/B01989_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'By selecting one of the cluster host worker instances, it is possible to see
    a list of executors that actually carry out cluster processing for that worker:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ANN in practice](img/B01989_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Finally, by selecting one of the executors, it is possible to see its history
    and configuration, as well as the links to the log file, and error information.
    At this level, with the log information provided, debugging is possible. These
    log files can be checked for processing error messages.
  prefs: []
  type: TYPE_NORMAL
- en: '![ANN in practice](img/B01989_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **ANN 1** application provides the following output to show that it has
    reclassified the same input data correctly. The reclassification has been successful,
    as each of the input patterns has been given the same label as it was trained
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'So, this shows that ANN training and test prediction will work with the same
    data. Now, I will train with the same data, but test with distorted or noisy data,
    an example of which I already demonstrated. This example can be found in the file
    called `test_ann2.scala`, in your software package. It is very similar to the
    first example, so I will just demonstrate the changed code. The application is
    now called `testann2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'An extra set of testing data is created, after the ANN model has been created
    using the training data. This testing data contains noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'This data is processed into input arrays, and is partitioned for cluster processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'It is then used to generate label predictions in the same way as the first
    example. If the model classifies the data correctly, then the same label values
    should be printed from 0.1 to 0.6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The code has already been compiled, so it can be run using the `spark-submit`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the cluster output from this script, which shows a successful classification
    using a trained ANN model, and some noisy test data. The noisy data has been classified
    correctly. For instance, if the trained model had become confused, it might have
    given a value of `0.15` for the noisy `close_square_test.img` test image in position
    one, instead of returning `0.1` as it did:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has attempted to provide you with an overview of some of the functionality
    available within the Apache Spark MLlib module. It has also shown the functionality
    that will soon be available in terms of ANN, or artificial neural networks, which
    is intended for release in Spark 1.3\. It has not been possible to cover all the
    areas of MLlib, due to the time and space allowed for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: You have been shown how to develop Scala-based examples for Naïve Bayes classification,
    K-Means clustering, and ANN or artificial neural networks. You have been shown
    how to prepare test data for these Spark MLlib routines. You have also been shown
    that they all accept the LabeledPoint structure, which contains features and labels.
    Also, each approach takes a training and prediction approach to training and testing
    a model using different data sets. Using the approach shown in this chapter, you
    can now investigate the remaining functionality in the MLlib library. You should
    refer to the [http://spark.apache.org/](http://spark.apache.org/) website, and
    ensure that when checking documentation that you refer to the correct version,
    that is, [http://spark.apache.org/docs/1.0.0/](http://spark.apache.org/docs/1.0.0/)
    for version 1.0.0.
  prefs: []
  type: TYPE_NORMAL
- en: Having examined the Apache Spark MLlib machine learning library, in this chapter,
    it is now time to consider Apache Spark's stream processing capability. The next
    chapter will examine stream processing using the Spark and Scala-based example
    code.
  prefs: []
  type: TYPE_NORMAL
