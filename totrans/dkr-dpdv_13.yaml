- en: '11: Docker Networking'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s always the network!
  prefs: []
  type: TYPE_NORMAL
- en: Any time there’s a an infrastructure problem, we always blame the network. Part
    of the reason is that networks are at the center of everything — **no network,
    no app!**
  prefs: []
  type: TYPE_NORMAL
- en: In the early days of Docker, networking was hard — really hard! These days it’s
    *almost* a pleasure ;-)
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll look at the fundamentals of Docker networking. Things
    like the Container Network Model (CNM) and `libnetwork`. We’ll also get our hands
    dirty building some networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we’ll split the chapter into three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The TLDR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deep dive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The commands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Networking - The TLDR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Docker runs applications inside of containers, and these need to communicate
    over lots of different networks. This means Docker needs strong networking capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Docker has solutions for container-to-container networks, as well
    as connecting to existing networks and VLANs. The latter is important for containerized
    apps that need to communicate with functions and services on external systems
    such as VM’s and physicals.
  prefs: []
  type: TYPE_NORMAL
- en: Docker networking is based on an open-source pluggable architecture called the
    Container Network Model (CNM). `libnetwork` is Docker’s real-world implementation
    of the CNM, and it provides all of Docker’s core networking capabilities. Drivers
    plug in to `libnetwork` to provide specific network topologies.
  prefs: []
  type: TYPE_NORMAL
- en: To create a smooth out-of-the-box experience, Docker ships with a set of native
    drivers that deal with the most common networking requirements. These include
    single-host bridge networks, multi-host overlays, and options for plugging into
    existing VLANs. Ecosystem partners extend things even further by providing their
    own drivers.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, `libnetwork` provides a native service discovery and basic
    container load balancing solution.
  prefs: []
  type: TYPE_NORMAL
- en: That’s this big picture. Let’s get into the detail.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Networking - The Deep Dive
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll organize this section of the chapter as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single-host bridge networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-host overlay networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting to existing networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service Discovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingress networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The theory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At the highest level, Docker networking comprises three major components:'
  prefs: []
  type: TYPE_NORMAL
- en: The Container Network Model (CNM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`libnetwork`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drivers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CNM is the design specification. It outlines the fundamental building blocks
    of a Docker network.
  prefs: []
  type: TYPE_NORMAL
- en: '`libenetwork` is a real-world implementation of the CNM, and is used by Docker.
    It’s written in Go, and implements the core components outlined in the CNM.'
  prefs: []
  type: TYPE_NORMAL
- en: Drivers extend the model by implementing specific network topologies such as
    VXLAN-based overlay networks.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 shows how they fit together at a very high level.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1](images/figure11-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look a bit closer at each.
  prefs: []
  type: TYPE_NORMAL
- en: The Container Network Model (CNM)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Everything starts with a design!
  prefs: []
  type: TYPE_NORMAL
- en: 'The design guide for Docker networking is the CNM. It outlines the fundamental
    building blocks of a Docker network, and you can read the full spec here: https://github.com/docker/libnetwork/blob/master/docs/design.md'
  prefs: []
  type: TYPE_NORMAL
- en: 'I recommend reading the entire spec, but at a high level, it defines three
    building blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: Sandboxes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Endpoints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A ***sandbox*** is an isolated network stack. It includes; Ethernet interfaces,
    ports, routing tables, and DNS config.
  prefs: []
  type: TYPE_NORMAL
- en: '***Endpoints*** are virtual network interfaces (E.g. `veth`). Like normal network
    interfaces, they’re responsible for making connections. In the case of the CNM,
    it’s the job of the *endpoint* to connect a *sandbox* to a *network*.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Networks*** are a software implementation of an 802.1d bridge (more commonly
    known as a switch). As such, they group together, and isolate, a collection of
    endpoints that need to communicate.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.2 shows the three components and how they connect.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 The Container Network Model (CNM)](images/figure11-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 The Container Network Model (CNM)
  prefs: []
  type: TYPE_NORMAL
- en: The atomic unit of scheduling in a Docker environment is the container, and
    as the name suggests, the Container Network Model is all about providing networking
    to containers. Figure 11.3 shows how CNM components relate to containers — sandboxes
    are placed inside of containers to provide them with network connectivity.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3](images/figure11-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3
  prefs: []
  type: TYPE_NORMAL
- en: Container A has a single interface (endpoint) and is connected to Network A.
    Container B has two interfaces (endpoints) and is connected to Network A **and**
    Network B. The containers will be able to communicate because they are both connected
    to Network A. However, the two *endpoints* in Container B cannot communicate with
    each other without the assistance of a layer 3 router.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to understand that *endpoints* behave like regular network
    adapters, meaning they can only be connected to a single network. Therefore, if
    a container needs connecting to multiple networks, it will need multiple endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.4 extends the diagram again, this time adding a Docker host. Although
    Container A and Container B are running on the same host, their network stacks
    are completely isolated at the OS-level via the sandboxes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4](images/figure11-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4
  prefs: []
  type: TYPE_NORMAL
- en: Libnetwork
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The CNM is the design doc, and `libnetwork` is the canonical implementation.
    It’s open-source, written in Go, cross-platform (Linux and Windows), and used
    by Docker.
  prefs: []
  type: TYPE_NORMAL
- en: In the early days of Docker, all the networking code existed inside the daemon.
    This was a nightmare — the daemon became bloated, and it didn’t follow the Unix
    principle of building modular tools that can work on their own, but also be easily
    composed into other projects. As a result, it all got ripped out and refactored
    into an external library called `libnetwork`. Nowadays, all of the core Docker
    networking code lives in `libnetwork`.
  prefs: []
  type: TYPE_NORMAL
- en: As you’d expect, it implements all three of the components defined in the CNM.
    It also implements native *service discovery*, *ingress-based container load balancing*,
    and the network control plane and management plane functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Drivers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If `libnetwork` implements the control plane and management plane functions,
    then drivers implement the data plane. For example, connectivity and isolation
    is all handled by drivers. So is the actual creation of network objects. The relationship
    is shown in Figure 11.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5](images/figure11-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5
  prefs: []
  type: TYPE_NORMAL
- en: Docker ships with several built-in drivers, known as native drivers or *local
    drivers*. On Linux they include; `bridge`, `overlay`, and `macvlan`. On Windows
    they include; `nat`, `overlay`, `transparent`, and `l2bridge`. We’ll see how to
    use some of them later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 3rd-parties can also write Docker network drivers. These are known as *remote
    drivers*, and examples include `calico`, `contiv`, `kuryr`, and `weave`.
  prefs: []
  type: TYPE_NORMAL
- en: Each driver is in charge of the actual creation and management of all resources
    on the networks it is responsible for. For example, an overlay network called
    “prod-fe-cuda” will be owned and managed by the `overlay` driver. This means the
    `overlay` driver will be invoked for the creation, management, and deletion of
    all resources on that network.
  prefs: []
  type: TYPE_NORMAL
- en: In order to meet the demands of complex highly-fluid environments,`libnetwork`
    allows multiple network drivers to be active at the same time. This means your
    Docker environment can sport a wide range of heterogeneous networks.
  prefs: []
  type: TYPE_NORMAL
- en: Single-host bridge networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The simplest type of Docker network is the single-host bridge network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The name tells us two things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single-host** tells us it only exists on a single Docker host and can only
    connect containers that are on the same host.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bridge** tells us that it’s an implementation of an 802.1d bridge (layer
    2 switch).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker on Linux creates single-host bridge networks with the built-in `bridge`
    driver, whereas Docker on Windows creates them using the built-in `nat` driver.
    For all intents and purposes, they work the same.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.6 shows two Docker hosts with identical local bridge networks called
    “mynet”. Even though the networks are identical, they are independent isolated
    networks. This means the containers in the picture cannot communicate directly
    because they are on different networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6](images/figure11-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6
  prefs: []
  type: TYPE_NORMAL
- en: Every Docker host gets a default single-host bridge network. On Linux it’s called
    “bridge”, and on Windows it’s called “nat” (yes, those are the same names as the
    drivers used to create them). By default, this is the network that all new containers
    will attach to unless you override it on the command line with the `--network`
    flag.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows the output of a `docker network ls` command on newly
    installed Linux and Windows Docker hosts. The output is trimmed so that it only
    shows the default network on each host. Notice how the name of the network is
    the same as the driver that was used to create it — this is coincidence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`The `docker network inspect` command is a treasure trove of great information!
    I highly recommended reading through its output if you’re interested in low-level
    detail.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`Docker networks built with the `bridge` driver on Linux hosts are based on
    the battle-hardened *linux bridge* technology that has existed in the Linux kernel
    for over 15 years. This means they’re high performance and extremely stable! It
    also means you can inspect them using standard Linux utilities. For example.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`The default “bridge” network, on all Linux-based Docker hosts, maps to an
    underlying *Linux bridge* in the kernel called “**docker0**”. We can see this
    from the output of `docker network inspect`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`The relationship between Docker’s default “bridge” network and the “docker0”
    bridge in the Linux kernel is shown in Figure 11.7.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7](images/figure11-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.8 extends the diagram by adding containers at the top that plug into
    the “bridge” network. The “bridge” network maps to the “docker0” Linux bridge
    in the host’s kernel, which can be mapped back to an Ethernet interface on the
    host via port mappings.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8](images/figure11-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the `docker network create` command to create a new single-host bridge
    network called “localnet”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`The new network is created, and will appear in the output of any future `docker
    network ls` commands. If you are using Linux, you will also have a new *Linux
    bridge* created in the kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the Linux `brctl` tool to look at the Linux bridges currently on the
    system. You may have to manually install the `brctl` binary using `apt-get install
    bridge-utils`, or the equivalent for your Linux distro.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`The output shows two bridges. The first line is the “docker0” bridge that
    we already know about. This relates to the default “bridge” network in Docker.
    The second bridge (br-20c2e8ae4bbb) relates to the new `localnet` Docker bridge
    network. Neither of them have spanning tree enabled, and neither have any devices
    connected (`interfaces` column).'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the bridge configuration on the host looks like Figure 11.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9](images/figure11-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create a new container and attach it to the new `localnet` bridge network.
    If you’re following along on Windows, you should substitute “`alpine sleep 1d`”
    with “`microsoft/powershell:nanoserver pwsh.exe -Command Start-Sleep 86400`”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`This container will now be on the `localnet` network. You can confirm this
    with a `docker network inspect`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`The output shows that the new “c1” container is on the `localnet` bridge/nat
    network.'
  prefs: []
  type: TYPE_NORMAL
- en: It we run the Linux `brctl show` command again, we’ll see c1’s interface attached
    to the `br-20c2e8ae4bbb` bridge.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`This is shown in Figure 11.10.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.10](images/figure11-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10
  prefs: []
  type: TYPE_NORMAL
- en: If we add another new container to the same network, it should be able to ping
    the “c1” container by name. This is because all new containers are registered
    with the embedded Docker DNS service so can resolve the names of all other containers
    on the same network.
  prefs: []
  type: TYPE_NORMAL
- en: '**Beware:** The default `bridge` network on Linux does not support name resolution
    via the Docker DNS service. All other *user-defined* bridge networks do!'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s test it.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new interactive container called “c2” and put it on the same `localnet`
    network as “c1”.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`Your terminal will switch into the “c2” container.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`*   From within the “c2” container, ping the “c1” container by name.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`It works! This is because the c2 container is running a local DNS resolver
    that forwards requests to an internal Docker DNS server. This DNS server maintains
    mappings for all containers started with the `--name` or `--net-alias` flag.``'
  prefs: []
  type: TYPE_NORMAL
- en: '``Try running some network-related commands while you’re still logged on to
    the container. It’s a great way of learning more about how Docker container networking
    works. The following snippet shows the `ipconfig` command ran from inside the
    “c2” Windows container previously created. You can match this IP address to the
    one shown in the `docker network inspect nat` output.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`So far, we’ve said that containers on bridge networks can only communicate
    with other containers on the same network. However, you can get around this using
    *port mappings*.'
  prefs: []
  type: TYPE_NORMAL
- en: Port mappings let you map a container port to a port on the Docker host. Any
    traffic hitting the Docker host on the configured port will be directed to the
    container. The high-level flow is shown in Figure 1.11
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11](images/figure11-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11
  prefs: []
  type: TYPE_NORMAL
- en: In the diagram, the application running in the container is operating on port
    80\. This is mapped to port 5000 on the host’s `10.0.0.15` interface. The end
    result is all traffic hitting the host on `10.0.0.15:5000` being redirected to
    the container on port 80.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through an example of mapping port 80 on a container running a web
    server, to port 5000 on the Docker host. The example will use NGINX on Linux.
    If you’re following along on Windows, you’ll need to substitute `nginx` with a
    Windows-based web server image.
  prefs: []
  type: TYPE_NORMAL
- en: Run a new web server container and map port 80 on the container to port 5000
    on the Docker host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`*   Verify the port mapping.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`This shows that port 80 in the container is mapped to port 5000 on all interfaces
    on the Docker host.` `*   Test the configuration by pointing a web browser to
    port 5000 on the Docker host. To complete this step, you will need to know the
    IP or DNS name of your Docker host. If you’re using Docker for Windows or Docker
    for Mac, you’ll be able to use `localhost` or `127.0.0.1`.![Figure 11.12](images/figure11-12.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.12
  prefs: []
  type: TYPE_NORMAL
- en: External systems, can now access the NGINX container running on the `localnet`
    bridge network via a port mapping to TCP port 5000 on the Docker host.``
  prefs: []
  type: TYPE_NORMAL
- en: '``Mapping ports like this works, but it’s clunky and doesn’t scale. For example,
    only a single container can bind to any port on the host. This means no other
    containers will be able to use port 5000 on the host we’re running the NGINX container
    on. This is one of the reason’s that single-host bridge networks are only useful
    for local development and very small applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-host overlay networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve got an entire chapter dedicated to multi-host overlay networks. So we’ll
    keep this section short.
  prefs: []
  type: TYPE_NORMAL
- en: Overlay networks are multi-host. They allow a single network to span multiple
    hosts so that containers on different hosts can communicate at layer 2\. They’re
    ideal for container-to-container communication, including container-only applications,
    and they scale well.
  prefs: []
  type: TYPE_NORMAL
- en: Docker provides a native driver for overlay networks. This makes creating them
    as simple as adding the `--d overlay` flag to the `docker network create` command.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to existing networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ability to connect containerized apps to external systems and physical networks
    is vital. A common example is a partially containerized app — the containerized
    parts will need a way to communicate with the non-containerized parts still running
    on existing physical networks and VLANs.
  prefs: []
  type: TYPE_NORMAL
- en: The built-in `MACVLAN` driver (`transparent` on Windows) was created with this
    in mind. It makes containers first-class citizens on the existing physical networks
    by giving each one its own MAC and IP addresses. We show this in Figure 11.13.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.13](images/figure11-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13
  prefs: []
  type: TYPE_NORMAL
- en: On the positive side, MACVLAN performance is good as it doesn’t require port
    mappings or additional bridges — you connect the container interface through to
    the hosts interface (or a sub-interface). However, on the negative side, it requires
    the host NIC to be in **promiscuous mode**, which isn’t allowed on most public
    cloud platforms. So MACVLAN is great for your corporate data center networks (assuming
    your network team can accommodate promiscuous mode), but it won’t work in the
    public cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dig a bit deeper with the help of some pictures and a hypothetical example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume we have an existing physical network with two VLANS:'
  prefs: []
  type: TYPE_NORMAL
- en: 'VLAN 100: 10.0.0.0/24'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VLAN 200: 192.168.3.0/24'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.14](images/figure11-14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14
  prefs: []
  type: TYPE_NORMAL
- en: Next, we add a Docker host and connect it to the network.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.15](images/figure11-15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15
  prefs: []
  type: TYPE_NORMAL
- en: 'We then have a requirement for a container (app service) to be plumbed into
    VLAN 100\. To do this, we create a new Docker network with the `macvlan` driver.
    However, the `macvlan` driver needs us to tell it a few things about the network
    we’re going to associate it with. Things like:'
  prefs: []
  type: TYPE_NORMAL
- en: Subnet info
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gateway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Range of IP’s it can assign to containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which interface or sub-interface on the host to use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following command will create a new MACVLAN network called “macvlan100”
    that will connect containers to VLAN 100.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`This will create the “macvlan100” network and the eth0.100 sub-interface.
    The config now looks like this.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.16](images/figure11-16.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16
  prefs: []
  type: TYPE_NORMAL
- en: MACVLAN uses standard Linux sub-interfaces, and you have to tag them with the
    ID of the VLAN they will connect to. In this example we’re connecting to VLAN
    100, so we tag the sub-interface with `.100` (`etho.100`).
  prefs: []
  type: TYPE_NORMAL
- en: We also used the `--ip-range` flag to tell the MACVLAN network which sub-set
    of IP addresses it can assign to containers. It’s vital that this range of addresses
    be reserved for Docker and not in use by other nodes or DHCP servers, as there
    is no management plane feature to check for overlapping IP ranges.
  prefs: []
  type: TYPE_NORMAL
- en: The `macvlan100` network is ready for containers, so let’s deploy one with the
    following command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`The config now looks like Figure 11.17\. But remember, the underlying network
    (VLAN 100) does not see any of the MACVLAN magic, it only sees the container with
    its MAC and IP addresses. And with that in mind, the “mactainer1” container will
    be able to ping and communicate with any other systems on VLAN 100\. Pretty sweet!'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** If you can’t get this to work, it might be because the host NIC is
    not in promiscuous mode. Remember that public cloud platforms do not allow promiscuous
    mode.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Figure 11.17](images/figure11-17.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we’ve got a MACVLAN network and used it to connect a new container
    to an existing VLAN. However, it doesn’t stop there. The Docker MACVLAN driver
    is built on top of the tried-and-tested Linux kernel driver with the same name.
    As such, it supports VLAN trunking. This means we can create multiple MACVLAN
    networks and connect containers on the same Docker host to them as shown in Figure
    11.18.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.18](images/figure11-18.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18
  prefs: []
  type: TYPE_NORMAL
- en: That pretty much covers MACVLAN. Windows offers a similar solution with the
    `transparent` driver.
  prefs: []
  type: TYPE_NORMAL
- en: Container and Service logs for troubleshooting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A quick note on troubleshooting connectivity issues before moving on to Service
    Discovery.
  prefs: []
  type: TYPE_NORMAL
- en: If you think you’re experiencing connectivity issues between containers, it’s
    worth checking the daemon logs and container logs (app logs).
  prefs: []
  type: TYPE_NORMAL
- en: 'On Windows systems, the daemon logs are stored under `~AppData\Local\Docker`,
    and you can view them in the Windows Event Viewer. On Linux, it depends what `init`
    system you’re using. If you’re running a `systemd`, the logs will go to `journald`
    and you can view them with the `journalctl -u docker.service` command. If you’re
    not running `systemd` you should look under the following locations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ubuntu systems running `upstart`: `/var/log/upstart/docker.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RHEL-based systems: `/var/log/messages`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Debian: `/var/log/daemon.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker for Mac: `~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/console-ring`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can also tell Docker how verbose you want daemon logging to be. To do this,
    you edit the daemon config file (`daemon.json`) so that “`debug`” is set to “`true`”
    and “`log-level`” is set to one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`debug` The most verbose option'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info` The default value and second-most verbose option'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`warn` Third most verbose option'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`error` Fourth most verbose option'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fatal` Least verbose option'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following snippet from a `daemon.json` enables debugging and sets the level
    to `debug`. It will work on all Docker platforms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`Be sure to restart Docker after making changes to the file.'
  prefs: []
  type: TYPE_NORMAL
- en: That was the daemon logs. What about container logs?
  prefs: []
  type: TYPE_NORMAL
- en: Logs from standalone containers can be viewed with the `docker container logs`
    command, and Swarm Service logs can be viewed with the `docker service logs` command.
    However, Docker supports lots of logging drivers, and they don’t all work with
    the `docker logs` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'As well as a driver and configuration for engine logs, every Docker host has
    a default logging driver and configuration for containers. Some of the drivers
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`json-file` (default)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`journald` (only works on Linux hosts running `systemd`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`syslog`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`splunk`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gelf`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`json-file` and `journald` are probably the easiest to configure, and they
    both work with the `docker logs` and `docker service logs` commands. The format
    of the commands is `docker logs <container-name>` and `docker service logs <service-name>`.'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using other logging drivers you can view logs using the 3-rd party
    platform’s native tools.
  prefs: []
  type: TYPE_NORMAL
- en: The following snippet from a `daemon.json` shows a Docker host configured to
    use `syslog`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`You can configure an individual container, or service, to start with a particular
    logging driver with the `--log-driver` and `--log-opts` flags. These will override
    anything set in `daemon.json`.'
  prefs: []
  type: TYPE_NORMAL
- en: Container logs work on the premise that your application is running as PID 1
    in its container, and sending logs to `STDOUT`, and errors to `STDERR`. The logging
    driver then forwards these “logs” to the locations configured via the logging
    driver.
  prefs: []
  type: TYPE_NORMAL
- en: If your application logs to a file, it’s possible to use a symlink to redirect
    log-file writes to STDOUT and STDERR.
  prefs: []
  type: TYPE_NORMAL
- en: The following is an example of running the `docker logs` command against a container
    called “vantage-db” configured to use the `json-file` logging driver.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`There’s a good chance you’ll find network connectivity errors reported in
    the daemon logs or container logs.'
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As well as core networking, `libnetwork` also provides some important network
    services.
  prefs: []
  type: TYPE_NORMAL
- en: '*Service discovery* allows all containers and Swarm services to locate each
    other by name. The only requirement is that they be on the same network.'
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, this leverages Docker’s embedded DNS server, as well as a DNS
    resolver in each container. Figure 11.19 shows container “c1” pinging container
    “c2” by name. The same principle applies to Swarm Services.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.19](images/figure11-19.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19
  prefs: []
  type: TYPE_NORMAL
- en: Let’s step through the process.
  prefs: []
  type: TYPE_NORMAL
- en: '**step 1:** The `ping c2` command invokes the local DNS resolver to resolve
    the name “c2” to an IP address. All Docker containers have a local DNS resolver.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 2:** If the local resolver does not have an IP address for “c2” in its
    local cache, it initiates a recursive query to the Docker DNS server. The local
    resolver is pre-configured to know the details of the embedded Docker DNS server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 3:** The Docker DNS server holds name-to-IP mappings for all containers
    created with the `--name` or `--net-alias` flags. This means it knows the IP address
    of container “c2”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 4:** The DNS server returns the IP address of “c2” to the local resolver
    in “c1”. It does this because the two containers are on the same network — if
    they were on different networks this would not work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 5:** The `ping` command is sent to the IP address of “c2”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every Swarm Service and standalone container started with the `--name` flag
    will register its name and IP with the Docker DNS service. This means all containers
    and service replicas can use the Docker DNS service to find each other.
  prefs: []
  type: TYPE_NORMAL
- en: However, service discovery is *network-scoped*. This means that name resolution
    only works for containers and Services on the same network. If two containers
    are on different networks, they will not be able to resolve each other.
  prefs: []
  type: TYPE_NORMAL
- en: One last point on service discovery and name resolution…
  prefs: []
  type: TYPE_NORMAL
- en: It’s possible to configure Swarm Services and standalone containers with customized
    DNS options. For example, the `--dns` flag lets you specify a list of custom DNS
    servers to use in case the embedded Docker DNS server cannot resolve a query.
    You can also use the `--dns-search` flag to add custom search domains for queries
    against unqualified names (i.e. when the query is not a fully qualified domain
    name).
  prefs: []
  type: TYPE_NORMAL
- en: On Linux, these all work by adding entries to the `/etc/resolv.conf` file inside
    the container.
  prefs: []
  type: TYPE_NORMAL
- en: The following example will start a new standalone container and add the infamous
    `8.8.8.8` Google DNS server, as well as `dockercerts.com` as search domain to
    append to unqualified queries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`#### Ingress load balancing'
  prefs: []
  type: TYPE_NORMAL
- en: 'Swarm supports two publishing modes that make Services accessible from outside
    of the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Ingress mode (default)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Host mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services published via *ingress mode* can be accessed from any node in the Swarm
    — even nodes **not** running a service replica. Services published via *host mode*
    can only be accessed via nodes running service replicas. Figure 11.20 shows the
    difference between the two modes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.20](images/figure11-20.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.20
  prefs: []
  type: TYPE_NORMAL
- en: Ingress mode is the default. This means that any time you publish a service
    with `-p` or `--publish` it will default to *ingress mode*. To publish a service
    in *host mode* you need to use the long format of the `--publish` flag **and**
    add `mode=host`. Let’s see an example using *host mode*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`A few notes about the command. `docker service create` lets you publish a
    service using either a *long form syntax* or *short form syntax*. The short form
    looks like this: `-p 5000:80` and we’ve seen it a few times already. However,
    you cannot publish a service in *host mode* using short form.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The long form looks like this: `--publish published=5000,target=80,mode=host`.
    It’s a comma-separate list with no whitespace after each comma. The options work
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`published=5000` makes the service available externally via port 5000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target=80` makes sure that external requests to the `published` port get mapped
    back to port 80 on the service replicas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode=host` makes sure that external requests will only reach the service if
    they come in via nodes running a service replica.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingress mode is what you’ll normally use.
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, *ingress mode* uses a layer 4 routing mesh called the **Service
    Mesh** or the **Swarm Mode Service Mesh**. Figure 11.21 shows the basic traffic
    flow of an external request to a service exposed in ingress mode.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.21](images/figure11-21.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.21
  prefs: []
  type: TYPE_NORMAL
- en: Let’s quickly walk through the diagram.
  prefs: []
  type: TYPE_NORMAL
- en: The command at the top is deploying a new Swarm service called “svc1”. It’s
    attaching the service to the `overnet` network and publishing it on port 5000.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Publishing a Swarm service like this (`--publish published=5000,target=80`)
    will publish it on port 5000 on the ingress network. As all nodes in a Swarm are
    attached to the ingress network, this means the port is published *swarm-wide*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Logic is implemented on the cluster ensuring that any traffic hitting the ingress
    network, via **any node**, on port 5000 will be routed to the “svc1” service on
    port 80.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, a single replica for the “svc1” service is deployed, and the
    cluster has a mapping rule that says “*all traffic hitting the ingress network
    on port 5000 needs routing to a node running a replica for the “svc1” service*”.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The red line shows traffic hitting `node1` on port 5000 and being routed to
    the service replica running on node2 via the ingress network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s vital to know that the incoming traffic could have hit any of the four
    Swarm nodes on port 5000 and we would get the same result. This is because the
    service is published *swarm-wide* via the ingress network.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also vital to know that if there were multiple replicas running, as shown
    in Figure 11.22, the traffic would be balanced across all replicas.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.22](images/figure11-22.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.22
  prefs: []
  type: TYPE_NORMAL
- en: Docker Networking - The Commands
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Docker networking has its own `docker network` sub-command. The main commands
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker network ls` Lists all networks on the local Docker host.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker network create` Creates new Docker networks. By default, it creates
    them with the `nat` driver on Windows, and the `bridge` driver on Linux. You can
    specify the driver (type of network) with the `-d` flag. `docker network create
    -d overlay overnet` will create a new overlay network called overnet with the
    native Docker `overlay` driver.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker network inspect` Provides detailed configuration information about
    a Docker network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker network prune` Deletes all unused networks on a Docker host.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker network rm` Deletes specific networks on a Docker host.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Container Network Model (CNM) is the master design document for Docker networking
    and defines the three major constructs that are used to build Docker networks
    — *sandboxes*, *endpoints*, and *networks*.
  prefs: []
  type: TYPE_NORMAL
- en: '`libnetwork` is the open-source library, written in Go, that implements the
    CNM. It’s used by Docker and is where all of the core Docker networking code lives.
    It also provides Docker’s network control plane and management plane.'
  prefs: []
  type: TYPE_NORMAL
- en: Drivers extend the Docker network stack (`libnetwork`) by adding code to implement
    specific network types, such as bridge networks and overlay networks. Docker ships
    with several built-in drivers, but you can also use 3rd-party drivers.
  prefs: []
  type: TYPE_NORMAL
- en: Single-host bridge networks are the most basic type of Docker network and are
    suitable for local development and very small applications. They do not scale,
    and they require port mappings if you want to publish your services outside of
    the network. Docker on Linux implements bridge networks using the built-in `bridge`
    driver, whereas Docker on Windows implements them using the built-in `nat` driver.
  prefs: []
  type: TYPE_NORMAL
- en: Overlay networks are all the rage and are excellent container-only multi-host
    networks. We’ll talk about them in-depth in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The `macvlan` driver (`transparent` on Windows) allows you to connect containers
    to existing physical networks and VLANs. They make containers first-class citizens
    by giving them their own MAC and IP addresses. Unfortunately, they require promiscuous
    on the host NIC, meaning they won’t work in the public cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Docker also uses `libnetwork` to implement basic service discovery, as well
    as a service mesh for container-based load balancing of ingress traffic.[PRE21]
  prefs: []
  type: TYPE_NORMAL
