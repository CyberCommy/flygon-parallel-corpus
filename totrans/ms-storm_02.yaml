- en: Storm Deployment, Topology Development, and Topology Options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to start with deployment of Storm on multiple
    node (three Storm and three ZooKeeper) clusters. This chapter is very important
    because it focuses on how we can set up the production Storm cluster and why we
    need the high availability of both the Storm Supervisor, Nimbus, and ZooKeeper
    (as Storm uses ZooKeeper for storing the metadata of the cluster, topology, and
    so on)?
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the key points that we are going to cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment of the Storm cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Program and deploy the word count example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different options of the Storm UI--kill, active, inactive, and rebalance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walkthrough of the Storm UI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic log level settings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating the Nimbus high availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storm prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should have the Java JDK and ZooKeeper ensemble installed before starting
    the deployment of the Storm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Java SDK 7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perform the following steps to install the Java SDK 7 on your machine. You
    can also go with JDK 1.8:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the Java SDK 7 RPM from Oracle's site ([http://www.oracle.com/technetwork/java/javase/downloads/index.html](http://www.oracle.com/technetwork/java/javase/downloads/index.html)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install the Java `jdk-7u<version>-linux-x64.rpm` file on your CentOS machine
    using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following environment variable in the `~/.bashrc` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the path of the `bin` directory of the JDK to the `PATH` system environment
    variable to the `~/.bashrc` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command to reload the `bashrc` file on the current login
    terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the Java installation as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Deployment of the ZooKeeper cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In any distributed application, various processes need to coordinate with each
    other and share configuration information. ZooKeeper is an application that provides
    all these services in a reliable manner. Being a distributed application, Storm
    also uses a ZooKeeper cluster to coordinate various processes. All of the states
    associated with the cluster and the various tasks submitted to Storm are stored
    in ZooKeeper. This section describes how you can set up a ZooKeeper cluster. We
    will be deploying a ZooKeeper ensemble of three nodes that will handle one node
    failure. Following is the deployment diagram of the three node ZooKeeper ensemble:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00007.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the ZooKeeper ensemble, one node in the cluster acts as the leader, while
    the rest are followers. If the leader node of the ZooKeeper cluster dies, then
    an election for the new leader takes places among the remaining live nodes, and
    a new leader is elected. All write requests coming from clients are forwarded
    to the leader node, while the follower nodes only handle the read requests. Also,
    we can't increase the write performance of the ZooKeeper ensemble by increasing
    the number of nodes because all write operations go through the leader node.
  prefs: []
  type: TYPE_NORMAL
- en: It is advised to run an odd number of ZooKeeper nodes, as the ZooKeeper cluster
    keeps working as long as the majority (the number of live nodes is greater than
    *n/2*, where *n* being the number of deployed nodes) of the nodes are running.
    So if we have a cluster of four ZooKeeper nodes (*3 > 4/2*; only one node can
    die), then we can handle only one node failure, while if we had five nodes (*3
    > 5/2*; two nodes can die) in the cluster, then we can handle two node failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Steps 1 to 4 need to be performed on each node to deploy the ZooKeeper ensemble:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the latest stable ZooKeeper release from the ZooKeeper site ([http://zookeeper.apache.org/releases.html](http://zookeeper.apache.org/releases.html)).
    At the time of writing, the latest version is ZooKeeper 3.4.6.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you have downloaded the latest version, unzip it. Now, we set up the `ZK_HOME`
    environment variable to make the setup easier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Point the `ZK_HOME` environment variable to the unzipped directory. Create
    the configuration file, `zoo.cfg`, at the `$ZK_HOME/conf` directory using the
    following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following properties to the `zoo.cfg` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `zoo1`, `zoo2`, and `zoo3` are the IP addresses of the ZooKeeper nodes.
    The following are the definitions for each of the properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tickTime`: This is the basic unit of time in milliseconds used by ZooKeeper.
    It is used to send heartbeats, and the minimum session timeout will be twice the
    `tickTime` value.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataDir`: This is the directory to store the in-memory database snapshots
    and transactional log.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clientPort`: This is the port used to listen to client connections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initLimit`: This is the number of `tickTime` values needed to allow followers
    to connect and sync to a leader node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`syncLimit`: This is the number of `tickTime` values that a follower can take
    to sync with the leader node. If the sync does not happen within this time, the
    follower will be dropped from the ensemble.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last three lines of the `server.id=host:port:port` format specify that there
    are three nodes in the ensemble. In an ensemble, each ZooKeeper node must have
    a unique ID number between 1 and 255\. This ID is defined by creating a file named
    `myid` in the `dataDir` directory of each node. For example, the node with the
    ID 1 (`server.1=zoo1:2888:3888`) will have a `myid` file at directory `/var/zookeeper`
    with `text 1` inside it.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this cluster, create the `myid` file at three locations, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command on each machine to start the ZooKeeper cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the status of the ZooKeeper nodes by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command on the `zoo1` node to check the first node''s status:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following information is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The first node is running in `follower` mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the status of the second node by performing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following information is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The second node is running in `leader` mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the status of the third node by performing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following information is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The third node is running in `follower` mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command on the leader machine to stop the leader node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, check the status of the remaining two nodes by performing the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the status of the first node using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following information is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The first node is again running in `follower` mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the status of the second node using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following information is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The third node is elected as the new leader.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, restart the third node with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This was a quick introduction to setting up ZooKeeper that can be used for development;
    however, it is not suitable for production. For a complete reference on ZooKeeper
    administration and maintenance, please refer to the online documentation at the
    ZooKeeper site at [http://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html](http://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Storm cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to set up a three nodes Storm cluster, of
    which one node will be the active master node (Nimbus) and the other two will
    be worker nodes (supervisors).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the deployment diagram of our three node Storm cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00008.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the steps that need to be performed to set up a three node
    Storm cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Install and run the ZooKeeper cluster. The steps for installing ZooKeeper are
    mentioned in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the latest stable Storm release from [https://storm.apache.org/downloads.html](https://storm.apache.org/downloads.html);
    at the time of writing, the latest version is Storm 1.0.2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you have downloaded the latest version, copy and unzip it in all three
    machines. Now, we will set the `$STORM_HOME` environment variable on each machine
    to make the setup easier. The `$STORM_HOME` environment contains the path of the
    Storm `home` folder (for example, export `STORM_HOME=/home/user/storm-1.0.2`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go to the `$STORM_HOME/conf` directory in the master nodes and add the following
    lines to the `storm.yaml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We are installing two master nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the `$STORM_HOME/conf` directory at each worker node and add the following
    lines to the `storm.yaml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If you are planning to execute the Nimbus and supervisor on the same machine,
    then add the `supervisor.slots.ports` property to the Nimbus machine too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the `$STORM_HOME` directory at the master nodes and execute the following
    command to start the master daemon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Go to the `$STORM_HOME` directory at each worker node (or supervisor node)
    and execute the following command to start the worker daemons:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Developing the hello world example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting the development, you should have Eclipse and Maven installed
    in your project. The sample topology explained here will cover how to create a
    basic Storm project, including a spout and bolt, and how to build, and execute
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Create a Maven project by using `com.stormadvance` as `groupId` and `storm-example`
    as `artifactId`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following Maven dependencies to the `pom.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Make sure the scope of the Storm dependency is provided, otherwise you will
    not be able to deploy the topology on the Storm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following Maven `build` plugins in the `pom.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Write your first sample spout by creating a `SampleSpout` class in the `com.stormadvance.storm_example`
    package. The `SampleSpout` class extends the serialized `BaseRichSpout` class.
    This spout does not connect to an external source to fetch data, but randomly
    generates the data and emits a continuous stream of records. The following is
    the source code of the `SampleSpout` class with an explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Write your first sample bolt by creating a `SampleBolt` class within the same
    package. The `SampleBolt` class extends the serialized `BaseRichBolt` class. This
    bolt will consume the tuples emitted by the `SampleSpout` spout and will print
    the value of the field `site` on the console. The following is the source code
    of the `SampleStormBolt` class with an explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a main `SampleStormTopology` class within the same package. This class
    creates an instance of the spout and bolt along with the classes, and chaines
    them together using a `TopologyBuilder` class. This class uses `org.apache.storm.LocalCluster`
    to simulate the Storm cluster. The `LocalCluster` mode is used for debugging/testing
    the topology on a developer machine before deploying it on the Storm cluster.
    The following is the implementation of the main class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Go to your project''s home directory and run the following commands to execute
    the topology in local mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now create a new topology class for deploying the topology on an actual Storm
    cluster. Create a main `SampleStormClusterTopology` class within the same package.
    This class also creates an instance of the spout and bolt along with the classes,
    and chains them together using a `TopologyBuilder` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Build your Maven project by running the following command on the projects home
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can deploy the topology to the cluster using the following Storm client
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command runs `TopologyMainClass` with the arguments `arg1` and
    `arg2`. The main function of `TopologyMainClass` is to define the topology and
    submit it to the Nimbus machine. The `storm jar` part takes care of connecting
    to the Nimbus machine and uploading the JAR part.
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in on a Storm Nimbus machine and execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code `~/storm_example-0.0.1-SNAPSHOT-jar-with-dependencies.jar`
    is the path of the `SampleStormClusterTopology` JAR that we are deploying on the
    Storm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following information is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `jps` command to see the number of running JVM processes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command''s output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, a `worker` is the JVM launched for the `SampleStormClusterTopology`
    topology.
  prefs: []
  type: TYPE_NORMAL
- en: The different options of the Storm topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section covers the following operations that a user can perform on the Storm
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Deactivate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rebalance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kill
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic log level settings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deactivate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Storm supports the deactivating a topology. In the deactivated state, spouts
    will not emit any new tuples into the pipeline, but the processing of the already
    emitted tuples will continue. The following is the command to deactivate the running
    topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Deactivate `SampleStormClusterTopology` using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following information is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Activate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Storm also the supports activating a topology. When a topology is activated,
    spouts will again start emitting tuples. The following is the command to activate
    the topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Activate `SampleStormClusterTopology` using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The following information is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Rebalance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of updating a the topology parallelism at the runtime is called
    a **rebalance**. A more detailed information of this operation acn be in [Chapter
    3](part0056.html#1LCVG0-6149dd15e07b443593cc93f2eb31ee7b), *Storm Parallelism
    and Data Partitioning*.
  prefs: []
  type: TYPE_NORMAL
- en: Kill
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Storm topologies are never-ending processes. To stop a topology, we need to
    kill it. When killed, the topology first enters into the deactivation state, processes
    all the tuples already emitted into it, and then stops. Run the following command
    to kill `SampleStormClusterTopology`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The following information is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run the `jps` command again to see the remaining JVM processes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command''s output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Dynamic log level settings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This allows the user to change the log level of topology on runtime without
    stopping the topology. The detailed information of this operation can be found
    at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Walkthrough of the Storm UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will show you how we can start the Storm UI daemon. However, before
    starting the Storm UI daemon, we assume that you have a running Storm cluster.
    The Storm cluster deployment steps are mentioned in the previous sections of this
    chapter. Now, go to the Storm home directory (`cd $STORM_HOME`) at the leader
    Nimbus machine and run the following command to start the Storm UI daemon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: By default, the Storm UI starts on the `8080` port of the machine where it is
    started. Now, we will browse to the `http://nimbus-node:8080` page to view the
    Storm UI, where Nimbus node is the IP address or hostname of the the Nimbus machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a screenshot of the Storm home page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00009.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Cluster Summary section
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This portion of the Storm UI shows the version of Storm deployed in the cluster,
    the uptime of the Nimbus nodes, number of free worker slots, number of used worker
    slots, and so on. While submitting a topology to the cluster, the user first needs
    to make sure that the value of the Free slots column should not be zero; otherwise,
    the topology doesn't get any worker for processing and will wait in the queue
    until a workers becomes free.
  prefs: []
  type: TYPE_NORMAL
- en: Nimbus Summary section
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This portion of the Storm UI shows the number of Nimbus processes that are running
    in a Storm cluster. The section also shows the status of the Nimbus nodes. A node
    with the status `Leader` is an active master while the node with the status `Not
    a Leader` is a passive master.
  prefs: []
  type: TYPE_NORMAL
- en: Supervisor Summary section
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This portion of the Storm UI shows the list of supervisor nodes running in the
    cluster, along with their Id, Host, Uptime, Slots, and Used slots columns.
  prefs: []
  type: TYPE_NORMAL
- en: Nimbus Configuration section
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This portion of the Storm UI shows the configuration of the Nimbus node. Some
    of the important properties are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`supervisor.slots.ports`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storm.zookeeper.port`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storm.zookeeper.servers`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storm.zookeeper.retry.interval`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`worker.childopts`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`supervisor.childopts`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a screenshot of Nimbus Configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00010.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Topology Summary section
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This portion of the Storm UI shows the list of topologies running in the Storm
    cluster, along with their ID, the number of workers assigned to the topology,
    the number of executors, number of tasks, uptime, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s deploy the sample topology (if it is not running already) in a remote
    Storm cluster by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We have created the `SampleStormClusterTopology` topology by defining three
    worker processes, two executors for `SampleSpout`, and four executors for `SampleBolt`.
  prefs: []
  type: TYPE_NORMAL
- en: After submitting `SampleStormClusterTopology` on the Storm cluster, the user
    has to refresh the Storm home page.
  prefs: []
  type: TYPE_NORMAL
- en: The following screenshot shows that the row is added for `SampleStormClusterTopology`
    in the Topology Summary section. The topology section contains the name of the
    topology, unique ID of the topology, status of the topology, uptime, number of
    workers assigned to the topology, and so on. The possible values of the Status
    fields are `ACTIVE`, `KILLED`, and `INACTIVE`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00011.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s click on `SampleStormClusterTopology` to view its detailed statistics.
    There are two screenshots for this. The first one contains the information about
    the number of workers, executors, and tasks assigned to the `SampleStormClusterTopology`
    topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00012.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next screenshot contains information about the spouts and bolts, including
    the number of executors and tasks assigned to each spout and bolt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00013.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The information shown in the previous screenshots is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Topology stats: This section will give information about the number of tuples
    emitted, transferred, and acknowledged, the capacity latency, and so on, within
    the windows of 10 minutes, 3 hours, 1 day, and since the start of the topology'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spouts (All time): This section shows the statistics of all the spouts running
    inside the topology'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bolts (All time): This section shows the statistics of all the bolts running
    inside the topology'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Topology actions: This section allows us to perform activate, deactivate, rebalance,
    kill, and other operations on the topologies directly through the Storm UI:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deactivate: Click on Deactivate to deactivate the topology. Once the topology
    is deactivated, the spout stops emitting tuples and the status of the topology
    changes to INACTIVE on the Storm UI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00014.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Deactivating the topology does not free the Storm resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activate: Click on the Activate button to activate the topology. Once the topology
    is activated, the spout again starts emitting tuples.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kill: Click on the Kill button to destroy/kill the topology. Once the topology
    is killed, it will free all the Storm resources allotted to this topology. While
    killing the topology, the Storm will first deactivate the spouts and wait for
    the kill time mentioned on the alerts box so that the bolts have a chance to finish
    the processing of the tuples emitted by the spouts before the kill command. The
    following screenshot shows how we can kill the topology through the Storm UI:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00015.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s go to the Storm UI''s home page to check the status of `SampleStormClusterToplogy`,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00016.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Dynamic log level settings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dynamic log level allows us to change the log level setting of the topology
    on the runtime from the Storm CLI and the Storm UI.
  prefs: []
  type: TYPE_NORMAL
- en: Updating the log level from the Storm UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Go through the following steps to update the log level from the Storm UI:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy `SampleStormClusterTopology` again on the Storm cluster if it is not
    running.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Browse the Storm UI at `http://nimbus-node:8080/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the `storm_example` topology.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now click on the Change Log Level button to change the `ROOT` logger of the
    topology, as shown in the following are the screenshots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00017.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Configure the entries mentioned in the following screenshots change the `ROOT`
    logger to ERROR:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you are planning to change the logging level to DEBUG, then you must specify
    the timeout (expiry time) for that log level, as shown in the following screenshots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the time mentioned in the expiry time is reached, the log level will go
    back to the default value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Clear button mentioned in the Action column will clear the log setting, and
    the application will set the default log setting again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Updating the log level from the Storm CLI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can modify the log level from the Storm CLI. The following is the command
    that the user has to execute from the Storm directory to update the log settings
    on the runtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `topology name` is the name of the topology, and `logger
    name` is the logger we want to change. If you want to change the `ROOT` logger,
    then use `ROOT` as a value of `logger name`. The `LEVEL` is the log level you
    want to apply. The possible values are `DEBUG`, `INFO`, `ERROR`, `TRACE`, `ALL`,
    `WARN`, `FATAL`, and `OFF`.
  prefs: []
  type: TYPE_NORMAL
- en: The `TIMEOUT` is the time in seconds. The log level will go back to normal after
    the timeout time. The value of `TIMEOUT` is mandatory if you are setting the log
    level to `DEBUG`/`ALL`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the command to change the log level setting for the `storm_example`
    topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the command to clear the log level setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered the installation of Storm and ZooKeeper clusters,
    the deployment of topologies on Storm clusters, the high availability of Nimbus
    nodes, and topology monitoring through the Storm UI. We have also covered the
    different operations a user can perform on running topology. Finally, we focused
    on how we can change the log level of running topology.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on the distribution of topologies on multiple
    Storm machines/nodes.
  prefs: []
  type: TYPE_NORMAL
