- en: Working with Storage and Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting
    Started with Kubernetes* we introduced the basic function of Kubernetes. Once
    you start to deploy some containers by Kubernetes, you need to consider the application's
    data lifecycle and CPU/memory resource management.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How a container behaves with volume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduce Kubernetes volume functionalities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practice and pitfalls of Kubernetes Persistent Volume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes resource management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes volume management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes and Docker use a local host disk by default. The Docker application
    may store and load any data onto the disk, for example, log data, temporary files,
    and application data. As long as the host has enough space and the application
    has necessary permission, data will exist as long as a container exists. In other
    words, when a container is closed the application exits, crashes, and reassigns
    a container to another host, and the data will be lost.
  prefs: []
  type: TYPE_NORMAL
- en: Container volume lifecycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to understand Kubernetes volume management, you need to understand
    the Docker volume lifecycle. The following example is how Docker behaves with
    a volume when a container restarts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: On Kubernetes, it also needs to care pod restart. In the case of a resource
    shortage, Kubernetes may stop a container and then restart a container on the
    same or another Kubernetes node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows how Kubernetes behaves when there is a resource
    shortage. One pod is killed and restarted when an out of memory error is received:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Sharing volume between containers within a pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting
    Started with Kubernetes* described that multiple containers within the same Kubernetes
    pod can share the same pod IP address, network port, and IPC, therefore, applications
    can communicate with each other through a localhost network; however, the filesystem
    is segregated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows that **Tomcat** and **nginx** are in the same pod.
    Those applications can communicate with each other via localhost. However, they
    can''t access each other''s `config` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Some applications won't affect these scenarios and behavior, but some applications
    may have some use cases that require them to use a shared directory or file. Therefore,
    developers and Kubernetes administrators need to be aware of the different types
    of stateless and stateful applications.
  prefs: []
  type: TYPE_NORMAL
- en: Stateless and stateful applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In terms of stateless applications, in this case use ephemeral volume. The application
    on the container doesn't need to preserve the data. Although stateless applications
    may write the data onto the filesystem while a container exists, but it is not
    important in terms of the application's lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the `tomcat` container runs some web applications. It also writes
    an application log under `/usr/local/tomcat/logs/`, but it won't be affected if
    it loses a `log` file.
  prefs: []
  type: TYPE_NORMAL
- en: However, what if you start to analyze an application log? Need to preserve due
    to auditing purpose? In this scenario, Tomcat can still be stateless, but share
    the `/usr/local/tomcat/logs` volume to another container such as Logstash ([https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)).
    Then Logstash will send a log to the chosen analytic store, such as Elasticsearch
    ([https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the `tomcat` container and `logstash` container *must be in the
    same Kubernetes pod* and share the `/usr/local/tomcat/logs` volume as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding figure shows how Tomcat and Logstash can share the `log` file
    using the Kubernetes `emptyDir` volume ([https://kubernetes.io/docs/concepts/storage/volumes/#emptydir)](https://kubernetes.io/docs/concepts/storage/volumes/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Tomcat and Logstash didn''t use network via localhost, but share the filesystem
    between `/usr/local/tomcat/logs` from the Tomcat container and `/mnt` from the
    Logstash container through Kubernetes `emptyDir` volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s create `tomcat` and `logstash` pod, and then see whether Logstash can
    see the Tomcat application log under `/mnt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this scenario, in the final destination Elasticsearch must be stateful. In
    terms of stateful means use Persistent Volume. The Elasticsearch container must
    preserve the data even if the container is restarted. In addition, you do not
    need to configure the Elasticsearch container within the same pod as Tomcat/Logstash.
    Because Elasticsearch should be a centralized log datastore, it can be separate
    from the Tomcat/Logstash pod and scaled independently.
  prefs: []
  type: TYPE_NORMAL
- en: Once you determine that your application needs a Persistent Volume, there are
    some different types of volume and different ways to manage Persistent Volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Persistent Volume and dynamic provisioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes supports a variety of Persistent Volume. For example, public cloud
    storage such as AWS EBS and Google Persistent Disk. It also supports network (distributed)
    filesystems such as NFS, GlusterFS, and Ceph. In addition, it can also support
    a block device such as iSCSI and Fibre Channel. Based on environment and infrastructure,
    a Kubernetes administrator can choose the best match types of Persistent Volume.
  prefs: []
  type: TYPE_NORMAL
- en: The following example is using GCP Persistent Disk as Persistent Volume. The
    first step is creating a GCP Persistent Disk and naming it `gce-pd-1`.
  prefs: []
  type: TYPE_NORMAL
- en: If you use AWS EBS or Google Persistent Disk, the Kubernetes node must be in
    the AWS or Google Cloud Platform.![](../images/00050.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: 'Then specify the name `gce-pd-1` in the `Deployment` definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It will mount the Persistent Disk from GCE Persistent Disk to `/usr/local/tomcat/logs`,
    which can persist Tomcat application logs.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent Volume claiming the abstraction layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Specifying a Persistent Volume into a configuration file directly, which makes
    a tight couple with a particular infrastructure. In previous example, this was
    Google Cloud Platform and also the disk name (`gce-pd-1`). From a container management
    point of view, pod definition shouldn't be locked-in to the specific environment
    because the infrastructure could be different based on the environment. The ideal
    pod definition should be flexible or abstract the actual infrastructure that specifies
    only volume name and mount point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, Kubernetes provides an abstraction layer that associates between
    the pod and the Persistent Volume, which is called the **Persistent Volume Claim**
    (**PVC**). It allows us to decouple from the infrastructure. The Kubernetes administrator
    just needs to pre-allocate a necessary size of the Persistent Volume in advance.
    Then Kubernetes will bind between the Persistent Volume and PVC:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following example is a definition of pod that uses PVC; let''s reuse the
    previous example (`gce-pd-1`) to register with Kubernetes first:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Then, create a PVC that associates with Persistent Volume (`pv-1`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that setting it as `storageClassName: ""` means, that it should explicitly
    use static provisioning. Some of the Kubernetes environments such as **Google
    Container Engine** (**GKE**), are already set up with Dynamic Provisioning. If
    we don''t specify `storageClassName: ""`, Kubernetes will ignore the existing
    `PersistentVolume` and allocates a new `PersistentVolume` when creating the `PersistentVolumeClaim`.![](../images/00054.jpeg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, `tomcat` setting has been decoupled from the specific volume to "`pvc-1`":'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Dynamic Provisioning and StorageClass
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PVC gives a degree of flexibility for Persistent Volume management. However,
    pre-allocating some Persistent Volumes pools might not be cost efficient, especially
    in a public cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes also helps this kind of situation by supporting Dynamic Provision
    for Persistent Volume. Kubernetes administrator defines the *provisioner* of the
    Persistent Volume, which is called `StorageClass`. Then, the Persistent Volume
    Claim asks `StorageClass` to dynamically allocate a Persistent Volume and then
    associates it with the PVC:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following example, AWS EBS is used as the `StorageClass`, and then,
    when creating the PVC, `StorageClass` dynamically create EBS registers it with
    Kubernetes Persistent Volume, and then attaches to PVC:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00057.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once `StorageClass` has been successfully created, create a PVC without PV,
    but specify the `StorageClass` name. In this example, this would be "`aws-sc`",
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, PVC asks `StorageClass` to create a Persistent Volume automatically on
    AWS as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that a Kubernetes provisioning tool such as kops ([https://github.com/kubernetes/kops](https://github.com/kubernetes/kops))
    and also Google Container Engine ([https://cloud.google.com/container-engine/](https://cloud.google.com/container-engine/))
    create a `StorageClass` by default. For example, kops sets up a default `StorageClass`
    as AWS EBS on an AWS environment. As well as Google Cloud Persistent disk on GKE.
    For more information, please refer to [Chapter 9](part0226.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on AWS* and [Chapter 10](part0247.html#7BHQU0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on GCP*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A problem case of ephemeral and persistent setting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may determine your application as stateless, because `datastore` function
    is handled by another pod or system. However, there are some pitfalls that sometimes
    applications actually store important files that you aren't aware of. For example,
    Grafana ([https://grafana.com/grafana](https://grafana.com/grafana)), it connects
    time series datasources such as Graphite ([https://graphiteapp.org](https://graphiteapp.org))
    and InfluxDB ([https://www.influxdata.com/time-series-database/](https://www.influxdata.com/time-series-database/)),
    so that people may determine whether Grafana is a stateless application.
  prefs: []
  type: TYPE_NORMAL
- en: However, Grafana itself also uses databases to store the user, organization,
    and dashboard metadata. By default, Grafana uses SQLite3 components and stores
    the database as `/var/lib/grafana/grafana.db`. Therefore, when a container is
    restarted, the Grafana setting will be all reset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates how Grafana behaves with ephemeral volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s create a Grafana `organizations` named `kubernetes org` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, look at the `Grafana` directory, there is a database file (`/var/lib/grafana/grafana.db`)
    timestamp that has been updated after creating a Grafana `organization`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00062.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'When the pod is deleted, ReplicaSet will start a new pod and check whether
    a Grafana `organization` exists or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It looks like the `sessions` directory has disappeared and `grafana.db` is
    also recreated by the Docker image again. Then if you access Web Console, the
    Grafana `organization` will also disappear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How about just using Persistent Volume for Grafana? But using ReplicaSet with
    Persistent Volume, it doesn't replicate (scale) properly. Because all of the pods
    attempt to mount the same Persistent Volume. In most cases, only the first pod
    can mount the Persistent Volume, then another pod will try to mount, and if it
    can't, it will give up. This happens if the Persistent Volume is capable of only
    RWO (read write once, only one pod can write).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, Grafana uses Persistent Volume to mount `/var/lib/grafana`;
    however, it can''t scale because Google Persistent Disk is RWO:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Even if the Persistent Volume has a capability of RWX (read write many, many
    pods can mount to read and write simultaneously), such as NFS, it won''t complain
    if multiple pods try to bind the same volume. However, we still need to consider
    whether multiple application instances can use the same folder/file or not. For
    example, if it replicates Grafana to two or more pods, it will be conflicted with
    multiple Grafana instances that try to write to the same `/var/lib/grafana/grafana.db`,
    and then data could be corrupted, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this scenario, Grafana must use backend databases such as MySQL or PostgreSQL
    instead of SQLite3 as follows. It allows multiple Grafana instances to read/write
    Grafana metadata properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Because RDBMS basically supports to connecting with multiple application instances
    via network, therefore, this scenario is perfectly suited being used by multiple
    pods. Note that Grafana supports using RDBMS as a backend metadata store; however,
    not all applications support RDBMS.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the Grafana configuration that uses MySQL/PostgreSQL, please visit the
    online documentation via:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.grafana.org/installation/configuration/#database](http://docs.grafana.org/installation/configuration/#database).'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the Kubernetes administrator carefully needs to monitor how an application
    behaves with volumes. And understand that in some use cases, just using Persistent
    Volume may not help because of issues that might arise when scaling pods.
  prefs: []
  type: TYPE_NORMAL
- en: If multiple pods need to access the centralized volume, then consider using
    the database as previously shown, if applicable. On the other hand, if multiple
    pods need an individual volume, consider using StatefulSet.
  prefs: []
  type: TYPE_NORMAL
- en: Replicating pods with a Persistent Volume using StatefulSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: StatefulSet was introduced in Kubernetes 1.5; it consists of a bond between
    the pod and the Persistent Volume. When scaling a pod that increases or decreases,
    pod and Persistent Volume are created or deleted together.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, pod creation process is serial. For example, when requesting Kubernetes
    to scale two additional StatefulSet, Kubernetes creates **Persistent Volume Claim
    1** and **Pod 1** first, and then creates **Persistent Volume Claim 2** and **Pod
    2**, but not simultaneously. It helps the administrator if an application registers
    to a registry during the application bootstrap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Even if one pod is dead, StatefulSet preserves the position of the pod (pod
    name, IP address, and related Kubernetes metadata) and also the Persistent Volume.
    Then, it attempts to recreate a container that reassigns to the same pod and mounts
    the same Persistent Volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'It helps to keep the number of pods/Persistent Volumes and the application
    remains online using the Kubernetes scheduler:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: StatefulSet with Persistent Volume requires Dynamic Provisioning and `StorageClass`
    because StatefulSet can be scalable. Kubernetes needs to know how to provision
    the Persistent Volume when adding more pods.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent Volume example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, there are some Persistent Volume examples that have been introduced.
    Based on the environment and scenario, the Kubernetes administrator needs to configure
    Kubernetes properly.
  prefs: []
  type: TYPE_NORMAL
- en: The following are some examples that build Elasticsearch clusters using different
    role nodes to configure different types of Persistent Volume. They will help you
    to decide how to configure and manage the Persistent Volume.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch cluster scenario
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elasticsearch is capable of setting up a cluster by using multiple nodes. As
    of Elasticsearch version 2.4, there are several different types, such as master,
    data, and coordinate nodes ([https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-node.html](https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-node.html)).
    Each node has a different role and responsibility in the cluster, therefore the
    corresponding Kubernetes configuration and Persistent Volume should align with
    the proper settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the components and roles of Elasticsearch nodes.
    The master node is the only node in the cluster that manages all Elasticsearch
    node registration and configuration. It can also have a backup node (master-eligible
    node) that can serve as the master node at any time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00070.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Data nodes hold and operate datastores in Elasticsearch. And the coordinating
    node handles HTTP requests from other applications, and then load balances/dispatches
    to the data nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch master node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The master node is the only node in the cluster. In addition, other nodes need
    to point to the master node because of registration. Therefore, the master node
    should use Kubernetes StatefulSet to assign a stable DNS name, such as `es-master-1`.
    Therefore, we have to use the Kubernetes service to assign DNS with a headless
    mode that assigns the DNS name to the pod IP address directly.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if the Persistent Volume is not required, because the master
    node does not need to persist an application's data.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch master-eligible node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The master-eligible node is a standby for the master node, and therefore there's
    no need to create another `Kubernetes` object. This means that scaling the master
    StatefulSet that assigns `es-master-2`, `es-master-3`, and `es-master-N` is enough.
    When the master node does not respond, there is a master node election within
    the master-eligible nodes to choose and elevate one node as the master node.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch data node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Elasticsearch data node is responsible for storing the data. In addition,
    we need to scale out if greater data capacity and/or more query requests are needed.
    Therefore, we can use StatefulSet with Persistent Volume to stabilize the pod
    and Persistent Volume. On the other hand, there's no need to have the DNS name,
    therefore no need to setup Kubernetes service for Elasticsearch data node.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch coordinating node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The coordinating node is a load balancer role in the Elasticsearch. Therefore,
    we need to scale out to handle HTTP traffic from external sources and persisting
    the data is not required. Therefore, we can use Kubernetes ReplicaSet with the
    Kubernetes service to expose the HTTP to the external service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows the commands used when we create all of the preceding
    Elasticsearch nodes by Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition, the following screenshot is the result we obtain after creating
    the preceding instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00072.jpeg)![](../images/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, external service (Kubernetes node:`30020`) is an entry point for
    external applications. For testing purposes, let's install `elasticsearch-head`
    ([https://github.com/mobz/elasticsearch-head](https://github.com/mobz/elasticsearch-head))
    to visualize the cluster information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect Elasticsearch coordination node to install the `elasticsearch-head`
    plugin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, access any Kubernetes node, URL as `http://<kubernetes-node>:30200/_plugin/head`.
    The following UI contains the cluster node information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00075.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The star icon indicates the Elasticsearch master node, the three black bullets
    are data nodes and the white circle bullet is the coordinator node.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this configuration, if one data node is down, no service impact will occur,
    as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../images/00076.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A few moments later, the new pod mounts the same PVC, which preserved `es-data-0`
    data. And then the Elasticsearch data node registers to master node again, after
    which the cluster health is back to green (normal), as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00077.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Due to StatefulSet and Persistent Volume, the application data is not lost on
    `es-data-0`. If you need more disk space, increase the number of data nodes. If
    you need to support more traffic, increase the number of coordinator nodes. If
    a backup of the master node is required, increase the number of master nodes to
    make some master-eligible nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the Persistent Volume combination of StatefulSet is very powerful,
    and can make the application flexible and scalable.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes resource management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting
    Started with Kubernetes* mentioned that Kubernetes has a scheduler that manages
    Kubernetes node and then determines where to deploy a pod. When node has enough
    resources such as CPU and memory, Kubernetes administrator can feel free to deploy
    an application. However, once it reaches its resource limit, the Kubernetes scheduler
    behaves different based on its configuration. Therefore, the Kubernetes administrator
    has to understand how to configure and utilize machine resources.'
  prefs: []
  type: TYPE_NORMAL
- en: Resource Quality of Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes has the concept of **Resource QoS** (**Quality of Service**), which
    helps an administrator to assign and manage pods by different priorities. Based
    on the pod''s setting, Kubernetes classifies each pod as:'
  prefs: []
  type: TYPE_NORMAL
- en: Guaranteed pod
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Burstable pod
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BestEffort pod
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The priority would be Guaranteed > Burstable > BestEffort, which means if the
    BestEffort pod and the Guaranteed pod exist in the same node, then when one of
    the pods consumes memory and to causes a node resource shortage, one of the BestEffort
    pods will be terminated to save the Guaranteed pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to configure Resource QoS, you have to set the resource request and/or
    resource limit in the pod definition. The following example is a definition of
    resource request and resource limit for nginx:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This example indicates the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type of resource definition** | **Resource name** | **Value** | **Mean**
    |'
  prefs: []
  type: TYPE_TB
- en: '| `requests` | `cpu` | `0.1` | At least 10% of 1 CPU core |'
  prefs: []
  type: TYPE_TB
- en: '|  | `memory` | `10Mi` | At least 10 Mbytes of memory |'
  prefs: []
  type: TYPE_TB
- en: '| `limits` | `cpu` | `0.5` | Maximum 50 % of 1 CPU core |'
  prefs: []
  type: TYPE_TB
- en: '|  | `memory` | `300Mi` | Maximum 300 Mbyte of memory |'
  prefs: []
  type: TYPE_TB
- en: 'For the CPU resource, acceptable value expressions for either cores (0.1, 0.2
    ... 1.0, 2.0) or millicpu (100m, 200m ... 1000m, 2000m). 1000 m is equivalent
    to 1 core. For example, if Kubernetes node has 2 cores CPU (or 1 core with hyperthreading),
    there are total of 2.0 cores or 2000 millicpu, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you run the nginx example (`requests.cpu: 0.1`), it occupies at least 0.1
    core, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00079.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As long as the CPU has enough spaces, it may occupy up to 0.5 cores (`limits.cpu:
    0.5`), as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00080.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can also see the configuration by using the `kubectl describe nodes` command
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00081.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that it shows a percentage that depends on the Kubernetes node's spec in
    the preceding example; as you can see the node has 1 core and 600 MB memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if it exceeds the memory limit, the Kubernetes scheduler
    determines that this pod is out of memory, and then it will kill a pod (`OOMKilled`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Configuring the BestEffort pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The BestEffort pod has the lowest priority in the Resource QoS configuration.
    Therefore, in case of a resource shortage, this pod will be the first one to be
    terminated. The use case of using BestEffort would be a stateless and recoverable
    application such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Worker process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proxy or cache node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the case of a resource shortage, this pod should yield CPU and memory resource
    to other higher priority pods. In order to configure a pod as the BestEffort pod,
    you need to set resource limit as 0, or not specify resource limit. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the resource setting is inherited by the `namespace default` setting.
    Therefore, if you intend to configure the pod as the BestEffort pod using the
    implicit setting, it might not configure as BestEffort if the namespace has a
    default resource setting as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00082.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, if you deploy to the default namespace using implicit setting,
    it applies a default CPU request as `request.cpu: 0.1` and then it becomes Burstable.
    On the other hand, if you deploy to `blank-namespace`, apply `request.cpu: 0`,
    and then it will become BestEffort.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring as the Guaranteed pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Guaranteed is the highest priority in Resource QoS. In the case of a resource
    shortage, the Kubernetes scheduler will try to retain the Guaranteed pod to the
    last.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the usage of a Guaranteed pod would be a mission critical node such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: Backend database with Persistent Volume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Master node (such as Elasticsearch master node and HDFS name node)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to configure as the Guaranteed pod, explicitly set the resource limit
    and resource request as the same value, or only set the resource limit. However,
    again, if the namespace has default resource setting, it might cause different
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Because Guaranteed pod has to set resource limit, if you are not 100% sure about
    the necessary CPU/memory resource of your application, especially maximum memory
    usage; you should use Burstable setting to monitor the application behavior for
    a while. Otherwise Kubernetes scheduler might terminate a pod (`OOMKilled`) even
    if the node has enough memory.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring as Burstable pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Burstable pod has a higher priority than BestEffort, but lower than Guaranteed.
    Unlike Guaranteed pod, resource limit setting is not mandatory; therefore pod
    can consume CPU and memory as much as possible while node resource is available.
    Therefore, it is good to be used by any type of application.
  prefs: []
  type: TYPE_NORMAL
- en: If you already know the minimal memory size of an application, you should specify
    request resource, which helps Kubernetes scheduler to assign to the right node.
    For example, there are two nodes that have 1 GB memory each. Node 1 already assigns
    600 MB memory and node 2 assigns 200 MB memory to other pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we create one more pod that has a resource request memory as 500 MB, then
    Kubernetes scheduler assigns this pod to node 2\. However, if the pod doesn''t
    have a resource request, the result will vary either node 1 or node 2\. Because
    Kubernetes doesn''t know how much memory this pod will consume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00083.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There is still important behavior of Resource QoS to discuss. The granularity
    of Resource QoS unit is pod level, not a container level. This means, if you configure
    a pod that has two containers, you intend to set container A as Guaranteed (request/limit
    are same value), and container B is Burstable (set only request). Unfortunately,
    Kubernetes configures this pod as Burstable because Kubernetes doesn't know what
    the limit of container B is.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrate that failed to configure as Guaranteed pod,
    it eventually configured as Burstable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though, change to configure resource limit only, but if container A has
    CPU limit only, then container B has memory limit only, then result will also
    be Burstable again because Kubernetes knows only either limit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, if you intend to configure pod as Guaranteed, you must set all containers
    as Guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring resource usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you start to configure to set a resource request and/or limit, your pod
    may not be scheduled to deploy by Kubernetes scheduler due to insufficient resources.
    In order to understand allocatable resources and available resources, use the
    `kubectl describe nodes` command to see the status.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows one node that has 600 MB memory and one core CPU.
    So allocatable resources are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00084.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, this node already runs some Burstable pod (use resource request) already
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00085.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The available memory is limited as approximately 20 MB. Therefore, if you submit
    Burstable pod that request more than 20 MB, it is never scheduled, as shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The error event can be captured by the `kubectl describe pod` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00087.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, you need to add more Kubernetes nodes to support more resources.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered Stateless and Stateful applications that use
    ephemeral volume or Persistent Volume. Both have pitfalls when an application
    restarts or a pod scales. In addition, Persistent Volume management on Kubernetes
    has been kept enhanced to make it easier, as you can see from such tools as StatefulSet
    and Dynamic Provisioning.
  prefs: []
  type: TYPE_NORMAL
- en: Also, Resource QoS helps Kubernetes scheduler to assign a pod to the right node
    based on request and limit based on priorities.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will introduce Kubernetes network and security, which configures
    pod and services more easier, and makes them scalable and secure.
  prefs: []
  type: TYPE_NORMAL
