- en: Chapter 13. Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency is the art of making a computer do (or appear to do) multiple things
    at once. Historically, this meant inviting the processor to switch between different
    tasks many times per second. In modern systems, it can also literally mean doing
    two or more things simultaneously on separate processor cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrency is not inherently an object-oriented topic, but Python''s concurrent
    systems are built on top of the object-oriented constructs we''ve covered throughout
    the book. This chapter will introduce you to the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Futures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AsyncIO
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency is complicated. The basic concepts are fairly simple, but the bugs
    that can occur are notoriously difficult to track down. However, for many projects,
    concurrency is the only way to get the performance we need. Imagine if a web server
    couldn't respond to a user's request until the previous one was completed! We
    won't be going into all the details of just how hard it is (another full book
    would be required) but we'll see how to do basic concurrency in Python, and some
    of the most common pitfalls to avoid.
  prefs: []
  type: TYPE_NORMAL
- en: Threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most often, concurrency is created so that work can continue happening while
    the program is waiting for I/O to happen. For example, a server can start processing
    a new network request while it waits for data from a previous request to arrive.
    An interactive program might render an animation or perform a calculation while
    waiting for the user to press a key. Bear in mind that while a person can type
    more than 500 characters per minute, a computer can perform billions of instructions
    per second. Thus, a ton of processing can happen between individual key presses,
    even when typing quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s theoretically possible to manage all this switching between activities
    within your program, but it would be virtually impossible to get right. Instead,
    we can rely on Python and the operating system to take care of the tricky switching
    part, while we create objects that appear to be running independently, but simultaneously.
    These objects are called **threads**; in Python they have a very simple API. Let''s
    take a look at a basic example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This example runs two threads. Can you see them? Every program has one thread,
    called the main thread. The code that executes from the beginning is happening
    in this thread. The second thread, more obviously, exists as the `InputReader`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: To construct a thread, we must extend the `Thread` class and implement the `run`
    method. Any code inside the `run` method (or that is called from within that method)
    is executed in a separate thread.
  prefs: []
  type: TYPE_NORMAL
- en: The new thread doesn't start running until we call the `start()` method on the
    object. In this case, the thread immediately pauses to wait for input from the
    keyboard. In the meantime, the original thread continues executing at the point
    `start` was called. It starts calculating squares inside a `while` loop. The condition
    in the `while` loop checks if the `InputReader` thread has exited its `run` method
    yet; once it does, it outputs some summary information to the screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the example and type the string "hello world", the output looks as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You will, of course, calculate more or less squares while typing the string
    as the numbers are related to both our relative typing speeds, and to the processor
    speeds of the computers we are running.
  prefs: []
  type: TYPE_NORMAL
- en: 'A thread only starts running in concurrent mode when we call the `start` method.
    If we want to take out the concurrent call to see how it compares, we can call
    `thread.run()` in the place that we originally called `thread.start()`. The output
    is telling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the thread never becomes alive and the `while` loop never executes.
    We wasted a lot of CPU power sitting idle while we were typing.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot of different patterns for using threads effectively. We won''t
    be covering all of them, but we will look at a common one so we can learn about
    the `join` method. Let''s check the current temperature in the capital city of
    every province in Canada:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This code constructs 10 threads before starting them. Notice how we can override
    the constructor to pass them into the `Thread` object, remembering to call `super`
    to ensure the `Thread` is properly initialized. Pay attention to this: the new
    thread isn''t running yet, so the `__init__` method is still executing from inside
    the main thread. Data we construct in one thread is accessible from other running
    threads.'
  prefs: []
  type: TYPE_NORMAL
- en: After the 10 threads have been started, we loop over them again, calling the
    `join()` method on each. This method essentially says "wait for the thread to
    complete before doing anything". We call this ten times in sequence; the for loop
    won't exit until all ten threads have completed.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can print the temperature that was stored on each thread object.
    Notice once again that we can access data that was constructed within the thread
    from the main thread. In threads, all state is shared by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing this code on my 100 mbit connection takes about two tenths of a second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If we run this code in a single thread (by changing the `start()` call to `run()`
    and commenting out the `join()` call), it takes closer to 2 seconds because each
    0.2 second request has to complete before the next one begins. This speedup of
    10 times shows just how useful concurrent programming can be.
  prefs: []
  type: TYPE_NORMAL
- en: The many problems with threads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Threads can be useful, especially in other programming languages, but modern
    Python programmers tend to avoid them for several reasons. As we'll see, there
    are other ways to do concurrent programming that are receiving more attention
    from the Python developers. Let's discuss some of these pitfalls before moving
    on to more salient topics.
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main problem with threads is also their primary advantage. Threads have
    access to all the memory and thus all the variables in the program. This can too
    easily cause inconsistencies in the program state. Have you ever encountered a
    room where a single light has two switches and two different people turn them
    on at the same time? Each person (thread) expects their action to turn the lamp
    (a variable) on, but the resulting value (the lamp is off) is inconsistent with
    those expectations. Now imagine if those two threads were transferring funds between
    bank accounts or managing the cruise control in a vehicle.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this problem in threaded programming is to "synchronize" access
    to any code that reads or writes a shared variable. There are a few different
    ways to do this, but we won't go into them here so we can focus on more Pythonic
    constructs. The synchronization solution works, but it is way too easy to forget
    to apply it. Worse, bugs due to inappropriate use of synchronization are really
    hard to track down because the order in which threads perform operations is inconsistent.
    We can't easily reproduce the error. Usually, it is safest to force communication
    between threads to happen using a lightweight data structure that already uses
    locks appropriately. Python offers the `queue.Queue` class to do this; it's functionality
    is basically the same as the `multiprocessing.Queue` that we will discuss in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, these disadvantages might be outweighed by the one advantage
    of allowing shared memory: it''s fast. If multiple threads need access to a huge
    data structure, shared memory can provide that access quickly. However, this advantage
    is usually nullified by the fact that, in Python, it is impossible for two threads
    running on different CPU cores to be performing calculations at exactly the same
    time. This brings us to our second problem with threads.'
  prefs: []
  type: TYPE_NORMAL
- en: The global interpreter lock
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to efficiently manage memory, garbage collection, and calls to machine
    code in libraries, Python has a utility called the **global interpreter lock**,
    or **GIL**. It''s impossible to turn off, and it means that threads are useless
    in Python for one thing that they excel at in other languages: parallel processing.
    The GIL''s primary effect, for our purposes is to prevent any two threads from
    doing work at the exact same time, even if they have work to do. In this case,
    "doing work" means using the CPU, so it''s perfectly ok for multiple threads to
    access the disk or network; the GIL is released as soon as the thread starts to
    wait for something.'
  prefs: []
  type: TYPE_NORMAL
- en: The GIL is quite highly disparaged, mostly by people who don't understand what
    it is or all the benefits it brings to Python. It would definitely be nice if
    our language didn't have this restriction, but the Python reference developers
    have determined that, for now at least, it brings more value than it costs. It
    makes the reference implementation easier to maintain and develop, and during
    the single-core processor days when Python was originally developed, it actually
    made the interpreter faster. The net result of the GIL, however, is that it limits
    the benefits that threads bring us, without alleviating the costs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the GIL is a problem in the reference implementation of Python that most
    people use, it has been solved in some of the nonstandard implementations such
    as IronPython and Jython. Unfortunately, at the time of publication, none of these
    support Python 3.
  prefs: []
  type: TYPE_NORMAL
- en: Thread overhead
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One final limitation of threads as compared to the asynchronous system we will
    be discussing later is the cost of maintaining the thread. Each thread takes up
    a certain amount of memory (both in the Python process and the operating system
    kernel) to record the state of that thread. Switching between the threads also
    uses a (small) amount of CPU time. This work happens seamlessly without any extra
    coding (we just have to call `start()` and the rest is taken care of), but the
    work still has tohappen somewhere.
  prefs: []
  type: TYPE_NORMAL
- en: This can be alleviated somewhat by structuring our workload so that threads
    can be reused to perform multiple jobs. Python provides a `ThreadPool` feature
    to handle this. It is shipped as part of the multiprocessing library and behaves
    identically to the `ProcessPool`, that we will discuss shortly, so let's defer
    discussion until the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The multiprocessing API was originally designed to mimic the thread API. However,
    it has evolved and in recent versions of Python 3, it supports more features more
    robustly. The multiprocessing library is designed when CPU-intensive jobs need
    to happen in parallel and multiple cores are available (given that a four core
    Raspberry Pi can currently be purchased for $35, there are usually multiple cores
    available). Multiprocessing is not useful when the processes spend a majority
    of their time waiting on I/O (for example, network, disk, database, or keyboard),
    but they are the way to go for parallel computation.
  prefs: []
  type: TYPE_NORMAL
- en: The multiprocessing module spins up new operating system processes to do the
    work. On Windows machines, this is a relatively expensive operation; on Linux,
    processes are implemented in the kernel the same way threads are, so the overhead
    is limited to the cost of running separate Python interpreters in each process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to parallelize a compute-heavy operation using similar constructs
    to those provided by the `threading` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This example just ties up the CPU for 200 million iterations. You may not consider
    this to be useful work, but it's a cold day and I appreciate the heat my laptop
    generates under such load.
  prefs: []
  type: TYPE_NORMAL
- en: The API should be familiar; we implement a subclass of `Process` (instead of
    `Thread`) and implement a `run` method. This method prints out the process ID
    (a unique number the operating system assigns to each process on the machine)
    before doing some intense (if misguided) work.
  prefs: []
  type: TYPE_NORMAL
- en: Pay special attention to the `if __name__ == '__main__':` guard around the module
    level code that prevents it to run if the module is being imported, rather than
    run as a program. This is good practice in general, but when using multiprocessing
    on some operating systems, it is essential. Behind the scenes, multiprocessing
    may have to import the module inside the new process in order to execute the `run()`
    method. If we allowed the entire module to execute at that point, it would start
    creating new processes recursively until the operating system ran out of resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'We construct one process for each processor core on our machine, then start
    and join each of those processes. On my 2014 era quad-core laptop, the output
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The first four lines are the process ID that was printed inside each `MuchCPU`
    instance. The last line shows that the 200 million iterations can run in about
    13 seconds on my machine. During that 13 seconds, my process monitor indicated
    that all four of my cores were running at 100 percent.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we subclass `threading.Thread` instead of `multiprocessing.Process` in `MuchCPU`,
    the output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This time, the four threads are running inside the same process and take close
    to three times as long to run. This is the cost of the global interpreter lock;
    in other languages or implementations of Python, the threaded version would run
    at least as fast as the multiprocessing version, We might expect it to be four
    times as long, but remember that many other programs are running on my laptop.
    In the multiprocessing version, these programs also need a share of the four CPUs.
    In the threading version, those programs can use the other three CPUs instead.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocessing pools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In general, there is no reason to have more processes than there are processors
    on the computer. There are a few reasons for this:'
  prefs: []
  type: TYPE_NORMAL
- en: Only `cpu_count()` processes can run simultaneously
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each process consumes resources with a full copy of the Python interpreter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communication between processes is expensive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating processes takes a nonzero amount of time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given these constraints, it makes sense to create at most `cpu_count()` processes
    when the program starts and then have them execute tasks as needed. It is not
    difficult to implement a basic series of communicating processes that does this,
    but it can be tricky to debug, test, and get right. Of course, Python being Python,
    we don't have to do all this work because the Python developers have already done
    it for us in the form of multiprocessing pools.
  prefs: []
  type: TYPE_NORMAL
- en: The primary advantage of pools is that they abstract away the overhead of figuring
    out what code is executing in the main process and which code is running in the
    subprocess. As with the threading API that multiprocessing mimics, it can often
    be hard to remember who is executing what. The pool abstraction restricts the
    number of places that code in different processes interact with each other, making
    it much easier to keep track of.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pools also seamlessly hide the process of passing data between processes. Using
    a pool looks much like a function call; you pass data into a function, it is executed
    in another process or processes, and when the work is done, a value is returned.
    It is important to understand that under the hood, a lot of work is being done
    to support this: objects in one process are being pickled and passed into a pipe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another process retrieves data from the pipe and unpickles it. Work is done
    in the subprocess and a result is produced. The result is pickled and passed into
    a pipe. Eventually, the original process unpickles it and returns it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All this pickling and passing data into pipes takes time and memory. Therefore,
    it is ideal to keep the amount and size of data passed into and returned from
    the pool to a minimum, and it is only advantageous to use the pool if a lot of
    processing has to be done on the data in question.
  prefs: []
  type: TYPE_NORMAL
- en: 'Armed with this knowledge, the code to make all this machinery work is surprisingly
    simple. Let''s look at the problem of calculating all the prime factors of a list
    of random numbers. This is a common and expensive part of a variety of cryptography
    algorithms (not to mention attacks on those algorithms!). It requires years of
    processing power to crack the extremely large numbers used to secure your bank
    accounts. The following implementation, while readable, is not at all efficient,
    but that''s ok because we want to see it using lots of CPU time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let's focus on the parallel processing aspects as the brute force recursive
    algorithm for calculating factors is pretty clear. We first construct a multiprocessing
    pool instance. By default, this pool creates a separate process for each of the
    CPU cores in the machine it is running on.
  prefs: []
  type: TYPE_NORMAL
- en: The `map` method accepts a function and an iterable. The pool pickles each of
    the values in the iterable and passes it into an available process, which executes
    the function on it. When that process is finished doing it's work, it pickles
    the resulting list of factors and passes it back to the pool. Once all the pools
    are finished processing work (which could take some time), the results list is
    passed back to the original process, which has been waiting patiently for all
    this work to complete.
  prefs: []
  type: TYPE_NORMAL
- en: It is often more useful to use the similar `map_async` method, which returns
    immediately even though the processes are still working. In that case, the results
    variable would not be a list of values, but a promise to return a list of values
    later by calling `results.get()`. This promise object also has methods like `ready()`,
    and `wait()`, which allow us to check whether all the results are in yet.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if we don't know all the values we want to get results for in
    advance, we can use the `apply_async` method to queue up a single job. If the
    pool has a process that isn't already working, it will start immediately; otherwise,
    it will hold onto the task until there is a free process available.
  prefs: []
  type: TYPE_NORMAL
- en: Pools can also be `close`d, which refuses to take any further tasks, but processes
    everything currently in the queue, or `terminate`d, which goes one step further
    and refuses to start any jobs still on the queue, although any jobs currently
    running are still permitted to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Queues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we need more control over communication between processes, we can use a `Queue`.
    `Queue` data structures are useful for sending messages from one process into
    one or more other processes. Any picklable object can be sent into a `Queue`,
    but remember that pickling can be a costly operation, so keep such objects small.
    To illustrate queues, let's build a little search engine for text content that
    stores all relevant entries in memory.
  prefs: []
  type: TYPE_NORMAL
- en: This is not the most sensible way to build a text-based search engine, but I
    have used this pattern to query numerical data that needed to use CPU-intensive
    processes to construct a chart that was then rendered to the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'This particular search engine scans all files in the current directory in parallel.
    A process is constructed for each core on the CPU. Each of these is instructed
    to load some of the files into memory. Let''s look at the function that does the
    loading and searching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Remember, this function is run in a different process (in fact, it is run in
    `cpucount()` different processes) from the main thread. It is passes a list of
    `path.path` objects and two `multiprocessing.Queue` objects; one for incoming
    queries and one to send outgoing results. These queues have a similar interface
    to the `Queue` class we discussed in [Chapter 6](ch06.html "Chapter 6. Python
    Data Structures"), *Python Data Structures*. However, they are doing extra work
    to pickle the data in the queue and pass it into the subprocess over a pipe. These
    two queues are set up in the main process and passed through the pipes into the
    search function inside the child processes.
  prefs: []
  type: TYPE_NORMAL
- en: The search code is pretty dumb, both in terms of efficiency and of capabilities;
    it loops over every line stored in memory and puts the matching ones in a list.
    The list is placed on a queue and passed back to the main process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the main process, which sets up these queues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For easier description, let's assume `cpu_count` is four. Notice how the import
    statements are placed inside the `if` guard? This is a small optimization that
    prevents them from being imported in each subprocess (where they aren't needed)
    on certain operating systems. We list all the paths in the current directory and
    then split the list into four approximately equal parts. We also construct a list
    of four `Queue` objects to send data into each subprocess. Finally, we construct
    a `single` results queue; this is passed into all four of the subprocesses. Each
    of them can put data into the queue and it will be aggregated in the main process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at the code that makes a search actually happen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This code performs a single search for `"def"` (because it's a common phrase
    in a directory full of Python files!). In a more production ready system, we would
    probably hook a socket up to this search code. In that case, we'd have to change
    the inter-process protocol so that the message coming back on the return queue
    contained enough information to identify which of many queries the results were
    attached to.
  prefs: []
  type: TYPE_NORMAL
- en: This use of queues is actually a local version of what could become a distributed
    system. Imagine if the searches were being sent out to multiple computers and
    then recombined. We won't discuss it here, but the multiprocessing module includes
    a manager class that can take a lot of the boilerplate out of the preceding code.
    There is even a version of the `multiprocessing.Manager` that can manage subprocesses
    on remote systems to construct a rudimentary distributed application. Check the
    Python multiprocessing documentation if you are interested in pursuing this further.
  prefs: []
  type: TYPE_NORMAL
- en: The problems with multiprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As threads do, multiprocessing also has problems, some of which we have already
    discussed. There is no best way to do concurrency; this is especially true in
    Python. We always need to examine the parallel problem to figure out which of
    the many available solutions is the best one for that problem. Sometimes, there
    is no best solution.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of multiprocessing, the primary drawback is that sharing data between
    processes is very costly. As we have discussed, all communication between processes,
    whether by queues, pipes, or a more implicit mechanism requires pickling the objects.
    Excessive pickling quickly dominates processing time. Multiprocessing works best
    when relatively small objects are passed between processes and a tremendous amount
    of work needs to be done on each one. On the other hand, if no communication between
    processes is required, there may not be any point in using the module at all;
    we can spin up four separate Python processes and use them independently.
  prefs: []
  type: TYPE_NORMAL
- en: The other major problem with multiprocessing is that, like threads, it can be
    hard to tell which process a variable or method is being accessed in. In multiprocessing,
    if you access a variable from another process it will usually overwrite the variable
    in the currently running process while the other process keeps the old value.
    This is really confusing to maintain, so don't do it.
  prefs: []
  type: TYPE_NORMAL
- en: Futures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start looking at a more asynchronous way of doing concurrency. Futures
    wrap either multiprocessing or threading depending on what kind of concurrency
    we need (tending towards I/O versus tending towards CPU). They don't completely
    solve the problem of accidentally altering shared state, but they allow us to
    structure our code such that it is easier to track down when we do so. Futures
    provide distinct boundaries between the different threads or processes. Similar
    to the multiprocessing pool, they are useful for "call and answer" type interactions
    in which processing can happen in another thread and then at some point in the
    future (they are aptly named, after all), you can ask it for the result. It's
    really just a wrapper around multiprocessing pools and thread pools, but it provides
    a cleaner API and encourages nicer code.
  prefs: []
  type: TYPE_NORMAL
- en: A future is an object that basically wraps a function call. That function call
    is run in the background in a thread or process. The future object has methods
    to check if the future has completed and to get the results after it has completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do another file search example. In the last section, we implemented
    a version of the `unix grep` command. This time, let''s do a simple version of
    the `find` command. The example will search the entire filesystem for paths that
    contain a given string of characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This code consists of a function named `find_files` that is run in a separate
    thread (or process, if we used `ProcessPoolExecutor`). There isn't anything particularly
    special about this function, but note how it does not access any global variables.
    All interaction with the external environment is passed into the function or returned
    from it. This is not a technical requirement, but it is the best way to keep your
    brain inside your skull when programming with futures.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Accessing outside variables without proper synchronization results in something
    called a **race** condition. For example, imagine two concurrent writes trying
    to increment an integer counter. They start at the same time and both read the
    value as 5\. Then they both increment the value and write back the result as 6\.
    But if two processes are trying to increment a variable, the expected result would
    be that it gets incremented by two, so the result should be 7\. Modern wisdom
    is that the easiest way to avoid doing this is to keep as much state as possible
    private and share them through known-safe constructs, such as queues.
  prefs: []
  type: TYPE_NORMAL
- en: We set up a couple variables before we get started; we'll be searching for all
    files that contain the characters `'.py'` for this example. We have a queue of
    futures that we'll discuss shortly. The `basedir` variable points to the root
    of the filesystem; `'/'` on Unix machines and probably `C:\` on Windows.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's have a short course on search theory. This algorithm implements
    breadth first search in parallel. Rather than recursively searching every directory
    using a depth first search, it adds all the subdirectories in the current folder
    to the queue, then all the subdirectories of each of those folders and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The meat of the program is known as an event loop. We can construct a `ThreadPoolExecutor`
    as a context manager so that it is automatically cleaned up and its threads closed
    when it is done. It requires a `max_workers` argument to indicate the number of
    threads running at a time; if more than this many jobs are submitted, it queues
    up the rest until a worker thread becomes available. When using `ProcessPoolExecutor`,
    this is normally constrained to the number of CPUs on the machine, but with threads,
    it can be much higher, depending how many are waiting on I/O at a time. Each thread
    takes up a certain amount of memory, so it shouldn't be too high; it doesn't take
    all that many threads before the speed of the disk, rather than number of parallel
    requests, is the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Once the executor has been constructed, we submit a job to it using the root
    directory. The `submit()` method immediately returns a `Future` object, which
    promises to give us a result eventually. The future is placed on the queue. The
    loop then repeatedly removes the first future from the queue and inspects it.
    If it is still running, it gets added back to the end of the queue. Otherwise,
    we check if the function raised an exception with a call to `future.exception()`.
    If it did, we just ignore it (it's usually a permission error, although a real
    app would need to be more careful about what the exception was). If we didn't
    check this exception here, it would be raised when we called `result()` and could
    be handled through the normal `try`...`except` mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming no exception occurred, we can call `result()` to get the return value
    of the function call. Since the function returns a list of subdirectories that
    are not symbolic links (my lazy way of preventing an infinite loop), `result()`
    returns the same thing. These new subdirectories are submitted to the executor
    and the resulting futures are tossed onto the queue to have their contents searched
    in a later iteration.
  prefs: []
  type: TYPE_NORMAL
- en: So that's all that is required to develop a future-based I/O-bound application.
    Under the hood, it's using the same thread or process APIs we've already discussed,
    but it provides a more understandable interface and makes it easier to see the
    boundaries between concurrently running functions (just don't try to access global
    variables from inside the future!).
  prefs: []
  type: TYPE_NORMAL
- en: AsyncIO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AsyncIO is the current state of the art in Python concurrent programming. It
    combines the concept of futures and an event loop with the coroutines we discussed
    in [Chapter 9](ch09.html "Chapter 9. The Iterator Pattern"), *The Iterator Pattern*.
    The result is about as elegant and easy to understand as it is possible to get
    when writing concurrent code, though that isn't saying a lot!
  prefs: []
  type: TYPE_NORMAL
- en: AsyncIO can be used for a few different concurrent tasks, but it was specifically
    designed for network I/O. Most networking applications, especially on the server
    side, spend a lot of time waiting for data to come in from the network. This can
    be solved by handling each client in a separate thread, but threads use up memory
    and other resources. AsyncIO uses coroutines instead of threads.
  prefs: []
  type: TYPE_NORMAL
- en: The library also provides its own event loop, obviating the need for the several
    lines long while loop in the previous example. However, event loops come with
    a cost. When we run code in an async task on the event loop, that code must return
    immediately, blocking neither on I/O nor on long-running calculations. This is
    a minor thing when writing our own code, but it means that any standard library
    or third-party functions that block on I/O have to have non-blocking versions
    created.
  prefs: []
  type: TYPE_NORMAL
- en: AsyncIO solves this by creating a set of coroutines that use the `yield from`
    syntax to return control to the event loop immediately. The event loop takes care
    of checking whether the blocking call has completed and performing any subsequent
    tasks, just like we did manually in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: AsyncIO in action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A canonical example of a blocking function is the `time.sleep` call. Let''s
    use the asynchronous version of this call to illustrate the basics of an AsyncIO
    event loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This is a fairly basic example, but it covers several features of AsyncIO programming.
    It is easiest to understand in the order that it executes, which is more or less
    bottom to top.
  prefs: []
  type: TYPE_NORMAL
- en: The second last line gets the event loop and instructs it to run a future until
    it is finished. The future in question is named `five_sleepers`. Once that future
    has done its work, the loop will exit and our code will terminate. As asynchronous
    programmers, we don't need to know too much about what happens inside that `run_until_complete`
    call, but be aware that a lot is going on. It's a souped up coroutine version
    of the futures loop we wrote in the previous chapter that knows how to deal with
    iteration, exceptions, function returns, parallel calls, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Now look a little more closely at that `five_sleepers` future. Ignore the decorator
    for a few paragraphs; we'll get back to it. The coroutine first constructs five
    instances of the `random_sleep` future. The resulting futures are wrapped in an
    `asyncio.async` task, which adds them to the loop's task queue so they can execute
    concurrently when control is returned to the event loop.
  prefs: []
  type: TYPE_NORMAL
- en: That control is returned whenever we call `yield from`. In this case, we call
    `yield from asyncio.sleep` to pause execution of this coroutine for two seconds.
    During this break, the event loop executes the tasks that it has queued up; namely
    the five `random_sleep` futures. These coroutines each print a starting message,
    then send control back to the event loop for a specific amount of time. If any
    of the sleep calls inside `random_sleep` are shorter than two seconds, the event
    loop passes control back into the relevant future, which prints its awakening
    message before returning. When the sleep call inside `five_sleepers` wakes up,
    it executes up to the next yield from call, which waits for the remaining `random_sleep`
    tasks to complete. When all the sleep calls have finished executing, the `random_sleep`
    tasks return, which removes them from the event queue. Once all five of those
    are completed, the `asyncio.wait` call and then the `five_sleepers` method also
    return. Finally, since the event queue is now empty, the `run_until_complete`
    call is able to terminate and the program ends.
  prefs: []
  type: TYPE_NORMAL
- en: The `asyncio.coroutine` decorator mostly just documents that this coroutine
    is meant to be used as a future in an event loop. In this case, the program would
    run just fine without the decorator. However, the `asyncio.coroutine` decorator
    can also be used to wrap a normal function (one that doesn't yield) so that it
    can be treated as a future. In this case, the entire function executes before
    returning control to the event loop; the decorator just forces the function to
    fulfill the coroutine API so the event loop knows how to handle it.
  prefs: []
  type: TYPE_NORMAL
- en: Reading an AsyncIO future
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An AsyncIO coroutine executes each line in order until it encounters a `yield
    from` statement, at which point it returns control to the event loop. The event
    loop then executes any other tasks that are ready to run, including the one that
    the original coroutine was waiting on. Whenever that child task completes, the
    event loop sends the result back into the coroutine so that it can pick up executing
    until it encounters another `yield from` statement or returns.
  prefs: []
  type: TYPE_NORMAL
- en: This allows us to write code that executes synchronously until we explicitly
    need to wait for something. This removes the nondeterministic behavior of threads,
    so we don't need to worry nearly so much about shared state.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's still a good idea to avoid accessing shared state from inside a coroutine.
    It makes your code much easier to reason about. More importantly, even though
    an ideal world might have all asynchronous execution happen inside coroutines,
    the reality is that some futures are executed behind the scenes inside threads
    or processes. Stick to a "share nothing" philosophy to avoid a ton of difficult
    bugs.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, AsyncIO allows us to collect logical sections of code together
    inside a single coroutine, even if we are waiting for other work elsewhere. As
    a specific instance, even though the `yield from asyncio.sleep` call in the `random_sleep`
    coroutine is allowing a ton of stuff to happen inside the event loop, the coroutine
    itself looks like it's doing everything in order. This ability to read related
    pieces of asynchronous code without worrying about the machinery that waits for
    tasks to complete is the primary benefit of the AsyncIO module.
  prefs: []
  type: TYPE_NORMAL
- en: AsyncIO for networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AsyncIO was specifically designed for use with network sockets, so let's implement
    a DNS server. More accurately, let's implement one extremely basic feature of
    a DNS server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The domain name system''s basic purpose is to translate domain names, such
    as www.amazon.com into IP addresses such as 72.21.206.6\. It has to be able to
    perform many types of queries and know how to contact other DNS servers if it
    doesn''t have the answer required. We won''t be implementing any of this, but
    the following example is able to respond directly to a standard DNS query to look
    up IPs for my three most recent employers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This example sets up a dictionary that dumbly maps a few domains to IPv4 addresses.
    It is followed by two functions that extract information from a binary DNS query
    packet and construct the response. We won't be discussing these; if you want to
    know more about DNS read RFC ("request for comment", the format for defining most
    Internet protocols) 1034 and 1035.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can test this service by running the following command in another terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Let's get on with the entrée. AsyncIO networking revolves around the intimately
    linked concepts of transports and protocols. A protocol is a class that has specific
    methods that are called when relevant events happen. Since DNS runs on top of
    **UDP** (**User Datagram Protocol**); we build our protocol class as a subclass
    of `DatagramProtocol`. This class has a variety of events that it can respond
    to; we are specifically interested in the initial connection occurring (solely
    so we can store the transport for future use) and the `datagram_received` event.
    For DNS, each received datagram must be parsed and responded to, at which point
    the interaction is over.
  prefs: []
  type: TYPE_NORMAL
- en: So, when a datagram is received, we process the packet, look up the IP, and
    construct a response using the functions we aren't talking about (they're black
    sheep in the family). Then we instruct the underlying transport to send the resulting
    packet back to the requesting client using its `sendto` method.
  prefs: []
  type: TYPE_NORMAL
- en: The transport essentially represents a communication stream. In this case, it
    abstracts away all the fuss of sending and receiving data on a UDP socket on an
    event loop. There are similar transports for interacting with TCP sockets and
    subprocesses, for example.
  prefs: []
  type: TYPE_NORMAL
- en: The UDP transport is constructed by calling the loop's `create_datagram_endpoint`
    coroutine. This constructs the appropriate UDP socket and starts listening on
    it. We pass it the address that the socket needs to listen on, and importantly,
    the protocol class we created so that the transport knows what to call when it
    receives data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the process of initializing a socket takes a non-trivial amount of time
    and would block the event loop, the `create_datagram_endpoint` function is a coroutine.
    In our example, we don''t really need to do anything while we wait for this initialization,
    so we wrap the call in `loop.run_until_complete`. The event loop takes care of
    managing the future, and when it''s complete, it returns a tuple of two values:
    the newly initialized transport and the protocol object that was constructed from
    the class we passed in.'
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, the transport has set up a task on the event loop that is
    listening for incoming UDP connections. All we have to do, then, is start the
    event loop running with the call to `loop.run_forever()` so that task can process
    these packets. When the packets arrive, they are processed on the protocol and
    everything just works.
  prefs: []
  type: TYPE_NORMAL
- en: The only other major thing to pay attention to is that transports (and, indeed,
    event loops) are supposed to be closed when we are finished with them. In this
    case, the code runs just fine without the two calls to `close()`, but if we were
    constructing transports on the fly (or just doing proper error handling!), we'd
    need to be quite a bit more conscious of it.
  prefs: []
  type: TYPE_NORMAL
- en: You may have been dismayed to see how much boilerplate is required in setting
    up a protocol class and underlying transport. AsyncIO provides an abstraction
    on top of these two key concepts called streams. We'll see an example of streams
    in the TCP server in the next example.
  prefs: []
  type: TYPE_NORMAL
- en: Using executors to wrap blocking code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AsyncIO provides its own version of the futures library to allow us to run
    code in a separate thread or process when there isn''t an appropriate non-blocking
    call to be made. This essentially allows us to combine threads and processes with
    the asynchronous model. One of the more useful applications of this feature is
    to get the best of both worlds when an application has bursts of I/O-bound and
    CPU-bound activity. The I/O-bound portions can happen in the event-loop while
    the CPU-intensive work can be spun off to a different process. To illustrate this,
    let''s implement "sorting as a service" using AsyncIO:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This is an example of good code implementing some really stupid ideas. The whole
    idea of sort as a service is pretty ridiculous. Using our own sorting algorithm
    instead of calling Python's `sorted` is even worse. The algorithm we used is called
    gnome sort, or in some cases, "stupid sort". It is a slow sort algorithm implemented
    in pure Python. We defined our own protocol instead of using one of the many perfectly
    suitable application protocols that exist in the wild. Even the idea of using
    multiprocessing for parallelism might be suspect here; we still end up passing
    all the data into and out of the subprocesses. Sometimes, it's important to take
    a step back from the program you are writing and ask yourself if you are trying
    to meet the right goals.
  prefs: []
  type: TYPE_NORMAL
- en: But let's look at some of the smart features of this design. First, we are passing
    bytes into and out of the subprocess. This is a lot smarter than decoding the
    JSON in the main process. It means the (relatively expensive) decoding can happen
    on a different CPU. Also, pickled JSON strings are generally smaller than pickled
    lists, so less data is passing between processes.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the two methods are very linear; it looks like code is being executed
    one line after another. Of course, in AsyncIO, this is an illusion, but we don't
    have to worry about shared memory or concurrency primitives.
  prefs: []
  type: TYPE_NORMAL
- en: Streams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous example should look familiar by now as it has a similar boilerplate
    to other AsyncIO programs. However, there are a few differences. You'll notice
    we called `start_server` instead of `create_server`. This method hooks into AsyncIO's
    streams instead of using the underlying transport/protocol code. Instead of passing
    in a protocol class, we can pass in a normal coroutine, which receives reader
    and writer parameters. These both represent streams of bytes that can be read
    from and written like files or sockets. Second, because this is a TCP server instead
    of UDP, there is some socket cleanup required when the program finishes. This
    cleanup is a blocking call, so we have to run the `wait_closed` coroutine on the
    event loop.
  prefs: []
  type: TYPE_NORMAL
- en: Streams are fairly simple to understand. Reading is a potentially blocking call
    so we have to call it with `yield from`. Writing doesn't block; it just puts the
    data on a queue, which AsyncIO sends out in the background.
  prefs: []
  type: TYPE_NORMAL
- en: Our code inside the `sort_request` method makes two read requests. First, it
    reads 8 bytes from the wire and converts them to an integer using big endian notation.
    This integer represents the number of bytes of data the client intends to send.
    So in the next call, to `readexactly`, it reads that many bytes. The difference
    between `read` and `readexactly` is that the former will read up to the requested
    number of bytes, while the latter will buffer reads until it receives all of them,
    or until the connection closes.
  prefs: []
  type: TYPE_NORMAL
- en: Executors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let's look at the executor code. We import the exact same `ProcessPoolExecutor`
    that we used in the previous section. Notice that we don't need a special AsyncIO
    version of it. The event loop has a handy `run_in_executor` coroutine that we
    can use to run futures on. By default, the loop runs code in `ThreadPoolExecutor`,
    but we can pass in a different executor if we wish. Or, as we did in this example,
    we can set a different default when we set up the event loop by calling `loop.set_default_executor()`.
  prefs: []
  type: TYPE_NORMAL
- en: As you probably recall from the previous section, there is not a lot of boilerplate
    for using futures with an executor. However, when we use them with AsyncIO, there
    is none at all! The coroutine automatically wraps the function call in a future
    and submits it to the executor. Our code blocks until the future completes, while
    the event loop continues processing other connections, tasks, or futures. When
    the future is done, the coroutine wakes up and continues on to write the data
    back to the client.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may be wondering if, instead of running multiple processes inside an event
    loop, it might be better to run multiple event loops in different processes. The
    answer is: "maybe". However, depending on the exact problem space, we are probably
    better off running independent copies of a program with a single event loop than
    to try to coordinate everything with a master multiprocessing process.'
  prefs: []
  type: TYPE_NORMAL
- en: We've hit most of the high points of AsyncIO in this section, and the chapter
    has covered many other concurrency primitives. Concurrency is a hard problem to
    solve, and no one solution fits all use cases. The most important part of designing
    a concurrent system is deciding which of the available tools is the correct one
    to use for the problem. We have seen advantages and disadvantages of several concurrent
    systems, and now have some insights into which are the better choices for different
    types of requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To wrap up this chapter, and the book, let's build a basic image compression
    tool. It will take black and white images (with 1 bit per pixel, either on or
    off) and attempt to compress it using a very basic form of compression known as
    run-length encoding. You may find black and white images a bit far-fetched. If
    so, you haven't enjoyed enough hours at [http://xkcd.com](http://xkcd.com)!
  prefs: []
  type: TYPE_NORMAL
- en: I've included some sample black and white BMP images (which are easy to read
    data into and leave a lot of opportunity to improve on file size) with the example
    code for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We'll be compressing the images using a simple technique called run-length encoding.
    This technique basically takes a sequence of bits and replaces any strings of
    repeated bits with the number of bits that are repeated. For example, the string
    000011000 might be replaced with 04 12 03 to indicate that 4 zeros are followed
    by 2 ones and then 3 more zeroes. To make things a little more interesting, we
    will break each row into 127 bit chunks.
  prefs: []
  type: TYPE_NORMAL
- en: I didn't pick 127 bits arbitrarily. 127 different values can be encoded into
    7 bits, which means that if a row contains all ones or all zeros, we can store
    it in a single byte; the first bit indicating whether it is a row of 0s or a row
    of 1s, and the remaining 7 bits indicating how many of that bit exists.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking up the image into blocks has another advantage; we can process individual
    blocks in parallel without them depending on each other. However, there's a major
    disadvantage as well; if a run has just a few ones or zeros in it, then it will
    take up `more` space in the compressed file. When we break up long runs into blocks,
    we may end up creating more of these small runs and bloat the size of the file.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with files, we have to think about the exact layout of the bytes
    in the compressed file. Our file will store two byte little-endian integers at
    the beginning of the file representing the width and height of the completed file.
    Then it will write bytes representing the 127 bit chunks of each row.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now before we start designing a concurrent system to build such compressed
    images, we should ask a fundamental question: Is this application I/O-bound or
    CPU-bound?'
  prefs: []
  type: TYPE_NORMAL
- en: My answer, honestly, is "I don't know". I'm not sure whether the app will spend
    more time loading data from disk and writing it back or doing the compression
    in memory. I suspect that it is a CPU bound app in principle, but once we start
    passing image strings into subprocesses, we may lose any benefit of parallelism.
    The optimal solution to this problem is probably to write a C or Cython extension,
    but let's see how far we can get in pure Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll build this application using bottom-up design. That way we''ll have
    some building blocks that we can combine into different concurrency patterns to
    see how they compare. Let''s start with the code that compresses a 127-bit chunk
    using run-length encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This code uses the `bitarray` class for manipulating individual zeros and ones.
    It is distributed as a third-party module, which you can install with the command
    `pip install bitarray`. The chunk that is passed into `compress_chunks` is an
    instance of this class (although the example would work just as well with a list
    of Booleans). The primary benefit of the bitarray in this case is that when pickling
    them between processes, they take up an 8th of the space of a list of Booleans
    or a bytestring of 1s and 0s. Therefore, they pickle faster. They are also a bit
    (pun intended) easier to work with than doing a ton of bitwise operations.
  prefs: []
  type: TYPE_NORMAL
- en: The method compresses the data using run-length encoding and returns a bytearray
    containing the packed data. Where a bitarray is like a list of ones and zeros,
    a bytearray is like a list of byte objects (each byte, of course, containing 8
    ones or zeros).
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm that performs the compression is pretty simple (although I'd like
    to point out that it took me two days to implement and debug it. Simple to understand
    does not necessarily imply easy to write!). It first sets the `last` variable
    to the type of bit in the current run (either `True` or `False`). It then loops
    over the bits, counting each one, until it finds one that is different. When it
    does, it constructs a new byte by making the leftmost bit of the byte (the 128
    position) either a zero or a one, depending on what the `last` variable contained.
    Then it resets the counter and repeats the operation. Once the loop is done, it
    creates one last byte for the last run, and returns the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'While we''re creating building blocks, let''s make a function that compresses
    a row of image data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This function accepts a bitarray named row. It splits it into chunks that are
    each 127 bits wide using a function that we'll define very shortly. Then it compresses
    each of those chunks using the previously defined `compress_chunk`, concatenating
    the results into a `bytearray`, which it returns.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define `split_bits` as a simple generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, since we aren''t certain yet whether this will run more effectively in
    threads or processes, let''s wrap these functions in a method that runs everything
    in a provided executor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This example barely needs explaining; it splits the incoming bits into rows
    based on the width of the image using the same `split_bits` function we have already
    defined (hooray for bottom-up design!).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this code will compress any sequence of bits, although it would bloat,
    rather than compress binary data that has frequent changes in bit values. Black
    and white images are definitely good candidates for the compression algorithm
    in question. Let''s now create a function that loads an image file using the third-party
    pillow module, converts it to bits, and compresses it. We can easily switch between
    executors using the venerable comment statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `image.convert()` call changes the image to black and white (one bit) mode,
    while `getdata()` returns an iterator over those values. We pack the results into
    a bitarray so they transfer across the wire more quickly. When we output the compressed
    file, we first write the width and height of the image followed by the compressed
    data, which arrives as a bytearray, which can be written directly to the binary
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Having written all this code, we are finally able to test whether thread pools
    or process pools give us better performance. I created a large (7200 x 5600 pixels)
    black and white image and ran it through both pools. The `ProcessPool` takes about
    7.5 seconds to process the image on my system, while the `ThreadPool` consistently
    takes about 9\. Thus, as we suspected, the cost of pickling bits and bytes back
    and forth between processes is eating almost all the efficiency gains from running
    on multiple processors (though looking at my CPU monitor, it does fully utilize
    all four cores on my machine).
  prefs: []
  type: TYPE_NORMAL
- en: So it looks like compressing a single image is most effectively done in a separate
    process, but only barely because we are passing so much data back and forth between
    the parent and subprocesses. Multiprocessing is more effective when the amount
    of data passed between processes is quite low.
  prefs: []
  type: TYPE_NORMAL
- en: So let's extend the app to compress all the bitmaps in a directory in parallel.
    The only thing we'll have to pass into the subprocesses are filenames, so we should
    get a speed gain compared to using threads. Also, to be kind of crazy, we'll use
    the existing code to compress individual images. This means we'll be running a
    `ProcessPoolExecutor` inside each subprocess to create even more subprocesses.
    I don't recommend doing this in real life!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This code uses the `compress_image` function we defined previously, but runs
    it in a separate process for each image. It doesn't pass an executor into the
    function, so `compress_image` creates a `ProcessPoolExecutor` once the new process
    has started running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we are running executors inside executors, there are four combinations
    of threads and process pools that we can be using to compress images. They each
    have quite different timing profiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Process pool per image | Thread pool per image |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Process pool per row** | 42 seconds | 53 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| **Thread pool per row** | 34 seconds | 64 seconds |'
  prefs: []
  type: TYPE_TB
- en: As we might expect, using threads for each image and again using threads for
    each row is the slowest, since the GIL prevents us from doing any work in parallel.
    Given that we were slightly faster when using separate processes for each row
    when we were using a single image, you may be surprised to see that it is faster
    to use a `ThreadPool` feature for rows if we are processing each image in a separate
    process. Take some time to understand why this might be.
  prefs: []
  type: TYPE_NORMAL
- en: My machine contains only four processor cores. Each row in each image is being
    processed in a separate pool, which means that all those rows are competing for
    processing power. When there is only one image, we get a (very modest) speedup
    by running each row in parallel. However, when we increase the number of images
    being processed at once, the cost of passing all that row data into and out of
    a subprocess is actively stealing processing time from each of the other images.
    So, if we can process each image on a separate processor, where the only thing
    that has to get pickled into the subprocess pipe is a couple filenames, we get
    a solid speedup.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we see that different workloads require different concurrency paradigms.
    Even if we are just using futures we have to make informed decisions about what
    kind of executor to use.
  prefs: []
  type: TYPE_NORMAL
- en: Also note that for typically-sized images, the program runs quickly enough that
    it really doesn't matter which concurrency structures we use. In fact, even if
    we didn't use any concurrency at all, we'd probably end up with about the same
    user experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'This problem could also have been solved using the threading and/or multiprocessing
    modules directly, though there would have been quite a bit more boilerplate code
    to write. You may be wondering whether or not AsyncIO would be useful here. The
    answer is: "probably not". Most operating systems don''t have a good way to do
    non-blocking reads from the filesystem, so the library ends up wrapping all the
    calls in futures anyway.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For completeness, here''s the code that I used to decompress the RLE images
    to confirm that the algorithm was working correctly (indeed, it wasn''t until
    I fixed bugs in both compression and decompression, and I''m still not sure if
    it is perfect. I should have used test-driven development!):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This code is fairly straightforward. Each run is encoded in a single byte. It
    uses some bitwise math to extract the color of the pixel and the length of the
    run. Then it sets each pixel from that run in the image, incrementing the row
    and column of the next pixel to check at appropriate intervals.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've covered several different concurrency paradigms in this chapter and still
    don't have a clear idea of when each one is useful. As we saw in the case study,
    it is often a good idea to prototype a few different strategies before committing
    to one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrency in Python 3 is a huge topic and an entire book of this size could
    not cover everything there is to know about it. As your first exercise, I encourage
    you to check out several third-party libraries that may provide additional context:'
  prefs: []
  type: TYPE_NORMAL
- en: execnet, a library that permits local and remote share-nothing concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel python, an alternative interpreter that can execute threads in parallel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cython, a python-compatible language that compiles to C and has primitives to
    release the gil and take advantage of fully parallel multi-threading.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyPy-STM, an experimental implementation of software transactional memory on
    top of the ultra-fast PyPy implementation of the Python interpreter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gevent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have used threads in a recent application, take a look at the code and
    see if you can make it more readable and less bug-prone by using futures. Compare
    thread and multiprocessing futures to see if you can gain anything by using multiple
    CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Try implementing an AsyncIO service for some basic HTTP requests. You may need
    to look up the structure of an HTTP request on the web; they are fairly simple
    ASCII packets to decipher. If you can get it to the point that a web browser can
    render a simple GET request, you'll have a good understanding of AsyncIO network
    transports and protocols.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you understand the race conditions that happen in threads when you
    access shared data. Try to come up with a program that uses multiple threads to
    set shared values in such a way that the data deliberately becomes corrupt or
    invalid.
  prefs: []
  type: TYPE_NORMAL
- en: Remember the link collector we covered for the case study in [Chapter 6](ch06.html
    "Chapter 6. Python Data Structures"), *Python Data Structures*? Can you make it
    run faster by making requests in parallel? Is it better to use raw threads, futures,
    or AsyncIO for this?
  prefs: []
  type: TYPE_NORMAL
- en: Try writing the run-length encoding example using threads or multiprocessing
    directly. Do you get any speed gains? Is the code easier or harder to reason about?
    Is there any way to speed up the decompression script by using concurrency or
    parallelism?
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter ends our exploration of object-oriented programming with a topic
    that isn't very object-oriented. Concurrency is a difficult problem and we've
    only scratched the surface. While the underlying OS abstractions of processes
    and threads do not provide an API that is remotely object-oriented, Python offers
    some really good object-oriented abstractions around them. The threading and multiprocessing
    packages both provide an object-oriented interface to the underlying mechanics.
    Futures are able to encapsulate a lot of the messy details into a single object.
    AsyncIO uses coroutine objects to make our code read as though it runs synchronously,
    while hiding ugly and complicated implementation details behind a very simple
    loop abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading *Python 3 Object-oriented Programming*, *Second Edition*.
    I hope you've enjoyed the ride and are eager to start implementing object-oriented
    software in all your future projects!
  prefs: []
  type: TYPE_NORMAL
