- en: Virtual Reality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Virtual Reality** (**VR**) is a very popular topic in game development these
    days. In this chapter, we will take a look at how the power of C++ can be leveraged
    to create an immersive VR experience. It should be noted that while the SDK used
    for the example integration is available for macOS, the hardware and example code
    presented in this chapter has not been tested on macOS and is not guaranteed to
    be supported. It should also be noted that you will need a VR headset and a significantly
    powerful PC and graphics card to run this chapter''s closing example. It is recommend
    that you have a CPU that matches or exceeds an Intel i5-4590 or AMD FX 8350, and
    a GPU that matches or exceeds an NVIDIA GeForce GTX 960 or AMD Radeon R9 290\.
    In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Current VR hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VR rendering concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Headset SDKs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing VR support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quick VR overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: VR is a computer technology that uses various forms of hardware to generate
    a simulation of the user's physical presence in a reconstructed or imaginary environment
    through the use of realistic imagery, sounds, and other sensations. A user that
    is in a VR environment is able to look around the artificial world, and with new
    advances in VR technology, move around in it and interact with virtual items or
    objects. While VR technology can date back to the 1950s, with recent advancements
    in computer graphics, processing, and power, VR has seen a resurgence. Well-known
    technology giants such as Facebook, Sony, Google, and Microsoft have bet big on
    virtual and augmented reality technologies. Not since the invention of the mouse
    has the way users interact with computers had such potential for innovation. Use
    cases for VR stretch beyond just game development. Many other fields are looking
    to VR technologies as a way to expand their own unique interactions. Healthcare,
    education, training, engineering, social sciences, marketing, and of course cinema
    and entertainment all hold promising opportunities for developers that possess
    the skill sets learned throughout this book and in game development in general.
    I often recommend that game developers looking for a change of pace, or a new
    challenge, look to the emerging VR development scene as an alternative use for
    their knowledge and skills base.
  prefs: []
  type: TYPE_NORMAL
- en: Current VR hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As developers, we are in a very fortunate time period of VR hardware development.
    There are many different options when it comes to VR hardware, including projected
    systems such as the **Cave Automatic Virtual Environment** (**CAVE**), **head-mounted
    displays** (**HMDs**), and even mobile phone based systems such as the Google
    Daydream and Cardboard. Here we will focus on immersive PC and console driven
    HMDs. Most of the technologies behind these HMDs are very similar. Each of the
    HMDs listed here have at least **s****ix degrees of freedom** (**6DOF**) in terms
    of movement, head tracking in 3D space, and some even have basic spatial awareness,
    often referred to as *room sense*. Development for each of these headsets can,
    at a high level, be approached in much the same way, but it is good to have a
    general understanding of each of these different devices. Next, we will take a
    quick look at some of the most common headsets currently available to the consumer.
  prefs: []
  type: TYPE_NORMAL
- en: Oculus Rift CV1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Originally starting out as a crowd funded project, the Oculus Rift has become
    one of the most popular headsets currently available. The Oculus Rift has seen
    a few iterations. The first and second hardware releases were geared towards developers
    (the DK1 and DK2). Upon purchase of the Oculus startup by Facebook, the social
    media giant released the first commercial version of the hardware known as the
    **Consumer Version 1** (**CV1**). While supported on the **Steam** gaming platform,
    the Oculus is very much tied to its own launcher and software platform. The headset
    currently only supports PC development:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/233cdd18-da4f-49ba-aaf2-df46b2c88dad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the features of Oculus Rift CV1:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Screen type**: AMOLED'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resolution**: 1080 x 1200 per eye'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Field of view**: ~110⁰'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Head tracking**: IMU (compass, accelerometer, gyroscope), IR optical tracing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The minimum recommended PC specifications are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPU**: NVIDIA GeForce GTX 970 or AMD Radeon R9 290'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU**: Intel i5-4590 or AMD FX 8350'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RAM**: 8 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OS**: Windows 7'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HTC Vive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Arguably the most popular headset currently available, the HTC Vive was created
    by HTC (a smartphone and tablet manufacturer) and the Valve corporation (a gaming
    company, best known for the Steam game platform). Often compared directly to the
    Oculus Rift, the HTC Vive does share many similarities in design, with slight
    differences that, in many developers'' minds, make the HTC Vive the superior piece
    of hardware:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c31b745-88c6-4288-b02e-d2871d5f1be2.jpg)![](img/520be352-da41-4fa4-9afe-90559bc7d79b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the features of HTC Vive:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Screen type**: AMOLED'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resolution**: 1080 x 1200 per eye'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Field of view**: 110⁰'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Head tracking**: IMU (compass, accelerometer, gyroscope), 2 IR base stations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The minimum recommended PC specifications are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPU**: NVIDIA GeForce GTX 970 or AMD Radeon R9 290'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU**: Intel i5-4590 or AMD FX 8350'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RAM**: 4 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OS**: Windows 7, Linux'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open Source Virtual Reality (OSVR) development kit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another very interesting hardware option is the OSVR kit, developed by Razer
    and Sensics. What makes the OSVR unique is that it is an open licensed, non-proprietary
    hardware platform and ecosystem. This gives developers lots of freedom when designing
    their AR/VR experiences. OSVR is also a software framework, which we will cover
    shortly. The framework, like the hardware, is open licence and designed to be
    cross-platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1155357a-8f9e-40d5-af16-a4ce34315a52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the features of OSVR:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Screen type**: AMOLED'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resolution**: 960 x 1080 per eye'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Field of view**: 100⁰'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Head tracking**: IMU (compass, accelerometer, gyroscope), IR optical tracing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The minimum recommended PC specifications are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPU**: NVIDIA GeForce GTX 970 or AMD Radeon R9 290'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU**: Intel i5-4590 or AMD FX 8350'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RAM**: 4 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OS**: Cross-platform support'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sony PlayStation VR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Originally referred to as **Project Morpheus**, the Sony PlayStation VR is
    the Sony corporation''s entry to the VR space. Unlike the other headsets in this
    list, the Sony PlayStation VR headset is not driven by a PC, but instead connects
    to the Sony PlayStation 4 gaming console. While not the highest fidelity or most
    technically advanced, by using the PS4 as its platform, the Sony PlayStation VR
    headset has a 30 million plus console base available:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38bde5a4-a6e8-407b-b44f-30801ab071cf.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the features of Sony PlayStation VR:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Screen type**: AMOLED'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resolution**: 960 x 1080 per eye'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Field of view**: ~100⁰'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Head tracking**: IMU (compass, accelerometer, gyroscope), IR optical tracing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Console hardware**: Sony PlayStation 4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Windows Mixed Reality headsets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the newest entries into the VR hardware space is the Windows Mixed Reality
    enabled group of headsets. While not a single headset design, Windows Mixed Reality
    has a set of specifications and software support that enables VR from the Windows
    10 desktop. Referred to as **Mixed Reality** (**MR**), the unique feature of these
    headsets is their built-in spatial awareness or room sense. Other headsets, such
    as the Oculus Rift and the HTC Vive, support similar features, but unlike the
    Windows MR devices, they require extra hardware to support tracking. This lack
    of extra hardware means that the Windows MR headsets should be simpler to set
    up and have the potential to make PC-powered VR experiences more portable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b8ab8b2-99a6-4c0a-a83a-8a28695a7fb8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the features of Windows MR headsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Screen type**: Various'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resolution**: Various'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Field of view**: Various'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Head tracking**: 9DoF inside out headset-based tracking system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The minimum recommended PC specifications are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPU**: NVIDIA GeForce GTX 960, AMD Radeon RX 460 or integrated Intel HD Graphics
    620'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU**: Intel i5-4590 or AMD FX 8350'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RAM**: 8 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OS**: Windows 10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VR rendering concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at VR from a rendering point of view, it quickly becomes apparent that
    VR poses some unique challenges. This is due in part to some necessary performance
    benchmarks that need to be achieved and the current limitations of rendering hardware.
    When rendering VR content, it is necessary to render at a higher resolution than
    standard high definition, often twice or more. The rendering also needs to be
    extremely quick, with frame rates of 90 fps or higher per eye being the benchmark.
    This, combined with the use of anti-aliasing and sampling techniques, means that
    rendering a VR scene requires upwards of five times the computation power of a
    standard game running at 1080p with 60 fps. In the upcoming sections, we will
    cover some of the key differences when rendering VR content, and touch on some
    concepts that you can implement to retain performance.
  prefs: []
  type: TYPE_NORMAL
- en: Working with frusta
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The biggest difference when developing a VR ready engine is understanding how
    to build a proper, clipped, view frustum when dealing with more than one viewpoint.
    In a typical non-VR game, you have a single view point (camera), from which you
    create a view frustum. Refer back to earlier in the book if you need a complete
    refresher, but this view frustum determines what will be rendered and ultimately
    displayed on screen to the user. The following is a diagram depicting a typical
    view frustum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09ce5ebe-8a4b-42a3-9907-400cfc948b74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When rendering in VR, you have at least one frustum per eye, commonly displaying
    to single HMD in stereoscopic, meaning there are a pair of images displayed on
    a single screen, allowing for the illusion of depth. Often these images are depicting
    the left and right eye view of the scene. What this means is that we must take
    into consideration the position of both *eye* frusta and produce a final view
    frustum for rendering by combining them both. The following is a diagram depiction
    of these view frusta:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7feba28c-0c2e-47e8-8980-0bd056436155.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When it comes to the creation of a single frustum that combines both the left
    and right eye frusta, it is actually quite easy. As depicted in the following
    diagram, you need to place the vertex of the new frustum between both of the eyes
    and slightly behind them. You then move the near clipping plane position so that
    it is aligned with either of the eye frustum''s clipping planes. This is important
    for the final display **frustum culling**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/820bfde5-6d8c-4ea7-9574-32035e768475.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can calculate this frustum with some simple math using the **interpupillary
    distance** (**IPD**), as demonstrated perfectly in the following diagram by Cass
    Everitt from the Oculus Rift team:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8cbe7fd-05cc-48cb-862d-f9af0cf32b60.png)'
  prefs: []
  type: TYPE_IMG
- en: We could also simplify this procedure by simply culling against the shared eye
    frusta top and bottom planes. While not technically forming a perfect frustum,
    using a culling algorithm that tests a single plane at a given time will produce
    the desired effect.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is most of this can be abstracted away, and in many headset SDKs
    there are methods to help you. It is, however, important to understand the difference
    between how frusta is used when compared to standard non-VR scene rendering.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing rendering performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with a single camera and view point, as with most non-VR games,
    we can simply treat the rendering process as one step within our engine. When
    working with multiple view points, this is different. We could, of course, just
    treat each view point as a single rendering task, processing one after the other,
    but this is would result in a slow-performing renderer.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the previous section, there is quite an overlap between what each
    *eye* sees. This provides us with the perfect opportunity to optimize our rendering
    process by sharing and reusing data. To do this, we can implement the concept
    of **data context**. Using this concept, we can classify which elements exist
    uniquely for a single eye, and which elements can be shared. Let''s take a look
    at these data contexts and how we can use them to speed up our rendering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Frame context**: Simply put, the frame context is used for any element that
    needs to be rendered and is view-orientation independent. This would include elements
    such as skyboxes, global reflections, water textures, and so on. Anything that
    can be shared across view points can be placed in this context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eye context**: This is the context for elements that cannot be shared between
    view points. This would include any element that needs stereo disparity when rendering.
    It is also in this context that we could store per eye data that would be used
    in our shader computations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using this simple separation of data into different contexts, we could then
    reorganize our rendering process to look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'While this appears on the surface to be basic, it is a very powerful concept.
    By separating the rendering in this manner and sharing what we can, we are greatly
    increasing the performance of our renderer overall. This is one of the simplest
    optimizations with one of the biggest returns. We can also carry this over to
    how we set up our shader uniforms, breaking them into context pieces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This division of data works great from a conceptual point of view, and each
    of these data pieces can be updated at different times, providing more performance
    overall.
  prefs: []
  type: TYPE_NORMAL
- en: That basically describes, at a high level, the performant way to handle the
    rendering of multiple view points for VR. As mentioned before, a large amount
    of the setup involved with getting the hardware and pipeline connected is abstracted
    away for us in the SDKs being developed. In the next section, we will look at
    some of these SDKs, and close out the chapter by looking at the implementation
    of an SDK in our example engine.
  prefs: []
  type: TYPE_NORMAL
- en: Headset SDKs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many SDKs available for implementing the various headsets and supporting
    hardware, with most manufactures supplying their own in some form. In the upcoming
    sections, we will quickly look at three of the most commonly used SDKs when developing
    PC driven HMD VR experiences:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Oculus PC SDK** ([https://developer.oculus.com/downloads/package/oculus-sdk-for-windows/](https://developer.oculus.com/downloads/package/oculus-sdk-for-windows/)):
    This SDK was specifically created for use when developing Oculus Rift HMD experiences
    and games in C++. The core SDK supplies everything developers need to gain access
    to rendering, tracking, input, and other core hardware functionalities. The core
    SDK is sublimated by other supporting SDKs for audio, platform, and avatar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenVR** ([https://github.com/ValveSoftware/openvr](https://github.com/ValveSoftware/openvr)): This
    is the SDK provided by the Valve corporation as the default API and runtime for
    the SteamVR platform. This is also the default SDK for HTC Vive HMD development,
    but is designed to have multiple vendor support. This means you have the ability
    to target multiple HMDs without having to know exactly which HMD is connected.
    This will be the SDK we implement for our example engine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OSVR** ([http://osvr.github.io/](http://osvr.github.io/)): The OSVR SDK,
    like its name states, is an open source SDK designed to work with multiple hardware
    vendors. This SDK is the default SDK for the HMD of the same name, the OSVR headset.
    The project is spearheaded by Razer and Sensics, with many large gaming partners
    signing on. The OSVR SDK is available for Microsoft Windows, Linux, Android, and
    macOS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing VR support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with many other systems we have looked at throughout the book, implementing
    VR support from scratch can be a very challenging and time-consuming process.
    However, much like those other systems, libraries and SDKs exist to help ease
    and simplify the process. In the next section, we will cover how we can add VR
    rendering support to our example engine using the OpenVR SDK provided by the Valve
    corporation. We will cover only the main points in full. To see a more complete
    overview of each method, refer to the comments in the example code, and visit
    the OpenVR SDK Wiki for more SDK-specific information ([https://github.com/ValveSoftware/openvr/wiki](https://github.com/ValveSoftware/openvr/wiki)).
  prefs: []
  type: TYPE_NORMAL
- en: Verifying HMD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start out, we need to do a few things to set up our hardware and environment.
    We need to first test if a headset is attached to the computer. Then we need check
    if the OpenVR runtime has been installed. We can then initialize the hardware
    and finally ask it a few questions about its capability. To do this, we will add
    some code to our `GameplayScreen` class; for brevity's sake we will skip over
    some sections. The full code can be found in the example project in the `Chapter11`
    folder of the code repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by checking to see if a VR headset has been attached to the computer
    and if the OpenVR (SteamVR) runtime has been installed. To do this, we will add
    the following to the `Build()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we throw an exception to be handled and logged if either of these checks
    fail. Now that we know we have some hardware and the required software, we can
    initialize the framework. To do this, we call the `InitVR` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `InitVR` function''s main purpose is to, in turn, call the `VR_Init` method
    of the OpenVR SDK. In order to do that, it needs to first create and set up an
    error handler. It will also require us to define what type of application this
    will be. In our case, we are stating that this will be a scene application, `vr::VRApplication_Scene`.
    This means we are creating a 3D application that will be drawing an environment.
    There are other options, such as creating a utility or overlay only applications.
    Finally, once we have the HMD initialized, with no errors, we ask the headset
    to tell us a little about itself. We do this using the `GetTrackedDeviceString`
    method that we will look at shortly. The whole `InitVR` method then looks like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `HandleVRError` method is just a simple helper method that takes the error
    passed in and throws an error to be handled and logged while providing an English
    translation of the error being thrown. The following is the method in its entirety:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The other method that the `InitVR` function calls is the `GetTrackedDeviceString`
    function. This is a function provided as part of the OpenVR example code, which
    allows us to return some information about the attached device. In our case, we
    are asking for the system name and the serial number properties, if available,
    for the attached device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, back in our `Build` method, now that we have completed the initialization
    steps, we can check that all went well by asking the system if the `VRCompositor`
    function is set to a value other than NULL. If it is, that means everything is
    ready to go and we can then ask our HMD what it would like our rendering target
    size to be and display that as a string output in our console window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The last thing we need to do is make sure we clean up on our program''s completion.
    Here, in the `Destroy` method of the `GamplayScreen`, we are first checking to
    see if the HMD was initialized; if it was we call the `VR_Shutdown` method and
    set the `m_hmd` variable to NULL. It is very important to call the `VR_Shutdown`
    on application closing, as if you do not, the OpenVR/SteamVR may hang and could
    require a reboot before it is operational again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if we go ahead and run this example, in the console window you should see
    something similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/3587d745-2294-4081-b48f-3a57744be739.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: Rendering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the HMD set up and talking with our engine, the next step is
    to render to it. The process is actually not that complicated; as mentioned before,
    a great deal is handled for us by the SDK. To keep things as simple as possible,
    this example is just a simple rendering example. We are not handling head tracking
    or input, we are simply just going to display a different color in each eye. As
    with the previous example, in order to save time and space, we are only going
    to cover the important elements for you to grasp the concept. The full code can
    be found in the example project in the `Chapter11` folder of the code repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed before, when rendering in stereoscopic, you are often rendering
    a single display that has been divided in half. We then pass the appropriate data
    to the half, depending on what is viewable in that eye. Look back to the *Working
    with frusta* section for a refresher on why this is. What this boils down to is
    that we need to create a framebuffer for each eye. To do this, we have a `RenderTarget`
    class that creates the framebuffer, attaches the texture, and finally creates
    the needed viewport (which is half of the total display width). To save space,
    I won''t print out the `RenderTarget` class; it is fairly straightforward and
    nothing we haven''t seen before. Instead, let''s move on to setup and the actual
    functions that will handle the displaying of the scene in the HMD. To start with,
    we need to connect our `RenderTarget` to our texture, and for proper implementation
    clear and set the buffers. To do this we add the following to our `OnEntry` method
    of `GameplayScreen`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: I will not go through the previous code line by line since we have seen all
    this done before. Now, with our buffers and textures set, we can move onto adding
    the drawing call.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenVR SDK provides the needed methods to handle the complex pieces of displaying
    VR scenes. The majority of this complex work is done by the compositor system.
    As stated by Valve, *"The compositor simplifies the process of displaying images
    to the user by taking care of distortion, prediction, synchronization, and other
    subtle issues that can be a challenge to get operating properly for a solid VR
    experience."*
  prefs: []
  type: TYPE_NORMAL
- en: 'To connect to the compositor subsystem, we have created a simple method called
    `SubmitFrames`. This method takes three arguments—a texture for each eye and a
    Boolean value to specify whether the color space should be `linear`. At the time
    of writing, we always want to specify that the color space should be `Gamma` for
    `OpenGL`. Inside the method, we get the device we wish to render to, set the color
    space, convert the texture, and then submit these textures to the `VRCompositor`,
    which then, under the hood, handles the displaying of the textures to the correct
    eye. The entire method looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'With our `SubmitFrames` function in place, we can then call the method form
    inside of the GameplayScreen update, right after the `glClear` function call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If you now run the example project, provided you have the necessary SteamVR
    framework installed, you should see different colors being shown in each eye of
    the headset.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While this was a quick introduction to the world of VR development, it should
    provide you with a great testing bed for your experience ideas. We learned how
    to handle multiple view frusta, learned about various hardware options, and finally
    looked at how we could add VR support to our example engine using the OpenVR SDK.
    As advancements in hardware progress, VR will continue to gain momentum and will
    continue to push into new fields. Understanding how VR rendering works as a whole
    provides a new level of depth to your development knowledge pool.
  prefs: []
  type: TYPE_NORMAL
