- en: Scaling and Upgrading Applications
  prefs: []
  type: TYPE_NORMAL
- en: TERNAIn this chapter, we will discuss the methods and strategies that we can
    use to dynamically scale containerized services running on Kubernetes to handle
    the changing traffic needs of our service. After following the recipes in this
    chapter, you will have the skills needed to create load balancers to distribute
    traffic to multiple workers and increase bandwidth. You will also know how to
    handle upgrades in production with minimum downtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling applications on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning applications to nodes with priority
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an external load balancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an ingress service and service mesh using Istio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an ingress service and service mesh using Linkerd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auto-healing pods in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing upgrades through blue/green deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recipes in this chapter assume that you have a functional Kubernetes cluster
    deployed by following one of the recommended methods described in [Chapter 1](a8580410-3e1c-4e28-8d18-aaf9d38d011f.xhtml),
    *Building Production-Ready Kubernetes Clusters*.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes command-line tool, `kubectl`, will be used for the rest of the
    recipes in this chapter since it's the main command-line interface for running
    commands against Kubernetes clusters. We will also use helm where Helm charts
    are available to deploy solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling applications on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will perform application and cluster scaling tasks. You
    will learn how to manually and also automatically scale your service capacity
    up or down in Kubernetes to support dynamic traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clone the `k8sdevopscookbook/src` repository to your workstation to use the
    manifest files in the `chapter7` directory, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Validating the installation of Metrics Server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manually scaling an application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoscaling applications using Horizontal Pod Autoscaler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating the installation of Metrics Server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The *Autoscaling applications using the Horizontal Pod Autoscaler* recipe in
    this section also requires Metrics Server to be installed on your cluster. Metrics
    Server is a cluster-wide aggregator for core resource usage data. Follow these
    steps to validate the installation of Metrics Server:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Confirm if you need to install Metrics Server by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If it''s been installed correctly, you should see the following node metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you get an error message stating `metrics not available yet`, then you need
    to follow the steps provided in the next chapter in the *Adding metrics using
    the Kubernetes Metrics Server* recipe to install Metrics Server.
  prefs: []
  type: TYPE_NORMAL
- en: Manually scaling an application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the usage of your application increases, it becomes necessary to scale
    the application up. Kubernetes is built to handle the orchestration of high-scale
    workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to understand how to manually scale an application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change directories to `/src/chapter7/charts/node`, which is where the local
    clone of the example repository that you created in the *Getting ready* section
    can be found:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the To-Do application example using the following command. This Helm
    chart will deploy two pods, including a Node.js service and a MongoDB service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the service IP of `my-ch7-app-node` to connect to the application. The
    following command will return an external address for the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the address from *Step 3* in a web browser. You will get a fully functional
    To-Do application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/1e02a226-7f1d-4804-98f7-6176144d9845.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Check the status of the application using `helm status`. You will see the number
    of pods that have been deployed as part of the deployment in the `Available` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Scale the node pod to `3` replicas from the current scale of a single replica:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the status of the application again and confirm that, this time, the
    number of available replicas is `3` and that the number of `my-ch7-app-node` pods
    in the `v1/Pod` section has increased to `3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To scale down your application, repeat *Step 5*, but this time with `2` replicas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: With that, you've learned how to scale your application when needed. Of course,
    your Kubernetes cluster resources should be able to support growing workload capacities
    as well. You will use this knowledge to test the service healing functionality
    in the *Auto-healing pods in Kubernetes* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The next recipe will show you how to autoscale workloads based on actual resource
    consumption instead of manual steps.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling applications using a Horizontal Pod Autoscaler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this recipe, you will learn how to create a **Horizontal Pod Autoscaler**
    (**HPA**) to automate the process of scaling the application we created in the
    previous recipe. We will also test the HPA with a load generator to simulate a
    scenario of increased traffic hitting our services. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, make sure you have the sample To-Do application deployed from the *Manually
    scaling an application* recipe. When you run the following command, you should
    get both MongoDB and Node pods listed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an HPA declaratively using the following command. This will automate
    the process of scaling the application between `1` to `5` replicas when the `targetCPUUtilizationPercentage`
    threshold is reached. In our example, the mean of the pods'' CPU utilization target
    is set to `50` percent usage. When the utilization goes over this threshold, your
    replicas will be increased:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Although the results may be the same most of the time, a declarative configuration
    requires an understanding of the Kubernetes object configuration specs and file
    format. As an alternative, `kubectl` can be used for the imperative management
    of Kubernetes objects.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you must have a request set in your deployment to use autoscaling.
    If you do not have a request for CPU in your deployment, the HPA will deploy but
    will not work correctly.
  prefs: []
  type: TYPE_NORMAL
- en: You can also create the same `HorizontalPodAutoscaler` imperatively by running
    the `$ kubectl autoscale deployment my-ch7-app-node --cpu-percent=50 --min=1 --max=5`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Confirm the number of current replicas and the status of the HPA. When you
    run the following command, the number of replicas should be `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the service IP of `my-ch7-app-node` so that you can use it in the next
    step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Start a new Terminal window and create a load generator to test the HPA. Make
    sure that you replace `YOUR_SERVICE_IP` in the following code with the actual
    service IP from the output of *Step 4*. This command will generate traffic to
    your To-Do application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Wait a few minutes for the Autoscaler to respond to increasing traffic. While
    the load generator is running on one Terminal, run the following command on a
    separate Terminal window to monitor the increased CPU utilization. In our example,
    this is set to `210%`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, check the deployment size and confirm that the deployment has been resized
    to `5` replicas as a result of the increased workload:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: On the Terminal screen where you run the load generator, press *Ctrl* + *C* to
    terminate the load generator. This will stop the traffic coming to your application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Wait a few minutes for the Autoscaler to adjust and then verify the HPA status
    by running the following command. The current CPU utilization should be lower.
    In our example, it shows that it went down to `0%`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the deployment size and confirm that the deployment has been scaled down
    to `1` replica as the result of stopping the traffic generator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this recipe, you learned how to automate how an application is scaled dynamically
    based on changing metrics. When applications are scaled up, they are dynamically
    scheduled on existing worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe showed you how to manually and automatically scale the number of
    pods in a deployment dynamically based on the Kubernetes metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, in *Step 2*,we created an Autoscaler that adjusts the number
    of replicas between the defined minimum using `minReplicas: 1` and `maxReplicas:
    5`. As shown in the following example, the adjustment criteria are triggered by
    the `targetCPUUtilizationPercentage: 50` metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`targetCPUUtilizationPercentage` was used with the `autoscaling/v1` APIs. You
    will soon see that `targetCPUUtilizationPercentage` will be replaced with an array
    called metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the new metrics and custom metrics, run the following command.
    This will return the manifest we created with V1 APIs into a new manifest using
    V2 APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This enables you to specify additional resource metrics. By default, CPU and
    memory are the only supported resource metrics. In addition to these resource
    metrics, v2 APIs enable two other types of metrics, both of which are considered
    custom metrics: per-pod custom metrics and object metrics. You can read more about
    this by going to the *Kubernetes HPA documentation* link mentioned in the *See
    also *section.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes pod Autoscaler using custom metrics: [https://sysdig.com/blog/kubernetes-autoscaler/](https://sysdig.com/blog/kubernetes-autoscaler/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes HPA documentation: [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Declarative Management of Kubernetes Objects Using Configuration Files: [https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imperative Management of Kubernetes Objects Using Configuration Files: [https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-config/](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-config/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning applications to nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will make sure that pods are not scheduled onto inappropriate
    nodes. You will learn how to schedule pods into Kubernetes nodes using node selectors,
    taints, toleration and by setting priorities.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Labeling nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning pods to nodes using nodeSelector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning pods to nodes using node and inter-pod affinity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Labeling nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes labels are used for specifying the important attributes of resources
    that can be used to apply organizational structures onto system objects. In this
    recipe, we will learn about the common labels that are used for Kubernetes nodes
    and apply a custom label to be used when scheduling pods into nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to list some of the default labels that
    have been assigned to your nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'List the labels that have been assigned to your nodes. In our example, we will
    use a kops cluster that''s been deployed on AWS EC2, so you will also see the
    relevant AWS labels, such as availability zones:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the list of the nodes in your cluster. We will use node names to assign
    labels in the next step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Label two nodes as `production` and `development`. Run the following command
    using your worker node names from the output of *Step 2*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that the new labels have been assigned to the nodes. This time, you
    should see `environment` labels on all the nodes except the node labeled `role=master`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: It is recommended to document labels for other people who will use your clusters.
    While they don't directly imply semantics to the core system, make sure they are still
    meaningful and relevant to all users.
  prefs: []
  type: TYPE_NORMAL
- en: Assigning pods to nodes using nodeSelector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this recipe, we will learn how to schedule a pod onto a selected node using
    the nodeSelector primitive:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a copy of the Helm chart we used in the *Manually scaling an application*
    recipe in a new directory called `todo-dev`. We will edit the templates later
    in order to specify `nodeSelector`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit the `deployment.yaml` file in the `templates` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Add `nodeSelector:` and `environment: "{{ .Values.environment }}"` right before
    the `containers:` parameter. This should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The Helm installation uses templates to generate configuration files. As shown
    in the preceding example, to simplify how you customize the provided values, `{{expr}}`
    is used, and these values come from the `values.yaml` file names. The `values.yaml`
    file contains the default values for a chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it may not be practical on large clusters, instead of using `nodeSelector`
    and labels, you can also schedule a pod on one specific node using the `nodeName`
    setting. In that case, instead of the `nodeSelector` setting, you add `nodeName:
    yournodename` to your deployment manifest.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve added the variable, edit the `values.yaml` file. This is where
    we will set the environment to the `development` label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `environment: development` line to the end of the files. It should
    look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit the `Chart.yaml` file and change the chart name to its folder name. In
    this recipe, it''s called `todo-dev`. After these changes, the first two lines
    should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the Helm dependencies and build them. The following commands will pull
    all the dependencies and build the Helm chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Examine the chart for issues. If there are any issues with the chart''s files,
    the linting process will bring them up; otherwise, no failures should be found:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the To-Do application example using the following command. This Helm
    chart will deploy two pods, including a Node.js service and a MongoDB service,
    except this time the nodes are labeled as `environment: development`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Check that all the pods have been scheduled on the development nodes using
    the following command. You will find the `my-app7-dev-todo-dev` pod running on
    the node labeled `environment: development`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: With that, you've learned how to schedule workload pods onto selected nodes
    using the `nodeSelector` primitive.
  prefs: []
  type: TYPE_NORMAL
- en: Assigning pods to nodes using node and inter-pod Affinity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to expand the constraints we expressed in
    the previous recipe, *Assigning pods to labeled nodes using nodeSelector*, using
    the affinity and anti-affinity features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use a scenario-based approach to simplify this recipe for different
    affinity selector options. We will take the previous example, but this time with
    complicated requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '`todo-prod` must be scheduled on a node with the `environment:production` label
    and should fail if it can''t.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`todo-prod` should run on a node that is labeled with `failure-domain.beta.kubernetes.io/zone=us-east-1a` or
    `us-east-1b` but can run anywhere if the label requirement is not satisfied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`todo-prod` must run on the same zone as `mongodb`, but should not run in the
    zone where `todo-dev` is running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The requirements listed here are only examples in order to represent the use
    of some affinity definition functionality. This is not the ideal way to configure
    this specific application. The labels may be completely different in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding scenario will cover both types of node affinity options (`requiredDuringSchedulingIgnoredDuringExecution`
    and `preferredDuringSchedulingIgnoredDuringExecution`). You will see these options
    later in our example. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a copy of the Helm chart we used in the *Manually scaling an application* recipe
    to a new directory called `todo-prod`. We will edit the templates later in order
    to specify `nodeAffinity` rules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit the `values.yaml` file. To access it, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Replace the last line, `affinity: {}`, with the following code. This change
    will satisfy the first requirement we defined previously, meaning that a pod can
    only be placed on a node with an `environment` label and whose value is `production`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: You can also specify more than one `matchExpressions` under the `nodeSelectorTerms`.
    In this case, the pod can only be scheduled onto a node where all `matchExpressions`
    are satisfied, which may limit your successful scheduling chances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it may not be practical on large clusters, instead of using `nodeSelector`
    and labels, you can also schedule a pod on a specific node using the `nodeName` setting.
    In this case, instead of the `nodeSelector` setting, add `nodeName: yournodename` to
    your deployment manifest.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, add the following lines right under the preceding code addition. This
    addition will satisfy the second requirement we defined, meaning that nodes with
    a label of `failure-domain.beta.kubernetes.io/zone` and whose value is `us-east-1a` or
    `us-east-1b`  will be preferred:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'For the third requirement, we will use the inter-pod affinity and anti-affinity
    functionalities. They allow us to limit which nodes our pod is eligible to be
    scheduled based on the labels on pods that are already running on the node instead
    of taking labels on nodes for scheduling. The following podAffinity `requiredDuringSchedulingIgnoredDuringExecution`
    rule will look for nodes where `app: mongodb` exist and use `failure-domain.beta.kubernetes.io/zone`
    as a topology key to show us where the pod is allowed to be scheduled:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following lines to complete the requirements. This time, the `podAntiAffinity
    preferredDuringSchedulingIgnoredDuringExecution` rule will look for nodes where
    `app: todo-dev` exists and use `failure-domain.beta.kubernetes.io/zone` as a topology
    key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit the `Chart.yaml` file and change the chart name to its folder name. In
    this recipe, it''s called `todo-prod`. After making these changes, the first two
    lines should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the Helm dependencies and build them. The following commands will pull
    all the dependencies and build the Helm chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Examine the chart for issues. If there are any issues with the chart files,
    the linting process will bring them up; otherwise, no failures should be found:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the To-Do application example using the following command. This Helm
    chart will deploy two pods, including a Node.js service and a MongoDB service,
    this time following the detailed requirements we defined at the beginning of this
    recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Check that all the pods that have been scheduled on the nodes are labeled as
    `environment: production` using the following command. You will find the `my-app7-dev-todo-dev` pod
    running on the nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: In this recipe, you learned about advanced pod scheduling practices while using
    a number of primitives in Kubernetes, including `nodeSelector`, node affinity,
    and inter-pod affinity. Now, you will be able to configure a set of applications
    that are co-located in the same defined topology or scheduled in different zones
    so that you have better **service-level agreement** (**SLA**) times.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The recipes in this section showed you how to schedule pods on preferred locations,
    sometimes based on complex requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *Labeling nodes* recipe, in *Step 1*, you can see that some standard
    labels have been applied to your nodes already. Here is a short explanation of
    what they mean and where they are used:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubernetes.io/arch`: This comes from the `runtime.GOARCH` parameter and is
    applied to nodes to identify where to run different architecture container images,
    such as x86, arm, arm64, ppc64le, and s390x, in a mixed architecture cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubernetes.io/instance-type`: This is only useful if your cluster is deployed
    on a cloud provider. Instance types tell us a lot about the platform, especially
    for AI and machine learning workloads where you need to run some pods on instances
    with GPUs or faster storage options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubernetes.io/os`: This is applied to nodes and comes from `runtime.GOOS`.
    It is probably less useful unless you have Linux and Windows nodes in the same
    cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`failure-domain.beta.kubernetes.io/region` and `/zone`: This is also more useful
    if your cluster is deployed on a cloud provider or your infrastructure is spread
    across a different failure-domain. In a data center, it can be used to define
    a rack solution so that you can schedule pods on separate racks for higher availability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kops.k8s.io/instancegroup=nodes`: This is the node label that''s set to the
    name of the instance group. It is only used with kops clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubernetes.io/hostname`: Shows the hostname of the worker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubernetes.io/role`: This shows the role of the worker in the cluster. Some
    common values include `node` for representing worker nodes and `master`, which
    shows the node is the master node and is tainted as not schedulable for workloads
    by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the *Assigning pods to nodes using node and inter-pod affinity* recipe, in *Step
    3*, the node affinity rule says that the pod can only be placed on a node with
    a label whose key is `environment` and whose value is `production`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 4*, the `affinity key: value` requirement is preferred (`preferredDuringSchedulingIgnoredDuringExecution`). The
    `weight` field here can be a value between `1` and `100`. For every node that
    meets these requirements, a Kubernetes scheduler computes a sum. The nodes with
    the highest total score are preferred.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another detail that''s used here is the `In` parameter. Node Affinity supports
    the following operators: `In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt`, and `Lt`.
    You can read more about the operators by looking at the *Scheduler affinities
    through examples* link mentioned in the *See also *section.'
  prefs: []
  type: TYPE_NORMAL
- en: If selector and affinity rules are not well planned, they can easily block pods
    getting scheduled on your nodes. Keep in mind that if you have specified both
    `nodeSelector` and `nodeAffinity` rules, both requirements must be met for the
    pod to be scheduled on the available nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 5*, inter-pod affinity is used (`podAffinity`) to satisfy the requirement
    in PodSpec. In this recipe, `podAffinity` is `requiredDuringSchedulingIgnoredDuringExecution`.
    Here, `matchExpressions` says that a pod can only run on nodes where `failure-domain.beta.kubernetes.io/zone`
    matches the nodes where other pods with the `app: mongodb` label are running.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 6*, the requirement is satisfied with `podAntiAffinity` using `preferredDuringSchedulingIgnoredDuringExecution`.
    Here, `matchExpressions` says that a pod can''t run on nodes where `failure-domain.beta.kubernetes.io/zone`
    matches the nodes where other pods with the `app: todo-dev` label are running.
    The weight is increased by setting it to `100`.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'List of known labels, annotations, and taints: [https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Assigning Pods to Nodes in the Kubernetes documentation: [https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More on labels and selectors in the Kubernetes documentation: [https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scheduler affinities through examples: [https://banzaicloud.com/blog/k8s-affinities/](https://banzaicloud.com/blog/k8s-affinities/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Node affinity and NodeSelector design document: [https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/nodeaffinity.md](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/nodeaffinity.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpod topological affinity and anti-affinity design document: [https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/podaffinity.md](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/podaffinity.md)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an external load balancer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The load balancer service type is a relatively simple service alternative to
    ingress that uses a cloud-based external load balancer. The external load balancer
    service type's support is limited to specific cloud providers but is supported
    by the most popular cloud providers, including AWS, GCP, Azure, Alibaba Cloud,
    and OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will expose our workload ports using a load balancer. We
    will learn how to create an external GCE/AWS load balancer for clusters on public
    clouds, as well as for your private cluster using `inlet-operator`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources. In this recipe, we are using a cluster that's
    been deployed on AWS using `kops`, as described in [Chapter 1](a8580410-3e1c-4e28-8d18-aaf9d38d011f.xhtml),
    *Building Production-Ready Kubernetes Clusters*, in the* Amazon Web Services*
    recipe. The same instructions will work on all major cloud providers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the example files, clone the `k8sdevopscookbook/src` repository to
    your workstation to use the configuration files in the `src/chapter7/lb` directory,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: After you've cloned the examples repository, you can move on to the recipes.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an external cloud load balancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the external address of the service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an external cloud load balancer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you create an application and expose it as a Kubernetes service, you usually
    need the service to be reachable externally via an IP address or URL. In this
    recipe, you will learn how to create a load balancer, also referred to as a cloud
    load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we have seen a couple of examples that used the load
    balancer service type to expose IP addresses, including the *Configuring and managing
    S3 object storage using MinIO* and *Application backup and recovery using Kasten* recipes
    in the previous chapter, as well as the To-Do application that was provided in
    this chapter in the *Assigning applications to nodes* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the MinIO application to learn how to create a load balancer. Follow
    these steps to create a service and expose it using an external load balancer
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Review the content of the `minio.yaml` file in the examples directory in `src/chapter7/lb`
    and deploy it using the following command. This will create a StatefulSet and
    a service where the MinIO port is exposed internally to the cluster via port number
    `9000`. You can choose to apply the same steps and create a load balancer for
    your own application. In that case, skip to *Step 2*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'List the available services on Kubernetes. You will see that the MinIO service
    shows `ClusterIP` as the service type and `none` under the `EXTERNAL-IP` field:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new service with the `TYPE` set to `LoadBalancer`. The following command
    will expose `port: 9000` of our MinIO application at `targetPort: 9000` using
    the `TCP` protocol, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will immediately create the `Service` object, but the
    actual load balancer on the cloud provider side may take 30 seconds to a minute
    to be completely initialized. Although the object will state that it's ready,
    it will not function until the load balancer is initialized. This is one of the
    disadvantages of cloud load balancers compared to ingress controllers, which we
    will look at in the next recipe, *Creating an ingress service and service mesh
    using Istio*.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an alternative to *Step 3*, you can also create the load balancer by using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Finding the external address of the service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to get the externally reachable address
    of the service:'
  prefs: []
  type: TYPE_NORMAL
- en: 'List the services that use the `LoadBalancer` type. The `EXTERNAL-IP` column
    will show you the cloud vendor-provided address:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are running on a cloud provider service such as AWS, you can also use
    the following command to get the exact address. You can copy and paste this into
    a web browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are running on a bare-metal server, then you probably won''t have a
    `hostname` entry. As an example, if you are running MetalLB ([https://metallb.universe.tf/](https://metallb.universe.tf/)),
    a load balancer for bare-metal Kubernetes clusters, or SeeSaw ([https://github.com/google/seesaw](https://github.com/google/seesaw)),
    a **Linux Virtual Server** (**LVS**)-based load balancing platform, you need to
    look for the `ip` entry instead:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will return a link similar to `https://containerized.me.us-east-1.elb.amazonaws.com:9000`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe showed you how to quickly create a cloud load balancer to expose
    your services with an external address.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Creating a cloud load balancer* recipe, in *Step 3*, when a load balancer
    service is created in Kubernetes, a cloud provider load balancer is created on
    your behalf without you having to go through the cloud service provider APIs separately.
    This feature helps you easily manage the creation of load balancers outside of
    your Kubernetes cluster, but at the same takes a bit of time to complete and requires
    a separate load balancer for every service, so this might be costly and not very
    flexible.
  prefs: []
  type: TYPE_NORMAL
- en: To give load balancers flexibility and add more application-level functionality,
    you can use ingress controllers. Using ingress, traffic routing can be controlled
    by rules defined in the ingress resource. You will learn more about popular ingress
    gateways in the next two recipes, *Creating an ingress service and service mesh
    using Istio* and *Creating an ingress service and service mesh using Linkerd*.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes documentation on the load balancer service type: [https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer](https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a load balancer on Amazon EKS: [https://docs.aws.amazon.com/eks/latest/userguide/load-balancing.html](https://docs.aws.amazon.com/eks/latest/userguide/load-balancing.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a load balancer on AKS: [https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard](https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a load balancer on Alibaba Cloud: [https://www.alibabacloud.com/help/doc-detail/53759.htm](https://www.alibabacloud.com/help/doc-detail/53759.htm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancer for your private Kubernetes cluster: [https://blog.alexellis.io/ingress-for-your-local-kubernetes-cluster/](https://blog.alexellis.io/ingress-for-your-local-kubernetes-cluster/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an ingress service and service mesh using Istio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Istio is a popular open source service mesh. In this section, we will get basic
    Istio service mesh functionality up and running. You will learn how to create
    a service mesh to secure, connect, and monitor microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Service mesh is a very detailed concept and we don't intend to explain any detailed
    use cases. Instead, we will focus on getting our service up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the `https://github.com/istio/istio` repository to your workstation,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We will use the examples in the preceding repository to install Istio on our
    Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Istio using Helm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verifying the installation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an ingress gateway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Istio using Helm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to install Istio:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the Istio CRDs that are required before we can deploy Istio:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Install Istio with the default configuration. This will deploy the Istio core
    components, that is, `istio-citadel`, `istio-galley`, `istio-ingressgateway`,
    `istio-pilot`, `istio-policy`, `istio-sidecar-injector`, and `istio-telemetry`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Enable automatic sidecar injection by labeling the namespace where you will
    run your applications. In this recipe, we will be using the `default` namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: To be able to get Istio functionality for your application, the pods need to
    run an Istio sidecar proxy. The preceding command will automatically inject the
    Istio sidecar. As an alternative, you can find the instructions for manually adding
    Istio sidecars to your pods using the `istioctl` command in the *Installing the
    Istio sidecar instructions* link provided in the *See also* section.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to confirm that Istio has been installed
    successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the number of Istio CRDs that have been created. The following command
    should return `23`, which is the number of CRDs that have been created by Istio:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command and confirm that the list of Istio core component
    services have been created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure that all the pods listed are in the `Running` state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Confirm the Istio injection enabled namespaces. You should only see `istio-injection`
    for the `default` namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: You can always enable injection for the other namespaces by adding the `istio-injection=enabled`
    label to a namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an ingress gateway
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instead of using a controller to load balance traffic, Istio uses a gateway.
    Let''s perform the following steps to create an Istio ingress gateway for our
    example application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Review the content of the `minio.yaml` file in the examples directory in `src/chapter7/lb` and
    deploy it using the following command. This will create a StatefulSet and a service
    where the MinIO port is exposed internally to the cluster via port number `9000`.
    You can also choose to apply the same steps and create an ingress gateway for
    your own application. In that case, skip to *Step 2*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the ingress IP and ports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new Istio gateway:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new `VirtualService` to forward requests to the MinIO instance via
    the gateway. This helps specify routing for the gateway and binds the gateway
    to the `VirtualService`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: This configuration will expose your services to external access using Istio,
    and you will have more control over rules.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe showed you how to quickly configure the Istio service mesh and use
    custom Istio resources such as ingress gateway to open a service to external access.
  prefs: []
  type: TYPE_NORMAL
- en: For the service mesh to function correctly, each pod in the mesh needs to run
    an Envoy sidecar. In the *Installing Istio using Helm* recipe, in *Step 3*, we
    enabled automatic injection for pods in the `default` namespace so that the pods
    that are deployed in that namespace will run the Envoy sidecar.
  prefs: []
  type: TYPE_NORMAL
- en: An ingress controller is a reverse-proxy that runs in the Kubernetes cluster
    and configures routing rules. In the *Creating an ingress gateway* recipe, in
    *Step 2*, unlike traditional Kubernetes ingress objects, we used Istio CRDs such
    as Gateway, VirtualService, and DestinationRule to create the ingress.
  prefs: []
  type: TYPE_NORMAL
- en: 'We created a gateway rule for the ingress Gateway using the `istio: ingressgateway` selector
    in order to accept HTTP traffic on port number `80`.'
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we created a VirtualService for the MinIO services we wanted to
    expose. Since the gateway may be in a different namespace, we used `minio-gateway.default` to
    set the gateway name.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have exposed our service using HTTP. You can read more about exposing
    the service using the HTTPS protocol by looking at the link in *See also* section.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although it is very popular, Istio is not the simplest ingress to deal with.
    We highly recommend that you look at all the options that are available for your
    use case and consider alternatives. Therefore, it is useful to know how to remove
    Istio.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting Istio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can delete Istio by using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: If you want to completely remove the deleted release records from the Helm records
    and free the release name to be used later, add the `--purge` parameter to the
    preceding commands.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Istio documentation: [https://istio.io/docs/](https://istio.io/docs/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Istio examples: [https://istio.io/docs/examples/bookinfo/](https://istio.io/docs/examples/bookinfo/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Installing the Istio sidecar: [https://istio.io/docs/setup/additional-setup/sidecar-injection/](https://istio.io/docs/setup/additional-setup/sidecar-injection/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Istio ingress tutorial from Kelsey Hightower: [https://github.com/kelseyhightower/istio-ingress-tutorial](https://github.com/kelseyhightower/istio-ingress-tutorial)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic management with Istio: [https://istio.io/docs/tasks/traffic-management/](https://istio.io/docs/tasks/traffic-management/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security with Istio: [https://istio.io/docs/tasks/security/](https://istio.io/docs/tasks/security/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy enforcement with Istio: [https://istio.io/docs/tasks/policy-enforcement/](https://istio.io/docs/tasks/policy-enforcement/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting telemetry information with Istio: [https://istio.io/docs/tasks/telemetry/](https://istio.io/docs/tasks/telemetry/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating Kubernetes ingress with Cert-Manager: [https://istio.io/docs/tasks/traffic-management/ingress/ingress-certmgr/](https://istio.io/docs/tasks/traffic-management/ingress/ingress-certmgr/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an ingress service and service mesh using Linkerd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will get basic Linkerd service mesh up and running. You
    will learn how to create a service mesh to secure, connect, and monitor microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Service mesh is a very detailed concept in itself and we don't intend to explain
    any detailed use cases here. Instead, we will focus on getting our service up
    and running.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the example files for this recipe, clone the `k8sdevopscookbook/src` repository
    to your workstation to use the configuration files in the `src/chapter7/linkerd` directory,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: After you've cloned the preceding repository, you can get started with the recipes.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Linkerd CLI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Linkerd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verifying a Linkerd deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viewing the Linkerd metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing the Linkerd CLI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To interact with Linkerd, you need to install the `linkerd` CLI. Follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the `linkerd` CLI by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `linkerd` CLI to your path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that the `linkerd` CLI has been installed by running the following command.
    It should show the server as unavailable since we haven''t installed it yet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Validate that `linkerd` can be installed. This command will check the cluster
    and point to issues if they exist:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: If the status checks are looking good, you can move on to the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Linkerd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to the alternatives, Linkerd is much easier to get started with and
    manage, so it is my preferred service mesh.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the Linkerd control plane using the Linkerd CLI. This command will
    use the default options and install the linkerd components in the `linkerd` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Pulling all the container images may take a minute or so. After that, you can
    verify the health of the components by following the next recipe, *Verifying a
    Linkerd deployment.*
  prefs: []
  type: TYPE_NORMAL
- en: Verifying a Linkerd deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Verifying Linkerd's deployment is as easy as the installation process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to validate the installation. This will display a
    long summary of control plane components and APIs and will make sure you are running
    the latest version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: If the status checks are good, you are ready to test Linkerd with a sample application.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Linkerd to a service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Follow these steps to add Linkerd to our demo application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change directories to the `linkerd` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy the demo application, which uses a mix of gRPC and HTTP calls to service
    a voting application to the user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the service IP of the demo application. This following command will return
    the externally accessible address of your application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the external address from *Step 3* in a web browser and confirm that the
    application is functional:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/d32ad784-1d4e-49cb-a1d3-90a8e88004d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Enable automatic sidecar injection by labeling the namespace where you will
    run your applications. In this recipe, we''re using the `emojivoto` namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: You can also manually inject a `linkerd` sidecar by patching the pods where
    you run your applications using the
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl get -n emojivoto deploy -o yaml | linkerd inject - | kubectl apply
    -f -` command. In this recipe, the `emojivoto` namespace is used.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deleting Linkerd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing the dashboard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can either use port forwarding or use ingress to access the dashboard. Let''s
    start with the simple way of doing things, that is, by port forwarding to your
    local system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'View the Linkerd dashboard by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Visit the following link in your browser to view the dashboard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: The preceding commands will set up a port forward from your local system to
    the `linkerd-web` pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to access the dashboard from an external IP, then follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the sample ingress definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit the ingress configuration in the `ingress-nginx.yaml` file in the `src/chapter7/linkerd`
    directory and change `- host`: `dashboard.example.com` on line 27 to the URL where
    you want your dashboard to be exposed. Apply the configuration using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example file uses `linkerddashboard.containerized.me` as the dashboard
    address. It also protects access with basic auth using `admin/admin` credentials.
    It is highly suggested that you use your own credentials by changing the base64-encoded
    key pair defined in the `auth` section of the configuration using the `username:password`
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting Linkerd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To remove the Linkerd control plane, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: This command will pull a list of all the configuration files for the Linkerd
    control plane, including namespaces, service accounts, and CRDs, and remove them.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linkerd documentation: [https://linkerd.io/2/overview/](https://linkerd.io/2/overview/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common tasks with Linkerd: [https://linkerd.io/2/tasks/](https://linkerd.io/2/tasks/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequently asked Linkerd questions and answers: [https://linkerd.io/2/faq/](https://linkerd.io/2/faq/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auto-healing pods in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes has self-healing capabilities at the cluster level. It restarts containers
    that fail, reschedules pods when nodes die, and even kills containers that don't
    respond to your user-defined health checks.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will perform application and cluster scaling tasks. You
    will learn how to use liveness and readiness probes to monitor container health
    and trigger a restart action in case of failures.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Testing self-healing pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding liveness probes to pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing self-healing pods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this recipe, we will manually remove pods in our deployment to show how
    Kubernetes replaces them. Later, we will learn how to automate this using a user-defined
    health check. Now, let''s test Kubernetes'' self-healing for destroyed pods:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a deployment or StatefulSet with two or more replicas. As an example,
    we will use the MinIO application we used in the previous chapter, in the *Configuring
    and managing S3 object storage using MinIO* recipe. This example has four replicas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'List the MinIO pods that were deployed as part of the StatefulSet. You will
    see four pods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Delete a pod to test Kubernetes'' auto-healing functionality and immediately
    list the pods again. You will see that the terminated pod will be quickly rescheduled
    and deployed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: With this, you have tested Kubernetes' self-healing after manually destroying
    a pod in operation. Now, we will learn how to add a health status check to pods
    to let Kubernetes automatically kill non-responsive pods so that they're restarted.
  prefs: []
  type: TYPE_NORMAL
- en: Adding liveness probes to pods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kubernetes uses liveness probes to find out when to restart a container. Liveness
    can be checked by running a liveness probe command inside the container and validating
    that it returns `0` through TCP socket liveness probes or by sending an HTTP request
    to a specified path. In that case, if the path returns a success code, then kubelet
    will consider the container to be healthy. In this recipe, we will learn how to
    send an HTTP request method to the example application. Let''s perform the following
    steps to add liveness probes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit the `minio.yaml` file in the `src/chapter7/autoheal/minio` directory and
    add the following `livenessProbe` section right under the `volumeMounts` section,
    before `volumeClaimTemplates`. Your YAML manifest should look similar to the following.
    This will send an HTTP request to the `/minio/health/live` location every `20`
    seconds to validate its health:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: For liveness probes that use HTTP requests to work, an application needs to
    expose unauthenticated health check endpoints. In our example, MinIO provides
    this through the `/minio/health/live` endpoint. If your workload doesn't have
    a similar endpoint, you may want to use liveness commands inside your pods to
    verify their health.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy the application. It will create four pods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Confirm the liveness probe by describing one of the pods. You will see a `Liveness`
    description similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'To test the liveness probe, we need to edit the `minio.yaml` file again. This
    time, set the `livenessProbe` port to  `8000`, which is where the application
    will not able to respond to the HTTP request. Repeat *Steps 2* and *3*, redeploy
    the application, and check the events in the pod description. You will see a `minio
    failed liveness probe, will be restarted` message in the events:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'You can confirm the restarts by listing the pods. You will see that every MinIO
    pod is restarted multiple times due to it having a failing liveness status:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: In this recipe, you learned how to implement the auto-healing functionality
    for applications that are running in Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe showed you how to use a liveness probe on your applications running
    on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Adding liveness probes to pods* recipe, in *Step 1*, we added an HTTP
    request-based health check.
  prefs: []
  type: TYPE_NORMAL
- en: By adding the StatefulSet path and port, we let kubelet probe the defined endpoints.
    Here, the `initialDelaySeconds` field tells kubelet that it should wait `120`
    seconds before the first probe. If your application takes a while to get the endpoints
    ready, then make sure that you allow enough time before the first probe; otherwise,
    your pods will be restarted before the endpoints can respond to requests.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 3*, the `periodSeconds` field specifies that kubelet should perform
    a liveness probe every `20` seconds. Again, depending on the applications' expected
    availability, you should set a period that is right for your application.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Configuring liveness and readiness probes: [https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes Best Practices: Setting up health checks:  [https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes](https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing upgrades through blue/green deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The blue-green deployment architecture is a method that's used to reduce downtime
    by running two identical production environments that can be switched between
    when needed. These two environments are identified as blue and green. In this
    section, we will perform rollover application upgrades. You will learn how to
    roll over a new version of your application with persistent storage by using blue/green
    deployment in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we will need a persistent storage provider to take snapshots
    from one version of the application and use clones with the other version of the
    application to keep the persistent volume content. We will use OpenEBS as a persistent
    storage provider, but you can also use any CSI-compatible storage provider.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure OpenEBS has been configured with the cStor storage engine by following
    the instructions in [Chapter 5](22439381-89a7-4cee-8aa1-77c63cb8a014.xhtml), *Preparing
    for Stateful Workload*s, in the *Persistent storage using OpenEBS* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating the blue deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the green deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Switching traffic from blue to green
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the blue deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many traditional workloads that won't work with Kubernetes' way of
    rolling updates. If your workload needs to deploy a new version and cut over to
    it immediately, then you may need to perform blue/green deployment instead. Using
    the blue/green deployment approach, we will label the current production blue.
    In the next recipe, we will create an identical production environment called
    green before redirecting the services to green.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to create the first application, which we
    will call blue:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change directory to where the examples for this recipe are located:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the content of the `blue-percona.yaml` file and use that to create the
    blue version of your application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the content of the `percona-svc.yaml`  file and use that to create the
    service. You will see that `selector` in the service is set to `app: blue`. This
    service will forward all the MySQL traffic to the blue pod:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the service IP for `percona`. In our example, the Cluster IP is `10.3.0.75`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit the `sql-loadgen.yaml` file and replace the target IP address with your
    percona service IP. In our example, it is `10.3.0.75`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the load generator by running the `sql-loadgen.yaml` job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: This job will generate a MySQL load targeting the IP of the service that was
    forwarded to the Percona workload (currently blue).
  prefs: []
  type: TYPE_NORMAL
- en: Creating the green deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to deploy the new version of the application
    as our green deployment. We will switch the service to green, take a snapshot
    of blue''s persistent volume, and deploy the green workload in a new pod:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a snapshot of the data from the blue application''s PVC and use
    it to deploy the green application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the content of the `green-percona.yaml` file and use that to create
    the green version of your application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: This pod will use a snapshot of the PVC from the blue application as its original
    PVC.
  prefs: []
  type: TYPE_NORMAL
- en: Switching traffic from blue to green
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to switch traffic from blue to the new green
    deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit the service using the following command and replace `blue` with `green`.
    Service traffic will be forwarded to the pod that is labeled `green`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: In this recipe, you have learned how to upgrade your application with a stateful
    workload using the blue/green deployment strategy.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Zero Downtime Deployments in Kubernetes with Jenkins: [https://kubernetes.io/blog/2018/04/30/zero-downtime-deployment-kubernetes-jenkins/](https://kubernetes.io/blog/2018/04/30/zero-downtime-deployment-kubernetes-jenkins/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Simple Guide to blue/green Deployment: [https://codefresh.io/kubernetes-tutorial/blue-green-deploy/](https://codefresh.io/kubernetes-tutorial/blue-green-deploy/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes blue-green Deployment Examples: [https://github.com/ContainerSolutions/k8s-deployment-strategies/tree/master/blue-green](https://github.com/ContainerSolutions/k8s-deployment-strategies/tree/master/blue-green)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
