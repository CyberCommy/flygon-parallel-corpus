- en: Logging with Kubernetes
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss cluster logging for Kubernetes clusters. We
    will talk about setting up a cluster to ingest logs, as well as how to view them
    using both self-managed and hosted solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Kubernetes logs locally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing application-specific logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building centralized logging in Kubernetes using the EFK stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging with Kubernetes using Google Stackdriver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a managed Kubernetes logging service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging for your Jenkins CI/CD environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recipes in this chapter expect you to have a functional Kubernetes cluster
    deployed by following one of the recommended methods described in [Chapter 1](a8580410-3e1c-4e28-8d18-aaf9d38d011f.xhtml),
    *Building Production-Ready Kubernetes Clusters*.
  prefs: []
  type: TYPE_NORMAL
- en: The *Logging for your Jenkins CI/CD environment* recipe in this chapter expects
    you to have a functional Jenkins server with an existing CI pipeline created by
    following one of the recommended methods described in [Chapter 3](811c24c7-debf-4487-91e9-81db1520c0aa.xhtml),
    *Building CI/CD Pipelines.*
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes command-line tool `kubectl` will be used for the rest of the
    recipes in this chapter since it's the main command-line interface for running
    commands against Kubernetes clusters. We will also use `helm` where Helm charts
    are available in order to deploy solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Kubernetes logs locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Kubernetes, logs can be used for debugging and monitoring activities to a
    certain level. Basic logging can be used to detect configuration problems, but
    for cluster-level logging, an external backend is required to store and query
    logs. Cluster-level logging will be covered in the *Building centralized logging
    in Kubernetes using the EFK stack* and *Logging Kubernetes using Google Stackdriver*
    recipes.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn how to access basic logs based on the options
    that are available in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clone the `k8sdevopscookbook/src` repository to your workstation to use the
    manifest files in the `chapter10` directory, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing logs through Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging services locally using Telepresence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing logs through Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This recipe will take you through how to access Kubernetes logs and debug services
    locally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to view logs by using the various options
    that are available in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the list of pods running in the `kube-system` namespace. The pods running
    in this namespace, especially `kube-apiserver`, `kube-controller-manager`, `kube-dns`,
    and `kube-scheduler`, play a critical role in the Kubernetes control plane:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'View the logs from a pod with a single container in the `kube-system` namespace.
    In this example, this pod is `kube-apiserver`. Replace the pod''s name and repeat
    this for the other pods as needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding output, you can find the time, source, and a short
    explanation of the event in the logs.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the logs can become long, though most of the time all you need
    is the last few events in the logs. If you don't want to get all the logs since
    you only need the last few events in the log, you can add `-tail` to the end of
    the command, along with the number of lines you want to look at. For example, `kubectl
    logs <podname> -n <namespace> -tail 10` would return the last 10 lines. Change
    the number as needed to limit the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pods can contain multiple containers. When you list the pods, the numbers under
    the `Ready` column show the number of containers inside the pod. Let''s view a
    specific container log from a pod with multiple containers in the `kube-system`
    namespace. Here, the pod we''re looking at is called `kube-dns`. Replace the pod''s
    name and repeat this for any other pods with multiple containers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the logs after a specific time, use the `--since-time` parameter with
    a date, similar to what can be seen in the following code. You can either use
    an absolute time or request a duration. Only the logs after the specified time
    or within the duration will be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of pod names, you can also view logs by label. Here, we''re listing
    pods using the `k8s-app=kube-dns` label. Since the pod contains multiple containers,
    we can use the `-c kubedns` parameter to set the target container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If the container has crashed or restarted, we can use the `-p` flag to retrieve
    logs from a previous instantiation of a container, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now you know how to access pod logs through Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging services locally using Telepresence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a build fails in your CI pipeline or a service running in a staging cluster
    contains a bug, you may need to run the service locally to troubleshoot it properly.
    However, applications depend on other applications and services on the cluster;
    for example, a database. Telepresence helps you run your code locally, as a normal
    local process, and then forwards requests to the Kubernetes cluster. This recipe
    will show you how to debug services locally while running a local Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to view logs through the various options
    that are available in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On **OSX**, install the Telepresence binary using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'On **Windows**, use Ubuntu on the **Windows Subsystem for Linux** (**WSL**).
    Then, on **Ubuntu**, download and install the Telepresence binary using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create a deployment of your application. Here, we''re using a `hello-world`
    example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Expose the service using an external `LoadBalancer` and get the service IP:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To be able to query the address, store the address in a variable using the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Send a query to the service. This will return a `Hello, world!` message similar
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create a local web service and replace the Kubernetes service
    `hello-world` message with the local web server service. First, create a directory
    and a file to be shared using the HTTP server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a web server and expose the service through port `8000` using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will start a proxy using the `vpn-tcp` method. Other methods
    can be found in the *Full list of Telepresence methods* link in the *See also*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: When a service is exposed over the network, remember that your computer is exposed
    to all the risks of running a web server. When you expose a web service using
    the commands described here, make sure that you don't have any important files
    in the `/tmp/local-test` directory that you don't want to expose externally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Send a query to the service. You will see that queries to the `hello-world`
    Service will be forwarded to your local web server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: To end the local service, use the `fg` command to bring the background Telepresence
    job in the current shell environment into the foreground. Then, use the *Ctrl*
    + *C* keys to exit it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you learned how to access logs and debug service problems locally.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Debugging services locally using Telepresence* recipe, in *Step 7*, we
    ran the `telepresence --swap-deployment` command to replace the service with a
    local web service.
  prefs: []
  type: TYPE_NORMAL
- en: Telepresence functions by building a two-way network proxy. The `--swap-deployment`
    flag is used to define the pod that will be replaced with a proxy pod on the cluster.
    Telepresence starts a `vpn-tcp` process to send all requests to the locally exposed
    port, that is, `8000`. The `--run python3 -m http.server 8000 &` flag tells Telepresence
    to run an `http.server` using Python 3 in the background via port `8000`.
  prefs: []
  type: TYPE_NORMAL
- en: In the same recipe, in *Step 9*, the `fg` command is used to move the background
    service to the foreground. When you exit the service, the old pod will be restored.
    You can learn about how Telepresence functions by looking at the *How Telepresence
    works* link in the *See also* section.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`kubectl` log commands: [https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Telepresence source code repository: [https://github.com/telepresenceio/telepresence](https://github.com/telepresenceio/telepresence)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Full list of Telepresence methods: [https://telepresence.io/reference/methods.html](https://telepresence.io/reference/methods.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How Telepresence works: [https://www.telepresence.io/discussion/how-it-works](https://www.telepresence.io/discussion/how-it-works)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How to use volume access support with Telepresence: [https://telepresence.io/howto/volumes.html](https://telepresence.io/howto/volumes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing application-specific logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Kubernetes, pod and deployment logs that are related to how pods and containers
    are scheduled can be accessed through the `kubectl logs` command, but not all
    application logs and commands are exposed through Kubernetes APIs. Getting access
    to these logs and shell commands inside a container may be required.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn how to access a container shell, extract logs,
    and update binaries for troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clone the `k8sdevopscookbook/src` repository to your workstation to use the
    manifest files under the `chapter10` directory, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Make sure you have a Kubernetes cluster ready and `kubectl` configured to manage
    the cluster resources.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting shell access in a container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing PostgreSQL logs inside a container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting shell access in a container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to create a deployment with multiple containers
    and get a shell into running containers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will deploy PostgreSQL on OpenEBS persistent volumes to
    demonstrate shell access. Change the directory to the example files directory
    in `src/chapter10/postgres`, which is where all the YAML manifest for this recipe
    are stored. Create a `ConfigMap` with a database name and credentials similar
    to the following or review them and use the `cm-postgres.yaml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the service for `postgres`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the `postgres.yaml` file and apply it to create the PostgreSQL StatefulSet.
    We can use this to deploy the pods and to auto-create the PV/PVC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the pods with the `postgres` label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Get a shell into the `postgres-0` container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will get you shell access to the running container.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing PostgreSQL logs inside a container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to get the logs from the application running
    inside a container:'
  prefs: []
  type: TYPE_NORMAL
- en: 'While you are in a shell, connect to the PostgreSQL database named `postgresdb`
    using the username `testuser`. You will see the PostgreSQL prompt, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'While on the PostgreSQL prompt, use the following command to create a table
    and add some data to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the log''s configuration details from`postgresql.conf`. You will see that
    the logs are stored in the `/var/log/postgresql` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'List and access the logs in the `/var/log/postgresql` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Optionally, while you''re inside the container, you can create a backup of
    our example `postgresdb` database in the `tmp` directory using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: With that, you have learned how to get shell access into a container and how
    to access the locally stored logs and files inside the container.
  prefs: []
  type: TYPE_NORMAL
- en: Building centralized logging in Kubernetes using the EFK stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As described in the the *Accessing Kubernetes logs locally* section, basic logging
    can be used to detect configuration problems, but for cluster-level logging, an
    external backend is required to store and query logs. A cluster-level logging
    stack can help you quickly sort through and analyze the high volume of production
    log data that's produced by your application in the Kubernetes cluster. One of
    the most popular centralized logging solutions in the Kubernetes ecosystem is
    the **Elasticsearch, Logstash, and Kibana** (**ELK**) stack.
  prefs: []
  type: TYPE_NORMAL
- en: In the ELK stack, Logstash is used as the log collector. Logstash uses slightly
    more memory than Fluent Bit, which is a low-footprint version of Fluentd. Therefore,
    in this recipe, we will use the **Elasticsearch, Fluent-bit, and Kibana** (**EFK**)
    stack. If you have an application that has Logstash dependencies, you can always
    replace Fluentd/Fluent Bit with Logstash.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn how to build a cluster-level logging system using
    the EFK stack to manage Kubernetes logs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clone the `k8sdevopscookbook/src` repository to your workstation to use the
    manifest files in the `chapter10` directory, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section will show you how to configure an EFK stack on your Kubernetes
    cluster. This section is further divided into the following subsections to make
    this process easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Elasticsearch Operator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requesting an Elasticsearch endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Kibana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregating logs with Fluent Bit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing Kubernetes logs on Kibana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Elasticsearch Operator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Elasticsearch is a highly scalable open source full-text search and analytics
    engine. Elasticsearch allows you to store, search, and analyze big volumes of
    data quickly. In this recipe, we will use it to store Kubernetes logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to get **Elastic Cloud on Kubernetes** (**ECK**) deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy Elasticsearch Operator and its CRDs using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Elasticsearch Operator will create its own **CustomResourceDefinition** (**CRD**).
    We will use this CRD later to deploy and manage Elasticsearch instances on Kubernetes.
    List the new CRDs using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new namespace called `logging`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Elasticsearch using the default parameters in the `logging` namespace
    using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the status of the Elasticsearch nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also confirm the pod''s status in the logging namespace using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'A three-node Elasticsearch cluster will be created. By default, the nodes we
    created here are all of the following types: master-eligible, data, and ingest.
    As your Elasticsearch cluster grows, it is recommended to create dedicated master-eligible,
    data, and ingest nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: Requesting the Elasticsearch endpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When an Elasticsearch cluster is created, a default user password is generated
    and stored in a Kubernetes secret. You will need the full credentials to request
    the Elasticsearch endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to request Elasticsearch access:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the password that was generated for the default `elastic` user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Request the Elasticsearch endpoint address:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are accessing the Kubernetes cluster remotely, you can create a port-forwarding
    service and use localhost, similar to what can be seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have access to our three-node small Elasticsearch cluster that we deployed
    on Kubernetes. Next, we need to deploy Kibana to complete the stack.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Kibana
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kibana is an open source data visualization dashboard that lets you visualize
    your Elasticsearch data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to get Kibana deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Kibana instance associated with the Elasticsearch cluster we created
    previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the status of the Kibana node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also confirm the pod''s status in the logging namespace using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: With that, you have both Elasticsearch and Kibana nodes deployed. Next, we will
    deploy fluent-bit to forward container logs to our Elasticsearch deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating logs with Fluent Bit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to get fluent-bit deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the password for the default `elastic` user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy the output of *Step 1* and edit the `fluent-bit-values.yaml` file in the
    `/src/chapter10/efk` directory. Replace the `http_passwd` value with the output
    of *Step 1* and save the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy fluent-bit using the Helm chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Confirm the pod''s status in the `logging` namespace using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: With that, you have deployed all the components of the EFK stack. Next, we will
    connect to the Kibana dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Kubernetes logs on Kibana
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to connect to the Kibana dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Confirm that the Kibana service has been created. By default, a `ClusterIP`
    service will be created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we connect to the dashboard, get the password for the default `elastic`
    user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a port-forwarding service to access the Kibana dashboard from your workstation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the Kibana dashboard at `https://localhost:5601` in your browser. Enter
    `elastic` as the username and the password from the output of *Step 2*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/57f5d206-05cf-4c26-866b-4a4abc8d4224.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the home page, click on the Connect to your Elasticsarch index button, as
    shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/246576ee-7aed-4c0b-9407-9432daf2f360.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Kibana will search for Elasticsearch index patterns. Define the index pattern
    that matches your results. In our example, we used `kubernetes_cluster-*`. Click
    on Next step to continue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/20aad22d-5877-49be-ac1f-732f0ac2ffcb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Specify Time Filter field name as `@timestamp` and click on the Create index
    pattern button, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/a15e4da8-853a-4994-a92e-1cf0dd55d416.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the Discover menu. It is the first icon from the top:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/f3b5d8a6-33bc-433a-a1e8-66c4faae9bb8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the Discover page, use the search field to look for keywords and filters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/f24da1c3-a9e2-4ee4-8fdc-19a587f8b051.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the keyword you are looking for can''t be found in the current time frame,
    you need to change the date range by clicking on the calendar icon next to the
    search field and clicking on the Apply button after the new range has been selected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/841f63e3-cc1f-4751-994d-07a55189c6ac.png)'
  prefs: []
  type: TYPE_IMG
- en: With that, you've learned how to configure an EFK stack on your Kubernetes cluster
    in order to manage and visualize cluster-wide logs.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Elastic Cloud on Kubernetes** (**ECK**): [https://github.com/elastic/cloud-on-k8s](https://github.com/elastic/cloud-on-k8s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment instructions on Red Hat OpenShift: [https://www.elastic.co/guide/en/cloud-on-k8s/0.9/k8s-openshift.html](https://www.elastic.co/guide/en/cloud-on-k8s/0.9/k8s-openshift.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elasticsearch Service documentation: [https://www.elastic.co/guide/en/cloud/current/index.html](https://www.elastic.co/guide/en/cloud/current/index.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Introduction to Kibana: [https://www.elastic.co/guide/en/kibana/7.4/introduction.html#introduction](https://www.elastic.co/guide/en/kibana/7.4/introduction.html#introduction)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fluentd documentation: [https://docs.fluentd.org/](https://docs.fluentd.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fluent Bit documentation: [https://docs.fluentbit.io/manual/](https://docs.fluentbit.io/manual/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rancher Elastic Stack Kubernetes Helm Charts: [https://github.com/rancher/charts/tree/master/charts/efk/v7.3.0](https://github.com/rancher/charts/tree/master/charts/efk/v7.3.0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kudo Elastic Operator: [https://github.com/kudobuilder/operators/tree/master/repository/elastic](https://github.com/kudobuilder/operators/tree/master/repository/elastic)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging Kubernetes using Google Stackdriver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use Google Stackdriver Kubernetes Engine Monitoring
    to monitor, isolate, and diagnose our containerized applications and microservices
    environments. You will learn how to use Stackdriver Kubernetes Engine Monitoring
    to aggregate logs, events, and metrics from your Kubernetes environment on GKE
    to help you understand your application's behavior in production.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Make sure you have a **Google Kubernetes Engine** (**GKE**) cluster ready and `kubectl` configured
    to manage the cluster resources. If you don't have one, you can follow the instructions
    in [Chapter 1](a8580410-3e1c-4e28-8d18-aaf9d38d011f.xhtml),* Building Production-Ready
    Kubernetes Clusters*, in the *Configuring a Kubernetes cluster on Google Cloud
    Platform* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Stackdriver Kubernetes Engine Monitoring support for GKE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring a workspace on Stackdriver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viewing GKE logs using Stackdriver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Stackdriver Kubernetes Engine Monitoring support for GKE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Installing Stackdriver Monitoring support allows you to easily monitor GKE
    clusters, debug logs, and analyze your cluster''s performance using advanced profiling
    and tracing capabilities. In this recipe, we will enable Stackdriver Kubernetes
    Engine Monitoring support to collect cluster metrics from our GKE cluster. Follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the Google Kubernetes Engine Console at [https://console.cloud.google.com/kubernetes](https://console.cloud.google.com/kubernetes).
    On this console, you will see a list of your GKE clusters. Here, we have one cluster
    called `k8s-devops-cookbook-1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/372750bc-333a-4cdd-ab51-d170f9ffa7db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the little pen-shaped Edit icon next to your cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/90e9bfc2-5ff1-40c6-9b78-c64ef32d9a0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the cluster configuration page, make sure that Legacy Stackdriver Logging and Legacy
    Stackdriver Monitoring are Disabled and that the Stackdriver Kubernetes Engine
    Monitoring option is set to Enabled:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/da27f3de-426d-42ce-b046-90f0487cef2d.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on the Save button to apply these changes to your cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Viewing GKE logs using Stackdriver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Enabling Stackdriver Monitoring support allows you to easily monitor GKE clusters,
    debug logs, and analyze your cluster performance using advanced profiling and
    tracing capabilities. In this recipe, we will learn how to access the logs of
    our Kubernetes cluster on GKE. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the Google Cloud Console, open the Stackdriver Logs Viewer by going to [https://console.cloud.google.com/logs/viewer](https://console.cloud.google.com/logs/viewer):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/6dc72aad-e31c-44e4-9f40-34c59a534cf0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the Resources menu, click on the Kubernetes Container option:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/9c8467a1-d497-49a0-85c7-15ef1ba13ebc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Stackdriver Logging view will show a list of logs for your container in
    the selected GKE cluster. Here, you can see the container logs for the last 7
    days being displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/fee97f63-b1f8-4adc-8f7a-e3d9da08d347.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Filter the log level to Critical and set the time frame to the Last 24 hours
    to view the most recent critical container logs. An example result can be seen
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/79df75ae-3657-4cf5-8fa1-0e75f9f33564.png)'
  prefs: []
  type: TYPE_IMG
- en: With that, you know how to use Stackdriver to view logs for GKE clusters and
    resources, such as containers that have been deployed on the GKE clusters.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Google Stackdriver Logging documentation: [https://cloud.google.com/logging/docs](https://cloud.google.com/logging/docs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic query example for Stackdriver: [https://cloud.google.com/logging/docs/view/basic-queries](https://cloud.google.com/logging/docs/view/basic-queries)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QuickStart using logging tools: [https://cloud.google.com/logging/docs/quickstart-sdk](https://cloud.google.com/logging/docs/quickstart-sdk)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stackdriver Logs Router Overview: [https://cloud.google.com/logging/docs/routing/overview](https://cloud.google.com/logging/docs/routing/overview)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a managed Kubernetes logging service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Running an EFK stack to store and maintain Kubernetes logs in your cluster is
    useful until something goes wrong with your cluster. It is recommended that you
    keep your log management system and production cluster separate so that you have
    access in case of cluster failure.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn how to use some of the freely available SaaS
    solutions to keep your cluster logs accessible, even if your cluster is not available.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Make sure you have a Kubernetes cluster ready and `kubectl` configured to manage
    the cluster resources.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding clusters to Director Online
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing logs using Director Online
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting clusters to Director Online
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenEBS Director provides a freely managed EFK stack as a SaaS solution so
    that you can store and manage your Kubernetes cluster logs. In this recipe, we
    will add our Kubernetes cluster to the Director SaaS platform to store our logs
    in the cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to [www.mayadata.io](http://www.mayadata.io) to sign in to your OpenEBS
    Enterprise Platform at [https://portal.mayadata.io/home](https://portal.mayadata.io/home):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/d4a3025e-de06-4cb0-ac6e-da81930d9252.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the Connect your Cluster button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/72a5ebef-10d5-4f86-bb29-ebd844bc5ad7.png)'
  prefs: []
  type: TYPE_IMG
- en: From the main menu, select Clusters and click on the Connect a new Cluster button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Choose your Kubernetes cluster location and name your project. Here, we''ve
    used an AWS cluster and set `AWSCluster` as our cluster name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/7df2b508-c62b-4198-94db-4353fb45f4a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Copy and execute the command on your first cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/94629c7f-7025-4da4-8bb0-b395f86e0c70.png)'
  prefs: []
  type: TYPE_IMG
- en: Shortly after doing this, Director Online will deploy a fluentd forwarder and
    aggregator on your cluster to collect logs on its platform.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing logs using Director Online
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenEBS Director''s free plan stores cluster logs for up to 1 week. Additional
    storage is provided with the premium plan. In this recipe, we will learn how to
    access logs using the managed EFK stack provided by Director Online:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [www.mayadata.io](http://www.mayadata.io) to sign in to your OpenEBS Enterprise
    Platform at [https://portal.mayadata.io/home](https://portal.mayadata.io/home).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the home menu, select Clusters and select your active cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From the left-hand menu, click on Logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/3488a356-52d6-4da6-92a5-d47a83b63887.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A Logs view will open on the Kibana Discover dashboard. Here, you can use search
    for and filter the functionalities of Elasticsearch and Kibana to manage your
    Kubernetes logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/5345e2ed-b058-48c4-b6ea-ef082a5d4e71.png)'
  prefs: []
  type: TYPE_IMG
- en: With that, you've learned how to simply keep logs accessible using managed Kubernetes
    logging solutions. You can use Director Online on multiple clusters and manage
    logs from a single interface.
  prefs: []
  type: TYPE_NORMAL
- en: Logging for your Jenkins CI/CD environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CI/CD pipelines can generate a great amount of metadata every day in busy build
    environments. Elasticsearch is the perfect platform for feeding this kind of data
    from Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn how to enable and access the logs of our Jenkins
    instance and analyze team efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All the operations mentioned in this recipe require a fully functional Jenkins
    deployment, as described in [Chapter 3](811c24c7-debf-4487-91e9-81db1520c0aa.xhtml),
    *Building CI/CD Pipelines*, in the *Setting up a CI/CD pipeline in Jenkins X*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the `k8sdevopscookbook/src` repository to your workstation to use the
    manifest files in the `chapter10` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Make sure you have a Kubernetes cluster, Jenkins X, and an EFK stack ready and that `kubectl` has
    been configured so that you can manage the cluster resources.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section will show you how to feed Jenkins logs to Elasticsearch. This
    section is further divided into the following subsections to make this process
    easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Fluentd plugin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming Jenkins logs to Elasticsearch using Fluentd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing the Fluentd plugin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fluentd is part of the EFK Stack, along with Elasticsearch and Kibana. It is
    an open source data collector for building a unified logging layer. This recipe
    will show you how to install the Fluentd plugin for Jenkins, which will forward
    the Jenkins logs to your Fluentd logger.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to install the Fluentd plugin on Jenkins:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Access your Jenkins service dashboard and click on the Manage Jenkins menu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/2357fc13-1486-437e-acc7-3b3a56faec5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the Manage Jenkins menu, click on the Manage Plugins button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/1685df62-9477-4eac-b329-7ac2c256b0e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the Available tab and search for `fluentd` in the Filter field. The
    result should look similar to the following. Click on the Install without restart button
    to install the Fluentd plugin:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/5ebd0c67-0d9a-4131-988c-7b6d180a3827.png)'
  prefs: []
  type: TYPE_IMG
- en: The Fluentd plugin will be installed without the need to restart the Jenkins
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming Jenkins logs to Elasticsearch using Fluentd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to configure the Fluentd plugin that we installed
    on Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to feed Jenkins logs to Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Manage Jenkins menu, click on the Configure System button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll through the settings. Under the Logger for Fluentd settings, enter a logger
    name. The logger name is used as a prefix for Fluentd. In the Host field, enter
    the Service name of your Fluentd service and the exposed port number. In our example,
    in our Kubernetes cluster, we used the stable/fluentd Helm chart to install Fluentd.
    The service name is `fluentd`. This is exposed via port `24220`. Save the changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/65b41a29-41bc-4a12-adb9-7cb3d3f85038.png)'
  prefs: []
  type: TYPE_IMG
- en: Select a job under the pipeline configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the Add post-build action button and select the Send to Fluentd option
    from the drop-down menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, the Fluentd plugin will push the logs to Elasticsearch through the log
    collector.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are using the ELK stack instead of Fluentd in the EFK stack, then follow
    the recipes given here. This section is further divided into the following subsections
    to make this process easier:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Logstash plugin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming Jenkins logs to Elasticsearch using Logstash
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing the Logstash plugin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logstash is part of the Elastic Stack, along with Beats, Elasticsearch, and
    Kibana. It is an open source data collection engine with real-time pipelining
    capabilities. In this recipe, you will learn how to install the Logstash plugin
    for Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to install the Logstash plugin for Jenkins:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Access your Jenkins service dashboard and click on the Manage Jenkins menu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/2b14f321-b91e-425f-849e-3fd029c91dc6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the Manage Jenkins menu, click on the Manage Plugins button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/1685df62-9477-4eac-b329-7ac2c256b0e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the Available tab and search for `logstash` in the Filter field. The
    result should look similar to the following. Click on the Install without restart
    button to install the Logstash plugin:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/6f810fc9-de04-4ce3-8761-ca8ed7b175dc.png)'
  prefs: []
  type: TYPE_IMG
- en: The Logstash plugin will be installed without you needing to restart your Jenkins
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming Jenkins logs to Elasticsearch using Logstash
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, we will show you how to configure the Logstash plugin that you
    installed on Jenkins previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to feed Jenkins logs to Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Manage Jenkins menu, click on the Configure System button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll through the settings. Under the Logstash settings, check the Enable sending
    logs to an Indexer checkbox. When this setting is enabled, it will open four new
    fields.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the URI field, enter the service name, followed by the indexer name; for
    example, `http://elasticsearch-es-http:9200/logstash/jenkins`. Enter your `elastic`
    username and password and save the changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/ce1927fe-3b98-4144-afa8-ff811fe1565e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, the Logstash plugin will push the logs to Elasticsearch through the log
    collector.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jenkins Logstash plugin documentation: [https://wiki.jenkins.io/display/JENKINS/Logstash+Plugin](https://wiki.jenkins.io/display/JENKINS/Logstash+Plugin)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jenkins FluentD plugin documentation: [https://github.com/jenkinsci/fluentd-plugin](https://github.com/jenkinsci/fluentd-plugin)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debug logging in Jenkins:[ https://wiki.jenkins.io/display/JENKINS/Logging](https://wiki.jenkins.io/display/JENKINS/Logging)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
