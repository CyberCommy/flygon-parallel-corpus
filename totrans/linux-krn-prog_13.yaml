- en: The CPU Scheduler - Part 2
  prefs: []
  type: TYPE_NORMAL
- en: In this, our second chapter on the Linux kernel CPU scheduler, we continue our
    coverage from the previous chapter. In the preceding chapter, we covered several
    key areas regarding the workings (and visualization) of the CPU scheduler on the
    Linux OS. This included topics on what exactly the KSE on Linux is, the POSIX
    scheduling policies that Linux implements, using `perf` to see the scheduler flow,
    and how the design of the modern scheduler is based upon modular scheduling classes. We
    also covered how to query any thread's scheduling policy and priority (using a
    couple of command line utilities), and delved deeper into the internal workings
    of the OS scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this background in place, we''re now ready to explore more on the CPU
    scheduler on Linux; in this chapter, we shall cover the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the flow with LTTng and `trace-cmd`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding, querying, and setting the CPU affinity mask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying and setting a thread's scheduling policy and priority
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU bandwidth control with cgroups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting mainline Linux into an RTOS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latency and its measurement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do expect that you've read (or have the equivalent knowledge of) the previous
    chapter before tackling this one.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I assume you have gone through [Chapter 1](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml), *Kernel
    Workspace Setup*, and have appropriately prepared a guest **Virtual Machine**
    (**VM**) running Ubuntu 18.04 LTS (or a later stable release) and installed all
    the required packages. If not, I highly recommend you do this first.
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book, I strongly recommend you first set up the
    workspace environment, including cloning this book's GitHub repository for the
    code, and work on it in a hands-on fashion. The repository can be found here: [https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming).
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the flow with LTTng and trace-cmd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we saw how we can visualize the flow of threads across
    the processor(s) with `perf` (and a few alternatives). Now, we proceed to do so
    with more powerful, more visual profiling tools: with LTTng (and the Trace Compass
    GUI) and with `trace-cmd` (an Ftrace frontend and the KernelShark GUI).'
  prefs: []
  type: TYPE_NORMAL
- en: Do note that the intent here is to introduce you to these powerful tracing technologies
    only; we do not have the scope nor space required to do full justice to these
    topics.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization with LTTng and Trace Compass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Linux Trace Toolkit Next Generation** (**LTTng**) is a set of open source
    tools enabling you to simultaneously trace both user and kernel space. A bit ironically,
    tracing the kernel is easy, whereas tracing user space (apps, libraries, and even
    scripts) requires the developer to manually insert instrumentation (so-called
    tracepoints) into the application (the tracepoint instrumentation for the kernel
    is supplied by LTTng as kernel modules). The high-quality LTTng documentation
    is available online here: [https://lttng.org/docs/v2.12/](https://lttng.org/docs/v2.12/)
    (covering version 2.12 as of the time of writing).
  prefs: []
  type: TYPE_NORMAL
- en: 'We do not cover the installation of LTTng here; the details are available at [https://lttng.org/docs/v2.12/#doc-installing-lttng](https://lttng.org/docs/v2.12/#doc-installing-lttng).
    Once installed (it''s kind of heavy – on my native x86_64 Ubuntu system, there
    are over 40 kernel modules loaded up pertaining to LTTng!), using LTTng - for
    a system-wide kernel session as we do here - is easy and is performed in two distinct
    stages: recording, followed by data analysis; these steps follow. (As this book
    is focused on kernel development, we don''t cover using LTTng to trace user space
    apps.)'
  prefs: []
  type: TYPE_NORMAL
- en: Recording a kernel tracing session with LTTng
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can record a system-wide kernel tracing session as follows (here, we deliberately
    keep the discussion as simple as possible):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new session and set the output directory to `<dir>` for saving tracing
    metadata:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Simply enable all kernel events (can lead to a large amount of tracing metadata
    being generated though):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Start recording a "kernel session":'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Allow some time to elapse (the longer you trace for, the more the disk space
    that's used by the tracing metadata). During this period, all kernel activity
    is being recorded by LTTng.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stop recording:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Destroy the session; don''t worry, this does not delete the tracing metadata:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: All the preceding commands should be run with admin privileges (or equivalent).
  prefs: []
  type: TYPE_NORMAL
- en: I have a few wrapper scripts to perform tracing with (LTTng, Ftrace, `trace-cmd`)
    at [https://github.com/kaiwan/L5_debug_trg/tree/master/kernel_debug/tracing](https://github.com/kaiwan/L5_debug_trg/tree/master/kernel_debug/tracing);
    do check them out.
  prefs: []
  type: TYPE_NORMAL
- en: The tracing metadata files (in the **Common Trace Format** (**CTF**) file format)
    gets saved to the preceding specified output directory.
  prefs: []
  type: TYPE_NORMAL
- en: Reporting with a GUI – Trace Compass
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data analysis can be performed in two broad ways – using a CLI-based system
    typically packaged along with LTTng called `babeltrace`, or via a sophisticated
    GUI called **Trace Compass**. The GUI is far more appealing; we only show its
    basic usage here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Trace Compass is a powerful cross-platform GUI application and integrates well
    with Eclipse. In fact, we quote directly from the Eclipse Trace Compass site ([https://projects.eclipse.org/projects/tools.tracecompass](https://projects.eclipse.org/projects/tools.tracecompass)):'
  prefs: []
  type: TYPE_NORMAL
- en: '"*Eclipse Trace Compass is an open source application to solve performance
    and reliability issues by reading and analyzing logs or traces of a system. Its
    goal is to provide views, graphs, metrics, and more to help extract useful information
    from traces, in a way that is more user-friendly and informative than huge text
    dumps.*"'
  prefs: []
  type: TYPE_NORMAL
- en: It can be downloaded (and installed) from here: [https://www.eclipse.org/tracecompass/](https://www.eclipse.org/tracecompass/).
  prefs: []
  type: TYPE_NORMAL
- en: Trace Compass minimally requires a **Java Runtime Environment** (**JRE**) to
    be installed as well. I installed one on my Ubuntu 20.04 LTS system with `sudo
    apt install openjdk-14-jre`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once installed, fire up Trace Compass, click on the File | Open Trace menu,
    and navigate to the output directory where you saved the trace metadata for your
    tracing session in the preceding steps. Trace Compass will read the metadata and
    display it visually, along with various perspectives and tool views made available.
    A partial screenshot from our brief system-wide kernel tracing session is shown
    here (*Figure 11.1*); you can literally see the context switch (shown as the `sched_switch` event
    – see the Event type column) from the `gnome-shell` process to the `swapper/1`
    kernel thread (the idle thread running on CPU #1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ac935e2-5441-4d2a-ac44-12283d2fadec.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Trace Compass GUI showing a sample kernel tracing session obtained
    via LTTng
  prefs: []
  type: TYPE_NORMAL
- en: Look carefully at the preceding screenshot (Figure 11.1); in the lower horizontal
    pane, not only do you get to see which kernel function executed, you *also *get
    (under the column labeled Contents) the parameter list along with the value each
    parameter had at the time! This can be very useful indeed.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing with trace-cmd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modern Linux kernels (from 2.6.27) embed a very powerful tracing engine called **Ftrace**.
    Ftrace is the rough kernel equivalent of the user space `strace(1)` utility, but
    that would be short-selling it! Ftrace allows the sysad (or developer, tester,
    or anyone with root privileges really) to literally look under the hood, seeing
    every single function being executed in kernel space, who (which thread) executed
    it, how long it ran for, what APIs it invoked, with interrupts (hard and soft)
    included as they occur, various types of latency measurements, and more. You can
    use Ftrace to learn about how system utilities, applications, and the kernel actually
    work, as well as to perform deep tracing at the level of the OS.
  prefs: []
  type: TYPE_NORMAL
- en: Here, in this book, we refrain from delving into the depths of raw Ftrace usage
    (as it deviates from the subject at hand); instead, it is just quicker and easier
    to use a user space wrapper over Ftrace, a more convenient interface to it, called `trace-cmd(1)` (again,
    we only scratch the surface, showing an example of how `trace-cmd` can be used).
  prefs: []
  type: TYPE_NORMAL
- en: For Ftrace details and usage, the interested reader will find this kernel document
    useful: [https://www.kernel.org/doc/Documentation/trace/ftrace.rst](https://www.kernel.org/doc/Documentation/trace/ftrace.rst).
  prefs: []
  type: TYPE_NORMAL
- en: 'Most modern Linux distros will allow the installation of `trace-cmd` via their
    package management system; on Ubuntu, for example, `sudo apt install trace-cmd` is
    sufficient to install it (if required for a custom Linux on, say, ARM, you can
    always cross-compile it from the source on its GitHub repository: [https://git.kernel.org/pub/scm/linux/kernel/git/rostedt/trace-cmd.git/tree/](https://git.kernel.org/pub/scm/linux/kernel/git/rostedt/trace-cmd.git/tree/)).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's perform a simple `trace-cmd` session; first, we shall record data samples while
    the `ps(1)` utility runs; then we shall examine the captured data both via the
    `trace-cmd report` **Command-Line Interface** (**CLI**) as well as a GUI frontend
    called KernelShark (it's in fact part of the `trace-cmd` package).
  prefs: []
  type: TYPE_NORMAL
- en: Recording a sample session with trace-cmd record
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we record a session with `trace-cmd(1)`; we use a few (of
    the many possible) option switches to `trace-cmd  record`; as usual, the man pages
    on `trace-cmd-foo(1)` (substitute `foo`with `check-events`, `hist`, `record`,
    `report`, `reset`, and so on) are very useful for finding various option switches
    and usage details. A few of the useful option switches particularly for `trace-cmd
    record` are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-o`: Specifies the output filename (if not specified, it defaults to `trace.dat`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-p`: The plugin to use, one of `function`, `function_graph`, `preemptirqsoff`,
    `irqsoff`, `preemptoff`, and `wakeup`; here, in our small demo, we use the `function-graph` plugin
    (several other plugins can be configured in the kernel as well).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-F`: The command (or app) to trace; this is very useful, allowing you to specify
    exactly which process (or thread) to exclusively trace (otherwise, tracing all
    threads can result in a lot of noise when attempting to decipher the output);
    similarly, you can use the `-P` option switch to specify the PID to trace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-r priority`: Runs the `trace-cmd` threads at the real-time priority specified
    (the typical range being 1 to 99; we shall cover querying and setting a thread''s
    scheduling policy and priority shortly); this gives a better bet on `trace-cmd`
    being able to capture samples as required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, we run a quick demo: we run `ps -LA`; while it runs, all kernel traffic
    it generates is (exclusively) captured by `trace-cmd` via it''s `record` functionality
    (we employ the `function-graph` plugin):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: A rather large data file results (as we captured all events and did a `ps -LA` displaying
    all threads alive, it took a while, and thus the data samples captured are large-ish.
    Also realize that by default, kernel tracing is performed across all CPUs on the
    system; you can change this via the `-M cpumask` option.)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example, we captured all events. The `-e` option switch to
    `trace-cmd(1)` allows you to specify a class of events to trace; for example,
    to trace the `ping(1)` utility and capture only events related to networking and
    kernel memory, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo trace-cmd record -e kmem -e net -p function_graph -F ping -c1 packtpub.com`.'
  prefs: []
  type: TYPE_NORMAL
- en: Reporting and interpretation with trace-cmd report (CLI)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Continuing from the preceding section, on the command line, we can get a (very!)
    detailed report of what occurred within the kernel when the `ps` process ran;
    use the `trace-cmd report` command to see this. We also pass along the `-l` option
    switch: it displays the report in what is referred to as Ftrace''s **latency format**, revealing
    many useful details; the `-i` switch of course specifies the input file to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it gets very interesting! We show a few partial screenshots of the (huge)
    output file that we opened with `vim(1)`; first we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d48a8129-fe10-4ecf-bebe-95c94684e5bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – A partial screenshot showing the output of the trace-cmd report
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at Figure 11.2; the call to the kernel API, `schedule()`, is deliberately
    highlighted and in bold font (*Figure 11.2*, on line `785303`!). In order to interpret
    everything on this line, we must understand each (white-space delimited) column;
    there are eight of them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Column 1: Here, it''s just the line number in the file that vim shows (let''s
    ignore it).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column 2: This is the process context that invoked this function (the function
    itself is in column #8); clearly, here, the process is `ps-PID` (its PID is appended
    after a `-` character).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column 3: useful! A series of five characters, which shows up in **latency
    format** (we used the `-l` option switch to `trace-cmd record`, remember!); this
    (in our preceding case, it''s `2.N..`) is very useful and can be interpreted as
    follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The very first character is the CPU core it was running upon (so here it was
    core #2) (note that, as a general rule, besides the first one, if the character
    is a period `.`, it means it''s zero or not applicable).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second character represents the hardware interrupt status:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.` implies the default hardware interrupts are enabled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`d` implies hardware interrupts are currently disabled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The third character represents the `need_resched` bit (we explained this in
    the previous chapter, in the *When does the scheduler run?* section):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.` implies it''s cleared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`N` implies it''s set (which implies that the kernel requires rescheduling
    to be performed ASAP!).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The fourth character has meaning only when an interrupt is in progress, otherwise,
    it is merely a `.`, implying we are in a process context; if an interrupt is in
    progress – implying we''re in an interrupt context – its value is one of the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`h` implies we are executing in a hardirq (or top half) interrupt context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`H` implies we are executing in a hardirq that occurred within a softirq.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s` implies we are executing in a softirq (or bottom half) interrupt context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fifth character represents the preemption count or depth; if it's a `.`,
    it's zero, implying the kernel is running in a preemptible state; if nonzero,
    an integer number shows up, implying that many kernel-level lock(s) have been
    taken, forcing the kernel into a non-preemptible state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By the way, the output is very similar to Ftrace''s raw output except that
    in the case of raw Ftrace, we would see only four characters – the first one (the
    CPU core number) does *not* show up here; it shows up as the leftmost column instead;
    here''s a partial screenshot of the raw Ftrace (not `trace-cmd`) latency format:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/eacd0313-28f8-4d88-92ae-6ad938231293.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – A partial screenshot focused on raw Ftrace's four-character latency
    format (fourth field)
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot was culled directly from the raw Ftrace output.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, interpreting our example for the call to `schedule()`, we can see that
    the characters are `2.N..` implying that the process `ps` with PID `22922` was
    executing on CPU core #2 in a process context (no interrupts) and the `need-resched`
    (technically, `thread_info.flags:TIF_NEED_RESCHED`) bit was set (indicating the
    need for a reschedule ASAP!).'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: (Back to the remaining columns in Figure 11.2 now)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column 4: Timestamp in *seconds:microseconds* format.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Column 5: The name of the event that occurred (here, as we''ve used the `function_graph` plugin,
    it will be either `funcgraph_entry` or `fungraph_exit`, implying function entry
    or exit respectively).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column 6 [optional]: The duration of the preceding function call with the time
    taken shown along with its unit (us = microseconds); a prefix character is used
    to denote whether the function execution took a long time (we simply treat it
    as part of this column); from the kernel Ftrace documentation (here: [https://www.kernel.org/doc/Documentation/trace/ftrace.rst](https://www.kernel.org/doc/Documentation/trace/ftrace.rst)),
    we have this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`+`, which implies that a function surpassed 10 microseconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`!`, which implies that a function surpassed 100 microseconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#`, which implies that a function surpassed 1,000 microseconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*`, which implies that a function surpassed 10 milliseconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@`, which implies that a function surpassed 100 milliseconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$`, which implies that a function surpassed 1 second'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column 7: Just the separator character `|`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column 8: The extreme-right column is the name of the kernel function being
    executed; an open brace on the right, `{`, implies the function is invoked just
    now; the column with only a close brace, `}`, implies the preceding function''s
    end (matching the open brace).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This level of detail can be extremely valuable in both troubleshooting kernel
    (and even user space) issues, and understanding the flow of the kernel in great
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'When `trace-cmd record` is used without the `-p function-graph` option switch,
    we do lose the nicely indented function call graph-like output, but we do gain
    something as well: you will now see all function parameters along with their runtime
    values to the right of every single function call! A truly valuable aid at times.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I can''t resist showing another snippet from the same report – another interesting
    example with regard to the very things we learned about how scheduling classes
    work on modern Linux (covered in the previous chapter); this actually shows up
    here in the `trace-cmd` output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dcdea06f-b228-4418-acb3-399effdf5053.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – A partial screenshot of trace-cmd report output
  prefs: []
  type: TYPE_NORMAL
- en: 'Interpret the preceding screenshot (*Figure 11.4*) closely: the second line
    (with the right-most function name column in bold font, as are the two functions
    immediately following it) shows that the `pick_next_task_stop()` function was
    invoked; this implies that a schedule occurred and the core scheduling code within
    the kernel went through its routine – it walks the linked list of scheduling classes
    in priority order, asking each whether it has a thread to schedule; if they do,
    the core scheduler context switches to it (as was explained in some detail in the
    previous chapter, in the *Modular scheduling classes* section).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure 11.4, you literally see this happen: the core scheduling code asks
    the **stop-sched** (**SS**), **deadline** (**DL**), and **real-time** (**RT**)
    classes whether they have any thread that wants to run, by invoking, in turn,
    the `pick_next_task_stop()`, `pick_next_task_dl()`, and `pick_next_task_rt()` functions.
    Apparently, for all of them, the answer is no, as the next function to run is
    that of the fair (CFS) class (why doesn''t the `pick_next_task_fair()` function
    show up in the preceding screenshot then? Ah, again, that''s code optimization
    for you: the kernel developers understand that this being the likely case, they
    check for it and directly invoke the fair class code most of the time).'
  prefs: []
  type: TYPE_NORMAL
- en: What we've covered here on the powerful Ftrace framework and the `trace-cmd`
    utility is just the basics; I urge you to look up the man pages on `trace-cmd-<foo>`(where
    `<foo>` is replaced with `record`, `report`, and so on)there are typically good
    examples shown there. Also, there are several very well-written articles on Ftrace
    (and `trace-cmd`) – please refer to the *Further reading *section for them.
  prefs: []
  type: TYPE_NORMAL
- en: Reporting and interpretation with a GUI frontend
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More good news: the `trace-cmd` toolset includes a GUI frontend, for more human-friendly
    interpretation and analysis, called KernelShark (though, in my opinion, it isn''t
    as full-featured as Trace Compass is). Installing it on Ubuntu/Debian is as simple
    as doing `sudo apt install kernelshark`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Below, we run `kernelshark`, passing the trace data file output from our preceding `trace-cmd` record
    session as the parameter to it (adjust the parameter to KernelShark to refer to
    the location where you''ve saved the tracing metadata):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'A screenshot of KernelShark running with the preceding trace data is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1406860-f2ad-4331-a266-5727a90114de.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – A screenshot of the kernelshark GUI displaying the earlier-captured
    data via trace-cmd
  prefs: []
  type: TYPE_NORMAL
- en: 'Interesting; the `ps` process ran on CPU #2 (as we saw with the CLI version
    previously). Here, we also see the functions executed in the lower tiled horizontal
    window pane; as an example, we have highlighted the entry for `pick_next_task_fair()`.
    The columns are quite obvious, with the `Latency` column format (four characters,
    not five) interpreted as we explained previously for (raw) Ftrace.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quick quiz**: What does the Latency format field `dN..`, seen in Figure 11.5, imply?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer: It implies that, currently, right now, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First column `d`: Hardware interrupts are disabled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second column `N`: The `need_resched` bit is set (implying the need to invoke
    the scheduler at the next available scheduling opportunity point).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Third column `.`: The kernel `pick_next_task_fair()` function''s code is running
    in a process context (the task being `ps` with a PID of `22545`; remember, Linux
    is a monolithic kernel!).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fourth column `.`: The preemption depth (count) is zero, implying the kernel
    is in a preemptible state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have covered using these powerful tools to help generate and visualize
    data related to kernel execution and scheduling, let''s move on to another area:
    in the next section, we focus on another important aspect – what exactly a thread''s
    CPU affinity mask is, and how you can programmatically (and otherwise) get/set
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding, querying, and setting the CPU affinity mask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The task structure, the root data structure containing several dozen thread
    attributes, has a few attributes directly pertaining to scheduling: the priority
    (the *nice* as well as the RT priority values), the scheduling class structure
    pointer, the runqueue the thread is on (if any), and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Among these is an important member, the **CPU affinity bitmask** (the actual
    structure member is `cpumask_t cpus_allowed`). This also tells you that the CPU
    affinity bitmask is a per-thread quantity; this makes sense - the KSE on Linux
    is a thread, after all. It's essentially an array of bits, each bit representing
    a CPU core (with sufficient bits available within the variable); if the bit corresponding
    to a core is set (`1`), the thread is allowed to be scheduled on and execute on
    that core; if cleared (`0`), it's not.
  prefs: []
  type: TYPE_NORMAL
- en: By default, all the CPU affinity mask bits are set; thus, the thread can run
    on any core. For example, on a box with (the OS seeing) four CPU cores, the default
    CPU affinity bitmask for each thread would be binary `1111` (`0xf`). (Glance at
    Figure 11.6 to see how the CPU affinity bitmask looks, conceptually speaking.)
  prefs: []
  type: TYPE_NORMAL
- en: 'At runtime, the scheduler decides which core the thread will actually run upon.
    In fact, think about it, it''s really implicit: by default, each CPU core has
    a runqueue associated with it; every runnable thread will be on a single CPU runqueue;
    it''s thus eligible to run and by default runs on the CPU that it''s runqueue
    represents. Of course, the scheduler has a load balancer component that can migrate
    threads to other CPU cores (runqueues, really) as the need arises (kernel threads
    called `migration/n`, where `n` is the core number assist in this task).'
  prefs: []
  type: TYPE_NORMAL
- en: The kernel does expose APIs to user space (system calls, of course, `sched_{s,g}etaffinity(2)` and
    their `pthread` wrapper library APIs), which allows an application to affine,
    or associate, a thread (or multiple threads) to particular CPU cores as it sees
    fit (and by the same logic, we can do this within the kernel as well for any given
    kernel thread). For example, setting the CPU affinity mask to `1010` binary, which
    equals `0xa` in hexadecimal, implies that the thread can execute *only* upon CPU
    cores one and three (counting starts from zero).
  prefs: []
  type: TYPE_NORMAL
- en: 'A key point: though you can manipulate the CPU affinity mask, the recommendation
    is to avoid doing so; the kernel scheduler understands the CPU topography in detail
    and can best load-balance the system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having said that, explicitly setting the CPU affinity mask of a thread can
    be beneficial due to the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Cache invalidation (and thus unpleasant cache "bouncing") can be greatly reduced
    by ensuring a thread always runs on the same CPU core.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thread migration costs between cores are effectively eliminated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU reservation—a strategy to bestow the core(s) exclusively to one thread by
    guaranteeing all other threads are explicitly not allowed to execute upon that
    core.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two are useful in some corner cases; the third one, CPU reservation,
    tends to be a technique used in some time-critical real-time systems where the
    cost of doing so is justified. Performing CPU reservation in practice is quite
    difficult to do though,  requiring OS-level intervention at (every!) thread creation;
    the cost might be prohibitive. For this reason, this is actually implemented by
    specifying that a certain CPU (or more) be *isolated* from all tasks; the Linux
    kernel provides a kernel parameter, `isolcpus`, for this very job.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this regard, we quote directly from the man page on the `sched_{s,g}etaffinity(2)` system
    calls:'
  prefs: []
  type: TYPE_NORMAL
- en: The isolcpus boot option can be used to isolate one or more CPUs at boot time,
    so that no processes are scheduled onto those CPUs. Following the use of this
    boot option, the only way to schedule processes onto the isolated CPUs is via sched_setaffinity() or
    the cpuset(7) mechanism. For further information, see the kernel source file Documentation/admin-guide/kernel-parameters.txt.
    As noted in that file, isolcpus is the preferred mechanism of isolating CPUs (versus
    the alternative of manually setting the CPU affinity of all processes on the system).
  prefs: []
  type: TYPE_NORMAL
- en: Note, though, the previously mentioned `isolcpus` kernel parameter is now considered
    deprecated; it's preferable to use the cgroups `cpusets` controller instead (`cpusets`
    is a cgroup feature or controller; we do have some coverage on cgroups later in
    this chapter, in the *CPU bandwidth control with cgroups* section).
  prefs: []
  type: TYPE_NORMAL
- en: We refer you to more details in the kernel parameter documentation (here: [https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt](https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt)),
    specifically under the parameter labeled `isolcpus=`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand the theory behind it, let's actually write a user space
    C program to query and/or set the CPU affinity mask of any given thread.
  prefs: []
  type: TYPE_NORMAL
- en: Querying and setting a thread's CPU affinity mask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a demonstration, we provide a small user space C program to query and set
    a user space process (or thread''s) CPU affinity mask. Querying the CPU affinity
    mask is achieved with the `sched_getaffinity(2)` system call and by setting it
    with its counterpart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'A specialized data type called `cpu_set_t` is what is used to represent the
    CPU affinity bitmask; it''s quite sophisticated: its size is dynamically allocated
    based on the number of CPU cores seen on the system. This CPU mask (of type `cpu_set_t`)
    must first be initialized to zero; the `CPU_ZERO()` macro achieves this (several
    similar helper macros exist; do refer to the man page on `CPU_SET(3)`). The second
    parameter in both the preceding system calls is the size of the CPU set (we simply
    use the `sizeof` operator to obtain it).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand this better, it''s instructive to see a sample run of our code
    (`ch11/cpu_affinity/userspc_cpuaffinity.c`); we run it on a native Linux system
    with 12 CPU cores:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b138bc28-5b26-4aa3-9362-ea201aaa2598.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Our demo user space app showing the CPU affinity mask
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have run the app with no parameters. In this mode, it queries the
    CPU affinity mask of itself (meaning, of the `userspc_cpuaffinity` calling process).
    We print out the bits of the bitmask: as you can clearly see in the preceding
    screenshot, it''s binary `1111 1111 1111` (which is equivalent to `0xfff`), implying
    that by default the process is eligible to run on any of the 12 CPU cores available
    on the system.'
  prefs: []
  type: TYPE_NORMAL
- en: The app detects the number of CPU cores available by running the `nproc(1)`
    utility via the useful `popen(3)` library API. Do note though, that the value
    returned by `nproc` is the number of CPU cores available to the calling process;
    it may be less than the actual number of CPU cores (it's usually the same); the
    number of available cores can be changed in a few ways, the proper way being via
    the cgroup `cpuset` resource controller (we cover some information on cgroups
    later in this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'The querying code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Our `disp_cpumask()` function draws the bitmask (we leave it to you to check
    it out).
  prefs: []
  type: TYPE_NORMAL
- en: If additional parameters are passed – the PID of the process (or thread), as
    the first parameter, and a CPU bitmask, as the second parameter – we then attempt
    to *set* the CPU affinity mask of that process (or thread) to the value passed.
    Of course, changing the CPU affinity bitmask requires you to own the process or
    have root privileges (more correctly, to have the `CAP_SYS_NICE` capability).
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick demo: in Figure 11.7, `nproc(1)` shows us the number of CPU cores;
    then, we run our app to query and set our shell process''s CPU affinity mask.
    On a laptop, let''s say that the affinity mask of `bash` is `0xfff` (binary `1111
    1111 1111`) to begin with, as expected; we change it to `0xdae` (binary `1101
    1010 1110`) and query it again to verify the change:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c6f31dd-946f-4559-9c3a-9c8de84d45f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Our demo app queries then sets the CPU affinity mask of bash to
    0xdae
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, this is interesting: to begin with, the app correctly detects the number
    of CPU cores available to it as 12; it then queries the (default) CPU affinity
    mask of the bash process (as we pass its PID as the first parameter); it shows
    up, as `0xfff`, as expected. Then, as we''ve also passed a second parameter –
    the bitmask to now set (`0xdae`) – it does so, setting the CPU affinity mask of
    bash to `0xdae`. Now, as the terminal window we''re on is this very same bash
    process, running `nproc` again shows the value as 8, not 12! That''s indeed correct:
    the bash process now has only eight CPU cores available to it. (This is as we
    don''t revert the CPU affinity mask to its original value on exit.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the relevant code to set the CPU affinity mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, you can see we first set up the `cpu_set_t` bitmask
    appropriately (by looping over each bit) and then employ the `sched_setaffinity(2)`
    system call to set the new CPU affinity mask on the given `pid`.
  prefs: []
  type: TYPE_NORMAL
- en: Using taskset(1) to perform CPU affinity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Akin to how (in the preceding chapter) we used the convenient user space utility
    program, `chrt(1)` to get (or set) a process'' (or thread''s) scheduling policy
    and/or priority, you can use the user space `taskset(1)` utility to get and/or
    set a given process'' (or thread''s) CPU affinity mask. A couple of quick examples
    follow; note that these examples were run on an x86_64 Linux system with 4 CPU
    cores:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `taskset` to query the CPU affinity mask of systemd (PID 1):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `taskset` to ensure that the compiler – and its descendants (the assembler
    and linker) – run only on the first two CPU cores; the first parameter to taskset
    is the CPU affinity bitmask (`03` is binary `0011`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Do look up the man page on `taskset(1)` for complete usage details.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the CPU affinity mask on a kernel thread
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As an example, if we want to demonstrate a synchronization technique called
    per-CPU variables, we are required to create two kernel threads and guarantee
    that each of them runs on a separate CPU core. To do so, we must set the CPU affinity
    mask of each kernel thread (the first one to `0`, the second to `1`, in order
    to have them execute on only CPUs `0` and `1` respectively). The thing is, it''s
    not a clean job – quite a *hack,* to be honest, and definitely *not* recommended.
    The following comment from that code shows why:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Later, we invoke the function pointer, in effect invoking the `sched_setaffinity` code,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Unconventional and controversial; it does work, but please avoid hacks like
    this in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you know how to get/set a thread''s CPU affinity mask, let''s move
    on to the next logical step: how to get/set a thread''s scheduling policy and
    priority! The next section delves into the details.'
  prefs: []
  type: TYPE_NORMAL
- en: Querying and setting a thread’s scheduling policy and priority
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 10](5391e3c1-30ad-4c75-a106-301259064881.xhtml), *The CPU Scheduler
    – Part 1*, in the *Threads – which scheduling policy and priority* section, you
    learned how to query the scheduling policy and priority of any given thread via
    `chrt(1)` (we also demonstrated a simple bash script to do so). There, we mentioned
    the fact that `chrt(1)` internally invokes the `sched_getattr(2)` system call
    in order to query these attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Very similarly, setting the scheduling policy and priority can be performed
    either by using the `chrt(1)` utility (making it simple to do so within a script,
    for example), or programmatically within a (user space) C application with the `sched_setattr(2)` system
    call. In addition, the kernel exposes other APIs: `sched_{g,s}etscheduler(2)` and
    its `pthread` library wrapper APIs, `pthread_{g,s}etschedparam(3)` (as these are
    all user space APIs, we leave it to you to browse through their man pages to get
    the details and try them out for yourself).'
  prefs: []
  type: TYPE_NORMAL
- en: Within the kernel – on a kernel thread
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you know by now, the kernel is most certainly not a process nor a thread.
    Having said that, the kernel does contain kernel threads; like their user space
    counterparts, kernel threads can be created as required (from within the core
    kernel, a device driver, a kernel module). They *are* schedulable entities (KSEs!)
    and, of course, each of them has a task structure; thus, their scheduling policy
    and priority can be queried or set as required..
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to the point at hand: to set the scheduling policy and/or priority of a
    kernel thread, the kernel typically makes use of the `kernel/sched/core.c:sched_setscheduler_nocheck()` (GFP
    exported) kernel API; here, we show its signature and an example of its typical
    usage; the comments that follow make it quite self-explanatory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'One good example of the kernel''s usage of kernel threads is when the kernel
    (quite commonly) uses threaded interrupts. Here, the kernel must create a dedicated
    kernel thread with the `SCHED_FIFO` (soft) real-time scheduling policy and a real-time
    priority value of `50` (halfway between), for interrupt handling purposes. The
    (relevant) code to do this is shown here as an example of setting scheduling policy
    and priority on a kernel thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: (Here, we don't show the code that creates the kernel thread via the `kthread_create()`
    API. Also, FYI, `MAX_USER_RT_PRIO` is the value `100`.)
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand to a good extent how CPU scheduling works at the level
    of the OS, we'll move on to yet another quite compelling discussion – that of
    cgroups; read on!
  prefs: []
  type: TYPE_NORMAL
- en: CPU bandwidth control with cgroups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the hazy past, the kernel community struggled mightily with a rather vexing
    issue: though scheduling algorithms and their implementations – the early 2.6.0
    O(1) scheduler, and a little later (with 2.6.23), the **Completely Fair Scheduler**
    (**CFS**) – promised, well, completely fair scheduling, it really wasn''t. Think
    about this for a moment: let''s say you are logged into a Linux server along with
    nine other people. Everything else being equal, it is likely that processor time
    is (more or less) fairly shared between all ten  people; of course, you will understand
    that it''s not really people that run, it''s processes and threads that run on
    their behalf.'
  prefs: []
  type: TYPE_NORMAL
- en: For now at least, let's assume it's mostly fairly shared. But, what if you write
    a user space program that, in a loop, indiscriminately spawns off several new
    threads, each of which perform a lot of CPU-intensive work (and perhaps as an
    added bonus, allocates large swathes of memory as well; a file (un)compressor
    app perhaps) in each loop iteration!? The CPU bandwidth allocation is no longer
    fair in any real sense of the term, your account will effectively hog the CPUs
    (and perhaps other system resources, such as memory, as well)!
  prefs: []
  type: TYPE_NORMAL
- en: 'A solution that precisely and effectively allocated and managed CPU (and other
    resource) bandwidth was required; ultimately, Google engineers obliged with patches
    that put the modern-day cgroups solution into the Linux kernel (in version 2.6.24).
    In a nutshell, cgroups is a kernel feature that allows the system administrator
    (or anyone with root access) to perform bandwidth allocation and fine-grained
    resource management on the various resources (or *controllers*, as they are called
    in the cgroup lexicon) on a system. Do note: using cgroups, it''s not just the
    processors (CPU bandwidth), but also memory, network, block I/O (and more)  bandwidth
    that can be carefully allocated and monitored as required by your project or product.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, hey, you''re interested now! How do you enable this cgroups feature? Simple
    – it''s a kernel feature you enable (or disable) at quite a fine granularity in
    the usual way: by configuring the kernel! The relevant menu (via the convenient
    `make menuconfig` interface) is `General setup / Control Group support`. Try this:
    `grep` your kernel config file for `CGROUP`; if required, tweak your kernel config,
    rebuild, reboot with the new kernel, and test. (We covered kernel configuration
    in detail back in [Chapter 2](e0b89a37-18a3-424d-8983-58c4ac0725f6.xhtml), *Building
    the 5.x Linux Kernel from Source – Part 1*, and the kernel build and install in
    [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml), *Building the 5.x Linux
    Kernel from Source – Part 2.*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Good news: cgroups is enabled by default on any (recent enough) Linux system
    that runs the systemd init framework. As mentioned just now, you can query the
    cgroup controllers enabled by `grep`-ping your kernel config file, and modify
    the config as desired.'
  prefs: []
  type: TYPE_NORMAL
- en: From it's initiation in 2.6.24, cgroups, like all other kernel features, continually
    evolves. Fairly recently, a point was reached where sufficiently improved cgroup
    features became incompatible with the old, resulting in a new cgroup release,
    one christened cgroups v2 (or simply cgroups2); this was declared production-ready
    in the 4.5 kernel series (with the older one now referred to as cgroups v1 or
    as the legacy cgroups implementation). Note that, as of the time of this writing,
    both can and do exist together (with some limitations; many applications and frameworks
    still use the older cgroups v1 and are yet to migrate to v2).
  prefs: []
  type: TYPE_NORMAL
- en: A detailed rationale of why to use cgroups v2 as opposed to cgroups v1 can be
    found within the kernel documentation here: [https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#issues-with-v1-and-rationales-for-v2](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#issues-with-v1-and-rationales-for-v2)
  prefs: []
  type: TYPE_NORMAL
- en: 'The man page on `cgroups(7)` describes in some detail the interfaces and various
    available (resource) controllers (or *subsystems* as they are sometimes referred
    to); for cgroups v1, they are `cpu`, `cpuacct`, `cpuset`, `memory`, `devices`,
    `freezer`, `net_cls`, `blkio`, `perf_event`, `net_prio`, `hugetlb`, `pids`, and
    `rdma`. We refer interested readers to said man page for details; as an example,
    the PIDS controller is very useful in preventing fork bombs (often, a silly but
    nevertheless deadly DoS attack where the `fork(2)` system call is issued within
    an infinite loop!), allowing you to limit the number of processes that can be
    forked off from that cgroup (or its descendants). On a Linux box with cgroups
    v1 running, peek at the content of `/proc/cgroups`: it reveals the v1 controllers
    available and their current usage.'
  prefs: []
  type: TYPE_NORMAL
- en: Control groups are exposed via a purpose-built synthetic (pseudo) filesystem,
    typically mounted under `/sys/fs/cgroup`. In cgroups v2, all controllers are mounted
    in a single hierarchy (or tree). This is unlike cgroups v1, where multiple controllers
    could be mounted under multiple hierarchies or groups. The modern init framework, *systemd,* is
    a user of both the v1 and v2 cgroups. The `cgroups(7)` man page indeed mentions
    the fact that `systemd(1)` auto-mounts a cgroups v2 filesystem during startup
    (at `/sys/fs/cgroup/unified`).
  prefs: []
  type: TYPE_NORMAL
- en: In cgroups v2, these are the supported controllers (or resource limiters or
    subsystems, if you will): `cpu`, `cpuset`, `io`, `memory`, `pids`, `perf_event`,
    and `rdma` (the first five being commonly deployed).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, the focus is on CPU scheduling; thus, we do not delve further
    into other controllers, but limit our discussions to an example of using the cgroups
    v2 `cpu` controller to limit CPU bandwidth allocation. For more on employing the
    other controllers, we refer you to the resources mentioned previously (along with
    several more found in the *Further reading* section of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Looking up cgroups v2 on a Linux system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let''s look up the available v2 controllers; to do so, locate the cgroups
    v2 mount point; it''s usually here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Hey, there aren''t any controllers present in `cgroup2`!? Actually, it will
    be this way in the presence of *mixed* cgroups, v1 and v2, which is the default
    (as of the time of writing). To exclusively make use of the later version – and
    thus have all configured controllers visible – you must first disable cgroups
    v1 by passing this kernel command-line parameter at boot: `cgroup_no_v1=all` (recall,
    all available kernel parameters can be conveniently seen here: [https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt](https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After rebooting the system with the preceding option, you can check that the
    kernel parameters you specified (via GRUB on an x86, or perhaps via U-Boot on
    an embedded system) have indeed been parsed by the kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Okay; now let''s retry looking up the `cgroup2` controllers; you should find
    that it''s typically mounted under `/sys/fs/cgroup/` - the `unified` folder is
    no longer present (now that we''ve booted with the `cgroup_no_v1=all` parameter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Ah, now we see them (the exact controllers you see depend on how the kernel's
    configured).
  prefs: []
  type: TYPE_NORMAL
- en: 'The rules governing the working of cgroups2 is beyond this book''s scope; if
    you''d like to, I suggest you read through it here: [https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#control-group-v2](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#control-group-v2).
    Also, all the `cgroup.<foo>` pseudo files under a cgroup are described in detail
    in the *Core Interface Files* section ([https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#core-interface-files](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#core-interface-files)). Similar
    information is presented, in a simpler way, within the excellent man page on `cgroups(7)` (look
    it up with `man 7 cgroups` on Ubuntu).'
  prefs: []
  type: TYPE_NORMAL
- en: Trying it out – a cgroups v2 CPU controller
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s try something interesting: we shall create a new sub-group under the
    cgroups v2 hierarchy on the system. We''ll then set up a CPU controller for it,
    run a couple of test processes (that hammer away on the system''s CPU cores),
    and set a user-specified upper limit on how much CPU bandwidth these processes
    can actually make use of!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we outline the steps you will typically take to do this (all of these
    steps require you to be running with root access):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure your kernel supports cgroups v2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should be running on a 4.5 or later kernel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the presence of mixed cgroups (both legacy v1 and newer v2, which, as of
    the time of writing, is the default), check that your kernel command line includes
    the `cgroup_no_v1=all` string. Here, we shall assume that the cgroup v2 hierarchy
    is supported and mounted at `/sys/fs/cgroup`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add a `cpu` controller to the cgroups v2 hierarchy; this is achieved by doing
    this, as root:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The kernel documentation on cgroups v2 ([https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#cpu](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#cpu)) does
    mention this point: *WARNING: cgroup2 doesn’t yet support control of realtime
    processes and the cpu controller can only be enabled when all RT processes are
    in the root cgroup. Be aware that system management software may already have
    placed RT processes into nonroot cgroups during the system boot process, and these
    processes may need to be moved to the root cgroup before the cpu controller can
    be enabled.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a sub-group: this is done by simply creating a directory with the required
    sub-group name under the cgroup v2 hierarchy; for example, to create a sub-group
    called `test_group`, use the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The interesting bit''s here: set up the max allowable CPU bandwidth for the
    processes that will belong to this sub-group; this is effected by writing into
    the `<cgroups-v2-mount-point>/<sub-group>/cpu.max` (pseudo) file. For clarity,
    the explanation of this file, as per the kernel documentation ([https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#cpu-interface-files](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#cpu-interface-files)),
    is reproduced here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In effect, all processes in the sub-control group will be collectively allowed to
    run for `$MAX` out of a period of `$PERIOD` microseconds; so, for example, with
    `MAX = 300,000` and `PERIOD = 1,000,000`, we're effectively allowing all processes
    within the sub-control group to run for 0.3 seconds out of a period of 1 second!
  prefs: []
  type: TYPE_NORMAL
- en: 'Insert some processes into the new sub-control group; this is achieved by writing
    their PIDs into the `<cgroups-v2-mount-point>/<sub-group>/cgroup.procs` pseudo-file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can further verify that they actually belong to this sub-group by looking
    up the content of each process's `/proc/<PID>/cgroup` pseudo-file; if it contains
    a line of the form `0::/<sub-group>`, then it indeed belongs to the sub-group!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That's it; *the processes under the new sub-group will now perform their work
    under the CPU bandwidth constraint imposed*; when done, they will die as usual...
    you can remove (or delete) the sub-group with a simple `rmdir <cgroups-v2-mount-point>/<sub-group>`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A bash script that actually carries out the preceding steps is available here:
    `ch11/cgroups_v2_cpu_eg/cgv2_cpu_ctrl.sh`. Do check it out! To make it interesting,
    it allows you to pass the maximum allowed CPU bandwidth – the `$MAX` value discussed
    in *step 4*! Not only that; we deliberately write a test script (`simp.sh`) that
    hammers on the CPU(s) – they generate integer values that we redirect to files.
    Thus, the number of integers they generated during their lifetime is an indication
    of how much CPU bandwidth was available to them... this way, we can test the script
    and actually see cgroups (v2) in action!'
  prefs: []
  type: TYPE_NORMAL
- en: 'A couple of test runs here will help you understand this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: You're expected to run it as root and to pass, as a parameter, the `$MAX` value
    (the usage screen seen previously quite clearly explains it, including displaying
    the valid range (the microseconds value)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we run the bash script with the parameter `800000`,
    implying a CPU bandwidth of 800,000 out of a period of 1,000,000; in effect, a
    quite high CPU utilization of 0.8 seconds out of every 1 second on the CPU (80%):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06814296-3ae2-44b7-911c-080a0dfa7de9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Screenshot of running our cgroups v2 CPU controller demo bash
    script with an effective max CPU bandwidth of 80%
  prefs: []
  type: TYPE_NORMAL
- en: 'Study our script''s output in *Figure 11.8*; you can see that it does its job:
    after verifying cgroup v2 support, it adds a `cpu` controller and creates a sub-group
    (called `test_group`). It then proceeds to launch two test processes called `j1`
    and `j2` (in reality, they''re just symbolic links to our `simp.sh` script). Once
    launched, they run of course. The script then queries and adds their PIDs to the
    sub-control group (as shown in *step 5*). We give the two processes 5 seconds
    to run; the script then displays the content of the files into which they wrote.
    It''s designed such that job `j1` writes integers starting from `1`, and job `j2`
    writes integers starting from `900`. In the preceding screenshot, you can clearly
    see that, in their lifetime, and under the effectively 80% CPU bandwidth available
    to it, job `j1` emits numbers from 1 to 68; similarly (under the same constraints),
    job `j2` emits numbers from `900` to `965` (a similar quantity of work, in effect).
    The script then cleans up, killing off the jobs and deleting the sub-group.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, to really appreciate the effect, we run our script again (study the
    following output), but this time with a maximum CPU bandwidth of just 1,000 (the
    `$MAX` value) – in effect, a max CPU utilization of just 0.1%!:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: What a difference! This time our jobs `j1` and `j2` could literally emit between
    just two and three integers (the values `1 2 3` for job j1 and `900 901 `for job
    j2, as seen in the preceding output), clearly proving the efficacy of the cgroups
    v2 CPU controller.
  prefs: []
  type: TYPE_NORMAL
- en: Containers, essentially lightweight VMs (to some extent), are currently a hot
    commodity. The majority of container technologies in use today (Docker, LXC, Kubernetes,
    and others) are, at heart, a marriage of two built-in Linux kernel technologies, namespaces,
    and cgroups.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we complete our brief coverage of a really powerful and useful kernel
    feature: cgroups. Let''s move on to the final section of this chapter: learning
    how you can turn regular Linux into a real-time operating system!'
  prefs: []
  type: TYPE_NORMAL
- en: Converting mainline Linux into an RTOS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mainline or vanilla Linux (the kernel you download from [https://kernel.org](https://kernel.org))
    is decidedly *not* a **Real-Time Operating System** (**RTOS**); it''s a **General
    Purpose Operating System** (**GPOS**; as is Windows, macOS, Unix). In an RTOS,
    where hard real-time characteristics come into play, not only must the software
    obtain the correct result, there are deadlines associated with doing so; it must
    guarantee it meets these deadlines, every single time. The mainline Linux OS,
    though not an RTOS, does a tremendous job: it easily qualifies as being a soft
    real-time OS (one where deadlines are met most of the time). Nevertheless, true
    hard real-time domains (for example, military operations, many types of transport,
    robotics, telecom, factory floor automation, stock exchanges, medical electronics,
    and so on) require an RTOS.'
  prefs: []
  type: TYPE_NORMAL
- en: Another key point in this context is that of **determinism**: an oft missed
    point regarding real-time is that the software response time need not always be
    really fast (responding, say, within a few microseconds); it may be a lot slower
    (in the range of, say, tens of milliseconds); by itself, that isn't what really
    matters in an RTOS. What does matter is that the system is reliable, working in
    the same consistent manner and always guaranteeing the deadline is met.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the time taken to respond to a scheduling request, should be consistent
    and not bounce all over the place. The variance from the required time (or baseline)
    is often referred to as the **jitter**; an RTOS works to keep the jitter tiny,
    even negligible. In a GPOS, this is often impossible and the jitter can vary tremendously
    - at one point being low and the next very high. Overall, the ability to maintain
    a stable even response with minimal jitter - even in the face of extreme workload
    pressures - is termed determinism, and is the hallmark of an RTOS. To provide
    such a deterministic response, algorithms must, as far as possible, be designed
    to correspond to *O(1)* time complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Thomas Gleixner, along with community support, has worked toward that goal for
    a long while now; for many years, in fact, ever since the 2.6.18 kernel, there
    have been offline patches that convert the Linux kernel into an RTOS. These patches
    can be found, for many versions of the kernel, here: [https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/](https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/).
    The older name for this project was `PREEMPT_RT`; later (October 2015 onward),
    the **Linux Foundation** (**LF**) took over stewardship of this project – a very
    positive step! – and renamed it the **Real-Time Linux** (**RTL**) Collaborative
    Project ([https://wiki.linuxfoundation.org/realtime/rtl/start#the_rtl_collaborative_project](https://wiki.linuxfoundation.org/realtime/rtl/start#the_rtl_collaborative_project)),
    or RTL (don't confuse this project with co-kernel approaches such as Xenomai or
    RTAI, or the older and now-defunct attempt called RTLinux).
  prefs: []
  type: TYPE_NORMAL
- en: 'An FAQ, of course, is "why aren''t these patches in mainline itself?" Well,
    it turns out that:'
  prefs: []
  type: TYPE_NORMAL
- en: Much of the RTL work has indeed been merged into the mainline kernel; this includes
    important areas such as the scheduling subsystem, mutexes, lockdep, threaded interrupts,
    PI, tracing, and so on. In fact, an ongoing primary goal of RTL is to get it merged
    as much as is feasible (we show a table summarizing this in the *Mainline and
    RTL – technical differences summarized* section).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linus Torvalds deems that Linux, being primarily designed and architected as
    a GPOS, should not have highly invasive features that only an RTOS really requires;
    so, though patches do get merged in, it's a slow deliberated process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have included several interesting articles and references to RTL (and hard
    real time) in the *Further reading* section of this chapter; do take a look.
  prefs: []
  type: TYPE_NORMAL
- en: 'What you''re going to do next is interesting indeed: you will learn how to
    patch the mainline 5.4 LTS kernel with the RTL patches, configure it, build, and
    boot it; you will thus end up running an RTOS – *Real-Time Linux or RTL*! We shall
    do this on our x86_64 Linux VM (or native system).'
  prefs: []
  type: TYPE_NORMAL
- en: We won't stop there; you will then learn more – the technical differences between
    regular Linux and RTL, what system latency is, and how, practically, to measure
    it. To do so, we shall first apply the RTL patch on the kernel source of the Raspberry
    Pi device, configure and build it, and use it as a test-bed for system latency
    measurement using the *cyclictest* app (you'll also learn to use modern BPF tools
    for measuring scheduler latencies). Let's get a move on, first building an RTL
    kernel for our 5.4 kernel on an x86_64!
  prefs: []
  type: TYPE_NORMAL
- en: Building RTL for the mainline 5.x kernel (on x86_64)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will learn, in a step-by-step, hands-on fashion, how exactly
    to patch, configure, and build Linux as an RTOS. As mentioned in the preceding
    section, these real-time patches have been around a long while; it's time to make
    use of them.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining the RTL patches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Navigate to [https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/](https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/) (or,
    if you''re on an alternate kernel, go to one directory level above this and select
    the required kernel version):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8335c076-4a92-4ece-b80e-f38139e5bc15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – Screenshot of the RTL patches for the 5.4 LTS Linux kernels
  prefs: []
  type: TYPE_NORMAL
- en: 'You will quickly notice that the RTL patches are available for only some versions
    of the kernel in question (here, 5.4.y); more on this follows. In the preceding
    screenshot, you can spot two broad types of patch files – interpret it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`patch-<kver>rt[nn].patch.[gz|xz]`: The prefix is `patch-`; this is the complete
    collection of patches required to patch the mainline kernel (version `<kver>`)
    **in one unified** (and compressed) file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patches-<kver>-rt[nn].patch.[gz|xz]`: The prefix is `patches-`; this compressed
    file contains every individual patch (as a separate file) that went into making
    up the patch series for this version of RTL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Also, as you should be aware, `<fname>.patch.gz` and `<fname>.patch.xz` are
    the same archive; it's just that the compressor differs – the `.sign` files are the
    PGP signature files.)
  prefs: []
  type: TYPE_NORMAL
- en: We shall use the first type; download the `patch-<kver>rt[nn].patch.xz` file
    to your target system by clicking on the link (or via `wget(1)`).
  prefs: []
  type: TYPE_NORMAL
- en: Notice that for the 5.4.x kernels (as of the time of writing), the RTL patches
    seem to be present only for version 5.4.54 and 5.4.69 (and not for 5.4.0, the
    kernel that we have been working with all along).
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the particular kernel version that the RTL patches apply against can
    certainly vary from what I've mentioned here at the time of this writing. That's
    expected - just follow the steps substituting the release number you're using
    with what's mentioned here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Don''t be worried – we shall show you a workaround in a moment. This is indeed
    going to be the case; the community cannot feasibly build patches against every
    single kernel release – there are just too many. This does have an important implication:
    either we patch our 5.4.0 kernel to, say, 5.4.69, or, we simply download the 5.4.69
    kernel to begin with and apply the RTL patches against it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first approach is doable but is more work (especially in the absence of
    a patching tools such as git/ketchup/quilt or similar; here, we choose not to
    use git to apply patches, just working on the stable kernel tree instead). As
    the Linux kernel patches are incremental, we will have to download every single
    patch from 5.4.0 until 5.4.69 (a total of 69 patches!), and apply them successively
    and in order: first 5.4.1, then 5.4.2, then 5.4.3, and so on until the final one!
    Here, to help keep things simple, since we know that the kernel to patch against
    is 5.4.69, it''s just easier to download and extract it instead. So, head on over
    to [https://www.kernel.org/](https://www.kernel.org/) and do so. Thus, here, we
    end up downloading two files:'
  prefs: []
  type: TYPE_NORMAL
- en: The compressed kernel source for mainline 5.4.69: [https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.4.69.tar.xz](https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.4.69.tar.xz)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RTL patch for 5.4.69: [https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/patches-5.4.69-rt39.tar.xz](https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/patches-5.4.69-rt39.tar.xz)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (As explained in detail in [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml),
    *Building the 5.x Linux Kernel from Source – Part 2*, if you intend to cross-compile
    the kernel for another target, the usual procedure is to build it on a suitably
    powerful workstation, so download it there.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, extract both the RTL patch file as well as the kernel code base `tar.xz`
    file to obtain the kernel source tree (here, it''s version 5.4.69; of course,
    these details have been well covered back in [Chapter 2](e0b89a37-18a3-424d-8983-58c4ac0725f6.xhtml),
    *Building the 5.x Linux Kernel from Source – Part 1*). By now, your working directory
    content should look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '(FYI, the `unxz(1)` utility can be used to extract the `.xz`-compressed patch
    file.) For the curious reader: take a peek at the patch (the file `patch-5.4.69-rt39.patch`),
    to see all the code-level changes wrought to bring about a hard real-time kernel;
    it''s non-trivial of course! An overview of the technical changes will be seen
    in the upcoming *Mainline and RTL – technical differences summarized* section.
    Now that we have things in place, let''s begin by applying the patch to the stable
    5.4.69 kernel tree; the following section covers just this.'
  prefs: []
  type: TYPE_NORMAL
- en: Applying the RTL patch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ensure you keep the extracted patch file, `patch-5.4.69-rt39.patch`, in the
    directory immediately above the 5.4.69 kernel source tree (as seen previously).
    Now, let''s apply the patch. Careful – (obviously) don''t attempt to apply the
    compressed file as the patch; extract and use the uncompressed patch file. To
    ensure that the patch applies correctly, we first employ the `--dry-run` (dummy
    run) option to `patch(1)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'All''s well, let''s now actually apply it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Great – we have the patched kernel for RTL ready now!
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are multiple ways and various shortcuts that can be employed;
    for example, you can also achieve the preceding via the `xzcat ../patch-5.4.69-rt39.patch.xz
    | patch -p1` command (or similar).
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and building the RTL kernel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have covered the kernel configuration and build steps in detail in [Chapter
    2](e0b89a37-18a3-424d-8983-58c4ac0725f6.xhtml), *Building the 5.x Linux Kernel
    from Source – Part 1*, and [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml),
    *Building the 5.x Linux Kernel from Source – Part 2*, hence we shan''t repeat
    it here. Pretty much everything remains the same; the only significant difference
    being that we must configure this kernel to take advantage of RTL (this is explained
    on the new RTL wiki site, here: [https://wiki.linuxfoundation.org/realtime/documentation/howto/applications/preemptrt_setup](https://wiki.linuxfoundation.org/realtime/documentation/howto/applications/preemptrt_setup)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To cut down the kernel features to be built to approximately match the present
    system configuration, we first, within the kernel source tree directory (`linux-5.4.69`),
    do the following (we also covered this back in [Chapter 2](e0b89a37-18a3-424d-8983-58c4ac0725f6.xhtml),
    *Building the 5.x Linux Kernel from Source - Part 1*, under the *Tuned kernel
    config via the localmodconfig approach* section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, fire up the kernel configuration with `make menuconfig`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the `General setup` sub-menu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f11d2aa0-2ff7-4939-b420-fbbfe1a49bd2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.10 – make menuconfig / General setup: configuring the RTL-patched
    kernel'
  prefs: []
  type: TYPE_NORMAL
- en: Once there, scroll down to the `Preemption Model` sub-menu; we see it highlighted
    in the preceding screenshot, along with the fact that the currently (by default)
    selected preemption model is `Voluntary Kernel Preemption (Desktop)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pressing *Enter* here leads us into the `Preemption Model` sub-menu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a9390c2b-8923-4583-bbe4-d037b7ff4a86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.11 – make menuconfig / General setup / Preemption Model: configuring
    the RTL-patched kernel'
  prefs: []
  type: TYPE_NORMAL
- en: There it is! Recall from the previous chapter, in the *Preemptible kernel* section,
    we described the fact that this very kernel configuration menu had three items
    (the first three seen in Figure 11.11). Now it has four. The fourth item – the
    `Fully Preemptible Kernel (Real-Time)` option – has been added on thanks to the
    RTL patch we just applied!
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to configure the kernel for RTL, scroll down and select the `Fully Preemptible
    Kernel (Real-Time)` menu option (refer Figure 11.1). This corresponds to the kernel
    `CONFIG_PREEMPT_RT` config macro, whose `< Help >` is quite descriptive (do take
    a gander); it does, in fact, conclude with the statement: *select this if you
    are building a kernel for systems which require real-time guarantees*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In earlier versions of the kernel (including 5.0.x), the `Preemption Model`
    sub-menu displayed five choices; two were for RT: one was termed Basic RT and
    the other was what we see here as the fourth choice – now (5.4.x) they''ve simply
    been folded into one true real-time option.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have selected the fourth option and saved and exited the `menuconfig`
    UI, (re)check that the full preemptible kernel – in effect, RTL – is selected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: All right, looks good! (Of course, before building, you can tweak other kernel
    config options as required for your product.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now build the RTL kernel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Once it successfully builds and installs, reboot the system; at boot, press
    a key to display the GRUB bootloader menu (holding down one of the *Shift* keys
    can help ensure the GRUB menu is displayed at boot); within the GRUB menu, select
    the newly built `5.4.69-rtl` RTL kernel (in fact, the kernel just installed is
    usually the default one selected at boot). It should boot now; once logged in
    and on a shell, let''s verify the kernel version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Notice `CONFIG_LOCALVERSION` set to the value `-rtl-llkd1`. (Also, with `uname
    -a`, the `PREEMPT RT` string will be seen.) We're now - as promised - running
    Linux, RTL, as a hard real-time operating system, an RTOS!
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s very important to understand, though, that for true hard real time, simply
    having a hard real-time kernel is *not* enough; you must also very carefully design
    and write your user space (apps, libraries, and tooling) as well as your kernel
    modules / drivers, to conform to real time as well. For example, frequent page
    faulting can throw determinism out of the proverbial window and result in high
    latencies (and high jitter). (Recall what you learned in [Chapter 9](dbb888a2-8145-4132-938c-1313a707b2f2.xhtml),
    *Kernel Memory Allocation for Module Authors – Part 2*, in the *A brief note on
    memory allocations and demand paging* section. Page faulting is a fact of life
    and can and does often occur; minor page faults will usually cause little to worry
    about. But in a hard RT scenario? And in any case, "major faults" will hamper
    performance.) Techniques such as using `mlockall(2)` to lock down all the pages
    of a real-time application process might well be required. This and several other
    techniques and tips for writing real-time code are provided here: [https://rt.wiki.kernel.org/index.php/HOWTO:_Build_an_
    RT-application](https://rt.wiki.kernel.org/index.php/HOWTO:_Build_an_RT-application).
    (Similarly, topics regarding CPU affinity and shielding, `cpuset` management,
    IRQ prioritization, and so on can be found on the older RT wiki site mentioned
    previously; [https://rt.wiki.kernel.org/index.php/Main_Page](https://rt.wiki.kernel.org/index.php/Main_Page).)'
  prefs: []
  type: TYPE_NORMAL
- en: So, great – you now know how to configure and build Linux as an RTOS! I encourage
    you to try this out for yourself. Moving along, we'll next summarize the key differences
    between the standard and RTL kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Mainline and RTL – technical differences summarized
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To give you a deeper understanding of this interesting topic area, in this
    section, we delve further into it: we summarize the key differences between the
    standard (or mainline) and RTL kernels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we summarize some of the key differences between the
    standard (or mainline) and RTL kernels. A primary goal of the RTL project is to
    ultimately become fully integrated into the regular mainline kernel tree. As this
    process is evolutionary, the merging of patches from RTL into mainline is slow
    but steady; interestingly, as you can see from the rightmost column in the following
    table, most of (around 80% at the time of writing) the RTL work has actually been
    already merged into the mainline kernel, and it continues to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Component / Feature** | **Standard or mainline (vanilla) Linux** | **RTL
    (fully preemptible / hard real-time Linux)** | **RT work merged into mainline?**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Spinlocks | The spinlock critical section is  non-preemptible kernel code
    | As preemptible as is humanly possible; called "sleeping spinlocks"! In effect,
    spinlocks have been converted into mutexes. | No |'
  prefs: []
  type: TYPE_TB
- en: '| Interrupt handling | Traditionally done via the top and bottom half (hardirq/tasklet/softirq)
    mechanism | Threaded interrupts: the majority of interrupt processing is done
    within a kernel thread (2.6.30, June 2009). | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| HRTs (High-Resolution Timers) | Available here due to merge from RTL | Timers
    with nanosecond resolution (2.6.16, March 2006). | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| RW locks | Unbounded; writers may starve | Fair RW locks with bounded writer
    latency. | No |'
  prefs: []
  type: TYPE_TB
- en: '| lockdep | Available here due to merge from RTL | Very powerful (kernel space)
    tool to detect and prove locking correctness or the lack thereof. | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Tracing | Some tracing technologies available here due to merge from RTL
    | Ftrace''s origins (and to some extent perf''s) were with the RT developers attempting
    to find latency issues. | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Scheduler | Many scheduler features available here due to merge from RTL
    | Work on real-time scheduling as well as the deadline scheduling class (`SCHED_DEADLINE`)
    was first done here (3.14, March 2014); also, full tickless operation (3.10, June
    2013). | Yes |'
  prefs: []
  type: TYPE_TB
- en: (Don't worry – we shall definitely cover many of the preceding details in subsequent
    chapters of the book.)
  prefs: []
  type: TYPE_NORMAL
- en: Of course, a well-known (at least it should be) rule of thumb is simply this: *there
    is no silver bullet*. This implies, of course, that no one solution will fit every
    need.
  prefs: []
  type: TYPE_NORMAL
- en: Please, if you haven't yet done so, do yourself a huge favor and read the still-so-relevant
    book *The Mythical Man-Month: Essays on Software Engineering *by Frederick P Brooks.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in [Chapter 10](5391e3c1-30ad-4c75-a106-301259064881.xhtml), *The
    CPU Scheduler – Part 1*, in the *Preemptible kernel* section, the Linux kernel
    can be configured with the `CONFIG_PREEMPT` option; this is often referred to
    as the **low-latency** (or **LowLat**) kernel and provides near real-time performance.
    In many domains (virtualization, telecoms, and so on) using a LowLat kernel might
    turn out to be better than using a hard real-time RTL kernel, mainly due to RTL's
    overheads. You often find that, with hard real-time, user space apps can suffer
    from throughput, reduced CPU availability, and thus higher latencies. (Refer to
    the *Further reading* section for a whitepaper from Ubuntu that conducts a comparison
    between a vanilla distro kernel, a low-latency preemptible, and a fully preemptible
    – effectively an RTL – kernel.)
  prefs: []
  type: TYPE_NORMAL
- en: With latencies in mind, the following section will help you understand what
    exactly is meant by system latencies; then, you'll learn some ways to measure
    it on a live system. On, on!
  prefs: []
  type: TYPE_NORMAL
- en: Latency and its measurement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We often come across the term latency; what exactly does it mean in the context
    of the kernel? A synonym for latency is delay and that''s a good hint. *The latency
    (or delay) is the time taken to react* – in our context here, the time between
    the kernel scheduler waking up a user space thread (or process), thus making it
    runnable, and the time when it does actually run on the processor is the **scheduling
    latency**. (Do be aware, though, the term scheduling latency is also used in another
    context, to mean the time interval within which every runnable task is guaranteed
    to run at least once; the tunable is here: `/proc/sys/kernel/sched_latency_ns`,
    and, at least on recent x86_64 Linux, defaults to 24 ms). Similarly, the time
    elapsed from when a hardware interrupt occurs (say a network interrupt) to when
    it''s actually serviced by it''s handler routine, is the interrupt latency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **cyclictest** user space program was written by Thomas Gleixner; its purpose:
    to measure kernel latencies. Its output values are in microseconds units. The
    average and maximum latency values are usually the ones of interest – if they
    fall within the acceptable range for the system, then all''s good; if not, it
    points to perhaps product-specific redesign and/or kernel configuration tweaking,
    checking other time-critical code paths (including user space), and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's use the cyclictest process itself as an example to clearly understand
    scheduling latency. The cyclictest process is run; internally, it issues `nanosleep(2)` (or,
    if the `-n` option switch is passed, the `clock_nanosleep(2)` system call), putting
    itself into a sleep state for the time interval specified. As these `*sleep()`
    system calls are obviously blocking, the kernel internally enqueues the cyclictest
    (for simplicity, we refer to it as `ct` in the following diagram) process into
    a wait queue, simply a kernel data structure that holds sleeping tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'A wait queue is associated with an event; when that event occurs, the kernel
    awakens all tasks sleeping on that event. Here, the event in question is the expiry
    of a timer; this is communicated by the timer hardware by emitting a hardware
    interrupt (or IRQ); this starts the chain of events that must happen to make the
    cyclictest process wake up and run on the processor. The key point here, of course,
    is that it''s easier said than done: many potential delays might occur on the
    path to the process actually running on a processor core! This is what the following
    diagram seeks to convey – the potential sources of latency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73e35574-dce9-40e5-ab2a-1c2a7f5a50eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 – The path to waking, context-switching, and running the cyclictest
    (ct) process; several latencies can occur
  prefs: []
  type: TYPE_NORMAL
- en: (Some of the preceding inputs stem from the excellent presentation *Using and
    Understanding the Real-Time Cyclictest Benchmark, Rowand, Oct 2013*.) Study Figure
    11.12 carefully; it shows the timeline from the hardware interrupt's assertion
    due to timer expiry (at time `t0`, as the sleep issued via the `nanosleep()` API
    by the cyclictest process is done at time `t1`), through IRQ handling (`t1` to
    `t3`), and the wakeup of the ct process – as a result of which it gets enqueued
    into the runqueue (between `t3` and `t4`) of the core it will eventually run upon.
  prefs: []
  type: TYPE_NORMAL
- en: From there, it will eventually become the highest priority, or best or most
    deserving, task for the scheduling class it belongs to (at time `t6`; we covered
    these details in the preceding chapter), thus, it will preempt the currently running
    thread (`t6`). The `schedule()` code will then execute (time `t7 `to `t8`), the
    context switch will occur at the tail-end of `schedule()`, and finally(!), and
    the cyclictest process will actually execute on a processor core (time `t9`).
    Though it might at first appear complex, the reality is that this is a simplified
    diagram as several other potential latency sources have been omitted (for example,
    latencies due to IPI, SMI, cache migration, multiple occurrences of the preceding
    events, additional interrupts firing at an inopportune moment causing more delays,
    and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'A rule of thumb for determining the maximum latency value of a user space task
    running with real-time priority is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As an example, the Raspberry Pi Model 3 CPU clock runs at a frequency of 1 GHz;
    its wavelength (the time between one clock cycle to the next) is the inverse of
    the frequency, that is, 10^(-9) or 1 nanosecond. So, from the preceding equation,
    the theoretical maximum latency should be (within) 10^(-7) seconds which is about
    10 ns (nanoseconds). As you shall soon discover, this is merely theoretical.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring scheduling latency with cyclictest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To make this more interesting (as well as to run the latency test on a constrained
    system), we shall perform latency measurements using the well-known cyclictest app
    – while the system is under some amount of load (via the `stress(1)` utility)
    – on the equally well-known Raspberry Pi device. This section is divided into
    four logical parts:'
  prefs: []
  type: TYPE_NORMAL
- en: First, set up the working environment on the Raspberry Pi device.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, download and apply the RT patches on the kernel source, configure, and
    build it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Third, install the cyclictest app, as well as a few other required packages
    (including `stress`), on the device.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fourth, run the test cases and analyze the results (even plotting graphs to
    help do so).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first step and most parts of the second have already been covered in detail
    in [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml), *Building the 5.x
    Linux Kernel from Source – Part 2*, in the *Kernel build for the Raspberry Pi*
    section. This includes downloading the Raspberry Pi-specific kernel source tree,
    configuring the kernel, and installing an appropriate toolchain; we won't repeat
    this information here. The only significant difference here is that we shall first
    have to apply the RT patches to the kernel source tree and configure for hard
    real-time; we cover this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get going!
  prefs: []
  type: TYPE_NORMAL
- en: Getting and applying the RTL patchset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Check the mainline or distribution kernel version that is running on your Raspberry
    Pi device (substitute the Raspberry Pi with any other device you may be running
    Linux on); for example, on the Raspberry Pi 3B+ I'm using, it's running the stock
    Raspbian (or Raspberry Pi OS) GNU/Linux 10 (buster) with the 5.4.51-v7+ kernel.
  prefs: []
  type: TYPE_NORMAL
- en: We'd like to build an RTL kernel for the Raspberry Pi with the closest possible
    matching kernel to the standard one it's currently running; for our case here,
    with it running 5.4.51[-v7+], the closest available RTL patches are for kernel
    versions 5.4.y-rt[nn] ([https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/](https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/));
    we shall come back to this shortly...
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: The steps to download the Raspberry Pi specific kernel source tree onto your
    host system disk have already been covered in [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml),
    *Building the 5.x Linux Kernel from Source – Part 2*, in the *Kernel build for
    the Raspberry Pi* section*;* do refer to it and obtain the source tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once this step completes, you should see a directory named `linux`; it holds
    the Raspberry Pi kernel source for (as of the time of writing) kernel version
    5.4.y. What''s the value of `y`? That''s easy; just do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The `SUBLEVEL` variable here is the value of `y`; clearly, it's 70, making the
    kernel version 5.4.70.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s download the appropriate real-time (RTL) patch: the best one would
    be an exact match, that is, the patch should be named something like `patch-5.4.70-rt[nn].tar.xz`.
    Lucky for us, it does indeed exist on the server; let''s get it (notice that we
    download the `patch-<kver>-rt[nn]` file; it''s simpler to work with as it''s the
    unified patch):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`wget https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/patch-5.4.70-rt40.patch.xz`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This does raise the question: what if the versions of the available RTL patches
    do *not *precisely match that of the device''s kernel version? Well, unfortunately,
    that does happen. In cases like this, to have the best chance of applying it against
    the device kernel, select the closest match and attempt to apply it; it often
    succeeds with perhaps minor warnings... If not, you will have to either manually
    tweak the code base to suit the patchset, or just switch to using a kernel version
    for which the RTL patch exists (recommended).'
  prefs: []
  type: TYPE_NORMAL
- en: Don't forget to uncompress the patch file!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now apply the patch (as shown previously, in the *Applying the RTL patch* section):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the patched kernel, turning on the `CONFIG_PREEMPT_RT` kernel config
    option (as explained previously):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, though, as we learned in [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml), *Building
    the 5.x Linux Kernel from Source – Part 2*, it''s *critical* that you set up the
    initial kernel config appropriately for the target; here, as the target device
    is the Raspberry Pi 3[B+], do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Customize your kernel configuration with the `make ARCH=arm menuconfig` command.
    Here, of course, you should go to `General setup / Preemption Model`, and select
    the fourth option, `CONFIG_PREEMPT_RT`, to turn on the hard real-time preemption
    features.
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I shall also assume that you have an appropriate toolchain for x86_64-to-ARM32
    for the Raspberry Pi installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Hint: Installing an appropriate toolchain (for x86_64-to-ARM32) can be as simple
    as `sudo apt install ​crossbuild-essential-armhf`. Now build the kernel (again,
    identical to the process we described previously, in the *Configuring and building
    the RTL kernel* section), with the difference being that we cross-compile it (using
    the x86_64-to-ARM32 cross-compiler we installed previously).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the just-built kernel modules; ensure you specify the location as the
    SD card''s root filesystem with the `INSTALL_MOD_PATH` environment variable (else
    it might overwrite your host''s modules, which would be disastrous!). Let''s say
    that the microSD card''s second partition (which contains the root filesystem)
    is mounted under `/media/${USER}/rootfs`, then do the following (in one single
    line):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Copy across the image files (the bootloader files, the kernel `zImage` file,
    the **Device Tree Blobs** (**DTBs**), the kernel modules) onto the Raspberry Pi
    SD card (these details are covered in the official Raspberry Pi documentation
    here: [https://www.raspberrypi.org/documentation/linux/kernel/building.md](https://www.raspberrypi.org/documentation/linux/kernel/building.md);
    we have also (lightly) covered this in [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml),
    *Building the 5.x Linux Kernel from Source – Part 2*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Test: boot the Raspberry Pi with the new kernel image in the SD card. You should
    be able to log in to a shell (typically over `ssh`). Verify the kernel version
    and config:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We are indeed running a hard real-time kernel on the device! So, good – that
    takes care of the "prep" portion; you are now in a position to proceed with the
    next step.
  prefs: []
  type: TYPE_NORMAL
- en: Installing cyclictest (and other required packages) on the device
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We intend to run test cases via the cyclictest app against both the standard
    and the newly minted RTL kernel. This implies, of course, that we must first obtain
    the cyclictest sources and build it on the device (note that the work here is
    being carried out on the Raspberry Pi).
  prefs: []
  type: TYPE_NORMAL
- en: Here's an article that does this very thing: *Latency of Raspberry Pi 3 on Standard
    and Real-Time Linux 4.9* *Kernel*: [https://metebalci.com/blog/latency-of-raspberry-pi-3-on-standard-and-real-time-linux-4.9-kernel/](https://metebalci.com/blog/latency-of-raspberry-pi-3-on-standard-and-real-time-linux-4.9-kernel/).
  prefs: []
  type: TYPE_NORMAL
- en: 'It mentions an issue faced running the RTL kernel on the Raspberry Pi 3 as
    well as a workaround (important!): (in addition to the usual ones) pass along
    these two kernel parameters: `dwc_otg.fiq_enable=0` and `dwc_otg.fiq_fsm_enable=0`. You
    can put these in the `/boot/cmdline.txt` file on the device.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, do ensure that all required packages are installed onto your Raspberry
    Pi:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The `libnuma-dev` package is optional and may not be available on the Raspberry
    Pi OS (you can proceed even without it).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now get the source code of cyclictest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'A bit peculiarly, initially, there will exist precisely one file, the `README`.
    Read it (surprise, surprise). It informs you how to obtain and build the stable
    version; it''s simple, just do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Happily for us, the **Open Source Automation Development Lab** (**OSADL**)
    has a very useful bash script wrapper over cyclictest; it runs cyclictest and
    even plots a latency graph. Grab the script from here: [https://www.osadl.org/uploads/media/mklatencyplot.bash](https://www.osadl.org/uploads/media/mklatencyplot.bash) (explanatory
    note on it: [https://www.osadl.org/Create-a-latency-plot-from-cyclictest-hi.bash-script-for-latency-plot.0.html?&no_cache=1&sword_list[0]=cyclictest](https://www.osadl.org/Create-a-latency-plot-from-cyclictest-hi.bash-script-for-latency-plot.0.html?&no_cache=1&sword_list%5B0%5D=cyclictest)).
    I have lightly modified it for our purposes; it''s here in the GitHub repository
    for this book: `ch11/latency_test/latency_test.sh`.'
  prefs: []
  type: TYPE_NORMAL
- en: Running the test cases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To get a good idea regarding the system (scheduling) latencies, we shall run
    three test cases; in all three, the cyclictest app will sample system latency
    while the `stress(1)` utility is putting the system under load:'
  prefs: []
  type: TYPE_NORMAL
- en: Raspberry Pi 3 model B+ (4 CPU cores) running the 5.4 32-bit RTL-patched kernel
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Raspberry Pi 3 model B+ (4 CPU cores) running the standard 5.4 32-bit Raspberry
    Pi OS kernel
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: x86_64 (4 CPU cores) Ubuntu 20.04 LTS running the standard 5.4 (mainline) 64-bit
    kernel
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We use a small wrapper script called `runtest` over the `latency_test.sh` script for
    convenience. It runs the `latency_test.sh` script to measure system latency while
    running the `stress(1)` utility; it invokes `stress` with the following parameters,
    to impose CPU, I/O, and memory loads on the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '(FYI, a later version of `stress` called `stress-ng` is available as well.)
    While the `stress` app executes, loading the system, the `cyclictest(8)` app samples
    system latencies, writing its `stdout` to a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: (Do refer to the man pages on both `stress(1)` and `cyclictest(8)` to understand
    the parameters.) It will run for an hour (for more accurate results, I suggest
    you run the test for a longer duration – perhaps 12 hours). Our `runtest` script
    (and the underlying ones) internally runs `cyclictest` with appropriate parameters;
    it captures and displays the minimum, average, and maximum latency wall clock
    time taken (via `time(1)`), and generates a histogram plot. Note that here, we
    run `cyclictest` for a (maximum) duration of an hour.
  prefs: []
  type: TYPE_NORMAL
- en: By default, our `runtest` wrapper script has a variable LAT with the pathname
    to the `latency_tests` directory set as follows: `LAT=~/booksrc/ch11/latency_tests`.
    Ensure that you first update it to reflect the location of the `latency_tests`
    directory on your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'A screenshot of running the scripts for our test case #1 – on the Raspberry
    Pi 3B+ running the RTL kernel – is seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a153d37e-90f6-41a0-bd42-98f3529de031.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – Running our first test case for cyclictest on a Raspberry Pi
    3B+ on the RTL kernel while under stress
  prefs: []
  type: TYPE_NORMAL
- en: Study the preceding screenshot; you can clearly see the system details, the
    kernel version (notice it's the RTL-patched `PREEMPT_RT` kernel!), and cyclictest's
    latency measurement results for the minimum, average, and maximum (scheduling)
    latency.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We carry out a similar procedure for the remaining two test cases and summarize
    the results of all three in Figure 11.14:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fda2c55a-4879-4baf-98d8-3ee0c4021206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – Results of the (simplistic) test cases we ran showing the min/avg/max
    latencies for different kernels and systems while under some stress
  prefs: []
  type: TYPE_NORMAL
- en: Interesting; though the maximum latency of the RTL kernel is much below the
    other standard kernels, both the minimum and, more importantly, average latencies
    are superior for the standard kernels. This ultimately results in superior overall
    throughput for the standard kernels (this very same point was stressed upon earlier).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `latency_test.sh` bash script invokes the `gnuplot(1)` utility to generate
    graphs, in such a manner that the title line shows the minimum/average/maximum
    latency values (in microseconds) and the kernel the test was run upon. Recollect
    that test case #1 and #2 ran on the Raspberry Pi 3B+ device, whereas test case
    #3 ran on a generic (and more powerful) x86_64 system). See here the `gnuplot`-ed
    graphs (for all three test cases):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9039fe61-7614-4f9f-96c8-6cdfd4dae665.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.15 – Test case #1 plot: cyclictest latency measurement on Raspberry
    Pi 3B+ running the 5.4 RTL kernel'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.15 shows the graph plotted by `gnuplot(1)` (called from within our `ch11/latency_test/latency_test.sh`
    script) for test case #1\. The **Device Under Test** (**DUT**), the Raspberry
    Pi 3B+, has four CPU cores (as seen by the OS). Notice how the graph shows us
    the story – the vast majority of samples are on the upper left, implying that,
    most of the time, the latency was very small (between 100,000 to 1 million latency
    samples (y-axis) fall between a few microseconds to 50 microseconds (x-axis)!).
    That''s really good! Of course, there will be outliers at the other extreme –
    samples on all CPU cores have much higher latencies (between 100 and 256 microseconds)
    though the number of samples is much smaller. The cyclictest app gives us the
    minimum, average, and maximum system latency values. With the RTL-patched kernel,
    while the max latency is actually excellent (quite low), the average latency can
    be fairly high:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e65ec4f6-374d-49c3-91bb-07520aaff1e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.16 – Test case #2 plot: cyclictest latency measurement on Raspberry
    Pi 3B+ running the standard (mainline) 5.4 kernel'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.16 shows the plot for test case #2\. Again, as with the previous
    test case – in fact, even more pronounced here – the vast majority of system latency
    samples exhibit very low latency! The standard kernel thus does a tremendous job;
    even the average latency is a "decent" value. However, the worst-case (max) latency
    value can be very large indeed – *showing us exactly why it''s not an RTOS*. For
    most workloads, the latency tends to be excellent "usually", but a few corner
    cases will tend to show up. In other words, it''s *not deterministic* – the key
    characteristic of an RTOS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d539b8dd-14e8-43cc-a86f-9db98d5af156.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.17 – Test case #3 plot: cyclictest latency measurement on an x86_64
    Ubuntu 20.04 LTS running the standard (mainline) 5.4 kernel'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.17 shows the plot for test case #3\. The variance – or **jitter** –
    here is even more pronounced (again, non-deterministic!), though the minimum and
    average system latency values are really very good. Of course, it''s run on a
    far more powerful system – a desktop-class x86_64 – than the previous two test
    cases. The max latency value – the few corner cases, although there are more of
    them here – tends to be quite high. Again, it''s not an RTOS – it''s not deterministic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Did you notice how the graphs clearly exhibit *jitter*: with test case #1 having
    the least amount (the graph tends to drop down to the x-axis quite quickly - meaning
    a very tiny number of latency samples, if not zero, exhibit high(er) latencies)
    and test case #3 having the most jitter (with much of the graph remaining well
    above the *x* axis!).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we emphasize this point: the results quite clearly show that it''s deterministic
    (a very small amount of jitter) with an RTOS and highly non-deterministic with
    a GPOS! (As a rule of thumb, standard Linux will result in approximately +/- 10
    microseconds of jitter for interrupt processing, whereas on a microcontroller
    running an RTOS, the jitter will be far less, around +/- 10 nanoseconds!)'
  prefs: []
  type: TYPE_NORMAL
- en: Doing this experiment, you will realize that benchmarking is a tricky thing;
    you shouldn't read too much into a few test runs (running the tests for a long
    while, having a large sample set, is important). Testing with realistic work loads
    you expect to experience on the system would be a far better way to see which
    kernel configuration yields superior performance; it does indeed vary with the
    workload!
  prefs: []
  type: TYPE_NORMAL
- en: (An interesting case study by Canonical shows statistics for regular, low-latency,
    and real-time kernels for certain workloads; look it up in the *Further reading*
    section of this chapter.) As mentioned before, quite often, the superior *max*
    latency characteristics of an RTL kernel can lead to inferior overall throughput
    (user space might suffer from reduced CPU due to RTL's rather ruthless prioritization).
  prefs: []
  type: TYPE_NORMAL
- en: Measuring scheduler latency via modern BPF tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Without going into too many details, we'd be amiss to leave out the recent and
    powerful [e]BPF Linux kernel feature and it's associated frontends; there are
    a few to specifically measure scheduler and runqueue-related system latencies.
    (We covered the installation of the [e]BPF tools back in [Chapter 1](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml), *Kernel
    Workspace Setup* under the *Modern tracing and performance analysis with [e]BPF* section).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes some of these tools (BPF frontends); all these
    tools need to be run as root (as with any BPF tool); they show their output as
    a histogram (with the time in microseconds by default):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **BPF tool** | ** What it measures** |'
  prefs: []
  type: TYPE_TB
- en: '| `runqlat-bpfcc` | Time a task spends waiting on a runqueue for it''s turn
    to run on the processor |'
  prefs: []
  type: TYPE_TB
- en: '| `runqslower-bpfcc` | (read as runqueue slower); time a task spends waiting
    on a runqueue for it''s turn to run on the processor, showing only those threads
    that exceed a given threshold, which is 10 ms by default (can be tuned by passing
    the time threshold as a parameter, in microseconds); in effect, you can see which
    tasks face (relatively) long scheduling delays |'
  prefs: []
  type: TYPE_TB
- en: '| `runqlen-bpfcc` | Shows scheduler runqueue length + occupancy (number of
    threads currently enqueued, waiting to run) |'
  prefs: []
  type: TYPE_TB
- en: The tools can also provide these metrics on a per-task basis, for every process
    on the system or even by PID namespace (for container analysis; of course, these
    options depend on the tool in question). Do look up more details (and even example
    usage!) from the man pages (section 8) on these tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are even more [e]BPF frontends related to scheduling: `cpudist- cpudist-bpfcc`, `cpuunclaimed-bpfcc`,
    `offcputime-bpfcc`, `wakeuptime-bpfcc`, and so on. See the *Further reading* section
    for resources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, there you are: by now, you''re able to not only understand but even measure
    system latencies (via both the `cyclictest` app and a few modern BPF tools).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We close this chapter with a few miscellaneous, yet useful small (kernel space)
    routines to check out:'
  prefs: []
  type: TYPE_NORMAL
- en: '`rt_prio()`: Given the priority as a parameter, returns a Boolean to indicate
    whether it''s a real-time task or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rt_task()`: Based on the priority value of the task, given the task structure
    pointer as a parameter, returns a Boolean to indicate whether it''s a real-time
    task or not (a wrapper over `rt_prio()`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`task_is_realtime()`: Similar, but based on the scheduling policy of the task.
    Given the task structure pointer as a parameter, returns a Boolean to indicate
    whether it''s a real-time task or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this, our second chapter on CPU scheduling on the Linux OS, you have learned
    several key things. Among them, you learned how to visualize kernel flow with
    powerful tools such as LTTng and the Trace Compass GUI, as well as with the `trace-cmd(1)`
    utility, a convenient frontend to the kernel's powerful Ftrace framework. You
    then saw how to programatically query and set any thread's CPU affinity mask.
    This naturally led to a discussion on how you can programmatically query and set
    any thread's scheduling policy and priority. The whole notion of being "completely
    fair" (via the CFS implementation) was brought into question, and some light was
    shed on the elegant solution called cgroups. You even learned how to leverage
    the cgroups v2 CPU controller to allocate CPU bandwidth as desired to processes
    in a sub-group. We then understood that though Linux is a GPOS, an RTL patchset
    very much exists, which, once applied and the kernel is configured and built,
    has you running Linux as a true hard real-time system, an RTOS.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned how to measure latencies on the system, via both the cyclictest
    app as well as a few modern BPF tools. We even tested with cyclictest on a Raspberry
    Pi 3 device, measuring and contrasting them on an RTL and a standard kernel.
  prefs: []
  type: TYPE_NORMAL
- en: That's quite a bit! Do take the time to properly understand the material, and
    work on it in a hands-on fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we conclude, here is a list of questions for you to test your knowledge
    regarding this chapter''s material: [https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions).
    You will find some of the questions answered in the book''s GitHub repo: [https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn).'
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To help you delve deeper into the subject with useful materials, we provide
    a rather detailed list of online references and links (and at times, even books)
    in a Further reading document in this book's GitHub repository. The *Further reading*
    document is available here: [https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md).
  prefs: []
  type: TYPE_NORMAL
