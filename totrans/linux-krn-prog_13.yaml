- en: The CPU Scheduler - Part 2
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: CPU调度程序-第2部分
- en: In this, our second chapter on the Linux kernel CPU scheduler, we continue our
    coverage from the previous chapter. In the preceding chapter, we covered several
    key areas regarding the workings (and visualization) of the CPU scheduler on the
    Linux OS. This included topics on what exactly the KSE on Linux is, the POSIX
    scheduling policies that Linux implements, using `perf` to see the scheduler flow,
    and how the design of the modern scheduler is based upon modular scheduling classes. We
    also covered how to query any thread's scheduling policy and priority (using a
    couple of command line utilities), and delved deeper into the internal workings
    of the OS scheduler.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第二章中，我们继续讨论Linux内核CPU调度程序，延续了上一章的内容。在上一章中，我们涵盖了关于Linux操作系统CPU调度程序工作（和可视化）的几个关键领域。这包括关于Linux上的KSE是什么，Linux实现的POSIX调度策略，使用`perf`来查看调度程序流程，以及现代调度程序设计是基于模块化调度类的。我们还介绍了如何查询任何线程的调度策略和优先级（使用一些命令行实用程序），并深入了解了操作系统调度程序的内部工作。
- en: 'With this background in place, we''re now ready to explore more on the CPU
    scheduler on Linux; in this chapter, we shall cover the following areas:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些背景，我们现在准备在Linux上更多地探索CPU调度程序；在本章中，我们将涵盖以下领域：
- en: Visualizing the flow with LTTng and `trace-cmd`
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LTTng和`trace-cmd`可视化流程
- en: Understanding, querying, and setting the CPU affinity mask
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解、查询和设置CPU亲和性掩码
- en: Querying and setting a thread's scheduling policy and priority
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询和设置线程的调度策略和优先级
- en: CPU bandwidth control with cgroups
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用cgroups控制CPU带宽
- en: Converting mainline Linux into an RTOS
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将主线Linux转换为RTOS
- en: Latency and its measurement
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 延迟及其测量
- en: We do expect that you've read (or have the equivalent knowledge of) the previous
    chapter before tackling this one.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期望您在阅读本章之前已经阅读过（或具有相应的知识）之前的章节。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: I assume you have gone through [Chapter 1](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml), *Kernel
    Workspace Setup*, and have appropriately prepared a guest **Virtual Machine**
    (**VM**) running Ubuntu 18.04 LTS (or a later stable release) and installed all
    the required packages. If not, I highly recommend you do this first.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设您已经阅读了（或具有相应的知识）之前的章节[第1章](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml) *内核工作空间设置*，并已经适当准备了一个运行Ubuntu
    18.04 LTS（或更高版本）的客户**虚拟机**（**VM**）并安装了所有必需的软件包。如果没有，我强烈建议您首先这样做。
- en: To get the most out of this book, I strongly recommend you first set up the
    workspace environment, including cloning this book's GitHub repository for the
    code, and work on it in a hands-on fashion. The repository can be found here: [https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用本书，我强烈建议您首先设置工作环境，包括克隆本书的GitHub存储库以获取代码，并进行实际操作。存储库可以在这里找到：[https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming)。
- en: Visualizing the flow with LTTng and trace-cmd
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LTTng和trace-cmd可视化流程
- en: 'In the previous chapter, we saw how we can visualize the flow of threads across
    the processor(s) with `perf` (and a few alternatives). Now, we proceed to do so
    with more powerful, more visual profiling tools: with LTTng (and the Trace Compass
    GUI) and with `trace-cmd` (an Ftrace frontend and the KernelShark GUI).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了如何使用`perf`（和一些替代方案）可视化线程在处理器上的流动。现在，我们将使用更强大、更直观的性能分析工具来做到这一点：使用LTTng（和Trace
    Compass GUI）以及`trace-cmd`（一个Ftrace前端和KernelShark GUI）。
- en: Do note that the intent here is to introduce you to these powerful tracing technologies
    only; we do not have the scope nor space required to do full justice to these
    topics.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里的意图是仅介绍您这些强大的跟踪技术；我们没有足够的范围或空间来充分涵盖这些主题。
- en: Visualization with LTTng and Trace Compass
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LTTng和Trace Compass进行可视化
- en: The **Linux Trace Toolkit Next Generation** (**LTTng**) is a set of open source
    tools enabling you to simultaneously trace both user and kernel space. A bit ironically,
    tracing the kernel is easy, whereas tracing user space (apps, libraries, and even
    scripts) requires the developer to manually insert instrumentation (so-called
    tracepoints) into the application (the tracepoint instrumentation for the kernel
    is supplied by LTTng as kernel modules). The high-quality LTTng documentation
    is available online here: [https://lttng.org/docs/v2.12/](https://lttng.org/docs/v2.12/)
    (covering version 2.12 as of the time of writing).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**Linux Trace Toolkit Next Generation**（**LTTng**）是一组开源工具，使您能够同时跟踪用户空间和内核空间。有点讽刺的是，跟踪内核很容易，而跟踪用户空间（应用程序、库甚至脚本）需要开发人员手动将仪器插入应用程序（所谓的tracepoints）（内核的tracepoint仪器由LTTng作为内核模块提供）。高质量的LTTng文档可以在这里在线获得：[https://lttng.org/docs/v2.12/](https://lttng.org/docs/v2.12/)（截至撰写本文时，覆盖版本2.12）。'
- en: 'We do not cover the installation of LTTng here; the details are available at [https://lttng.org/docs/v2.12/#doc-installing-lttng](https://lttng.org/docs/v2.12/#doc-installing-lttng).
    Once installed (it''s kind of heavy – on my native x86_64 Ubuntu system, there
    are over 40 kernel modules loaded up pertaining to LTTng!), using LTTng - for
    a system-wide kernel session as we do here - is easy and is performed in two distinct
    stages: recording, followed by data analysis; these steps follow. (As this book
    is focused on kernel development, we don''t cover using LTTng to trace user space
    apps.)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不涵盖LTTng的安装；详细信息可在[https://lttng.org/docs/v2.12/#doc-installing-lttng](https://lttng.org/docs/v2.12/#doc-installing-lttng)找到。一旦安装完成（它有点庞大-在我的本机x86_64
    Ubuntu系统上，有超过40个与LTTng相关的内核模块加载！），使用LTTng-就像我们在这里做的系统范围内的内核会话-是容易的，并且分为两个明显的阶段：记录，然后是数据分析；这些步骤如下。（由于本书专注于内核开发，我们不涵盖使用LTTng跟踪用户空间应用程序。）
- en: Recording a kernel tracing session with LTTng
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用LTTng记录内核跟踪会话
- en: 'You can record a system-wide kernel tracing session as follows (here, we deliberately
    keep the discussion as simple as possible):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照以下方式记录系统范围内的内核跟踪会话（在这里，我们故意保持讨论尽可能简单）：
- en: 'Create a new session and set the output directory to `<dir>` for saving tracing
    metadata:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新会话，并将输出目录设置为`<dir>`以保存跟踪元数据：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Simply enable all kernel events (can lead to a large amount of tracing metadata
    being generated though):'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只需启用所有内核事件（可能会导致生成大量跟踪元数据）：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Start recording a "kernel session":'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始记录“内核会话”：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Allow some time to elapse (the longer you trace for, the more the disk space
    that's used by the tracing metadata). During this period, all kernel activity
    is being recorded by LTTng.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 允许一些时间流逝（您跟踪的时间越长，跟踪元数据使用的磁盘空间就越多）。在此期间，LTTng正在记录所有内核活动。
- en: 'Stop recording:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止记录：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Destroy the session; don''t worry, this does not delete the tracing metadata:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 销毁会话；不用担心，这不会删除跟踪元数据：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: All the preceding commands should be run with admin privileges (or equivalent).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所有前面的命令都应该以管理员权限（或等效权限）运行。
- en: I have a few wrapper scripts to perform tracing with (LTTng, Ftrace, `trace-cmd`)
    at [https://github.com/kaiwan/L5_debug_trg/tree/master/kernel_debug/tracing](https://github.com/kaiwan/L5_debug_trg/tree/master/kernel_debug/tracing);
    do check them out.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一些包装脚本可以进行跟踪（LTTng、Ftrace、`trace-cmd`），在[https://github.com/kaiwan/L5_debug_trg/tree/master/kernel_debug/tracing](https://github.com/kaiwan/L5_debug_trg/tree/master/kernel_debug/tracing)中查看。
- en: The tracing metadata files (in the **Common Trace Format** (**CTF**) file format)
    gets saved to the preceding specified output directory.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪元数据文件（以**通用跟踪格式**（**CTF**）文件格式）保存到前面指定的输出目录。
- en: Reporting with a GUI – Trace Compass
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用GUI进行报告 - Trace Compass
- en: The data analysis can be performed in two broad ways – using a CLI-based system
    typically packaged along with LTTng called `babeltrace`, or via a sophisticated
    GUI called **Trace Compass**. The GUI is far more appealing; we only show its
    basic usage here.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析可以通过两种方式进行 - 使用通常与LTTng捆绑在一起的基于CLI的系统`babeltrace`，或者通过一个复杂的GUI称为**Trace
    Compass**。GUI更具吸引力；我们这里只展示了它的基本用法。
- en: 'Trace Compass is a powerful cross-platform GUI application and integrates well
    with Eclipse. In fact, we quote directly from the Eclipse Trace Compass site ([https://projects.eclipse.org/projects/tools.tracecompass](https://projects.eclipse.org/projects/tools.tracecompass)):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Trace Compass是一个功能强大的跨平台GUI应用程序，并且与Eclipse集成得很好。实际上，我们直接引用自Eclipse Trace Compass网站（[https://projects.eclipse.org/projects/tools.tracecompass](https://projects.eclipse.org/projects/tools.tracecompass)）：
- en: '"*Eclipse Trace Compass is an open source application to solve performance
    and reliability issues by reading and analyzing logs or traces of a system. Its
    goal is to provide views, graphs, metrics, and more to help extract useful information
    from traces, in a way that is more user-friendly and informative than huge text
    dumps.*"'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: “*Eclipse Trace Compass是一个开源应用程序，通过读取和分析系统的日志或跟踪来解决性能和可靠性问题。它的目标是提供视图、图形、指标等，以帮助从跟踪中提取有用信息，这种方式比庞大的文本转储更加用户友好和信息丰富。*”
- en: It can be downloaded (and installed) from here: [https://www.eclipse.org/tracecompass/](https://www.eclipse.org/tracecompass/).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以从这里下载（和安装）：[https://www.eclipse.org/tracecompass/](https://www.eclipse.org/tracecompass/)。
- en: Trace Compass minimally requires a **Java Runtime Environment** (**JRE**) to
    be installed as well. I installed one on my Ubuntu 20.04 LTS system with `sudo
    apt install openjdk-14-jre`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Trace Compass最低需要安装**Java Runtime Environment**（**JRE**）。我在我的Ubuntu 20.04 LTS系统上安装了一个，使用`sudo
    apt install openjdk-14-jre`。
- en: 'Once installed, fire up Trace Compass, click on the File | Open Trace menu,
    and navigate to the output directory where you saved the trace metadata for your
    tracing session in the preceding steps. Trace Compass will read the metadata and
    display it visually, along with various perspectives and tool views made available.
    A partial screenshot from our brief system-wide kernel tracing session is shown
    here (*Figure 11.1*); you can literally see the context switch (shown as the `sched_switch` event
    – see the Event type column) from the `gnome-shell` process to the `swapper/1`
    kernel thread (the idle thread running on CPU #1):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，启动Trace Compass，单击“文件”|“打开跟踪”菜单，并导航到您在前面步骤中保存跟踪会话的跟踪元数据的输出目录。Trace Compass将读取元数据并以可视化方式显示，以及提供各种透视图和工具视图。我们的简短系统范围内的内核跟踪会话的部分屏幕截图显示在这里（*图11.1*）；您可以清楚地看到上下文切换（显示为`sched_switch`事件
    - 请参阅事件类型列）从`gnome-shell`进程到`swapper/1`内核线程（在CPU＃1上运行的空闲线程）：
- en: '![](img/1ac935e2-5441-4d2a-ac44-12283d2fadec.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ac935e2-5441-4d2a-ac44-12283d2fadec.png)'
- en: Figure 11.1 – Trace Compass GUI showing a sample kernel tracing session obtained
    via LTTng
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 - Trace Compass GUI显示通过LTTng获得的示例内核跟踪会话
- en: Look carefully at the preceding screenshot (Figure 11.1); in the lower horizontal
    pane, not only do you get to see which kernel function executed, you *also *get
    (under the column labeled Contents) the parameter list along with the value each
    parameter had at the time! This can be very useful indeed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细看前面的屏幕截图（图11.1）；在下方的水平窗格中，不仅可以看到执行的内核函数，*还*可以（在标签为内容的列下）看到每个参数在那个时间点的值！这确实非常有用。
- en: Visualizing with trace-cmd
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用trace-cmd进行可视化
- en: Modern Linux kernels (from 2.6.27) embed a very powerful tracing engine called **Ftrace**.
    Ftrace is the rough kernel equivalent of the user space `strace(1)` utility, but
    that would be short-selling it! Ftrace allows the sysad (or developer, tester,
    or anyone with root privileges really) to literally look under the hood, seeing
    every single function being executed in kernel space, who (which thread) executed
    it, how long it ran for, what APIs it invoked, with interrupts (hard and soft)
    included as they occur, various types of latency measurements, and more. You can
    use Ftrace to learn about how system utilities, applications, and the kernel actually
    work, as well as to perform deep tracing at the level of the OS.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现代Linux内核（从2.6.27开始）嵌入了一个非常强大的跟踪引擎，称为**Ftrace**。Ftrace是用户空间`strace(1)`实用程序的粗糙内核等效物，但这样说有点贬低它了！Ftrace允许系统管理员（或开发人员、测试人员，或任何具有root权限的人）直接查看内核空间中执行的每个函数，执行它的是谁（哪个线程），运行时间有多长，它调用了哪些API，包括发生的中断（硬中断和软中断），各种类型的延迟测量等等。您可以使用Ftrace了解系统实用程序、应用程序和内核的实际工作原理，以及在操作系统级别执行深度跟踪。
- en: Here, in this book, we refrain from delving into the depths of raw Ftrace usage
    (as it deviates from the subject at hand); instead, it is just quicker and easier
    to use a user space wrapper over Ftrace, a more convenient interface to it, called `trace-cmd(1)` (again,
    we only scratch the surface, showing an example of how `trace-cmd` can be used).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们不深入研究原始Ftrace的用法（因为这偏离了手头的主题）；相反，使用一个用户空间包装器覆盖Ftrace，一个更方便的接口，称为`trace-cmd(1)`，只是更快更容易（再次强调，我们只是浅尝辄止，展示了`trace-cmd`的一个示例）。
- en: For Ftrace details and usage, the interested reader will find this kernel document
    useful: [https://www.kernel.org/doc/Documentation/trace/ftrace.rst](https://www.kernel.org/doc/Documentation/trace/ftrace.rst).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Ftrace的详细信息和用法，感兴趣的读者会发现这个内核文档有用：[https://www.kernel.org/doc/Documentation/trace/ftrace.rst](https://www.kernel.org/doc/Documentation/trace/ftrace.rst)。
- en: 'Most modern Linux distros will allow the installation of `trace-cmd` via their
    package management system; on Ubuntu, for example, `sudo apt install trace-cmd` is
    sufficient to install it (if required for a custom Linux on, say, ARM, you can
    always cross-compile it from the source on its GitHub repository: [https://git.kernel.org/pub/scm/linux/kernel/git/rostedt/trace-cmd.git/tree/](https://git.kernel.org/pub/scm/linux/kernel/git/rostedt/trace-cmd.git/tree/)).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代Linux发行版都允许通过其软件包管理系统安装`trace-cmd`；例如，在Ubuntu上，`sudo apt install trace-cmd`就足以安装它（如果需要在自定义的Linux上，比如ARM，您总是可以从其GitHub存储库上的源代码进行交叉编译：[https://git.kernel.org/pub/scm/linux/kernel/git/rostedt/trace-cmd.git/tree/](https://git.kernel.org/pub/scm/linux/kernel/git/rostedt/trace-cmd.git/tree/)）。
- en: Let's perform a simple `trace-cmd` session; first, we shall record data samples while
    the `ps(1)` utility runs; then we shall examine the captured data both via the
    `trace-cmd report` **Command-Line Interface** (**CLI**) as well as a GUI frontend
    called KernelShark (it's in fact part of the `trace-cmd` package).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一个简单的`trace-cmd`会话；首先，我们将在运行`ps(1)`实用程序时记录数据样本；然后，我们将通过`trace-cmd report`**命令行界面**（CLI）以及一个名为KernelShark的GUI前端来检查捕获的数据（它实际上是`trace-cmd`包的一部分）。
- en: Recording a sample session with trace-cmd record
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用trace-cmd record记录一个示例会话
- en: 'In this section, we record a session with `trace-cmd(1)`; we use a few (of
    the many possible) option switches to `trace-cmd  record`; as usual, the man pages
    on `trace-cmd-foo(1)` (substitute `foo`with `check-events`, `hist`, `record`,
    `report`, `reset`, and so on) are very useful for finding various option switches
    and usage details. A few of the useful option switches particularly for `trace-cmd
    record` are as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用`trace-cmd(1)`记录一个会话；我们使用了一些（许多可能的）选项开关来记录`trace-cmd`；通常，`trace-cmd-foo(1)`（用`check-events`、`hist`、`record`、`report`、`reset`等替换`foo`）的man页面非常有用，可以找到各种选项开关和用法详情。特别适用于`trace-cmd
    record`的一些有用选项开关如下：
- en: '`-o`: Specifies the output filename (if not specified, it defaults to `trace.dat`).'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-o`：指定输出文件名（如果未指定，则默认为`trace.dat`）。'
- en: '`-p`: The plugin to use, one of `function`, `function_graph`, `preemptirqsoff`,
    `irqsoff`, `preemptoff`, and `wakeup`; here, in our small demo, we use the `function-graph` plugin
    (several other plugins can be configured in the kernel as well).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-p`：要使用的插件之一，如`function`、`function_graph`、`preemptirqsoff`、`irqsoff`、`preemptoff`和`wakeup`；在我们的小型演示中，我们使用了`function-graph`插件（内核中还可以配置其他几个插件）。'
- en: '`-F`: The command (or app) to trace; this is very useful, allowing you to specify
    exactly which process (or thread) to exclusively trace (otherwise, tracing all
    threads can result in a lot of noise when attempting to decipher the output);
    similarly, you can use the `-P` option switch to specify the PID to trace.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -F：要跟踪的命令（或应用程序）；这非常有用，可以让您精确指定要独占跟踪的进程（或线程）（否则，跟踪所有线程在尝试解密输出时可能会产生大量噪音）；同样，您可以使用`-P`选项开关来指定要跟踪的PID。
- en: '`-r priority`: Runs the `trace-cmd` threads at the real-time priority specified
    (the typical range being 1 to 99; we shall cover querying and setting a thread''s
    scheduling policy and priority shortly); this gives a better bet on `trace-cmd`
    being able to capture samples as required.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-r priority`：以指定的实时优先级运行`trace-cmd`线程（典型范围为1到99；我们将很快介绍查询和设置线程的调度策略和优先级）；这样可以更好地捕获所需的样本。'
- en: 'Here, we run a quick demo: we run `ps -LA`; while it runs, all kernel traffic
    it generates is (exclusively) captured by `trace-cmd` via it''s `record` functionality
    (we employ the `function-graph` plugin):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们进行了一个快速演示：我们运行`ps -LA`；在运行时，所有内核流量都（独占地）由`trace-cmd`通过其`record`功能捕获（我们使用了`function-graph`插件）：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: A rather large data file results (as we captured all events and did a `ps -LA` displaying
    all threads alive, it took a while, and thus the data samples captured are large-ish.
    Also realize that by default, kernel tracing is performed across all CPUs on the
    system; you can change this via the `-M cpumask` option.)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个相当大的数据文件（因为我们捕获了所有事件并且进行了`ps -LA`显示所有活动线程，所以花了一些时间，因此捕获的数据样本相当大。还要意识到，默认情况下，内核跟踪是在系统上的所有CPU上执行的；您可以通过`-M
    cpumask`选项进行更改）。
- en: 'In the preceding example, we captured all events. The `-e` option switch to
    `trace-cmd(1)` allows you to specify a class of events to trace; for example,
    to trace the `ping(1)` utility and capture only events related to networking and
    kernel memory, run the following command:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，我们捕获了所有事件。`-e`选项开关允许您指定要跟踪的事件类别；例如，要跟踪`ping(1)`实用程序并仅捕获与网络和内核内存相关的事件，请运行以下命令：
- en: '`sudo trace-cmd record -e kmem -e net -p function_graph -F ping -c1 packtpub.com`.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`sudo trace-cmd record -e kmem -e net -p function_graph -F ping -c1 packtpub.com`。'
- en: Reporting and interpretation with trace-cmd report (CLI)
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用trace-cmd report（CLI）进行报告和解释
- en: 'Continuing from the preceding section, on the command line, we can get a (very!)
    detailed report of what occurred within the kernel when the `ps` process ran;
    use the `trace-cmd report` command to see this. We also pass along the `-l` option
    switch: it displays the report in what is referred to as Ftrace''s **latency format**, revealing
    many useful details; the `-i` switch of course specifies the input file to use:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now it gets very interesting! We show a few partial screenshots of the (huge)
    output file that we opened with `vim(1)`; first we have the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d48a8129-fe10-4ecf-bebe-95c94684e5bd.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – A partial screenshot showing the output of the trace-cmd report
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at Figure 11.2; the call to the kernel API, `schedule()`, is deliberately
    highlighted and in bold font (*Figure 11.2*, on line `785303`!). In order to interpret
    everything on this line, we must understand each (white-space delimited) column;
    there are eight of them:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Column 1: Here, it''s just the line number in the file that vim shows (let''s
    ignore it).'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column 2: This is the process context that invoked this function (the function
    itself is in column #8); clearly, here, the process is `ps-PID` (its PID is appended
    after a `-` character).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column 3: useful! A series of five characters, which shows up in **latency
    format** (we used the `-l` option switch to `trace-cmd record`, remember!); this
    (in our preceding case, it''s `2.N..`) is very useful and can be interpreted as
    follows:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The very first character is the CPU core it was running upon (so here it was
    core #2) (note that, as a general rule, besides the first one, if the character
    is a period `.`, it means it''s zero or not applicable).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second character represents the hardware interrupt status:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.` implies the default hardware interrupts are enabled.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`d` implies hardware interrupts are currently disabled.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The third character represents the `need_resched` bit (we explained this in
    the previous chapter, in the *When does the scheduler run?* section):'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.` implies it''s cleared.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`N` implies it''s set (which implies that the kernel requires rescheduling
    to be performed ASAP!).'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The fourth character has meaning only when an interrupt is in progress, otherwise,
    it is merely a `.`, implying we are in a process context; if an interrupt is in
    progress – implying we''re in an interrupt context – its value is one of the following:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`h` implies we are executing in a hardirq (or top half) interrupt context.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`H` implies we are executing in a hardirq that occurred within a softirq.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s` implies we are executing in a softirq (or bottom half) interrupt context.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fifth character represents the preemption count or depth; if it's a `.`,
    it's zero, implying the kernel is running in a preemptible state; if nonzero,
    an integer number shows up, implying that many kernel-level lock(s) have been
    taken, forcing the kernel into a non-preemptible state.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By the way, the output is very similar to Ftrace''s raw output except that
    in the case of raw Ftrace, we would see only four characters – the first one (the
    CPU core number) does *not* show up here; it shows up as the leftmost column instead;
    here''s a partial screenshot of the raw Ftrace (not `trace-cmd`) latency format:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/eacd0313-28f8-4d88-92ae-6ad938231293.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – A partial screenshot focused on raw Ftrace's four-character latency
    format (fourth field)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot was culled directly from the raw Ftrace output.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'So, interpreting our example for the call to `schedule()`, we can see that
    the characters are `2.N..` implying that the process `ps` with PID `22922` was
    executing on CPU core #2 in a process context (no interrupts) and the `need-resched`
    (technically, `thread_info.flags:TIF_NEED_RESCHED`) bit was set (indicating the
    need for a reschedule ASAP!).'
  id: totrans-88
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: (Back to the remaining columns in Figure 11.2 now)
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column 4: Timestamp in *seconds:microseconds* format.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Column 5: The name of the event that occurred (here, as we''ve used the `function_graph` plugin,
    it will be either `funcgraph_entry` or `fungraph_exit`, implying function entry
    or exit respectively).'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第5列：发生的事件的名称（在这里，我们使用了`function_graph`插件，它将是`funcgraph_entry`或`fungraph_exit`，分别表示函数的进入或退出）。
- en: 'Column 6 [optional]: The duration of the preceding function call with the time
    taken shown along with its unit (us = microseconds); a prefix character is used
    to denote whether the function execution took a long time (we simply treat it
    as part of this column); from the kernel Ftrace documentation (here: [https://www.kernel.org/doc/Documentation/trace/ftrace.rst](https://www.kernel.org/doc/Documentation/trace/ftrace.rst)),
    we have this:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第6列[可选]：前一个函数调用的持续时间，显示了所花费的时间及其单位（us = 微秒）；前缀字符用于表示函数执行时间很长（我们简单地将其视为此列的一部分）；来自内核Ftrace文档（这里：[https://www.kernel.org/doc/Documentation/trace/ftrace.rst](https://www.kernel.org/doc/Documentation/trace/ftrace.rst)），我们有以下内容：
- en: '`+`, which implies that a function surpassed 10 microseconds'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`+`，这意味着一个函数超过了10微秒'
- en: '`!`, which implies that a function surpassed 100 microseconds'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`!`，这意味着一个函数超过了100微秒'
- en: '`#`, which implies that a function surpassed 1,000 microseconds'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#`，这意味着一个函数超过了1,000微秒'
- en: '`*`, which implies that a function surpassed 10 milliseconds'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*`，这意味着一个函数超过了10毫秒'
- en: '`@`, which implies that a function surpassed 100 milliseconds'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`@`，这意味着一个函数超过了100毫秒'
- en: '`$`, which implies that a function surpassed 1 second'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$`，这意味着一个函数超过了1秒'
- en: 'Column 7: Just the separator character `|`.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第7列：只是分隔符`|`。
- en: 'Column 8: The extreme-right column is the name of the kernel function being
    executed; an open brace on the right, `{`, implies the function is invoked just
    now; the column with only a close brace, `}`, implies the preceding function''s
    end (matching the open brace).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第8列：极右列是正在执行的内核函数的名称；右边的开括号`{`表示刚刚调用了该函数；只有一个闭括号`}`的列表示前一个函数的结束（与开括号匹配）。
- en: This level of detail can be extremely valuable in both troubleshooting kernel
    (and even user space) issues, and understanding the flow of the kernel in great
    detail.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这种详细程度在排除内核（甚至用户空间）问题和深入了解内核流程方面非常有价值。
- en: 'When `trace-cmd record` is used without the `-p function-graph` option switch,
    we do lose the nicely indented function call graph-like output, but we do gain
    something as well: you will now see all function parameters along with their runtime
    values to the right of every single function call! A truly valuable aid at times.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`trace-cmd record`而没有使用`-p function-graph`选项开关时，我们失去了漂亮的缩进函数调用图形式的输出，但我们也得到了一些东西：现在你将看到每个函数调用右侧的所有函数参数及其运行时值！这在某些时候确实是一个非常有价值的辅助工具。
- en: 'I can''t resist showing another snippet from the same report – another interesting
    example with regard to the very things we learned about how scheduling classes
    work on modern Linux (covered in the previous chapter); this actually shows up
    here in the `trace-cmd` output:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我忍不住想展示同一份报告中的另一个片段 - 另一个关于我们在现代Linux上学到的调度类如何工作的有趣例子（在上一章中介绍过）；这实际上在`trace-cmd`输出中显示出来了：
- en: '![](img/dcdea06f-b228-4418-acb3-399effdf5053.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![ ](img/dcdea06f-b228-4418-acb3-399effdf5053.png)'
- en: Figure 11.4 – A partial screenshot of trace-cmd report output
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 - `trace-cmd`报告输出的部分截图
- en: 'Interpret the preceding screenshot (*Figure 11.4*) closely: the second line
    (with the right-most function name column in bold font, as are the two functions
    immediately following it) shows that the `pick_next_task_stop()` function was
    invoked; this implies that a schedule occurred and the core scheduling code within
    the kernel went through its routine – it walks the linked list of scheduling classes
    in priority order, asking each whether it has a thread to schedule; if they do,
    the core scheduler context switches to it (as was explained in some detail in the
    previous chapter, in the *Modular scheduling classes* section).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细解释前面的截图（*图11.4*）：第二行（右侧函数名列为粗体字体，紧随其后的两个函数也是如此）显示了`pick_next_task_stop()`函数被调用；这意味着发生了一次调度，内核中的核心调度代码按照优先级顺序遍历调度类的链表，询问每个类是否有要调度的线程；如果有，核心调度程序上下文切换到它（正如在前一章中详细解释的那样，在*模块化调度类*部分）。
- en: 'In Figure 11.4, you literally see this happen: the core scheduling code asks
    the **stop-sched** (**SS**), **deadline** (**DL**), and **real-time** (**RT**)
    classes whether they have any thread that wants to run, by invoking, in turn,
    the `pick_next_task_stop()`, `pick_next_task_dl()`, and `pick_next_task_rt()` functions.
    Apparently, for all of them, the answer is no, as the next function to run is
    that of the fair (CFS) class (why doesn''t the `pick_next_task_fair()` function
    show up in the preceding screenshot then? Ah, again, that''s code optimization
    for you: the kernel developers understand that this being the likely case, they
    check for it and directly invoke the fair class code most of the time).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在图11.4中，你真的看到了这种情况发生：核心调度代码询问**stop-sched**（**SS**）、**deadline**（**DL**）和**real-time**（**RT**）类是否有任何想要运行的线程，依次调用`pick_next_task_stop()`、`pick_next_task_dl()`和`pick_next_task_rt()`函数。显然，对于所有这些类，答案都是否定的，因为接下来要运行的函数是公平（CFS）类的函数（为什么`pick_next_task_fair()`函数在前面的截图中没有显示呢？啊，这又是代码优化：内核开发人员知道这是可能的情况，他们会直接调用公平类代码大部分时间）。
- en: What we've covered here on the powerful Ftrace framework and the `trace-cmd`
    utility is just the basics; I urge you to look up the man pages on `trace-cmd-<foo>`(where
    `<foo>` is replaced with `record`, `report`, and so on)there are typically good
    examples shown there. Also, there are several very well-written articles on Ftrace
    (and `trace-cmd`) – please refer to the *Further reading *section for them.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里介绍的强大的Ftrace框架和`trace-cmd`实用程序只是基础；我建议你查阅`trace-cmd-<foo>`（其中`<foo>`被替换为`record`、`report`等）的man页面，那里通常会显示很好的示例。此外，关于Ftrace（和`trace-cmd`）还有一些非常好的文章
    - 请参考*进一步阅读*部分。
- en: Reporting and interpretation with a GUI frontend
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用GUI前端进行报告和解释
- en: 'More good news: the `trace-cmd` toolset includes a GUI frontend, for more human-friendly
    interpretation and analysis, called KernelShark (though, in my opinion, it isn''t
    as full-featured as Trace Compass is). Installing it on Ubuntu/Debian is as simple
    as doing `sudo apt install kernelshark`.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Below, we run `kernelshark`, passing the trace data file output from our preceding `trace-cmd` record
    session as the parameter to it (adjust the parameter to KernelShark to refer to
    the location where you''ve saved the tracing metadata):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'A screenshot of KernelShark running with the preceding trace data is shown
    here:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1406860-f2ad-4331-a266-5727a90114de.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – A screenshot of the kernelshark GUI displaying the earlier-captured
    data via trace-cmd
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Interesting; the `ps` process ran on CPU #2 (as we saw with the CLI version
    previously). Here, we also see the functions executed in the lower tiled horizontal
    window pane; as an example, we have highlighted the entry for `pick_next_task_fair()`.
    The columns are quite obvious, with the `Latency` column format (four characters,
    not five) interpreted as we explained previously for (raw) Ftrace.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '**Quick quiz**: What does the Latency format field `dN..`, seen in Figure 11.5, imply?'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer: It implies that, currently, right now, we have the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'First column `d`: Hardware interrupts are disabled.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second column `N`: The `need_resched` bit is set (implying the need to invoke
    the scheduler at the next available scheduling opportunity point).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Third column `.`: The kernel `pick_next_task_fair()` function''s code is running
    in a process context (the task being `ps` with a PID of `22545`; remember, Linux
    is a monolithic kernel!).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fourth column `.`: The preemption depth (count) is zero, implying the kernel
    is in a preemptible state.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have covered using these powerful tools to help generate and visualize
    data related to kernel execution and scheduling, let''s move on to another area:
    in the next section, we focus on another important aspect – what exactly a thread''s
    CPU affinity mask is, and how you can programmatically (and otherwise) get/set
    it.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Understanding, querying, and setting the CPU affinity mask
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The task structure, the root data structure containing several dozen thread
    attributes, has a few attributes directly pertaining to scheduling: the priority
    (the *nice* as well as the RT priority values), the scheduling class structure
    pointer, the runqueue the thread is on (if any), and so on.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Among these is an important member, the **CPU affinity bitmask** (the actual
    structure member is `cpumask_t cpus_allowed`). This also tells you that the CPU
    affinity bitmask is a per-thread quantity; this makes sense - the KSE on Linux
    is a thread, after all. It's essentially an array of bits, each bit representing
    a CPU core (with sufficient bits available within the variable); if the bit corresponding
    to a core is set (`1`), the thread is allowed to be scheduled on and execute on
    that core; if cleared (`0`), it's not.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: By default, all the CPU affinity mask bits are set; thus, the thread can run
    on any core. For example, on a box with (the OS seeing) four CPU cores, the default
    CPU affinity bitmask for each thread would be binary `1111` (`0xf`). (Glance at
    Figure 11.6 to see how the CPU affinity bitmask looks, conceptually speaking.)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'At runtime, the scheduler decides which core the thread will actually run upon.
    In fact, think about it, it''s really implicit: by default, each CPU core has
    a runqueue associated with it; every runnable thread will be on a single CPU runqueue;
    it''s thus eligible to run and by default runs on the CPU that it''s runqueue
    represents. Of course, the scheduler has a load balancer component that can migrate
    threads to other CPU cores (runqueues, really) as the need arises (kernel threads
    called `migration/n`, where `n` is the core number assist in this task).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: The kernel does expose APIs to user space (system calls, of course, `sched_{s,g}etaffinity(2)` and
    their `pthread` wrapper library APIs), which allows an application to affine,
    or associate, a thread (or multiple threads) to particular CPU cores as it sees
    fit (and by the same logic, we can do this within the kernel as well for any given
    kernel thread). For example, setting the CPU affinity mask to `1010` binary, which
    equals `0xa` in hexadecimal, implies that the thread can execute *only* upon CPU
    cores one and three (counting starts from zero).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 内核确实向用户空间暴露了API（系统调用，当然，`sched_{s,g}etaffinity(2)`及其`pthread`包装库API），这允许应用程序根据需要将线程（或多个线程）关联到特定的CPU核心上（按照相同的逻辑，我们也可以在内核中为任何给定的内核线程执行此操作）。例如，将CPU亲和性掩码设置为`1010`二进制，相当于十六进制的`0xa`，意味着该线程只能在CPU核心一和三上执行（从零开始计数）。
- en: 'A key point: though you can manipulate the CPU affinity mask, the recommendation
    is to avoid doing so; the kernel scheduler understands the CPU topography in detail
    and can best load-balance the system.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键点：尽管您可以操纵CPU亲和性掩码，但建议避免这样做；内核调度程序详细了解CPU拓扑，并且可以最佳地平衡系统负载。
- en: 'Having said that, explicitly setting the CPU affinity mask of a thread can
    be beneficial due to the following reasons:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，显式设置线程的CPU亲和性掩码可能是有益的，原因如下：
- en: Cache invalidation (and thus unpleasant cache "bouncing") can be greatly reduced
    by ensuring a thread always runs on the same CPU core.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过确保线程始终在同一CPU核心上运行，可以大大减少缓存失效（从而减少不愉快的缓存“跳动”）。
- en: Thread migration costs between cores are effectively eliminated.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心之间的线程迁移成本被有效地消除。
- en: CPU reservation—a strategy to bestow the core(s) exclusively to one thread by
    guaranteeing all other threads are explicitly not allowed to execute upon that
    core.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU保留——一种策略，通过保证所有其他线程明确不允许在该核心上执行，将核心（或核心）专门分配给一个线程。
- en: The first two are useful in some corner cases; the third one, CPU reservation,
    tends to be a technique used in some time-critical real-time systems where the
    cost of doing so is justified. Performing CPU reservation in practice is quite
    difficult to do though,  requiring OS-level intervention at (every!) thread creation;
    the cost might be prohibitive. For this reason, this is actually implemented by
    specifying that a certain CPU (or more) be *isolated* from all tasks; the Linux
    kernel provides a kernel parameter, `isolcpus`, for this very job.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 前两者在某些特殊情况下很有用；第三个，CPU保留，往往是在一些时间关键的实时系统中使用的一种技术，其成本是合理的。但实际上，进行CPU保留是相当困难的，需要在（每个！）线程创建时进行操作；成本可能是禁止的。因此，这实际上是通过指定某个CPU（或更多）从所有任务中*隔离*出来来实现的；Linux内核提供了一个内核参数`isolcpus`来完成这项工作。
- en: 'In this regard, we quote directly from the man page on the `sched_{s,g}etaffinity(2)` system
    calls:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，我们直接引用了`sched_{s,g}etaffinity(2)`系统调用的man页面上的内容：
- en: The isolcpus boot option can be used to isolate one or more CPUs at boot time,
    so that no processes are scheduled onto those CPUs. Following the use of this
    boot option, the only way to schedule processes onto the isolated CPUs is via sched_setaffinity() or
    the cpuset(7) mechanism. For further information, see the kernel source file Documentation/admin-guide/kernel-parameters.txt.
    As noted in that file, isolcpus is the preferred mechanism of isolating CPUs (versus
    the alternative of manually setting the CPU affinity of all processes on the system).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: isolcpus引导选项可用于在引导时隔离一个或多个CPU，以便不会安排任何进程到这些CPU上运行。在使用此引导选项之后，将进程调度到被隔离的CPU的唯一方法是通过sched_setaffinity()或cpuset(7)机制。有关更多信息，请参阅内核源文件Documentation/admin-guide/kernel-parameters.txt。如该文件中所述，isolcpus是隔离CPU的首选机制（与手动设置系统上所有进程的CPU亲和性的替代方案相比）。
- en: Note, though, the previously mentioned `isolcpus` kernel parameter is now considered
    deprecated; it's preferable to use the cgroups `cpusets` controller instead (`cpusets`
    is a cgroup feature or controller; we do have some coverage on cgroups later in
    this chapter, in the *CPU bandwidth control with cgroups* section).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，先前提到的`isolcpus`内核参数现在被认为是不推荐使用的；最好使用cgroups的`cpusets`控制器代替（`cpusets`是一个cgroup特性或控制器；我们稍后在本章中会对cgroups进行一些介绍，在*使用cgroups进行CPU带宽控制*部分）。
- en: We refer you to more details in the kernel parameter documentation (here: [https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt](https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt)),
    specifically under the parameter labeled `isolcpus=`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您在内核参数文档中查看更多详细信息（在此处：[https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt](https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt)），特别是在标记为`isolcpus=`的参数下。
- en: Now that you understand the theory behind it, let's actually write a user space
    C program to query and/or set the CPU affinity mask of any given thread.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经了解了它的理论，让我们实际编写一个用户空间C程序来查询和/或设置任何给定线程的CPU亲和性掩码。
- en: Querying and setting a thread's CPU affinity mask
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询和设置线程的CPU亲和性掩码
- en: 'As a demonstration, we provide a small user space C program to query and set
    a user space process (or thread''s) CPU affinity mask. Querying the CPU affinity
    mask is achieved with the `sched_getaffinity(2)` system call and by setting it
    with its counterpart:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 作为演示，我们提供了一个小型用户空间C程序来查询和设置用户空间进程（或线程）的CPU亲和性掩码。使用`sched_getaffinity(2)`系统调用来查询CPU亲和性掩码，并使用其对应的设置来设置它。
- en: '[PRE8]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'A specialized data type called `cpu_set_t` is what is used to represent the
    CPU affinity bitmask; it''s quite sophisticated: its size is dynamically allocated
    based on the number of CPU cores seen on the system. This CPU mask (of type `cpu_set_t`)
    must first be initialized to zero; the `CPU_ZERO()` macro achieves this (several
    similar helper macros exist; do refer to the man page on `CPU_SET(3)`). The second
    parameter in both the preceding system calls is the size of the CPU set (we simply
    use the `sizeof` operator to obtain it).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand this better, it''s instructive to see a sample run of our code
    (`ch11/cpu_affinity/userspc_cpuaffinity.c`); we run it on a native Linux system
    with 12 CPU cores:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b138bc28-5b26-4aa3-9362-ea201aaa2598.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Our demo user space app showing the CPU affinity mask
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have run the app with no parameters. In this mode, it queries the
    CPU affinity mask of itself (meaning, of the `userspc_cpuaffinity` calling process).
    We print out the bits of the bitmask: as you can clearly see in the preceding
    screenshot, it''s binary `1111 1111 1111` (which is equivalent to `0xfff`), implying
    that by default the process is eligible to run on any of the 12 CPU cores available
    on the system.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: The app detects the number of CPU cores available by running the `nproc(1)`
    utility via the useful `popen(3)` library API. Do note though, that the value
    returned by `nproc` is the number of CPU cores available to the calling process;
    it may be less than the actual number of CPU cores (it's usually the same); the
    number of available cores can be changed in a few ways, the proper way being via
    the cgroup `cpuset` resource controller (we cover some information on cgroups
    later in this chapter).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'The querying code is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Our `disp_cpumask()` function draws the bitmask (we leave it to you to check
    it out).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: If additional parameters are passed – the PID of the process (or thread), as
    the first parameter, and a CPU bitmask, as the second parameter – we then attempt
    to *set* the CPU affinity mask of that process (or thread) to the value passed.
    Of course, changing the CPU affinity bitmask requires you to own the process or
    have root privileges (more correctly, to have the `CAP_SYS_NICE` capability).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick demo: in Figure 11.7, `nproc(1)` shows us the number of CPU cores;
    then, we run our app to query and set our shell process''s CPU affinity mask.
    On a laptop, let''s say that the affinity mask of `bash` is `0xfff` (binary `1111
    1111 1111`) to begin with, as expected; we change it to `0xdae` (binary `1101
    1010 1110`) and query it again to verify the change:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c6f31dd-946f-4559-9c3a-9c8de84d45f4.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Our demo app queries then sets the CPU affinity mask of bash to
    0xdae
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, this is interesting: to begin with, the app correctly detects the number
    of CPU cores available to it as 12; it then queries the (default) CPU affinity
    mask of the bash process (as we pass its PID as the first parameter); it shows
    up, as `0xfff`, as expected. Then, as we''ve also passed a second parameter –
    the bitmask to now set (`0xdae`) – it does so, setting the CPU affinity mask of
    bash to `0xdae`. Now, as the terminal window we''re on is this very same bash
    process, running `nproc` again shows the value as 8, not 12! That''s indeed correct:
    the bash process now has only eight CPU cores available to it. (This is as we
    don''t revert the CPU affinity mask to its original value on exit.)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the relevant code to set the CPU affinity mask:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding code snippet, you can see we first set up the `cpu_set_t` bitmask
    appropriately (by looping over each bit) and then employ the `sched_setaffinity(2)`
    system call to set the new CPU affinity mask on the given `pid`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Using taskset(1) to perform CPU affinity
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Akin to how (in the preceding chapter) we used the convenient user space utility
    program, `chrt(1)` to get (or set) a process'' (or thread''s) scheduling policy
    and/or priority, you can use the user space `taskset(1)` utility to get and/or
    set a given process'' (or thread''s) CPU affinity mask. A couple of quick examples
    follow; note that these examples were run on an x86_64 Linux system with 4 CPU
    cores:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在前一章中使用方便的用户空间实用程序`chrt(1)`来获取（或设置）进程（或线程）的调度策略和/或优先级，您可以使用用户空间`taskset(1)`实用程序来获取和/或设置给定进程（或线程）的CPU亲和性掩码。以下是一些快速示例；请注意，这些示例是在一个具有4个CPU核心的x86_64
    Linux系统上运行的：
- en: 'Use `taskset` to query the CPU affinity mask of systemd (PID 1):'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`taskset`查询systemd（PID 1）的CPU亲和性掩码：
- en: '[PRE11]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Use `taskset` to ensure that the compiler – and its descendants (the assembler
    and linker) – run only on the first two CPU cores; the first parameter to taskset
    is the CPU affinity bitmask (`03` is binary `0011`):'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`taskset`确保编译器及其后代（汇编器和链接器）仅在前两个CPU核心上运行；taskset的第一个参数是CPU亲和性位掩码（`03`是二进制`0011`）：
- en: '[PRE12]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Do look up the man page on `taskset(1)` for complete usage details.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 查阅`taskset(1)`的手册页面以获取完整的使用详情。
- en: Setting the CPU affinity mask on a kernel thread
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在内核线程上设置CPU亲和性掩码
- en: 'As an example, if we want to demonstrate a synchronization technique called
    per-CPU variables, we are required to create two kernel threads and guarantee
    that each of them runs on a separate CPU core. To do so, we must set the CPU affinity
    mask of each kernel thread (the first one to `0`, the second to `1`, in order
    to have them execute on only CPUs `0` and `1` respectively). The thing is, it''s
    not a clean job – quite a *hack,* to be honest, and definitely *not* recommended.
    The following comment from that code shows why:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想演示一种称为per-CPU变量的同步技术，我们需要创建两个内核线程，并确保它们分别在不同的CPU核心上运行。为此，我们必须设置每个内核线程的CPU亲和性掩码（第一个设置为`0`，第二个设置为`1`，以便它们只在CPU
    `0`和`1`上执行）。问题是，这不是一个干净的工作 - 老实说，相当*糟糕*，绝对*不*推荐。代码中的以下注释显示了原因：
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Later, we invoke the function pointer, in effect invoking the `sched_setaffinity` code,
    like so:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后，我们调用函数指针，实际上调用`sched_setaffinity`代码，如下所示：
- en: '[PRE14]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Unconventional and controversial; it does work, but please avoid hacks like
    this in production.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 非常不寻常和有争议；它确实有效，但请在生产中避免这样的黑客行为。
- en: 'Now that you know how to get/set a thread''s CPU affinity mask, let''s move
    on to the next logical step: how to get/set a thread''s scheduling policy and
    priority! The next section delves into the details.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道如何获取/设置线程的CPU亲和性掩码，让我们继续下一个逻辑步骤：如何获取/设置线程的调度策略和优先级！下一节将深入细节。
- en: Querying and setting a thread’s scheduling policy and priority
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询和设置线程的调度策略和优先级
- en: In [Chapter 10](5391e3c1-30ad-4c75-a106-301259064881.xhtml), *The CPU Scheduler
    – Part 1*, in the *Threads – which scheduling policy and priority* section, you
    learned how to query the scheduling policy and priority of any given thread via
    `chrt(1)` (we also demonstrated a simple bash script to do so). There, we mentioned
    the fact that `chrt(1)` internally invokes the `sched_getattr(2)` system call
    in order to query these attributes.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](5391e3c1-30ad-4c75-a106-301259064881.xhtml)中，*CPU调度器-第1部分*，在*线程-哪种调度策略和优先级*部分，您学会了如何通过`chrt(1)`查询任何给定线程的调度策略和优先级（我们还演示了一个简单的bash脚本来实现）。在那里，我们提到了`chrt(1)`内部调用`sched_getattr(2)`系统调用来查询这些属性。
- en: 'Very similarly, setting the scheduling policy and priority can be performed
    either by using the `chrt(1)` utility (making it simple to do so within a script,
    for example), or programmatically within a (user space) C application with the `sched_setattr(2)` system
    call. In addition, the kernel exposes other APIs: `sched_{g,s}etscheduler(2)` and
    its `pthread` library wrapper APIs, `pthread_{g,s}etschedparam(3)` (as these are
    all user space APIs, we leave it to you to browse through their man pages to get
    the details and try them out for yourself).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 非常类似地，可以通过使用`chrt(1)`实用程序（例如在脚本中简单地这样做）或在（用户空间）C应用程序中使用`sched_setattr(2)`系统调用来设置调度策略和优先级。此外，内核还公开其他API：`sched_{g,s}etscheduler(2)`及其`pthread`库包装器API，`pthread_{g,s}etschedparam(3)`（由于这些都是用户空间API，我们让您自行查阅它们的手册页面以获取详细信息并尝试它们）。
- en: Within the kernel – on a kernel thread
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在内核中-在内核线程上
- en: As you know by now, the kernel is most certainly not a process nor a thread.
    Having said that, the kernel does contain kernel threads; like their user space
    counterparts, kernel threads can be created as required (from within the core
    kernel, a device driver, a kernel module). They *are* schedulable entities (KSEs!)
    and, of course, each of them has a task structure; thus, their scheduling policy
    and priority can be queried or set as required..
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道，内核绝对不是一个进程也不是一个线程。话虽如此，内核确实包含内核线程；与它们的用户空间对应物一样，内核线程可以根据需要创建（从核心内核、设备驱动程序、内核模块中）。它们是可调度实体（KSEs！），当然，它们每个都有一个任务结构；因此，它们的调度策略和优先级可以根据需要查询或设置。
- en: 'So, to the point at hand: to set the scheduling policy and/or priority of a
    kernel thread, the kernel typically makes use of the `kernel/sched/core.c:sched_setscheduler_nocheck()` (GFP
    exported) kernel API; here, we show its signature and an example of its typical
    usage; the comments that follow make it quite self-explanatory:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，就要点而言：要设置内核线程的调度策略和/或优先级，内核通常使用`kernel/sched/core.c:sched_setscheduler_nocheck()`（GFP导出）内核API；在这里，我们展示了它的签名和典型用法的示例；随后的注释使其相当不言自明。
- en: '[PRE15]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'One good example of the kernel''s usage of kernel threads is when the kernel
    (quite commonly) uses threaded interrupts. Here, the kernel must create a dedicated
    kernel thread with the `SCHED_FIFO` (soft) real-time scheduling policy and a real-time
    priority value of `50` (halfway between), for interrupt handling purposes. The
    (relevant) code to do this is shown here as an example of setting scheduling policy
    and priority on a kernel thread:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 内核对内核线程的一个很好的例子是内核（相当常见地）使用线程化中断。在这里，内核必须创建一个专用的内核线程，其具有`SCHED_FIFO`（软）实时调度策略和实时优先级值为`50`（介于中间），用于处理中断。这里展示了设置内核线程调度策略和优先级的相关代码：
- en: '[PRE16]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: (Here, we don't show the code that creates the kernel thread via the `kthread_create()`
    API. Also, FYI, `MAX_USER_RT_PRIO` is the value `100`.)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: （这里我们不展示通过`kthread_create()` API创建内核线程的代码。另外，FYI，`MAX_USER_RT_PRIO`的值是`100`。）
- en: Now that you understand to a good extent how CPU scheduling works at the level
    of the OS, we'll move on to yet another quite compelling discussion – that of
    cgroups; read on!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您在很大程度上了解了操作系统级别的CPU调度是如何工作的，我们将继续进行另一个非常引人入胜的讨论——cgroups；请继续阅读！
- en: CPU bandwidth control with cgroups
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用cgroups进行CPU带宽控制
- en: 'In the hazy past, the kernel community struggled mightily with a rather vexing
    issue: though scheduling algorithms and their implementations – the early 2.6.0
    O(1) scheduler, and a little later (with 2.6.23), the **Completely Fair Scheduler**
    (**CFS**) – promised, well, completely fair scheduling, it really wasn''t. Think
    about this for a moment: let''s say you are logged into a Linux server along with
    nine other people. Everything else being equal, it is likely that processor time
    is (more or less) fairly shared between all ten  people; of course, you will understand
    that it''s not really people that run, it''s processes and threads that run on
    their behalf.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，内核社区曾经为一个相当棘手的问题而苦苦挣扎：尽管调度算法及其实现（早期的2.6.0 O(1)调度器，稍后（2.6.23）的**完全公平调度器**（CFS））承诺了完全公平的调度，但实际上并非如此。想想这个：假设您与其他九个人一起登录到Linux服务器。其他一切都相等的情况下，处理器时间可能（或多或少）在所有十个人之间（相对）公平地共享；当然，您会明白，真正运行的不是人，而是代表他们运行的进程和线程。
- en: For now at least, let's assume it's mostly fairly shared. But, what if you write
    a user space program that, in a loop, indiscriminately spawns off several new
    threads, each of which perform a lot of CPU-intensive work (and perhaps as an
    added bonus, allocates large swathes of memory as well; a file (un)compressor
    app perhaps) in each loop iteration!? The CPU bandwidth allocation is no longer
    fair in any real sense of the term, your account will effectively hog the CPUs
    (and perhaps other system resources, such as memory, as well)!
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 至少目前，让我们假设它基本上是公平的。但是，如果您编写一个用户空间程序，在循环中不加选择地生成多个新线程，每个线程都执行大量的CPU密集型工作（也许还额外分配大量内存；例如文件（解）压缩应用程序）！那么CPU带宽分配在任何实际意义上都不再公平，您的账户将有效地占用CPU（也许还占用其他系统资源，如内存）！
- en: 'A solution that precisely and effectively allocated and managed CPU (and other
    resource) bandwidth was required; ultimately, Google engineers obliged with patches
    that put the modern-day cgroups solution into the Linux kernel (in version 2.6.24).
    In a nutshell, cgroups is a kernel feature that allows the system administrator
    (or anyone with root access) to perform bandwidth allocation and fine-grained
    resource management on the various resources (or *controllers*, as they are called
    in the cgroup lexicon) on a system. Do note: using cgroups, it''s not just the
    processors (CPU bandwidth), but also memory, network, block I/O (and more)  bandwidth
    that can be carefully allocated and monitored as required by your project or product.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 需要一个精确有效地分配和管理CPU（和其他资源）带宽的解决方案；最终，谷歌工程师提供了补丁，将现代cgroups解决方案放入了Linux内核（在2.6.24版本）。简而言之，cgroups是一个内核功能，允许系统管理员（或任何具有root访问权限的人）对系统上的各种资源（或在cgroup词汇中称为*控制器*）执行带宽分配和细粒度资源管理。请注意：使用cgroups，不仅可以仔细分配和监视处理器（CPU带宽），还可以根据项目或产品的需要仔细分配和监视内存、网络、块I/O（等等）带宽。
- en: 'So, hey, you''re interested now! How do you enable this cgroups feature? Simple
    – it''s a kernel feature you enable (or disable) at quite a fine granularity in
    the usual way: by configuring the kernel! The relevant menu (via the convenient
    `make menuconfig` interface) is `General setup / Control Group support`. Try this:
    `grep` your kernel config file for `CGROUP`; if required, tweak your kernel config,
    rebuild, reboot with the new kernel, and test. (We covered kernel configuration
    in detail back in [Chapter 2](e0b89a37-18a3-424d-8983-58c4ac0725f6.xhtml), *Building
    the 5.x Linux Kernel from Source – Part 1*, and the kernel build and install in
    [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml), *Building the 5.x Linux
    Kernel from Source – Part 2.*)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，嘿，您现在感兴趣了！如何启用这个cgroups功能？简单——这是一个您可以通过通常的方式在内核中启用（或禁用）的内核功能：通过配置内核！相关菜单（通过方便的`make
    menuconfig`界面）是`General setup / Control Group support`。尝试这样做：在内核配置文件中使用`grep`查找`CGROUP`；如果需要，调整内核配置，重新构建，使用新内核重新启动并进行测试。（我们在[第2章](e0b89a37-18a3-424d-8983-58c4ac0725f6.xhtml)中详细介绍了内核配置，*从源代码构建5.x
    Linux内核–第1部分*，以及在[第3章](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml)中介绍了内核构建和安装，*从源代码构建5.x
    Linux内核–第2部分*）。
- en: 'Good news: cgroups is enabled by default on any (recent enough) Linux system
    that runs the systemd init framework. As mentioned just now, you can query the
    cgroup controllers enabled by `grep`-ping your kernel config file, and modify
    the config as desired.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息：cgroups在运行systemd init框架的任何（足够新的）Linux系统上默认启用。正如刚才提到的，您可以通过查询cgroup控制器来查看启用的控制器，并根据需要修改配置。
- en: From it's initiation in 2.6.24, cgroups, like all other kernel features, continually
    evolves. Fairly recently, a point was reached where sufficiently improved cgroup
    features became incompatible with the old, resulting in a new cgroup release,
    one christened cgroups v2 (or simply cgroups2); this was declared production-ready
    in the 4.5 kernel series (with the older one now referred to as cgroups v1 or
    as the legacy cgroups implementation). Note that, as of the time of this writing,
    both can and do exist together (with some limitations; many applications and frameworks
    still use the older cgroups v1 and are yet to migrate to v2).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: A detailed rationale of why to use cgroups v2 as opposed to cgroups v1 can be
    found within the kernel documentation here: [https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#issues-with-v1-and-rationales-for-v2](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#issues-with-v1-and-rationales-for-v2)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'The man page on `cgroups(7)` describes in some detail the interfaces and various
    available (resource) controllers (or *subsystems* as they are sometimes referred
    to); for cgroups v1, they are `cpu`, `cpuacct`, `cpuset`, `memory`, `devices`,
    `freezer`, `net_cls`, `blkio`, `perf_event`, `net_prio`, `hugetlb`, `pids`, and
    `rdma`. We refer interested readers to said man page for details; as an example,
    the PIDS controller is very useful in preventing fork bombs (often, a silly but
    nevertheless deadly DoS attack where the `fork(2)` system call is issued within
    an infinite loop!), allowing you to limit the number of processes that can be
    forked off from that cgroup (or its descendants). On a Linux box with cgroups
    v1 running, peek at the content of `/proc/cgroups`: it reveals the v1 controllers
    available and their current usage.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Control groups are exposed via a purpose-built synthetic (pseudo) filesystem,
    typically mounted under `/sys/fs/cgroup`. In cgroups v2, all controllers are mounted
    in a single hierarchy (or tree). This is unlike cgroups v1, where multiple controllers
    could be mounted under multiple hierarchies or groups. The modern init framework, *systemd,* is
    a user of both the v1 and v2 cgroups. The `cgroups(7)` man page indeed mentions
    the fact that `systemd(1)` auto-mounts a cgroups v2 filesystem during startup
    (at `/sys/fs/cgroup/unified`).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: In cgroups v2, these are the supported controllers (or resource limiters or
    subsystems, if you will): `cpu`, `cpuset`, `io`, `memory`, `pids`, `perf_event`,
    and `rdma` (the first five being commonly deployed).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, the focus is on CPU scheduling; thus, we do not delve further
    into other controllers, but limit our discussions to an example of using the cgroups
    v2 `cpu` controller to limit CPU bandwidth allocation. For more on employing the
    other controllers, we refer you to the resources mentioned previously (along with
    several more found in the *Further reading* section of this chapter).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Looking up cgroups v2 on a Linux system
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let''s look up the available v2 controllers; to do so, locate the cgroups
    v2 mount point; it''s usually here:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Hey, there aren''t any controllers present in `cgroup2`!? Actually, it will
    be this way in the presence of *mixed* cgroups, v1 and v2, which is the default
    (as of the time of writing). To exclusively make use of the later version – and
    thus have all configured controllers visible – you must first disable cgroups
    v1 by passing this kernel command-line parameter at boot: `cgroup_no_v1=all` (recall,
    all available kernel parameters can be conveniently seen here: [https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt](https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt)).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'After rebooting the system with the preceding option, you can check that the
    kernel parameters you specified (via GRUB on an x86, or perhaps via U-Boot on
    an embedded system) have indeed been parsed by the kernel:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Okay; now let''s retry looking up the `cgroup2` controllers; you should find
    that it''s typically mounted under `/sys/fs/cgroup/` - the `unified` folder is
    no longer present (now that we''ve booted with the `cgroup_no_v1=all` parameter):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Ah, now we see them (the exact controllers you see depend on how the kernel's
    configured).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'The rules governing the working of cgroups2 is beyond this book''s scope; if
    you''d like to, I suggest you read through it here: [https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#control-group-v2](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#control-group-v2).
    Also, all the `cgroup.<foo>` pseudo files under a cgroup are described in detail
    in the *Core Interface Files* section ([https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#core-interface-files](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#core-interface-files)). Similar
    information is presented, in a simpler way, within the excellent man page on `cgroups(7)` (look
    it up with `man 7 cgroups` on Ubuntu).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Trying it out – a cgroups v2 CPU controller
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s try something interesting: we shall create a new sub-group under the
    cgroups v2 hierarchy on the system. We''ll then set up a CPU controller for it,
    run a couple of test processes (that hammer away on the system''s CPU cores),
    and set a user-specified upper limit on how much CPU bandwidth these processes
    can actually make use of!'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we outline the steps you will typically take to do this (all of these
    steps require you to be running with root access):'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure your kernel supports cgroups v2:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should be running on a 4.5 or later kernel.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the presence of mixed cgroups (both legacy v1 and newer v2, which, as of
    the time of writing, is the default), check that your kernel command line includes
    the `cgroup_no_v1=all` string. Here, we shall assume that the cgroup v2 hierarchy
    is supported and mounted at `/sys/fs/cgroup`.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add a `cpu` controller to the cgroups v2 hierarchy; this is achieved by doing
    this, as root:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The kernel documentation on cgroups v2 ([https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#cpu](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#cpu)) does
    mention this point: *WARNING: cgroup2 doesn’t yet support control of realtime
    processes and the cpu controller can only be enabled when all RT processes are
    in the root cgroup. Be aware that system management software may already have
    placed RT processes into nonroot cgroups during the system boot process, and these
    processes may need to be moved to the root cgroup before the cpu controller can
    be enabled.*'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a sub-group: this is done by simply creating a directory with the required
    sub-group name under the cgroup v2 hierarchy; for example, to create a sub-group
    called `test_group`, use the following:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The interesting bit''s here: set up the max allowable CPU bandwidth for the
    processes that will belong to this sub-group; this is effected by writing into
    the `<cgroups-v2-mount-point>/<sub-group>/cpu.max` (pseudo) file. For clarity,
    the explanation of this file, as per the kernel documentation ([https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#cpu-interface-files](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#cpu-interface-files)),
    is reproduced here:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In effect, all processes in the sub-control group will be collectively allowed to
    run for `$MAX` out of a period of `$PERIOD` microseconds; so, for example, with
    `MAX = 300,000` and `PERIOD = 1,000,000`, we're effectively allowing all processes
    within the sub-control group to run for 0.3 seconds out of a period of 1 second!
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Insert some processes into the new sub-control group; this is achieved by writing
    their PIDs into the `<cgroups-v2-mount-point>/<sub-group>/cgroup.procs` pseudo-file:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can further verify that they actually belong to this sub-group by looking
    up the content of each process's `/proc/<PID>/cgroup` pseudo-file; if it contains
    a line of the form `0::/<sub-group>`, then it indeed belongs to the sub-group!
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That's it; *the processes under the new sub-group will now perform their work
    under the CPU bandwidth constraint imposed*; when done, they will die as usual...
    you can remove (or delete) the sub-group with a simple `rmdir <cgroups-v2-mount-point>/<sub-group>`.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A bash script that actually carries out the preceding steps is available here:
    `ch11/cgroups_v2_cpu_eg/cgv2_cpu_ctrl.sh`. Do check it out! To make it interesting,
    it allows you to pass the maximum allowed CPU bandwidth – the `$MAX` value discussed
    in *step 4*! Not only that; we deliberately write a test script (`simp.sh`) that
    hammers on the CPU(s) – they generate integer values that we redirect to files.
    Thus, the number of integers they generated during their lifetime is an indication
    of how much CPU bandwidth was available to them... this way, we can test the script
    and actually see cgroups (v2) in action!'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'A couple of test runs here will help you understand this:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You're expected to run it as root and to pass, as a parameter, the `$MAX` value
    (the usage screen seen previously quite clearly explains it, including displaying
    the valid range (the microseconds value)).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we run the bash script with the parameter `800000`,
    implying a CPU bandwidth of 800,000 out of a period of 1,000,000; in effect, a
    quite high CPU utilization of 0.8 seconds out of every 1 second on the CPU (80%):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06814296-3ae2-44b7-911c-080a0dfa7de9.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Screenshot of running our cgroups v2 CPU controller demo bash
    script with an effective max CPU bandwidth of 80%
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Study our script''s output in *Figure 11.8*; you can see that it does its job:
    after verifying cgroup v2 support, it adds a `cpu` controller and creates a sub-group
    (called `test_group`). It then proceeds to launch two test processes called `j1`
    and `j2` (in reality, they''re just symbolic links to our `simp.sh` script). Once
    launched, they run of course. The script then queries and adds their PIDs to the
    sub-control group (as shown in *step 5*). We give the two processes 5 seconds
    to run; the script then displays the content of the files into which they wrote.
    It''s designed such that job `j1` writes integers starting from `1`, and job `j2`
    writes integers starting from `900`. In the preceding screenshot, you can clearly
    see that, in their lifetime, and under the effectively 80% CPU bandwidth available
    to it, job `j1` emits numbers from 1 to 68; similarly (under the same constraints),
    job `j2` emits numbers from `900` to `965` (a similar quantity of work, in effect).
    The script then cleans up, killing off the jobs and deleting the sub-group.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'However, to really appreciate the effect, we run our script again (study the
    following output), but this time with a maximum CPU bandwidth of just 1,000 (the
    `$MAX` value) – in effect, a max CPU utilization of just 0.1%!:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: What a difference! This time our jobs `j1` and `j2` could literally emit between
    just two and three integers (the values `1 2 3` for job j1 and `900 901 `for job
    j2, as seen in the preceding output), clearly proving the efficacy of the cgroups
    v2 CPU controller.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Containers, essentially lightweight VMs (to some extent), are currently a hot
    commodity. The majority of container technologies in use today (Docker, LXC, Kubernetes,
    and others) are, at heart, a marriage of two built-in Linux kernel technologies, namespaces,
    and cgroups.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we complete our brief coverage of a really powerful and useful kernel
    feature: cgroups. Let''s move on to the final section of this chapter: learning
    how you can turn regular Linux into a real-time operating system!'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Converting mainline Linux into an RTOS
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mainline or vanilla Linux (the kernel you download from [https://kernel.org](https://kernel.org))
    is decidedly *not* a **Real-Time Operating System** (**RTOS**); it''s a **General
    Purpose Operating System** (**GPOS**; as is Windows, macOS, Unix). In an RTOS,
    where hard real-time characteristics come into play, not only must the software
    obtain the correct result, there are deadlines associated with doing so; it must
    guarantee it meets these deadlines, every single time. The mainline Linux OS,
    though not an RTOS, does a tremendous job: it easily qualifies as being a soft
    real-time OS (one where deadlines are met most of the time). Nevertheless, true
    hard real-time domains (for example, military operations, many types of transport,
    robotics, telecom, factory floor automation, stock exchanges, medical electronics,
    and so on) require an RTOS.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Another key point in this context is that of **determinism**: an oft missed
    point regarding real-time is that the software response time need not always be
    really fast (responding, say, within a few microseconds); it may be a lot slower
    (in the range of, say, tens of milliseconds); by itself, that isn't what really
    matters in an RTOS. What does matter is that the system is reliable, working in
    the same consistent manner and always guaranteeing the deadline is met.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: For example, the time taken to respond to a scheduling request, should be consistent
    and not bounce all over the place. The variance from the required time (or baseline)
    is often referred to as the **jitter**; an RTOS works to keep the jitter tiny,
    even negligible. In a GPOS, this is often impossible and the jitter can vary tremendously
    - at one point being low and the next very high. Overall, the ability to maintain
    a stable even response with minimal jitter - even in the face of extreme workload
    pressures - is termed determinism, and is the hallmark of an RTOS. To provide
    such a deterministic response, algorithms must, as far as possible, be designed
    to correspond to *O(1)* time complexity.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Thomas Gleixner, along with community support, has worked toward that goal for
    a long while now; for many years, in fact, ever since the 2.6.18 kernel, there
    have been offline patches that convert the Linux kernel into an RTOS. These patches
    can be found, for many versions of the kernel, here: [https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/](https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/).
    The older name for this project was `PREEMPT_RT`; later (October 2015 onward),
    the **Linux Foundation** (**LF**) took over stewardship of this project – a very
    positive step! – and renamed it the **Real-Time Linux** (**RTL**) Collaborative
    Project ([https://wiki.linuxfoundation.org/realtime/rtl/start#the_rtl_collaborative_project](https://wiki.linuxfoundation.org/realtime/rtl/start#the_rtl_collaborative_project)),
    or RTL (don't confuse this project with co-kernel approaches such as Xenomai or
    RTAI, or the older and now-defunct attempt called RTLinux).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'An FAQ, of course, is "why aren''t these patches in mainline itself?" Well,
    it turns out that:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Much of the RTL work has indeed been merged into the mainline kernel; this includes
    important areas such as the scheduling subsystem, mutexes, lockdep, threaded interrupts,
    PI, tracing, and so on. In fact, an ongoing primary goal of RTL is to get it merged
    as much as is feasible (we show a table summarizing this in the *Mainline and
    RTL – technical differences summarized* section).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linus Torvalds deems that Linux, being primarily designed and architected as
    a GPOS, should not have highly invasive features that only an RTOS really requires;
    so, though patches do get merged in, it's a slow deliberated process.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have included several interesting articles and references to RTL (and hard
    real time) in the *Further reading* section of this chapter; do take a look.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'What you''re going to do next is interesting indeed: you will learn how to
    patch the mainline 5.4 LTS kernel with the RTL patches, configure it, build, and
    boot it; you will thus end up running an RTOS – *Real-Time Linux or RTL*! We shall
    do this on our x86_64 Linux VM (or native system).'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: We won't stop there; you will then learn more – the technical differences between
    regular Linux and RTL, what system latency is, and how, practically, to measure
    it. To do so, we shall first apply the RTL patch on the kernel source of the Raspberry
    Pi device, configure and build it, and use it as a test-bed for system latency
    measurement using the *cyclictest* app (you'll also learn to use modern BPF tools
    for measuring scheduler latencies). Let's get a move on, first building an RTL
    kernel for our 5.4 kernel on an x86_64!
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Building RTL for the mainline 5.x kernel (on x86_64)
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will learn, in a step-by-step, hands-on fashion, how exactly
    to patch, configure, and build Linux as an RTOS. As mentioned in the preceding
    section, these real-time patches have been around a long while; it's time to make
    use of them.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining the RTL patches
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Navigate to [https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/](https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/) (or,
    if you''re on an alternate kernel, go to one directory level above this and select
    the required kernel version):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8335c076-4a92-4ece-b80e-f38139e5bc15.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – Screenshot of the RTL patches for the 5.4 LTS Linux kernels
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'You will quickly notice that the RTL patches are available for only some versions
    of the kernel in question (here, 5.4.y); more on this follows. In the preceding
    screenshot, you can spot two broad types of patch files – interpret it as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '`patch-<kver>rt[nn].patch.[gz|xz]`: The prefix is `patch-`; this is the complete
    collection of patches required to patch the mainline kernel (version `<kver>`)
    **in one unified** (and compressed) file.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patches-<kver>-rt[nn].patch.[gz|xz]`: The prefix is `patches-`; this compressed
    file contains every individual patch (as a separate file) that went into making
    up the patch series for this version of RTL.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Also, as you should be aware, `<fname>.patch.gz` and `<fname>.patch.xz` are
    the same archive; it's just that the compressor differs – the `.sign` files are the
    PGP signature files.)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: We shall use the first type; download the `patch-<kver>rt[nn].patch.xz` file
    to your target system by clicking on the link (or via `wget(1)`).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Notice that for the 5.4.x kernels (as of the time of writing), the RTL patches
    seem to be present only for version 5.4.54 and 5.4.69 (and not for 5.4.0, the
    kernel that we have been working with all along).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the particular kernel version that the RTL patches apply against can
    certainly vary from what I've mentioned here at the time of this writing. That's
    expected - just follow the steps substituting the release number you're using
    with what's mentioned here.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Don''t be worried – we shall show you a workaround in a moment. This is indeed
    going to be the case; the community cannot feasibly build patches against every
    single kernel release – there are just too many. This does have an important implication:
    either we patch our 5.4.0 kernel to, say, 5.4.69, or, we simply download the 5.4.69
    kernel to begin with and apply the RTL patches against it.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'The first approach is doable but is more work (especially in the absence of
    a patching tools such as git/ketchup/quilt or similar; here, we choose not to
    use git to apply patches, just working on the stable kernel tree instead). As
    the Linux kernel patches are incremental, we will have to download every single
    patch from 5.4.0 until 5.4.69 (a total of 69 patches!), and apply them successively
    and in order: first 5.4.1, then 5.4.2, then 5.4.3, and so on until the final one!
    Here, to help keep things simple, since we know that the kernel to patch against
    is 5.4.69, it''s just easier to download and extract it instead. So, head on over
    to [https://www.kernel.org/](https://www.kernel.org/) and do so. Thus, here, we
    end up downloading two files:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: The compressed kernel source for mainline 5.4.69: [https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.4.69.tar.xz](https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.4.69.tar.xz)
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RTL patch for 5.4.69: [https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/patches-5.4.69-rt39.tar.xz](https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/patches-5.4.69-rt39.tar.xz)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (As explained in detail in [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml),
    *Building the 5.x Linux Kernel from Source – Part 2*, if you intend to cross-compile
    the kernel for another target, the usual procedure is to build it on a suitably
    powerful workstation, so download it there.)
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, extract both the RTL patch file as well as the kernel code base `tar.xz`
    file to obtain the kernel source tree (here, it''s version 5.4.69; of course,
    these details have been well covered back in [Chapter 2](e0b89a37-18a3-424d-8983-58c4ac0725f6.xhtml),
    *Building the 5.x Linux Kernel from Source – Part 1*). By now, your working directory
    content should look similar to this:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '(FYI, the `unxz(1)` utility can be used to extract the `.xz`-compressed patch
    file.) For the curious reader: take a peek at the patch (the file `patch-5.4.69-rt39.patch`),
    to see all the code-level changes wrought to bring about a hard real-time kernel;
    it''s non-trivial of course! An overview of the technical changes will be seen
    in the upcoming *Mainline and RTL – technical differences summarized* section.
    Now that we have things in place, let''s begin by applying the patch to the stable
    5.4.69 kernel tree; the following section covers just this.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Applying the RTL patch
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ensure you keep the extracted patch file, `patch-5.4.69-rt39.patch`, in the
    directory immediately above the 5.4.69 kernel source tree (as seen previously).
    Now, let''s apply the patch. Careful – (obviously) don''t attempt to apply the
    compressed file as the patch; extract and use the uncompressed patch file. To
    ensure that the patch applies correctly, we first employ the `--dry-run` (dummy
    run) option to `patch(1)`:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'All''s well, let''s now actually apply it:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Great – we have the patched kernel for RTL ready now!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are multiple ways and various shortcuts that can be employed;
    for example, you can also achieve the preceding via the `xzcat ../patch-5.4.69-rt39.patch.xz
    | patch -p1` command (or similar).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and building the RTL kernel
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have covered the kernel configuration and build steps in detail in [Chapter
    2](e0b89a37-18a3-424d-8983-58c4ac0725f6.xhtml), *Building the 5.x Linux Kernel
    from Source – Part 1*, and [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml),
    *Building the 5.x Linux Kernel from Source – Part 2*, hence we shan''t repeat
    it here. Pretty much everything remains the same; the only significant difference
    being that we must configure this kernel to take advantage of RTL (this is explained
    on the new RTL wiki site, here: [https://wiki.linuxfoundation.org/realtime/documentation/howto/applications/preemptrt_setup](https://wiki.linuxfoundation.org/realtime/documentation/howto/applications/preemptrt_setup)).'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'To cut down the kernel features to be built to approximately match the present
    system configuration, we first, within the kernel source tree directory (`linux-5.4.69`),
    do the following (we also covered this back in [Chapter 2](e0b89a37-18a3-424d-8983-58c4ac0725f6.xhtml),
    *Building the 5.x Linux Kernel from Source - Part 1*, under the *Tuned kernel
    config via the localmodconfig approach* section):'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, fire up the kernel configuration with `make menuconfig`:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the `General setup` sub-menu:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f11d2aa0-2ff7-4939-b420-fbbfe1a49bd2.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.10 – make menuconfig / General setup: configuring the RTL-patched
    kernel'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Once there, scroll down to the `Preemption Model` sub-menu; we see it highlighted
    in the preceding screenshot, along with the fact that the currently (by default)
    selected preemption model is `Voluntary Kernel Preemption (Desktop)`.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pressing *Enter* here leads us into the `Preemption Model` sub-menu:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a9390c2b-8923-4583-bbe4-d037b7ff4a86.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.11 – make menuconfig / General setup / Preemption Model: configuring
    the RTL-patched kernel'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: There it is! Recall from the previous chapter, in the *Preemptible kernel* section,
    we described the fact that this very kernel configuration menu had three items
    (the first three seen in Figure 11.11). Now it has four. The fourth item – the
    `Fully Preemptible Kernel (Real-Time)` option – has been added on thanks to the
    RTL patch we just applied!
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to configure the kernel for RTL, scroll down and select the `Fully Preemptible
    Kernel (Real-Time)` menu option (refer Figure 11.1). This corresponds to the kernel
    `CONFIG_PREEMPT_RT` config macro, whose `< Help >` is quite descriptive (do take
    a gander); it does, in fact, conclude with the statement: *select this if you
    are building a kernel for systems which require real-time guarantees*.'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In earlier versions of the kernel (including 5.0.x), the `Preemption Model`
    sub-menu displayed five choices; two were for RT: one was termed Basic RT and
    the other was what we see here as the fourth choice – now (5.4.x) they''ve simply
    been folded into one true real-time option.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have selected the fourth option and saved and exited the `menuconfig`
    UI, (re)check that the full preemptible kernel – in effect, RTL – is selected:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: All right, looks good! (Of course, before building, you can tweak other kernel
    config options as required for your product.)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now build the RTL kernel:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once it successfully builds and installs, reboot the system; at boot, press
    a key to display the GRUB bootloader menu (holding down one of the *Shift* keys
    can help ensure the GRUB menu is displayed at boot); within the GRUB menu, select
    the newly built `5.4.69-rtl` RTL kernel (in fact, the kernel just installed is
    usually the default one selected at boot). It should boot now; once logged in
    and on a shell, let''s verify the kernel version:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Notice `CONFIG_LOCALVERSION` set to the value `-rtl-llkd1`. (Also, with `uname
    -a`, the `PREEMPT RT` string will be seen.) We're now - as promised - running
    Linux, RTL, as a hard real-time operating system, an RTOS!
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s very important to understand, though, that for true hard real time, simply
    having a hard real-time kernel is *not* enough; you must also very carefully design
    and write your user space (apps, libraries, and tooling) as well as your kernel
    modules / drivers, to conform to real time as well. For example, frequent page
    faulting can throw determinism out of the proverbial window and result in high
    latencies (and high jitter). (Recall what you learned in [Chapter 9](dbb888a2-8145-4132-938c-1313a707b2f2.xhtml),
    *Kernel Memory Allocation for Module Authors – Part 2*, in the *A brief note on
    memory allocations and demand paging* section. Page faulting is a fact of life
    and can and does often occur; minor page faults will usually cause little to worry
    about. But in a hard RT scenario? And in any case, "major faults" will hamper
    performance.) Techniques such as using `mlockall(2)` to lock down all the pages
    of a real-time application process might well be required. This and several other
    techniques and tips for writing real-time code are provided here: [https://rt.wiki.kernel.org/index.php/HOWTO:_Build_an_
    RT-application](https://rt.wiki.kernel.org/index.php/HOWTO:_Build_an_RT-application).
    (Similarly, topics regarding CPU affinity and shielding, `cpuset` management,
    IRQ prioritization, and so on can be found on the older RT wiki site mentioned
    previously; [https://rt.wiki.kernel.org/index.php/Main_Page](https://rt.wiki.kernel.org/index.php/Main_Page).)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: So, great – you now know how to configure and build Linux as an RTOS! I encourage
    you to try this out for yourself. Moving along, we'll next summarize the key differences
    between the standard and RTL kernels.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Mainline and RTL – technical differences summarized
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To give you a deeper understanding of this interesting topic area, in this
    section, we delve further into it: we summarize the key differences between the
    standard (or mainline) and RTL kernels.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we summarize some of the key differences between the
    standard (or mainline) and RTL kernels. A primary goal of the RTL project is to
    ultimately become fully integrated into the regular mainline kernel tree. As this
    process is evolutionary, the merging of patches from RTL into mainline is slow
    but steady; interestingly, as you can see from the rightmost column in the following
    table, most of (around 80% at the time of writing) the RTL work has actually been
    already merged into the mainline kernel, and it continues to be:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '| **Component / Feature** | **Standard or mainline (vanilla) Linux** | **RTL
    (fully preemptible / hard real-time Linux)** | **RT work merged into mainline?**
    |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| Spinlocks | The spinlock critical section is  non-preemptible kernel code
    | As preemptible as is humanly possible; called "sleeping spinlocks"! In effect,
    spinlocks have been converted into mutexes. | No |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: '| Interrupt handling | Traditionally done via the top and bottom half (hardirq/tasklet/softirq)
    mechanism | Threaded interrupts: the majority of interrupt processing is done
    within a kernel thread (2.6.30, June 2009). | Yes |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
- en: '| HRTs (High-Resolution Timers) | Available here due to merge from RTL | Timers
    with nanosecond resolution (2.6.16, March 2006). | Yes |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: '| RW locks | Unbounded; writers may starve | Fair RW locks with bounded writer
    latency. | No |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| lockdep | Available here due to merge from RTL | Very powerful (kernel space)
    tool to detect and prove locking correctness or the lack thereof. | Yes |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| Tracing | Some tracing technologies available here due to merge from RTL
    | Ftrace''s origins (and to some extent perf''s) were with the RT developers attempting
    to find latency issues. | Yes |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| Scheduler | Many scheduler features available here due to merge from RTL
    | Work on real-time scheduling as well as the deadline scheduling class (`SCHED_DEADLINE`)
    was first done here (3.14, March 2014); also, full tickless operation (3.10, June
    2013). | Yes |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: (Don't worry – we shall definitely cover many of the preceding details in subsequent
    chapters of the book.)
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Of course, a well-known (at least it should be) rule of thumb is simply this: *there
    is no silver bullet*. This implies, of course, that no one solution will fit every
    need.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Please, if you haven't yet done so, do yourself a huge favor and read the still-so-relevant
    book *The Mythical Man-Month: Essays on Software Engineering *by Frederick P Brooks.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in [Chapter 10](5391e3c1-30ad-4c75-a106-301259064881.xhtml), *The
    CPU Scheduler – Part 1*, in the *Preemptible kernel* section, the Linux kernel
    can be configured with the `CONFIG_PREEMPT` option; this is often referred to
    as the **low-latency** (or **LowLat**) kernel and provides near real-time performance.
    In many domains (virtualization, telecoms, and so on) using a LowLat kernel might
    turn out to be better than using a hard real-time RTL kernel, mainly due to RTL's
    overheads. You often find that, with hard real-time, user space apps can suffer
    from throughput, reduced CPU availability, and thus higher latencies. (Refer to
    the *Further reading* section for a whitepaper from Ubuntu that conducts a comparison
    between a vanilla distro kernel, a low-latency preemptible, and a fully preemptible
    – effectively an RTL – kernel.)
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: With latencies in mind, the following section will help you understand what
    exactly is meant by system latencies; then, you'll learn some ways to measure
    it on a live system. On, on!
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Latency and its measurement
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We often come across the term latency; what exactly does it mean in the context
    of the kernel? A synonym for latency is delay and that''s a good hint. *The latency
    (or delay) is the time taken to react* – in our context here, the time between
    the kernel scheduler waking up a user space thread (or process), thus making it
    runnable, and the time when it does actually run on the processor is the **scheduling
    latency**. (Do be aware, though, the term scheduling latency is also used in another
    context, to mean the time interval within which every runnable task is guaranteed
    to run at least once; the tunable is here: `/proc/sys/kernel/sched_latency_ns`,
    and, at least on recent x86_64 Linux, defaults to 24 ms). Similarly, the time
    elapsed from when a hardware interrupt occurs (say a network interrupt) to when
    it''s actually serviced by it''s handler routine, is the interrupt latency.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'The **cyclictest** user space program was written by Thomas Gleixner; its purpose:
    to measure kernel latencies. Its output values are in microseconds units. The
    average and maximum latency values are usually the ones of interest – if they
    fall within the acceptable range for the system, then all''s good; if not, it
    points to perhaps product-specific redesign and/or kernel configuration tweaking,
    checking other time-critical code paths (including user space), and so on.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Let's use the cyclictest process itself as an example to clearly understand
    scheduling latency. The cyclictest process is run; internally, it issues `nanosleep(2)` (or,
    if the `-n` option switch is passed, the `clock_nanosleep(2)` system call), putting
    itself into a sleep state for the time interval specified. As these `*sleep()`
    system calls are obviously blocking, the kernel internally enqueues the cyclictest
    (for simplicity, we refer to it as `ct` in the following diagram) process into
    a wait queue, simply a kernel data structure that holds sleeping tasks.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'A wait queue is associated with an event; when that event occurs, the kernel
    awakens all tasks sleeping on that event. Here, the event in question is the expiry
    of a timer; this is communicated by the timer hardware by emitting a hardware
    interrupt (or IRQ); this starts the chain of events that must happen to make the
    cyclictest process wake up and run on the processor. The key point here, of course,
    is that it''s easier said than done: many potential delays might occur on the
    path to the process actually running on a processor core! This is what the following
    diagram seeks to convey – the potential sources of latency:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73e35574-dce9-40e5-ab2a-1c2a7f5a50eb.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 – The path to waking, context-switching, and running the cyclictest
    (ct) process; several latencies can occur
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: (Some of the preceding inputs stem from the excellent presentation *Using and
    Understanding the Real-Time Cyclictest Benchmark, Rowand, Oct 2013*.) Study Figure
    11.12 carefully; it shows the timeline from the hardware interrupt's assertion
    due to timer expiry (at time `t0`, as the sleep issued via the `nanosleep()` API
    by the cyclictest process is done at time `t1`), through IRQ handling (`t1` to
    `t3`), and the wakeup of the ct process – as a result of which it gets enqueued
    into the runqueue (between `t3` and `t4`) of the core it will eventually run upon.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: From there, it will eventually become the highest priority, or best or most
    deserving, task for the scheduling class it belongs to (at time `t6`; we covered
    these details in the preceding chapter), thus, it will preempt the currently running
    thread (`t6`). The `schedule()` code will then execute (time `t7 `to `t8`), the
    context switch will occur at the tail-end of `schedule()`, and finally(!), and
    the cyclictest process will actually execute on a processor core (time `t9`).
    Though it might at first appear complex, the reality is that this is a simplified
    diagram as several other potential latency sources have been omitted (for example,
    latencies due to IPI, SMI, cache migration, multiple occurrences of the preceding
    events, additional interrupts firing at an inopportune moment causing more delays,
    and so on).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'A rule of thumb for determining the maximum latency value of a user space task
    running with real-time priority is the following:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As an example, the Raspberry Pi Model 3 CPU clock runs at a frequency of 1 GHz;
    its wavelength (the time between one clock cycle to the next) is the inverse of
    the frequency, that is, 10^(-9) or 1 nanosecond. So, from the preceding equation,
    the theoretical maximum latency should be (within) 10^(-7) seconds which is about
    10 ns (nanoseconds). As you shall soon discover, this is merely theoretical.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Measuring scheduling latency with cyclictest
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To make this more interesting (as well as to run the latency test on a constrained
    system), we shall perform latency measurements using the well-known cyclictest app
    – while the system is under some amount of load (via the `stress(1)` utility)
    – on the equally well-known Raspberry Pi device. This section is divided into
    four logical parts:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: First, set up the working environment on the Raspberry Pi device.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, download and apply the RT patches on the kernel source, configure, and
    build it.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Third, install the cyclictest app, as well as a few other required packages
    (including `stress`), on the device.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fourth, run the test cases and analyze the results (even plotting graphs to
    help do so).
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first step and most parts of the second have already been covered in detail
    in [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml), *Building the 5.x
    Linux Kernel from Source – Part 2*, in the *Kernel build for the Raspberry Pi*
    section. This includes downloading the Raspberry Pi-specific kernel source tree,
    configuring the kernel, and installing an appropriate toolchain; we won't repeat
    this information here. The only significant difference here is that we shall first
    have to apply the RT patches to the kernel source tree and configure for hard
    real-time; we cover this in the next section.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Let's get going!
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Getting and applying the RTL patchset
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Check the mainline or distribution kernel version that is running on your Raspberry
    Pi device (substitute the Raspberry Pi with any other device you may be running
    Linux on); for example, on the Raspberry Pi 3B+ I'm using, it's running the stock
    Raspbian (or Raspberry Pi OS) GNU/Linux 10 (buster) with the 5.4.51-v7+ kernel.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: We'd like to build an RTL kernel for the Raspberry Pi with the closest possible
    matching kernel to the standard one it's currently running; for our case here,
    with it running 5.4.51[-v7+], the closest available RTL patches are for kernel
    versions 5.4.y-rt[nn] ([https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/](https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/));
    we shall come back to this shortly...
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go step by step:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: The steps to download the Raspberry Pi specific kernel source tree onto your
    host system disk have already been covered in [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml),
    *Building the 5.x Linux Kernel from Source – Part 2*, in the *Kernel build for
    the Raspberry Pi* section*;* do refer to it and obtain the source tree.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once this step completes, you should see a directory named `linux`; it holds
    the Raspberry Pi kernel source for (as of the time of writing) kernel version
    5.4.y. What''s the value of `y`? That''s easy; just do the following:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The `SUBLEVEL` variable here is the value of `y`; clearly, it's 70, making the
    kernel version 5.4.70.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s download the appropriate real-time (RTL) patch: the best one would
    be an exact match, that is, the patch should be named something like `patch-5.4.70-rt[nn].tar.xz`.
    Lucky for us, it does indeed exist on the server; let''s get it (notice that we
    download the `patch-<kver>-rt[nn]` file; it''s simpler to work with as it''s the
    unified patch):'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`wget https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/patch-5.4.70-rt40.patch.xz`.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'This does raise the question: what if the versions of the available RTL patches
    do *not *precisely match that of the device''s kernel version? Well, unfortunately,
    that does happen. In cases like this, to have the best chance of applying it against
    the device kernel, select the closest match and attempt to apply it; it often
    succeeds with perhaps minor warnings... If not, you will have to either manually
    tweak the code base to suit the patchset, or just switch to using a kernel version
    for which the RTL patch exists (recommended).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Don't forget to uncompress the patch file!
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'Now apply the patch (as shown previously, in the *Applying the RTL patch* section):'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Configure the patched kernel, turning on the `CONFIG_PREEMPT_RT` kernel config
    option (as explained previously):'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, though, as we learned in [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml), *Building
    the 5.x Linux Kernel from Source – Part 2*, it''s *critical* that you set up the
    initial kernel config appropriately for the target; here, as the target device
    is the Raspberry Pi 3[B+], do this:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Customize your kernel configuration with the `make ARCH=arm menuconfig` command.
    Here, of course, you should go to `General setup / Preemption Model`, and select
    the fourth option, `CONFIG_PREEMPT_RT`, to turn on the hard real-time preemption
    features.
  id: totrans-355
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I shall also assume that you have an appropriate toolchain for x86_64-to-ARM32
    for the Raspberry Pi installed:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Hint: Installing an appropriate toolchain (for x86_64-to-ARM32) can be as simple
    as `sudo apt install ​crossbuild-essential-armhf`. Now build the kernel (again,
    identical to the process we described previously, in the *Configuring and building
    the RTL kernel* section), with the difference being that we cross-compile it (using
    the x86_64-to-ARM32 cross-compiler we installed previously).'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the just-built kernel modules; ensure you specify the location as the
    SD card''s root filesystem with the `INSTALL_MOD_PATH` environment variable (else
    it might overwrite your host''s modules, which would be disastrous!). Let''s say
    that the microSD card''s second partition (which contains the root filesystem)
    is mounted under `/media/${USER}/rootfs`, then do the following (in one single
    line):'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Copy across the image files (the bootloader files, the kernel `zImage` file,
    the **Device Tree Blobs** (**DTBs**), the kernel modules) onto the Raspberry Pi
    SD card (these details are covered in the official Raspberry Pi documentation
    here: [https://www.raspberrypi.org/documentation/linux/kernel/building.md](https://www.raspberrypi.org/documentation/linux/kernel/building.md);
    we have also (lightly) covered this in [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml),
    *Building the 5.x Linux Kernel from Source – Part 2*).
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Test: boot the Raspberry Pi with the new kernel image in the SD card. You should
    be able to log in to a shell (typically over `ssh`). Verify the kernel version
    and config:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We are indeed running a hard real-time kernel on the device! So, good – that
    takes care of the "prep" portion; you are now in a position to proceed with the
    next step.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Installing cyclictest (and other required packages) on the device
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We intend to run test cases via the cyclictest app against both the standard
    and the newly minted RTL kernel. This implies, of course, that we must first obtain
    the cyclictest sources and build it on the device (note that the work here is
    being carried out on the Raspberry Pi).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Here's an article that does this very thing: *Latency of Raspberry Pi 3 on Standard
    and Real-Time Linux 4.9* *Kernel*: [https://metebalci.com/blog/latency-of-raspberry-pi-3-on-standard-and-real-time-linux-4.9-kernel/](https://metebalci.com/blog/latency-of-raspberry-pi-3-on-standard-and-real-time-linux-4.9-kernel/).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'It mentions an issue faced running the RTL kernel on the Raspberry Pi 3 as
    well as a workaround (important!): (in addition to the usual ones) pass along
    these two kernel parameters: `dwc_otg.fiq_enable=0` and `dwc_otg.fiq_fsm_enable=0`. You
    can put these in the `/boot/cmdline.txt` file on the device.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'First, do ensure that all required packages are installed onto your Raspberry
    Pi:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The `libnuma-dev` package is optional and may not be available on the Raspberry
    Pi OS (you can proceed even without it).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now get the source code of cyclictest:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'A bit peculiarly, initially, there will exist precisely one file, the `README`.
    Read it (surprise, surprise). It informs you how to obtain and build the stable
    version; it''s simple, just do the following:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Happily for us, the **Open Source Automation Development Lab** (**OSADL**)
    has a very useful bash script wrapper over cyclictest; it runs cyclictest and
    even plots a latency graph. Grab the script from here: [https://www.osadl.org/uploads/media/mklatencyplot.bash](https://www.osadl.org/uploads/media/mklatencyplot.bash) (explanatory
    note on it: [https://www.osadl.org/Create-a-latency-plot-from-cyclictest-hi.bash-script-for-latency-plot.0.html?&no_cache=1&sword_list[0]=cyclictest](https://www.osadl.org/Create-a-latency-plot-from-cyclictest-hi.bash-script-for-latency-plot.0.html?&no_cache=1&sword_list%5B0%5D=cyclictest)).
    I have lightly modified it for our purposes; it''s here in the GitHub repository
    for this book: `ch11/latency_test/latency_test.sh`.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Running the test cases
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To get a good idea regarding the system (scheduling) latencies, we shall run
    three test cases; in all three, the cyclictest app will sample system latency
    while the `stress(1)` utility is putting the system under load:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Raspberry Pi 3 model B+ (4 CPU cores) running the 5.4 32-bit RTL-patched kernel
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Raspberry Pi 3 model B+ (4 CPU cores) running the standard 5.4 32-bit Raspberry
    Pi OS kernel
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: x86_64 (4 CPU cores) Ubuntu 20.04 LTS running the standard 5.4 (mainline) 64-bit
    kernel
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We use a small wrapper script called `runtest` over the `latency_test.sh` script for
    convenience. It runs the `latency_test.sh` script to measure system latency while
    running the `stress(1)` utility; it invokes `stress` with the following parameters,
    to impose CPU, I/O, and memory loads on the system:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '(FYI, a later version of `stress` called `stress-ng` is available as well.)
    While the `stress` app executes, loading the system, the `cyclictest(8)` app samples
    system latencies, writing its `stdout` to a file:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: (Do refer to the man pages on both `stress(1)` and `cyclictest(8)` to understand
    the parameters.) It will run for an hour (for more accurate results, I suggest
    you run the test for a longer duration – perhaps 12 hours). Our `runtest` script
    (and the underlying ones) internally runs `cyclictest` with appropriate parameters;
    it captures and displays the minimum, average, and maximum latency wall clock
    time taken (via `time(1)`), and generates a histogram plot. Note that here, we
    run `cyclictest` for a (maximum) duration of an hour.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: By default, our `runtest` wrapper script has a variable LAT with the pathname
    to the `latency_tests` directory set as follows: `LAT=~/booksrc/ch11/latency_tests`.
    Ensure that you first update it to reflect the location of the `latency_tests`
    directory on your system.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 'A screenshot of running the scripts for our test case #1 – on the Raspberry
    Pi 3B+ running the RTL kernel – is seen here:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a153d37e-90f6-41a0-bd42-98f3529de031.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – Running our first test case for cyclictest on a Raspberry Pi
    3B+ on the RTL kernel while under stress
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Study the preceding screenshot; you can clearly see the system details, the
    kernel version (notice it's the RTL-patched `PREEMPT_RT` kernel!), and cyclictest's
    latency measurement results for the minimum, average, and maximum (scheduling)
    latency.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the results
  id: totrans-392
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We carry out a similar procedure for the remaining two test cases and summarize
    the results of all three in Figure 11.14:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fda2c55a-4879-4baf-98d8-3ee0c4021206.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – Results of the (simplistic) test cases we ran showing the min/avg/max
    latencies for different kernels and systems while under some stress
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: Interesting; though the maximum latency of the RTL kernel is much below the
    other standard kernels, both the minimum and, more importantly, average latencies
    are superior for the standard kernels. This ultimately results in superior overall
    throughput for the standard kernels (this very same point was stressed upon earlier).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 'The `latency_test.sh` bash script invokes the `gnuplot(1)` utility to generate
    graphs, in such a manner that the title line shows the minimum/average/maximum
    latency values (in microseconds) and the kernel the test was run upon. Recollect
    that test case #1 and #2 ran on the Raspberry Pi 3B+ device, whereas test case
    #3 ran on a generic (and more powerful) x86_64 system). See here the `gnuplot`-ed
    graphs (for all three test cases):'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9039fe61-7614-4f9f-96c8-6cdfd4dae665.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.15 – Test case #1 plot: cyclictest latency measurement on Raspberry
    Pi 3B+ running the 5.4 RTL kernel'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.15 shows the graph plotted by `gnuplot(1)` (called from within our `ch11/latency_test/latency_test.sh`
    script) for test case #1\. The **Device Under Test** (**DUT**), the Raspberry
    Pi 3B+, has four CPU cores (as seen by the OS). Notice how the graph shows us
    the story – the vast majority of samples are on the upper left, implying that,
    most of the time, the latency was very small (between 100,000 to 1 million latency
    samples (y-axis) fall between a few microseconds to 50 microseconds (x-axis)!).
    That''s really good! Of course, there will be outliers at the other extreme –
    samples on all CPU cores have much higher latencies (between 100 and 256 microseconds)
    though the number of samples is much smaller. The cyclictest app gives us the
    minimum, average, and maximum system latency values. With the RTL-patched kernel,
    while the max latency is actually excellent (quite low), the average latency can
    be fairly high:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e65ec4f6-374d-49c3-91bb-07520aaff1e2.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.16 – Test case #2 plot: cyclictest latency measurement on Raspberry
    Pi 3B+ running the standard (mainline) 5.4 kernel'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.16 shows the plot for test case #2\. Again, as with the previous
    test case – in fact, even more pronounced here – the vast majority of system latency
    samples exhibit very low latency! The standard kernel thus does a tremendous job;
    even the average latency is a "decent" value. However, the worst-case (max) latency
    value can be very large indeed – *showing us exactly why it''s not an RTOS*. For
    most workloads, the latency tends to be excellent "usually", but a few corner
    cases will tend to show up. In other words, it''s *not deterministic* – the key
    characteristic of an RTOS:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d539b8dd-14e8-43cc-a86f-9db98d5af156.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.17 – Test case #3 plot: cyclictest latency measurement on an x86_64
    Ubuntu 20.04 LTS running the standard (mainline) 5.4 kernel'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.17 shows the plot for test case #3\. The variance – or **jitter** –
    here is even more pronounced (again, non-deterministic!), though the minimum and
    average system latency values are really very good. Of course, it''s run on a
    far more powerful system – a desktop-class x86_64 – than the previous two test
    cases. The max latency value – the few corner cases, although there are more of
    them here – tends to be quite high. Again, it''s not an RTOS – it''s not deterministic.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: 'Did you notice how the graphs clearly exhibit *jitter*: with test case #1 having
    the least amount (the graph tends to drop down to the x-axis quite quickly - meaning
    a very tiny number of latency samples, if not zero, exhibit high(er) latencies)
    and test case #3 having the most jitter (with much of the graph remaining well
    above the *x* axis!).'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we emphasize this point: the results quite clearly show that it''s deterministic
    (a very small amount of jitter) with an RTOS and highly non-deterministic with
    a GPOS! (As a rule of thumb, standard Linux will result in approximately +/- 10
    microseconds of jitter for interrupt processing, whereas on a microcontroller
    running an RTOS, the jitter will be far less, around +/- 10 nanoseconds!)'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Doing this experiment, you will realize that benchmarking is a tricky thing;
    you shouldn't read too much into a few test runs (running the tests for a long
    while, having a large sample set, is important). Testing with realistic work loads
    you expect to experience on the system would be a far better way to see which
    kernel configuration yields superior performance; it does indeed vary with the
    workload!
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: (An interesting case study by Canonical shows statistics for regular, low-latency,
    and real-time kernels for certain workloads; look it up in the *Further reading*
    section of this chapter.) As mentioned before, quite often, the superior *max*
    latency characteristics of an RTL kernel can lead to inferior overall throughput
    (user space might suffer from reduced CPU due to RTL's rather ruthless prioritization).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Measuring scheduler latency via modern BPF tools
  id: totrans-411
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Without going into too many details, we'd be amiss to leave out the recent and
    powerful [e]BPF Linux kernel feature and it's associated frontends; there are
    a few to specifically measure scheduler and runqueue-related system latencies.
    (We covered the installation of the [e]BPF tools back in [Chapter 1](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml), *Kernel
    Workspace Setup* under the *Modern tracing and performance analysis with [e]BPF* section).
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes some of these tools (BPF frontends); all these
    tools need to be run as root (as with any BPF tool); they show their output as
    a histogram (with the time in microseconds by default):'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '| **BPF tool** | ** What it measures** |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
- en: '| `runqlat-bpfcc` | Time a task spends waiting on a runqueue for it''s turn
    to run on the processor |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
- en: '| `runqslower-bpfcc` | (read as runqueue slower); time a task spends waiting
    on a runqueue for it''s turn to run on the processor, showing only those threads
    that exceed a given threshold, which is 10 ms by default (can be tuned by passing
    the time threshold as a parameter, in microseconds); in effect, you can see which
    tasks face (relatively) long scheduling delays |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
- en: '| `runqlen-bpfcc` | Shows scheduler runqueue length + occupancy (number of
    threads currently enqueued, waiting to run) |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
- en: The tools can also provide these metrics on a per-task basis, for every process
    on the system or even by PID namespace (for container analysis; of course, these
    options depend on the tool in question). Do look up more details (and even example
    usage!) from the man pages (section 8) on these tools.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'There are even more [e]BPF frontends related to scheduling: `cpudist- cpudist-bpfcc`, `cpuunclaimed-bpfcc`,
    `offcputime-bpfcc`, `wakeuptime-bpfcc`, and so on. See the *Further reading* section
    for resources.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: 'So, there you are: by now, you''re able to not only understand but even measure
    system latencies (via both the `cyclictest` app and a few modern BPF tools).'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'We close this chapter with a few miscellaneous, yet useful small (kernel space)
    routines to check out:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '`rt_prio()`: Given the priority as a parameter, returns a Boolean to indicate
    whether it''s a real-time task or not.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rt_task()`: Based on the priority value of the task, given the task structure
    pointer as a parameter, returns a Boolean to indicate whether it''s a real-time
    task or not (a wrapper over `rt_prio()`).'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`task_is_realtime()`: Similar, but based on the scheduling policy of the task.
    Given the task structure pointer as a parameter, returns a Boolean to indicate
    whether it''s a real-time task or not.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-425
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this, our second chapter on CPU scheduling on the Linux OS, you have learned
    several key things. Among them, you learned how to visualize kernel flow with
    powerful tools such as LTTng and the Trace Compass GUI, as well as with the `trace-cmd(1)`
    utility, a convenient frontend to the kernel's powerful Ftrace framework. You
    then saw how to programatically query and set any thread's CPU affinity mask.
    This naturally led to a discussion on how you can programmatically query and set
    any thread's scheduling policy and priority. The whole notion of being "completely
    fair" (via the CFS implementation) was brought into question, and some light was
    shed on the elegant solution called cgroups. You even learned how to leverage
    the cgroups v2 CPU controller to allocate CPU bandwidth as desired to processes
    in a sub-group. We then understood that though Linux is a GPOS, an RTL patchset
    very much exists, which, once applied and the kernel is configured and built,
    has you running Linux as a true hard real-time system, an RTOS.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned how to measure latencies on the system, via both the cyclictest
    app as well as a few modern BPF tools. We even tested with cyclictest on a Raspberry
    Pi 3 device, measuring and contrasting them on an RTL and a standard kernel.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: That's quite a bit! Do take the time to properly understand the material, and
    work on it in a hands-on fashion.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-429
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we conclude, here is a list of questions for you to test your knowledge
    regarding this chapter''s material: [https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions).
    You will find some of the questions answered in the book''s GitHub repo: [https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn).'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-431
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To help you delve deeper into the subject with useful materials, we provide
    a rather detailed list of online references and links (and at times, even books)
    in a Further reading document in this book's GitHub repository. The *Further reading*
    document is available here: [https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
