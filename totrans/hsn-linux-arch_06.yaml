- en: Analyzing Performance in a Gluster System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the [Chapter 4](51a50d0f-7932-431e-ba32-c5d62d29b71e.xhtml), *Using GlusterFS
    on the Cloud Infrastructure*, we have completed a working implementation of GlusterFS,
    we can focus on the testing aspect of the solution. We will look at a high-level
    overview of what was deployed and explain the reasoning behind the chosen components.
  prefs: []
  type: TYPE_NORMAL
- en: Once the configuration is defined, we can go through testing the performance
    to verify that we are achieving the expected results. We can then conduct availability
    testing by deliberately bringing down nodes while performing I/O.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will see how we can scale the solution both vertically and horizontally.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: A high-level overview of the implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going through Performance testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance availability testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling the solution vertically and horizontally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s the list of technical resources for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Zpool iostat—used for performance monitoring on ZFS: [https://docs.oracle.com/cd/E19253-01/819-5461/gammt/index.html](https://docs.oracle.com/cd/E19253-01/819-5461/gammt/index.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sysstat—used for live block performance statistics: [https://github.com/sysstat/sysstat](https://github.com/sysstat/sysstat)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The iostat man page containing the different options for the command: [http://sebastien.godard.pagesperso-orange.fr/man_iostat.html](http://sebastien.godard.pagesperso-orange.fr/man_iostat.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FIO documentation to provide configuration parameters and usage: [https://media.readthedocs.org/pdf/fio/latest/fio.pdf](https://media.readthedocs.org/pdf/fio/latest/fio.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GlusterFS monitoring workload documentation on how to view statistics: [https://gluster.readthedocs.io/en/latest/Administrator%20Guide/Monitoring%20Workload/](https://gluster.readthedocs.io/en/latest/Administrator%20Guide/Monitoring%20Workload/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of the implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After having the solution deployed and configured in [Chapter 4](51a50d0f-7932-431e-ba32-c5d62d29b71e.xhtml),
    *Using GlusterFS on the Cloud Infrastructure*, we can validate the performance
    of the implementation. The primary goal is to understand how this can be done
    and the tools that are available.
  prefs: []
  type: TYPE_NORMAL
- en: Let's first take a step back and see what we implemented.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 4](51a50d0f-7932-431e-ba32-c5d62d29b71e.xhtml), *Using GlusterFS
    on the Cloud Infrastructure*, we deployed GlusterFS version 4.1 on an Azure **virtual
    machine** (**VM**). We used ZFS as the storage backend for the bricks by using
    four disks per node on a three-node setup. The following diagram offers a high-level
    overview of how this is distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ac944da-0715-4ecc-a291-82eb52a1d9bb.png)'
  prefs: []
  type: TYPE_IMG
- en: This setup gives 1 TB of usable space. The volume can tolerate an entire node
    going down while still serving data to the clients.
  prefs: []
  type: TYPE_NORMAL
- en: This setup should be able to deliver approximately 375 **megabytes per second**
    (**MB/s**), handle several hundred clients at once, and should be reasonably straightforward
    to scale both horizontally and vertically.
  prefs: []
  type: TYPE_NORMAL
- en: Performance testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now need to validate that the theoretical performance can be achieved through
    actual implementation. Let's break this down into several parts.
  prefs: []
  type: TYPE_NORMAL
- en: Performance theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's figure out how much performance we should be getting based on the specifications
    of the setup. Consider that each of the nodes should provide a maximum of 125
    MB/s. The disk subsystem is more than capable of delivering performance since
    each disk yields 60 MB/s.
  prefs: []
  type: TYPE_NORMAL
- en: The total achievable performance should be around 375 MB/s, assuming that the
    client or clients can keep up by sending or requesting enough data to the volume.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll be using three main tools to validate and test the performance of the
    solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '`zpool iostat`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iostat`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible I/O tester** (**FIO**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these tools works at a different level. Let's now detail what each one
    does and how to understand the information that they provide.
  prefs: []
  type: TYPE_NORMAL
- en: The ZFS zpool iostat command
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ZFS works on the backend volume level; the `zpool iostat -v` command gives performance
    statistics for each of the members in the ZFS volume and statistics for the ZFS
    volume as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: The command can provide real-time data by passing a number in seconds that it
    will iterate after that period of time has passed. For example, `zpool iostat
    -v 1` reports disk statistics each second. Here, the `-v` option shows each of
    the members of the pool and their respective data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This tool helps to present the performance at the lowest level possible because
    it shows data from each of the disks, from each of the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c22ef836-7633-4f8f-acdb-e7f1108b756c.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we used the extra `-L` and `-P` options so that the absolute paths
    of the device's files or the **Universally Unique Identifier** (**UUID**) are
    printed; this is because we created the pool using the unique identifier of each
    of the disks.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding screenshot, we can see four main groups, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pool`: This is created with each of the members.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`capacity`: This is the amount of space that is allocated to each device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`operations`: This is the number of IOPSes that are done on each of the devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bandwidth`: This is the throughput of each device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first line, the command prints the statistics since the last boot. Remember
    that this tool helps to present the performance from a ZFS-pool level.
  prefs: []
  type: TYPE_NORMAL
- en: iostat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As part of the `sysstat` package, `iostat` provides low-level performance statistics
    from each of the devices. `iostat` bypasses filesystems and volumes and presents
    the RAW performance data from each of the block devices in the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `iostat` tool can be run with options to alter the information that is
    printed onscreen, for example, `iostat -dxctm 1`. Let''s explore what each part
    does:'
  prefs: []
  type: TYPE_NORMAL
- en: '`iostat`: This is the primary command.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`d`: This prints the device utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x`: This displays the extended device statistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`c`: This displays the CPU utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t`: This displays the time for each report printed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`m`: This ensures that the statistics will be displayed in MB/s.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1`: This is the amount of time in seconds in which `iostat` prints data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following screenshot, you can see that `iostat` displays information
    in different columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecf9dba7-7455-4f09-8839-5f6db3e3cf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There''s no need to go through all of the columns, but the most important ones
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Device`: This shows the block devices that are present on the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r/s`: These are the read operations per second.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`w/s`: These are the write operations per second.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rMB/s`: These are the MB/s read from the device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wMB/s`: These are the MB/s written to the device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r_await`: This is the average time in milliseconds for read requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`w_await`: This is the average time in milliseconds for write requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `r_await` and `w_await` columns in conjunction with the `avg-cpu %iowait`
    time are essential; this is because these metrics can help determine whether one
    of the devices has increased latency over the others. A high CPU `iowait` time
    means that the CPU is continuously waiting for I/O to complete, which, in turn,
    might mean that the block devices have high latency.
  prefs: []
  type: TYPE_NORMAL
- en: The `iostat` tool can be run on each of the nodes in the cluster, providing
    low-level statistics for each of the disks that make up the GlusterFS volume.
  prefs: []
  type: TYPE_NORMAL
- en: Details on the rest of the columns can be found on the man page for `iostat`.
  prefs: []
  type: TYPE_NORMAL
- en: The FIO tester
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FIO is a benchmarking tool that is used to conduct performance testing by generating
    synthetic workloads and presenting a summary of the I/O metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `fio` does not come by default on CentOS, but it is available in the
    base repository and can be installed by running `sudo yum install -y fio`.
  prefs: []
  type: TYPE_NORMAL
- en: This tool is exceptionally helpful as it allows us to perform tests that are
    close to what the real workload of the system will be—by allowing the user to
    change parameters such as the block size, file size, and thread count. FIO can
    deliver data that is close to real-world performance. This level of customization
    can be potentially confusing as it provides many options for workload simulation,
    and some of these are not very intuitive at first.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to perform testing with FIO is by creating a configuration
    file, which tells the software how to behave; a configuration file looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s break it down so that we can understand how each part of the configuration
    file works:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[global]`: This denotes the configuration parameters that affect the entire
    test (parameters for individual files can be set).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name=`: This is the name of the test; it can be anything meaningful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rw=randrw`: This tells FIO what type of I/O to perform; in this case, it does
    random reads and writes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rwmixread` and `rwmixwrite`: These tell FIO what percentage or mix of reads
    and writes to perform—in this case, it is a 50-50 mix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`group_reporting=1`: This is used to give statistics for the entire test rather
    than for each of the jobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bs=1M`: This is the block size that FIO uses when performing the test; it
    can be changed to a value that mimics the workload intended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numjobs=4`: This controls how many threads are opened per file. Ideally, this
    can be used to match the number of users or threads that will be using the storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`runtime=180`: This controls, in seconds, how long the test will run for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ioengine=libaio`: This controls the type of I/O engine to be used. The most
    common is `libaio` as it resembles most workloads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iodepth=64`: This controls the I/O depth of the test; a higher number allows
    the storage device to be used at its fullest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the file group controls how many files are created for the test and
    what their size will be. Certain settings, such as `iodepth`, can be added to
    this group that only affect the file where the parameter is defined. Another consideration
    is that `fio` opens a thread based on the `numjobs` parameter for each of the
    files. In the preceding configuration, it will open a total of 16 threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run FIO, simply move into the directory where the mount point is located
    and point it to the configuration file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that FIO requires root privileges, so make sure that FIO is run with `sudo`.
  prefs: []
  type: TYPE_NORMAL
- en: 'While FIO is running, it displays statistics such as throughput and IOPS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de6fd649-943a-4f3e-809d-96770f8fa49a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once done, FIO reports the test statistics on screen. The main things to look
    for are the IOPS and **bandwidth** (**BW**) for both read and write operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab8a1e8b-1fb9-4905-9803-8e2156cf7651.png)'
  prefs: []
  type: TYPE_IMG
- en: From the test results, we can see that the GlusterFS volume can sustain about
    150 MB/s of both read and write operations simultaneously. We're off by 75 MB/s
    from the theoretical maximum performance of the cluster; in this specific case,
    we're hitting a network limit.
  prefs: []
  type: TYPE_NORMAL
- en: FIO can be extremely effective at validating performance and detecting problems; `fio`
    can be run on clients mounting the Gluster volume or directly on the bricks of
    each of the nodes. You can use FIO for testing existing solutions in order to
    validate performance needs; just make sure the settings in the FIO configuration
    are changed based on what needs to be tested.
  prefs: []
  type: TYPE_NORMAL
- en: GlusterFS provides some tools to monitor performance from the perspective of
    volume. These can be found in the GlusterFS documentation page, under *Monitoring
    Workload*.
  prefs: []
  type: TYPE_NORMAL
- en: Availability testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Making sure that the cluster is able to tolerate a node going down is crucial
    because we can confirm that no downtime occurs if a node is lost.
  prefs: []
  type: TYPE_NORMAL
- en: This can be done by forcibly shutting down one of the nodes while the others
    continue to serve data. To function as a synthetic workload, we can use FIO to
    perform a continuous test while one of the nodes is being shut down.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see that the `gfs2` node was not present,
    but the FIO test continued serving data as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac25b2de-f97a-4d29-bd05-06fdf2b45162.png)'
  prefs: []
  type: TYPE_IMG
- en: Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling this setup is relatively straightforward. As previously mentioned, we
    can either scale vertically, by adding more disks to each of the nodes, or scale
    horizontally, by adding more nodes to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling vertically is considerably simpler than horizontally as it requires
    fewer resources. For example, a single disk can be added to the ZFS pool on each
    of the nodes—effectively increasing the available space by 256 GB if three 128
    GB disks are added.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding disks to the ZFS pool can be done with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: From the previous command, `brick1` is the name of the pool and `disk-id` is
    the UUID of the recently added disk or disks.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling horizontally requires the exact setup to be mirrored on a new node and
    then added to the cluster. This requires a new set of disks. The advantage is
    that the available space and performance will grow accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at an overview of the implementation done in the
    previous [Chapter 4](51a50d0f-7932-431e-ba32-c5d62d29b71e.xhtml), *Using GlusterFS
    on the Cloud Infrastructure*, so that we could have a fresh understanding of what
    was implemented in order to understand how we could test performance. Given the
    previous setup, the implementation should be capable of a theoretical 375 MB/s
    of throughput. We can validate this number with several tools that work at different
    levels.
  prefs: []
  type: TYPE_NORMAL
- en: For ZFS volumes, we can use the `zpool iostat` command, which provides data
    for each of the block devices that are part of the ZFS volume. `iostat` can be
    used to determine performance for all of the block devices present in the system.
    These commands can only be run on each of the nodes of the cluster. To be able
    to verify the actual performance of the implementation, we used the FIO tool,
    which can simulate specific workloads by changing the parameters of how I/O is
    performed. This tool can be used on each of the nodes on the brick level or on
    each of the Gluster clients on the GlusterFS volume to get a general overview
    of the performance that is achievable by the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We went through how we can perform availability testing by purposely shutting
    down one of the nodes while performing a test through FIO. Finally, scaling the
    solution can be done either vertically, by adding disks to each of the volumes
    in each of the nodes, or horizontally, by adding an entirely new node to the cluster. Your
    main takeaway from this chapter is to consider how the configuration that was
    implemented can be validated using widely available tools. These are just a set
    of tools. Many other tools might be available, which could be better for the solution
    that you're implementing.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll jump into creating a highly-available self-healing
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is MB/s?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `zpool iostat`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Where can I run `zpool iostat`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `iostat`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does `r_await` mean?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is CPU IOWAIT time?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is FIO?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can I run FIO?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an FIO configuration file?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can I validate availability in a Gluster cluster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can I scale vertically?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Learning Microsoft Azure Storage* by Mohamed Waly: [https://www.packtpub.com/big-data-and-business-intelligence/learning-microsoft-azure-storage](https://www.packtpub.com/big-data-and-business-intelligence/learning-microsoft-azure-storage)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
