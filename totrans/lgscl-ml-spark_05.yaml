- en: Chapter 5.  Supervised and Unsupervised Learning by Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 2](part0023_split_000.html#LTSU2-5afe140a04e845e0842b44be7971e11a
    "Chapter 2. Machine Learning Best Practices"), *Machine Learning Best Practices* readers,
    learned some theoretical underpinnings of basic machine learning techniques. Whereas,
    [Chapter 3](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d "Chapter 3. Understanding
    the Problem by Understanding the Data"), *Understanding the Problem by Understanding
    the Data,* describes the basic data manipulation using Spark''s APIs such as RDD,
    DataFrame, and Datasets. [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Knowledge
    through Feature Engineering*, on the other hand, describes feature engineering
    from both the theoretical and practical point of view. However, in this chapter,
    the reader will learn the practical know-how needed quickly and powerfully to
    apply supervised and unsupervised techniques on the available data to the new
    problems through some widely used examples based on the understandings from the
    previous chapters. These examples we are talking about will be demonstrated from
    the Spark perspective. In a nutshell, the following topics will be covered throughout
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommender system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced learning and generalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As stated in [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 1. Introduction to Data Analytics with Spark"), *Introduction to Data
    Analytics with Spark* and [Chapter 2](part0023_split_000.html#LTSU2-5afe140a04e845e0842b44be7971e11a
    "Chapter 2. Machine Learning Best Practices"), *Machine* *Learning Best Practices*,
    machine learning techniques can be categorized further into three major classes
    of algorithms: supervised learning, unsupervised learning, and the recommender
    system. Where classification and regression algorithms are widely used in the
    supervised learning application development, clustering, on the other hand, falls
    in the category of unsupervised learning. In this section, we will describe some
    examples of the supervised learning technique.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we will provide some example of the same example presented using Spark.
    On the other hand, an example of the clustering technique will be discussed in
    the section: *Unsupervised learning*, where a regression technique often models
    the past relationship between variables to predict their future changes (up or
    down). Here we show two real-life examples of classification and regression algorithms
    respectively. In contrast, a classification technique takes a set of data with
    known labels and learns how to label new records based on that information:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example (classification)**: Gmail uses a machine learning technique called
    classification to designate if an e-mail is spam or not, based on the data of
    an e-mail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example (regression)**: As an example, suppose you are an online currency
    trader and you work on Forex or Fortrade. Right now you have two currency pairs
    in mind to buy or sell say: GBP/USD and USD/JPY. If you look at these two pairs
    carefully, USD is a common in these two pairs. Now if you look at the historical
    prices of USD, GBP, or JPY you can predict the future outcome of whether you should
    open the trade in buy or sell. These types of problems can be resolved with supervised
    learning techniques using regression analysis:![Machine learning classes](img/00164.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 1: Classification, clustering, and collaborative filtering-the big picture'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, clustering and dimensionality reduction are commonly used
    for unsupervised learning. Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example (clustering)**: Google News uses a technique called clustering to
    group news articles into different categories, based on title and content. Clustering
    algorithms discover groupings that occur in collections of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example (collaborative filtering)**: The collaborative filtering algorithm
    is often used in the recommendation system development. Renowned companies such
    as Amazon and Netflix use a machine learning technique called collaborative filtering,
    to determine which products users will like based on their history and similarity
    to other users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example (dimensionality reduction)**: Dimensionality reduction is often used
    to make the available dataset that is high dimensional. For example, suppose you
    have an image of size 2048x1920, and you would like to reduce the dimension to
    1080x720 without sacrificing the quality much. In this case, popular algorithms
    such as **Principal Component Analysis** (**PCA**) or **Singular Value Decomposition**
    (**SVD**) can be used although you can also implement the SVD to implement the
    PCA. This is why SVD is more widely used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As already stated, a supervised learning application makes predictions based
    on a set of examples and the goal is to learn general rules that map inputs to
    outputs aligning with the real world. For example, a dataset for spam filtering
    usually contains spam messages as well as non-spam messages. Consequently, we
    could know which messages in the training set are spams or non-spam. Therefore,
    supervised learning is the machine learning technique of inferring a function
    from the labeled training data. The following steps are involved in supervised
    learning tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the ML model with the training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the test dataset to test the model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, the dataset for training the ML model, in this case, is labeled with
    the value of interest and a supervised learning algorithm looks for patterns in
    those value labels. After the algorithm has found the required patterns, those
    patterns can be used to make predictions for unlabeled test data.
  prefs: []
  type: TYPE_NORMAL
- en: A typical use of the supervised learning is diverse and commonly used in the
    bioinformatics, cheminformatics, database marketing, handwriting recognition,
    information retrieval, object recognition in computer vision, optical character
    recognition, spam detection, pattern recognition, speech recognition, and so on,
    and in these applications mostly the classification technique is used. On the
    other hand, supervised learning is a special case of downward causation in biological
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More on how the supervised learning technique works from the theoretical perspective
    can be found on these books: Vapnik, V. N. *The Nature of Statistical Learning
    Theory (2nd Ed.)*, Springer Verlag, 2000; and Mehryar M., Afshin R. Ameet T. (2012)
    Foundations of Machine Learning, The MIT Press ISBN 9780262018258.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Classification is a family of supervised machine learning algorithms that designate
    input as belonging to one of the several pre-defined classes. Some common use
    cases for classification include:'
  prefs: []
  type: TYPE_NORMAL
- en: Credit card fraud detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: E-mail spam detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification data is labeled, for example, as spam/non-spam or fraud/non-fraud.
    Machine learning assigns a label or class to new data. You classify something
    based on pre-determined features. Features are the *if questions* that you ask.
    The label is the answer to those questions. For example, if an object walks, swims,
    and quacks like a duck, then the label would be *duck*. Or suppose for a flight
    is delayed on to be a departure or arrival by more than say 1 hour, it would be
    a delay; otherwise not a delay.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning with Spark - an example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will demonstrate an example by analyzing an air-flight delay. The dataset
    named `On_Time_Performance_2016_1.csv` from the United Department of Transportation
    website at [http://www.transtats.bts.gov/](http://www.transtats.bts.gov/) will
    be used.
  prefs: []
  type: TYPE_NORMAL
- en: Air-flight delay analysis using Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are using flight information for 2016\. For each flight, we have the following
    information presented in *Table 1* (we have presented only a few fields out of
    444,827 rows and 110 columns as of May 17, 2016):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Data field** | **Description** | **Example value** |'
  prefs: []
  type: TYPE_TB
- en: '| `DayofMonth` | Day of month | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| `DayOfWeek` | Day of week | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| `TailNum` | Tail number for the plane | N505NK |'
  prefs: []
  type: TYPE_TB
- en: '| `FlightNum` | Flight number | 48 |'
  prefs: []
  type: TYPE_TB
- en: '| `AirlineID` | Airline ID | 19805 |'
  prefs: []
  type: TYPE_TB
- en: '| `OriginAirportID` | Origin airport ID | JFK |'
  prefs: []
  type: TYPE_TB
- en: '| `DestAirportID` | Destination airport ID | LAX |'
  prefs: []
  type: TYPE_TB
- en: '| `Dest` | Destination airport code | 1424 |'
  prefs: []
  type: TYPE_TB
- en: '| `CRSDepTime` | Schedule departure time | 10:00 |'
  prefs: []
  type: TYPE_TB
- en: '| `DepTime` | Actual departure time | 10:30 |'
  prefs: []
  type: TYPE_TB
- en: '| `DepDelayMinutes` | Departure delay in minutes | 30 |'
  prefs: []
  type: TYPE_TB
- en: '| `CRSArrTime` | Schedule arrival time | 22:45 |'
  prefs: []
  type: TYPE_TB
- en: '| `ArrTime` | Actual arrival time | 23:45 |'
  prefs: []
  type: TYPE_TB
- en: '| `ArrDelayMinutes` | Arrival delay in minutes | 60 |'
  prefs: []
  type: TYPE_TB
- en: '| `CRSElapsedTime` | Elapsed time | 825 |'
  prefs: []
  type: TYPE_TB
- en: '| `Distance` | Total distance | 6200 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Sample data from the "On Time On Time Performance 2016_1" dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this scenario, we will build a tree to predict the label of delayed or not
    delayed based on the following features shown in the figure, which is small snapshot
    of an air flight dataset. Here `ArrDelayMinutes` is 113 which should be classified
    in delayed (1.0) and other rows are less than 60 minutes so the label should be
    0.0 (not delayed). From this dataset, we will do some operation such as feature
    extraction, transformation, and selection. *Table 2* shows the top five rows related
    to the features we will be considering for this example as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Label**: Delayed and not delayed - delayed if delay >60 minutes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Features**: {`DayOfMonth`, `WeekOfday`, `CRSdeptime`, `CRSarrtime`, `Carrier`,
    `CRSelapsedtime`, `Origin`, `Dest`, `ArrDelayMinutes`}![Air-flight delay analysis
    using Spark](img/00004.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 2: Selected feature for air-flight delay prediction'
  prefs: []
  type: TYPE_NORMAL
- en: Loading and parsing the Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before performing the feature extraction, we need to load and parse the dataset.
    This step also includes: loading packages and related dependencies, reading the
    dataset as a DataFrame, making the POJO or Bean class, and adding the new label
    column based on requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Load required packages and dependencies**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For reading csv files, we used the csv reader provided by the Databricks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Create the Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to create a Spark session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Read and parse the csv file using Dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset contains many columns that we will not include as a feature in
    this example. So we will select from the DataFrame only the features that we have
    mentioned previously. This DataFrame output has already been shown in *Figure
    2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a the output of top 5 rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading and parsing the Dataset](img/00081.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 4: Making a POJO or Bean class**'
  prefs: []
  type: TYPE_NORMAL
- en: The POJO class we have developed is called `Flight` where the required features
    and label field will be defined with the corresponding setter and getter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We believe the preceding class is self-explanatory, which is used for setting
    and getting the feature values from the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5: Adding the new label column based on the delay column**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the delay is greater than 40 then the label should be 1 otherwise it should
    be 0\. Create a new Dataset using the Flight bean class. This dataset can contain
    an empty string for the `ArrDelayMinutes` column. So before mapping we filtered
    the rows containing the empty string from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now create a new Dataest from the RDD we created above as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now show the top 5 rows from the data frame `flightDelayData` in following
    *Figure 3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[Output:]'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading and parsing the Dataset](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3:The DataFrame showing the new label column
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For extracting the feature, we have to make numerical values and if there are
    any text values, then we have to make a labeled vector for applying a machine
    learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1 :Transformation towards feature extraction**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we will transform the columns containing text into double values columns.
    Here we use `StringIndexer` for making a unique index for each unique text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[Output]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature extraction](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Uunique indices for each unique text'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2: Making the feature vectors using the vector assembler**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make the feature vector with the vector assembler and transform it to the labelled
    vector for applying the machine learning algorithm (decision tree). Note, here
    we used the decision tree to show just an example since it shows better classification
    accuracies. Based on the algorithm and model selection and tuning, you will be
    further able to explore and use other classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now transform the assembler into a Dataset of row as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now convert the Dataset into `JavaRDD` for making the feature vectors as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Map the RDD for `LabeledPoint` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now print the first five values as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[Output]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature extraction](img/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The corresponding assembled vectors'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the training and testing set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here we will prepare the training dataset from the dataset of the labeled vector.
    Initially, we will make a training set where 15% of records will be non-delayed
    and 85% will be delayed records. Finally, the training and testing dataset will
    be prepared as 70% and 30% respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Make training and test set from the whole Dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a new RDD by filtering the RDD based on the labels (that is,
    1 and 0) we created previously as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now union the two RDDs using the `union()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now further convert the combined RDD into Dataset of Row as follows (max categories
    is set to be 4):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to do the vector indexer for the categorical variables as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the feature indexer using the `VectorIndexerModel` estimator.
    Now the next task is to do the string indexing using the `StringIndexerModel`
    estimator as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, split the Dataset of Row into training and test (70% and 30% respectively
    but you should adjust the values based on your requirements) set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Well done! Now our dataset is ready to train the model, right? For the time
    being, we will naively select a classifier to say let's use the decision tree
    classifier to solve our purpose. You can try this with other multiclass classifiers
    based on examples provided in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*, [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*,
    and [Chapter 8](part0067_split_000.html#1VSLM1-5afe140a04e845e0842b44be7971e11a
    "Chapter 8.  Adapting Your Machine Learning Models"), *Adapting Your Machine Learning
    Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in *Figure 2*, training and test data will be collected from the raw
    data. After the feature engineering process has been done, the RDD of feature
    vectors with labels or ratings will be used next to be processed by the classification
    algorithm before building the predictive model (as shown in *Figure 6*) and at
    the end the test data will be used for testing the model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the model](img/00070.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Supervised learning using Spark'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we prepare the values for the parameters that will be required for the
    decision tree. You might have wondered why we are talking about the decision tree.
    The reason is simple since using the decision tree (that is, the **binary decision
    tree**)we observed better prediction accuracy compared to the Naive Bayes approaches.
    Refer to *Table 2* which describes the categorical features and their significance
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Categorical features** | **Mapping** | **Significant** |'
  prefs: []
  type: TYPE_TB
- en: '| categoricalFeaturesInfo | 0 -> 31 | Specifies that the feature index 0 (which
    represents the day of the month) has 31 categories [values {0, ..., 31}] |'
  prefs: []
  type: TYPE_TB
- en: '| categoricalFeaturesInfo | 1 -> 7 | Represents days of the week, and specifies
    that the feature index 1 has seven categories |'
  prefs: []
  type: TYPE_TB
- en: '| Carrier | 0 -> N | N signifies the numbers from 0 to up to the number of
    distinct carriers |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Categorical features and their significance'
  prefs: []
  type: TYPE_NORMAL
- en: Now we will describe the approach of the decision tree construction in brief.
    We will use the CategoricalFeaturesInfo that specifies which features are categorical
    and how many categorical values each of those features can take during the tree
    construction process. This is given as a map from the feature index to the number
    of categories for that feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the model is trained by making associations between the input features
    and the labeled output associated with those features. We train the model using
    the `DecisionTreeClassifier` method which returns a `DecisionTreeModel` eventually
    as shown in *Figure 7*. The detailed source code for constructing the tree will
    be shown later in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the model](img/00084.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The binary decision tree generated for the air-flight delay analysis
    (partially shown)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Train the decision tree model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the decision tree classifier model, we need to have the necessary
    labels and features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Convert the indexed labels back to original labels**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a decision tree pipeline, we need to have the original labels apart
    from the indexed labels. So, let''s do it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Chain the indexer and tree in a single pipeline**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new pipeline where the stages are as follows: `labelIndexer`, `featureIndexer`,
    `dt`, `labelConverter` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now fit the pipeline using the training set we created in *Step 8* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Testing the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here we will test the models as shown in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Make the prediction on the test dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make the prediction on the test set by transforming the `PipelineModel` and
    show the performance parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Evaluate the model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluate the model by the multiclass classification evaluator and print the
    accuracy and test error as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment produces the classification accuracy and test error
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Please note that since we randomly split the dataset into training and testing,
    you might get a different result. The classification accuracy is 75.40%, which
    is not good, we believe.
  prefs: []
  type: TYPE_NORMAL
- en: However, now it's your turn to use a different classifier and tune before deploying
    the model. A more details discussion will be carried out in [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models,* regarding
    tuning the ML models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Print the decision tree**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to print the decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This code segment produces a decision tree as shown in *Figure 7*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Stop the Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stop the Spark session using the `stop()` method of Spark as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This is a good practice that you initiate a Spark session and close or stop
    it properly to avoid the memory leak in your applications.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In unsupervised learning, data points have no labels related to them; therefore,
    we need to put labels on them algorithmically. In other words, the correct classes
    of the training dataset in unsupervised learning are unknown.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, classes have to be inferred from the unstructured datasets which
    implies that the goal of an unsupervised learning algorithm is to pre-process
    the data in some structured ways by describing its structure. The main objective
    of the unsupervised learning algorithms or techniques is to explore the unknown
    patterns of the input data that are mostly unlabeled. In this way, it is closely
    related to the problem of density estimation used in theoretical and applied statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning, however, also comprehends many other techniques to summarize
    and explain the key features of the data including exploratory data analysis for
    finding these hidden patterns, even grouping the data points or features and applying
    the unsupervised learning technique based on data mining methods for the data
    pre-processing.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this obstacle in unsupervised learning, clustering techniques are
    used typically to group the unlabeled samples based on certain similarity measures,
    mining hidden patterns towards feature learning.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For in-depth theoretical knowledge, how the unsupervised algorithms work, please
    refer to these three books: Bousquet, O.; von Luxburg, U.; Raetsch, G., eds. (2004).
    *Advanced Lectures on Machine Learning*. Springer-Verlag. ISBN 978-3540231226\.
    Or Duda, Richard O.; Hart, Peter E.; Stork, David G. (2001). *Unsupervised Learning
    and Clustering*. *Pattern Classification (2nd Ed.)*. Wiley. ISBN 0-471-05669-3
    and Jordan, Michael I.; Bishop, Christopher M. (2004). *Neural Networks*. In Allen
    B. Tucker. Computer *Science Handbook, Second Edition (Section VII: Intelligent
    Systems)*. Boca Raton, FL: Chapman & Hall/CRC Press LLC. ISBN 1-58488-360-X.'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In clustering, an algorithm groups objects into categories by analyzing similarities
    between input examples where similar objects or features are clustered and marked
    using circles around them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering uses include: **Search results grouping** such as grouping of customers,
    **anomaly detection** for suspicious pattern finding, **text categorization**
    for finding useful patterns in the tests, **social network analysis** for finding
    coherent groups, **data center computing clusters** for finding a way of putting
    related computers together to improve performance, **astronomic data analysis**
    for galaxy formation, and **real estate data analysis** for identifying neighborhoods
    based on similar features. Moreover, clustering uses unsupervised algorithms,
    which do not have the outputs in advance.'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering using the K-means algorithm begins by initializing all the coordinates
    to centroids. Note that Spark also supports other clustering algorithms such as Gaussian
    mixture, **Power Iteration Clustering** (**PIC**), **Latent Dirichlet Allocation**
    (**LDA**), Bisecting k-means, and Streaming k-means. Whereas, the Gaussian mixture
    is mainly used for expectation minimization as an optimization algorithm, the
    LDA, on the other hand, is used for the document classification and clustering.
    PIC is used for the clustering vertices of a graph given pairwise similarities
    as edge properties. Bisecting K-means is faster than the regular K-means, but
    it will generally produce a different clustering. Therefore, to keep the discussion
    simpler we will use the K-means algorithm for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Interested readers should refer the Spark ML and Spark MLlib based clustering
    techniques at [https://spark.apache.org/docs/latest/ml-clustering.html](https://spark.apache.org/docs/latest/ml-clustering.html)
    and [https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html)
    web pages respectively to get more insights. With every pass of the algorithm,
    each point is assigned to its nearest centroid based on some distance metric,
    usually the **Euclidean distance**.
  prefs: []
  type: TYPE_NORMAL
- en: Note that there are other ways to calculate the distance, for example, the **Chebyshev
    distance** is used to measure the distance by considering only the most significant
    dimensions. The **Hamming distance algorithm** is used to identify the difference
    bit by bit of two strings. The **Mahalanobis distance** is used to normalize the
    covariance matrix to make the distance metric scale-invariant.
  prefs: []
  type: TYPE_NORMAL
- en: The **Manhattan distance** is used to measure the distance following only axis-aligned
    directions. The **Minkowski distance algorithm** is used to make the Euclidean
    distance, Manhattan distance, and Chebyshev distance generalize. The **Haversine
    distance** is used to measure the great-circle distances between two points on
    a sphere from their longitudes and latitudes. Considering these distance measuring
    algorithms, it is clear that the Euclidean distance algorithm would be the most
    appropriate to solve our problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The centroids are then updated to be the centers of all the points assigned
    to it in that pass. This repeats until there is a minimum change in the centers.
    The k-means algorithm is an iterative algorithm and works in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster assignment step**: This algorithm will go through each data point
    and, depending upon which centroid it is nearer to, it will be assigned that centroid
    and, in turn, the cluster it represents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Move centroid step**: This algorithm will take each centroid and move it
    to the mean of the data points in the cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning with Spark - an example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will use the *Saratoga NY Homes* downloaded from the URL [http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html](http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html) to
    demonstrate an example of clustering as an unsupervised learning technique using
    Spark in Java. The dataset contains several features as follows: Price, Lot Size,
    Waterfront, Age, Land Value, New Construct, Central Air, Fuel Type, Heat Type,
    Sewer Type, Living Area, Pct.College, Bedrooms, Fireplaces, Bathrooms, and the
    number of Rooms. However, among those columns, we have shown only some selected
    columns in *Table 3*. Note that the original dataset was downloaded and later
    on converted into a corresponding text file as a tab delimiter:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Price** | **Lot Size** | **Water Front** | **Age** | **Land Value** | **Rooms**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 132500 | 0.09 | 0 | 42 | 5000 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 181115 | 0.92 | 0 | 0 | 22300 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 109000 | 0.19 | 0 | 133 | 7300 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 155000 | 0.41 | 0 | 13 | 18700 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 86060 | 0.11 | 0 | 0 | 15000 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 120000 | 0.68 | 0 | 31 | 14000 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 153000 | 0.4 | 0 | 33 | 23300 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 170000 | 1.21 | 0 | 23 | 146000 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| 90000 | 0.83 | 0 | 36 | 222000 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 122900 | 1.94 | 0 | 4 | 212000 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 325000 | 2.29 | 0 | 123 | 126000 | 12 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Sample data from the "Saratoga NY Homes" dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'We further took only the first two features (that is, Price and Lot Size) using
    the Spark feature learning algorithm presented in the previous chapter for simplicity.
    Our target is to show an exploratory analysis based on these two features for
    possible neighborhoods of the house located in the same area. First, look at the
    basic scatter plot diagram based on the value in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised learning with Spark - an example](img/00014.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Cluster of the neighborhoods'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s clearly seen that there are four cluster on the plot marked as circles
    in *Figure 8*. However, finding a number of clusters is a tricky task. Here, we
    have the advantage of visual inspection, which is not available for data on hyperplanes
    or multidimensional data. Now we need to find the same result using Spark. For
    simplicity, we will use the K-means clustering API of Spark. The use of raw data
    and finding feature vectors is shown in *Figure 9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised learning with Spark - an example](img/00015.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Unsupervised learning using Spark'
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering of the neighborhood
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before performing the feature extraction, we need to load and parse the Saratoga
    NY Homes dataset. This step also includes: loading packages and related dependencies,
    reading the dataset as RDD, model training and prediction, collecting the local
    parsed data, and comparing clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Import statistics and related classes**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to import statistics and related classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Create the Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to create a Spark session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Load the Saratoga NY Homes.txt**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read, parse, and create RDDs from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4:Transform the data into an RDD of dense vectors**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you follow the preceding step carefully, actually we have created the normal
    RDD. Therefore, that RDD has to be converted into the corresponding `JavaRDD`
    before mapping into a dense vector using Vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5: Train the model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the model by specifying four clusters and five iterations. Just refer
    the following code for doing that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'You should receive the results as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6: Show the cluster centers**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code should produce the center of the clusters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7: Evaluate the model error rate**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce the result as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 8: Predict the cluster for the second element**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Output prediction: 0'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9: Stop the Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stop the Spark session using the `stop()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 10: Cluster comparing**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s compare the cluster assignments by k-means versus the ones we have
    done individually. The k-means algorithm gives the cluster IDs starting from 0\.
    Once you inspect the data, you find out the following mapping between the A to
    D cluster IDs we gave versus K-means in Table 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Cluster name** | **Cluster number** | **Cluster assignment** |'
  prefs: []
  type: TYPE_TB
- en: '| A | 3 | A=>3 |'
  prefs: []
  type: TYPE_TB
- en: '| B | 1 | B=>1 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 0 | C=>0 |'
  prefs: []
  type: TYPE_TB
- en: '| D | 2 | D=>2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Cluster assignment for the neighbourhood K-means clustering example'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s pick some of the data from different parts of the chart and predict
    which cluster it belongs to. Let''s look at the house (say 1 as an example) data,
    which has a lot size of 876 square feet and is priced at $665K:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[Output] Prediction: 2'
  prefs: []
  type: TYPE_NORMAL
- en: That means the house with the preceding properties falls in the cluster 2\.
    You can test the prediction capability with more data of course. Let's do some
    neighborhood analysis to see what meaning these clusters carry. We can assume
    that most of the houses in cluster 3 are near downtown. The cluster 2 houses are
    on hilly terrain for example.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we dealt with a very small set of features; common sense and
    visual inspection would also lead us to the same conclusions. However, if you
    want to acquire more accuracy, of course, you should construct more meaningful
    features by not only considering only the lot size and the house price but other
    features like the number of rooms, house age, land value, heating type, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: However, it would not be wise to include the *Waterfront* as a meaningful feature
    since no house has a water garden in front of the house in this example. We will
    provide a detailed analysis towards the better accuracy of meaningful a prediction
    in next chapter, where we will demonstrate these considerations.
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of the k-means algorithm is that it does the clustering on the data
    with an unlimited number of features. It is a great tool to use when you have
    a raw data and would like to know the patterns in that data. However, deciding
    pn the number of clusters prior to doing the experiment might not be successful
    but sometimes may lead to an overfitting problem or an underfitting problem.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To overcome the aformentioned limitation of the K-means, we have some more robust
    algorithms like **Markov Chain Monte Carlo** (**MCMC** , see also [https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo))
    presented in Tribble, Seth D., *Markov chain Monte Carlo algorithms using completely
    uniformly distributed driving sequences*, Diss. Stanford University, 2007\. Moreover,
    a more technical discussion can be found at the URL [http://www.autonlab.org/tutorials/kmeans11.pdf](http://www.autonlab.org/tutorials/kmeans11.pdf)
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Recommender system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A recommender system is an original killer application which is a subclass
    of an information filtering system that looks to predict the rating or preference
    from the users that they usually provide to an item. The concept of recommender
    systems has become very common in recent years and has been subsequently applied
    in different applications. The most popular ones are probably products (for example,
    movies, music, books, research articles), news, search queries, social tags, and
    so on). Recommender systems can be typed into four categories as stated in [Chapter
    2](part0023_split_000.html#LTSU2-5afe140a04e845e0842b44be7971e11a "Chapter 2. Machine
    Learning Best Practices"), *Machine Learning Best Practices.* These are shown
    in *Figure 10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The collaborative filtering system**: This is the accumulation of a consumer''s
    preferences and recommendations to other users based on likeness in behavioral
    patterns **Content-based systems**: Here the supervised machine learning is used
    to persuade a classifier to distinguish between interesting and uninteresting
    items for the users'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid recommender systems**: This is a recent research and hybrid approach
    (that is, combining collaborative filtering and content-based filtering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge-based systems**: Here knowledge about users and products are used
    to understand what fulfils a user''s requirements, using a perception tree, decision
    support systems, and case-based reasoning:![Recommender system](img/00028.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 10: Hierarchy of the recommendation systems'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the technical viewpoint, we can further categorize them as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The item **hierarchy** is the weakest one where it is naively assuming that
    one item is correlated to another, for example, if you buy a printer, it is more
    likely that you will buy the ink. Previously this approach was used by **BestBuy**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribute-based recommendation**: Assumes that you like action movies starring
    Sylvester Stallone, therefore, you might be like the Rambo series. **Netflix**
    used to use this approach'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaborative filtering** (U**ser-user similarity):** This assumes and exemplifies
    those people like you who brought baby milk also bought diapers. Target use this
    approach'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaborative filtering** (**Item-item similarity)**: This assumes and exemplifies
    that people who like Godfather series also like Scarface. Netflix currently uses
    this approach'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Social, interest and graph-based approach**: This assumes for example that
    your friend who likes Michel Jackson will also like *Just Beat It*. The tech giant
    like **LinkedIn** and **Facebook** use this approach'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model-based approach**: This uses an advanced algorithm such as **SVM**,
    **LDA**, and **SVD** based on the implicit features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As shown in *Figure 11*, the model-based recommender system that widely used
    advanced algorithms such as SVM, LDA, or SVD is the most robust approach in the
    recommender system class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recommender system](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The recommender system from the technical point of view'
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative filtering in Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As already mentioned, the collaborative filtering techniques are commonly used
    for recommender systems. However, Spark MLlib currently supports model-based collaborative
    filtering only. Here, users and products are described by a small set of latent
    factors. The latent factors are later used for making the prediction of the missing
    entries. According to the Spark API reference for the collaborative filtering
    on [http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html):
    the **Alternating Least Squares** (**ALS**) (also known as non-linear least square,that
    is, NLS; see more at [https://en.wikipedia.org/wiki/Non-linear_least_squares](https://en.wikipedia.org/wiki/Non-linear_least_squares))
    algorithm is used to learn these latent factors by considering the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`numBlocks` is the number of blocks used for the parallelized computation using
    the native LAPACK'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank` is the number of latent factors during the machine learning model building'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iterations` are the number of iterations needed to gain more accurate predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lambda` signifies the regularization parameter for the ALS algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`implicitPrefs` specifies which feedback to be used (explicit feedback ALS
    variant or one adapted for implicit feedback data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha` specifies the baseline confidence in preference observations for the
    ALS algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At first, the ALS, which is an iterative algorithm, is used to model the rating
    matrix as the multiplication of low-ranked users and product factors. After that,
    the learning task is done by using these factors by minimizing the reconstruction
    error of the observed ratings.
  prefs: []
  type: TYPE_NORMAL
- en: However, the unknown ratings can successively be calculated by multiplying these
    factors together. The approach for the move recommendation or any other recommendation
    based on the collaborative filtering technique used in the Spark MLlib has been
    proven a high performer with high prediction accuracy and is scalable for the
    billions of ratings on commodity clusters used by companies such as Netflix. In
    following this way, a company such as Netflix can recommend movies to its subscribers
    based on the predicted ratings. The ultimate target is to increase the sales and
    of course the customer satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: For brevity and page limitation, we will not show the movie recommendations
    using the collaborative filtering approach in this chapter. However, a step-by-step
    example using Spark will be shown in [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data"), *Advanced*
    *Machine Learning with Streaming and Graph Data*.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the time being, interested readers are advised to visit the Spark website
    for the latest API and codes for the same at this URL: [http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html),
    where an example has been presented to show the sample movie recommendations using
    the ALS algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced learning and generalizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss some advanced aspects of learning, for example
    how we can generalize the supervised learning techniques for semi-supervised learning,
    active learning, structured prediction, and reinforcement learning. Moreover,
    reinforcement and semi-supervised learning will be discussed in brief.
  prefs: []
  type: TYPE_NORMAL
- en: Generalizations of supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several ways in which the standard supervised learning problem can
    be generalized:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Semi-supervised learning**: In this generalization technique, only the required
    output values for selected features are provided for a subset of the training
    data to build and evaluate the machine learning model. On the other hand, the
    remaining data is kept unchanged or unlabeled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Active learning**: In contrast, in active learning, algorithms typically
    interactively collect new features by making queries to a human user instead of
    assuming all the training features are given. Consequently, the queries used here
    are based on unlabeled data. Interestingly, it is also an example that combines
    semi-supervised learning with active learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structured prediction**: Sometimes the desired features need to be extracted
    or selected from complex objects like a parse tree or a labelled graph, and then
    the standard supervised or unsupervised methods must be improved towards adaptability
    for making it generalized. To be more precise, for example when a supervised machine
    learning technique tries to predict structured or unstructured texts such as translating
    NLP sentences into syntactic representation, structured prediction evolves that
    need to handle a large-scale parse tree. To make this task easier, often the structured
    SVMs or Markov logic networks or constrained conditional models are used that
    technically extend and update the classical supervised learning algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning to rank**: Machine-learned ranking is required when the input itself
    is a subset of objects and the desired output is a ranking of those objects, then
    the standard methods must be extended or improved similarly to the structure prediction
    technique.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Interested readers can refer to these two URLs: [https://en.wikipedia.org/wiki/Learning_to_rank](https://en.wikipedia.org/wiki/Learning_to_rank)
    and [https://en.wikipedia.org/wiki/Structured_prediction](https://en.wikipedia.org/wiki/Structured_prediction), where
    a more details discussion can be found.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have discussed some supervised, unsupervised, and recommender systems from
    a theoretical and Spark's perspective. However, there are numerous examples for
    the supervised, unsupervised, reinforcement or recommendation systems too. Nevertheless,
    we have tried to present some simple examples for the sake of simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: We will provide more insights on these examples in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*. More feature incorporation, extraction, selection
    using Spark ML and Spark MLlib pipelines, model scaling, and tuning will be discussed
    too. We also intend to provide some examples including data collection to model
    building and prediction.
  prefs: []
  type: TYPE_NORMAL
