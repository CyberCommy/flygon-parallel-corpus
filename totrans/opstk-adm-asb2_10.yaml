- en: Chapter 10. Health Check Your Cloud with Nagios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The topic of monitoring happens to be something I hold very close to my heart.
    I spent years *watching* many organizations' websites and applications to ensure
    that their availability holds as close as possible to 99.99% uptime. This task
    was not for the meek of heart in any realm of things. The thing that got me through
    it all was having a solid method to monitoring the environments that did not require
    me to literally watch it every second of the day. In this chapter, we will step
    through some of the common approaches to checking the health of your OpenStack
    cloud manually and then leverage Ansible to set up my favorite open source monitoring
    tool, Nagios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have been experimenting with the **openstack-ansible** (**OSA**) deployment
    method throughout the book, let''s continue to leverage the built-in Ansible capabilities
    part of OSA to perform various system checks. Keep in mind that what we do here
    should not replace any third-party monitoring tool that most likely will do a
    better job keeping the tasks to be monitored in a reliable rotation. We will use
    the native capabilities part of OpenStack and Linux to provide a quick view of
    your clouds health. As well as along the way, we will review other monitoring
    tips and tricks:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring the cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core OpenStack services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Nagios and importing checks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting your metrics via SNMP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding the playbook and roles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing playbook and role
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If allowed I would like to cover some monitoring basics before getting started
    here. Hopefully, the three principles I will share here are not new to you. When
    evaluating to monitor something, there are three base principles to keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Keep it simple
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep your monitoring close to your infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create good monitors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first point is pretty easy to absorb, as it is rather self-explanatory.
    The worst thing you could ever do is over complicate something as important as
    monitoring. This principle not only applies to your overall approach, but it also
    applies to the tool you choose to do the monitoring. If you have to create a **Visio**
    diagram of your monitoring platform, it is too complicated.
  prefs: []
  type: TYPE_NORMAL
- en: The next point of keeping your monitoring close to your infrastructure is meant
    to express that the tool used to monitor should physically reside close to the
    infrastructure/application. The monitoring platform should not have to traverse
    VPNs or multiple firewalls just to poll the devices/applications. Centrally place
    the monitoring platform, so you can poll as many systems as possible with minimal
    path to travel. You should be able to open up one or two ports in a firewall to
    enable monitoring, and this should be turned into a standardized process part
    of standing up new devices/applications.
  prefs: []
  type: TYPE_NORMAL
- en: The last point is another rather self-explanatory concept. Creating good monitors
    is critical, and it will avoid false positive monitor alerts. Over time individuals
    will begin to ignore monitoring alerts if they mostly all turn out to be false
    alarms. Make sure that each monitoring check works as expected and is tuned to
    avoid false alarms as much as possible. Never launch a new alert monitor without
    testing it during various workloads and time of the day. Also, it should go without
    saying to make sure that the monitor adds value and is not redundant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have gotten the basics out of the way, we can now focus on monitoring
    OpenStack. This would normally cover the following four areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring the physical hardware (base resource consumption)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring the OpenStack API endpoints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring the OpenStack services processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring the Compute nodes via the infrastructure nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the first two areas are honestly better suited for a monitoring tool to
    handle, we will not be focusing on these tools in this book. So our focus will
    be primarily on checking the health of the infrastructure services (that is, Galera,
    RabbitMQ, and so on), the core OpenStack services processes, and the Compute nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**ProTip**'
  prefs: []
  type: TYPE_NORMAL
- en: When monitoring the OpenStack API endpoints, make sure to include the endpoint
    response times as a metric being recorded. This allows you to identify and track
    any service-related slowdowns that eventually could cause a service failure. Capturing
    this information allows you to see performance trends over time, which could proactively
    allow you to address service related issues before failures occur. The fix could
    be something as simple as adding more containers running that particular service,
    tuning service parameters, and/or database tuning.
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack service processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before moving to the next section, I felt it would be helpful to include some
    details on the OpenStack service processes. Here is a table outlining the OpenStack
    services and the associated process names. As anything related to OpenStack, the
    process names are subject to change on a per release basis. I hope that this will at
    least be a good starting point.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Service Name** | **Code-Name** | **Process Name** |'
  prefs: []
  type: TYPE_TB
- en: '| Compute | Nova | nova-api-metadata, nova-api-os-compute, nova-cert, nova-compute,
    nova-consoleauth, nova-spicehtml5proxy, nova-api-ec2, nova-api, nova-conductor,
    nova-scheduler |'
  prefs: []
  type: TYPE_TB
- en: '| Object Storage | Swift | swift-proxy-server, swift-account-server, swift-account-auditor,
    swift-account-replicator, swift-account-reaper, swift-container-server, swift-container-auditor,
    swift-container-replicator, swift-container-updater, swift-object-server, swift-object-auditor,
    swift-object-replicator, swift-object-updater |'
  prefs: []
  type: TYPE_TB
- en: '| Image | Glance | glance-api, glance-registry |'
  prefs: []
  type: TYPE_TB
- en: '| Identity | Keystone | keystone-all, apache2 |'
  prefs: []
  type: TYPE_TB
- en: '| Dashboard | Horizon | apache2 |'
  prefs: []
  type: TYPE_TB
- en: '| Networking | Neutron | neutron-dhcp-agent, neutron-l3-agent, neutron-linuxbridge-agent,
    neutron-metadata-agent, neutron-metering-agent, neutron-server |'
  prefs: []
  type: TYPE_TB
- en: '| Block Storage | Cinder | cinder-api, cinder-volume, cinder-scheduler |'
  prefs: []
  type: TYPE_TB
- en: '| Orchestration | Heat | heat-api, heat-api-cfn, heat-api-cloudwatch, heat-engine
    |'
  prefs: []
  type: TYPE_TB
- en: '| Telemetry | Ceilometer | ceilometer-agent-compute, ceilometer-agent-central,
    ceilometer-agent-notification, ceilometer-collector, ceilometer-alarm-evaluator,
    ceilometer-alarm-notifier, ceilometer-api |'
  prefs: []
  type: TYPE_TB
- en: '| Database | Trove | trove-api, trove-taskmanager, trove-conductor |'
  prefs: []
  type: TYPE_TB
- en: Infrastructure services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The foundation of all the OpenStack services are called infrastructure services.
    These are the services/components needed just for OpenStack to work on any level.
    Those components are an SQL database server software, database-clustering software,
    and messaging server software. In our specific case, those components in the same
    order will be MariaDB, Galera, and RabbitMQ. Making sure that all of these components
    are healthy and working as expected in a top priority. Each of these software
    packages has native commands to report on their health, so we are covered there.
    So the challenge would then be what the best way to query for this information
    against clouds of various sizes is. Imagine that you have a 20-node control plane.
    You could execute the health check command twenty times or just execute one command
    using Ansible to get the status back.
  prefs: []
  type: TYPE_NORMAL
- en: MariaDB and Galera
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Starting with the first two components, there is a way to execute one command
    to do both, a MySQL check as well as check the health of the database cluster.
    If you remember back in [Chapter 2](ch02.html "Chapter 2. Introduction to Ansible"),
    *Introduction to Ansible*, we covered the topic of dynamic inventory and how OSA
    has prebuilt dynamic inventory scripts available that we can use to ease cloud
    management. We will use that capability here to streamline the process of checking
    on these infrastructure services.
  prefs: []
  type: TYPE_NORMAL
- en: It is useful to have a quick reminder walk through of how to use the OSA dynamic
    inventory scripts with Ansible. From the root OSA deployment (`/opt/openstack-ansible/playbooks`) directory,
    you can use the defined group names to call the containers where any of the OpenStack
    services reside. Each OpenStack service and infrastructure component has a group
    defined within the dynamic inventory. As related to the specific task we are working
    on presently, there is a group named `galera_container`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This group contains all the containers where MySQL and Galera are installed
    for the cloud. You would then substitute this group name for any host names you
    would normally provide within the `hosts` file located inside the `root` directory
    of the playbook. Try executing the following command against your OSA cloud to
    reveal the details for your Galera containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![MariaDB and Galera](graphics/image_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keep in mind that the previous example was collected against an **AIO** (**All-In-One**)
    deployment of OpenStack. Normally, you should find three or more different containers
    listed under the `galera_container` group.
  prefs: []
  type: TYPE_NORMAL
- en: 'The area that we haven''t covered as it relates to Ansible is the ability to
    issue more basic ad hoc commands using just the Ansible runtime package. Execute
    the following command within a command prompt where Ansible is installed to see
    the details on how to use the basic Ansible program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You will note that the parameters are very similar to the `ansible-playbook`
    program with the main difference being that the `ansible` program is not intended
    to execute playbooks. Rather it is meant to be able to execute ad hoc tasks directly
    on the command line using all the same modules you would normally use with a playbook.
    We will use the basic Ansible program to demonstrate how to retrieve the status
    of these infrastructure services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if we put this all together, the working example of reporting on the health
    of MySQL and Galera will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command told Ansible to use the `galera_container` group as the
    hosts to run the `shell` command against. The `shell` command will call MySQL
    and execute the `show status` query. The output of the command will look similar
    to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![MariaDB and Galera](graphics/image_10_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Again, due to using an AIO deployment, you will note that the example shows
    a cluster size of only one. Normally, the cluster size should show three or more,
    and the status will be displayed for each container (the output will be repeated
    for each container). Key areas to look out for are: each container reports success,
    the cluster size is correct, and the cluster ID is the same across all clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will use the very same principles as we did earlier for MariaDB/Galera to
    check on the status and health of the RabbitMQ cluster. The group name for the
    RabbitMQ containers is `rabbit_mq_container`, and we can see the details of the
    group by executing the following command within the OSA root deployment directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now go ahead and test out a few commands to report on the RabbitMQ cluster
    health. The first command here will report directly on the cluster status, and
    the second command will list out all the queues that contain messages (in other
    words queues that are not empty):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the commands will look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![RabbitMQ](graphics/image_10_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Having each container report back a *success*, having the list of running nodes
    match exactly and each showing the same cluster name are the areas that matter
    the most. Do not stress too much if you find queues with messages still present.
    The idea is those messages should clear in an acceptable period of time. Use this
    metric to seek out any trends in messages getting stuck in the queue for too long.
  prefs: []
  type: TYPE_NORMAL
- en: Core OpenStack services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all the infrastructure services covered, we can move on to the core OpenStack
    services. In this section, we will cover a few principles that can be used for
    any of the OpenStack services. This approach allows you to interchange any of
    the basic approaches for any service basing it on your personal needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first three services I normally go in and check are Keystone, Nova, and
    Neutron. These services can have adverse effects on many other services within
    your cloud and need to be running properly to technically have a functioning cloud.
    While there is no distinct OpenStack command you can use to check the Keystone
    service, it will become very apparently obvious if the Keystone service is not
    operational as any/all OpenStack CLI commands will fail. I personally find the
    easiest way to test our Keystone is to either log into the Horizon dashboard or
    issue the following OpenStack CLI command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you get back the list of services using Keystone, you have just tested passing
    user credentials to Keystone for authentication and Keystone returning a proper
    token for authorization. With us taking care of testing out Keystone, the next
    step can be to issue two OpenStack CLI commands that will quickly report on the
    state of Nova and Neutron:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `nova service-list` command will poll all Nova-related containers and Compute
    nodes to determine their zone, status, state, and time either the status or state
    was changed. The output of this command will look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Core OpenStack services](graphics/image_10_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, the `neutron agent-list` command will do the same thing as the above
    except for the Neutron-related components. You will note that in the upcoming
    example a smiley face graphic is used to determine the status of the neutron agents.
    The state of the agents will also be reported back with this command as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Core OpenStack services](graphics/image_10_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, you will have to rely on checking directly on the statuses of
    the actual OpenStack service processes running within their containers to do more
    detailed monitoring. This would be similar to some of the methods published on
    the OpenStack website, http://docs.openstack.org/ops-guide/operations.html. The
    main difference is that we will use Ansible to execute the commands across all
    containers or nodes as needed. Using the basic Ansible program and the OpenStack
    service processes table mentioned previously, you will be able to check the status
    of the processes running OpenStack within your cloud. The following are a few
    examples of how this can be accomplished. It is recommended to get a complete
    output of the dynamic inventory for your cloud, so you will be aware of all the
    groups defined. Use the following command to get the complete inventory output
    (this command assumes that you are in the root OSA deployment directory):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Save the JSON output at some place where you can reference it in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Service and process check examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following examples show that how you can execute service and process monitor
    checks using Ansible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You can use any of these examples to determine the health of your OpenStack
    services on a cloud of any size, big or small. Imagine that the power of being
    able to query the `nova-compute` service status across a cloud with 200 nodes
    in one command. Good stuff right? Well, of course, we have to try to take it to
    the next level by incorporating more advance monitoring tools into the mix to
    create a more robust solution.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Nagios and import checks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reflecting back to the first principle mentioned previously concerning monitoring,
    keep it simple. It felt like we could not keep it any simpler than going with
    one of the leading open source monitoring platforms, Nagios Core. If you are unfamiliar
    with Nagios, take a moment to read up on it by visiting [http://www.nagios.com/products](http://www.nagios.com/products).
  prefs: []
  type: TYPE_NORMAL
- en: Yes, yes, while it may not be the flashiest dashboard visually, it is one of
    the most powerful and lightweight monitoring applications I have used. With Nagios,
    you have ultimate control on many aspects of your monitoring ecosystem. It ranges
    from being able to create custom plugins, all the way to explicitly defining execution
    windows for that host. Administration can be handled directly from the flat files,
    or you can use many of the third-party tools, such as **NConf** at [http://exchange.nagios.org/directory/Addons/Configuration/NConf/details](http://exchange.nagios.org/directory/Addons/Configuration/NConf/details).
    With the launch of the new version, XI, more and more of the features only found
    in the third-party tools are built right in. Some of the new features that stand
    out are the advanced graphs, integration into Incident management tools, and cleaner
    SLA reports.
  prefs: []
  type: TYPE_NORMAL
- en: Collect your metrics via SNMP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of course, with great capability sometimes comes great overhead. Typically,
    I have found that keeping your monitoring close to your infrastructure avoids
    limiting what you are monitoring due to firewall restrictions and so on. It is
    strongly recommend to use SNMP (UDP port `161`), rather than the NRPE agent, no
    agent installation is needed. As well as, I normally stick with Perl-written plugins
    to ease troubleshooting. Creating good'monitors is essential to minimize false
    alerts, which in time turn into ignored alerts. If you find a service check continuously
    sending off false alerts, FIX IT! Do not let it linger for days.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the power behind OpenStack exposing all functionality through APIs,
    monitoring is made easy. Custom plugin scripts can be created to monitor the whole
    OpenStack stack and cross-reference any bottlenecks to physical infrastructure
    problems. This type of proactive monitoring can lead to preventing down time leading
    to outages.
  prefs: []
  type: TYPE_NORMAL
- en: Since I have such a deep seeded love for OSA, it seemed only fitting to put
    together a series of Ansible playbooks to handle most of the Nagios and NConf
    setup. Also, because I love to pay it forward, included are OSA customized Nagios
    configs (`checkcommands`, services and a bunch of global Nagios configs) which
    can be used to monitor your OpenStack cloud within minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Coding the playbooks and roles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will now use all of that Ansible magic to create a series
    of playbooks and roles to set up Nagios to monitor your OpenStack cloud. Once
    completed, you will have a fully functioning Nagios installation that will be
    customized to monitor OpenStack with some of the monitoring checks we reviewed
    in the previous section. This time around we broken up the tasks into eight roles
    in order to keep things simple. Let's review each role in the later .
  prefs: []
  type: TYPE_NORMAL
- en: snmp-config
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first role we will create will include those tasks needed to set up the
    foundation for collecting the monitoring data. The name of the file will be `main.yml`
    located within the `role` directory named `snmp-config/tasks`. The contents of
    this file will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The first four tasks are simply handling the steps needed to install and configure
    SNMP on each host you will be monitoring. This will include installing the `snmp`
    package, copying the custom config file into place, and editing the SNMP options.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task handling the custom `snmp.conf` will use a `template` file stored
    in the `snmp-config/templates` directory of this role. The reason for this is
    so we can leverage the variables defined in your playbook already instead of hard
    coding parameters. The contents of the file will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: install-nagios
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next role will focus on installing Nagios and its prerequisites. The file
    will be named `main.yml` located within the `role` directory named `install-nagios/tasks`.
    The contents of this file will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This role is pretty straightforward in stepping through the tasks needed to
    perform a clean installation of Nagios. Since we will be monitoring Ubuntu systems,
    the last two tasks in this role were included for install the Ubuntu logo into
    Nagios.
  prefs: []
  type: TYPE_NORMAL
- en: nagios-plugins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This role will be responsible for installing our custom Nagios plugins that
    we will use to monitor our cloud. The file will be named `main.yml` within the
    `role` directory named `nagios-plugins/tasks`. Here, you will find these contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The preceding role copies and sets up two very important files (`nagios-plugins.tar`
    and `NagiosConfig.zip`) on the Nagios server. Without these plugins and configurations,
    you will just have a plain vanilla Nagios installation. By running this role,
    you basically are getting a preconfigured Nagios setup ready to monitor an OpenStack
    cloud deployed with OSA. With this model, you can also customize the plugins or
    specific configurations being attached to the Nagios server. If you are feeling
    curious, feel free to crack open these archives and take a look.
  prefs: []
  type: TYPE_NORMAL
- en: install-nconf
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This role could technically be considered optional, as you do not need NConf
    to run a Nagios server. I personally have found NConf to be a great compliment
    to Nagios as far as configuring your service checks and hosts. The file will be
    named `main.yml` within the `role` directory named `install-nconf/tasks`. Here
    are the contents of this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very similar to the role that handles installing Nagios, it covers the required
    steps to install and configure NConf. More details on how to install NConf can
    be found at [http://www.nconf.org/dokuwiki/doku.php?id=nconf:help:documentation:start:installation](http://www.nconf.org/dokuwiki/doku.php?id=nconf:help:documentation:start:installation).
  prefs: []
  type: TYPE_NORMAL
- en: nconf-post-install
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider this role a follow-up to the previous one as it handles the post-install
    steps for the NConf installation. It will handle the cleanup of specific files
    once the install has completed. The file will be named `main.yml` within the `role`
    directory named `nconf-post-install/tasks`. Here are the contents of this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The next two roles are intended to prepare the controller nodes to monitor the
    OpenStack processes and API's running on the local containers. You must be able
    to run the service checks remotely over SSH. The good news is that the Nagios
    plugin to do this already exists (`check_by_ssh`).
  prefs: []
  type: TYPE_NORMAL
- en: create-nagios-user
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The name of this role basically explains exactly what tasks it will handle.
    It will create a user named Nagios, and this user will serve as a service account
    for the Nagios plugin. The file will be named `main.yml` within the `role` directory
    named `create-nagios-user/tasks`. Here are the contents of this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: infra-plugin-config
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This role will install additional SNMP package prerequisites and install local
    Nagios plugins directly on the controller node. Via SSH, Nagios will execute these
    local plugins and report the status back to Nagios to record. This is where you
    have to say that you just love technology. The file will be named `main.yml` within
    the `role` directory named `infra-plugin-config/tasks`. Here are the contents
    of this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: nagios-post-install
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Last and certainly not least is the final role that makes up this series. The
    final role will complete the Nagios configuration and set up NConf to work with
    your Nagios installation. The file will be named `main.yml` within the `role`
    directory named `nagios-post-install/tasks`. Here are the contents of this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: To support these roles, we now need to create the variable files that will go
    along with it. Since we will use three separate hosts to execute the series of
    roles against, there will be three global variable files needed. The file names
    are `hosts`, `all_containers`, and `nagios-server`; they will be saved to the
    `group_vars/` directory of the playbook.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keep in mind that the values defined in the variable file are intended to be
    changed before each execution for normal everyday use.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a bunch of new variables added in this chapter. Let''s take a moment
    to review the contents of each variable file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a moment to break down the new variables. The summary is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since there are two global variable files that share the same variable names,
    please make sure to keep the variable value in sync if you want both reports in
    the same directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the variable file completed, we can move on to creating the master playbook
    file. Since there will be manual configurations that need to be taken care of
    inbetween the playbooks to be run, the master playbook was broken up into multiple
    playbooks. The contents of the first playbook named `base.yml` will look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The next playbook is named `base-nagios.yml`, and the contents will look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following playbook is named `base-nconf.yml`, and the contents will look
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The next playbook is named `post-nconf-install.yml`, and the contents will
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The next playbook is named `base-infra.yml`, and the contents will look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The next playbook is named `post-nagios-install.yml`, and the contents will
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The playbook and role names can be anything you choose. Specific names have
    been provided here in order to allow you to easily follow along and reference
    the completed code found in the GitHub repository. The only warning is that whatever
    you decide to name the roles must remain uniform when referenced from within the
    playbook(s).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our inventory file for these playbooks and roles turns out to be a very simple
    one. Inside the inventory file, we will only have to define the address of the
    Nagios server. An example of this is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: I hope that you are happy with how it all came out. In keeping with our tradition,
    we will finish up the chapter with a quick review of the playbooks and roles just
    created with a little added extra instructions included.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing playbook and role
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's jump right into examining the roles we created.
  prefs: []
  type: TYPE_NORMAL
- en: 'The completed role and file named `main.yml` located in the `snmp-config/tasks`
    directory looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The completed role and file named `main.yml` located in the `install-nagios/tasks`
    directory looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The completed role and file named `main.yml` located in the `nagios-plugins/tasks`
    directory looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The completed role and file named `main.yml` located in the `install-nconf/tasks`
    directory looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The completed role and file named `main.yml` located in the `nconf-post-install/tasks`
    directory looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The completed role and file named `main.yml` located in the `create-nagios-user/tasks`
    directory looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The completed role and file named `main.yml` located in the `infra-plugin-config/tasks`
    directory looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The completed role and file named `main.yml` located in the `nagios-post-install/tasks`
    directory looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding global variable file is named `all_containers` and is saved
    to the `group_vars/` directory of the complete playbook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding global variable file is named `hosts` and is saved to the
    `group_vars/` directory of the complete playbook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding global variable file is named `nagios-server` and is saved
    to the `group_vars/` directory of the complete playbook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the master playbook file has been created and will be located in the `root`
    directory of the `playbook` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`base.yml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '`base-nagios.yml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '`base-nconf.yml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '`post-nconf-install.yml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '`base-infra.yml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '`post-nagios-install.yml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, in the end, we have created the `hosts` file, which also is located
    in the `root` directory of the `playbook` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The complete set of code can again be found in the following GitHub repository
    at `https://github.com/os-admin-with-ansible/os-admin-with-ansible-v2/tree/master/nagios-openstack`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we finish up this topic, we of course need to test out our work and
    add in some additional instructions to complete the Nagios setup. At the end of
    running these playbooks and roles, you will have a powerful monitoring machine
    for your OpenStack clouds and other applications. Assuming that you have cloned
    the GitHub repository earlier, the command to test out the playbook from the Deployment
    node will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Move the playbooks and roles into the OSA deployment directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In order to leverage the dynamic inventory capabilities that  come with OSA,
    the playbooks and roles need to be local to  the  deployment directory. Trust
    me you will like this!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following playbook to install and configure SNMP on your OSA cloud:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If the SNMP service does not start the first time, please execute the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following playbook to install and configure Nagios onto your monitoring
    server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Then connect to the monitoring server via SSH to execute the following commands
    to set  the *nagiosadmin* user password (used to log in to Nagios web dashboard)
    and restart Nagios:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following playbook to install and configure NConf onto your monitoring
    server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '**NConf initial configuration**: My attempt to automate this part was not successful,
    so you have to finish the NConf configuration using the NConf web console. Browse
    `http://<monitoring server IP>/nconf` and follow the prompts to complete the initial
    configuration. Here are the suggested inputs and keep the defaults for the others:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '**Execute the post NConf playbook**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following playbook to configure the OSA nodes to allow for monitoring
    via SSH:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In order to monitor the OpenStack processes and APIs running on the local containers,
    you must run the service checks remotely over SSH. The good news is that the Nagios
    plugin to do this already exists (`check_by_ssh`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Confirm the Nagios and NConf installation: in a browser, go to the following
    URLs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http://<monitoring server IP>/nagios3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http://<monitoring server IP>/nconf`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time to configure Nagios for monitoring OSA.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unfortunately, this part does require manual configuration as each installation
    will differ too much to automate. In the big picture, this will just help you
    sharpen your Nagios skills. Do not worry; a copy of the Nagios directory was already
    taken. This step will take some time and should not be rushed.
  prefs: []
  type: TYPE_NORMAL
- en: The first step here is to customize the Nagios configuration files located in
    the `/etc/nagios3/rpc-nagios-configs` directory on your monitoring server. All
    the configuration files are important, but the most critical ones are the `advanced_services.cfg`
    and `hosts.cfg` files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the `advanced_services.cfg` file, you will need to update each service
    check with the IP addresses of the containers within your OSA install. The fastest
    way to get that information is to execute the following command and capture the
    output on each infrastructure node: `lxc-ls --fancy`. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The same goes for the `hosts.cfg` file; please update the OSA node names and
    IP addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Please also add the following to the bottom of the `resources.cfg` file located
    in the root of the Nagios directory (`/etc/nagios3`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: If you are having trouble making the updates to the configurations using an
    editor, do not stress out as the next step will make this process a bit easier.
  prefs: []
  type: TYPE_NORMAL
- en: Import Nagios configuration into NConf
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next appended the contents of the configuration files in the `/etc/nagios3/rpc-nagios-configs`
    directory to the current Nagios configuration files (add to bottom). Every host,
    host group, check, service, and contact group is uniquely named so as not to conflict
    with the current Nagios setup. Then, we will step through the instructions found
    on the NConf website, [http://www.nconf.org/dokuwiki/doku.php?id=nconf:help:how_tos:import:import_nagios](http://www.nconf.org/dokuwiki/doku.php?id=nconf:help:how_tos:import:import_nagios).
  prefs: []
  type: TYPE_NORMAL
- en: 'As the NConf tutorial suggests, first run the commands with the `-s` parameters
    to simulate the import process first. After being able to run with no errors,
    remove the `-s` parameter to do the final import. Having connected to the monitoring
    server via SSH, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Now you can edit all the Nagios configurationss within the NConf web console.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the post Nagios playbook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Generate your first Nagios config
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you are satisfied with all of your custom Nagios configurations (trust
    me that you will do this a couple of times), click on the **Generate Nagios config**
    link on the sidebar of the NConf web console. It will note if any errors were
    encountered. From time to time, you will see warnings, and they are just warnings,
    nothing urgent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Last but not the least, from the monitoring server, execute the following command
    to deploy the Nagios configurations to Nagios (may need to use `sudo`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having a rock solid monitor platform on your side is the key to cloud success
    (actually any production  systems success). Remember that this is only a starting
    point. I expect you to improve/customize it for your specific needs. I am looking
    forward to seeing all your great work in the future. Please make sure to share
    any changes you make, remember Open Source is all about sharing. Before wrapping
    up this final chapter, let's take a moment to recap what we discussed. First we
    covered some monitoring tips and tricks. Then examined the OpenStack components
    worth monitoring. Next, we learned how to use Ansible ad hoc commands. We then
    transitioned into how to set up Nagios and import the custom plugins for the service
    checks. Finally, we developed Ansible playbooks and roles to automate the base
    installation of Nagios and NConf with customizing it to completely monitor an
    OpenStack cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Well ladies and gentleman, this has been fun and honestly a privilege to be
    allowed to share these automation examples with you for the second time around.
    Please keep up the great work and also keep an eye out for future revisions as
    both OpenStack and Ansible continues to mature. I am really looking forward to
    hearing your feedback and seeing how you took these examples to the next level.
  prefs: []
  type: TYPE_NORMAL
