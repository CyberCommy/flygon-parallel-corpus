- en: Web Crawling with Scrapy – Mapping the Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 2](part0029.html#RL0A0-5a228e2885234f4ba832bb786a6d0c80), *Interacting
    with Web Applications*, we learned how to interact with a web application programmatically
    using Python and the requests library. In this chapter, we will cover the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Web application mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating our own crawler/spider with Scrapy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making our crawler recursive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scraping interesting stuff
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web application mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember in [Chapter 1](part0019.html#I3QM0-5a228e2885234f4ba832bb786a6d0c80),
    *Introduction to Web Application Penetration Testing*, that we learned about the
    penetration testing process. In that process, the second phase was mapping.
  prefs: []
  type: TYPE_NORMAL
- en: In the mapping phase, we need to build a map or catalog of the application resources
    and functionalities. As a security tester, we aim to identify all the components
    and entry points in the app. The main components that we are interested in are
    the resources that take parameters as input, the forms, and the directories.
  prefs: []
  type: TYPE_NORMAL
- en: The mapping is mainly performed with a crawler. Crawlers are also known as spiders,
    and usually, they perform scraping tasks, which means that they will also extract
    interesting data from the application such as emails, forms, comments, hidden
    fields, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to perform application mapping, we have the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: The first technique is crawling. The idea is to request the first page, pass
    all the content, extract all the links in scope, and repeat this with the links
    that have been discovered until the entire application is covered. Then, we can
    use an HTTP proxy to identify all the resources and links that may be missed by
    a crawler. Basically, most of the URLs that are generated dynamically in the browser
    with JavaScript will be missed by the crawler, as the crawler does not interpret
    JS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another technique is to discover resources that are not linked anywhere in the
    application by using dictionary attacks. We'll build our own BruteForcer in the
    next section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, we have an example of how the Burp proxy creates application mapping
    using the proxy and the spider functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see the directories, the static pages, and the pages that accept parameters,
    with the different parameters and the different values.
  prefs: []
  type: TYPE_NORMAL
- en: All the interesting parts will be used for handling vulnerabilities using different
    techniques such as SQL injection, cross-site scripting, XML injection, and LDAP
    injection. Basically, the aim of mapping is to cover all the applications in order
    to identify the interesting resources for the vulnerability identification phase.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll start developing our own crawler. Let's get ready!
  prefs: []
  type: TYPE_NORMAL
- en: Creating our own crawler/spider with Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll create our first Scrapy project. We'll define our objective,
    create our spider, and finally, run it and see the results.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to define what we want to accomplish. In this case, we want
    to create a crawler that will extract all the book titles from [https://www.packtpub.com/](https://www.packtpub.com/).
    In order to do so, we need to analyze our target. If we go to the [https://www.packtpub.com/](https://www.packtpub.com/)
    website and right-click on a book title and select Inspect, we will see the source
    code of that element. We can see, in this case, that the book title has this format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Creating a crawler for extracting all the book titles
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see `div` with a `class` of `book-block-title`, and then the title
    name. Keep this in mind or in a notebook, as that would be even better. We need
    this to define what we want to extract in our crawl process. Now, let''s get coding:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go back to our virtual machine and open a Terminal. In order to create
    a crawler, we''ll change to the `/Examples/Section-3` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to create our project with the following Scrapy command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In our case, the name of the crawler is `basic_crawler`.
  prefs: []
  type: TYPE_NORMAL
- en: When we create a project, Scrapy automatically generates a folder with the basic
    structure of the crawler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inside the `basic_crawler` directory, you will see another folder called `basic_crawler`.
    We are interested in working with the `items.py` file and the content of the `spiders`
    folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: These are the two files we'll work with.
  prefs: []
  type: TYPE_NORMAL
- en: So, we open the Atom editor, and add our project with Add Project Folder... under Examples
    | Section-3 | basic crawler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we need to open `items.py` in the Atom editor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: When working with Scrapy, we need to specify what the things we're interested
    in getting are while crawling a website. These things are called items in Scrapy,
    and think about them as our data module.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's edit the `items.py` file and define our first item. We can see in
    the preceding screenshot that the `BasicCrawlerItem` class was created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ll create a variable called `title`, and that will be an object of the
    class `Field`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We can delete the remaining part of the code after `title = scrappy.Field()`
    as it is not used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is all for now with this file.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move onto our spider. For the spider, we'll work on the `spiderman.py` file,
    which is created for this exercise in order to save time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We first need to copy it from `Examples/Section-3/examples/spiders/spiderman-base.py`
    to `/Examples/Section-3/basic_crawler/basic_crawler/spiders/spiderman.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, open the file in the editor and we can see at the top of the file the
    imports needed for this to work. We have `BaseSpider`, which is the basic crawling
    class. Then, we have `Selector`, which will help us to extract data using cross
    path. `BasicCrawlerItem` is the model we created in the `items.py` file. Finally,
    find a `Request` that will perform the request to the website:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we have the `class MySpider`, which has the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: This is the name of our spider, which is needed to invoke it later.
    In our case, it is `basic_crawler`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`allowed_domains`: This is a list of domains that are allowed to be crawled.
    Basically, this is done to keep the crawler in bounds of the project; in this
    case, we''re using `packtpub.com`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_urls`: This is a list that contains the starting URLs where the crawler
    will start with the process. In this case, it is `https://www.packtpub.com`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parse`: As the name suggests, here is where the parsing of the results happens.
    We instantiate the `Selector`, parsing it with the `response` of the request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we define the `book_titles` variable that will contain the results of
    executing the following cross path query. The cross path query is based on the
    analysis we performed at the beginning of the chapter. This will result in an
    array containing all of the book titles extracted with the defined cross path
    from the response content. Now, we need to loop that array and create books of
    the `BasicCrawlerItem` type, and assign the extracted book title to the title
    of the book.
  prefs: []
  type: TYPE_NORMAL
- en: That's all for our basic crawler. Let's go to the Terminal and change the directory
    to `basic_crawler` and then run the crawler with `scrapy crawl basic_crawler`.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the results are printed in the console and we can see the book titles being
    scraped correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s save the output of the folder in a file by adding `-o books.json
    -t`, followed by the type of the file that is `json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now, run it. We'll open the `books.json` file with `vi books.json`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the book titles being extracted as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There are some extra tabs and spaces in the titles, but we got the name of the
    books. This will be the minimal structure needed to create a crawler, but you
    might be wondering that we are just scraping the index page. How do we make it
    recursively crawl a whole website? That is a great question, and we'll answer
    this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Making our crawler recursive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll start learning how to extract links, and then we''ll
    use them to make the crawler recursive. Now that we have created the basic structure
    of a crawler, let''s add some functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: First, let's copy the prepared `spiderman.py` file for this exercise. Copy it
    from `examples/spiders/spiderman-recursive.py` to `basic_crawler/basic_crawler/spiders/spiderman.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, go back to our editor. As we would like to make the crawler recursive,
    for this purpose, we will once again work the `spiderman.py` file and start with
    adding another extractor. However, this time we''ll add the links instead of titles,
    as highlighted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, we need to make sure that the links are valid and complete, so we''ll
    create a regular expression that will validate links highlighted in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00031.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This regular expression should validate all HTTP and HTTPS absolute links. Now
    that we have the code to extract the links, we need an array to control the visited
    links, as we don't want to repeat links and waste resources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we need to create a loop to iterate over the links found, and if the
    link is an absolute URL and has not been visited before, we `yield` a request
    with that URL to continue the process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If the link failed the validation, it means it is a relative URL. So, we will
    join that relative URL with the base URL, where this link was obtained from by
    creating a valid absolute URL. Then, we'll use the `yield` request.
  prefs: []
  type: TYPE_NORMAL
- en: Save it and then go to the console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we change the directory to `basic_crawler`, run it with `scrapy crawl
    basic_crawler -t json -o test.json`, and then press *Enter*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can see that it is working now. We are recursively crawling and scraping
    all of the pages in the website:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This could take a long time, so we cancel by pressing *Ctrl* + *C* and we'll
    get the file with the results up to this point.
  prefs: []
  type: TYPE_NORMAL
- en: Let's open the `test.json` file with the `vi test.json` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the following screenshot, we have a lot of book titles for
    multiple pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Congratulations! We have built a web application crawler.
  prefs: []
  type: TYPE_NORMAL
- en: Think about all the tasks you can automate now.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping interesting stuff
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll take a look at how to extract other interesting information
    such as emails, forms, and comments that will be useful for our security analysis.
  prefs: []
  type: TYPE_NORMAL
- en: We've added recursive capabilities to our crawler, so now we are ready to add
    more features. In this case, we'll be adding some extraction capabilities for
    emails because it is always useful to have a valid account, which could be handy
    during our tests. Forms will be useful where there's information being submitted
    from the browser to the application. Comments could provide interesting information,
    which developers may have left in production without realizing.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is more stuff that you can obtain from web applications but these are
    usually the most useful:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s add these fields into our item. Open the `items.py` file in Atom
    and add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will be used to indicate where the information was found.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s get back to the `spiderman.py` file. Let''s copy a prepared `spicderman.py`
    file. We''ll copy `examples/spiders/spiderman-c.py` to `basic_crawler/basic_crawler/spiders/spiderman.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let's go back to the editor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In order to extract emails, we need to add the highlighted code to our `spiderman.py`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00035.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This selector could yield some false positives as it will extract any word that
    contains an `@` symbol, as well as the loop to store the results detected by the
    selector into our items.
  prefs: []
  type: TYPE_NORMAL
- en: And that's it, with the code, we'll now extract all the email addresses we find
    while crawling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to do the same to extract the `forms` actions. The cross path
    will get the action attribute for the forms, which points to the page that will
    process the data submitted by the user. Then, we iterate over the findings and
    add it to the `items.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: That's it for forms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s do the same for code `comments`. We''ll create the extractor, and
    again, iterate over the results and add it to the items. Now, we can run the crawler
    and see the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Let's go back to the Terminal, and in `basic_crawler`, we'll type `scrapy crawl
    basic_crawler -o results.json -t json` and hit *Enter*.
  prefs: []
  type: TYPE_NORMAL
- en: It will take a long time to finish the crawling. We'll stop it by pressing *CTRL*
    + *C* after a while.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it is finished, we can open up `results.json` with the Atom editor and
    inspect the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Congratulations! You've extended the crawler to extract interesting information
    about a website.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the results, form, comments, and so on. I suggest you look at other
    ways on how to deal with the results such as passing them or storing them into
    SQLite or MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have created your first web crawler using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw what web application mapping is. We learned how to create
    a basic web application crawler. In this chapter, we added recursion capabilities
    and also learned how to make our crawler recursive.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned how to develop a web application crawler using Python and
    the Scrapy library. This will be useful for mapping the web application structure
    and to harvest interesting information such as forms, emails, and comments from
    the source code of the pages.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we know how to map a web application using a crawler, but most of the applications
    have hidden resources. These resources are not accessible for all the users or
    are not linked by all. Luckily, we can use the brute force technique to discover
    directories, files, or parameters in order to find vulnerabilities or interesting
    information that we can use in our tests.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 4](part0049.html#1ENBI0-5a228e2885234f4ba832bb786a6d0c80), *Resources
    Discovery*, we'll write a tool to perform brute force attacks in different parts
    of the web application.
  prefs: []
  type: TYPE_NORMAL
