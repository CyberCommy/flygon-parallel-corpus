- en: Chapter 4. Building Docker Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Manually networking containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying your own bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using an OVS bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using an OVS bridge to connect Docker hosts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OVS and Docker together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we've seen in earlier chapters, Docker does a great job of handling many
    container networking needs. However, this does not limit you to using only Docker-provided
    network elements to connect containers. So while Docker can facilitate the networking
    for you, you can also connect containers manually. The drawback to this approach
    is that Docker becomes unaware of the network state of the container because it
    wasn't involved in network provisioning. As we'll see in [Chapter 7](ch07.html
    "Chapter 7. Working with Weave Net"), *Working with Weave Net*, Docker now also
    supports custom or third-party network drivers that help bridge the gap between
    native Docker and third-party or custom container network configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Manually networking containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html "Chapter 1. Linux Networking Constructs"), *Linux Networking
    Constructs* and [Chapter 2](ch02.html "Chapter 2. Configuring and Monitoring Docker
    Networks"), *Configuring and Monitoring Docker Networks*, we reviewed common Linux
    network constructs as well as covered the Docker native options for container
    networking. In this recipe, we'll walk through how to manually network a container
    the same way that Docker does in the default bridge network mode. Understanding
    how Docker handles networking provisioning for containers is a key building block
    in understanding non-native options for container networking.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll be demonstrating the configuration on a single Docker
    host. It is assumed that this host has Docker installed and that Docker is in
    its default configuration. In order to view and manipulate networking settings,
    you''ll want to ensure that you have the `iproute2` toolset installed. If not
    present on the system, it can be installed by using the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In order to make network changes to the host, you'll also need root-level access.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to manually provision a container''s network, we need to explicitly
    tell Docker not to provision a container''s network at runtime. To do this, we
    run a container using a network mode of `none`. For instance, we can start one
    of the web server containers without any network configuration by using this syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After the container starts, we can check its network configuration using the
    `docker exec` subcommand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the container doesn''t have any interfaces defined besides
    its local loopback interface. At this point, there is no means to connect to the
    container. What we''ve done is essentially created a container in a bubble:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Because we're aiming to mimic the default network configuration, we now need
    to find a way to connect the container `web1` to the `docker0` bridge and assign
    it an IP address from within the bridges IP allocation (`172.17.0.0/16`).
  prefs: []
  type: TYPE_NORMAL
- en: That being said, the first thing we need to do is create the interfaces that
    we'll use to connect the container to the `docker0` bridge. As we saw in [Chapter
    1](ch01.html "Chapter 1. Linux Networking Constructs"), *Linux Networking Constructs*,
    Linux has a networking component named **Virtual Ethernet** (**VETH**) pairs,
    which will work well for this purpose. One end of the interface will connect to
    the `docker0` bridge and the other end will connect to the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating our VETH pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, we now have two interfaces that are directly associated with each
    other. Let''s now bind one end to the `docker0` bridge and turn up the interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The state of the interface at this point will show as `LOWERLAYERDOWN`. This
    is because the other end of the interface is unbound and still in a down state.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to connect the other end of the VETH pair to the container.
    This is where things get interesting. Docker creates each container in its own
    network namespace. This means the other end of the VETH pair needs to land in
    the container's network namespace. The trick is determining what the container's
    network namespace is. The namespace for a given container can be located in two
    different ways.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first way relies on correlating the container''s **process ID** (**PID**)
    to a defined network namespace. It''s more involved than the second method but
    gives you some good background as to some of the internals of network namespaces.
    As you might recall from [Chapter 3](ch03.html "Chapter 3. User-Defined Networks"),
    *User-Defined Networks*, by default we can''t use the command-line tool `ip netns`
    to view Docker-created namespaces. In order to view them, we need to create a
    symlink that ties the location of where Docker stores its network namespaces (`/var/run/docker/netns`),
    to the location that `ip netns` is looking (`/var/run/netns`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if we attempt to list the namespaces, we should see at least one listed
    in the return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'But how do we know that this is the namespace associated with this container?
    To make that determination, we first need to find the PID of the container in
    question. We can retrieve this information by inspecting the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have PID, we can use the `ip netns identify` subcommand to find
    the network namespace name from the PID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even if you choose to use the second method, make sure that you create the symlink
    so that `ip netns` works for later steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second way to find a container network namespace is much easier. We can
    simply inspect and reference the container''s network configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Notice the field named `SandboxKey`. You'll notice the file path references
    the location where we said that Docker stores its network namespaces. The filename
    referenced in this path is the name of the container's network namespace. Docker
    refers to network namespaces as sandboxes, hence the naming convention used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the network namespace name we can build the connectivity between
    the container and the `docker0` bridge. Recall that VETH pairs can be used to
    connect network namespaces together. In this example, we''ll be placing the container
    end of the VETH pair in the container''s network namespace. This will bridge the
    container into the default network namespace on the `docker0` bridge. To do this,
    we''ll first move the container end of the VETH pair into the namespace we discovered
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can validate the VETH pair is in the namespace using the `docker exec` subcommand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we''ve successfully bridged the container and the default namespace
    together using a VETH pair, so our connectivity now looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: However, the container `web1` is still lacking any kind of IP connectivity since
    it has not yet been allocated a routable IP address. Recall in [Chapter 1](ch01.html
    "Chapter 1. Linux Networking Constructs"), *Linux Networking Constructs*, we saw
    that a VETH pair interface can be assigned an IP address directly. To give the
    container a routable IP address, Docker simply allocates an unused IP address
    from the `docker0` bridge subnet to the container end of the VETH pair.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: IPAM is a huge advantage of allowing Docker to manage your container networking
    for you. Without IPAM, you'll need to track allocations on your own and make sure
    that you don't assign any overlapping IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we could turn up the interface and we should have reachability
    to the container from the host. But before we do that let''s make things a little
    cleaner by renaming the `container_end` VETH pair to just `eth0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can turn up the newly named `eth0` interface, which is the container
    side of the VETH pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If we check from the host, we should now have reachability to the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'With the connectivity in place, our topology now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, while we have IP connectivity, it's only for hosts on the same subnet. The
    last remaining piece would be to solve for container connectivity at the host
    level. For outbound connectivity the host hides the container's IP address behind
    the host's interface IP addresses. For inbound connectivity, in the default network
    mode, Docker uses port mappings to map a random high port on the Docker host's
    NIC to the container's exposed port.
  prefs: []
  type: TYPE_NORMAL
- en: 'Solving for outbound in this case is as simple as giving the container a default
    route pointing at the `docker0` bridge and ensuring that you have a netfilter
    masquerade rule that will cover this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If you''re using the `docker0` bridge as we did in this example, you won''t
    need to add a custom netfilter masquerade rule. This is because the default masquerade
    rule already covers the entire subnet of the `docker0` bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'For inbound services, we''ll need to create a custom rule that uses **Network
    Address Translation** (**NAT**) to map a random high port on the host to the exposed
    service port in the container. We could do that with a rule like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we NAT the port `32799` on the host interface to port `80` on
    the container. This will allow systems on the outside network to access the web
    server running in `web1` via the Docker host's interface on port `32799`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, we have successfully replicated what Docker provides in the default
    network mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This should give you some appreciation for what Docker does behind the scenes.
    Keeping track of container IP addressing, port allocations for published ports,
    and the `iptables` rule set are three of the major things that Docker tracks on
    your behalf. Given the ephemeral nature of containers, this would be almost impossible
    to do manually.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying your own bridge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the majority of network scenarios, Docker relies heavily on the `docker0`
    bridge. The `docker0` bridge is created automatically when the Docker engine service
    is started and is the default connection point for any containers spawned by the
    Docker service. We also saw in earlier recipes that it was possible to modify
    some of this bridge's attributes at a service level. In this recipe, we'll show
    you how to tell Docker to use a different bridge entirely.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll be demonstrating the configuration on a single Docker
    host. It is assumed that this host has Docker installed and that Docker is in
    its default configuration. In order to view and manipulate networking settings,
    you''ll want to ensure that you have the `iproute2` toolset installed. If not
    present on the system, it can be installed using the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In order to make network changes to the host, you'll also need root-level access.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Much like any of the other service level parameters, specifying a different
    bridge for Docker to use is done through updating the systemd drop-in file we
    showed you how to create in [Chapter 2](ch02.html "Chapter 2. Configuring and
    Monitoring Docker Networks"), *Configuring and Monitoring Docker Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we specify the new bridge, let''s first make sure that there are no
    containers running, stop the Docker service, and delete the `docker0` bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: At this point, the default `docker0` bridge has been deleted. Now, let's create
    a new bridge for Docker to use.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you aren't familiar with the `iproute2` command-line tool please refer to
    the examples in [Chapter 1](ch01.html "Chapter 1. Linux Networking Constructs"),
    *Linux Networking Constructs*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We first created a bridge named `mybridge1`, then gave it an IP address of
    `10.11.12.1/24`, and finally turned up the interface. At this point, the interface
    is up and reachable. We can now tell Docker to use this bridge as its default
    bridge. To do this, edit the systemd drop-in file for Docker and make sure that
    the last line reads as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now save the file, reload the systemd configuration, and start the Docker service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if we start a container, we should see it assigned to the bridge `mybridge1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the `docker0` bridge was not created when the service was started.
    Also, notice that we see one side of a VETH pair in the default namespace whose
    master interface is `mybridge1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using what we learned from the first recipe in this chapter, we can also confirm
    that the other end of the VETH pair is in the container''s network namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We can tell this is a VETH pair because it uses the `<interface>@<interface>`
    naming syntax. And if we compare the VETH pair interface numbers, we can see that
    these two are a match with the host side of the VETH pair, having an index of
    `22` connecting to the container side of the VETH pair with an index of `21`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You'll probably notice that I flip between using the `ip netns exec` and `docker
    exec` commands to execute commands inside a container. The point of that is not
    to be confusing but rather to show what Docker is doing on your behalf. It should
    be noted that, in order to use the `ip netns exec` syntax, you need the symlink
    in place that we demonstrated in an earlier recipe. Using the `ip netns exec`
    is only required when you're manually configuring namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the network configuration of the container, we can see that Docker
    has assigned it an IP address within the range of the `mybridge1` subnet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Docker is now also keeping track of IP allocations for the bridge as it allocates
    container IP addresses. IP address management is a large understated value that
    Docker provides in the container network space. Mapping IP addresses to containers
    and managing that on your own would be a significant undertaking.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last piece of this would be handling the NAT configuration for the container.
    Since the `10.11.12.0/24` space is not routable, we''ll need to hide or masquerade
    the container''s IP address behind a physical interface on the Docker host. Luckily,
    so long as Docker is managing the bridge for you, Docker still takes care of making
    the appropriate netfilter rules. We can make sure that this is in place by inspecting
    the netfilter ruleset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In addition, since we exposed ports on the container with the `-P` flag, the
    inbound NAT has also been allocated. We can see this NAT translation as well in
    the DOCKER chain of the same preceding output. In summary, as long as you're using
    a Linux bridge, Docker will handle the entire configuration for you just as it
    did with the `docker0` bridge.
  prefs: []
  type: TYPE_NORMAL
- en: Using an OVS bridge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For users that are looking for additional features, **OpenVSwitch** (**OVS**)
    is becoming a popular replacement for the native Linux bridge. OVS offers a dramatic
    enhancement to the Linux bridge at the cost of a slightly higher level of complexity.
    For instance, an OVS bridge cannot be managed directly by the `iproute2` toolset
    we have been using up to this point and requires its own command-line management
    tools. However, if you're looking for features that don't exist on the Linux bridge,
    OVS is likely your best choice. Docker cannot natively manage an OVS bridge, so
    using one requires that you build the connectivity between the bridge and the
    container manually. That is, we can't just tell the Docker service to use an OVS
    bridge rather than the default `docker0` bridge. In this recipe, we'll show how
    to install, configure, and connect containers directly to an OVS bridge in place
    of the standard `docker0` bridge.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll be demonstrating the configuration on a single Docker
    host. It is assumed that this host has Docker installed and that Docker is in
    its default configuration. In order to view and manipulate networking settings
    you''ll want to ensure that you have the `iproute2` toolset installed. If not
    present on the system, it can be installed using the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In order to make network changes to the host, you'll also need root-level access.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step we''ll need to perform is to install OVS on our Docker host.
    To do this, we can pull down the OVS package directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned earlier, OVS has its own command-line toolset and one of the tools
    is named `ovs-vsctl`, which is used to directly manage OVS bridges. More specifically,
    `ovs-vsctl` is used to view and manipulate the OVS configuration database. To
    ensure that OVS gets installed correctly, we can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the OVS version number and verifies that we have communication
    to OVS. The next thing we want to do is to create an OVS bridge. To do it, we''ll
    once again use the `ovs-vsctl` command-line tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will add an OVS bridge named `ovs_bridge`. Once created, we can
    view the bridge interface just like we did any other network interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'However, to view any bridge-specific information, we''ll once again need to
    rely on the `ocs-vsctl` command-line tool. We can see information about the bridge
    using the `show` subcommand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Assigning the OVS bridge an IP address and changing its state can once again
    be done using the more familiar `iproute2` tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Once up, the interface acts much like any other bridge interface. We can see
    the IP interface is up and the local host can access it directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The next thing we want to do is to create our VETH pair that we''ll use to
    connect a container to the OVS bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Once created, we need to add the OVS end of the VETH pair to the OVS bridge.
    This is one of the big areas where OVS differs from a standard Linux bridge. Each
    connection to OVS is in the form of a port. This is a much stronger imitation
    of a physical switch than what the Linux bridge provides. Once again, because
    we''re interacting directly with the OVS bridge, we''ll need to use the `ovs-vsctl`
    command-line tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Once added, we can query the OVS to see all the bridge''s ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If you examine the defined interfaces, you''ll see that the OVS end of the
    VETH pair lists `ovs-system` as its master:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Without getting into too much detail, this is expected. The `ovs-system` interface
    represents the OVS data path. For now, just know that this is the expected behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the OVS end is completed, we need to focus on the container end. The
    first step here will be to start a container without any network configuration.
    Next, we''ll follow the same steps we did earlier to manually connect a container
    namespace to the other end of a VETH pair:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the container:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Find the containers network namespace:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Move the container end of the VETH pair into that namespace:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Rename the VETH interface to `eth0`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Set `eth0` interface''s IP address to a valid IP in that subnet:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Bring the container-side interface up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Bring the OVS side of the VETH pair up:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, the container is successfully connected to the OVS and reachable
    through the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to dig a little deeper into OVS, we can examine the switches'' MAC
    address table by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the MAC address that it learned on port `1`. But what is port `1`? To
    look at all of the ports for a given OVS you can use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that `port 1` is the OVS bridge that we provisioned and what
    we attached the OVS end of the VETH pair to.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the amount of work required to make the connection to OVS can
    be extensive. Luckily, there are some great tools out there that can help make
    this a lot easier. One of the more notable tools was built by Jérôme Petazzoni
    and is named **Pipework**. It''s available on GitHub at the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jpetazzo/pipework](https://github.com/jpetazzo/pipework)'
  prefs: []
  type: TYPE_NORMAL
- en: If we use Pipework to plumb the connections to OVS, and assume that the bridge
    is already created, we can take the number of steps required to connect the container
    to the bridge from `6` to `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Pipework, you must first download it from GitHub. This can be done by
    using the Git client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'To demonstrate using Pipework, let''s start a new container named `web2` without
    any network configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, all we have to do to connect this container to our existing OVS bridge
    is to run the following command, which specifies the name of the OVS bridge ,
    the container name, and the IP address we wish to assign to the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Pipework takes care of all the leg work for us including resolving the container
    name to a network namespace, creating a unique VETH pair, properly placing the
    ends of the VETH pair in the container and on the specified bridge, and assigning
    the container an IP address.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipework can also help us add additional interfaces to a container on the fly.
    Considering that we started this container with a network mode of `none`, the
    container currently only has a connection to the OVS based on the first Pipework
    configuration. However, we can add the connection to the `docker0` bridge back
    by using Pipework as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The syntax is similar, but in this case we specify the interface name we wanted
    to use (`eth0`) and also add a gateway for the interface of `172.17.0.1`. This
    will allow the container to use the `docker0` bridge as a default gateway and,
    in turn, allow it outbound access using the default Docker masquerade rule. We
    can verify the configuration is present in the container with a couple, `docker
    exec` commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: So while Pipework can make a lot of this manual work easier, you should always
    look to see if Docker has a native means to provide the network connectivity you're
    looking for. Having Docker manage your container network connectivity has lots
    of benefits including automatic IPAM allocations and netfilter configuration for
    inbound and outbound connectivity to your containers. Many of these non-native
    configurations already have third-party Docker network plugins in the works that
    will allow you to leverage them seamlessly from Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Using an OVS bridge to connect Docker hosts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous recipe showed how we can use OVS in place of a standard Linux bridge.
    This, by itself, isn't very interesting since it doesn't do a lot more than a
    standard Linux bridge can. What may be interesting is using some of the more advanced
    features of OVS in conjunction with your Docker containers. For instance, once
    the OVS bridges are created, it's rather trivial to provision GRE tunnels between
    two distinct Docker hosts. This would allow any containers connected to either
    Docker host to talk directly to each other. In this recipe, we'll discuss the
    configuration required to connect two Docker hosts using an OVS provided GRE tunnel.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Again, this recipe is for the purpose of example only. This behavior is already
    supported by Docker's user-defined overlay network type. If for some reason you
    need to use GRE rather than VXLAN, this might be a suitable alternative. As always,
    make sure that you use any Docker native networking constructs before you start
    customizing your own. It will save you a lot of headache down the road!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll be demonstrating the configuration on two Docker hosts.
    The hosts need to be able to talk to each other across the network. It is assumed
    that hosts have Docker installed and that Docker is in its default configuration.
    In order to view and manipulate networking settings, you''ll want to ensure that
    you have the `iproute2` toolset installed. If not present on the system, it can
    be installed by using the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: In order to make network changes to the host, you'll also need root-level access.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the purpose of this recipe, we'll start by assuming a base configuration
    on both hosts used in this example. That is, each host only has Docker installed,
    and its configuration is unchanged from the default.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topology we''ll use will look like what''s shown in the following image.
    Two Docker hosts on two distinct subnets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The goal of this configuration will be to provision OVS on each host, connect
    the containers to the OVS, and then connect the two OVS switches together to allow
    OVS-to-OVS direct communication over GRE. We''ll follow these steps on each host
    to accomplish this:'
  prefs: []
  type: TYPE_NORMAL
- en: Install OVS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add an OVS bridge named `ovs_bridge`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign that bridge an IP address.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a container with its network mode set to `none`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use Pipework to connect that container to the OVS bridge (it is assumed that
    you have Pipework installed on each host. If you do not, please refer to the previous
    recipe for installation steps).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a GRE tunnel to the other host using OVS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s start the configuration on the first host `docker1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, you should have a container up and running. You should be able
    to reach the container from the local Docker host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s perform a similar configuration on the second host `docker3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'This completes the configuration of the second host. Ensure that you have connectivity
    to the `web2` container running on the local host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, our topology looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If our goal is to allow the container `web1` to talk directly to the container
    `web2`, we would have two options. First, since Docker is unaware of the OVS switch,
    it is not attempting to apply netfilter rules based on containers connected to
    it. That being said, with the correct routing configuration, the two containers
    could natively route to each other. However, even in this trivial example, that
    would be a lot of configuration. Since we''re sharing a common subnet between
    the two hosts (as Docker does even in its default mode), the configuration becomes
    less than trivial. To make this work, you would need to do the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Add routes into each container telling them that the specific `/32` route for
    the other container lived off subnet. This is because each container believes
    that the entire `10.11.12.0/24` network is local since they both have an interface
    on that network. You would need a prefix more specific (smaller) than `/24` to
    force the container to route to reach the destination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add routes on each Docker host telling them that the specific `/32` route for
    the other containers lived off subnet. Again, this is because each host believes
    that the entire `10.11.12.0/24` network is local since they both have an interface
    on that network. You would need a prefix more specific (smaller) than `/24` to
    force the host to route to reach the destination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add routes on the multilayer switch, so it knows that `10.11.12.100` is reachable
    via `10.10.10.101` (`docker1`) and `10.11.12.200` is reachable via `192.168.50.101`
    (`docker3`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now imagine if you were working with a real network and had to add those routes
    on every device in the path. The second, and better, option is to create a tunnel
    between the two OVS bridges. This would prevent the network from ever seeing the
    `10.11.12.0/24` traffic, which means it doesn''t need to know how to route it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Fortunately for us, this configuration is easy to do with OVS. We simply add
    another OVS port of type GRE and specify the other Docker host as the remote tunnel
    destination.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the host `docker1`, build the GRE tunnel as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'On the host `docker3`, build the GRE tunnel as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, the two containers should be able to communicate with one another
    directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'As a final proof that this is traversing the GRE tunnel, we can run `tcpdump`
    on one of the host''s physical interfaces while doing a ping test between the
    containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: OVS and Docker together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The recipes until this point have shown several possibilities for what''s possible
    when manually configuring Docker networks. Although these are all possible solutions,
    they all require a fair amount of manual intervention and configuration and are
    not easily consumable in their current form. If we use the previous recipe as
    an example, there are a few notable drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: You are responsible for keeping track of IP allocations on the containers increasing
    your risk of assigning conflicting IP addresses to different containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no dynamic port mapping or inherent outbound masquerading to facilitate
    communication between a container and the rest of the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While we used Pipework to lessen the configuration burden, there was still a
    fair amount of manual configuration that needed to be done to connect a container
    to the OVS bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The majority of the configuration would not persist through a host reboot by
    default
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This being said, using what we've learned so far, there is a different way that
    we can leverage the GRE capability of OVS while still using Docker to manage container
    networking. In this recipe, we'll review that solution as well as describe how
    to make it a more persistent solution that will survive a host reboot.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Again, this recipe is for the purpose of example only. This behavior is already
    supported by Docker's user-defined overlay network type. If for some reason, you
    need to use GRE rather than VXLAN, this might be a suitable alternative. As always,
    make sure that you use any Docker native networking constructs before you start
    customizing your own. It will save you a lot of headache down the road!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll be demonstrating the configuration on two Docker hosts.
    The hosts need to be able to talk to each other across the network. It is assumed
    that hosts have Docker installed and that Docker is in its default configuration.
    In order to view and manipulate networking settings, you''ll want to ensure that
    you have the `iproute2` toolset installed. If not present on the system, it can
    be installed using the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: In order to make network changes to the host, you'll also need root-level access.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the previous recipe for inspiration, our new topology will look similar,
    but will have one significant difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You'll notice that each host now has a Linux bridge named `newbridge`. We're
    going to tell Docker to use this bridge rather than the `docker0` bridge for default
    container connectivity. This means that we're only using OVS for its GRE capabilities
    turning it into a slave to `newbridge`. Using a Linux bridge for container connectivity
    means that Docker is able to do IPAM for us as well as handle inbound and outbound
    netfilter rules. Using a bridge other than `docker0` has more to do with configuration
    than usability, as we'll see shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to once again start the configuration from scratch assuming that
    each host only has Docker installed in its default configuration. The first thing
    we want to do is to configure the two bridges we''ll be using on each host. We''ll
    start with the host `docker1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we have both an OVS bridge as well as a standard Linux bridge
    configured on the host. To finish up the bridge configuration, we need to create
    the GRE interface on the OVS bridge and then bind the OVS bridge to the Linux
    bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the bridge configuration is complete, we can tell Docker to use `newbridge`
    as its default bridge. We do that by editing the systemd drop-in file and adding
    the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Notice that, in addition to telling Docker to use a different bridge, I'm also
    telling Docker to only allocate container IP addressing from `10.11.12.128/26`.
    When we configure the second Docker host (`docker3`), we'll tell Docker to only
    assign container IP addressing from `10.11.12.192/26`. This is a hack, but it
    prevents the two Docker hosts from issues overlapping IP addresses without having
    to be aware of what the other host has already allocated.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Chapter 3](ch03.html "Chapter 3. User-Defined Networks"), *User-Defined Networks*,
    demonstrated that the native overlay network gets around this by tracking IP allocations
    between all hosts that participate in the overlay network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To make Docker use the new options, we need to reload the system configuration
    and restart the Docker service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, start a container without specifying a network mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the first container we ran gets the first available IP address
    in the `10.11.12.128/26` network. Now, let''s move on to configuring the second
    host `docker3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'On this host, tell Docker to use the following options by editing the systemd
    drop-in file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Reload the system configuration and restart the Docker service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Now spin up a container on this host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, each container should be able to talk to the other across the
    GRE tunnel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: In addition, each host still has all the benefits Docker provides through IPAM,
    publishing ports, and container masquerading for outbound access.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify port publication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can verify outbound access through the default Docker masquerade rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: The last advantage to this setup is that we can easily make it persist through
    host reboots. The only configuration that will need to be recreated will be the
    configuration for the Linux bridge `newbridge` and the connection between `newbridge`
    and the OVS bridge. To make this persistent, we can add the following configuration
    in each host's network configuration file (`/etc/network/interfaces`).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ubuntu will not process bridge-related configuration in the interface's file
    unless you have the bridge utilities package installed on the host.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Host `docker1`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Host `docker3`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: By putting the `newbridge` configuration information into the network start
    script, we accomplish two tasks. First, we create the bridge that Docker is expecting
    to use before the actual Docker service starts. Without this, the Docker service
    would fail to start because it couldn't find the bridge. Second, this configuration
    allows us to bind the OVS to `newbridge` at the same time that the bridge is created
    by specifying the bridge's `bridge_ports`. Because this configuration was done
    manually before through the `ip link` command, the binding would not persist through
    a system reboot.
  prefs: []
  type: TYPE_NORMAL
