- en: Chapter 10. Statistical Programming and Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll look at the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the built-in statistics library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average of values in a Counter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the coefficient of a correlation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing regression parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing an autocorrelation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confirming that the data is random – the null hypothesis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locating outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing many variables in one pass
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data analysis and statistical processing are very import applications for sophisticated,
    modern programming languages. The subject area is vast. The Python ecosystem includes
    a number of add-on packages that provide sophisticated data exploration, analysis,
    and decision-making features.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at some basic statistical calculations that we can do with Python's
    built-in libraries and data structures. We'll look at the question of correlation
    and how to create a regression model.
  prefs: []
  type: TYPE_NORMAL
- en: We'll also look at questions of randomness and the null hypothesis. It's essential
    to be sure that there really is a measurable statistical effect in a set of data.
    We can waste a lot of compute cycles analyzing insignificant noise if we're not
    careful.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at a common optimization technique, as well. It helps to produce
    results quickly. A poorly designed algorithm applied to a very large set of data
    can be an unproductive waste of time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image/614271.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using the built-in statistics library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A great deal of **exploratory data analysis** ( **EDA** ) involves getting
    a summary of the data. There are several kinds of summary that might be interesting:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Central Tendency** : Values such as the mean, mode, and median can characterize
    the center of a set of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extrema** : The minimum and maximum are as important as the central measures
    of some data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variance** : The variance and standard deviation are used to describe the
    dispersal of the data. A large variance means the data is widely distributed;
    a small variance means the data clusters tightly around the central value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we get basic descriptive statistics in Python?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll look at some simple data that can be used for statistical analysis. We've
    been given a file of raw data, called `anscombe.json` . It's a JSON document that
    has four series of ( *x* , *y* ) pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can read this data with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We've defined the `Path` to the data file. We can then use the `Path` object
    to read the text from this file. This text is used by `json.loads()` to build
    a Python object from the JSON data.
  prefs: []
  type: TYPE_NORMAL
- en: We've included an `object_pairs_hook` so that this function will build the JSON
    using the `OrderedDict` class instead of the default `dict` class. This will preserve
    the original order of items in the source document.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine the data like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The overall JSON document is a sequence of subdocuments with keys such as `I`
    and `II` . Each subdocument has two fields—`series` and `data` . Within the `data`
    value, there's a list of observations that we want to characterize. Each observation
    has a pair of values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This is a list of dict structure, typical of JSON documents. Each dict has a
    series name, with a key `series` , and a sequence of data values, with a key `data`
    . The list within `data` is a sequence of items, and each item has an `x` and
    a `y` value.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find a specific series in this data structure, we have a number of choices:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `for...if...return` statement sequence:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This `for` statement examines each series in the sequence of values. The series
    is a dictionary with a key of `'series'` that has the series name. The `if` statement
    compares the series name with the target name, and returns the first match. This
    will return `None` for an unknown series name.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can access the data like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use a filter that finds all matches, from which the first is selected:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This `filter()` function examines each series in the sequence of values. The
    series is a dictionary with a key of `'series'` that has the series name. The
    `name_match` lambda object will compare the name key of the series with the target
    name, and return all of the matches. This is used to build a `list` object. If
    each key is unique, the first item is the only item. This will raise an `IndexError`
    exception for an unknown series name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can access the data like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use a generator expression that, similar to the filter, finds all matches.
    We pick the first from the resulting sequence:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This generator expression examines each series in the sequence of values. The
    series is a dictionary with a key of `'series'` that has the series name. Instead
    of a lambda object, or function, the expression `s['series'] == series_name` will
    compare the name key of the series with the target name, and pass all of the matches.
    This is used to build a `list` object, and the first item from the list is returned.
    This will raise an `IndexError` exception for an unknown series name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can access the data like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'There are some examples of this kind of processing in the *Implementing "there
    exists" Processing* recipe in [Chapter 8](text00088.html#page "Chapter 8. Functional
    and Reactive Programming Features") , *Functional and Reactive Programming Features*
    Once we''ve picked a series from the data, we''ll also need to pick a variable
    from the series. This can be done with a generator function or a generator expression:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'A series dictionary has a `data` key with the sequence of data values. Each
    data value is a dictionary with two keys, `x` , and `y` . This `data_iter()` function
    will pick one of those variables from each dictionary in the data. This function
    will generate a sequence of values that can be used for detailed analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we picked the series `IV` . From that series, we picked the `x`
    variable from each observation. The length of the resulting list shows us that
    there were 11 observations in this series.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To compute the mean and median, use the `statistics` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This uses `get_series()` and `data_iter()` to select sample values from one
    variable of a given series. The `mean()` and `median()` functions handle this
    task nicely. There are several variations on the median calculation that are available.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute `mode` , use the collections module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This uses `get_series()` and `data_iter()` to select sample values from one
    variable of a given series. The `Counter` object does this job very elegantly.
    We actually get a complete frequency histogram from this operation. The result
    of the `most_common()` method shows both the value and the number of times it
    occurred.
  prefs: []
  type: TYPE_NORMAL
- en: We can also use the `mode()` function in the `statistics` module. This function
    has the advantage of raising an exception when there is no obvious mode. This
    has the disadvantage of not providing any additional information to help locate
    multimodal data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The extrema are computed with the built-in `min()` and `max()` functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This uses `get_series()` and `data_iter()` to select sample values from one
    variable of a given series. The built-in `max()` and `min()` functions provide
    the values for the extrema.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute variance (and standard deviation), we can also use the `statistics`
    module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This uses `get_series()` and `data_iter()` to select sample values from one
    variable of a given series. The statistics module provides the `variance()` and
    `stdev()` functions that compute the statistical measures of interest.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These functions are generally first class parts of the Python standard library.
    We''ve looked in three places for useful functions:'
  prefs: []
  type: TYPE_NORMAL
- en: The `min()` and `max()` functions are built-in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `collections` module has the `Counter` class, which can create a frequency
    histogram. We can get the mode from this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `statistics` module has `mean()` , `median()` , `mode()` , `variance()`
    , and `stdev()` , which provide a variety of statistical measures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `data_iter()` is a generator function. We can only use the results
    of this generator once. If we only want to compute a single statistical summary
    value, that will work nicely.
  prefs: []
  type: TYPE_NORMAL
- en: When we want to compute more than one value, we need to capture the result of
    the generator in a collection object. In these examples, we've used `data_iter()`
    to build a `list` object so that we can process it more than once.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our original data structure, `data` , is a sequence of mutable dictionaries.
    Each dictionary has two keys—`series` and `data` . We can update this dictionary
    with the statistical summaries. The resulting object can be saved for later analysis
    or display.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a starting point for this kind of processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: For each one one the data series, we've used the `data_iter()` function to extract
    the individual samples. We've applied the `mean()` function to those samples.
    The result is saved back into the `series` object, using a string key made from
    the function name, `mean` , the `_` character, and the `variable_name` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that a great deal of this function is boilerplate code. The overall structure
    would have to be repeated for median, mode, minimum, maximum, and so on. Looking
    at changing the function from `mean()` to something else shows that there are
    two things that change in this boilerplate code:'
  prefs: []
  type: TYPE_NORMAL
- en: The key that is used to update the series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function that's evaluated for the selected sequence of samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We don''t need to supply the function''s name; we can extract the name from
    a function object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that we can write a higher-order function that applies a number
    of functions to a set of samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We've replaced the specific function, `mean()` , with a parameter name, `function`
    , that can be bound to any Python function. The processing will apply the given
    function to the results of `data_iter()` . This summary is then used to update
    the series dictionary using the function's name, the `_` character, and the `variable_name`
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'This higher-level `set_summary()` function looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This will update our document with four summaries based on `mean()` , `median()`
    , `max()` , and `min()` . We can use any Python function, so functions such as `sum()`
    can be used in addition to functions like those shown earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Because `statistics.mode()` will raise an exception for cases where there's
    no single modal value, this function may need a `try:` block to catch the exception
    and put some useful result into the `series` object. It may also be appropriate
    to allow the exception to propagate to notify the collaborating function that
    the data is suspicious.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our revised document will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can save this to a file and use it for further analysis. Using `pathlib`
    to work with file names, we might do something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This will create a second file adjacent to the source file. The name will have
    the same stem as the source file, but the stem will be extended with the string
    `_stats` and a suffix of `.json` .
  prefs: []
  type: TYPE_NORMAL
- en: Average of values in a Counter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `statistics` module has a number of useful functions. These are based on
    having each individual data sample available for processing. In some cases, however,
    the data has been grouped into bins. We might have a `collections.Counter` object
    instead of a simple list. Rather than values, we now have (value, frequency) pairs.
  prefs: []
  type: TYPE_NORMAL
- en: How can we do statistical processing on (value, frequency) pairs?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The general definition of the mean is the sum of all of the values divided
    by the number of values. It''s often written like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](Image00027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We've defined some set of data, *C* , as a sequence of individual values, *C*
    = { *c* [0] *, c* [1] *, c* [2] *, ... ,c[n]* }, and so on. The mean of this collection,
    μ [*C*] , is the sum of the values over the number of values, *n* .
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s a tiny change that helps to generalize this definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](Image00028.jpg)![Getting ready](Image00029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The value of *S* ( *C* ) is the sum of the values. The value of *n* ( *C* )
    is the sum using one instead of each value. In effect,  *S* ( *C* ) is the sum
    of *c* [*i*] ¹ and  *n* ( *C* ) is the sum of  *c* [*i*] ⁰ . We can easily implement
    these as simple Python generator expressions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can reuse these definition in a number of places. Specifically, we can now
    define the mean, μ [*C*] , like this:'
  prefs: []
  type: TYPE_NORMAL
- en: μ [*C *] = *S* ( *C* )/ *n* ( *C* )
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use this general idea to provide statistical calculations on data that''s
    already been collected into bins. When we have a `Counter` object, we have values
    and frequencies. The data structure can be described like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*F* = { *c* [0] : *f* [0] , *c* [1] : *f* [1] , *c* [2] : *f* [2] , ... *c[m]*
    : *f[m]* }'
  prefs: []
  type: TYPE_NORMAL
- en: 'The values, *c[i]* , are paired with a frequency, *f[i]* . This makes two small
    changes to perform similar calculations for ![Getting ready](Image00030.jpg)  and
    ![Getting ready](Image00031.jpg)  :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](Image00032.jpg)![Getting ready](Image00033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We've defined ![Getting ready](Image00030.jpg)  to use the product of frequency
    and value. Similarly, we've defined ![Getting ready](Image00031.jpg)  to use the
    frequencies. We've included the hat, ^, on each name to make it clear that these
    functions don't work for simple lists of values; these functions work for lists
    of (value, frequency) pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'These need to be implemented in Python. As an example, we''ll use the following
    `Counter` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This data is from the *Using the built-in statistics library* recipe. The `Counter`
    object looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This shows the various values in a set of samples as well as the frequencies
    for each distinct value.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Define the sum of a `Counter` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the total number of values in a `Counter` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now combine these to compute a mean of data that has been put into bins:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `Counter` is a dictionary. The keys of this dictionary are the actual values
    being counted. The values in the dictionary are the frequencies for each item.
    This means that the `items()` method will produce value and frequency information
    that can be used by our calculations.
  prefs: []
  type: TYPE_NORMAL
- en: We've transformed each of the definitions for ![How it works...](Image00030.jpg)  and ![How
    it works...](Image00031.jpg)  into generator expressions. Because Python is designed
    to follow the mathematical formalisms closely, the code follows the math in a
    relatively direct way.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To compute the variance (and standard deviation) we''ll need two more variations
    on this theme. We can define an overall mean of a frequency distribution, μ [*F*]
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](Image00034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where *c[i]* is the key from the `Counter` object, *F* , and *f[i]* is the frequency
    value for the given key from the `Counter` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The variance, VAR [*F*] , can be defined in a way that depends on the mean,
    μ [*F*] . The formula is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](Image00035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This computes the difference between a value, *c* [*i*] , and the mean μ [*F*]
    . This is weighted by the number of times this value occurs, *f[i]* . The sum
    of these weighted differences is divided by the count, ![There's more...](Image00031.jpg)  ,
    minus one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard deviation, σ [*F*] , is the square root of the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: σ [*F*] = √VAR [*F*]
  prefs: []
  type: TYPE_NORMAL
- en: This version of the standard deviation is quite stable mathematically, and therefore
    is preferred. It requires two passes through the data, but for some edge cases,
    the cost of making multiple passes is better than an erroneous result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another variation on the calculation does not depend on the mean, μ [*F*] .
    This isn''t as mathematically stable as the previous version. This variation separately
    computes the sum of squares of values, the sum of the values, and the count of
    the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](Image00036.jpg)![There''s more...](Image00037.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This requires one extra sum computation. We''ll need to compute the sum of
    the values squared, ![There''s more...](Image00038.jpg)  :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Given these three sum functions, ![There''s more...](Image00031.jpg)  , ![There''s
    more...](Image00030.jpg)  , and ![There''s more...](Image00039.jpg)  , we can
    define the variance for a binned summary,  *F*  :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `counter_variance()` function fits the mathematical definition very closely.
    The Python version moves the 1/( *n* - 1) term around as a minor optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `counter_variance()` function, we can compute the standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This allows us to see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also make use of the `elements()` method of a `Counter` object. While
    simple, this will create a potentially large intermediate data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We've used the `elements()` method of a `Counter` object to create an expanded
    list of all of the elements in the counter. We can compute statistical summaries
    of these elements. For a large `Counter` , this can become a very large intermediate
    data structure.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the  *Designing classes with lots of processing*  recipe in [Chapter 6](text00070.html#page
    "Chapter 6. Basics of Classes and Objects") , *Basics of Classes and Objects,*
    we looked at this from a slightly different perspective. In that recipe, our objective
    was simply to conceal a complex data structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Analyzing many variables in one pass* recipe, in this chapter will address
    some efficiency considerations. In that recipe, we'll look at ways to compute
    multiple sums in a single pass through the data elements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the coefficient of a correlation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the  *Using the built-in statistics library* and *Average of values in a
    Counter*  recipes, we looked at ways to summarize data. These recipes showed how
    to compute a central value, as well as variance and extrema.
  prefs: []
  type: TYPE_NORMAL
- en: Another common statistical summary involves the degree of correlation between
    two sets of data. This is not directly supported by Python's standard library.
  prefs: []
  type: TYPE_NORMAL
- en: One commonly used metric for correlation is called **Pearson's r** . The *r*
    -value is number between -1 and +1 that expresses the probability that the data
    values will correlate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'A value of zero says the data is random. A value of *0.95* suggests that 95%
    of the values correlate, and 5% don''t correlate well. A value of *-.95* says
    that 95% of the values have an inverse correlation: when one variable increases,
    the other decreases.'
  prefs: []
  type: TYPE_NORMAL
- en: How can we determine if two sets of data correlate?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One expression for Pearson''s *r* is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](Image00040.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This relies on a large number of individual summations of various parts of a
    dataset. Each of the ∑ *z* operators can be implemented via the Python `sum()`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use data from the *Using the built-in statistics library* recipe. We
    can read this data with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We've defined the `Path` to the data file. We can then use the `Path` object
    to read the text from this file. This text is used by `json.loads()` to build
    a Python object from the JSON data.
  prefs: []
  type: TYPE_NORMAL
- en: We've included an `object_pairs_hook` so that this function will build the JSON
    using the `OrderedDict` class instead of the default `dict` class. This will preserve
    the original order of items in the source document.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine the data like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The overall JSON document is a sequence of subdocuments with keys like `I` .
    Each subdocument has two fields—`series` and `data` . Within the `data` value
    there's a list of observations that we want to characterize. Each observation
    has a pair of values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This set of data has four series, each of which is represented as a list-of-dict
    structures. Within each series, the individual items are a dictionary with `x`
    and `y` keys.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Identify the various kinds of sums required. For this expression, we see the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ∑ *x[i] , y[i]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ∑ *x[i]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ∑ *y[i]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ∑ *x[i]* ²
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ∑ *y[i]* ²
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![How to do it...](Image00041.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: The count, *n* , can be defined really as the sum of one for each data in the
    source dataset. This can also be thought of as  *x[i]* ^∘ or *y[i]* ^∘ .
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `sqrt()` function from the `math` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function that wraps the calculation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the various sums using the built-in `sum()` function. This is indented
    within the function definition. We''ll use the value of the `data` parameter:
    a sequence of values from a given series. The input data must have two keys, `x`
    and `y` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the final calculation of *r* based on the various sums. Be sure the indentation
    matches properly. For more help, see [Chapter 3](text00039.html#page "Chapter 3. Function
    Definitions") , *Function Definitions* :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use this to determine the degree of correlation between the various
    series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: All four series have approximately the same coefficient of correlation. This
    doesn't mean the series are related to each other. It means that within each series,
    82% of the *x* values predict a *y* value. This is almost exactly nine of the
    11 values in each series.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The overall formula looks rather complex. However, it decomposes into a number
    of separate sums and a final calculation that combines the sums. Each of the sums
    operations can be expressed very succinctly in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conventionally, the mathematical notation might look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](Image00042.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This translates to Python in a very direct way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The final correlation ratio can be simplified somewhat. When we replace the
    more complex looking ![How it works...](Image00042.jpg)  with the slightly more
    Pythonic  *S*  ( *x* ), we can better see the overall form of the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](Image00043.jpg)'
  prefs: []
  type: TYPE_IMG
- en: While simple, the implementation shown isn't optimal. It makes six separate
    passes over the data to compute each of the various reductions. As a kind of proof
    of concept this implementation works well. This implementation has the advantage
    of demonstrating that the programming works. It also serves as a starting point
    for creating unit tests and refactoring the algorithm to optimize the processing.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The algorithm, while clear, is inefficient. A more efficient version would process
    the data once. To do this, we'll have to write an explicit `for` statement that
    makes a single pass through the data. Within the body of the `for` statement,
    the various sums are computed.
  prefs: []
  type: TYPE_NORMAL
- en: 'An optimized algorithm looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We've initialized a number of results to zero, then accumulated values into
    these results from a source of data items, `data` . Since this uses the data value
    once only, this will work with any iterable data source.
  prefs: []
  type: TYPE_NORMAL
- en: The calculation of *r* from these sums doesn't change.
  prefs: []
  type: TYPE_NORMAL
- en: 'What''s important is the parallel structure between the initial version of
    the algorithm and the revised version that has been optimized to compute all of
    the summaries in one pass. The clear symmetry of the two versions helps validate
    two things:'
  prefs: []
  type: TYPE_NORMAL
- en: The initial implementation matches the rather complex formula
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The optimized implementation matches the initial implementation and the complex
    formula
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This symmetry coupled with proper test cases provides confidence that the implementation
    is correct.
  prefs: []
  type: TYPE_NORMAL
- en: Computing regression parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we've determined that two variables have some kind of relationship, the
    next step is to determine a way to estimate the dependent variable from the value
    of the independent variable. With most real-world data, there are a number of
    small factors that will lead to random variation around a central trend. We'll
    be estimating a relationship that minimizes these errors.
  prefs: []
  type: TYPE_NORMAL
- en: In the simplest cases, the relationship between variables is linear. When we
    plot the data points, they will tend to cluster around a line. In other cases,
    we can adjust one of the variables by computing a logarithm or raising it to a
    power to create a linear model. In more extreme cases, a polynomial is required.
  prefs: []
  type: TYPE_NORMAL
- en: How can we compute the linear regression parameters between two variables?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The equation for an estimated line is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](Image00044.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Given the independent variable, *x* , the estimated or predicted value of the
    dependent variable, ![Getting ready](Image00045.jpg)  , is computed from the α
    and β parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to find values of α and β that produce the minimal overall error
    between the estimated values, ![Getting ready](Image00045.jpg)  , and the actual
    values for  *y*  . Here''s the computation of β:'
  prefs: []
  type: TYPE_NORMAL
- en: β = *r[xy]* (σ [*x*] /σ *[y]* )
  prefs: []
  type: TYPE_NORMAL
- en: Where  *r[xy]* is the correlation coefficient. See the *Computing the coefficient
    of correlation* recipe. The definition of σ [*x*] is the standard deviation of
    *x* . This value is given directly by the `statistics` module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the computation of α:'
  prefs: []
  type: TYPE_NORMAL
- en: α = μ [*y*] - βμ [*x*]
  prefs: []
  type: TYPE_NORMAL
- en: Where μ [*x*] is the mean of of *x* . This, also, is given directly by the `statistics`
    module.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use data from the *Using the built-in statistics library* recipe. We
    can read this data with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We've defined the `Path` to the data file. We can then use the `Path` object
    to read the text from this file. This text is used by `json.loads()` to build
    a Python object from the JSON data.
  prefs: []
  type: TYPE_NORMAL
- en: We've included an `object_pairs_hook` so that this function will build the JSON
    using the `OrderedDict` class instead of the default `dict` class. This will preserve
    the original order of items in the source document.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine the data like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The overall JSON document is a sequence of subdocuments with keys such as `I`
    . Each subdocument has two fields: `series` and `data` . Within the `data` value
    there''s a list of observations that we want to characterize. Each observation
    has a pair of values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This set of data has four series, each of which is represented as a list-of-dict
    structures. Within each series, the individual items are a dictionary with `x`
    and `y` keys.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the `correlation()` function and the `statistics` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function that will produce the regression model, `regression()` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the various values required:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the β and α values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this `regression()` function to compute the regression parameters
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the formula that predicts an expected `y` from a given `x`
    value. The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: In all cases, the equations are ![How to do it...](Image00046.jpg)  . This estimation
    appears to be a pretty good predictor of the the actual values of  *y*  .
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The two target formulae for α and β are not complex. The formula for β decomposes
    into the correlation value used with two standard deviations. The formula for α
    uses the β value and two means. Each of these is part of a previous recipe. The
    correlation calculation contains the actual complexity.
  prefs: []
  type: TYPE_NORMAL
- en: The core design technique is to build new features using as many existing features
    as possible. This spreads the test cases around so that the foundational algorithms
    are used (and tested) widely.
  prefs: []
  type: TYPE_NORMAL
- en: The analysis of the performance of *Computing the coefficient of a correlation*
    is important, and applies here, as well. This process makes five separate passes
    over the data to get the correlation as well as the various means and standard
    deviations.
  prefs: []
  type: TYPE_NORMAL
- en: As a kind of proof of concept, this implementation demonstrates that the algorithm
    will work. It also serves as a starting point for creating unit tests. Given a
    working algorithm, then, it makes sense to refactor the code to optimize the processing.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The algorithm shown earlier, while clear, is inefficient. In order to process
    the data once, we''ll have to write an explicit `for` statement that makes a single
    pass through the data. Within the body of the `for` statement, we''ll need to
    compute the various sums. We''ll also need to compute some values derived from
    the sums, including the mean and standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We've initialized a number of results to zero, then accumulated values into
    these results from a source of data items, `data` . Since this uses the data value
    once only, this will work with any iterable data source.
  prefs: []
  type: TYPE_NORMAL
- en: The calculation of `r_xy` from these sums doesn't change from the previous examples.
    Nor does the calculation of the α or β values, `a` and `b` . Since these final
    results are the same as the previous version, we have confidence that this optimization
    will compute the same answer but do it with only one pass over the data.
  prefs: []
  type: TYPE_NORMAL
- en: Computing an autocorrelation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many cases, events occur in a repeating cycle. If the data correlates with
    itself, this is called an autocorrelation. With some data, the interval may be
    obvious because there's some visible external influence, such as seasons or tides.
    With some data, the interval may be difficult to discern.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Computing the coefficient of a correlation* recipe, we looked at a way
    to measure correlation between two sets of data.
  prefs: []
  type: TYPE_NORMAL
- en: If we suspect we have cyclic data, can we leverage the previous correlation
    function to compute an autocorrelation?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The core concept behind autocorrelation is the idea of a correlation through
    a shift in time, T. The measurement for this is sometimes expressed as *r[xx]*
    (T): the correlation between *x* and *x* with a time shift of T.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume we have a handy correlation function, *R* ( *x* , *y* ). It compares
    two sequences, [ *x* [0] , *x* [1] , *x* [2] , ...] and [ *y* [0] , *y* [1] ,
    *y* [2] , ...], and returns the coefficient of correlation between the two sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '*r[xy]* = *R* ([ *x* [0] , *x* [1] , *x* [2] , ...], [ *y* [0] , *y* [1] ,
    *y* [2] , ...])'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply this to autocorrelation by using as a time-shift in the index
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '*r[xx]* (T) = *R* ([ *x* [0] , *x* [1] , *x* [2] , ...], [ *x* [0+T] , *x*
    [1+T] , *x* [2+T] , ...])'
  prefs: []
  type: TYPE_NORMAL
- en: We've computed the correlation between values of *x* that are offset from each
    other by T. If T = 0, we're comparing each item with itself, the correlation is
    *r[xx]* (0) = 1.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use some data that we suspect has a seasonal signal in it. This is data
    from [http://www.esrl.noaa.gov/gmd/ccgg/trends/](http://www.esrl.noaa.gov/gmd/ccgg/trends/)
    . We can visit [ftp://ftp.cmdl.noaa.gov/ccg/co2/trends/co2_mm_mlo.txt](ftp://ftp.cmdl.noaa.gov/ccg/co2/trends/co2_mm_mlo.txt)
    to download the file of the raw data.
  prefs: []
  type: TYPE_NORMAL
- en: The file has a preamble with lines that start with `#` . These must be filtered
    out of the data. We'll use the  *Picking a subset – three ways to filter*  recipe in
    [Chapter 8](text00088.html#page "Chapter 8. Functional and Reactive Programming
    Features") , *Functional and Reactive Programming Features* , that will remove
    the lines that aren't useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'The remaining lines are in seven columns with space as the separator between
    values. We''ll use the *Reading delimited files with the CSV module* recipe in
    the [Chapter 9](text00099.html#page "Chapter 9. Input/Output, Physical Format,
    and Logical Layout") , *Input/Output, Physical Format, and Logical Layout*  to
    read CSV data. In this case, the comma in CSV will be a space character. The result
    will be a little awkward to use, so we''ll use the *Upgrading CSV from Dictreader
    to namespace reader* recipe in [Chapter 9](text00099.html#page "Chapter 9. Input/Output,
    Physical Format, and Logical Layout") , *Input/Output, Physical Format, and Logical
    Layout* to create a more useful namespace with properly converted values. In that
    recipe, we imported the `CSV` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are two functions to handle the essential aspects of the physical format
    of the file. The first is a filter to reject comment lines; or, viewed the other
    way, pass non-comment lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The `non_comment_iter()` function will iterate through the given source and
    reject lines that start with `#` . All other lines will be passed untouched.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `non_comment_iter()` function can be used to build a CSV reader that handles
    the lines of valid data. The reader needs some additional configuration to define
    the data columns and the details of the CSV dialect involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The `raw_data_iter()` function defines the seven column headers. It also specifies
    that the column delimiter is a space, and the additional spaces at the front of
    each column of data can be skipped. The input to this function must be stripped
    of comment lines, generally by using a filter function such as `non_comment_iter()`
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of this function are rows of data in the form of dictionaries with
    seven keys. These rows look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the values are all strings, a pass of cleansing and conversion is required.
    Here''s a row cleansing function that can be used in a generator expression. This
    will build a `SimpleNamespace` object, so we''ll need to import that definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: This function will convert each dictionary row to a `SimpleNamespace` by applying
    a conversion function to the values in the dictionary. Most of the items are floating-point
    numbers, so the `float()` function is used. A few of the items are integers, and
    the `int()` function is used for those.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write the following kind of generator expression to apply this cleansing
    function to each row of raw data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: This will apply the `cleanse()` function to each row of data. Generally, the
    expectation is that the rows will come from the `raw_data_iter()` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying the `cleanse()` function to each row will create data that looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: This data is very easy to work with. The individual fields can be identified
    by a simple name, and the data values have been converted to Python internal data
    structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'These functions can be combined into a stack as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: The `get_data()` generator function is a stack of generator functions and generator
    expressions. It returns an iterator which will yield individual rows of the source
    data. The `non_comment_iter()` function will read enough lines to be able to yield
    a single non-comment line. The `raw_data_iter()` function will parse a line of
    CSV and yield a dictionary with a single row of data.
  prefs: []
  type: TYPE_NORMAL
- en: The `cleansed_data` generator expression will apply the `cleanse()` function
    to each dictionary of raw data. The individual rows are handy `SimpleNamespace`
    data structures that can be used elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: This generator binds all of the individual steps into a transformation pipeline.
    When steps need to be changed, this becomes the focus of the change. We can add
    filters, or replace parsing or cleansing functions here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The context for using the `get_data()` function will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: We'll need to open a source file. We can provide the file to the `get_data()`
    function. This function will emit each row in a form that can easily be used for
    statistical processing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the `correlation()` function from the `ch10_r03` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the relevant time series data item from the source data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we'll use the interpolated data. If we try to use the average
    data, there are reporting gaps that would force us to locate periods without the
    gaps. The interpolated data has values to fill in the gaps.
  prefs: []
  type: TYPE_NORMAL
- en: We've created a `list` object from the generator expression because we'll be
    doing more than one summary operation on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a number of time offsets, T, compute the correlation. We''ll use time offsets
    from `1` to `20` periods. Since the data is collected monthly, we suspect that
    T = 12 will have the highest correlation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The `correlation()` function from the *Computing the coefficient of correlation*
    recipe expects a small dictionary with two keys: `x` and `y` . The first step
    is to build an array of these dictionaries. We''ve used the `zip()` function to
    combine two sequences of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`co2_ppm[:-tau]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`co2_ppm[tau:]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `zip()` function will combine values from each slice of the `data` . The
    first slice starts at the beginning. The second starts `tau` positions into the
    sequence. Generally, the second sequence will be shorter, and the `zip()` function
    will stop processing when the sequence is exhausted.
  prefs: []
  type: TYPE_NORMAL
- en: We've used `co2_ppm[:-tau]` as one of the argument values to the `zip()` function
    to make it perfectly clear that we're skipping some items at the end of the sequence.
    We're skipping the same number of items that are omitted from the beginning of
    the second sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve taken just the first 60 values to compute the autocorrelation with various
    time offset values. The data is provided monthly. We can see a very strong annual
    correlation. We''ve highlighted this row of output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: When the time shift is `12` , the *r[xx]* (12) = .981\. A similarly striking
    autocorrelation is available for almost any subset of the data. This high correlation
    confirms an annual cycle to the data.
  prefs: []
  type: TYPE_NORMAL
- en: The overall dataset contains almost 700 samples spanning over 58 years. It turns
    out that the seasonal variation signal is not as clear over the entire span of
    time. This means that there is another, longer period signal that is drowning
    out the annual variation signal.
  prefs: []
  type: TYPE_NORMAL
- en: The presence of this other signal suggests that something more complex is going
    on. This effect is on a timescale longer than five years. Further analysis is
    required.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the elegant features of Python is the array slicing concept. In the *Slicing
    and dicing a list* recipe in [Chapter 4](text00048.html#page "Chapter 4. Built-in
    Data Structures – list, set, dict") , *Built-in Data Structures – list, set, dict*
    , we looked at the basics of slicing a list. When doing autocorrelation calculations,
    array slicing gives us a wonderful tool for comparing two subsets of the data
    with very little complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The essential elements of the algorithm amounted to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The pairs are built from `A=a zip()` of two slices of the `co2_ppm` sequence.
    These two slices build the expected (`x` ,`y` ) pairs that are used to create
    a temporary object, `data` . Given this `data` object, an existing `correlation()`
    function computed the correlation metric.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can observe the 12-month seasonal cycle repeatedly throughout the dataset
    using a similar array slicing technique. In the example, we used this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code uses the first `60` samples of the available `699` . We could
    begin the slice at various places and use various sizes of the slice to confirm
    that the cycle is present throughout the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a model that shows how the 12 months of data behave. Because
    there''s a repeating cycle, the sine function is the most likely candidate for
    a model. We''d be doing a fit using this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](Image00047.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The mean of the sine function itself is zero, so the *K* factor is the mean
    of a given 12-month period. The function, *f* ( *x* - φ), will convert month numbers
    to proper values in the range -2π ≤ *f* ( *x* - φ) ≤ 2π. A function such as  *f*
    ( *x* ) = 2π(( *x* -6)/12) might be appropriate. Finally, the scaling factor,
    *A* , scales the data to match the minimum and maximum for a given month.
  prefs: []
  type: TYPE_NORMAL
- en: Long-term model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While interesting, this analysis doesn't locate the long-term trend that was
    obscuring the annual oscillation. To locate that trend, it is necessary to reduce
    each 12-month sequence of samples to a single, annual, central value. The median
    or the mean will work well for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a sequence of monthly average values using the following generator
    expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'This generator will build a sequence of dictionaries. Each dictionary has the
    required `x` and `y` items that are used by the regression function. The `x` value
    is a value that is a simple surrogate for the year and month: it''s a number that
    grows from zero to 696\. The `y` value is the average of 12 monthly values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The regression calculation is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows a pronounced line, with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long-term model](Image00048.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The *x* value is a month number offset from the first month in the dataset,
    which is March, 1958\. For example, March of 1968 would have an *x* value of 120\.
    The yearly average CO[2] parts per million would be *y* = 323.1\. The actual average
    for this year was 323.27\. As you can see, these are very similar values.
  prefs: []
  type: TYPE_NORMAL
- en: The *r* ² value for this `correlational` model, which shows how the equation
    fits the data, is 0.98\. This rising slope is the signal, which in the long run
    dominates the seasonal fluctuations.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Computing the coefficient of a correlation* recipe shows the core function
    for the computing correlation between a series of values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Computing regression parameters* recipe shows additional background for
    determining the detailed regression parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confirming that the data is random – the null hypothesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the important statistical questions is framed as the null hypothesis
    and an alternate hypothesis about sets of data. Let''s assume we have two sets
    of data, *S1* and *S2* . We can form two kinds of hypothesis about the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Null** : Any differences are minor random effects and there are no significant
    differences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alternate** : The differences are statistically significant. Generally, the
    likelihood of this is less than 5%.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we evaluate data to see if it's truly random of if there's some meaningful
    variation?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we have a strong background in statistics, we can leverage statistical theory
    to evaluate the standard deviations of samples and determine if there is a significant
    difference between two distributions. If we are weak in statistics, but have a
    strong background in programming, we can do a little coding and achieve similar
    results without the theory.
  prefs: []
  type: TYPE_NORMAL
- en: There are a variety of ways that we can compare sets of data to see if they're
    significantly different or the differences are random variations. In some cases,
    we might be able to provide a detailed simulation of the phenomena. If we use
    Python's built-in random number generator, we'll get data that's essentially the
    same as truly random real-world events. We can compare a simulation against measured
    data to see if they're the same or not.
  prefs: []
  type: TYPE_NORMAL
- en: The simulation technique only works when a simulation is reasonably complete.
    Discrete events in casino gambling, for example, are easy to simulate. Some kinds
    of discrete events in web transactions, such as the items in a shopping cart,
    are easy to simulate. But some phenomena are hard to simulate precisely.
  prefs: []
  type: TYPE_NORMAL
- en: In the cases where we can't do a simulation, we have a number of resampling
    techniques that are available. We can shuffle the data, use bootstrapping, or
    use cross-validation. In these cases, we'll use the data that's available to look
    for random effects.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll compare three subsets of the data in the *Computing an autocorrelation*
    recipe. These are data values from two adjacent years and a third year that is
    widely separated from the other two. Each year has 12 samples, and we can easily
    compute the means of these groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve created three subsets for three of the available years of data. Each
    subset is created with a simple filter that creates a list of values for which
    the year matches a target value. We can compute statistics on these subsets as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The three averages are different. Our hypothesis is that the differences between
    `1959` and `1960` means are just ordinary random variation with no significance.
    The differences between the `1959` and `2014` means, however, are statistically
    significant.
  prefs: []
  type: TYPE_NORMAL
- en: 'The permutation or shuffling technique works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each permutation of the pooled data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The observed difference between the means of `1959` data and `1960` data is
    *316.91-315.97 = 0.94* . We can call this *T[obs]* , the observed test measurement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create two subsets, *A* , and *B*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the difference between the means, *T*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Count the number of differences, *T* , larger than  *T[obs]* and smaller than 
    *T[obs]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The two counts show us how our observed difference compares with all possible
    differences. For largish sets of data, there can be a large number of permutations.
    In our case, we know that the number of combinations of 24 samples taken 12 at
    a time is given by this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](Image00049.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can compute the value for *n* = 24 and *k* = 12:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: There are a hair more than 2.7 million permutations. We can use functions in
    the `itertools` module to generate these. The `combinations()` function will emit
    the various subsets. Processing takes over 5 minutes (320 seconds).
  prefs: []
  type: TYPE_NORMAL
- en: An alternative plan is to use randomized subsets. Using 270,156 randomized samples
    can be done in about 35 seconds. Using just 10% of the combinations provides an
    answer that's accurate enough to determine if the two samples are statistically
    similar and the null hypothesis is true, or if the two samples are different.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll be using the `random` and `statistics` modules. The `shuffle()` function
    is central to randomizing the samples. We''ll also be using the `mean()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We could simply count values above and below the observed difference between
    the samples. Instead, we''ll create a `Counter` and collect differences in 2,000
    steps from -0.001 to +0.001\. This will provide some confidence that the differences
    are normally distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function that accepts two separate sets of samples. These will be
    combined, and random subsets drawn from the collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the observed difference between the means, *T[obs]* :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a `Counter` to collect details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the combined universe of samples. We can concatenate the two lists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Use a `for` statement to do a large number of resamples; 270,415 can take 35
    seconds. It''s easy to expand or contract the subset to balance a need for accuracy
    and the speed of calculation. The bulk of the processing will be nested inside
    this loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Shuffle the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Pick two subsets that match the original sets of data in size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Because of the way Python list indices work, we are assured that the two lists
    completely separate the values in the universe. Since the ending index value,
    `len(s2)` , is not included in the first list, this kind of slice clearly separates
    all items.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the difference between the means. In this case, we''ll scale this by
    `1000` and convert to an integer so that we can accumulate a frequency distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: An alternative to creating a histogram of delta values is to count values above
    and below *T[obs]* . Using the full histogram provides confidence that the data
    is statistically normal.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the `for` loop, we can summarize the `counts` showing how many are above
    the observed difference and how many are below. If either value is less than 5%,
    this is a statistically significant difference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the `randomized()` function for the data from `1959` and `1960`
    , we see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: This shows that 11% of the data was above the observed difference and 88% of
    the data was below. This is well within the realm of normal statistical noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run this for data from `1959` and `2014` , we see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: The data involved only one example out of 270,415 that was above the observed
    difference in means, *T[obs]* . The change from `1959` to `2014` is statistically
    significant, with a probability of 3.7 x 10^(-6) .
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computing all 2.7 million permutations gives the exact answer. It's faster to
    use randomized subsets instead of computing all possible permutations. The Python
    random number generator is excellent, and it assures us that the randomized subsets
    will be fairly distributed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve used two techniques to compute randomized subsets of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle the entire universe with `random.shuffle(u)`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Partition the universe with code similar to `a, b = u[x:], u[:x]`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computing means of the two partitions is done with the `statistics` module.
    We could define somewhat more efficient algorithms which did the shuffling, partitioning,
    and mean computation all in a single pass through the data. This more efficient
    algorithm will omit the creation of a complete histogram for the permuted differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding algorithm turned each difference into a value between -1000 and
    +1000 using this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: This allows us to compute a frequency distribution with a `Counter` . This will
    show that most of the differences really are zero; something to be expected for
    normally distributed data. Seeing the distribution assures us that there isn't
    some hidden bias in the random number generation and shuffling algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of populating a `Counter` , we can simply count the above and below
    values. The simplest form of this comparison between a permutation''s difference
    and the observed difference, *T[obs]* , is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: This counts the number of resampling differences that are larger than the observed
    difference. From this, we can compute the number below the observation via `below
    = limit-above` . This will give us a simple percentage value.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can speed processing up a tiny bit more by changing the way we compute the
    mean of each random subset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a pool of numbers, *P* , we''re creating two disjoint subsets, *A* ,
    and *B* , such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A* ∪ *B* = *P* ∧ *A* ∩ *B* = ∅'
  prefs: []
  type: TYPE_NORMAL
- en: The union of the *A* and *B* subsets covers the entire universe, *P* . There
    are no missing values because the intersection between *A* and *B* is an empty
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall sum, *S[p]* , can be computed just once:'
  prefs: []
  type: TYPE_NORMAL
- en: '*S[P]* = ∑ *P*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We only need to compute a sum for one subset, *S[A]* :'
  prefs: []
  type: TYPE_NORMAL
- en: '*S[A] = ∑ A*'
  prefs: []
  type: TYPE_NORMAL
- en: This means that the other subset sum is simply a subtraction. We don't need
    a costly process to compute a second sum.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sizes of the sets, *N[A]* , and *N[B]* , similarly, are constant. The means,
    μ [*A*] and μ [*B*] , can be calculated quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: μ [*A*] = ( *S[A]* / *N[A]* )
  prefs: []
  type: TYPE_NORMAL
- en: μ [*B*] = ( *S[P]* - *S[A]* )/ *N[B]*
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to a slight change in the resample loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: By computing just one sum, `s_a` , we shave processing time off of the random
    resampling procedure. We don't need to compute the sum of the other subset, since
    we can compute this as a difference between the sum of the entire universe of
    values. We can then avoid using the `mean()` function, and compute the means directly
    from the sums and the fixed counts.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of optimization makes it quite easy to reach a statistical decision
    quickly. Using resampling means that we don't need to rely on a complex theoretical
    knowledge of statistics; we can resample the existing data to show that a given
    sample meets the null hypothesis or is outside of the expectations, and some alternative
    hypothesis is called for.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This process can be applied to other statistical decision procedures. This includes
    the *Computing regression parameters* and *Computing an autocorrelation* recipes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locating outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we have statistical data, we often find data points which can be described
    as outliers. An outlier deviates from other samples, and may indicate bad data
    or a new discovery. Outliers are, by definition, rare events.
  prefs: []
  type: TYPE_NORMAL
- en: Outliers may be simple mistakes in data gathering. They might represent a software
    bug, or perhaps a measuring device that isn't calibrated properly. Perhaps a log
    entry is unreadable because a server crashed or a timestamp is wrong because a
    user entered data improperly.
  prefs: []
  type: TYPE_NORMAL
- en: Outliers may also be of interest because there is some other signal that is
    difficult to detect. It might be novel, or rare, or outside the accurate calibration
    of our devices. In a web log it might suggest a new use case for an application
    or signal the start of a new kind of hacking attempt.
  prefs: []
  type: TYPE_NORMAL
- en: How do we locate and label potential outliers?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An easy way to locate outliers is to normalize the values to make them Z-scores.
    A Z- score converts the measured value to a ratio between the measured value and
    the mean measured in units of standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Z[i]* = ( *x[i]* - μ [*x*] )/σ [*x*]'
  prefs: []
  type: TYPE_NORMAL
- en: Where μ [*x*] is the mean of a given variable, *x* , and σ [*x*] is the standard
    deviation of that variable. We can compute these values using the `statistics`
    module.
  prefs: []
  type: TYPE_NORMAL
- en: 'This, however, can be somewhat misleading because the Z-scores are limited
    by the number of samples involved. Consequently, the *NIST Engineering and Statistics
    Handbook* , *section 1.3.5.17* , suggests using the following rule for detecting
    outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](Image00050.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**MAD** ( **Median Absolute Deviation** ) is used instead of the standard deviation.
    The MAD is the median of the absolute values of the deviations between each sample,
    *x[i]* , and the population median, *x* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](Image00051.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The scaling factor of *0.6745* is used to scale these scores so that a *M[i]*
    value greater than 3.5 can be identified as an outlier. Note that this is parallel
    to the calculation of the sample variance. The variance measure uses a mean, this
    measure uses a median. The value, 0.6745, is widely-used in the literature as
    the appropriate value to locate outliers.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use some data from the *Using the built-in statistics library* recipe
    that includes some relatively smooth datasets and some datasets that have egregious
    outliers. The data is in a JSON document that has four series of ( *x* , *y* )
    pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can read this data with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: We've defined the `Path` to the data file. We can then use the `Path` object
    to read the text from this file. This text is used by `json.loads()` to build
    a Python object from the JSON data.
  prefs: []
  type: TYPE_NORMAL
- en: We've included an `object_pairs_hook` so that this function will build the JSON
    using the `OrderedDict` class instead of the default `dict` class. This will preserve
    the original order of items in the source document.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine the data such as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'The overall JSON document is a sequence of subdocuments with keys such as `I`
     and `II` . Each subdocument has two fields: `series`  and `data` . The `data`
     value is a list of observations that we want to characterize. Each observation
    is a pairs measurements.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Import the `statistics` module. We'll be doing a number of median calculations.
    In addition, we can use some of the features of `itertools` , such as `compress()`
    and `filterfalse()` .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `absdev()` mapping. This will either use a given median or compute
    the actual median of the samples. It will then return a generator that provides
    all of the absolute deviations from the median:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `median_absdev()` reduction. This will locate the median of a sequence
    of absolute deviation values. This computes the MAD value used to detect outliers.
    This can compute a median or it can be given a median already computed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the modified Z-score mapping, `z_mod()` . This will compute the median
    for the dataset, and use this to compute the MAD. The deviation value is then
    used to compute modified Z-scores based on this deviation value. The returned
    value is an iterator over the modified Z-scores. Because multiple passes are made
    over the data, the input can''t be an iterable collection, so it must be a sequence
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: In this implementation, we've used a constant, `0.6745` . In some vases, we
    might want to make this a parameter. We might use `def z_mod(data, threshold=0.6745)`
    to allow changing this value.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, there's a possibility that the MAD value is zero. This can happen
    when the majority of the values don't deviate from the median. When more than
    half of the points have the same value, the median absolute deviation will be
    zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the outlier filter based on the modified Z mapping, `z_mod()` . Any
    value over 3.5 can be labeled as an outlier. The statistical summaries can then
    be computed with and without the outlier values. The `itertools` module has a
    `compress()` function which can use a sequence of Boolean selector values to choose
    items from the original data sequence based on the results of the `z_mod()` computation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: The `pass_outliers()` function passes only the outliers. The `reject_outliers()`
    function passes the non-outlier values. Often, we'll display two results—the whole
    set of data, and the whole set with outliers rejected.
  prefs: []
  type: TYPE_NORMAL
- en: Most of these functions make multiple references to the input data parameter—an
    iterable cannot be used. These functions must be given a `Sequence` object. A
    `list` or a `tuple` are examples of a `Sequence` .
  prefs: []
  type: TYPE_NORMAL
- en: We can use the `pass_outliers()` to locate the outlier values. This can be handy
    to identify the suspicious data values. We can use the `reject_outliers()` to
    provide data with the outliers removed from consideration.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The stack of transformations can be summarized like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce the population to compute a population median.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Map each value to an absolute deviation from the population median.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduce the absolute deviations to create a median absolute deviation, MAD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Map each value to the modified Z-score using the population median and the MAD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filter the results based on the modified Z-scores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We've defined each transformation function in this stack separately. We can
    use recipes from [Chapter 8](text00088.html#page "Chapter 8. Functional and Reactive
    Programming Features") , *Functional and Reactive Programming Features* , to create
    smaller functions and use the built-in `map()` and `filter()` functions to implement
    this process.
  prefs: []
  type: TYPE_NORMAL
- en: We can't easily use the built-in `reduce()` function to define a median computation.
    To compute a median, we have to use a recursive median finding algorithm that
    partitions the data into smaller and smaller subsets, one of which has the median
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how we can apply this to the given sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: We've iterated through each of the series in the source data. The computation
    of `series_data` extracts one of the series from the source data. Each of the
    series has two variables, `x` and `y` . Within the set of samples, we can use
    the `pass_outliers()` function to locate outliers in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `except` clause handles a `ZeroDivisionError` exception. This exception
    is raised by the `z_mod()` function for a particularly pathological set of data.
    Here''s the line of output that shows this odd-looking data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: In this case, at least half the values are the same. That single majority value
    will be taken as the median. The absolute deviations from the median will be zero
    for this subset. Consequently, the MAD will be zero. In this case, the idea of
    outliers is suspicious because the data don't seem to reflect ordinary statistical
    noise, either.
  prefs: []
  type: TYPE_NORMAL
- en: This data does not fit the general model, and a different kind of analysis must
    be applied to this variable. The very idea of outliers may have to be rejected
    because of the peculiar nature of the data.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've used `itertools.compress()` to pass or reject outliers. We can also use
    the `filter()` and `itertools.filterfalse()` functions in a similar way. We'll
    look at some optimizations of `compress()` as well as ways to use `filter()` instead
    of `compress()` .
  prefs: []
  type: TYPE_NORMAL
- en: 'We used two similar looking function definitions to `pass_outliers` and `reject_outliers`
    . This design suffers from an unpleasant duplication of critical program logic;
    it breaks the DRY principle. Here are the two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: The difference between `pass_outliers()` and `reject_outliers()` is tiny, and
    amounts to a logical negation of an expression. We have `>=` in one version and
    `<` in another. This kind of code difference is is not always trivial to validate.
    If the logic was more complex, performing the logical negation is a place where
    a design error can creep into the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can extract one version of the filter rule to create something like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then modify the two uses of the `compress()` function to make the logical
    negation explicit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: Exposing the filter rule as a separate lambda object or function definition
    helps reduce the code duplication. The negation is made more obvious. Now the
    two versions can be compared easily to be sure that they have appropriate semantics.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to use the `filter()` function, we have to make a radical transformation
    to the processing pipeline. The `filter()` higher-order function requires a decision
    function that creates a true/false result for each raw value. Processing this
    will combine a modified Z-score calculation with the decision threshold. The decision
    function must compute this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](Image00052.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It must compute this in order to determine the outlier status for each *x[i]*
    value. This decision function requires two additional inputs—the population median,
    ![There''s more...](Image00053.jpg)  , and the MAD value. This makes the filter
    decision function rather complex. It would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: This `outlier()` function can be used with the `filter()` to pass outliers.
    It can be used with `itertools.filterfalse()` to reject outliers and create a
    subset that is free from erroneous values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use this `outlier()` function, we''ll need to create a function
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'This computes the two overall reductions: `population_median` , and `mad` .
    Given these two values, we can create a partial function, `outlier_partial()`
    . This function has values bound for the the first two positional parameter values,
    `mad` , and `population_median` . The resulting partial function requires only
    the individual data value for processing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `outlier_partial()` and `filter()` processing are equivalent to this generator
    expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: It's not clear that this expression has a distinct advantage over using the
    `compress()` function in the `itertools` module. It can, however, be somewhat
    more clear for programmers who are more comfortable with `filter()` .
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: See [http://www.itl.nist.gov/div898/handbook/eda/section3/eda35h.htm](http://www.itl.nist.gov/div898/handbook/eda/section3/eda35h.htm)
    for detection of outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing many variables in one pass
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many cases, we'll have data with multiple variables that we'd like to analyze.
    The data can be visualized as filling in a grid, with each row containing a specific
    outcome. Each outcome row has multiple variables in columns.
  prefs: []
  type: TYPE_NORMAL
- en: We can follow the pattern of column-major order and process each variable (from
    a column of data) independently. This will lead to visiting each row of data multiple
    times. Or, we can use the pattern of row-major order and process all the variables
    at once for each row of data.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of a focus on each variable is that we can write a relatively
    simple processing stack. We'll have multiple stacks, one per variable, but each
    stack can reuse common functions from the `statistics` module.
  prefs: []
  type: TYPE_NORMAL
- en: The disadvantage of this kind of focus is that processing each variable for
    a very large dataset requires reading the raw data from OS files. This part of
    the process can be the most time-consuming. Indeed, the time required to read
    the data often dominates the time required to do statistical analyses. The I/O
    cost is so high that specialized systems such as Hadoop have been invented to
    try and speed up access to extremely large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: How can we make one pass through a set of data and collect a number of descriptive
    statistics?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The variables that we might want to analyze will fall into a number of categories.
    For example, statisticians often segregate variables into categories such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous real-valued data** : These variables are often measured by floating-point
    values, they have a well defined unit of measure, and they can take on values
    with a precision limited by the accuracy of the measurement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discrete or categorical data** : These variables take on a value selected
    from a finite domain. In some cases, we can enumerate the domain in advance. In
    other cases, the domain''s values must be discovered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ordinal data** : This provides a ranking or ordering. Generally, the ordinal
    value is a number, but no other statistical summaries apply to this number, since
    it''s not really a measurement; it has no units.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Count data** : This variable is a summary of individual discrete outcomes.
    It can be treated as if it were continuous by computing a real-valued mean of
    an otherwise discrete count.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variables may be independent of each other or they may depend on other variables.
    In the initial phases of a study, the dependence may not be known. In later phases,
    one objective of the software is to discover the dependencies. Later, software
    may be used to model the dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the varieties of data, we need to treat each variable as a distinct
    item. We can't treat them all as simple floating-point values. Properly acknowledging
    the differences will lead to a hierarchy of class definitions. Each subclass will
    contain the unique features for a variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two overall design patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Eager** : We can compute the various summaries as early as possible. In some
    cases, we don''t have to accumulate very much data for this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lazy** : We compute the summaries as late as possible. This means we''ll
    be accumulating data, and using properties to compute the summaries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For very large sets of data, we want to have a hybrid solution. We'll compute
    some summaries eagerly, and also use properties to compute the final results from
    those summaries.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use some data from the *Using the built-in statistics library* recipe
    that includes just two variables in a number of similar data series. The variables
    are named *x* and *y* and are both real-valued variables. The *y* variable should
    depend on the *x* variable, so correlation and regression models apply there.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can read this data with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: We've defined the `Path` to the data file. We can then use the `Path` object
    to read the text from this file. This text is used by `json.loads()` to build
    a Python object from the JSON data.
  prefs: []
  type: TYPE_NORMAL
- en: We've included an `object_pairs_hook` so that this function will build the JSON
    using the `OrderedDict` class instead of the default `dict` class. This will preserve
    the original order of items in the source document.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine the data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'The overall JSON document is a sequence of subdocuments with keys such as `''I''`
    . Each subdocument has two fields: `"series"` and `"data"` . Within the `"data"`
    array there''s a list of observations that we want to characterize. Each observation
    has a pair of values.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Define a class to handle the analysis of the variable. This should handle all
    conversions and cleansing. We''ll use the hybrid process approach: we''ll update
    the sums and counts as each data element arrives. We won''t compute the final
    mean or standard deviation until these attributes are requested:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we've defined summaries for `count` , `sum` , and sum of squares.
    We can extend this class to add more computations. For the median or mode, we
    will have to accumulate the individual values, and change the design to be entirely
    lazy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define instances to handle the input columns. We''ll create two instances of
    our `SimpleStats` class. In this recipe, we''ve chosen two variables that are
    so much alike that a single class covers both cases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a mapping from actual column titles to the statistics-computing objects.
    In some cases, the columns may not be identified by names: we might be using column
    indexes. In this case, a sequence of objects will match the sequence of columns
    in each row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to process all rows, using the statistics-computing objects
    for each column within each row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: The outer `for` statement processes each row of data. The inner `for` statement
    processes each column of each row. The processing is clearly in row-major order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Display results or summaries from the various objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: We can apply the analysis function to a series of data values. This will return
    the dictionary that has the statistical summaries.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've created a class that handles cleansing, filtering, and statistics processing
    for a specific kind of column. When confronted with various kinds of columns,
    we'll need multiple class definitions. The idea is to be able to easily create
    a hierarchy of related classes.
  prefs: []
  type: TYPE_NORMAL
- en: We create an instance of this class for each specific column that we're going
    to analyze. In this example, the `SimpleStats` was designed for a column of simple
    floating-point values. Other designs would be appropriate for discrete or ordinal
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The externally-facing features of this class are an `add()` method. Each individual
    data value is provided to this method. The `mean` and `stdev` properties compute
    summary statistics.
  prefs: []
  type: TYPE_NORMAL
- en: The class also defines a `cleanse()` method that handles the data conversion
    needs. This can be extended to handle the possibility of invalid data. It might
    filter the values instead of raising an exception. This method must be overridden
    to handle more complex data conversions.
  prefs: []
  type: TYPE_NORMAL
- en: We've created a collection of individual statistics-processing objects. In this
    example, both items in the collection are instances of `SimpleStats` . In most
    cases, there will be multiple classes involved, and the collection of statistics
    processing objects might be rather complex.
  prefs: []
  type: TYPE_NORMAL
- en: This collection of `SimpleStats` objects is applied to each row of data. A `for`
    statement uses the keys of the mapping, which are also column names to associate
    each column's data with the appropriate statistics-processing object.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the statistical summaries must be computed lazily. To spot outliers,
    for instance, we need all of the data. One common approach to locating outliers
    required computing a median, computing the absolute deviations from the median,
    and then a median of these absolute deviations. See the *Locating outliers* recipe.
    To compute the mode, we would accumulate all of the data values into a `Counter`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this design, we've tacitly assumed that all columns are completely independent.
    In some cases, we'll need to combine columns to derive additional data items.
    This will lead to a more complex class definition that may include a reference
    to other instances of `SimpleStats` . This can become rather involved to be sure
    that columns are handled in dependency order.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the  *Using stacked generator expressions* recipe in [Chapter 8](text00088.html#page
    "Chapter 8. Functional and Reactive Programming Features") , *Functional And Reactive
    Programming Features* , we may have a multistage processing that involves enrichment
    and computing derived values. This further constrains the ordering among the column
    processing rules. One way to handle this is to provide each analyzer with a reference
    to the relevant other analyzers. We might have something like the following rather
    complex set of class definitions.
  prefs: []
  type: TYPE_NORMAL
- en: First, we'll define two classes to handle date columns and time columns in isolation.
    Then we'll combine these to create a timestamp column based on the two input columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the class to handle a date column in isolation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'The `DateStats` class only implements the `add()` method. It cleanses the data
    and retains a current value. We can define something similar for processing the
    time column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: The `TimeStats` class is similar to `DateStats` ; it only implements the `add()`
    method. Both classes focus on cleaning the source data and retaining the current
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a class that depends on the results of the previous two classes. This
    will use the `current` attribute of the `DateStats` and `TimeStats` objects to
    get the currently available value from each of these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: The `DateTimeStats` class combines the results of two objects. It requires an
    instance of the `DateStats` class and an instance of the `TimeStats` class. From
    these other two objects, the current cleansed value is available as the `current`
    attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `value` parameter is not used for the `DateTimeStats` implementation
    of the `add()` method. Instead of accepting `value` as an argument, a value is
    collected from the two other cleansing objects. This requires that the other two
    columns were processed before this derived column is processed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to be sure that the values are available, some additional processing
    is required for each row. The basic date and time processing maps to specific
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: This `column_stats` mapping can be used to apply two foundational data cleansing
    operations against each row of data. However, we also have derived data that must
    be computed after the foundational data is done.
  prefs: []
  type: TYPE_NORMAL
- en: 'We might have something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve built an instance of `DateTimeStats` that depends on two other statistical
    process objects: `date_stats` and `time_stats` . The `add()` method of this object
    will fetch the current values from each of the other two objects. If we had other
    derived columns, we could collect them into this mapping.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This `derived_stats` mapping can be used to apply statistical processing operations
    to create and analyze derived data. The overall processing loop now has two phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: We've computed statistics for the columns that are present in the source data.
    Then we computed statistics for the derived columns. This has the pleasant feature
    of being configured using just two mappings. We can change the classes that are
    used by updating the `column_stats` and `derived_stats` mappings.
  prefs: []
  type: TYPE_NORMAL
- en: Using map()
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We've used explicit `for` statements to apply each statistics object to the
    appropriate column data. We can also use a generator expression. We can even try
    to use the `map()` function. See the *Combining map and reduce transformations*
    recipe in [Chapter 8](text00088.html#page "Chapter 8. Functional and Reactive
    Programming Features") , *Functional and Reactive Programming Features* , for
    some additional background on this technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative data gathering collection could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: Instead of the object, we've provided a function that applies the object's `add()`
    method to the given data value.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this collection, we can use a generator expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: This will apply the `data_gathering[k]` function to each value, `k` , that's
    available in the row.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: See the *Designing classes with lots of processing* and *Using properties for
    lazy attributes* recipes in [Chapter 6](text00070.html#page "Chapter 6. Basics
    of Classes and Objects") , *Basics of Classes and Objects,* for some additional
    design alternatives that fit into this overall approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
