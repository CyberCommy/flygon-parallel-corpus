- en: Chapter 5. Real-time Graph Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce you to graph analysis using Storm to persist
    data to a graph database and query that data to discover relationships. Graph
    databases are databases that store data as graph structures with vertices, edges,
    and properties, and focus primarily on relationships between the entities.
  prefs: []
  type: TYPE_NORMAL
- en: With the advent of social media sites such as Twitter, Facebook, and LinkedIn,
    social graphs have become ubiquitous. Analyzing relationships between people,
    the products they buy, the recommendations they make, and even the words they
    use can be analyzed to reveal patterns that would be difficult with traditional
    data models. For example, when LinkedIn shows that you are four steps away from
    another person based on your network, when Twitter offers suggestions of people
    to follow, or when Amazon suggests products you may be interested in, they are
    leveraging what they know about your relationship graph. Graph databases are designed
    for this type of relationship analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will build an application that ingests a subset of the
    Twitter firehose (the real-time feed of all tweets made by Twitter users) and
    based on the content of each message, creates nodes (vertices) and relationships
    (edges) in a graph database that we can then analyze. The most obvious graph structure
    within Twitter is based on the follows / followed by relationship between users,
    but we can infer additional relationships by looking beyond these explicit relationships.
    By looking at the content of messages, we can use message metadata (hashtags,
    user mentions, and so on) to identify, for example, users who mention the same
    subjects or tweet related hashtags. In this chapter, we will cover the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic graph database concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TinkerPop graph APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph data modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interacting with the Titan-distributed graph database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a Trident state implementation backed by a graph database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Today''s social media websites capture a wealth of information. Many social
    media services such as Twitter, Facebook, and LinkedIn are based largely on relationships:
    who you follow, are friends with, or have a business connection to. Beyond the
    obvious and explicit relationships, social media interactions also create a persistent
    set of implicit connections that can be easily taken for granted. With Twitter,
    for example, the obvious relationships consist of those one follows and who one
    is followed by. The less obvious relationships are the connections created, perhaps
    unknowingly, just by using the service. Have you directly messaged someone on
    Twitter? If yes, then you''ve formed a connection. Tweeted a URL? If yes, again
    a connection. Liked a product, service, or comment on Facebook? Connection. Even
    the act of using a specific word or phrase in a tweet or post can be thought of
    as creating a connection. By using that word, you are forming a connection with
    it, and by using it repeatedly, you are strengthening that connection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at data as "everything is a connection," then we can build a structured
    dataset and analyze it to expose broader patterns. If Bob does not know Alice,
    but both Bob and Alice have tweeted the same URL, we can infer a connection from
    this fact. As our dataset grows, its value will also grow as the number of connections
    in the network increases (similar to Metcalfe''s law: [http://en.wikipedia.org/wiki/Metcalfe''s_law](http://en.wikipedia.org/wiki/Metcalfe''s_law)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we begin querying our dataset, the value for storing data in a graph database
    will quickly become evident as we glean patterns from the growing network of connections.
    The graph analysis we perform is applicable to a number of real-world use cases
    that include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Targeted advertising
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation engines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The architecture for our application is relatively simple. We will create a
    Twitter client application that reads a subset of the Twitter firehose and writes
    each message to a Kafka queue as a JSON data structure. We'll then use the Kafka
    spout to feed that data into our storm topology. Finally, our storm topology will
    analyze the incoming messages and populate the graph database.
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture](img/8294_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Twitter client
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Twitter provides a comprehensive RESTful API that in addition to a typical request-response
    interface also provides a streaming API that supports long-lived connections.
    The Twitter4J Java library ([http://twitter4j.org/](http://twitter4j.org/)) offers
    full compatibility with the latest version of the Twitter API and takes care of
    all the low-level details (connection management, OAuth authentication, and JSON
    parsing) with a clean Java API. We will use Twitter4J to connect to the Twitter-streaming
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka spout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, we developed a Logback Appender extension that allowed
    us to easily publish data to a Kafka queue, and we used Nathan Marz's Kafka spout
    ([https://github.com/nathanmarz/storm-contrib](https://github.com/nathanmarz/storm-contrib))
    to consume the data in a Storm topology. While it would be easy enough to write
    a Storm spout using Twitter4J and the Twitter streaming API, using Kafka and the
    Kafka Spout gives us transactional, exactly-once semantics, and built-in fault
    tolerance that we would otherwise have to implement ourselves. For more information
    on installing and running Kafka refer to [Chapter 4](ch04.html "Chapter 4. Real-time
    Trend Analysis"), *Real-time Trend Analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: A titan-distributed graph database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Titan is a distributed graph database optimized for storing and querying graph
    structures. Like Storm and Kafka, Titan databases can run as a cluster and can
    scale horizontally to accommodate increasing data volume and user load. Titan
    stores its data in one of the three configurable storage backends: Apache Cassandra,
    Apache HBase, and Oracle Berkely Database. The choice of storage backend depends
    on which two properties of the CAP theorem are desired. In respect to a database,
    the CAP theorem stipulates that a distributed system cannot simultaneously make
    all of the following guarantees:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency**: All clients see the current data regardless of modifications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability**: The system continues to operate as expected despite node
    failures'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition Tolerance**: The system continues to operate as expected despite
    network or message failure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A titan-distributed graph database](img/8294_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For our use case, consistency is not critical to our application. We are far
    more concerned with scalability and fault tolerance. If we look at the CAP theorem
    triangle, shown in the preceding diagram, it becomes clear that Cassandra is the
    storage backend of choice.
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to graph databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A graph is a network of objects (vertices) with directed connections (edges)
    between them. The following diagram illustrates a simple social graph similar
    to what one might find on Twitter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A brief introduction to graph databases](img/8294_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this example, users are represented by vertices (nodes), and relationships
    are expressed as edges (connection). Note that the edges in the graph are directed,
    allowing an additional degree of expressiveness. This allows, for example, to
    express the fact that Bob and Alice follow one another, and Alice follows Ted
    but Ted does not follow Alice. This relationship would be more cumbersome to model
    without directed edges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many graph databases follow a property graph model. A property graph extends
    the basic graph model by allowing a set of properties (key-value pairs) to be
    assigned to vertices and edges as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A brief introduction to graph databases](img/8294_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The ability to associate property metadata to objects and relationships in a
    graph model provides powerful support metadata for graph algorithms and queries.
    For example, adding the **since** property to the **Follows** edge would enable
    us to efficiently query for all the users who started following a particular user
    in a given year.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to relational databases, relationships in a graph database are explicit
    as opposed to implicit. Relationships in a graph database are full-blown data
    structures rather than implied connections (that is, foreign keys). Under the
    hood, graph databases' underlying data structures are heavily optimized for graph
    traversal. While it is entirely possible to model a graph in a relational database,
    it is often less efficient than a graph-centric model. In a relational data model,
    traversing a graph structure can be computationally expensive as it involves joining
    many tables. In a graph database, it is a more natural process of traversing links
    between nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the graph – the TinkerPop stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TinkerPop is a group of open source projects focused on graph technologies such
    as database access, data flow, and graph traversal. Blueprints, the foundation
    of the TinkerPop stack, is a generic Java API for interacting with property graphs
    in much the same way JDBC provides a generic interface to relational databases.
    Other projects in the stack add additional functionalities on top of that foundation
    so that they can be used with any graph database that implements the Blueprints
    API.
  prefs: []
  type: TYPE_NORMAL
- en: '![Accessing the graph – the TinkerPop stack](img/8294_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The components of the TinkerPop stack include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Blueprints**: Graph API Blueprints is a collection of interfaces that provide
    access to a property graph data model. Implementations are available for graph
    databases including Titan, Neo4J, MongoDB, and many others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipes**: Dataflow Processing Pipes is a dataflow framework for defining and
    connecting various data operations as a process graph. Manipulating data with
    Pipes'' primitives closely resembles data processing in Storm. Pipes dataflow
    are **directed acyclic graphs** (**DAG**), much like a Storm topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gremlin**: Gremlin is a graph traversal language. It is a Java-based **domain
    specific language** (**DSL**) for graph traversal, query, analysis, and manipulation.
    The Gremlin distribution comes with a Groovy-based shell that allows the use of
    interactive analysis and modification of a Blueprints graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frames**: Frames is an object-to-graph mapping framework analogous to an
    ORM but tailored for graphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Furnace**: The Furnace project aims to provide implementations of many common
    graph algorithms for Blueprints property graphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rexster**: Rexster is a graph server that exposes Blueprints graphs through
    a REST API, as well as a binary protocol.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For our purposes, we will be focusing on the Blueprints API for populating a
    graph from a Storm topology and Gremlin for graph queries and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating the graph with the Blueprints API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Blueprints API is very straightforward. The following code listing uses
    the Blueprints API to create the graph depicted in the previous diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first line of code instantiates an implementation of the `com.tinkerpop.blueprints.Graph`
    interface. In this case, we're creating an in-memory, toy graph (`com.tinkerpop.blueprints.impls.tg.TinkerGraph`)
    for exploration. Later, we will demonstrate how to connect to a distributed graph
    database.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may be wondering why we are passing `null` as a parameter to the `addVertex()`
    and `addEdge()` methods at the first argument. This argument is essentially a
    suggestion to the underlying Blueprints implementation for a unique ID for the
    object. Passing in `null` as the ID simply has the effect of letting the underlying
    implementation assign an ID to the new object.
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating the graph with the Gremlin shell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Gremlin is a high-level Java API built on the top of the Pipes and Blueprints
    APIs. In addition to the Java API, Gremlin also includes a Groovy-based API and
    ships with an interactive shell (or REPL) that allows you to directly interact
    with a Blueprints graph. The Gremlin shell allows you to create and/or connect
    to the shell and query virtually any Blueprints graph. The following code listing
    illustrates the process of executing the Gremlin shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to querying a graph, it is also easy to create and manipulate graphs
    using Gremlin. The following code listing consists of Gremlin Groovy code that
    will create the same graph illustrated in the previous diagram and is the Groovy
    equivalent of the Java code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You will learn more about using the Gremlin API and DSL later in the chapter
    once we've built a topology to populate a graph and are ready to analyze the graph
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Software installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The application we're building will utilize Apache Kafka and its dependencies
    (Apache ZooKeeper). If you haven't done so already, set up ZooKeeper and Kafka
    according to the instructions in the *ZooKeeper installation* section in [Chapter
    2](ch02.html "Chapter 2. Configuring Storm Clusters"), *Configuring Storm Clusters*,
    and the *Installing Kafka* section in [Chapter 4](ch04.html "Chapter 4. Real-time
    Trend Analysis"), *Real-time Trend Analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: Titan installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To install Titan, download the Titan 0.3.x complete package from Titan''s downloads
    page ([https://github.com/thinkaurelius/titan/wiki/Downloads](https://github.com/thinkaurelius/titan/wiki/Downloads)),
    and extract it to a convenient location by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Titan''s complete distribution package includes everything that is necessary
    for running Titan with any of the supported storage backends: Cassandra, HBase,
    and BerkelyDB. There are also backend-specific distributions if you are only interested
    in using a specific storage backend.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Both Storm and Titan use the Kryo ([https://code.google.com/p/kryo/](https://code.google.com/p/kryo/))
    library for Java object serialization. At the time of writing, Storm and Titan
    use different versions of the Kryo library, which will cause problems when the
    two are used in conjunction.
  prefs: []
  type: TYPE_NORMAL
- en: 'To patch Titan in order to properly enable serialization between Storm and
    Titan, replace the `kryo.jar` file in the Titan distribution with the `kryo.jar`
    file that comes with Storm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, you can test the installation by running the Gremlin shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`GraphOfTheGodsFactory` is a class included with Titan that will create and
    populate a Titan database with a sample graph that represents the relationships
    between the characters and places in the Roman pantheon. Passing a directory path
    to the `create()` method will return a Blueprints graph implementation, specifically
    a `com.thinkaurelius.titan.graphdb.database.StandardTitanGraph` instance that
    uses a combination of BerkelyDB and Elasticsearch for a storage backend. Since
    the Gremlin shell is a Groovy REPL, we can easily verify this by looking at the
    class of the `g` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Setting up Titan to use the Cassandra storage backend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've seen that Titan supports different storage backends. Exploring all three
    options is beyond the scope of this chapter (you can learn more about Titan and
    its configuration options at [http://thinkaurelius.github.io/titan/](http://thinkaurelius.github.io/titan/)),
    so we will focus on using the Cassandra ([http://cassandra.apache.org](http://cassandra.apache.org))
    storage backend.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Cassandra
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to download and run Cassandra, we need to execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The default file that comes with the Cassandra distribution will create a single-node
    Cassandra database running locally. If there is an error during the startup, you
    may need to configure Cassandra by editing the `${CASSANDRA_HOME}/conf/cassandra.yaml`
    and/or `${CASSANDRA_HOME}/conf/log4j-server.properties` files. The most common
    problems are usually related to the lack of file-write permissions on `/var/lib/cassandra`
    (where, by default, Cassandra stores its data) and `/var/log/cassandra` (the default
    Cassandra log location).
  prefs: []
  type: TYPE_NORMAL
- en: Starting Titan with the Cassandra backend
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run Titan with Cassandra, we need to configure it to connect to our Cassandra
    server. Create a new file called `storm-blueprints-cassandra.yaml` with the following
    contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can probably surmise, this configures Titan to connect to the Cassandra
    instance running locally.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this project, we may not need to actually run the Titan server. Since we're
    using Cassandra, Storm and Gremlin should be able to share the backend without
    any issues.
  prefs: []
  type: TYPE_NORMAL
- en: With the Titan backend configured, we are ready to create our data model.
  prefs: []
  type: TYPE_NORMAL
- en: Graph data model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The primary entity in our data model is a Twitter user. A Twitter user can
    perform the following relationship-forming actions when posting a tweet:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a word
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mention a hashtag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mention another user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mention a URL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retweet another user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Graph data model](img/8294_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This concept maps very naturally into a graph model. In the model, we will
    have four different entity types (vertices):'
  prefs: []
  type: TYPE_NORMAL
- en: '**User**: This represents a Twitter user account'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Word**: This represents any word contained in a tweet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**URL**: This represents any URL contained in a tweet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hashtag**: This represents any hashtag contained in a tweet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Relationships (edges) will consist of the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**mentions_user**: Using this action, a user mentions another user'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**retweets_user**: Using this action, a user retweets another user''s post'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**follows_user**: Using this action, a user follows another user'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mentions_hashtag**: Using this action, a user mentions a hashtag'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**uses_word**: Using this action, the user uses a specific word in a tweet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mentions_url**: Using this action, a user tweets a specific URL'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The user vertex models a user''s Twitter account information, which is shown
    in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| User [vertex] |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| type | String | `"user"` |'
  prefs: []
  type: TYPE_TB
- en: '| user | String | Twitter screen name |'
  prefs: []
  type: TYPE_TB
- en: '| name | String | Twitter name |'
  prefs: []
  type: TYPE_TB
- en: '| location | String | Twitter location |'
  prefs: []
  type: TYPE_TB
- en: 'The URL vertex provides a reference point for unique URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| URL [vertex] |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| type | String | `"url"` |'
  prefs: []
  type: TYPE_TB
- en: '| value | String | URL |'
  prefs: []
  type: TYPE_TB
- en: 'The hashtag vertex allows us to store unique hashtags:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hashtag [vertex] |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| type | String | `"hashtag"` |'
  prefs: []
  type: TYPE_TB
- en: '| value | String |   |'
  prefs: []
  type: TYPE_TB
- en: 'We store individual words in the word vertex:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Word [vertex] |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| type | String | `"word"` |'
  prefs: []
  type: TYPE_TB
- en: '| value | String |   |'
  prefs: []
  type: TYPE_TB
- en: 'The `mentions_user` edge is used for relationships between user objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '| mentions_user [edge] |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| user | String | The ID of the user mentioned |'
  prefs: []
  type: TYPE_TB
- en: 'The `mentions_url` edge represents a relationship between the User and URL
    objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '| mentions_url [edge] |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| user | String | The ID of the user mentioned |'
  prefs: []
  type: TYPE_TB
- en: Connecting to the Twitter stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to connect to the Twitter API, we must first generate a set of OAuth
    tokens that will enable our application to authenticate with Twitter. This is
    done by creating a Twitter application that is associated with your account and
    then authorizing that application to access your account. If you do not already
    have a Twitter account, create one now and log in to it. Once you are logged in
    to Twitter, generate the OAuth tokens by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://dev.twitter.com/apps/new](https://dev.twitter.com/apps/new) and
    log in if necessary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a name and description for your application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a URL for your application. In our case, the URL is unimportant since
    we're not creating an app that will be distributed like a mobile app. Entering
    a placeholder URL here is fine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Submit the form. The next page will display the details of the OAuth settings
    for your application. Note the **Consumer key** and **Consumer secret** values
    since we will need those for our application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the bottom of the page, click on the **Create my access token** button. This
    will generate an OAuth Access token and a secret key that will allow an application
    to access your account on your behalf. We will also need these values for our
    application. Do not share these values as they would allow someone else to authenticate
    as you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting up the Twitter4J client
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Twitter4J client is broken down into a number of different modules that
    can be pieced together depending on our needs. For our purposes, we need the `core`
    module that provides essential functionalities such as HTTP transport, OAuth,
    and access to the basic Twitter API. We will also use the `stream` module for
    accessing the streaming API. These modules can be included in the project by adding
    the following Maven dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The OAuth configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By default, Twitter4J will search the classpath for a `twitter4j.properties`
    file and load OAuth tokens from that file. The easiest way to do this is to create
    the file in the `resources` folder of your Maven project. Add the tokens you generated
    earlier to this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We're now ready to use the Twitter4J client to connect to Twitter's streaming
    API to consume tweets in real time.
  prefs: []
  type: TYPE_NORMAL
- en: The TwitterStreamConsumer class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The purpose of our Twitter client is straightforward; it will perform the following
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect to the Twitter streaming API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Request a stream of tweets filtered by a set of keywords
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a JSON data structure based on the status message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write the JSON data to Kafka for consumption by the Kafka spout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `main()` method of the `TwitterStreamConsumer` class creates a `TwitterStream`
    object and registers an instance of `StatusListener` as a listener. The `StatusListener`
    interface is used as an asynchronous event handler that is notified whenever a
    stream-related event occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: After registering the listener, we create a `FilterQuery` object to filter the
    stream based on a set of keywords. For convenience, we use the program arguments
    as the list of keywords so the filter criteria can be easily changed from the
    command line.
  prefs: []
  type: TYPE_NORMAL
- en: The TwitterStatusListener class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `TwitterStatusListener` class performs most of the heavy lifting in our
    application. The `StatusListener` class defines several callback methods for events
    that can occur during the lifetime of a stream. The `onStatus()` method is our
    primary interest, since it is the method that gets calls whenever a new Tweet
    arrives. The following is the code for the `TwitterStatusListener` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In addition to the raw text of the status message, the `Status` object includes
    convenient methods for accessing all the associated metadata, such as user information,
    the hashtags, URLs, and user mentions contained in the tweet. The bulk of our
    `onStatus()` method builds up the JSON structure before finally logging it to
    the Kafka queue via the Logback Kafka Appender.
  prefs: []
  type: TYPE_NORMAL
- en: Twitter graph topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Twitter graph topology will read raw tweet data from the Kafka queue, parse
    out the relevant information, and then create nodes and relationships in the Titan
    graph database. Instead of writing to the graph database individually for each
    tuple received, we will implement a trident state implementation for performing
    persistence operations in bulk using Trident's transaction mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: This approach offers several benefits. First, for graph databases, such as Titan
    that supports transactions, we can leverage this capability to provide additional
    exactly-once processing guarantees. Second, it allows us to perform a bulk-write
    followed by a bulk-commit (when supported) for an entire batch of tuples rather
    than a write-commit operation for each individual tuple. Finally, by using the
    generic Blueprints API, our Trident state implementation will be largely agnostic
    to the underlying graph database implementation, allowing any Blueprints graph
    database backend to be easily swapped in and out.
  prefs: []
  type: TYPE_NORMAL
- en: '![Twitter graph topology](img/8294_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first component of the topology consists of `JSONProjectFunction,which`
    we developed in [Chapter 7](ch07.html "Chapter 7. Integrating Druid for Financial
    Analytics"), *Integrating Druid for Financial Analytics*, which simply parses
    the raw JSON data to extract only the information we are interested in. In this
    case, we are mainly interested in the timestamp of the message and the JSON representation
    of the Twitter status message.
  prefs: []
  type: TYPE_NORMAL
- en: The JSONProjectFunction class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is a code snippet explaining the `JSONProjectFunction` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Implementing GraphState
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The heart of the topology will be a Trident state implementation responsible
    for translating Trident tuples into graph structures and persisting them. Recall
    that a Trident state implementation consists of three components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`StateFactory`: The `StateFactory` interface defines the method Trident uses
    to create the persistent `State` objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`State`: The Trident `State` interface defines the `beginCommit()` and `commit()`
    methods that are called before and after a Trident batch partition is written
    to the backing store. If the write succeeds (that is, all tuples are processed
    without error), Trident will call the `commit()` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StateUpdater`: The `StateUpdater` interface defines the `updateState()` method
    that is called to update the state, given that there is a batch of tuples. Trident
    passes three arguments to this method: the `State` object to be updated, a list
    of `TridentTuple` objects that represents a batch partition, and a `TridentCollector`
    instance that can be used to optionally emit additional tuples as a result of
    the state update.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these abstractions provided by Trident, we will introduce two
    additional interfaces that will support the use of any Blueprints graph database
    (`GraphFactory`) and isolate any use-case-specific business logic (`GraphTupleProcessor`).
    Before diving in to the Trident state implementation, let's quickly look at these
    interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: GraphFactory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `GraphFactory` interface contract is simple: given a `Map` object that
    represents the Storm and topology configuration, return a `com.tinkerpop.blueprints.Graph`
    implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This interface allows us to plug in any Blueprints-compatible graph implementation
    simply by providing an implementation of the `makeGraph()` method. Later, we will
    implement this interface to return a connection to a Titan graph database.
  prefs: []
  type: TYPE_NORMAL
- en: GraphTupleProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `GraphTupleProcessor` interface provides an abstraction between the Trident
    state implementation and any use-case-specific business logic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Given a graph object, `TridentTuple`, and `TridentCollector`, manipulating the
    graph and optionally emitting additional tuples is the job of a `GraphTupleProcessor`.
    Later in the chapter, we will implement this interface to populate a graph based
    on the content of a Twitter status message.
  prefs: []
  type: TYPE_NORMAL
- en: GraphStateFactory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Trident's `StateFactory` interface represents the entry point for a state implementation.
    When a Trident topology using state components (via the `Stream.partitionPersist()`
    and `Stream.persistentAggregate()` methods) initializes, Storm calls the `StateFactory.makeState()`
    method to create a State instance for each batch partition. The number of batch
    partitions is determined by the parallelism of the stream. Storm passes this information
    to the `makeState()` method via the `numPartitions` and `partitionIndex` parameters,
    allowing state implementations to perform partition-specific logic if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: In our use case, we're not concerned with partitions, so the `makeState()` method
    just uses a `GraphFactory` instance to instantiate a `Graph` instance used to
    construct a `GraphState` instance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: GraphState
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our `GraphState` class provides implementations for `State.beginCommit()` and
    `State.commit()` methods that will be called when a batch partition is about to
    take place and when it has successfully completed, respectively. In our case,
    we override the `commit()` method to check if the internal `Graph` object supports
    transactions, and if so, call the `TransactionalGraph.commit()` method to complete
    the transaction.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `State.beginCommit()` method may be called multiple times if there are failures
    within a Trident batch and the batch is replayed, while the `State.commit()` method
    will only get called once when all partition state updates have completed successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code snippet of the `GraphState` class is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `GraphState.update()` method does the core processing of the transaction
    between the calls to the `State.beginCommit()` and `State.commit()` methods. If
    the `update()` method succeeds for all batch partitions, the Trident transaction
    will complete and the `State.commit()` method will be called.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the `update()` method that actually updates the graph state is simply
    a public method of the `GraphState` class and not overridden. As you will see,
    we will have the opportunity to call this method directly in our `StateUpdater`
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: GraphUpdater
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `GraphUpdater` class implements the `updateState()` method that Storm will
    call (potentially repeatedly in the case of batch failures/replays) just after
    the call to `State.beginCommit()`. The first argument to the `StateUpdater.updateState()`
    method is a Java generics-typed instance of our state implementation that we use
    to call our `GraphState.update()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Implementing GraphFactory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `GraphFactory` interface we defined earlier creates a TinkerPop Graph implementation,
    where a `Map` object represents a Storm configuration. The following code illustrates
    how to create `TitanGraph` backed by Cassandra:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Implementing GraphTupleProcessor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to populate the graph database with relationships gleaned from Twitter
    status messages, we need to implement the `GraphTupleProcessor` interface. The
    following code illustrates parsing the Twitter status message's JSON object and
    creating `"user"` and `"hashtag"` vertices with `"mentions"` relationships.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Putting it all together – the TwitterGraphTopology class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating our final topology consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Consume raw JSON from the Kafka spout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract and project only the data we are interested in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build and connect the Trident `GraphState` implementation to our stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TwitterGraphTopology class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's look at the TwitterGraphTopology class in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the application, begin by executing the `TwitterStreamConsumer` class,
    passing in a list of keywords you want to use to query the Twitter firehose. For
    example, if we want to build a graph of users discussing big data, we might use
    `bigdata` and `hadoop` as query parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `TwitterStreamConsumer` class will connect to the Twitter Streaming API
    and begin queuing data to Kafka. With the `TwitterStreamConsumer` application
    running, we can then deploy `TwitterGraphTopology` to begin populating the Titan
    database.
  prefs: []
  type: TYPE_NORMAL
- en: Let `TwitterStreamConsumer` and `TwitterGraphTopology` run for a while. Depending
    on the popularity of the keywords used for the query, it may take some time for
    the dataset to grow to a meaningful level. We can then connect to Titan with the
    Gremlin shell to analyze the data with graph queries.
  prefs: []
  type: TYPE_NORMAL
- en: Querying the graph with Gremlin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To query the graph, we need to launch the Gremlin shell and create a `TitanGraph`
    instance connected to the local Cassandra backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `g` variable now contains a `Graph` object we can use to issue graph traversal
    queries. The following are a few sample queries you can use to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To find all the users who have tweeted `#hadoop hashtag` and to show the number
    of times they have done this, use the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To count the number of times the `#hadoop hashtag` has been tweeted, use the
    following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The Gremlin DSL is very powerful; covering the complete API could fill an entire
    chapter (if not a whole book). To further explore the Gremlin language, we encourage
    you to explore the following online documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: The official Gremlin Wiki at [https://github.com/tinkerpop/gremlin/wiki](https://github.com/tinkerpop/gremlin/wiki)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GremlinDocs reference guide at [http://gremlindocs.com](http://gremlindocs.com)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL2Gremlin (sample SQL queries and their Gremlin equivalents) at [http://sql2gremlin.com](http://sql2gremlin.com)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced you to graph databases by creating a topology
    that monitors a subset of the Twitter firehose and persists that information to
    the Titan graph database for further analysis. We've also demonstrated the reuse
    of generic components by using generic building blocks from earlier chapters such
    as the Logback Kafka appender.
  prefs: []
  type: TYPE_NORMAL
- en: While graph databases are not perfect for every use case, they represent a powerful
    weapon in your arsenal of polyglot persistence tools. Polyglot persistence is
    a term often used to describe a software architecture that involves multiple types
    of data stores such as relational, key-value, graph, document, and so on. Polyglot
    persistence is all about choosing the right database for the right job. In this
    chapter, we introduced you to graph data models, and have hopefully inspired you
    to explore situations where a graph may be the best data model to support a given
    use case. Later in the book, we will create a Storm application that persists
    data to multiple data stores, each for a specific purpose.
  prefs: []
  type: TYPE_NORMAL
