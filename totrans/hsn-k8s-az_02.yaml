- en: 1\. Introduction to Docker and Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes has become the leading standard in container orchestration. Since
    its inception in 2014, it has gained tremendous popularity. It has been adopted
    by start-ups as well as major enterprises, and the major public cloud vendors
    all offer a managed Kubernetes service.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes builds upon the success of the Docker container revolution. Docker
    is both a company and the name of a technology. Docker as a technology is the
    standard way of creating and running software containers, often called Docker
    containers. A container itself is a way of packaging software that makes it easy
    to run that software on any platform, ranging from your laptop to a server in
    a data center, to a cluster running in the public cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Docker is also the name of the company behind the Docker technology. Although
    the core technology is open source, the Docker company focuses on reducing complexity
    for developers through a number of commercial offerings.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes takes Docker containers to the next level. Kubernetes is a container
    orchestrator. A container orchestrator is a software platform that makes it easy
    to run many thousands of containers on top of thousands of machines. It automates
    a lot of the manual tasks required to deploy, run, and scale applications. The
    orchestrator will take care of scheduling the right container to run on the right
    machine, and it will take care of health monitoring and failover, as well as scaling
    your deployed application.
  prefs: []
  type: TYPE_NORMAL
- en: Docker and Kubernetes are both open-source software projects. Open-source software
    allows developers from many companies to collaborate on a single piece of software.
    Kubernetes itself has contributors from companies such as Microsoft, Google, Red
    Hat, VMware, and many others.
  prefs: []
  type: TYPE_NORMAL
- en: The three major public cloud platforms – **Azure,** **Amazon Web Services**
    (**AWS**), and **Google Cloud Platform** (**GCP**) – all offer a managed Kubernetes
    service. This is attracting a lot of interest in the market since the virtually
    unlimited compute power and the ease of use of these managed services make it
    easy to build and deploy large-scale applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Azure Kubernetes Service** (**AKS**) is Azure''s managed service for Kubernetes.
    It manages the complexity of putting together all the preceding services for you.
    In this book, you will learn how to use AKS to run your applications. Each chapter
    will introduce new concepts, which you will apply through the many examples in
    this book.'
  prefs: []
  type: TYPE_NORMAL
- en: As an engineer, however, it is still very useful to understand the technologies
    that underpin AKS. We will explore these foundations in this chapter. You will
    learn about Linux processes, and how they are related to Docker. You will see
    how various processes fit nicely into Docker, and how Docker fits nicely into
    Kubernetes. Even though Kubernetes is technically a container runtime-agnostic
    platform, Docker is the most commonly used container technology and is used everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces fundamental Docker concepts so that you can begin your
    Kubernetes journey. This chapter also briefly introduces the basics that will
    help you build containers, implement clusters, perform container orchestration,
    and troubleshoot applications on AKS. Having cursory knowledge of what's in this
    chapter will demystify much of the work needed to build your authenticated, encrypted,
    highly scalable applications on AKS. Over the next chapters, you will gradually
    build scalable and secure applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The software evolution that brought us here
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fundamentals of Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fundamentals of Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fundamentals of AKS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The aim of this chapter is to introduce the essentials rather than to provide
    a thorough information source describing Docker and Kubernetes. To begin with,
    we'll first take a look at how software has evolved to get us to where we are
    now.
  prefs: []
  type: TYPE_NORMAL
- en: The software evolution that brought us here
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two major software development evolutions that enabled the popularity
    of Docker and Kubernetes. One is the adoption of a microservices architectural
    style. Microservices allow an application to be built from a collection of small
    services that each serve a specific function. The other evolution that enabled
    Docker and Kubernetes is DevOps. DevOps is a set of cultural practices that allows
    people, processes, and tools to build and release software faster, more frequently,
    and more reliably.
  prefs: []
  type: TYPE_NORMAL
- en: Although you can use both Docker and Kubernetes without using either microservices
    or DevOps, the technologies are most widely adopted for deploying microservices
    using DevOps methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll discuss both evolutions, starting with microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Software development has drastically evolved over time. Initially, software
    was developed and run on a single system, typically a mainframe. A client could
    connect to the mainframe through a terminal, and only through that terminal. This
    changed when computer networks became common when the client-server programming
    model emerged. A client could connect remotely to a server, and even run part
    of the application on their own system while connecting to the server to retrieve
    part of the data the application required.
  prefs: []
  type: TYPE_NORMAL
- en: The client-server programming model has evolved toward truly distributed systems.
    Distributed systems are different from the traditional client-server model as
    they have multiple different applications running on multiple different systems,
    all interconnected.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, a microservices architecture is common when developing distributed
    systems. A microservices-based application consists of a group of services that
    work together to form the application, while the individual services themselves
    can be built, tested, deployed, and scaled independently from each other. The
    style has many benefits but also has several disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: A key part of a microservices architecture is the fact that each individual
    service serves one and only one core function. Each service serves a single bounded
    business function. Different services work together to form the complete application.
    Those services work together over network communication, commonly using HTTP REST
    APIs or gRPC.
  prefs: []
  type: TYPE_NORMAL
- en: This architectural approach is commonly adopted by applications run using Docker
    and Kubernetes. Docker is used as the packaging format for the individual services,
    while Kubernetes is the orchestrator that deploys and manages the different services
    running together.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the Docker and Kubernetes specifics, let's first explore
    the benefits and downsides of adopting microservices.
  prefs: []
  type: TYPE_NORMAL
- en: '**Advantages of running microservices**'
  prefs: []
  type: TYPE_NORMAL
- en: There are several advantages to running a microservices-based application. The
    first is the fact that each service is independent of the other services. The
    services are designed to be small enough (hence micro) to handle the needs of
    a business domain. As they are small, they can be made self-contained and independently
    testable, and so are independently releasable.
  prefs: []
  type: TYPE_NORMAL
- en: This leads to the fact that each microservice is independently scalable as well.
    If a certain part of the application is getting more demand, that part of the
    application can be scaled independently from the rest of the application.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that services are independently scalable also means they are independently
    deployable. There are multiple deployment strategies when it comes to microservices.
    The most popular are rolling upgrades and blue/green deployments.
  prefs: []
  type: TYPE_NORMAL
- en: With a rolling upgrade, a new version of the service is deployed only to part
    of the end user community. This new version is carefully monitored and gradually
    gets more traffic if the service is healthy. If something goes wrong, the previous
    version is still running, and traffic can easily be cut over.
  prefs: []
  type: TYPE_NORMAL
- en: With a blue/green deployment, you would deploy the new version of the service
    in isolation. Once the new version of the service is deployed and tested, you
    would cut over 100% of the production traffic to the new version. This allows
    for a clean transition between service versions.
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of the microservices architecture is that each service can be
    written in a different programming language. This is described as being **polyglot**
    – able to understand and use multiple languages. For example, the front end service
    can be developed in a popular JavaScript framework, the back end can be developed
    in C#, while the machine learning algorithm can be developed in Python. This allows
    you to select the right language for the right service, and to have the developers
    use the languages they are most familiar with.
  prefs: []
  type: TYPE_NORMAL
- en: '**Disadvantages of running microservices**'
  prefs: []
  type: TYPE_NORMAL
- en: There's a flip side to every coin, and the same is true for microservices. While
    there are multiple advantages to a microservices-based architecture, this architecture
    has its downsides as well.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices designs and architectures require a high degree of software development
    maturity in order to be implemented correctly. Architects who understand the domain
    very well must ensure that each service is bounded and that different services
    are cohesive. Since services are independent of each other and versioned independently,
    the software contract between these different services is important to get right.
  prefs: []
  type: TYPE_NORMAL
- en: Another common issue with a microservices design is the added complexity when
    it comes to monitoring and troubleshooting such an application. Since different
    services make up a single application, and those different services run on multiple
    servers, both logging and tracing such an application is a complicated endeavor.
  prefs: []
  type: TYPE_NORMAL
- en: Linked to the aforementioned disadvantages is that, typically, in microservices,
    you need to build more fault tolerance into your application. Due to the dynamic
    nature of the different services in an application, faults are more likely to
    happen. In order to guarantee application availability, it is important to build
    fault tolerance into the different microservices that make up an application.
    Implementing patterns such as retry logic or circuit breakers is critical to avoid
    a single fault causing application downtime.
  prefs: []
  type: TYPE_NORMAL
- en: Often linked to microservices, but a separate transformation, is the DevOps
    movement. We will explore what DevOps means in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: DevOps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DevOps literally means the combination of development and operations. More
    specifically, DevOps is the union of people, processes, and tools to deliver software
    faster, more frequently, and more reliably. DevOps is more about a set of cultural
    practices than about any specific tools or implementations. Typically, DevOps
    spans four areas of software development: planning, developing, releasing, and
    operating software.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many definitions of DevOps exist. The authors have adopted this definition,
    but you as a reader are encouraged to explore different definitions in the literature
    around DevOps.
  prefs: []
  type: TYPE_NORMAL
- en: The DevOps culture starts with planning. In the planning phase of a DevOps project,
    the goals of a project are outlined. These goals are outlined both at a high level
    (called an *Epic*) and at a lower level (in *Features* and *Tasks*). The different
    work items in a DevOps project are captured in the feature backlog. Typically,
    DevOps teams use an agile planning methodology working in programming sprints.
    Kanban boards are often used to represent project status and to track work. As
    a task changes status from *to do* to *doing* to *done*, it moves from left to
    right on a Kanban board.
  prefs: []
  type: TYPE_NORMAL
- en: When work is planned, actual development can be done. Development in a DevOps
    culture isn't only about writing code, but also about testing, reviewing, and
    integrating with team members. A version control system such as Git is used for
    different team members to share code with each other. An automated **continuous
    integration** (**CI**) tool is used to automate most manual tasks such as testing
    and building code.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a feature is code-complete, tested, and built, it is ready to be delivered.
    The next phase in a DevOps project can start: delivery. A **continuous delivery**
    (**CD**) tool is used to automate the deployment of software. Typically, software
    is deployed to different environments, such as testing, quality assurance, or
    production. A combination of automated and manual gates is used to ensure quality
    before moving to the next environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when a piece of software is running in production, the operations phase
    can start. This phase involves the maintaining, monitoring, and supporting of
    an application in production. The end goal is to operate an application reliably
    with as little downtime as possible. Any issues are to be identified as proactively
    as possible. Bugs in the software need to be tracked in the backlog.
  prefs: []
  type: TYPE_NORMAL
- en: The DevOps process is an iterative process. A single team is never in a single
    phase of the process. The whole team is continuously planning, developing, delivering,
    and operating software.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple tools exist to implement DevOps practices. There are point solutions
    for a single phase, such as Jira for planning or Jenkins for CI and CD, as well
    as complete DevOps platforms, such as GitLab. Microsoft operates two solutions
    that enable customers to adopt DevOps practices: Azure DevOps and GitHub. Azure
    DevOps is a suite of services to support all phases of the DevOps process. GitHub
    is a separate platform that enables DevOps software development. GitHub is known
    as the leading open-source software development platform, hosting over 40 million
    open-source projects.'
  prefs: []
  type: TYPE_NORMAL
- en: Both microservices and DevOps are commonly used in combination with Docker and
    Kubernetes. After this introduction to microservices and DevOps, we'll continue
    this first chapter with the fundamentals of Docker and containers and then the
    fundamentals of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentals of Docker containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A form of container technology has existed in the Linux kernel since the 1970s.
    The technology powering today's containers, called cgroups, was introduced into
    the Linux kernel in 2006 by Google. The Docker company popularized the technology
    in 2013 by introducing an easy developer workflow. The company gave its name to
    the technology, so the name Docker can refer to both the company as well as the
    technology. Most commonly though, we use Docker to refer to the technology.
  prefs: []
  type: TYPE_NORMAL
- en: Docker as a technology is both a packaging format and a container runtime. We
    refer to packaging as an architecture that allows an application to be packaged
    together with its dependencies, such as binaries and runtime. The runtime points
    at the actual process of running the container images.
  prefs: []
  type: TYPE_NORMAL
- en: You can experiment with Docker by creating a free Docker account at Docker Hub
    ([https://hub.docker.com/](https://hub.docker.com/)) and using that login to open
    Docker Labs ([https://labs.play-with-docker.com/](https://labs.play-with-docker.com/)).
    This will give you access to an environment with Docker pre-installed that is
    valid for 4 hours. We will be using Docker Labs in this section as we build our
    own container and image.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although we are using the browser-based Docker Labs in this chapter to introduce
    Docker, you can also install Docker on your local desktop or server. For workstations,
    Docker has a product called Docker Desktop ([https://www.docker.com/products/docker-desktop](https://www.docker.com/products/docker-desktop))
    that is available for Windows and Mac to create Docker containers locally. On
    servers – both Windows and Linux – Docker is also available as a runtime for containers.
  prefs: []
  type: TYPE_NORMAL
- en: Docker images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Docker uses an image to start a new container. An image contains all the software
    you need to run within your container. Container images can be stored locally
    on your machine, as well as in a container registry. There are public registries,
    such as the public Docker Hub ([https://hub.docker.com/](https://hub.docker.com/)),
    or private registries, such as **Azure Container Registry** (**ACR**). When you,
    as a user, don't have an image locally on your PC, you will pull an image from
    a registry using the `docker pull` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will pull an image from the public Docker Hub
    repository and run the actual container. You can run this example in Docker Labs
    by following these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of these commands will look similar to *Figure 1.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Docker image pulled from the public Docker Hub repositoryand the output
    displaying the image ID and size. Also, running the Docker image results in a
    picture of a whale saying "boo".](image/Figure_1.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Example of running Docker in Docker Labs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What happened here is that Docker first pulled your image in multiple parts
    and stored it locally on the machine it was running on. When we ran the actual
    application, it used that local image to start a container. If we look at the
    commands in detail, you will see that `docker pull` took in a single parameter,
    `docker/whalesay`. If you don't provide a private container registry, Docker will
    look in the public Docker Hub for images, which is where Docker pulled our image
    from. The `docker run` command took in a couple of arguments. The first argument
    was `docker/whalesay`, which is the reference to the image. The next two arguments,
    `cowsay boo`, are commands that were passed to the running container to execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous example, we learned that it is possible to run a container
    without building an image first. It is, however, very common that you will want
    to build your own images. To do this, you use a **Dockerfile**. A Dockerfile contains
    steps that Docker will follow to start from a base image and build your image.
    These instructions can range from adding files to installing software or setting
    up networking. An example of a Dockerfile is provided in the following code snippet,
    which we''ll create in our Docker playground:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: There are three lines in this Dockerfile. The first one will instruct Docker
    which image to use as a source image for this new image. The next step is a command
    that is run to add new functionality to our image. In this case, updating our
    `apt` repository and installing an application called `fortunes`. Finally, the
    `CMD` command tells Docker which command to execute when a container based on
    this image is run.
  prefs: []
  type: TYPE_NORMAL
- en: You typically save a Dockerfile in a file called `Dockerfile`, without an extension.
    To build our image, you need to execute the `docker build` command and point it
    to the Dockerfile you created. In building the Docker image, the process will
    read the Dockerfile and execute the different steps in the Dockerfile. This command
    will also output the steps it took to run a container and build your image. Let's
    walk through a demo of building our own image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to create this Dockerfile, open up a text editor via the `vi Dockerfile`
    command. vi is an advanced text editor in the Linux command line. If you are not
    familiar with it, let''s walk through how you would enter the text in there:'
  prefs: []
  type: TYPE_NORMAL
- en: After you've opened vi, hit the `i` key to enter insert mode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, either copy-paste or type the three code lines.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Afterward, hit the *Esc* key, and type `:wq!` to write (w) your file and quit
    (q) the text editor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to execute `docker build` to build our image. We will add a
    final bit to that command, namely adding a tag to our image so we can call it
    by a useful name. To build your image, you will use the `docker build -t smartwhale
    .` command (don't forget to add the final dot here).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will now see Docker execute a number of steps – three in our case – in
    order to build our image. After your image is built, you can run your application.
    To run your container, you would run `docker run smartwhale`, and you should see
    an output similar to *Figure 1.2*. However, you will probably see a different
    smart quote. This is due to the `fortunes` application generating different quotes.
    If you run the container multiple times, you will see different quotes appear,
    as shown in *Figure 1.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Output of the docker run smartwhale command displaying a smart quote; generated
    by the fortunes application.](image/Figure_1.2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Example of running a custom container'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: That concludes our overview and demo of Docker. In this section, you started
    with an existing container image and launched that on Docker Labs. Afterward,
    you took that a step further and built your own container image and started containers
    using your own image. You have now learned what it takes to build and run a container.
    In the next section, we will cover Kubernetes. Kubernetes allows you to run multiple
    containers at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes as a container orchestration platform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building and running a single container seems easy enough. However, things can
    get complicated when you need to run multiple containers across multiple servers.
    This is where a container orchestrator can help. A container orchestrator takes
    care of scheduling containers to be run on servers, restarting containers when
    they fail, moving containers to a new host when that host becomes unhealthy, and
    much more.
  prefs: []
  type: TYPE_NORMAL
- en: The current leading orchestration platform is Kubernetes ([https://kubernetes.io/](https://kubernetes.io/)).
    Kubernetes was inspired by the Borg project in Google, which, by itself, was running
    millions of containers in production.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes takes a declarative approach to orchestration; that is, you specify
    what you need and Kubernetes takes care of deploying the workload you specified.
    You don't need to start these containers manually yourself anymore, as Kubernetes
    will launch the Docker containers you specified.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although Kubernetes supports multiple container runtimes, Docker is the most
    popular runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout the book, we will build multiple examples that run containers in
    Kubernetes, and you will learn more about the different objects in Kubernetes.
    In this introductory chapter, we''ll introduce three elementary objects in Kubernetes
    that you will likely see in every application: a Pod, a Deployment, and a Service.'
  prefs: []
  type: TYPE_NORMAL
- en: Pods in Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Pod in Kubernetes is the essential scheduling block. A Pod is a group of one
    or more containers. This means a Pod contains either a single container or multiple
    containers. When creating a Pod with a single container, you can use the terms
    container and Pod interchangeably. However, the term Pod is still preferred.
  prefs: []
  type: TYPE_NORMAL
- en: When a Pod contains multiple containers, these containers share the same filesystem
    and the same network namespace. This means that when a container that is part
    of a Pod writes a file, other containers in that same Pod can read that file.
    This also means that all containers in a Pod can communicate with each other using
    `localhost` networking.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of design, you should only put containers that need to be tightly
    integrated in the same pod. Imagine the following situation: you have an old web
    application that does not support HTTPS. You want to upgrade that application
    to support HTTPS. You could create a Pod that contains your old web application
    and includes another container that would do SSL offloading for that application
    as described in *Figure 1.3*. Your users would connect to your application using
    HTTPS, while the container in the middle converts HTTPS traffic to HTTP:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Example of a multi-container Pod that does HTTPS offloading, illustrating
    how the user makes an HTTPS request to the HTTPS offload container, which in turn
    makes an HTTP request to the old web app within the Pod.](image/Figure_1.3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Example of a multi-container Pod that does HTTPS offloading'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This design principle is known as a sidecar. Microsoft has a free e-book available
    that describes multiple multi-container Pod designs and designing distributed
    systems ([https://azure.microsoft.com/resources/designing-distributed-systems/](https://azure.microsoft.com/resources/designing-distributed-systems/)).
  prefs: []
  type: TYPE_NORMAL
- en: A Pod, whether it be a single or a multi-container Pod, is an ephemeral resource.
    This means that a Pod can be terminated at any point and restarted on another
    node. When this happens, the state that was stored in that Pod will be lost. If
    you need to store state in your application, you either need to store that in
    a `StatefulSet`, which we'll touch on in *Chapter 3*, *Application deployment
    on AKS*, or store the state outside of Kubernetes in an external database.
  prefs: []
  type: TYPE_NORMAL
- en: Deployments in Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Deployment in Kubernetes provides a layer of functionality around Pods. It
    allows you to create multiple Pods from the same definition and to easily perform
    updates to your deployed Pods. A Deployment also helps with scaling your application,
    and potentially even autoscaling your application.
  prefs: []
  type: TYPE_NORMAL
- en: Under the covers, a Deployment creates a `ReplicaSet`, which in turn will create
    the Pod you requested. A `ReplicaSet` is another object in Kubernetes. The purpose
    of a `ReplicaSet` is to maintain a stable set of Pods running at any given time.
    If you perform updates to your Deployment, Kubernetes will create a new `ReplicaSet`
    that will contain the updated Pods. By default, Kubernetes will do a rolling upgrade
    to the new version. This means that it will start a few new Pods. If those are
    running correctly, then it will terminate a few old Pods and continue this loop
    until only new Pods are running.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graphical representation of the relationship between Deployments, ReplicaSets,
    and Pods.](image/Figure_1.4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure1.4: Relationship between Deployment, ReplicaSet, and Pods'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Services in Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Service in Kubernetes is a network-level abstraction. This allows you to expose
    the multiple Pods you have in your Deployment under a single IP address and a
    single DNS name.
  prefs: []
  type: TYPE_NORMAL
- en: Each Pod in Kubernetes has its own private IP address. You could theoretically
    connect your applications using this private IP address. However, as mentioned
    before, Kubernetes Pods are ephemeral, meaning they can be terminated and moved,
    which would impact their IP address. By using a Service, you can connect your
    applications together using a single IP address. When a Pod moves from one node
    to another, the Service will ensure traffic is routed to the correct endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have introduced Kubernetes and three essential objects with
    Kubernetes. In the next section, we'll introduce AKS.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Kubernetes Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Azure Kubernetes Service** (**AKS**) makes creating and managing Kubernetes
    clusters easier.'
  prefs: []
  type: TYPE_NORMAL
- en: A typical Kubernetes cluster consists of a number of master nodes and a number
    of worker nodes. A node within Kubernetes is equivalent to a **virtual machine**
    (**VM**). The master nodes contain the Kubernetes API and a database that contains
    the cluster state. The worker nodes are the VMs that run your actual workload.
  prefs: []
  type: TYPE_NORMAL
- en: AKS makes it a lot easier to create a cluster. When you create an AKS cluster,
    AKS sets up the Kubernetes master for you, free of charge. AKS will then create
    VMs in your subscription, and turn those VMs into worker nodes of your Kubernetes
    cluster in your network. You only pay for those VMs; you don't pay for the master.
  prefs: []
  type: TYPE_NORMAL
- en: Within AKS, Kubernetes Services are integrated with Azure Load Balancer and
    Kubernetes Ingresses are integrated with the application gateway. Azure Load Balancer
    is a layer-4 network load balancer Service; the application gateway is a layer-7
    HTTP-based load balancer. The integration between Kubernetes and both Services
    means that when you create a Service or Ingress in Kubernetes, Kubernetes will
    create a rule in an Azure load balancer or application gateway respectively. Azure
    Load Balancer or Application Gateway will then route the traffic to the right
    node in your cluster that hosts your Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, AKS adds a number of functionalities that make it easier to manage
    a cluster. AKS contains logic to upgrade clusters to newer Kubernetes versions.
    It also has the ability to easily scale your clusters, either making them bigger
    or smaller.
  prefs: []
  type: TYPE_NORMAL
- en: The service also comes with integrations that make operations easier. AKS clusters
    come pre-configured with integration with **Azure Active Directory** (**Azure
    AD**) to make managing identities and **role-based access control** (**RBAC**)
    straightforward. RBAC is the configuration process that defines which users get
    access to resources and which actions they can take against those resources. AKS
    is also integrated into Azure Monitor for containers, which makes monitoring and
    troubleshooting your Deployments simpler.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we introduced the concepts of Docker and Kubernetes. We ran
    a number of containers, starting with an existing image and then using an image
    we built ourselves. After that demo, we explored three essential Kubernetes objects:
    the Pod, the Deployment, and the Service.'
  prefs: []
  type: TYPE_NORMAL
- en: This provides the common context for the remaining chapters, where you will
    deploy Dockerized applications in Microsoft AKS. You will see how the AKS **Platform
    as a Service** (**PaaS**) offering from Microsoft streamlines Deployment by handling
    many of the management and operational tasks that you would have to do yourself
    if you managed and operated your own Kubernetes infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce the Azure portal and its components in
    the context of creating your first AKS cluster.
  prefs: []
  type: TYPE_NORMAL
