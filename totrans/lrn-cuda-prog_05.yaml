- en: CUDA Application Profiling and Debugging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CUDA provides many programming tools for developers. These tools are the compilers,
    profilers, the IDE and its plugins, debuggers, and memory checkers. Learning about
    these tools will help you analyze your application and help you accomplish the
    development projects we will be covering. In this chapter, we will cover the basic
    usage of these tools and discuss how to apply these to application development.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Profiling focused target ranges in GPU applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual profiling against the remote machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging a CUDA application with CUDA error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asserting local GPU values using CUDA Assert
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging a CUDA application with Nsight Visual Studio Edition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging a CUDA application with Nsight Eclipse Edition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging a CUDA application with CUDA-GDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runtime validation with CUDA-memcheck
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete this chapter, it is recommended that you use an NVIDIA GPU card
    later than the Pascal architecture. In other words, your GPU's compute capability
    should be equal to or greater than 60\. If you are unsure of your GPU's architecture,
    please visit NVIDIA's site, [https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus),
    and confirm your GPU's compute capability.
  prefs: []
  type: TYPE_NORMAL
- en: The sample code for this chapter has been developed and tested with version
    10.1 of CUDA Toolkit. In general, it is recommended that you use the latest CUDA
    version if applicable.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling focused target ranges in GPU applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NVIDIA's Visual Profiler is a handy tool for finding bottlenecks in GPU applications
    and understanding their operations. Although it provides fluent information of
    the application operations, those can be redundant if you just want to focus on
    a specific area of code. In this situation, limiting the range of profiling is
    more productive.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling targets can be specific code blocks, GPU, and time. Specifying the
    code blocks is called **focused profiling**. This technique is useful when you
    want to focus on profiling on a specific kernel function, or profiling on the
    part of a large GPU application. Targeting GPUs or time will be covered after
    we cover focused profiling.
  prefs: []
  type: TYPE_NORMAL
- en: Limiting the profiling target in code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To benefit from focused profiling, you may want to include the featured header
    file in your source code, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can specify your targeting range of profiling using `cudaProfilerStart()` and
    `cudaProfilerStop()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, you need to profile your application with a specific flag, `--profile-from-start`.
  prefs: []
  type: TYPE_NORMAL
- en: This option doesn't let the profiler start profiling until the request arrives. If
    you want to profile your application using NVIDIA Visual Profiler, make sure to
    tick the Start execution with profiling enabled checkbox in the setting view.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps cover how to control the NVIDIA profiler using some simple
    example code. To make this easier, we will reuse the sample code that we used
    to operate matrix multiplication in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml),
    *CUDA Thread Programming*:'
  prefs: []
  type: TYPE_NORMAL
- en: Write a CUDA application with two simple SGEMM CUDA kernel functions. The kernel
    functions are identical but have different names, that is, `sgemm_kernel_A()`
    and `sgemm_kernel_B()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make two iterative calls, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s compile the code and profile using `nvprof`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When you open the generated `profile-original.nvvp` file using the Visual Profiler,
    you will have the profiled result, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bdf10e5-d9dc-48b0-b300-bff170a481a5.png)'
  prefs: []
  type: TYPE_IMG
- en: This timeline includes whole profiled information from when the application
    started. However, we can say that the profiled result contains unnecessary information
    when we want to optimize our kernel functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps cover how to specify the profile focusing area:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Place `#include <cuda_profiler_api.h>` on top of the source code to enable
    focused profile APIs. Then, we can embrace the area we are interested in using `cudaProfilerStart()`
    and `cudaProfilerStop()`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile your code and view the updated profiled result using the Visual Profiler.
    We have to provide the `--profile-from-start off` option to the profiler, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When you open the newly generated profile result, the profiler only reports
    the specified part of the application, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9bcc2ff9-c3a3-4af8-b741-593489860e3d.png)'
  prefs: []
  type: TYPE_IMG
- en: The profile result is restricted. The preceding screenshot shows the kernel's
    execution from when it started GPU execution. As a result, you can eliminate having
    to profile an application's initialization and other irrelevant operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In conclusion, the focused profile has several benefits, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It helps you focus on the module you're currently developing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It lets you remove irrelevant operations from the report in the profiler, for
    example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An external module's behavior that doesn't have any relation to your code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application initialization delay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It helps you save time when it comes to finding the targeting function in the
    timeline view.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limiting the profiling target with time or GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The NVIDIA profiler has other options that can limit profile targets. You can
    use the following options with focused profiling, too:'
  prefs: []
  type: TYPE_NORMAL
- en: The `--timeout <second>` option limits application execution time. This option
    is useful when you need to profile an application that has a long execution time
    with iterative operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `--devices <gpu ids>` option specifies the GPUs to profile. This option
    helps you narrow down GPU kernel operations in a multiple GPU application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, you don''t have to collect all the metrics if you only want to focus
    on a few kernel functions. You can just stipulate your interest to the profiler
    with the `--kernels`, `--event`, and `--metrics` options. You can use those options
    along with other profile options, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: After importing the collected metrics into the timeline profile result, you
    will find that the targeted kernels only have metrics information.
  prefs: []
  type: TYPE_NORMAL
- en: There are many other versatile profile features in CPU sampling, such as marking
    profile range, OpenMP and OpenACC profiles, and so on. If you want to take a look
    at the features of the NVIDIA profiler, check out the following profiler introduction
    talk by Jeff Larkin from NVIDIA: [https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Profilers.pdf](https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Profilers.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA's official profiler user guide provides details about the NVIDIA profiler's
    functions ([https://docs.nvidia.com/cuda/profiler-users-guide/index.html](https://docs.nvidia.com/cuda/profiler-users-guide/index.html.)).
  prefs: []
  type: TYPE_NORMAL
- en: Profiling with NVTX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With focused profiling, we can profile a limited, specific area by using `cudaProfilerStart()` and `cudaProfilerStop()`.
    However, if we want to analyze functional performance in a complex application,
    it is limited. For this situation, the CUDA profiler provides timeline annotations
    via the **NVIDIA Tools Extension** (**NVTX**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using NVTX, we can annotate the CUDA code. We can use the NVTX API as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we can define a range as a group of codes and annotate that
    range manually. Then, the CUDA profiler provides a timeline trace of the annotation
    so that we can measure the execution time of code blocks. One drawback of this
    is that the NVTX APIs are host functions, so we need to synchronize the host and
    GPU if the target code blocks are pure GPU kernel calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about this, let''s apply this NVTX code to the previous focused
    profiling example. First, we should include an NVTX header file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will insert `nvtxRangePushA()` and `nvtxRangePop()` into several places,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have enlarged the focused profile area to monitor
    the NVTX operation. We also have `Data Transfer`, `Kernel A`, `Kernel B`, and
    `Kernel Execution` as NVTX ranges. NVTX supports multi-level annotations, so the `Kernel
    A` and `Kernel B` ranges will be included in the `Kernel Execution` timeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compile the code, we should provide the `-lnvToolsExt` option to the `nvcc`
    compiler to provide NVTX API''s definition. We can compile the code using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the NVIDIA profiler can collect NVTX annotations without extra options.
    We can profile the application using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the timeline profiled result. In this screenshot,
    we can see Markers and Ranges colored in green. These green bars have the annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6943cf2-08ba-4c8e-974b-03436038efdf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding screenshot provides us with the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: We can identify where the memory copy operation has been called following the
    NVTX annotation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can divide the functional positions by wrapping the area, for example, `kernel
    A` and `kernel B`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NVTX annotation can stack multiple levels of annotations. As we can see, `kernel
    A` and `kernel B` are included in the `kernel execution` annotation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following document not only introduces NVTX but also explains how to use
    different colors using NVTX: [https://devblogs.nvidia.com/cuda-pro-tip-generate-custom-application-profile-timelines-nvtx](https://devblogs.nvidia.com/cuda-pro-tip-generate-custom-application-profile-timelines-nvtx).
    One of the applications of NVTX is to profile deep learning networks with NVTX
    annotations. This provides insight into network operation bottlenecks. We will
    discuss this in [Chapter 10](d0e9e8ff-bc17-4031-bb0e-1cfd310aff6f.xhtml), *Deep
    Learning Acceleration with CUDA*, of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: Visual profiling against the remote machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NVIDIA Visual Profiler also can profile remote applications. This feature
    eases the profiling task when it comes to remote application development, especially
    when you develop your application on the server-side.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways of using visual profilers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Profiling on the host with the host CUDA application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By collecting profile data using the `nvprof` CLI on the target side, copying
    the file to the host, and opening it using the Visual Profiler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling the application on the target platform using the host machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual profiling directly in the host machine is convenient and can save development
    time. Also, remote profiling provides the same user experience that profiling
    a GPU application on a host machine does. One exception is that we should establish
    a remote connection. Another benefit OS host-managed visual profiling provides
    is that the profiler collects metric information on-demand automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NVIDIA profiler communicates with the NVIDIA profiler in the host machine
    and collects profiled data. Therefore, you need to confirm that your host machine
    – desktop or laptop – should connect to the remote machine. The following diagram
    shows the overview of this connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b19fd33-9512-4762-b55b-76746e212d43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s try to profile a GPU application remotely. The following steps cover
    how to profile a remote GPU application in NVIDIA Visual Profiler:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, go to File | New session. When you click the New Session menu, you will
    see the following dialog window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/98f0371c-d094-481c-a716-ba64d95ddc8c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we need to add a connection, which we do by going to the Manage connection... menu.
    Then, the New Remote Connectiondialog will appear. Add your remote machine information
    by clicking the Add button and putting your remote machine information in the
    appropriate sections. Then, close the dialog by clicking the Finish button. When
    you''re finished, you will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a163c51c-0e85-4386-8545-6527b9445305.png)'
  prefs: []
  type: TYPE_IMG
- en: As we discussed previously, the host and remote machine communicate over SSH,
    whose default port number is 22\. If the host machine uses another port for SSH,
    you have to inform it of that port number in the new remote session creation dialog.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to set up CUDA Toolkit paths in the remote machine by clicking
    the Manage... button at the right-hand side of Toolkit/Script*. *A good start
    is to use the Detect button. It finds the `nvcc` path and sets up the configuration
    information automatically. If automatic detection failed, you have to input the
    configuration information manually. When you''ve finished with the configuration
    process, click the Finish button, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/214bb1ee-1f3e-413a-a15a-6ee7ab8ecbc9.png)'
  prefs: []
  type: TYPE_IMG
- en: Specify the GPU application's binary by clicking the Browse button on the right-hand
    side of the File text box. It will ask for your remote machine login password.
    Find your application path and set the application's path. You can also put the
    application's arguments if you need to control the application's behavior. When
    you've finished setting up the application and connection, click the Next button
    to set the profiler's options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we will set up the profiling options. NVIDIA Visual Profiler allows us
    to set the profiler''s options using checkboxes as shown in the following screenshot.
    By clicking Finish, the profiler collects profile data from the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/258424fc-ee37-4d52-be8b-9572821710c5.png)'
  prefs: []
  type: TYPE_IMG
- en: You will see the timelined profiled output, as profiled on the host machine.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, analyze the performance of the profiled timeline graph. Click any kernel
    function you want to analyze. Click the Perform Kernel Analysis button; the profiler
    tool will collect the related metrics information. By doing this, you can quickly
    get a report regarding the performance limiters and find the bottlenecks of the
    kernel function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Debugging a CUDA application with CUDA error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having dedicated exception checks and checking errors is one of the base features
    that make high-quality software. CUDA functions report the error by returning
    their status for each function call. Not only CUDA APIs, but kernel functions
    and the CUDA library''s API call follow this rule. Therefore, detecting a recurring
    error is the start of identifying errors in CUDA execution. For example, let''s
    assume that we have allocated global memory using the `cudaMalloc()` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'What if the global memory has insufficient free space to allocate new memory
    space? In this case, the `cudaMalloc()` function returns an error to report an
    out of memory exception. Errors that are triggered by kernel calls can be captured
    from the flags using `cudaGetLastError()`. This returns the recorded error status
    and resets the flag''s value. However, handle this flag with caution: its return
    doesn''t guarantee that the error occurred at the GPU''s last execution and the
    flag is reset manually.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The return from CUDA APIs and the return of the `cudaGetLastError()` function
    are of the `cudaError_t` type. This `cudaError_t` type is a predefined integer
    type, and the application identifies which type of error has occurred. For example,
    this type is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Memorizing or translating all of these values is impractical. For this purpose,
    the CUDA sample code provides a helper function, `checkCudaError()`, which is
    located in `common/inc/cuda_helper.h`. This function prints out the error message
    when the CUDA function returns errors. Its function definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Since this function is defined as a macro, we can identify the line where the
    error occurred.
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways we can use this function. One is to include the `cuda_helper.h`
    file in the source code. Alternatively, we can copy the function code into somewhere
    in the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will embrace all the CUDA API classes with `checkCudaErrors()`, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'For the kernel function call, we will use the `cudaGetLastError()` function
    to get the kernel call''s error flag, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this code has a problem: the kernel operation is asynchronous with
    the host so that `cudaGetLastError()` can only catch the host side''s return values.
    It is highly possible that the error was triggered somewhere in the application.
    To resolve this situation, you can use any host and device synchronization function;
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s test the error detection code by modifying the source code to generate
    an error. For example, you can request `cudaMemcpy` to copy a larger memory space
    than the allocated size. In this case, the application returns an error message,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can pass a `NULL` point for the CUDA kernel so that the
    kernel accesses the invalid memory space. In this case, the application reports
    an illegal address error in `cudaDeviceSynchronize()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This error checking macro is quite useful because it reports where the error
    occurs in the source code. However, this report has a missing point that its error
    detected position does not match with the real error occurred position.
  prefs: []
  type: TYPE_NORMAL
- en: 'The error message should report the position where we copying memory that''s
    larger than the allocated memory results in an illegal value error immediately.
    So, the developer can identify the error message right after the kernel call.
    However, this error checking code only works on a host. Therefore, this can confuse
    the GPU operations if they''re not synchronized properly. For example, if we didn''t
    set the synchronization and just checking error, then the `cudaDeviceSynchronize()`
    function can report an error from the wrong place. In this case, we can set `CUDA_LAUNCH_BLOCKING=1`
    environment variable to make all the kernel execution to be synchronized with
    the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Line `36` at `sgemm.cu` is the `cudaGetLastError()` call, right after the `sgemm`
    kernel call. That's where we place an intended error. We can identify the correct
    error position at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two official documents that can help you learn about the different
    types of CUDA errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`include/driver_types.h` in the CUDA Toolkit root path'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asserting local GPU values using CUDA assert
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though your GPU application works without any systematic errors, you need
    to check the computed result to make sure that the execution works as it was designed
    to. For this purpose, CUDA provides the `assert` function, which checks whether
    the argument value is zero. If it is, this function raises an error flag so that
    the host can identify that there is an error in the kernel function.
  prefs: []
  type: TYPE_NORMAL
- en: 'An assertion is used to validate that the operation result is as expected.
    In CUDA programming, the `assert` function can be called from the device code
    and can stop the kernel''s execution when the given argument is zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the declaration of the `assert` function, which is the same as it is
    for C/C++. When the assertion is triggered, the application stops and reports
    its error message. If the application is launched by the debugger, it works as
    a breakpoint so that the developer can debug the given information. For instance,
    the output message looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Since the output message directs the exact CUDA block and thread index, the
    developer can analyze the directed CUDA thread's execution easily.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's apply the assertion and see how it can detect intended errors. We
    will modify the SGEMM operation code that we used in the *Profiling focused target
    ranges in GPU applications* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, place the assertion code in the middle of a kernel function. We will
    see the effect of the expression, which should be false. The assertion code can
    be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You can try other index values or try other possible errors too. Compile the
    code and run it to see the output. The following code shows the output error of
    this modification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The error message reports that the assertion triggered the code location, the
    kernel function's name, and GPU's thread index. With this information, we can
    find out where we should start analyzing easily.
  prefs: []
  type: TYPE_NORMAL
- en: Actually, the usage of the `assert` function is the same as the `assert` function
    in normal C/C++ programming. One difference is that the `assert` function works
    in the device code. Therefore, it reports not only the event location and the
    expression, but also shows the block and thread index.
  prefs: []
  type: TYPE_NORMAL
- en: However, using assertion has an impact on application performance. Therefore,
    we should only use assertion for debugging purposes. It is recommended to disable
    it when we're running in a production environment. You can disable assertion at
    compile time by adding the `NDEBUG` preprocessed macro before including `assert.h`.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging a CUDA application with Nsight Visual Studio Edition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For Windows application developers, the CUDA Toolkit provides Nsight Visual
    Studio Edition, which enables GPU computing in Visual Studio. This tool works
    as an extension of Visual Studio, but you can build, debug, profile, and trace
    GPU applications along with the host. If your working platform is not Windows,
    the contents in this section won't be applicable, so you can skip it.
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA debugger allows us to monitor the local values on a GPU kernel for
    each CUDA thread. Like normal host debugging, you can set breakpoints in the kernel
    code and trigger them. You can also place conditions such as other normal breakpoints.
    With this feature, you can trigger breakpoints for a specific CUDA thread index
    and review their local variables.
  prefs: []
  type: TYPE_NORMAL
- en: This tool can be installed along with the CUDA Toolkit. You can obtain the latest
    version from the website. It is not mandatory, but it is recommended when your
    development environment is using the old CUDA Toolkit on the latest GPU and its
    driver. Visit the NVIDIA Nsight web page ([https://developer.nvidia.com/nsight-visual-studio-edition](https://developer.nvidia.com/nsight-visual-studio-edition))
    to download and install Nsight. You'll need an NVIDIA developer membership to
    obtain the software. You will also need to install the recommended display driver
    version.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the CUDA tools by going to Menu | Nsight in the Visual Studio
    menu bar. There are several tools in this menu, some of which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Graphics debugging**: A debugger for graphics (Direct3D, OpenGL, and Vulkan)
    applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CUDA debugging (Next-Gen)**: A debugger for debugging CPU and GPU code simultaneously
    (Turing, Volta, and Pascal with the latest drivers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CUDA debugging (Legacy)**: A debugger for GPU kernels only (Pascal with the
    old drivers, Maxwell, and Kepler)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance analysis**: For the analysis of the current GPU''s application
    performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CUDA memory checker**: For checking GPU memory violations during runtime
    (as covered in the previous section)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will focus on CUDA debugging (Next-Gen). This is because
    the Next-Gen debugger can support the latest architectures, including Turing and
    Volta. The CUDA memory checker will be covered at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's configure a sample project and see how we can debug the application
    using Nsight Visual Studio Edition. You may use the default sample code or replace
    the code with the previous CUDA code we covered. You can also use the given sample
    code in the `05_debug/05_debug_with_vs` file. It is some simple SAXPY code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the project properties to generate the proper device target code. In the
    project''s property page, you can specify the target code version. List the architecture
    versions you want to use in the CUDA C/C++ | Code Generation text box:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db945ffd-c8da-46ed-9229-1b098affb0ee.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows the CUDA device code's generation property page.
    You can set several `nvcc` options, such as the target GPU's compute capability,
    register limitations per thread, and CUDA kernel information that's verbose during
    compile time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Place breakpoints at line 34, where the middle of the kernel function is, and
    at line 75, where we copy data from the host to the device. Then, compile and
    start debugging using one of these methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to Nsight in the Visual Studio menu bar and click on Start CUDA Debugging
    (Next-Gen).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right-click on the project in Solution Explorer and choose Debug | Start CUDA
    Debugging (Next-Gen).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go to the Nsight CUDA debugging toolbar and click Start CUDA Debugging (Next-Gen).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Window's firewall may ask if you trust and want to allow the network connection
    of Nsight. This is normal, since Nsight uses the internal network to monitor GPU
    devices. Click *Accept* and continue the debugging. The current Nsight Visual
    Studio Edition provides two types of debugging options. It depends on the target
    GPU architecture version. If your GPU is Volta or Turing, it is recommended to
    use Next-Gen debugging. If your GPU is Pascal, the proper debugger differs, depending
    on the driver version. To clarify, please visit the supported GPU list from NVIDIA: [http://developer.nvidia.com/nsight-visual-studio-edition-supported-gpus-full-list](http://developer.nvidia.com/nsight-visual-studio-edition-supported-gpus-full-list).
  prefs: []
  type: TYPE_NORMAL
- en: The application will stop where the application starts. Continue the trace.
    The application will stop at line 75 on the host and line 34 on the device. From
    this, we can learn that the Nsight can trace a GPU application on the host and
    the device simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: When the yellow arrow stops in the kernel function, you can review the local
    variables. The thread index is `0` in global indexes. Since CUDA issues multiple
    CUDA warps and CUDA threads in parallel, you can review the other threads' local
    variables by changing `blockIdx` and `threadIdx`. The basic CUDA thread debugging
    control unit is a warp. In other words, you can control the debugger so that it
    traverses active warps. The Nsight debugger provides this feature in the Previous
    Active Warp/Next Active Warp menu in the Nsight menu bar.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the Nsight debug controls that appear while
    we''re debugging:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dcf5eef0-05d5-4e19-8c5b-8ccc28f89508.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you change the warp, you will find that the local variables that are monitored
    in the Autos panel update the index, along with the warp. For example, the following
    screenshot shows the Autos window, which reports the local variables of the selected
    thread in an active warp, that is, the local variable''s value that''s being monitored
    by the leading thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0989a8ac-17c6-4b59-822b-da1874dd3056.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Autos value is updated following the selected thread changes. The following
    screenshot shows the changes that were made by moving to the next active warp:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/384f174f-f511-412d-bc3c-8a2a020852fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Next-Gen CUDA debugger provides three types of windows—warp info, lanes,
    and GPU registers. The yellow arrow denotes current GPU execution, and its information
    is shown in three aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Warp Info window provides another way we can select an active warp. You
    can open the window from Nsight | Window | Warp Info in the menu bar. The window
    looks as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/792d956c-f385-4966-95e6-d0568f22959c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each row denotes the active warps in the CUDA grid. The fourth column, Shader
    Info, shows each warp''s block and leading thread index. The fifth column, threads,
    shows the CUDA thread''s status in the warp. The color of the cell represents
    each thread''s status. They are all red since we are watching them at the breakpoints,
    but you will see other colors during the debugging process. The following screenshot
    explains what each color means in terms of thread state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3d55d2f-75c5-4ea0-9b0a-66750ca6ef39.png)'
  prefs: []
  type: TYPE_IMG
- en: Double-click any warp to find out how the local variables in the autos window
    are updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Lanes window allows you to select specific CUDA threads within the selected
    active warp. A lane means a single thread in a warp. You can open the window from
    Nsight | Window | Lanes. By double-clicking one lane, you can find that the local
    variables in the autos window are updated according to the updated index:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/36e26550-f466-4939-947b-7577ff253123.png)'
  prefs: []
  type: TYPE_IMG
- en: The lanes winn information in an active warp.
  prefs: []
  type: TYPE_NORMAL
- en: The Registers window shows the current state of the GPU registers. They will
    be red if their value is updated.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn how to use Nsight Visual Studio Edition, please read the
    official user guide from NVIDIA. It introduces how to configure a debugging environment,
    how to use it, and many detailed tips for various situations ([https://docs.nvidia.com/nsight-visual-studio-edition/Nsight_Visual_Studio_Edition_User_Guide.htm](https://docs.nvidia.com/nsight-visual-studio-edition/Nsight_Visual_Studio_Edition_User_Guide.htm)).
  prefs: []
  type: TYPE_NORMAL
- en: Debugging a CUDA application with Nsight Eclipse Edition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For Linux and OSX platform development, the CUDA Toolkit provides Nsight Eclipse
    Edition. This tool is based on Eclipse, so that developers can easily get used
    to this tool in CUDA C development.
  prefs: []
  type: TYPE_NORMAL
- en: Nsight Eclipse Edition was built on top of Eclipse for CUDA application development.
    You can use it to edit, build, debug, and profile your CUDA applications. It makes
    CUDA C/C++ development in Linux and OSX easy. This tool is installed with the
    CUDA Toolkit as a package, so you don't have to install this tool separately.
    However, it is required to configure Java 7 for its operation if you are using
    Linux.
  prefs: []
  type: TYPE_NORMAL
- en: Nsight Eclipse Edition was built with Eclipse version 4.4.0 (Luna, released
    in 2014) and was built based on Java 7.
  prefs: []
  type: TYPE_NORMAL
- en: Nsight can be executed with the `nsight` command from a Terminal or from the
    X window application list.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's open Nsight from your Terminal or X window desktop so that we can
    compile and analyze the given example. Either create a new CUDA project or open
    the provided sample project in `05_debug/06_debug_with_eclipse`. If you want to
    create a project, select CUDA C/C++ Project. Empty Project just gives you an empty
    project, while CUDA Runtime Project gives you a project with some sample code
    inside it. If you want to use the sample project, import it using File | Import
    | Existing Projects into Workspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s place a breakpoint in the `sgemm` kernel function. Like a normal C/C++
    project in Eclipse, you can build and debug the CUDA application in `nsight`.
    Place a breakpoint at line 23 as a starting point of the kernel function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b942282b-f014-4cca-8c91-4cd000f0f6f9.png)'
  prefs: []
  type: TYPE_IMG
- en: A good starting point for kernel function debugging is right after thread index
    calculation. Place a breakpoint to halt the GPU's execution. Now, compile and
    start debugging by clicking the green bug in the menu panel. While the debug window
    switches the debugging perspectives, click continue until you get to the breakpoint
    we have placed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nsight allows you to monitor local variables and registers in active warps.
    First, it stops the application at the leading CUDA thread (CUDA thread `0`) in
    the CUDA grid. Then, you can move to the other CUDA active warp from the debug
    window and inspect each CUDA thread using the CUDA window, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85a86088-303b-4e26-9837-80e14a25fc35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the local variable information for a selected
    CUDA thread. Nsight updates those values whenever they are updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9510862e-2a8b-4fec-b0df-5b181ca0f452.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows the Debug window and CUDA window in Eclipse's
    debug perspective window. The debug window provides CUDA warp selection among
    the active warps on the selected GPU and CUDA windows and enables lane selection
    within the selected active warp.
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA also has an Nsight Eclipse Edition user guide. You can learn more about
    this tool by going to [https://docs.nvidia.com/cuda/nsight-eclipse-edition-getting-started-guide/index.html](https://docs.nvidia.com/cuda/nsight-eclipse-edition-getting-started-guide/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Debugging a CUDA application with CUDA-GDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CUDA Toolkit provides CUDA-GDB, which supports CUDA C/C++ debugging for
    programs such as C/C++ GDB. This is useful for directly debugging CUDA C/C++ applications
    where there's no X window environment or remote debugging.
  prefs: []
  type: TYPE_NORMAL
- en: To debug a GPU application, the `Makefile` should include the `-g` debugging
    flag for the host and the `-G` debugging flag for the GPU. Basically, CUDA's GDB
    usage is identical to the host debugging, except there are some extra debugging
    features alongside the CUDA operations. For example, we can set specific CUDA
    threads and CUDA-aware breakpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Breakpoints of CUDA-GDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's cover how `cuda-gdb` can help us detect errors in code. We will set breakpoints
    in the code and look at the local values on the host and the GPU. For this, move
    your working directory to `05_debug/07_debug_with_gdb directory`. We will check
    the `cuda-gdb` operation by matching it with the appropriate line.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, let’s compile the source using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we should execute `cuda-gdb` so that we can debug the application on
    the Terminal, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can place a breakpoint on a specific line in the code, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can put a breakpoint on the kernel function''s name, as follows.
    This will trigger the breakpoint at the entry point of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Answer `y` if the `cuda-gdb` warning states that *the breakpoint wants to make
    pending on future shared library load*. You can also have breakpoints on the host
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'One problem with using breakpoints is that the breakpoint will be triggered
    according to how many CUDA threads there are. Therefore, we should provide conditional
    information to have a breakpoint against specific CUDA threads. The conditional
    breakpoint is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, we can modify the condition of the predefined breakpoint as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s execute the sample application using the `run` command. If the application
    meets any breakpoint, CUDA-GDB provides information about it. The following code
    shows the `cuda-gdb` report when the application meets the breakpoint at line
    `21`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now, it's time to use the GDB command to trace the code or monitor the active
    variables. We can trace the kernel function with next (or `n`), step (or `s`),
    continue (or `c`), and finish (or `fin`). However, we should use the `continue`
    command when we get to the end of the kernel code and need to switch the target
    hardware between the host and the device.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting variables with CUDA-GDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On the top of the default GDB commands, CUDA-GDB provides debugging features
    that can work with CUDA kernels. Here's what you can do with CUDA-GDB.
  prefs: []
  type: TYPE_NORMAL
- en: Listing kernel functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like a normal function, CUDA-GDB can place breakpoints on the kernel functions.
    Once your application has been stopped by the breakpoint, you can list them as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the preceding output shows the kernel's configuration information
    and input parameter variables.
  prefs: []
  type: TYPE_NORMAL
- en: Variables investigation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CUDA-GDB helps us trace specific CUDA threads by selecting a specific thread
    block index and thread index. With this feature, you can move your current focus
    to the specified thread. In this example, the block size is 16 and the `col` variable
    is defined as a CUDA thread index in the `x` dimension. This following code shows
    how CUDA-GDB reports the selected local variable''s value by changing the thread
    index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the current focusing thread information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: With the information at hand, we can trace the CUDA thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to learn more about CUDA-GDB, please check the user guide documentation
    from NVIDIA: [https://docs.nvidia.com/cuda/cuda-gdb/index.html](https://docs.nvidia.com/cuda/cuda-gdb/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime validation with CUDA-memcheck
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One difficult point of CUDA programming is handling memory space. Since CUDA
    threads operate in parallel, the boundary condition or unexpected indexing operation
    can violate valid memory space. CUDA memcheck is a runtime testing tool that validates
    memory access if any GPU operation exceeds the invalid memory space. This tool
    detects the following memory errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Location | Description | Precise |'
  prefs: []
  type: TYPE_TB
- en: '| Memory access error | Device | Invalid memory access (out of bound, misaligned)
    | O |'
  prefs: []
  type: TYPE_TB
- en: '| Hardware exception | Device | Errors from the hardware | X |'
  prefs: []
  type: TYPE_TB
- en: '| Malloc/free errors | Device | Incorrect use of `malloc()`/`free()` in CUDA
    kernels | O |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA API errors | Host | The CUDA API''s error return | O |'
  prefs: []
  type: TYPE_TB
- en: '| cudaMalloc memory leaks | Host | Device memory that''s allocated using `cudaMalloc()`
    did not free by the application | O |'
  prefs: []
  type: TYPE_TB
- en: '| Device heap memory leaks | Device | Allocated device memory using `malloc()`
    in device code is not freed by the application | X |'
  prefs: []
  type: TYPE_TB
- en: Precise (O) means that memcheck can specify the crashed line and file. On the
    other hand, imprecise (X) means that the tool can identify the error, but cannot
    specify the error points due to the concurrency status. `cuda-memcheck` does not
    require recompilation for the test. However, if we compile with some extra `nvcc`
    options, we can trace error points. The `nvcc` options including `-lineinfo`,
    which generates line number information, and `-Xcompiler -rdynamic`, which is
    used to retain function symbols.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, `cuda-memcheck` is a standalone tool and validates GPU applications
    at runtime. The following command shows its format in standalone mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This tool can also work with CUDA-GDB and help the developer identify errors
    and debug them. In the CUDA-GDB command line, use the `set cuda memcheck on` command
    to enable memory checks. This way, CUDA-GDB can identify memory-related exceptions.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting memory out of bounds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s see how `cuda-memcheck` can detect memory exceptions and work with
    CUDA-GDB. To ease this, we will make some erroneous code and see how `cuda-memcheck`
    reports the result. Let''s begin with some clean code. You can use the given sample
    code in `05_debug/08_cuda_memcheck` for this. Let''s test the code using `cuda-memcheck`
    and validate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s put some erroneous code into the kernel function, as follows. You
    can put another error if you prefer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s compile and launch the code. The kernel will return a CUDA error and `checkCudaErrors()` will
    report an error message, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this information is insufficient if we wish to identify which line
    in the kernel code is the root cause of the problem. Using `cuda-memcheck`, we
    can identify which CUDA thread and memory space triggered the error with a stack
    address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2be60221-e0bb-4993-a55c-d90934c2cd8a.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows a part of the standalone execution of `cuda-memcheck`,
    which shows all the detected errors from the kernel where the error occurred.
    In this case, `cuda-memcheck` reports that it detected a memory violation error
    at line `27`. By default, `cuda-memcheck` stops the application's execution when
    an error is detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this situation, we can find the root cause easily by inspecting the related
    variables using `cuda-gdb`. To do this, we need to launch the application with
    `cuda-gdb` and enable `cuda-memcheck`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This procedure makes `cuda-gdb` report illegal memory access detection from
    `cuda-memcheck`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7718fc0f-e576-4c4e-9d31-2a921c5045bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding screenshot shows a report from `cuda-gdb` with `cuda-memcheck`.
    The developer can easily identify that line `27` in `simple_sgemm_oob.cu` triggered
    the reported error. From the given information, we can start to investigate which
    piece of memory accessed the invalid space, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Without arduous effort, we can determine that accessing `A[row * K + i]` triggers
    an error and that the requested value exceeds the global memory's (`A`) allocated
    space. In this manner, you can narrow down the root cause without much effort.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting other memory errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CUDA memcheck tool provides additional software validation features, some
    of which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Description** | **Option** |'
  prefs: []
  type: TYPE_TB
- en: '| Memory leak | For identifying memory leaks | `--leak-check full` |'
  prefs: []
  type: TYPE_TB
- en: '| Race check | For the analysis of the racing hazard of conflicting access
    between multiple threads to the shared memory | `--tool racecheck` |'
  prefs: []
  type: TYPE_TB
- en: '| Init check | Identifying device global memory access without initialization
    | `--tool initcheck` |'
  prefs: []
  type: TYPE_TB
- en: '| Sync check | Validates the correct use of synchronization primitives such
    as `__syncthreads()`, `__syncwarp()`, and cooperative group APIs | `--tool synccheck`
    |'
  prefs: []
  type: TYPE_TB
- en: These tools assume that the memory accesses are correct or verified and do not
    check for memory errors. Due to this, you need to confirm that no memory errors
    exist in your application. Other useful memcheck options include `--save`, which
    we can use to save the output to a disk, and `--print-level`, which we can use
    to control the output detail level.
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA provides a user guide for `cuda-memcheck`. This document will help you
    validate your application using a GPU and detect unexpected errors ([https://docs.nvidia.com/cuda/cuda-memcheck/index.html](https://docs.nvidia.com/cuda/cuda-memcheck/index.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Profiling GPU applications with Nsight Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover the newly introduced CUDA profiler tools, that
    is, Nsight Systems and Nsight Compute. These profilers support the Volta architecture
    and onwards GPUs. It is major profiler in the Turing architecture GPU. We will
    cover the Nsight Systems first, before covering Nsight Compute in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Nsight Systems ([https://developer.nvidia.com/nsight-systems](https://developer.nvidia.com/nsight-systems))
    is a system-wide performance analysis tool that can visualize operations in the
    timeline and easily find optimization points. In terms of the timeline analysis
    aspects, Nsight Systems provides system-side utilization information so that we
    can analyze the bottleneck points. We can get Nsight Systems from the NVIDIA website,
    but CUDA 10 includes Nsight Systems in the toolkit package by default. All we
    have to do is make sure it is installed correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the CLI, we should set the `PATH` to ease our operation because its path
    is separated with ordinary CUDA binaries. Let''s include that in the `PATH` environment
    variable using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Nsight Systems provides two interfaces: one for the GUI and one for the CLI.
    On a host machine, we can collect the application''s sampling information by running
    the application via a GUI. On the remote machine, we can collect the profiled
    data via a CLI with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This option can be interpreted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Option | Switches |'
  prefs: []
  type: TYPE_TB
- en: '| Tracing | `-t`/`--trace` | `cuda`: For tracing CUDA operations,`nvtx`: For
    tracing `nvtx` tags,`cublas`, `cudnn`, `opengl`,`openacc`: For tracing the API
    operation,`osrt`: For tracing OS runtime libraries,`none`: No API trace |'
  prefs: []
  type: TYPE_TB
- en: '| Output file | `-o`/`--output` | Output filename |'
  prefs: []
  type: TYPE_TB
- en: '| Show output | `-w`/`--show-`output | `true`/`false`: Prints out the behavior
    of the profiler on the Terminal |'
  prefs: []
  type: TYPE_TB
- en: 'For example, we can obtain a profiled file named `sgemm.qdrep` from the `02_nvtx`
    SGEMM application. Let''s compare the profiled output between Nsight Systems and
    the NVIDIA Visual Profiler. We can collect the Nsight System''s profile data with
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the profiled timeline view from Nsight Systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56430c7c-9262-4bd9-baac-e5f00e655e18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the profiled timeline view from the NVIDIA Visual
    Profiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52c8ca52-275f-4dd2-a7cc-d17b30f0dfc0.png)'
  prefs: []
  type: TYPE_IMG
- en: The Visual Profiler shows the operation event blocks, but Night Systems shows
    the system utilization together. Therefore, we can easily see which resource—CPU
    core, GPU, or PCIe bus—has an impact on performance. Also, Nsight Systems provides
    a more interactive profiling experience. When you double-click any function operation,
    the Nsight Systems Viewer expands the timeline to fit the window and helps us
    inspect the operation. In addition, Nsight Systems makes it easy for us to discover
    the number of kernel executions that are occurring under a certain NVTX area.
    In the Visual Profiler timeline view, the kernel executions look like a single
    execution, but Nsight Systems shows the separated execution.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have determined that a function should be optimized, we can move
    on to Nsight Compute, which is another new profiler that inspects the GPU operations
    of the kernel function.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling a kernel with Nsight Compute
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nsight Compute is a kernel-level profiler for computations. It collects GPU
    metric information and helps us focus on the CUDA kernel's optimization. In other
    words, this tool covers the Visual Profiler's performance analysis features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nsight Compute provides two interfaces: the GUI and the CLI. The GUI supports
    the host and the remote application profile, while the CLI works on the target
    machine. However, we can get the profiled data and review the results using the
    GUI.'
  prefs: []
  type: TYPE_NORMAL
- en: Profiling with the CLI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For ease of using the Nsight Compute CLI, we need to set the `PATH` environment
    variable for the Nsight Compute path in `/usr/local/cuda-10.1/NsightCompute-2019.3/nv-nsight-cu-cli`.
    Then, we can collect profile data using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: This command collects GPU execution metric information and saves the data to
    the specified file. If we don't provide an output filename, Nsight Compute reports
    the collected metric reports to the console, which provides a fast metric performance
    report over the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we can specify the profiling target, we can limit Nsight Compute to collect
    the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--kernel-regex`: Specifies the kernel to the profile'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--devices`: Focuses on profiling a specific GPU'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This feature is useful when we have to look at the report on the console.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling with the GUI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By opening a new project in Nsight Compute, we can initiate the profile operation.
    The following screenshot shows the profile configuration. For host application
    development, make a connection to the localhost. Alternatively, you can specify
    the target GPU server we want to profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56cc2622-8952-4b5f-b66b-c9f5ae257f4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Of course, we can also open the `nsight-cuprof-report` file, which was generated
    with the CLI tool over the target machine. For example, we can make the sgemm
    profiled file with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: For OSX users, Nsight Systems will require the target `glib` library for remote
    profiling. In this case, we should copy the library from the Nsight Compute installation
    image. It provides the required libraries as a directory named target and copies
    that directory to the `Applications/NVIDIA Nsight Compute.app/target` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the ease of this lab, we will use a reduction sample code from [Chapter
    3](71d77c43-0064-491e-9b43-307a05bd6555.xhtml), *CUDA Thread Programming*. It
    has two parallel reduction implementations with different addressing. You can
    find the code from `03_cuda_thread_programming/05_warp_divergence` directory.
    Click the Launch button when you finish to set the connection and application
    Executable text bar as shown in the connect to progress diagram. Then, put *Ctrl*
    + *I*, *Ctrl* + *K* key to run to next kernel function, and then the profiler
    will stop at `reduction_kernel_1`. Put *Ctrl* + *I*, *Ctrl* + *P* key to profile
    this kernel. Then you''ll get the following output. This picture shows Nsight
    Compute''s GUI-based profiling for the first kernel profiling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/970759b9-b14a-437c-a647-92e4659b6e9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Output showing GUI-based profiling (for the first kernel profiling)
  prefs: []
  type: TYPE_NORMAL
- en: It provides interactive profiling and debugging. Using the step control debug
    buttons, we can debug the CUDA API and kernel functions. We can also move to the
    next kernel function or the next profile range using the control button on the
    left side API stream panel. On the right panel, you can get the detail profiled
    information of the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: We also can get the profiled result automatically by enabling the auto profile
    with the following procedure—go to the Menu bar and select Profile | Auto Profile.
    Then, proceed with the application. Nsight Systems will profile all the kernel
    functions. Alternatively, you can profile the kernel function manually by clicking
    the Profile Kernel button on the top of the window. When we use the CLI's collected
    profiled results, we will just see profiled data from all the kernel functions.
  prefs: []
  type: TYPE_NORMAL
- en: Performance analysis report
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we could see on the right panel in the interactive profile window, the Nsight
    Compute provides a performance analysis report. From the report, we can identify
    the performance limiters and investigate the underutilized resources. Also, Nsight
    Compute provides optimization recommendations based on resource utilization statistics.
    We can also identify them from direct profile.
  prefs: []
  type: TYPE_NORMAL
- en: Also, Nsight Compute provides optimization recommendations by analyzing the
    GPU components utilizations. It finds a bottleneck and suggests a recommended
    investigation to optimize the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'This report page provides each component''s utilizations such as compute, memory,
    scheduler, instruction, warp, and so on. Furthermore, you can get even more details
    by extending the left top arrow for each component. The following picture shows
    an example report of the Memory Workload Analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21b05b61-2775-47c0-9f4c-66a009420ec4.png)'
  prefs: []
  type: TYPE_IMG
- en: In the Nsight Compute, we can get such detailed information easily. In the previous
    profiler, NVIDIA Profiler, we should execute each analysis to obtain such information.
  prefs: []
  type: TYPE_NORMAL
- en: Baseline compare
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'During the optimization process, we should compare the new result from the
    baseline operation. To make this task easy for us, Nsight Compute provides the baseline
    compare feature. Click the Add baseline button at the top of the performance report
    panel and change it to the other kernel function. Then, we can use the Nsight
    Compute to compare kernel function utilizations. The following screen shows this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2f2eadc-5230-453e-911b-038a21d64ac6.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of kernel function utilizations
  prefs: []
  type: TYPE_NORMAL
- en: This is useful if we wish to trace our optimization efforts and identify the
    effective components.
  prefs: []
  type: TYPE_NORMAL
- en: Source view
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Nsight Compute provides various pages that we can investigate. One of its useful
    pages is the Source page. If the CUDA application is built with `-lineinfo` option,
    Nsight Compute can show correlated information with CUDA C/C++ source with CUDA
    SASS code. Then, we can analyze the bottleneck code and investigate how it is
    related to the SASS code level. Also, it provides a Live Registers number so that
    we can investigate the number of required registers in the kernel function. The
    following screenshot shows the Source page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69711bc7-a6e5-461d-a97d-bdff04a62418.png)'
  prefs: []
  type: TYPE_IMG
- en: If you need to learn more about this feature, you can find the related information
    in this document—[https://docs.nvidia.com/nsight-compute/NsightCompute/index.html#profiler-report-source-page](https://docs.nvidia.com/nsight-compute/NsightCompute/index.html#profiler-report-source-page).
  prefs: []
  type: TYPE_NORMAL
- en: Nsight Compute provides a CUDA kernel performance analysis centric operation
    that we can use to verify that Night Systems and Nsight Compute have a different
    optimization scope.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered how to profile a GPU application and debug
    it. Understanding these CUDA tools will help you develop efficiently and effectively
    as they help you find bottlenecks pragmatically and find errors and bugs in a
    short time.
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we have focused on single GPU application development. However,
    many GPU applications use multiple GPUs to achieve better performance. In the
    next chapter, we will cover how to write code that works on multiple GPUs and
    aim for scalable performance. You will learn what can make an impact on performance
    and how to achieve good performance levels. You will also be able to apply the
    tools that we covered in this chapter on the next chapter's problems to reinforce
    multiple GPU systems and their experiences.
  prefs: []
  type: TYPE_NORMAL
