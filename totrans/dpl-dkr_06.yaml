- en: Advanced Deployment Topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have spent a decent amount of time talking about container communication
    and security, but in this chapter, we will take a look at taking deployments even
    further by covering the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced debugging techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing queue messaging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running security checks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container security in depth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also look at a few other tools and techiniques that will help you manage
    your deployments better.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced debugging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ability to debug containers in the wild is a very important topic and we
    previously covered some of the more basic techniques that can be of use here.
    But there are cases where `docker ps` and `docker exec` just aren't enough, so
    in this section, we will examine a few more tools you can add to your toolbox
    that can help resolve those tricky issues.
  prefs: []
  type: TYPE_NORMAL
- en: Attaching to a container's process space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There may be times when a container is running with a minimalist distribution
    such as Alpine Linux ([https://www.alpinelinux.org/](https://www.alpinelinux.org/))
    and the container in question has an issue with a process that you would like
    to debug but also lacks the most basic tooling you need for debugging included.
    By default, Docker isolates all containers in their individual process namespace
    so our current debugging workflow, which we used before by attaching to that container
    directly and trying to figure out what was wrong with very limited tooling is
    not going to help us much here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily for us though, Docker is fully capable of joining the process namespaces
    of two containers with the `docker run --pid "container:<name_or_id>"` flag, so
    that we can attach a debug tooling container directly onto the affected one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we can just attach a debugging container into the same PID namespace
    and debug any oddly behaving process this way and can keep the original container
    pristine from the installation of debug tooling! Using this technique, the original
    container can be kept small since the tooling can be shipped separately and the
    container remains running throughout the debugging process so your task will not
    be rescheduled. That said, whenever you are debugging different containers using
    this method, be careful not to kill the processes or the threads within it as
    they have a likely chance of cascading and killing the whole container, halting
    your investigation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly enough, this `pid` flag can also be invoked with `--pid host`
    to share the host''s process namespace if you have a tool that does not run on
    your distribution and there is a Docker container for it (or, alternatively, if
    you want to use a container for the management of the host''s processes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It should be apparent as to how much capability this flag's functionality can
    provide for both running and debugging applications, so do not hesitate to use
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Warning! Sharing the host's process namespace with the container is a big security
    hole as a malicious container can easily commandeer or DoS the host by manipulating
    processes, especially if the container's user is running as a root. Due to this,
    exercise extreme caution when utilizing `--pid host` and ensure that you use this
    flag only on containers you trust completely.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging the Docker daemon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If none of these techniques have helped you so far, you can try to run the Docker
    container and check what the daemon API is doing with `docker system events`,
    which tracks almost all actions that are triggered on its API endpoint. You can
    use this for both auditing and debugging, but generally, the latter is its primary
    purpose as you can see in the following example.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the first terminal, run the following command and leave it running so that
    we can see what information we can collect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'On another Terminal, we will run a new container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After you have done this start and stop of the container, the `events` command
    in the first terminal should have output something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Its use is fairly niche but this type of tracing, along with the other tips
    and tricks we have discussed so far, should provide you with the tools to tackle
    almost any type of problem on a Docker-based cluster. Everything already mentioned
    aside, in my personal experience, there have also been a couple of times where
    `gdb` was required as well as a couple of times when a problem turned out to be
    an upstream bug. Because of that, be prepared to get your hands dirty when scaling
    up as the chance of novel problems increases too.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Networking is one of the most important things for Docker clusters and it needs
    to be kept operational and running smoothly on clusters for the whole system to
    operate in any capacity. With that in mind, it stands to reason that it behooves
    us to cover a few of the topics that we have not talked about yet but that are
    important in most real-world deployments, big and small. There is a big chance
    you will encounter at least one of these use cases in your own deployments so
    I would recommend a full read-through, but your mileage may vary.
  prefs: []
  type: TYPE_NORMAL
- en: Static host configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some specific configurations, you may have a host on your network that needs
    to be mapped or re-mapped to a specific IP address for the container that is trying
    to reach it. This allows a flexible configuration of named servers and can be
    a real life-saver for static hosts on the network without a good network DNS server.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add such a host mapping to a container, you can run the container with `docker
    run --add-host` and using this flag, an entry in `/etc/hosts` is added that matches
    your input so that you can properly route your requests to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned, this can be very useful when you have a non-containerized service
    for which you do not want to hardcode the IP into the container that also is not
    resolvable from the Internet DNS servers.
  prefs: []
  type: TYPE_NORMAL
- en: DNS configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Speaking of DNS, we should probably talk a bit about Docker DNS handling. By
    default, Docker Engine uses the DNS settings from the host, but in some advanced
    deployment settings where the network that the cluster is being deployed in is
    within an already built-out network, there may be times when the engine or the
    container needs to be configured with a custom DNS setting or the DNS search prefix
    (also know as the domain name). In such cases, you are able to override the default
    DNS settings of the Docker Engine easily by adding the `dns` and/or `dns-search`
    parameters to `/etc/docker/daemon.json` and restarting the daemon. Both parameters
    allow multiple values and are pretty self-explanatory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In all networking setups that I have ever worked on, I have not seen a situation
    where overriding DNS server IPs or DNS search prefixes is a better option to deploying
    your own DHCP server within the network and setting the appropriate options for
    the DNS server(s) (`option 6`) and domain name (`option 15`), which the machine
    will pick up when initializing the network interface. If you would like to find
    out more about these DHCP flags, I would highly recommend that you visit [https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol#DHCP_options](https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol#DHCP_options)
    and read up on them before using the parameters we mentioned previously.Caution!
    In some cases where the engine host's DNS servers are pointed to `localhost` ranges,
    as they are in most `systemd-resolve` and `dnsmasq` setups, the container cannot
    access the host's `localhost` address and is thus replaced with Google's DNS servers
    (`8.8.8.8` and `8.8.4.4`) by default for all containers running on that instance.
    If you would like to retain the host's DNS setting within the container, you must
    ensure that the DNS resolver in the configuration is not one on the `localhost`
    IP range and is accessible by container networks. You can find more information
    about this at [https://docs.docker.com/engine/userguide/networking/default_network/configure-dns/](https://docs.docker.com/engine/userguide/networking/default_network/configure-dns/).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are not interested in engine-wide configuration and are only trying
    to override a single container''s DNS settings, you can do the equivalent action
    by adding `--dns` and `--dns-search` options to the `docker run` command, which
    ends up replacing the default `/etc/resolv.conf` settings in the relevant container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the settings in the container have been changed to match our
    parameters. In our case, any DNS resolution will flow to the `4.4.4.2` server
    and any unqualified hostname will first be attempted to get resolved as `<host>.domain.com`.
  prefs: []
  type: TYPE_NORMAL
- en: Overlay networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We only briefly touched on this in [Chapter 4](80265a90-781c-4bcf-80a8-945d30d22720.xhtml),
    *Scaling the Containers,* but in order to get our containers to work with the
    Swarm service discovery, we had to create this type of network though we didn't
    really spend much time explaining what it is. In the context of Docker Swarm,
    containers on one machine cannot reach containers on a different machine as their
    networks are routed directly to the next hop as they traverse the network and
    a bridge network prevents each container from reaching its neighbor on the same
    node. To hook all of the containers together in this multi-host setup seamlessly,
    you can create an overlay network that spans any Swarm nodes that are part of
    the cluster. Sadly, this type of network is only available in Docker Swarm clusters,
    so in general, it has limited portability across the orchestration tooling but
    you can create one with `docker network create -d overlay network_name`. Since
    we have already covered an example of a deployment using this type of a network
    in [Chapter 4](80265a90-781c-4bcf-80a8-945d30d22720.xhtml), *Scaling the Containers,*
    you can look it up there to see it in action.
  prefs: []
  type: TYPE_NORMAL
- en: Caution! Overlay networks do not communicate data securely by default with other
    nodes, so using the `--opt encrypted` flag when creating one is highly encouraged
    where network transport cannot be trusted fully. Using this option will incur
    some processing cost and will require you to allow port `50` communication within
    your cluster, but in most cases, it should be worth it turning it on.
  prefs: []
  type: TYPE_NORMAL
- en: Docker built-in network mappings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In previous chapters, we were mostly working with containers with the default
    network settings, which were utilizing the `bridge` network in most cases since
    that is the default, but this is not the only type of networking that can be used
    for a container. The following is a list of the available network connections,
    and almost all of them can be set through the `docker run --network` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bridge`: As mentioned in earlier chapters, this type of network creates an
    independent virtual interface on the host for communicating with the container,
    and the container can communicate with the host and the Internet. Generally, inter-container
    communication is prevented in this type of a network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`none`: Disables all networking communication for the container. This is useful
    with containers that only contain tooling and have no need for network communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host`: Uses the host''s networking stack and does not create any virtual interfaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<network_name_or_id>`: Connects to a named network. This flag is useful when
    you create a network and want to put multiple containers in the same networking
    grouping. For example, this would be useful for hooking up multiple chatty containers
    such as Elasticsearch into their own isolated network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<container_name_or_id>`: This allows you to connect to a networking stack
    of the specified container. Just like the `--pid` flag, this is very useful for
    debugging running containers without directly attaching to them, though the network
    may need to be created with the `--attachable` flag depending on the network driver
    used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning! Using the `host` networking switch gives the container full access
    to local system services and as such is a liability when used in any context other
    than testing. Use extreme caution when this flag is used, but luckily, there are
    only very few cases (if any) where there will be a legitimate use for this mode.
  prefs: []
  type: TYPE_NORMAL
- en: Docker communication ports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unless you are running Docker Swarm, you will probably never need to worry
    about what ports Docker uses to communicate, but this is something that is relatively
    good to know as a point of reference should you ever encounter such configurations
    in the field or you want to have such deployments within your clusters. The list
    is pretty short, but each port is very important for the operation of most Swarm
    clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: High availability pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we spent the majority of our time working with socket-based communication
    between nodes on a cluster, which is generally something that makes sense to most
    people and has tooling built around it in almost every programming language. So,
    it is the first tool that people transitioning their classic infrastructure to
    containers usually go for, but for large-and-beyond scales where you are dealing
    with pure data processing, it simply does not work well due to the back-pressure
    caused by exceeding the capacity of a particular stage on the rest of the processing
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you imagine each cluster service as a consecutive set of transformation
    steps, the socket-based system would go through a loop of steps similar to these:'
  prefs: []
  type: TYPE_NORMAL
- en: Opening a listening socket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Looping forever doing the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Waiting for data on a socket from the previous stage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing that data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending the processed data to the next stage's socket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But what happens in that last step if the next stage is already at the maximum
    capacity? What most socket-based systems will do is either throw an exception
    and completely fail the processing pipeline for this particular piece of data
    or prevent the execution from continuing and keep retrying to send the data to
    the next stage until it succeeds. Since we don't want to fail the processing pipeline
    as the result was not an error and we do not want to keep our worker waiting around
    for the next stage to unblock, we need something that can hold inputs to stages
    in an ordered structure so that the previous stage can continue working on its
    own new set of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Container messaging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the scenario that we just talked about, where back-pressure on individual
    processing stages causes cascade backflow stoppages, message queues (often alternatively
    referred to as pub/sub messaging systems) are here to provide us with the exact
    solution we need. Message queues generally store data as messages in a **First-In**,
    **First-Out** (**FIFO**) queue structure and work by allowing the sender to add 
    the desired inputs to a particular stage's queue ("enqueue") and the worker (listener)
    to trigger on new messages within that queue. When the worker is processing a
    message, the queue hides it from the rest of the workers and when the worker is
    complete and successful, the message is removed from the queue permanently. By
    operating on results in an asynchronous manner, we can allow the senders to keep
    working on their own tasks and completely modularize the data processing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see queues in action, let''s say we have two running containers and within
    a very short period of time, messages **A**, **B**, **C**, and **D** arrive one
    after another as inputs from some imaginary processing step (red indicating the
    top of the queue):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4ca21d49-acf4-4270-9a8b-5ebe5a31716f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Internally, the queue tracks their ordering, and initially, neither of the
    container queue listeners have noticed the messages, but very quickly, they get
    a notification that there is new work to be done, so they get the messages in
    the order in which they were received. The messaging queue (depending on the exact
    implementation) marks those messages as unavailable for other listeners and sets
    a timeout for the worker to complete. In this example **Message A** and **Message
    B** have been marked for processing by the available workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8c7c07c9-7469-4ebd-9884-94731f6384d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'During this process, let''s assume that **Container** 1 had a catastrophic
    failure and it just died. **Message A** timeout on the queue expires without it
    being finished so the queue puts it back on top and makes it available again for
    listeners while our other container keeps on working:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/23561640-e0a7-44ad-8f4a-b5076d565155.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With **Message B** successfully completed, **Container 2** notifies the queue
    that the task is done and the queue removes it completely from its lists. With
    that out of the way, the container now takes the topmost message, which turns
    out to be the unfinished **Message A** and the process continues just like before:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/97d75585-51a9-4eef-bdf8-9bb82855def0.png)'
  prefs: []
  type: TYPE_IMG
- en: While this cluster stage has been dealing with failures and overloading, the
    previous stage that put all of these messages in the queue kept working on its
    dedicated workload. Our current stage also has not lost any data even though half
    of our processing capability got forcefully removed at a random point in time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new pseudocode loop for a worker would be a bit more like this now:'
  prefs: []
  type: TYPE_NORMAL
- en: Register as a listener on a queue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loop forever doing the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wait for a message from the queue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process the data from the queue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Send the processed data to the next queue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this new system, if there is any kind of processing slowdown in the pipeline,
    the queues for those overloaded stages will start to grow in size, but if the
    earlier stages slow down, the queues will shrink until they are empty. As long
    as the maximum queue size can handle the volume of messages and the overloaded
    stages can handle the average demands, you can ascertain that all the data that
    is in the pipeline will be eventually processed and your triggers for scaling
    up stages are pretty much as simple as noticing larger queues that are not caused
    by bugs. This not only helps mitigate differences in pipeline stage scaling, but
    it also helps preserve data if pieces of your cluster go down since the queues
    will grow during failure time and then empty as you bring your infrastructure
    back to fully working - and all of this will happen without data loss.
  prefs: []
  type: TYPE_NORMAL
- en: If this bundle of benefits was not enough of a positive, consider that you can
    now have a guarantee that the data was processed since the queue keeps the data
    around so if a worker dies, the queue will (as we've seen earlier) put the message
    back in the queue to possibly get processed by another worker, unlike socket-based
    processing which would just silently die in that case. The increase in processing
    density, increase in failure tolerance, and better handling of burst data makes
    queues extremely attractive to container developers. If all your communication
    is also done with queues, service discovery might not even be needed for these
    workers except to tell them where the queue manager is since the queue is doing
    that discovery work for you.
  prefs: []
  type: TYPE_NORMAL
- en: Unsurprisingly, most queues come at a development cost, which is why they are
    not as widely in use as one might expect. In most cases, you will not only need
    to add custom queue client libraries to your worker code, but in many types of
    deployments, you will also need a process or a daemon somewhere that will be the
    main queue arbitrator that handles the messages. In fact, I would probably go
    as far as to say that choosing the messaging system alone is a research task onto
    itself, but if you're looking for quick answers, generally Apache Kafka ([https://kafka.apache.org/](https://kafka.apache.org/)),
    RabbitMQ ([https://www.rabbitmq.com/](https://www.rabbitmq.com/)), and Redis-backed
    custom implementations ([https://redis.io/](https://redis.io/)) seem to be more
    popular in clustering contexts for in-house messaging queues going from the biggest
    deployments to the smallest, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: As with all things we have been covering so far, most cloud providers offer
    some type of service for this (AWS SQS, Google Cloud Pub/Sub, Azure Queue Storage,
    and so on) so that you don't have to build it yourself. If you are OK with spending
    a little bit more money, you can utilize these and not worry about hosting the
    daemon process yourself. Historically, messaging queues have been hard to maintain
    and manage properly in house, so I would venture to say that many, if not most,
    cloud systems use these services instead of deploying their own.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing our own messaging queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the theory out of the way, let's see how we can build our own little queue
    publisher and listener. For our example here, we will use one of the simpler messaging
    systems based on Redis called `bull` ([https://www.npmjs.com/package/bull](https://www.npmjs.com/package/bull)).
    First we will write the code that will run this whole system, and to make things
    easy for us, we will use the same image for both the consumer and the producer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a new directory, create the following:'
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, this code is also in the GitHub repository and you can view it
    or clone it from [https://github.com/sgnn7/deploying_with_docker/tree/master/chapter_6/redis_queue](https://github.com/sgnn7/deploying_with_docker/tree/master/chapter_6/redis_queue)
    if you do not want to type the full text.
  prefs: []
  type: TYPE_NORMAL
- en: package.json
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This file is pretty much just a copy of our older example with the addition
    of the `bull` package and a name change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: index.js
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`index.js` is a single-file app that either sends a timestamp every 1.5 seconds
    to the queue or reads from the queue depending on the invocation argument. The
    queue location is defined by the `QUEUE_HOST` environment variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Dockerfile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Nothing special here: the file is pretty much a trimmed-down version of our
    older Node.js app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will build the image now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'With the image building out of the way, we can now write out our stack definition
    file: `swarm_application.yml`. We are pretty much creating the queue server, the
    queue listener, and the queue sender on a single network and making sure that
    they can find each other here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Having both image built and the stack definition, we can launch our queue cluster
    to see whether it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we could add any number of senders and listeners (within reason)
    and our system will work just fine in a very asynchronous style, increasing throughput
    at both ends. As a reminder, though, if you decide to go this route, another queue
    type is highly advised (Kafka, SQS, and so on) but the underlying principles are
    pretty much the same.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered some security issues in previous chapters, but for some issues
    that seem to be frequently ignored, we need to cover them with a little bit more
    depth than the small info box in the middle of the text and see why they are such
    large issues when used improperly. While it might seem like a lot of work to implement
    all the things we pointed out in various warnings and info boxes, the smaller
    the attack surface you provide to your potential intruders, the better you will
    be in the long run. That said, unless you are working on deploying this system
    for a government agency, I expect that there will be some compromises but I urge
    you to strongly weigh the pros and cons for each otherwise you risk getting that
    dreaded midnight call about an intrusion.
  prefs: []
  type: TYPE_NORMAL
- en: Ironically, hardened systems usually take so much time to develop and deploy
    that they are often obsolete or provide marginal business value by the time they
    are in production environments, and due to their carefully assembled pieces, they
    are rarely (if ever) updated with a newer functionality, have patches applied
    to them quickly, or code improvements done on the source so it is a truly a double-edged
    sword. There is *never* a perfect solution, only a range of things you are comfortable
    with to some degree of dealing with. Historically, I have mostly seen horrible
    execution on either extremes of the fence so my advice here is that you look for
    a blend of the two if possible.
  prefs: []
  type: TYPE_NORMAL
- en: Mounting the Docker socket into the container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is by far the most egregious security hole that developers completely disregard
    when deploying containerized solutions. For various things related to container
    management, often advice on the Internet is generally leaning toward bind-mounting
    the Docker socket (`/var/run/docker.sock`) into the container, but the thing rarely
    mentioned is effectively giving the host's root-level access to such a container
    when you do this. Since the Docker's socket is actually just an API endpoint and
    the Docker daemon runs as the root, the container can simply escape its containment
    by launching other containers with the host's system folders being mounted on
    them and then executing arbitrary commands on them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on using the Docker socket as a RESTful endpoint, you
    can take a look at the source code or explore a bit through the documentation
    for Docker Engine API at [https://docs.docker.com/engine/api/v1.31/](https://docs.docker.com/engine/api/v1.31/).
    The only thing you will generally need to do to use it through a tool such as
    `curl` is to add `--unix-socket <socket_path>` and, optionally `-H "Content-Type:
    application/json"` for `POST` requests.Docker has been making strides at turning
    its service into a userspace one from a root-level one, but so far, this feature
    has not materialized in any practical manner. While personally I have reservations
    about this happening anytime soon, keep an eye out for this feature as at some
    point it may actually get released and become a usable feature which would be
    a huge step forward for container security.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the theory of how to misuse the Docker socket, now we will break out of
    our container though we will stop short of actually doing anything damaging to
    the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: It should be apparent now how the benign container was able to root the host
    with just a few CLI commands. While some of it is predicated on the container
    process running as the root, the same could possibly be done if the Docker group
    ID clashes with a non-privileged group within the container, but with nitpicks
    aside, suffice it to say that mounting the Docker socket without fully understanding
    the implications can lead to a very painful breach. With that in mind, there are
    (albeit rare) legitimate uses of this technique so use your best judgment here.
  prefs: []
  type: TYPE_NORMAL
- en: Host security scans
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As part of a drive to increase the security of deployments, a tool was released
    by Docker that can help easily identify the most common security issues with a
    host running a Docker Engine called **Docker Bench for Security**. This tool will
    scan and verify a large number of possible weaknesses in your configuration and
    will present them in a very easy-to-read listing. You can download and run this
    image just like you would one of the other regular containers available on Docker
    Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: Warning! This security scan requires many permissions (`--net host`, `--pid
    host`, Docker socket mounting, and so on) that we have covered as generally really
    bad ideas to run on a host since they present a pretty large attack vector for
    malicious actors but on the other hand, the scan needs those permissions to check
    the settings you have. As such, I would highly recommend running this type of
    security scan on a clone of the host machine that you are trying to test in a
    network-isolated environment in order to prevent compromises of your infrastructure
    if the scanning image is maliciously modified.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The list is pretty long, so most of the output lines were removed, but you should
    have a pretty good idea about what this tool does and how to use it. Note that
    this is not the only product in this space (e.g. Clair from CoreOS at [https://github.com/coreos/clair](https://github.com/coreos/clair))
    so try to use as many of them as you can in order to see where your weaknesses
    in the infrastructure lie.
  prefs: []
  type: TYPE_NORMAL
- en: Read-only containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the development of our previous examples spanning most of the chapters,
    we did not really pay much attention to whether containers changed the state of
    the filesystem while running. This is not such a problem for test and development
    systems, but in production, it is very important to lock things down even further
    in order to prevent malicious runtime exploitation from both internal and external
    sources. For this purpose, there is a `docker run --read-only` flag, which (unsurprisingly)
    mounts the container''s root filesystem as read-only. By doing this, we ensure
    that all data that is not mounted with volumes is as pristine as when we built
    the image, ensuring consistency and protecting your cluster. The only thing that
    you might need to be careful of if you run the containers in this manner is that
    locations for temporary storage of files in places such as `/run`, `/tmp`, and
    `/var/tmp` are extremely likely to be required by the container during execution,
    so these mounts should be additionally mounted as `tmpfs` volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: If you do not expect your container to change anything on the filesystem and
    since containers should generally not need to write to paths such as `/usr`, using
    this flag in production is highly recommended, so apply it liberally to all your
    static services if possible.
  prefs: []
  type: TYPE_NORMAL
- en: Base system (package) updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We talked a little about this previously, but it seems that in most online documentation
    and blogs, package updates have been sorely neglected in coverage within the context
    of Docker containers. While there are supporters of both camps, it is important
    to remember that there is no guarantee that the tagged images available from places
    such as Docker Hub have been built with the latest updates, and even in cases
    where they are, the tagged image might have been built a while ago and, as such,
    won't contain the latest security patches.
  prefs: []
  type: TYPE_NORMAL
- en: While it is true that within Docker containers, the host's kernel is used to
    run the context of the container, a security hole in any of the supporting libraries
    within the container can (and usually does) result in a breach that can often
    cascade onto the host and into your whole network. Due to this fact, my personal
    recommendation for containers that will be deployed to production is that you
    should always make sure that the container is built with the latest libraries
    if possible. There are definite risks, albeit small, in manually upgrading packages
    on some base images that are caused by library incompatibilities that occur when
    you do the upgrade, but as a general rule, it is a risk worth taking.
  prefs: []
  type: TYPE_NORMAL
- en: 'In most cases, in order to do this kind of upgrade, just like we covered earlier
    in most of our Docker examples, you pretty much need to invoke the system upgrade
    lines specific to the base OS distribution of the image in `Dockerfile`. For our
    default deployment OS (Ubuntu LTS), this operation is done with `apt-get update`
    and `apt-get dist-upgrade`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Caution! Do not forget that by default, `docker build` will cache all individual
    layers that have unchanged `Dockerfile` directives, so this command will work
    as expected the first time, but its layer will be pulled from the cache any subsequent
    time it is used if none of the lines preceding it have changed due to the fact
    that this line will stay the same regardless of packages changing upstream. If
    you want to ensure that you get the latest updates, you will have to break the
    cache either by changing a line above `apt-get` in your `Dockerfile` or by adding
    `--no-cache` to your `docker build` command. Also, note that using `--no-cache`,
    all layers will be regenerated, possibly causing a prolonged build cycle and/or
    registry disk use.
  prefs: []
  type: TYPE_NORMAL
- en: Privileged mode versus --cap-add and --cap-drop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some advanced things that you might want to do within a container, such as
    **Docker-in-Docker (DinD)**, NTP, mounting loopback devices, and many others,
    will require higher privileges than the ones given to the root user of the container
    by default. As such, additional privileges need to be allowed for the container
    to run without issues, so for that use case, Docker has a very simple but extremely
    broad privileged mode that adds the complete host''s capabilities to the container.
    To use this mode, just append `--privileged` to the `docker run` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Docker-in-Docker** (commonly known as **DinD**) is a special configuration
    of a container that allows you to run the Docker Engine within the container that
    is already running on a Docker Engine but without sharing the Docker socket, which
    allows (if precautions are taken) a more secure and robust way to build containers
    within your infrastructure that is already containerized. The prevalence of this
    configuration is somewhat rare but is very powerful when used as part of a **Continuous
    Integration** (**CI**) and **Continuous Delivery** (**CD**) setup.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, adding this flag removes all errors from the output as we can
    now change the system time.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the functionality of this mode explained, we can now talk about why ideally,
    if possible, you should never use the privileged mode. By default, the privileged
    mode allows practically full access to most of the host''s systems and is not
    granular enough to make sense in most circumstances, so after you figure out that
    your container needs additional privileges, you should selectively add them with
    `--cap-add` instead. These flags are standard Linux capability identifiers that
    you can find in places such as [http://man7.org/linux/man-pages/man7/capabilities.7.html](http://man7.org/linux/man-pages/man7/capabilities.7.html)
    and allow fine-tuning to the level of access you desire. If we now convert our
    previous NTP daemon example into this new style, it should look a bit more like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If you noticed, we still have an error visible due to another missing capability,
    but the `settimeofday` error is gone, which is the most important problem that
    we needed to fix for this container to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly enough, we can also drop capabilities from our container that
    are not being used with `--cap-drop` if we want to increase security. For this
    flag, there is also a special keyword, `ALL`, that can be used to drop all available
    privileges. If we use this to fully lock down our NTP container but have everything
    working, let us see what that will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have removed all capabilities first and then added back the few that
    are really needed to run the container, and as you can see, things are working
    just fine. In your own deployments, I would strongly suggest that if you have
    spare development capacity or are security-oriented, take some time to lock your
    running containers in this manner as they will be much more secure and you will
    be much more sure that the container is running with the principle of least privilege.
  prefs: []
  type: TYPE_NORMAL
- en: The **Principle of Least Privilege** is a concept in computer security where
    you allow only the minimal privileges needed to run a component to the user or
    a service. This principle is very much a staple of high-security implementations
    but is often not found elsewhere due to the assumed overhead of managing the access
    even though it is a great way to increase the security and stability of your systems.
    If you would like to find out more about this concept, you should definitely make
    your way to [https://en.wikipedia.org/wiki/Principle_of_least_privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege)
    and check it out.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have learned many advanced tools and techniques needed
    to deploy robust clusters, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Additional debugging options to manage container issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep dives into Docker's advanced networking topics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing our own queue messaging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various security hardening tips and tricks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these topics combined with previous material should cover the gamut of
    deployment needs for most clusters. But in the next chapter, we will see what
    issues we need to worry about when the number of hosts, services, and tasks reaches
    levels that aren't generally expected and we start seeing the clusters fall apart
    and what we can do to mitigate such problems.
  prefs: []
  type: TYPE_NORMAL
