- en: Chapter 6. Monitoring and Backups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will take a look at the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Signing up for MMS and setting up an MMS monitoring agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing users and groups in MMS Console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring instances and setting up alerts in MMS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up monitoring alerts in MMS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Back up and restore data in Mongo using out-of-the-box tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring MMS Backup service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing backups in MMS Backup service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring and backup is an important aspect of any mission-critical software
    in production. Monitoring proactively lets us take action whenever an abnormal
    event occurs in the system that can compromise data consistency, availability,
    or the performance of the system. Issues may come to light after having a significant
    impact in the absence of monitoring the systems proactively. We covered administration-related
    recipes in [Chapter 4](ch04.html "Chapter 4. Administration"), *Administration*,
    and both these activities are part of it; however, they demand a separate chapter
    as the content to be covered is extensive. In this chapter, we will see how to
    monitor various parameters and set up alerts for various parameters of your MongoDB
    cluster using the **Mongo Monitoring Service** (**MMS**). We will look at some
    mechanisms to backup data using the out-of-the-box tools and also using the MMS
    backup service.
  prefs: []
  type: TYPE_NORMAL
- en: Signing up for MMS and setting up an MMS monitoring agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MMS is a cloud-based or on-premise service that enables you to monitor your
    MongoDB cluster. The on-premise version is available with an enterprise subscription
    only. It gives you one central place that lets the administrators monitor the
    health of the server instances and the boxes on which the instances are running.
    In this recipe, we will see what the software requirements are and how to set
    up MMS for Mongo.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be starting a single instance of `mongod`, which we will use for monitoring
    purposes. Refer to the recipe *Installing single node MongoDB* from [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* to start a MongoDB instance and connect to it from a Mongo shell. The
    monitoring agent used for sending the statistics of the mongo instance to the
    monitoring service uses Python and pymongo. Refer to the recipe *Connecting to
    a single node using a Python client* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* to learn more
    about how to install Python and pymongo, the Python client of MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you don''t already have a MMS account, then log in to [https://mms.mongodb.com/](https://mms.mongodb.com/)
    and sign up for an account. On signing up and logging in, you should see the following
    page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/B04831_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Click on the **Get Started** button under **Monitoring**.
  prefs: []
  type: TYPE_NORMAL
- en: Once we reach the **Download Agent** option in the menu, click on the appropriate
    OS platform to download the agent. Follow the instructions given after selecting
    the appropriate OS platform. Note down the **apiKey** too. For example, if the
    Windows platform is selected, we would see the following:![How to do it…](img/B04831_06_02.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the installation is complete, open the file `monitoring-agent.config`.
    It will be present in the configuration folder selected while installing the agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look out for the key `mmsApiKey` in the file and set its value to the API key
    that was noted down in step 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the service is started (we have to go to `services.msc` on Windows, which
    can be done by typing `services.msc` in the run dialog (Windows + *R*) and start
    the service manually). The service would be named **MMS Monitoring Agent**. On
    the web page, click on the **Verify Agent** button. If all goes well, the started
    agent will be verified and a success message will be shown.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to configure the host. This host is the one that is seen from
    the agent's perspective running on the organization or individual's infrastructure.
    The following screen shows the screen used for the addition of a host. The hostname
    is the internal hostname (the hostname on the client's network), and the MMS on
    the cloud doesn't need to reach out to the MongoDB processes. It is the agent
    that collects the data from these mongodb processes and sends the data to the
    MMS service.![How to do it…](img/B04831_06_03.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the host's details are added, click on the **Verify Host** button. Once
    verification is done, click the **Start Monitoring** button.
  prefs: []
  type: TYPE_NORMAL
- en: We have successfully set up MMS and added one host to it that will be monitored.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we set up an MMS agent and monitoring for a standalone MongoDB
    instance. The installation and setup process is pretty simple. We also added a
    standalone instance and all was okay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a replica set up and running (refer to the recipe *Starting
    multiple instances as part of a replica set* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server*, for more details
    on how to start a replica set) and the three members are listening to ports `27000`,
    `27001`, and `27002`. Refer to point number 6 in the *How to do it…* section where
    we set up one standalone host. In the drop-down menu for **Host Type** select
    **Replica Set** and in the **Internal hostname**, give a valid hostname of any
    member of the replica set (in my case **Amol-PC** and port **27001** were given,
    which is a secondary instance); all other instances will be automatically discovered
    and will be visible under the hosts, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We didn't see what is to be done when security is enabled on the cluster, which
    is pretty common in production environments and we have replica sets or shard
    setup. If authentication is enabled, we need proper credentials for the MMS agent
    to gather the statistics. The **DB Username** and **DB Password** that we give
    while adding a new host (point number 6 in the *How to do it…* section) should
    have a minimum of `clusterAdmin` and `readAnyDatabase` role.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What we saw in this recipe was how to set up MMS agent and create an account
    from the MMS console. However, we can add groups and users for the MMS console
    as an administrator granting various users privileges for performing various operations
    on different groups. In the next recipe, we will throw some light on user and
    group management in the MMS console.
  prefs: []
  type: TYPE_NORMAL
- en: Managing users and groups in MMS console
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we saw how to set up an MMS account and set up an MMS
    agent. In this recipe, we will throw some light on how to set up the groups and
    user access to the MMS console.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the previous recipe for setting up the agent and MMS account. This
    is the only prerequisite for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by going to **Administration** | **Users** on the left-hand side of the
    screen, as shown here:![How to do it…](img/B04831_06_06.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, you can view the existing users and also add new users. On clicking the
    **Add User** (encircled in the top right corner of the preceding image) button,
    you should see the following popup window that allows you to add a new user:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/B04831_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screen will be used to add users. Take a note of the various available
    roles.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, go to **Administration** | **My Groups** to view and add new groups
    by clicking on the **Add Group** button. In the text box, type the name of the
    group. Remember that the name of the group you enter should be available globally.
    The given name of the group should be unique across all users of MMS and not just
    your account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When a new group is created, it will be visible in the top left corner in a
    drop-down menu for all the groups, as shown here:![How to do it…](img/B04831_06_07.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can switch between the groups using this drop-down menu, which should show
    all the details and stats relevant to the selected group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that a group once created cannot be deleted. So be careful while creating
    one.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tasks we did in the recipe are pretty straightforward and don't need a lot
    of explanation except for one question. When and why do we add a group? It is
    when we want to segregate our MongoDB instances by different environments or applications.
    There will be a different MMS agent running for each group. Creating a new group
    is necessary when we want to have separate monitoring groups for different environments
    of an application (Development, QA, Production, and so on), and each group has
    different privileges for the users. That is, the same agent cannot be used for
    two different groups. While configuring the MMS agent, we give it an API key unique
    to the group. To view the API key for the group, select the appropriate group
    from the drop-down menu at the top of the screen (if your user has access to only
    group, the drop-down menu won't be seen) and go to **Administration** | **Group
    Settings** as shown in the next screenshot. The **Group ID** and the **API Key**
    will both be shown on the top of the page.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that not all user roles will see this option. For example, read-only users
    can only personalize their profile and most of the other options will not be visible.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring instances and setting up alerts on MMS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous couple of recipes showed us how to set up an MMS account, set up
    an agent, add hosts, and manage user access to MMS console. The core objective
    of MMS is monitoring the host instances, which has not been discussed yet. In
    this recipe, we will perform some operations on the host that we added to MMS
    in the first recipe and monitor it from the MMS console.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Follow the recipe *Signing up for MMS and setting up an MMS monitoring agent*,
    and that is pretty much all that is needed for this recipe. You may choose to
    have a standalone instance or a replica set, either way is fine. Also, open a
    mongo shell and connect to the primary instance from it (it is a replica set).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by logging into MMS console and clicking on **Deployment** on the left.
    Then, click on the **Deployment** link in the submenu again, as shown in the following
    screenshot:![How to do it…](img/B04831_06_10.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on one of the hostnames to see a large variety of graphs showing various
    statistics. In this recipe, we will analyze the majority of these.
  prefs: []
  type: TYPE_NORMAL
- en: Open the bundle downloaded for the book. In [Chapter 4](ch04.html "Chapter 4. Administration"),
    *Administration*, we used a JavaScript to keep the server busy with some operations
    named `KeepServerBusy.js`. We will be using the same script this time around.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the operating system shell, execute the following with the `.js` file in
    current directory. The shell connects to port `27000` in my case for the primary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once it's started, keep it running and give it some 5 to 10 minutes before you
    start monitoring the graphs on MMS console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html "Chapter 4. Administration"), *Administration*, we
    saw a recipe, *The mongostat and mongotop utilities* that demonstrated how these
    utilities can be used to get the current operations and resource utilization.
    That is a fairly basic and helpful way to monitor a particular instance. MMS,
    however, gives us one place to monitor the MongoDB instance with pretty easy-to-understand
    graphs. MMS also gives us historical stats, which `mongostat` and `mongotop` cannot
    give.
  prefs: []
  type: TYPE_NORMAL
- en: Before we go ahead with the analysis of the metrics, I would like to mention
    that in the case of MMS monitoring, the data is not queried nor sent out over
    the public network. It is just the statistics that are sent over a secure channel
    by the agent. The source code for the agent is open source and is available for
    examination if needed. The mongod servers need not be accessible from the public
    network as the cloud-based MMS service never communicates with the server instances
    directly. It is the MMS agent that communicates with the MMS service. Typically,
    one agent is enough to monitor several servers unless you plan to segregate them
    into different groups. Also, it is recommended to run the agent on a dedicated
    machine/virtual machine and not share it with any of the mongod or mongos instances
    unless it is a less crucial test instance group you are monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see some of these statistics on the console; we start with the memory-related
    ones. The following graph shows the resident, mapped, and virtual memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the resident memory for the data set is 82 MB, which is very
    low and it is the actual physical memory used up by the mongod process. This current
    value is significantly below the free memory available and generally this will
    increase over a period of time till it reaches a point where it has used up a
    large chunk of the total physical available memory. This is automatically taken
    care of by the mongod server process, and we can't force it to use up more memory
    even though it is available on the machine it is running on.
  prefs: []
  type: TYPE_NORMAL
- en: The mapped memory, on other hand, is about the total size of the database and
    is mapped by MongoDB. This size can be (and usually is) much higher than the physical
    memory available, which enables the mongod process to address the entire dataset
    as it is present in memory even if it isn't. MongoDB offloads the responsibility
    of mapping and loading data to and from disk to the underlying operating system.
    Whenever a memory location is accessed and it is not available in the RAM (that
    is, the resident memory), the operating system fetches the page into memory, evicting
    some pages to make space for the new page if necessary. What exactly is a memory
    mapped file? Let's try to see with a super scaled down version. Suppose we have
    a file of 1 KB (1024 bytes) and the RAM is only 512 bytes, then obviously we cannot
    have the whole file in memory. However, you can ask the operating system to map
    this file to available RAM in pages. Suppose each page is 128 bytes, then the
    total file is 8 pages (128 * 8 = 1024). But the OS can only load four pages, and
    we assume it loaded the first 4 pages (up to 512 bytes) in the memory. When we
    access byte number 200, it is okay and found in the memory as it is in present
    on page 2\. But what if we access byte 800, which is logically on page 7 that
    is not loaded in memory? What OS does is takes one page out from the memory and
    loads this page 7 containing byte number 800\. MongoDB as an application gives
    the impression that everything was loaded in the memory and was accessed by the
    byte index, but actually it wasn't and the OS transparently did the work for us.
    Since the page accessed was not present in the memory and we had to go to the
    disk to load it in the memory it is called a **page fault**.
  prefs: []
  type: TYPE_NORMAL
- en: Getting back to the stats shown in the graph, the virtual memory contains all
    the memory usage including the mapped memory plus any additional memory used,
    such as the memory associated with the thread stack associated with each connection.
    If journaling is enabled, this size will definitely be more than twice than that
    of the mapped memory as journaling too will have a separate memory mapping for
    the data. Thus, we have two addresses mapping the same memory location. This doesn't
    mean that the page will be loaded twice. It just means that two different memory
    locations can be used to address the same physical memory. Very high virtual memory
    might need some investigation. There is no predetermined definition of a too high
    or low value; generally these values are monitored for your system under normal
    circumstances when you are happy with the performance of your system. These benchmark
    values should then be compared with the figures seen when system performance goes
    down and then appropriate action can be taken.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw earlier, page faults are caused when an accessed memory location is
    not present in the resident memory, causing the OS to load the page from memory.
    This IO activity will definitely cause performance to go down and too many page
    faults can bring down database performance dramatically. The following screenshot
    shows quite a few page faults happening per minute. However, if the disk used
    is an SSD instead of spinning disks, the hit in terms of seek time from the drive
    might not be significantly high.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A large number of page faults usually occur when there isn't enough physical
    memory to accommodate the data set and the OS needs to get the data from the disk
    into memory. Note that this stat shown in the preceding screenshot is taken on
    a Windows platform and might seem high for a very trivial operation. This value
    is the sum of hard and soft page faults and doesn't really give a true figure
    of how good (or bad) the system is. These figures would be different on a Unix-based
    OS. There is a JIRA ([https://jira.mongodb.org/browse/SERVER-5799](https://jira.mongodb.org/browse/SERVER-5799))
    open as of the writing of this book which reports this problem.
  prefs: []
  type: TYPE_NORMAL
- en: One thing you might need to remember is that in production systems, MongoDB
    doesn't work well with a NUMA architecture and you might see a lot of page faults
    happening even if the available memory seems to be high enough. Refer to the URL
    [http://docs.mongodb.org/manual/administration/production-notes/](http://docs.mongodb.org/manual/administration/production-notes/)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an additional graph which gives some details about non-mapped memory.
    As we saw earlier in this section, there are three types of memory: mapped, resident,
    and virtual. Mapped memory is always less than virtual memory. Virtual memory
    will be more than twice that of mapped memory if journaling is enabled. If we
    look at the image given in this section earlier, we see that the mapped memory
    is 192 MB whereas the virtual memory is 532MB. Since journaling is enabled, the
    memory is more than twice that of the mapped memory. When journaling is enabled,
    the same page of data is mapped twice in memory. Note that the page is physically
    loaded only once, it is just that the same location can be accessed using two
    different addresses. Let''s find the difference between the virtual memory, which
    is 532MB, and twice the mapped memory that is 384 MB (2 * 192 = 384). The difference
    between these figures is 148 MB (532 - 384).'
  prefs: []
  type: TYPE_NORMAL
- en: What we see here is the portion of virtual memory that is not mapped memory.
    This value is the same as what we just calculated.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As mentioned earlier, a high or low value for non-mapped memory is not defined,
    however when the value reaches GBs we might have to investigate; possibly the
    number of open connections is high and we need to check if there is a leak with
    client applications not closing them after using it. There is a graph that gives
    us the number of connections open and it looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Once we know the number of connections and find it too high as compared to the
    expected count, we will need to find the clients who have opened the connections
    to that instance. We can execute the following JavaScript code from the shell
    to get those details. Unfortunately, at the time of writing this book, MMS doesn't
    have this feature to list out the client connection details.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `db.currentOp` method returns all the idle and system operations in the
    result. We then iterate through all the results and print out the client host
    and the connection details. A typical document in the result of the `currentOp`
    looks like this. You can choose to tweak the preceding code to include more details
    as per your need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In [Chapter 4](ch04.html "Chapter 4. Administration"), *Administration*, we
    saw a recipe, *The mongostat and mongotop utilities* that was used to get some
    details on the percent of time a database was locked and the number of update,
    insert, delete, and getmore operations executed per second. You may refer to these
    recipes and try them out. We had used the same JavaScript that we have used currently
    to keep the server busy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In MMS console, we have graphs giving these details as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first one, `opcounters`, shows the number of operations executed at a particular
    point in time. This should be similar to what we saw using the `mongostat` utility.
    The one on the right shows us the percentage of time a database was locked. The
    drop-down menu lists the database names. We can select an appropriate database
    that we want to see the stats for. Again, this statistic can be seen using the
    `mongostat` utility. The only difference is that with the command-line utility,
    we see the stats as of the current time, whereas here we see the historical stats
    too.
  prefs: []
  type: TYPE_NORMAL
- en: In MongoDB, indexes are stored in BTrees and the next graph shows the number
    of times the BTree index was accessed, hit, and missed. At the minimum, the RAM
    should be enough to accommodate the indexes for optimum performance. So in this
    metric, the misses should be 0 or very low. A high number of misses results in
    a page fault for the index and possibly additional page faults for the corresponding
    data if the query is not covered, that is, all its data cannot be sourced from
    the index, which is a double blow for performance. One good practice while querying
    is to use projections and fetch only the necessary fields from the document. This
    is helpful whenever we have our selected fields present in an index, in which
    case the query becomes covered and all the necessary data is sourced from the
    index only. To learn more about on covered indexes, refer to the recipe *Creating
    index and viewing plans of queries* in [Chapter 2](ch02.html "Chapter 2. Command-line
    Operations and Indexes"), *Command-line Operations and Indexes*.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For busy applications if the volumes are very high, with multiple write and
    read operations contending for lock, the operations queue up. Untill Version 2.4
    of MongoDB, the locks are at the database level. Thus, even if the writes are
    happening on another collection, read operations on any collection in that database
    will block. This queuing operation affects the performance of the system and is
    a good indicator that the data might need to be sharded across to scale the system.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember, no value is defined as high or low; it is the acceptable value on
    an application-to-application basis.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: MongoDB flushes the data from the journal immediately and the data file periodically
    to disk. The following metrics give us the flush time per minute at a given point
    of time. If the flush takes up a significant percentage of the time per minute,
    we can safely say that the write operations are forming a bottleneck for performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen monitoring of the MongoDB instances/cluster in this recipe. However,
    setting up alerts to get notifications when certain threshold values are crossed
    is what we still haven't seen. In the next recipe, we will see how to achieve
    this with a sample alert that is sent out over an e-mail when the page faults
    exceed a predetermined value.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monitoring the hardware, such as CPU usage, is pretty useful and MMS console
    does support that. However, it needs munin-node to be installed to enable CPU
    monitoring. Refer to the page [http://mms.mongodb.com/help/monitoring/configuring/](http://mms.mongodb.com/help/monitoring/configuring/)
    for setting up munin-node and hardware monitoring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For updating the monitoring agent, refer to the page [http://mms.mongodb.com/help/monitoring/tutorial/update-mms/](http://mms.mongodb.com/help/monitoring/tutorial/update-mms/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up monitoring alerts in MMS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we saw how to monitor various metrics from MMS console.
    This is a great way to see all the stats in one place and get an overview of the
    health of the MongoDB instances and cluster. However, it is not possible to monitor
    the system continuously, 24/7, for the support personnel and there has to be some
    mechanism to automatically send out alerts in case some threshold is exceeded.
    In this recipe we will set up an alert whenever the page faults exceeds 1000.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the previous recipe to set up Monitoring Mongo Instances using MMS.
    That is the only prerequisite for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Click on the **Activity** option on the left side menu, and then **Alert Settings**.
    On the **Alert Settings** page, click on **Add Alert**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a new alert for the **Host** that is a primary instance and if the page
    faults exceed a given number, which is 1000 page faults per minute. The notification
    is chosen to be an e-mail in this case and the interval after which the alert
    will be sent is 10 minutes.![How to do it…](img/B04831_06_20.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Save** to save the alert.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The steps were pretty simple and we were successful in setting up MMS alerts
    when the page faults exceeded 1000 per minute. As we saw in the previous recipe,
    no fixed value is classified as high or low. It is something that is acceptable
    for your system, which comes with benchmarking the system during the testing phases
    in your environment. Similar to page faults, there is a vast array of alerts that
    can be set up. Once an alert is raised, it will be sent every 10 minutes, as we
    have set, until the condition for sending the alerts is not met. In this case,
    if the number of page faults falls below 1000 or somebody manually acknowledges
    the alert, no further alerts will be sent further for that incident.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we see in the following screenshot, the alert is open and we can acknowledge
    the alert:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'On clicking on **Acknowledge**, the following popup will let us choose the
    duration for which we will acknowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This means that for this particular incident, no more alerts will be sent out
    until the selected time period elapses.
  prefs: []
  type: TYPE_NORMAL
- en: The Open alerts can be viewed by clicking on the **Activities** menu option
    on the left.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visit the URL [http://www.mongodb.com/blog/post/five-mms-monitoring-alerts-keep-your-mongodb-deployment-track](http://www.mongodb.com/blog/post/five-mms-monitoring-alerts-keep-your-mongodb-deployment-track)
    for some of the important alerts that you should set up for your deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Back up and restore data in Mongo using out-of-the-box tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will look at some basic backup and restore operations using
    utilities such as `mongodump` and `mongorestore` to back up and restore files.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start a single instance of mongod. Refer to the recipe *Installing
    single node MongoDB* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting
    the Server"), *Installing and Starting the Server*, to start a mongo instance
    and connect to it from a mongo shell. We will need some data to backup. If you
    already have some data in your test database, that will be fine. If not, create
    some from the `countries.geo.json` file available in the code bundle using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the data in the `test` database, execute the following (assuming we want
    to export the data to a local directory called `dump` in the current directory):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Verify that there is data in the `dump` directory. All files will be `.bson`
    files, one per collection in the respective database folder created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s import the data back into the mongo server using the following command.
    This is again with the assumption that we have the directory `dump` in the current
    directory with the required `.bson` files present in it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just a couple of steps executed to export and restore the data. Let's now see
    what it exactly does and what the command-line options for this utility are. The
    `mongodump` utility is used to export the database into the `.bson` files, which
    can then be later used to restore the data in the database. The export utility
    exports one folder per database except local database, and then each of them will
    have one `.bson` file per collection. In our case, we used the `-oplog` option
    to export a part of the oplog too and the data will be exported to the `oplog.bson`
    file. Similarly, we import the data back into the database using the `mongorestore`
    utility. We explicitly ask the existing data to be dropped by providing the `--drop`
    option before the import and replay of the contents in the oplog if any.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mongodump` utility simply queries the collection and exports the contents
    to the files. The bigger the collection, the longer it will take to restore the
    contents. It is thus advisable to prevent write operations when the dump is being
    taken. In the case of sharded environments, the balancer should be turned off.
    If the dump is taken while the system is running, export with the `-oplog` option
    to export the contents of the oplog as well. This oplog can then be used to restore
    to the point in time data. The following table shows some of the important options
    available for the `mongodump` and `mongorestore` utility, first for `mongodump`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Option | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `--help` | Shows all the possible, supported options and a brief description
    of these options. |'
  prefs: []
  type: TYPE_TB
- en: '| `-h` or `--host` | The host to connect to. By default, it is localhost on
    port `27017`. If a standalone instance is to be connected to, we can set the hostname
    as `<hostname>:<port number>`. For a replica set, the format will be `<replica
    set name>/<hostname>:<port>,….<hostname>:<port>` where the comma-separated list
    of hostnames and port is called the **seed list**. It can contain all or a subset
    of hostnames in a replica set. |'
  prefs: []
  type: TYPE_TB
- en: '| `--port` | The port number of the target MongoDB instance. This is not really
    relevant if the port number is provided in the previous `-h` or `--host` option.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `-u` or `--username` | Provides the username of the user using which the
    data would be exported. Since the data is read from all databases, the user is
    at least expected to have read privileges in all databases. |'
  prefs: []
  type: TYPE_TB
- en: '| `-p` or `--password` | The password used in conjunction with the username.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `--authenticationDatabase` | The database in which the user credentials are
    kept. If not specified, the database specified in the `--db` option is used. |'
  prefs: []
  type: TYPE_TB
- en: '| `-d` or `--db` | The database to back up. If not specified, then all the
    databases are exported. |'
  prefs: []
  type: TYPE_TB
- en: '| `-c` or `--collection` | The collection in the database to be exported. |'
  prefs: []
  type: TYPE_TB
- en: '| `-o` or `--out` | The directory to which the files will be exported. By default,
    the utility will create a dump folder in the current directory and export the
    contents to that directory. |'
  prefs: []
  type: TYPE_TB
- en: '| `--dbpath` | If we don''t intend to connect to the database server and instead
    directly read from the database file. The value is the path of the directory where
    the database files will be found. The server should not be up and running while
    reading directly from the database files as the export locks the data files, which
    can''t happen if a server is up and running. A lock file will be created in the
    directory while the lock is acquired. |'
  prefs: []
  type: TYPE_TB
- en: '| `--oplog` | With the option enabled, the data from the oplog from the time
    the export process started is also exported. Without this option enabled, the
    data in the export will not represent a single point in time if writes are happening
    in parallel as the export process can take few hours and it simply is a query
    operation on all the collections. Exporting the oplog gives an option to restore
    to a point in time data. There is no need to specify this option if you are preventing
    write operations while the export is in progress. |'
  prefs: []
  type: TYPE_TB
- en: Similarly, for the `mongorestore` utility, here are the options. The meaning
    of the options `--help`, `-h`, or `--host`, `--port`, `-u`, or `--username`, `-p`
    or `--password`, `--authenticationDatabase`, `-d`, or `--db`, `-c` or `--collection`.
  prefs: []
  type: TYPE_NORMAL
- en: '| Option | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `--dbpath` | If we don''t intend to connect to the database server and instead
    directly write to the database file, use this option. The value is the path of
    the directory where the database files will be found. The server should not be
    up and running while writing directly to the database files as the restore operation
    locks the data files, which can''t happen if a server is up and running. A lock
    file will be created in the directory while the lock is acquired. |'
  prefs: []
  type: TYPE_TB
- en: '| `--drop` | Drop the existing data in the collection before restoring the
    data from the exported dumps. |'
  prefs: []
  type: TYPE_TB
- en: '| `--oplogReplay` | If the data was exported while writes to the database were
    allowed and if the `--oplog` option was enabled during export, the oplog exported
    will be replayed on the data to bring all the data in the database to the same
    point in time. |'
  prefs: []
  type: TYPE_TB
- en: '| `--oplogLimit` | The value of this parameter is a number representing the
    time in seconds. This option is used in conjunction with `oplogReplay` command
    line option, which is used to tell the restore utility to replay the oplog and
    stop just at the limit specified by this option. |'
  prefs: []
  type: TYPE_TB
- en: You might think, *Why not copy the files and take a backup?* That works well
    but there are a few problems associated with it. First, you cannot get a point-in-time
    backup unless write operations are disabled. Secondly, the space used for backups
    is very high as the copy would also copy the 0 padded files of the database as
    against the `mongodump`, which exports just the data.
  prefs: []
  type: TYPE_NORMAL
- en: Having said that, filesystem snapshotting is a commonly used practice for backups.
    One thing to remember is while taking the snapshot the journal files and the data
    files need to come in the same snapshot for consistency.
  prefs: []
  type: TYPE_NORMAL
- en: If you were using **Amazon Web Services** (**AWS**), it would be highly recommended
    that you upload your database backups to AWS S3\. As you may be aware, AWS offers
    extremely high data redundancy with a very low storage cost.
  prefs: []
  type: TYPE_NORMAL
- en: Download the script `generic_mongodb_backup.sh` from the Packt Publishing website
    and use it to automate your backup creation and upload to AWS S3.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring MMS Backup service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MMS Backup is a relatively new offering by MongoDB for real-time incremental
    backup of your MongoDB instances, replica sets, and shards, and offers you point
    in time recovery of your instances. The service is available as on-premise (in
    your data center) or cloud. However, we will demonstrate the on-cloud service
    that is the only option for the Community and Basic subscription. For more details
    on the available options, you can visit different product offerings by MongoDB
    at [https://www.mongodb.com/products/subscriptions](https://www.mongodb.com/products/subscriptions).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mongo MMS Backup service will work only on Mongo 2.0 and above. We will start
    a single server that we will backup. MMS backup relies on the oplog for continuous
    backup and since oplog is available only in replica sets, the server needs to
    be started as a replica set. Refer to the recipe *Connecting to a single node
    using a Python client* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting
    the Server"), *Installing and Starting the Server* to learn more about how to
    install Python and PyMongo, the Python client of Mongo.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you don't already have a MMS account, then log in to [https://mms.mongodb.com/](https://mms.mongodb.com/)
    and sign up for an account. For screenshots, refer to the recipe *Signing up for
    MMS and setting up an MMS monitoring agent* in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a single instance of Mongo and replace the value of the appropriate filesystem
    path on your machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `smallfiles` and `oplogSize` are options only set for testing
    purposes and are not to be used in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a shell, connect to the instance in step 1 and initiate the replica set
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The replica set will be up and running in some time.
  prefs: []
  type: TYPE_NORMAL
- en: Go back to the browser to `mms.mongodb.com`. Add a new host by clicking on the
    **+ Add Host** button. Set the type as replica set and the hostname as your hostname
    and the port as the default one `27017` in our case. Refer to the recipe *Signing
    up for MMS and setting up an MMS monitoring agent* for the screenshots of the
    **Add Host** process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the host is successfully added, register for MMS backup by clicking on
    the **Backup** option the left and then **Begin Setup**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An SMS or Google authenticator can be used for registration. If a smartphone
    is available with Android, iOS, or Blackberry OS, Google authenticator is a good
    option. For countries like India, Google Authenticator is the only option available.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assuming Google Authenticator is not configured already and we are planning
    to use it, we would need the app to be installed on your smartphone. Go to the
    respective app store of your mobile OS platform and install the Google Authenticator
    software.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the software installed on the phone, come back to the browser. You should
    see the following screen on selecting the Google Authenticator:![How to do it…](img/B04831_06_22.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Begin the setup for a new account by scanning the QR code from the Google Authenticator
    application. If barcode scanning is a problem, you may choose to manually enter
    the key given on the right side of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the scanning or the key is entered successfully, your smartphone should
    show a 6-digit number that changes every 30 seconds. Enter that number in the
    **Authentication Code** box given on the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is important not to delete this account in Google Authenticator on your phone
    as this will be used in future whenever we wish to change any settings related
    to backup, such as stopping backup, changing exclusion list, and almost any operation
    in MMS backup. The QR code and key will not be visible again once the setup is
    done. You will have to contact MongoDB support to get the configuration reset.
  prefs: []
  type: TYPE_NORMAL
- en: Once the authentication is done, the next screen you should see is the billing
    address and billing details, such as the card you register. All charges below
    $5 are waived so you should be ok to try out a small test instance before being
    charged.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the credit card details are saved, we move ahead with the setup. We will
    have for installation a backup agent. This is a separate agent from the monitoring
    agent. Choose the appropriate platform and follow the instructions for its installation.
    Take a note of the location where the configuration files of the agent will be
    placed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A new popup will contain the instruction/link to the archive/installer for the
    platform and the steps to install. It should also contain the `apiKey`. Take a
    note of the API key; we will need it in the next step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the installation is complete, open the `local.config` file placed in the
    `config` directory of the agent installation (the location that was shown/modified
    during installation of the agent) and paste/type in the `apiKey` noted down in
    the previous step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the agent is configured and started, click on the **Verify Agent** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the agent is successfully verified, we will start by adding a host to back
    up. The drop-down menu should show us all the replica sets and shards we have
    added. Select the appropriate one and set the **Sync source** as the primary instance,
    as that is the only one we have in our standalone instance. **Sync source** is
    only used for the initial sync process. Whenever we have a proper replica set
    with multiple instances, I prefer using a secondary as a sync process instance.![How
    to do it…](img/B04831_06_23.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the instance is not started with security, leave the **DB Username** and
    **DB Password** fields blank.
  prefs: []
  type: TYPE_NORMAL
- en: Click on the button **Manage excluded namespaces** if you wish to skip a particular
    database or collection being backed up. If nothing is provided, by default everything
    will be backed up. The format for the collection name will be `<databasename>.<collection
    name>`. Alternatively, it could be just the database name, in which case all collections
    in that database would not be eligible for backup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the details are all ok, click on the **Start** button. This should complete
    the setup of the backup process for a replica set on MMS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The installation steps I performed were on Windows OS and the service needs
    to be started manually in that case. Press Windows + *R* and type `services.msc`.
    The name of the service is MMS Backup Agent.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The steps are pretty simple and this is all we need to do to set up a server
    for Mongo MMS backup. One important thing mentioned earlier is that MMS backup
    uses multifactor authentication for any operation once the backup is set up, and
    the account set up in Google Authenticator for MongoDB should not be deleted.
    There is no way to recover the original key used for setting up the authenticator.
    You will have to clear the Google Authenticator settings and set up a new key.
    To do that, click on the **Help & Support** link in the bottom-left corner of
    the screen and click on **How do I reset my two-factor authentication?**.
  prefs: []
  type: TYPE_NORMAL
- en: On clicking the link, a new window will open up and ask for the username. An
    e-mail will be sent out to the registered e-mail ID which allows you to reset
    the two-factor authentication.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As mentioned, oplog is used to synchronize the current MongoDB instance with
    the MMS service. However, for the initial sync, an instance's data files are used.
    The instance to use is provided by us when we set up the backup of replica set.
    As this is a resource-heavy operation, I prefer to use a secondary instance for
    this on busy systems so as not to add more querying on the primary instance by
    the MMS backup agent. Once the instance is done with initial synchronization,
    the oplog of the primary instance will be used to get the data on a continuous
    basis. Agent does write to a collection called `mms.backup` in admin database
    periodically.
  prefs: []
  type: TYPE_NORMAL
- en: 'The backup agent for MMS backup is different from the MMS monitoring agent.
    Though there is no restriction on having them both running on the same machine,
    you might need to evaluate that before having such a setup in production. The
    safe bet would be to have them running on separate machines. Never run either
    of these agents with a mongod or mongos instance on the same box in production.
    There are a couple of important reasons why it is not recommended to run the agent
    on the same box as the mongod instances:'
  prefs: []
  type: TYPE_NORMAL
- en: The resource utilization of the agent is dependent on the cluster size it monitors.
    We don't want the agent to use a lot of resources affecting the performance of
    the production instance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent could be monitoring a lot of server instances at a time. Since there
    is only one instance of this agent, we do not want it to go down during database
    server maintenance and restart.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The community edition of MongoDB built with SSL or enterprise versions with
    the SSL option used for communication between the client and the mongo server
    must do some additional steps. The first step is to check the **My deployment
    supports SSL for MongoDB connections** flag when we set up the replica set for
    backup (see step 15). Note the check box at the bottom in the screenshot that
    should be checked. Secondly, open the `local.config` file for the MMS configuration
    and look out for these two properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The first is the fully qualified path of the certifying authority's certificate
    in PEM format. This certificate will be used to verify the certificate presented
    by the mongod instance running over SSL. The second property can be set to `false`
    if certificate verification is to be disabled, this is however not a recommended
    option. As far as the traffic between the backup agent and MMS backup is concerned,
    data sent from the agent to the MMS service over SSL is secure irrespective of
    whether SSL is enabled on your MongoDB instances or not. The data at rest in the
    data center for the backed up data is not encrypted.
  prefs: []
  type: TYPE_NORMAL
- en: 'If security is enabled on the mongod instance, a username and password need
    to be provided, which will be used by the MMS backup agent. The username and password
    are provided while setting up backup for the replica set, as in step 15\. Since
    the agent needs to read the oplog, possibly all databases for the initial sync
    and write data to `admin` database the following roles are expected for the user:
    `readAnyDatabase`, `clusterAdmin`, `readWrite` on `admin` and `local` databases,
    and `userAdminAnyDatabase`. This is in case in version 2.4 and above. In versions
    prior to v2.4, we would expect the user to have read access on all the databases
    and read/write access to admin and local databases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While setting up a replica set for backup you may get an error like, `Insufficient
    oplog size: The oplog window must be at least 1 hours over the last 24 hours for
    all active replica set members. Please increase the oplog.`. While you may think
    this is always something to do with oplog size, it is also seen when the replica
    set has an instance that is in recovering state. This might feel misleading, so
    do look out for recovering nodes, if any, in the replica set while setting up
    a backup for a replica set. As per the MMS support too, it seems too restrictive
    to not set up a replica set for backup with some recovering nodes, and it might
    be fixed in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: Managing backups in MMS Backup service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we saw how to set up MMS backup service and a simple
    one member replica set was set up for backup. Though a single member replica set
    makes no sense at all, it was needed as a standalone instance cannot be set up
    for backup in MMS. In this recipe, we dive deeper and look at the operations we
    can perform on the server that is set up for backup, such as starting, stopping,
    or terminating a backup; managing exclusion lists; managing backup snapshots and
    retention; and restoring to point in time data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous recipe is all that is needed for this recipe. The necessary setup
    is expected to be done as we are going to use the same server we had set up for
    backup in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the server up and running, let's import some data to it. It can be anything,
    but we chose to use the `countries.geo.json` file that was used in the last chapter.
    It should be available in the bundle downloaded from the Packt site.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the data into a collection called `countries` in the `test`
    database. Use the following command to do it. The following import command was
    executed with the current directory having the `countries.geo.json` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We have already seen how to exclude namespaces when the replica set backup
    was being set up. We will now see how to exclude namespaces once the backup for
    a replica set is done. Click on the **Backup** menu option on the left and then
    the **Replica Set Status**, which opens by default when **Backup** is clicked.
    Click on the **Gear** button on the right side of the row where the replica set
    is shown. It should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/B04831_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we see in the preceding image, click on **Edit Excluded Namespaces** and
    type in the name of the collection that we want to exclude. Suppose we want to
    exclude the `applicationLogs` collection in `test` database, type in `test.applicationLogs`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On saving, you will be asked to enter the token code that is currently displayed
    on your Google Authenticator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On successful validation of the code, the namespace `test.applicationLogs` will
    be added to the list of namespaces excluded from being backed up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We now shall see how to manage snapshot scheduling. A snapshot is the state
    of the database as of a particular point in time. To manage the snapshot frequency
    and retention policy, click on the **Gear** button shown in the previous screenshot
    and click on **Edit Snapshot Schedule**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can see in the following image, we can set the times when the snapshots
    are taken and their retention period. We will discuss more on this in the next
    section. Any changes to it would need multifactor authentication to save the changes.![How
    to do it…](img/B04831_06_25.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now see how we go about restoring the data using MMS backup. At any
    point in time whenever we want to restore the data, click on **Backup** and **Replica
    Set Status**/**Shard Cluster Status** click on **set/cluster name**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On clicking it, we will see the snapshots that are saved against this set.
    It should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/B04831_06_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We have encircled some of the portions on the screen which we will see one by
    one.
  prefs: []
  type: TYPE_NORMAL
- en: To restore to the time when the snapshot was taken, click on the **Restore this
    snapshot** link in the **Actions** column of the grid.![How to do it…](img/B04831_06_27.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding image shows us how we can export the data either over HTTPS or
    SCP. We select HTTPS for now, and click **Authenticate**. We will see about SCP
    in the next section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the token that is received either over SMS or seen on Google Authenticator
    and click **Finalize Request** on entering the auth code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On successful authentication, click on **Restore Jobs**. This is a one-time
    download that will let you download the `tar.gz` archive. Click on the **download**
    link to download the `tar.gz` archive.![How to do it…](img/B04831_06_28.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the archive is downloaded, extract it to get the database files within
    it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop the mongod instance, replace the database files with the ones that are
    extracted, and restart the server to get the data as of the time when the snapshot
    was taken. Note that the database file will not contain data for the collection
    that was excluded from backup if all.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now see how to get point in time data using MMS backup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Replica Set Status** / **Shard Cluster Status** and then the cluster/set
    which is to be restored.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the right-hand side of the screen, click on the **Restore** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This should give a list of available snapshots or you may enter a custom time.
    Check **Use Custom Point In Time**. Click on the **Date** field and select a date
    and a time to which you want to restore the data in Hours and Minutes and click
    **Next**. Note that the **Point in Time** feature only restores to a point in
    last 24 Hours.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, you will be asked to specify the format as HTTPS or SCP. The subsequent
    steps are similar to what we did on the previous occasion, from step 14 onwards.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After setting up the backup for a replica set, we imported random data into
    the database. Backup for this database would be done by MMS and later on we will
    restore the database using this backup. We saw how to exclude namespaces from
    being backed up in steps 2-5.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the snapshot and retention policy settings, we can see we have the
    choice of the time interval in which the snapshots are to be taken and the number
    of days they are to be retained (step 9). We can see that by default snapshots
    are taken every 6 hours and they are saved for 2 days. The snapshot that is taken
    at the end of the day gets saved for a week. The snapshot taken at the end of
    the week and month are saved for 4 weeks and 13 months respectively. A snapshot
    can be taken once every 6, 8, 12, and 24 hours. However, you need to understand
    the flip side of taking snapshots after long time duration. Suppose the last snapshot
    is taken at 18 hours; getting the data as of that time for restore is very easy
    as it is stored on the MMS backup servers. However, we need the data as of 21:30
    hours for restoration. Since MMS backup supports point in time backup, it would
    use the base snapshot at 18:00 hours and then just replay the changes on it after
    the snapshot is taken at 21:30 hours. This is similar to how an oplog would be
    replayed on the data. There is a cost for this replay and thus getting point in
    time backup is slightly more expensive than getting the data from a snapshot.
    Here, we had to replay the data for 3.5 hours, from 18:00 hours to 21:30 hours.
    Imagine if the snapshots were set to be taken after 12 hours and our first snapshot
    is taken at 00:00 hours, then we would have snapshots at 00:00 hours and 12:00
    hours every day. To restore the data as of 21:30 hours with 12:00 hours as the
    last snapshot, we will have to replay 9.5 hours of data. This is much more expensive.
  prefs: []
  type: TYPE_NORMAL
- en: More frequent snapshots means more storage space usage but less time needed
    to restore a database to a given point in time. At the same time, less frequent
    snapshots require less storage but at the cost of more time to restore the data
    to a point in time. You need to decide and have a trade-off between these two,
    space and time of restoration. For the daily snapshot, we can choose retention
    of between 3 to 180 days. Similarly, for the weekly and monthly snapshots, the
    retention period can be chosen between 1 to 52 weeks and 1 to 36 months, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The screenshot in step 9 has a column for the expiry of the snapshot. This,
    for the first snapshot taken, is 1 year, whereas others expire in 2 days. The
    expiration is as per what we discussed in the last paragraph. On changing the
    expiration values, the old snapshots are not affected or adjusted as per the changed
    times. However, the new snapshots taken will be as per the modified settings for
    the retention and frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw how to download the dump (step 10 onwards) and then use it to restore
    the data in the database. It was pretty straightforward and doesn''t need a lot
    of explanation except a couple of things. First, if the data is for a shard, there
    will be multiple folders—one for each shard and each of them will have the database
    files as against what we saw here in case of a replica set where we have a single
    folder with database files in it. Finally, let''s look at the screen when we choose
    SCP as the option:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: SCP is short for secure copy. The files will be copied over a secure channel
    to a machine's filesystem. The host that is given needs to have a public IP which
    will be used to SCP. This makes a lot of sense when we want the data from MMS
    to be delivered to a machine running on Unix OS on the cloud, for example, one
    of the AWS virtual instances. Rather than getting the file using HTTPS on our
    local machine and then reuploading it to the server on the cloud, you can specify
    the location on which the data needs to be copied in the Target Directory block,
    the hostname, and the credentials. There are a couple of ways of authentication
    too. A password is an easy way with an additional option to SSH key pair. If you
    have to configure host's firewalls on the cloud to allow incoming traffic over
    the SSH port, the public IP addresses are given at the bottom of the screen (`64.70.114.115/32`
    or `4.71.186.0/24` in our screenshot). You should whitelist them to allow incoming
    secure copy requests over port `22`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen running backups using MMS which uses oplogs for this purpose. There
    is a recipe called *Implementing triggers in Mongo using oplog* in [Chapter 5](ch05.html
    "Chapter 5. Advanced Operations"), *Advanced Operations*, which uses oplog to
    implement trigger-like functionalities. This concept is the backbone of the real-time
    backup used by MMS backup service.
  prefs: []
  type: TYPE_NORMAL
