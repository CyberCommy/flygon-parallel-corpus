- en: Chapter 6. Monitoring and Backups
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。监控和备份
- en: 'In this chapter, we will take a look at the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将查看以下教程：
- en: Signing up for MMS and setting up an MMS monitoring agent
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册MMS并设置MMS监控代理
- en: Managing users and groups in MMS Console
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MMS控制台中管理用户和组
- en: Monitoring instances and setting up alerts in MMS
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MMS中监视实例并设置警报
- en: Setting up monitoring alerts in MMS
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MMS中设置监控警报
- en: Back up and restore data in Mongo using out-of-the-box tools
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用现成的工具备份和恢复Mongo中的数据
- en: Configuring MMS Backup service
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置MMS备份服务
- en: Managing backups in MMS Backup service
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MMS备份服务中管理备份
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Monitoring and backup is an important aspect of any mission-critical software
    in production. Monitoring proactively lets us take action whenever an abnormal
    event occurs in the system that can compromise data consistency, availability,
    or the performance of the system. Issues may come to light after having a significant
    impact in the absence of monitoring the systems proactively. We covered administration-related
    recipes in [Chapter 4](ch04.html "Chapter 4. Administration"), *Administration*,
    and both these activities are part of it; however, they demand a separate chapter
    as the content to be covered is extensive. In this chapter, we will see how to
    monitor various parameters and set up alerts for various parameters of your MongoDB
    cluster using the **Mongo Monitoring Service** (**MMS**). We will look at some
    mechanisms to backup data using the out-of-the-box tools and also using the MMS
    backup service.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中，监控和备份是任何关键任务关键软件的重要方面。主动监控让我们在系统中发生异常事件时采取行动，这些事件可能危及数据一致性、可用性或系统性能。如果没有主动监控系统，问题可能在对系统产生重大影响后才会显现出来。我们在[第4章](ch04.html
    "第4章。管理")中涵盖了与管理相关的教程，这两个活动都是其中的一部分；但是，它们需要一个单独的章节，因为要涵盖的内容很广泛。在本章中，我们将看到如何使用**Mongo
    Monitoring Service**（**MMS**）监控MongoDB集群的各种参数并设置警报。我们将研究一些使用现成工具和MMS备份服务备份数据的机制。
- en: Signing up for MMS and setting up an MMS monitoring agent
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注册MMS并设置MMS监控代理
- en: MMS is a cloud-based or on-premise service that enables you to monitor your
    MongoDB cluster. The on-premise version is available with an enterprise subscription
    only. It gives you one central place that lets the administrators monitor the
    health of the server instances and the boxes on which the instances are running.
    In this recipe, we will see what the software requirements are and how to set
    up MMS for Mongo.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: MMS是一个基于云或本地的服务，可以让您监视MongoDB集群。本地版本仅适用于企业订阅。它为管理员提供了一个中心位置，让管理员监视服务器实例的健康状况以及实例所在的服务器。在本教程中，我们将看到软件要求是什么，以及如何为Mongo设置MMS。
- en: Getting ready
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will be starting a single instance of `mongod`, which we will use for monitoring
    purposes. Refer to the recipe *Installing single node MongoDB* from [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* to start a MongoDB instance and connect to it from a Mongo shell. The
    monitoring agent used for sending the statistics of the mongo instance to the
    monitoring service uses Python and pymongo. Refer to the recipe *Connecting to
    a single node using a Python client* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* to learn more
    about how to install Python and pymongo, the Python client of MongoDB.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将启动一个`mongod`的单个实例，用于监视目的。参考[第1章](ch01.html "第1章。安装和启动服务器")中的*安装单节点MongoDB*的步骤，启动MongoDB实例并从Mongo
    shell连接到它。用于将mongo实例的统计信息发送到监控服务的监控代理使用Python和pymongo。参考[第1章](ch01.html "第1章。安装和启动服务器")中的*使用Python客户端连接到单节点*的步骤，了解如何安装Python和pymongo，MongoDB的Python客户端。
- en: How to do it…
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤…
- en: 'If you don''t already have a MMS account, then log in to [https://mms.mongodb.com/](https://mms.mongodb.com/)
    and sign up for an account. On signing up and logging in, you should see the following
    page:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有MMS帐户，请登录[https://mms.mongodb.com/](https://mms.mongodb.com/)并注册一个帐户。注册并登录后，您应该看到以下页面：
- en: '![How to do it…](img/B04831_06_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![操作步骤…](img/B04831_06_01.jpg)'
- en: Click on the **Get Started** button under **Monitoring**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 单击**监控**下的**开始**按钮。
- en: Once we reach the **Download Agent** option in the menu, click on the appropriate
    OS platform to download the agent. Follow the instructions given after selecting
    the appropriate OS platform. Note down the **apiKey** too. For example, if the
    Windows platform is selected, we would see the following:![How to do it…](img/B04831_06_02.jpg)
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦到达菜单中的**下载代理**选项，请单击适当的操作系统平台以下载代理。选择适当的操作系统平台后，按照给定的说明进行操作。也记下**apiKey**。例如，如果选择了Windows平台，我们将看到以下内容：![操作步骤…](img/B04831_06_02.jpg)
- en: Once the installation is complete, open the file `monitoring-agent.config`.
    It will be present in the configuration folder selected while installing the agent.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完成后，打开`monitoring-agent.config`文件。它将位于安装代理时选择的配置文件夹中。
- en: Look out for the key `mmsApiKey` in the file and set its value to the API key
    that was noted down in step 1.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文件中查找关键的`mmsApiKey`，并将其值设置为在第1步中记录的API密钥。
- en: Once the service is started (we have to go to `services.msc` on Windows, which
    can be done by typing `services.msc` in the run dialog (Windows + *R*) and start
    the service manually). The service would be named **MMS Monitoring Agent**. On
    the web page, click on the **Verify Agent** button. If all goes well, the started
    agent will be verified and a success message will be shown.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦服务启动（我们必须在Windows上转到`services.msc`，可以通过在运行对话框中输入`services.msc`（Windows + *R*）并手动启动服务来完成）。服务将被命名为**MMS
    Monitoring Agent**。在网页上，点击**验证代理**按钮。如果一切顺利，启动的代理将被验证，并显示成功消息。
- en: The next step is to configure the host. This host is the one that is seen from
    the agent's perspective running on the organization or individual's infrastructure.
    The following screen shows the screen used for the addition of a host. The hostname
    is the internal hostname (the hostname on the client's network), and the MMS on
    the cloud doesn't need to reach out to the MongoDB processes. It is the agent
    that collects the data from these mongodb processes and sends the data to the
    MMS service.![How to do it…](img/B04831_06_03.jpg)
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是配置主机。这个主机是从代理的角度看到的，在组织或个人基础设施上运行。下面的屏幕显示了用于添加主机的屏幕。主机名是内部主机名（客户网络上的主机名），云上的MMS不需要访问MongoDB进程。收集这些mongodb进程的数据并将数据发送到MMS服务的是代理。![如何操作...](img/B04831_06_03.jpg)
- en: Once the host's details are added, click on the **Verify Host** button. Once
    verification is done, click the **Start Monitoring** button.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦添加了主机的详细信息，请单击**验证主机**按钮。验证完成后，单击**开始监视**按钮。
- en: We have successfully set up MMS and added one host to it that will be monitored.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功设置了MMS并向其添加了一个将被监视的主机。
- en: How it works…
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we set up an MMS agent and monitoring for a standalone MongoDB
    instance. The installation and setup process is pretty simple. We also added a
    standalone instance and all was okay.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们设置了一个MMS代理和监视一个独立的MongoDB实例。安装和设置过程非常简单。我们还添加了一个独立的实例，一切都很好。
- en: 'Suppose we have a replica set up and running (refer to the recipe *Starting
    multiple instances as part of a replica set* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server*, for more details
    on how to start a replica set) and the three members are listening to ports `27000`,
    `27001`, and `27002`. Refer to point number 6 in the *How to do it…* section where
    we set up one standalone host. In the drop-down menu for **Host Type** select
    **Replica Set** and in the **Internal hostname**, give a valid hostname of any
    member of the replica set (in my case **Amol-PC** and port **27001** were given,
    which is a secondary instance); all other instances will be automatically discovered
    and will be visible under the hosts, as shown here:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经设置并运行了一个副本集（参考第1章中的教程*作为副本集的一部分启动多个实例*，*安装和启动服务器*，了解如何启动副本集的更多细节），三个成员正在监听端口`27000`，`27001`和`27002`。参考*如何操作...*部分中的第6点，我们设置了一个独立的主机。在**主机类型**的下拉菜单中选择**副本集**，在**内部主机名**中，给出副本集的任何成员的有效主机名（在我的情况下，给出了**Amol-PC**和端口**27001**，这是一个辅助实例）；所有其他实例将被自动发现，并在主机下可见，如下所示：
- en: '![How it works…](img/B04831_06_05.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/B04831_06_05.jpg)'
- en: We didn't see what is to be done when security is enabled on the cluster, which
    is pretty common in production environments and we have replica sets or shard
    setup. If authentication is enabled, we need proper credentials for the MMS agent
    to gather the statistics. The **DB Username** and **DB Password** that we give
    while adding a new host (point number 6 in the *How to do it…* section) should
    have a minimum of `clusterAdmin` and `readAnyDatabase` role.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有看到在集群上启用安全性时应该做什么，这在生产环境中非常常见，我们有副本集或分片设置。如果启用了身份验证，我们需要MMS代理收集统计信息的正确凭据。在添加新主机时（*如何操作...*部分的第6点），我们给出的**DB用户名**和**DB密码**应该具有至少`clusterAdmin`和`readAnyDatabase`角色。
- en: There's more…
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: What we saw in this recipe was how to set up MMS agent and create an account
    from the MMS console. However, we can add groups and users for the MMS console
    as an administrator granting various users privileges for performing various operations
    on different groups. In the next recipe, we will throw some light on user and
    group management in the MMS console.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们看到了如何设置MMS代理并从MMS控制台创建帐户。但是，我们可以作为管理员为MMS控制台添加组和用户，授予各种用户在不同组上执行各种操作的权限。在下一个教程中，我们将对MMS控制台中的用户和组管理进行一些解释。
- en: Managing users and groups in MMS console
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在MMS控制台中管理用户和组
- en: In the previous recipe, we saw how to set up an MMS account and set up an MMS
    agent. In this recipe, we will throw some light on how to set up the groups and
    user access to the MMS console.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个教程中，我们看到了如何设置MMS帐户并设置MMS代理。在这个教程中，我们将对如何设置组和用户访问MMS控制台进行一些解释。
- en: Getting ready
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the previous recipe for setting up the agent and MMS account. This
    is the only prerequisite for this recipe.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有关设置代理和MMS帐户，请参阅上一个教程。这是本教程的唯一先决条件。
- en: How to do it…
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start by going to **Administration** | **Users** on the left-hand side of the
    screen, as shown here:![How to do it…](img/B04831_06_06.jpg)
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，转到屏幕左侧的**管理** | **用户**，如下所示：![如何操作...](img/B04831_06_06.jpg)
- en: 'Here, you can view the existing users and also add new users. On clicking the
    **Add User** (encircled in the top right corner of the preceding image) button,
    you should see the following popup window that allows you to add a new user:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以查看现有用户并添加新用户。单击前图中右上角的**添加用户**（圈起来的）按钮，您应该看到以下弹出窗口，允许您添加新用户：
- en: '![How to do it…](img/B04831_06_08.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/B04831_06_08.jpg)'
- en: The preceding screen will be used to add users. Take a note of the various available
    roles.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的屏幕将用于添加用户。注意各种可用角色。
- en: Similarly, go to **Administration** | **My Groups** to view and add new groups
    by clicking on the **Add Group** button. In the text box, type the name of the
    group. Remember that the name of the group you enter should be available globally.
    The given name of the group should be unique across all users of MMS and not just
    your account.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，转到**管理** | **我的组**，通过单击**添加组**按钮查看和添加新组。在文本框中，输入组的名称。请记住，您输入的组名应该在全球范围内可用。所给组的名称应在MMS的所有用户中是唯一的，而不仅仅是您的帐户。
- en: When a new group is created, it will be visible in the top left corner in a
    drop-down menu for all the groups, as shown here:![How to do it…](img/B04831_06_07.jpg)
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can switch between the groups using this drop-down menu, which should show
    all the details and stats relevant to the selected group.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that a group once created cannot be deleted. So be careful while creating
    one.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tasks we did in the recipe are pretty straightforward and don't need a lot
    of explanation except for one question. When and why do we add a group? It is
    when we want to segregate our MongoDB instances by different environments or applications.
    There will be a different MMS agent running for each group. Creating a new group
    is necessary when we want to have separate monitoring groups for different environments
    of an application (Development, QA, Production, and so on), and each group has
    different privileges for the users. That is, the same agent cannot be used for
    two different groups. While configuring the MMS agent, we give it an API key unique
    to the group. To view the API key for the group, select the appropriate group
    from the drop-down menu at the top of the screen (if your user has access to only
    group, the drop-down menu won't be seen) and go to **Administration** | **Group
    Settings** as shown in the next screenshot. The **Group ID** and the **API Key**
    will both be shown on the top of the page.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_09.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: Note that not all user roles will see this option. For example, read-only users
    can only personalize their profile and most of the other options will not be visible.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring instances and setting up alerts on MMS
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous couple of recipes showed us how to set up an MMS account, set up
    an agent, add hosts, and manage user access to MMS console. The core objective
    of MMS is monitoring the host instances, which has not been discussed yet. In
    this recipe, we will perform some operations on the host that we added to MMS
    in the first recipe and monitor it from the MMS console.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Follow the recipe *Signing up for MMS and setting up an MMS monitoring agent*,
    and that is pretty much all that is needed for this recipe. You may choose to
    have a standalone instance or a replica set, either way is fine. Also, open a
    mongo shell and connect to the primary instance from it (it is a replica set).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by logging into MMS console and clicking on **Deployment** on the left.
    Then, click on the **Deployment** link in the submenu again, as shown in the following
    screenshot:![How to do it…](img/B04831_06_10.jpg)
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on one of the hostnames to see a large variety of graphs showing various
    statistics. In this recipe, we will analyze the majority of these.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Open the bundle downloaded for the book. In [Chapter 4](ch04.html "Chapter 4. Administration"),
    *Administration*, we used a JavaScript to keep the server busy with some operations
    named `KeepServerBusy.js`. We will be using the same script this time around.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the operating system shell, execute the following with the `.js` file in
    current directory. The shell connects to port `27000` in my case for the primary:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once it's started, keep it running and give it some 5 to 10 minutes before you
    start monitoring the graphs on MMS console.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html "Chapter 4. Administration"), *Administration*, we
    saw a recipe, *The mongostat and mongotop utilities* that demonstrated how these
    utilities can be used to get the current operations and resource utilization.
    That is a fairly basic and helpful way to monitor a particular instance. MMS,
    however, gives us one place to monitor the MongoDB instance with pretty easy-to-understand
    graphs. MMS also gives us historical stats, which `mongostat` and `mongotop` cannot
    give.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Before we go ahead with the analysis of the metrics, I would like to mention
    that in the case of MMS monitoring, the data is not queried nor sent out over
    the public network. It is just the statistics that are sent over a secure channel
    by the agent. The source code for the agent is open source and is available for
    examination if needed. The mongod servers need not be accessible from the public
    network as the cloud-based MMS service never communicates with the server instances
    directly. It is the MMS agent that communicates with the MMS service. Typically,
    one agent is enough to monitor several servers unless you plan to segregate them
    into different groups. Also, it is recommended to run the agent on a dedicated
    machine/virtual machine and not share it with any of the mongod or mongos instances
    unless it is a less crucial test instance group you are monitoring.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Let's see some of these statistics on the console; we start with the memory-related
    ones. The following graph shows the resident, mapped, and virtual memory.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_11.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: As we can see, the resident memory for the data set is 82 MB, which is very
    low and it is the actual physical memory used up by the mongod process. This current
    value is significantly below the free memory available and generally this will
    increase over a period of time till it reaches a point where it has used up a
    large chunk of the total physical available memory. This is automatically taken
    care of by the mongod server process, and we can't force it to use up more memory
    even though it is available on the machine it is running on.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: The mapped memory, on other hand, is about the total size of the database and
    is mapped by MongoDB. This size can be (and usually is) much higher than the physical
    memory available, which enables the mongod process to address the entire dataset
    as it is present in memory even if it isn't. MongoDB offloads the responsibility
    of mapping and loading data to and from disk to the underlying operating system.
    Whenever a memory location is accessed and it is not available in the RAM (that
    is, the resident memory), the operating system fetches the page into memory, evicting
    some pages to make space for the new page if necessary. What exactly is a memory
    mapped file? Let's try to see with a super scaled down version. Suppose we have
    a file of 1 KB (1024 bytes) and the RAM is only 512 bytes, then obviously we cannot
    have the whole file in memory. However, you can ask the operating system to map
    this file to available RAM in pages. Suppose each page is 128 bytes, then the
    total file is 8 pages (128 * 8 = 1024). But the OS can only load four pages, and
    we assume it loaded the first 4 pages (up to 512 bytes) in the memory. When we
    access byte number 200, it is okay and found in the memory as it is in present
    on page 2\. But what if we access byte 800, which is logically on page 7 that
    is not loaded in memory? What OS does is takes one page out from the memory and
    loads this page 7 containing byte number 800\. MongoDB as an application gives
    the impression that everything was loaded in the memory and was accessed by the
    byte index, but actually it wasn't and the OS transparently did the work for us.
    Since the page accessed was not present in the memory and we had to go to the
    disk to load it in the memory it is called a **page fault**.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Getting back to the stats shown in the graph, the virtual memory contains all
    the memory usage including the mapped memory plus any additional memory used,
    such as the memory associated with the thread stack associated with each connection.
    If journaling is enabled, this size will definitely be more than twice than that
    of the mapped memory as journaling too will have a separate memory mapping for
    the data. Thus, we have two addresses mapping the same memory location. This doesn't
    mean that the page will be loaded twice. It just means that two different memory
    locations can be used to address the same physical memory. Very high virtual memory
    might need some investigation. There is no predetermined definition of a too high
    or low value; generally these values are monitored for your system under normal
    circumstances when you are happy with the performance of your system. These benchmark
    values should then be compared with the figures seen when system performance goes
    down and then appropriate action can be taken.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: As we saw earlier, page faults are caused when an accessed memory location is
    not present in the resident memory, causing the OS to load the page from memory.
    This IO activity will definitely cause performance to go down and too many page
    faults can bring down database performance dramatically. The following screenshot
    shows quite a few page faults happening per minute. However, if the disk used
    is an SSD instead of spinning disks, the hit in terms of seek time from the drive
    might not be significantly high.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_12.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: A large number of page faults usually occur when there isn't enough physical
    memory to accommodate the data set and the OS needs to get the data from the disk
    into memory. Note that this stat shown in the preceding screenshot is taken on
    a Windows platform and might seem high for a very trivial operation. This value
    is the sum of hard and soft page faults and doesn't really give a true figure
    of how good (or bad) the system is. These figures would be different on a Unix-based
    OS. There is a JIRA ([https://jira.mongodb.org/browse/SERVER-5799](https://jira.mongodb.org/browse/SERVER-5799))
    open as of the writing of this book which reports this problem.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: One thing you might need to remember is that in production systems, MongoDB
    doesn't work well with a NUMA architecture and you might see a lot of page faults
    happening even if the available memory seems to be high enough. Refer to the URL
    [http://docs.mongodb.org/manual/administration/production-notes/](http://docs.mongodb.org/manual/administration/production-notes/)
    for more details.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an additional graph which gives some details about non-mapped memory.
    As we saw earlier in this section, there are three types of memory: mapped, resident,
    and virtual. Mapped memory is always less than virtual memory. Virtual memory
    will be more than twice that of mapped memory if journaling is enabled. If we
    look at the image given in this section earlier, we see that the mapped memory
    is 192 MB whereas the virtual memory is 532MB. Since journaling is enabled, the
    memory is more than twice that of the mapped memory. When journaling is enabled,
    the same page of data is mapped twice in memory. Note that the page is physically
    loaded only once, it is just that the same location can be accessed using two
    different addresses. Let''s find the difference between the virtual memory, which
    is 532MB, and twice the mapped memory that is 384 MB (2 * 192 = 384). The difference
    between these figures is 148 MB (532 - 384).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: What we see here is the portion of virtual memory that is not mapped memory.
    This value is the same as what we just calculated.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_13.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: 'As mentioned earlier, a high or low value for non-mapped memory is not defined,
    however when the value reaches GBs we might have to investigate; possibly the
    number of open connections is high and we need to check if there is a leak with
    client applications not closing them after using it. There is a graph that gives
    us the number of connections open and it looks as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，非映射内存的高低值并没有明确定义，但是当值达到GB时，我们可能需要进行调查；可能打开的连接数很高，我们需要检查是否有客户端应用程序在使用后没有关闭连接。有一个图表显示了打开的连接数，如下所示：
- en: '![How it works…](img/B04831_06_14.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/B04831_06_14.jpg)'
- en: Once we know the number of connections and find it too high as compared to the
    expected count, we will need to find the clients who have opened the connections
    to that instance. We can execute the following JavaScript code from the shell
    to get those details. Unfortunately, at the time of writing this book, MMS doesn't
    have this feature to list out the client connection details.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道连接数，并且发现它与预期计数相比太高，我们将需要找到打开连接到该实例的客户端。我们可以从shell中执行以下JavaScript代码来获取这些详细信息。不幸的是，在撰写本书时，MMS没有这个功能来列出客户端连接的详细信息。
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `db.currentOp` method returns all the idle and system operations in the
    result. We then iterate through all the results and print out the client host
    and the connection details. A typical document in the result of the `currentOp`
    looks like this. You can choose to tweak the preceding code to include more details
    as per your need:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`db.currentOp`方法返回结果中所有空闲和系统操作。然后我们遍历所有结果并打印出客户端主机和连接详细信息。`currentOp`结果中的典型文档如下。您可以选择调整前面的代码，根据需要包含更多详细信息：'
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In [Chapter 4](ch04.html "Chapter 4. Administration"), *Administration*, we
    saw a recipe, *The mongostat and mongotop utilities* that was used to get some
    details on the percent of time a database was locked and the number of update,
    insert, delete, and getmore operations executed per second. You may refer to these
    recipes and try them out. We had used the same JavaScript that we have used currently
    to keep the server busy.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html "第4章。管理")中，我们看到了一个名为* mongostat和mongotop实用程序*的配方，用于获取数据库被锁定的时间百分比以及每秒执行的更新、插入、删除和获取操作的数量。您可以参考这些配方并尝试它们。我们使用了与当前用于使服务器繁忙的相同的JavaScript。
- en: 'In MMS console, we have graphs giving these details as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在MMS控制台中，我们有图表显示以下详细信息：
- en: '![How it works…](img/B04831_06_15.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/B04831_06_15.jpg)'
- en: The first one, `opcounters`, shows the number of operations executed at a particular
    point in time. This should be similar to what we saw using the `mongostat` utility.
    The one on the right shows us the percentage of time a database was locked. The
    drop-down menu lists the database names. We can select an appropriate database
    that we want to see the stats for. Again, this statistic can be seen using the
    `mongostat` utility. The only difference is that with the command-line utility,
    we see the stats as of the current time, whereas here we see the historical stats
    too.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个`opcounters`显示在特定时间点执行的操作数量。这应该类似于我们使用`mongostat`实用程序看到的内容。右侧的内容显示了数据库被锁定的时间百分比。下拉菜单列出了数据库名称。我们可以选择要查看统计信息的适当数据库。同样，这个统计数据可以使用`mongostat`实用程序来查看。唯一的区别是，使用命令行实用程序，我们可以看到当前时间的统计数据，而在这里我们也可以看到历史统计数据。
- en: In MongoDB, indexes are stored in BTrees and the next graph shows the number
    of times the BTree index was accessed, hit, and missed. At the minimum, the RAM
    should be enough to accommodate the indexes for optimum performance. So in this
    metric, the misses should be 0 or very low. A high number of misses results in
    a page fault for the index and possibly additional page faults for the corresponding
    data if the query is not covered, that is, all its data cannot be sourced from
    the index, which is a double blow for performance. One good practice while querying
    is to use projections and fetch only the necessary fields from the document. This
    is helpful whenever we have our selected fields present in an index, in which
    case the query becomes covered and all the necessary data is sourced from the
    index only. To learn more about on covered indexes, refer to the recipe *Creating
    index and viewing plans of queries* in [Chapter 2](ch02.html "Chapter 2. Command-line
    Operations and Indexes"), *Command-line Operations and Indexes*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在MongoDB中，索引存储在B树中，下图显示了B树索引被访问、命中和未命中的次数。最低限度，RAM应该足够容纳索引以实现最佳性能。因此，在这个度量标准中，未命中应该为0或非常低。未命中的次数过高会导致索引的页面错误，如果查询没有被覆盖，可能会导致相应数据的额外页面错误，也就是说，所有数据无法从索引中获取，这对性能来说是一个双重打击。在查询时的一个好的做法是使用投影，并且只从文档中获取必要的字段。每当我们选择的字段存在于索引中时，这对于查询是有帮助的，这种情况下查询变成了覆盖查询，所有必要的数据只从索引中获取。要了解更多关于覆盖索引的信息，请参考[第2章](ch02.html
    "第2章。命令行操作和索引")中的*创建索引和查看查询计划*这个章节。
- en: '![How it works…](img/B04831_06_16.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/B04831_06_16.jpg)'
- en: For busy applications if the volumes are very high, with multiple write and
    read operations contending for lock, the operations queue up. Untill Version 2.4
    of MongoDB, the locks are at the database level. Thus, even if the writes are
    happening on another collection, read operations on any collection in that database
    will block. This queuing operation affects the performance of the system and is
    a good indicator that the data might need to be sharded across to scale the system.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于繁忙的应用程序，如果卷非常大，多个写入和读取操作争夺锁定，操作排队。直到MongoDB的2.4版本，锁定是在数据库级别进行的。因此，即使在另一个集合上进行写入，对该数据库中的任何集合进行读取操作也会被阻塞。这种排队操作会影响系统的性能，并且是数据可能需要分片以扩展系统的良好指标。
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Remember, no value is defined as high or low; it is the acceptable value on
    an application-to-application basis.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，没有定义高或低的值；它是应用程序到应用程序基础上的可接受值。
- en: '![How it works…](img/B04831_06_18.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/B04831_06_18.jpg)'
- en: MongoDB flushes the data from the journal immediately and the data file periodically
    to disk. The following metrics give us the flush time per minute at a given point
    of time. If the flush takes up a significant percentage of the time per minute,
    we can safely say that the write operations are forming a bottleneck for performance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB立即从日志中刷新数据，并定期将数据文件刷新到磁盘。以下指标给出了在给定时间点每分钟的刷新时间。如果刷新占据了相当大的时间百分比，我们可以安全地说写操作正在形成性能瓶颈。
- en: '![How it works…](img/B04831_06_17.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/B04831_06_17.jpg)'
- en: There's more…
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We have seen monitoring of the MongoDB instances/cluster in this recipe. However,
    setting up alerts to get notifications when certain threshold values are crossed
    is what we still haven't seen. In the next recipe, we will see how to achieve
    this with a sample alert that is sent out over an e-mail when the page faults
    exceed a predetermined value.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们看到了如何监视MongoDB实例/集群。然而，设置警报以在某些阈值值被超过时收到通知，这是我们还没有看到的。在下一篇文章中，我们将看到如何通过一个示例警报来实现这一点，当页面错误超过预定值时会通过电子邮件发送警报。
- en: See also
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Monitoring the hardware, such as CPU usage, is pretty useful and MMS console
    does support that. However, it needs munin-node to be installed to enable CPU
    monitoring. Refer to the page [http://mms.mongodb.com/help/monitoring/configuring/](http://mms.mongodb.com/help/monitoring/configuring/)
    for setting up munin-node and hardware monitoring.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监视硬件，如CPU使用率，非常有用，MMS控制台也支持。然而，需要安装munin-node才能启用CPU监视。请参考页面[http://mms.mongodb.com/help/monitoring/configuring/](http://mms.mongodb.com/help/monitoring/configuring/)设置munin-node和硬件监视。
- en: For updating the monitoring agent, refer to the page [http://mms.mongodb.com/help/monitoring/tutorial/update-mms/](http://mms.mongodb.com/help/monitoring/tutorial/update-mms/).
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要更新监控代理，请参考页面[http://mms.mongodb.com/help/monitoring/tutorial/update-mms/](http://mms.mongodb.com/help/monitoring/tutorial/update-mms/)。
- en: Setting up monitoring alerts in MMS
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在MMS中设置监控警报
- en: In the previous recipe, we saw how to monitor various metrics from MMS console.
    This is a great way to see all the stats in one place and get an overview of the
    health of the MongoDB instances and cluster. However, it is not possible to monitor
    the system continuously, 24/7, for the support personnel and there has to be some
    mechanism to automatically send out alerts in case some threshold is exceeded.
    In this recipe we will set up an alert whenever the page faults exceeds 1000.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇文章中，我们看到了如何从MMS控制台监视各种指标。这是一个很好的方式，可以在一个地方看到所有的统计数据，并了解MongoDB实例和集群的健康状况。然而，不可能持续24/7监视系统，对于支持人员来说必须有一些机制在某些阈值超过时自动发送警报。在这篇文章中，我们将设置一个警报，每当页面错误超过1000时就会触发。
- en: Getting ready
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the previous recipe to set up Monitoring Mongo Instances using MMS.
    That is the only prerequisite for this recipe.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 参考上一篇文章，设置使用MMS监视Mongo实例。这是本篇文章的唯一先决条件。
- en: How to do it…
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: Click on the **Activity** option on the left side menu, and then **Alert Settings**.
    On the **Alert Settings** page, click on **Add Alert**.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击左侧菜单中的**活动**选项，然后单击**警报设置**。在**警报设置**页面上，单击**添加警报**。
- en: Add a new alert for the **Host** that is a primary instance and if the page
    faults exceed a given number, which is 1000 page faults per minute. The notification
    is chosen to be an e-mail in this case and the interval after which the alert
    will be sent is 10 minutes.![How to do it…](img/B04831_06_20.jpg)
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为**主机**添加一个新的警报，如果页面错误超过给定数量，即每分钟1000个页面错误。在这种情况下，通知选择为电子邮件，警报发送间隔为10分钟。![操作步骤...](img/B04831_06_20.jpg)
- en: Click on **Save** to save the alert.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击**保存**以保存警报。
- en: How it works…
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The steps were pretty simple and we were successful in setting up MMS alerts
    when the page faults exceeded 1000 per minute. As we saw in the previous recipe,
    no fixed value is classified as high or low. It is something that is acceptable
    for your system, which comes with benchmarking the system during the testing phases
    in your environment. Similar to page faults, there is a vast array of alerts that
    can be set up. Once an alert is raised, it will be sent every 10 minutes, as we
    have set, until the condition for sending the alerts is not met. In this case,
    if the number of page faults falls below 1000 or somebody manually acknowledges
    the alert, no further alerts will be sent further for that incident.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤非常简单，我们成功地设置了当页面错误超过每分钟1000次时的MMS警报。正如我们在上一篇文章中看到的，没有固定值被归类为高或低。这是可以接受的，需要在您的环境中的测试阶段对系统进行基准测试。与页面错误类似，还有大量可以设置的警报。一旦触发警报，将按照我们设置的每10分钟发送一次，直到不满足发送警报的条件为止。在这种情况下，如果页面错误数量低于1000或有人手动确认了警报，那么将不会再发送进一步的警报。
- en: 'As we see in the following screenshot, the alert is open and we can acknowledge
    the alert:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如下面的屏幕截图所示，警报已打开，我们可以确认警报：
- en: '![How it works…](img/B04831_06_21.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/B04831_06_21.jpg)'
- en: 'On clicking on **Acknowledge**, the following popup will let us choose the
    duration for which we will acknowledge:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 单击**确认**后，将弹出以下窗口，让我们选择确认的持续时间：
- en: '![How it works…](img/B04831_06_30.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/B04831_06_30.jpg)'
- en: This means that for this particular incident, no more alerts will be sent out
    until the selected time period elapses.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在这种特定情况下，直到所选时间段过去，将不会再发送警报。
- en: The Open alerts can be viewed by clicking on the **Activities** menu option
    on the left.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 单击左侧的**活动**菜单选项即可查看打开的警报。
- en: See also
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Visit the URL [http://www.mongodb.com/blog/post/five-mms-monitoring-alerts-keep-your-mongodb-deployment-track](http://www.mongodb.com/blog/post/five-mms-monitoring-alerts-keep-your-mongodb-deployment-track)
    for some of the important alerts that you should set up for your deployment
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问网址[http://www.mongodb.com/blog/post/five-mms-monitoring-alerts-keep-your-mongodb-deployment-track](http://www.mongodb.com/blog/post/five-mms-monitoring-alerts-keep-your-mongodb-deployment-track)了解一些应该为您的部署设置的重要警报
- en: Back up and restore data in Mongo using out-of-the-box tools
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用现成的工具备份和恢复Mongo中的数据
- en: In this recipe, we will look at some basic backup and restore operations using
    utilities such as `mongodump` and `mongorestore` to back up and restore files.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start a single instance of mongod. Refer to the recipe *Installing
    single node MongoDB* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting
    the Server"), *Installing and Starting the Server*, to start a mongo instance
    and connect to it from a mongo shell. We will need some data to backup. If you
    already have some data in your test database, that will be fine. If not, create
    some from the `countries.geo.json` file available in the code bundle using the
    following command:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How to do it…
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the data in the `test` database, execute the following (assuming we want
    to export the data to a local directory called `dump` in the current directory):'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Verify that there is data in the `dump` directory. All files will be `.bson`
    files, one per collection in the respective database folder created.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s import the data back into the mongo server using the following command.
    This is again with the assumption that we have the directory `dump` in the current
    directory with the required `.bson` files present in it:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works…
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just a couple of steps executed to export and restore the data. Let's now see
    what it exactly does and what the command-line options for this utility are. The
    `mongodump` utility is used to export the database into the `.bson` files, which
    can then be later used to restore the data in the database. The export utility
    exports one folder per database except local database, and then each of them will
    have one `.bson` file per collection. In our case, we used the `-oplog` option
    to export a part of the oplog too and the data will be exported to the `oplog.bson`
    file. Similarly, we import the data back into the database using the `mongorestore`
    utility. We explicitly ask the existing data to be dropped by providing the `--drop`
    option before the import and replay of the contents in the oplog if any.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mongodump` utility simply queries the collection and exports the contents
    to the files. The bigger the collection, the longer it will take to restore the
    contents. It is thus advisable to prevent write operations when the dump is being
    taken. In the case of sharded environments, the balancer should be turned off.
    If the dump is taken while the system is running, export with the `-oplog` option
    to export the contents of the oplog as well. This oplog can then be used to restore
    to the point in time data. The following table shows some of the important options
    available for the `mongodump` and `mongorestore` utility, first for `mongodump`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '| Option | Description |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| `--help` | Shows all the possible, supported options and a brief description
    of these options. |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '| `-h` or `--host` | The host to connect to. By default, it is localhost on
    port `27017`. If a standalone instance is to be connected to, we can set the hostname
    as `<hostname>:<port number>`. For a replica set, the format will be `<replica
    set name>/<hostname>:<port>,….<hostname>:<port>` where the comma-separated list
    of hostnames and port is called the **seed list**. It can contain all or a subset
    of hostnames in a replica set. |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| `--port` | The port number of the target MongoDB instance. This is not really
    relevant if the port number is provided in the previous `-h` or `--host` option.
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| `-u` or `--username` | Provides the username of the user using which the
    data would be exported. Since the data is read from all databases, the user is
    at least expected to have read privileges in all databases. |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| `-p` or `--password` | The password used in conjunction with the username.
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| `--authenticationDatabase` | The database in which the user credentials are
    kept. If not specified, the database specified in the `--db` option is used. |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| `-d` or `--db` | The database to back up. If not specified, then all the
    databases are exported. |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| `-c` or `--collection` | The collection in the database to be exported. |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| `-o` or `--out` | The directory to which the files will be exported. By default,
    the utility will create a dump folder in the current directory and export the
    contents to that directory. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| `--dbpath` | If we don''t intend to connect to the database server and instead
    directly read from the database file. The value is the path of the directory where
    the database files will be found. The server should not be up and running while
    reading directly from the database files as the export locks the data files, which
    can''t happen if a server is up and running. A lock file will be created in the
    directory while the lock is acquired. |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| `--oplog` | With the option enabled, the data from the oplog from the time
    the export process started is also exported. Without this option enabled, the
    data in the export will not represent a single point in time if writes are happening
    in parallel as the export process can take few hours and it simply is a query
    operation on all the collections. Exporting the oplog gives an option to restore
    to a point in time data. There is no need to specify this option if you are preventing
    write operations while the export is in progress. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: Similarly, for the `mongorestore` utility, here are the options. The meaning
    of the options `--help`, `-h`, or `--host`, `--port`, `-u`, or `--username`, `-p`
    or `--password`, `--authenticationDatabase`, `-d`, or `--db`, `-c` or `--collection`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '| Option | Description |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| `--dbpath` | If we don''t intend to connect to the database server and instead
    directly write to the database file, use this option. The value is the path of
    the directory where the database files will be found. The server should not be
    up and running while writing directly to the database files as the restore operation
    locks the data files, which can''t happen if a server is up and running. A lock
    file will be created in the directory while the lock is acquired. |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| `--drop` | Drop the existing data in the collection before restoring the
    data from the exported dumps. |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| `--oplogReplay` | If the data was exported while writes to the database were
    allowed and if the `--oplog` option was enabled during export, the oplog exported
    will be replayed on the data to bring all the data in the database to the same
    point in time. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| `--oplogLimit` | The value of this parameter is a number representing the
    time in seconds. This option is used in conjunction with `oplogReplay` command
    line option, which is used to tell the restore utility to replay the oplog and
    stop just at the limit specified by this option. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: You might think, *Why not copy the files and take a backup?* That works well
    but there are a few problems associated with it. First, you cannot get a point-in-time
    backup unless write operations are disabled. Secondly, the space used for backups
    is very high as the copy would also copy the 0 padded files of the database as
    against the `mongodump`, which exports just the data.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Having said that, filesystem snapshotting is a commonly used practice for backups.
    One thing to remember is while taking the snapshot the journal files and the data
    files need to come in the same snapshot for consistency.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: If you were using **Amazon Web Services** (**AWS**), it would be highly recommended
    that you upload your database backups to AWS S3\. As you may be aware, AWS offers
    extremely high data redundancy with a very low storage cost.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Download the script `generic_mongodb_backup.sh` from the Packt Publishing website
    and use it to automate your backup creation and upload to AWS S3.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Configuring MMS Backup service
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MMS Backup is a relatively new offering by MongoDB for real-time incremental
    backup of your MongoDB instances, replica sets, and shards, and offers you point
    in time recovery of your instances. The service is available as on-premise (in
    your data center) or cloud. However, we will demonstrate the on-cloud service
    that is the only option for the Community and Basic subscription. For more details
    on the available options, you can visit different product offerings by MongoDB
    at [https://www.mongodb.com/products/subscriptions](https://www.mongodb.com/products/subscriptions).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mongo MMS Backup service will work only on Mongo 2.0 and above. We will start
    a single server that we will backup. MMS backup relies on the oplog for continuous
    backup and since oplog is available only in replica sets, the server needs to
    be started as a replica set. Refer to the recipe *Connecting to a single node
    using a Python client* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting
    the Server"), *Installing and Starting the Server* to learn more about how to
    install Python and PyMongo, the Python client of Mongo.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you don't already have a MMS account, then log in to [https://mms.mongodb.com/](https://mms.mongodb.com/)
    and sign up for an account. For screenshots, refer to the recipe *Signing up for
    MMS and setting up an MMS monitoring agent* in this chapter.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a single instance of Mongo and replace the value of the appropriate filesystem
    path on your machine:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that the `smallfiles` and `oplogSize` are options only set for testing
    purposes and are not to be used in production.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a shell, connect to the instance in step 1 and initiate the replica set
    as follows:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The replica set will be up and running in some time.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Go back to the browser to `mms.mongodb.com`. Add a new host by clicking on the
    **+ Add Host** button. Set the type as replica set and the hostname as your hostname
    and the port as the default one `27017` in our case. Refer to the recipe *Signing
    up for MMS and setting up an MMS monitoring agent* for the screenshots of the
    **Add Host** process.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the host is successfully added, register for MMS backup by clicking on
    the **Backup** option the left and then **Begin Setup**.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An SMS or Google authenticator can be used for registration. If a smartphone
    is available with Android, iOS, or Blackberry OS, Google authenticator is a good
    option. For countries like India, Google Authenticator is the only option available.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assuming Google Authenticator is not configured already and we are planning
    to use it, we would need the app to be installed on your smartphone. Go to the
    respective app store of your mobile OS platform and install the Google Authenticator
    software.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the software installed on the phone, come back to the browser. You should
    see the following screen on selecting the Google Authenticator:![How to do it…](img/B04831_06_22.jpg)
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Begin the setup for a new account by scanning the QR code from the Google Authenticator
    application. If barcode scanning is a problem, you may choose to manually enter
    the key given on the right side of the screen.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the scanning or the key is entered successfully, your smartphone should
    show a 6-digit number that changes every 30 seconds. Enter that number in the
    **Authentication Code** box given on the screen.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is important not to delete this account in Google Authenticator on your phone
    as this will be used in future whenever we wish to change any settings related
    to backup, such as stopping backup, changing exclusion list, and almost any operation
    in MMS backup. The QR code and key will not be visible again once the setup is
    done. You will have to contact MongoDB support to get the configuration reset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Once the authentication is done, the next screen you should see is the billing
    address and billing details, such as the card you register. All charges below
    $5 are waived so you should be ok to try out a small test instance before being
    charged.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the credit card details are saved, we move ahead with the setup. We will
    have for installation a backup agent. This is a separate agent from the monitoring
    agent. Choose the appropriate platform and follow the instructions for its installation.
    Take a note of the location where the configuration files of the agent will be
    placed.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A new popup will contain the instruction/link to the archive/installer for the
    platform and the steps to install. It should also contain the `apiKey`. Take a
    note of the API key; we will need it in the next step.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the installation is complete, open the `local.config` file placed in the
    `config` directory of the agent installation (the location that was shown/modified
    during installation of the agent) and paste/type in the `apiKey` noted down in
    the previous step.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the agent is configured and started, click on the **Verify Agent** button.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the agent is successfully verified, we will start by adding a host to back
    up. The drop-down menu should show us all the replica sets and shards we have
    added. Select the appropriate one and set the **Sync source** as the primary instance,
    as that is the only one we have in our standalone instance. **Sync source** is
    only used for the initial sync process. Whenever we have a proper replica set
    with multiple instances, I prefer using a secondary as a sync process instance.![How
    to do it…](img/B04831_06_23.jpg)
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the instance is not started with security, leave the **DB Username** and
    **DB Password** fields blank.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Click on the button **Manage excluded namespaces** if you wish to skip a particular
    database or collection being backed up. If nothing is provided, by default everything
    will be backed up. The format for the collection name will be `<databasename>.<collection
    name>`. Alternatively, it could be just the database name, in which case all collections
    in that database would not be eligible for backup.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the details are all ok, click on the **Start** button. This should complete
    the setup of the backup process for a replica set on MMS.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The installation steps I performed were on Windows OS and the service needs
    to be started manually in that case. Press Windows + *R* and type `services.msc`.
    The name of the service is MMS Backup Agent.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The steps are pretty simple and this is all we need to do to set up a server
    for Mongo MMS backup. One important thing mentioned earlier is that MMS backup
    uses multifactor authentication for any operation once the backup is set up, and
    the account set up in Google Authenticator for MongoDB should not be deleted.
    There is no way to recover the original key used for setting up the authenticator.
    You will have to clear the Google Authenticator settings and set up a new key.
    To do that, click on the **Help & Support** link in the bottom-left corner of
    the screen and click on **How do I reset my two-factor authentication?**.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: On clicking the link, a new window will open up and ask for the username. An
    e-mail will be sent out to the registered e-mail ID which allows you to reset
    the two-factor authentication.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_31.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: As mentioned, oplog is used to synchronize the current MongoDB instance with
    the MMS service. However, for the initial sync, an instance's data files are used.
    The instance to use is provided by us when we set up the backup of replica set.
    As this is a resource-heavy operation, I prefer to use a secondary instance for
    this on busy systems so as not to add more querying on the primary instance by
    the MMS backup agent. Once the instance is done with initial synchronization,
    the oplog of the primary instance will be used to get the data on a continuous
    basis. Agent does write to a collection called `mms.backup` in admin database
    periodically.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'The backup agent for MMS backup is different from the MMS monitoring agent.
    Though there is no restriction on having them both running on the same machine,
    you might need to evaluate that before having such a setup in production. The
    safe bet would be to have them running on separate machines. Never run either
    of these agents with a mongod or mongos instance on the same box in production.
    There are a couple of important reasons why it is not recommended to run the agent
    on the same box as the mongod instances:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: The resource utilization of the agent is dependent on the cluster size it monitors.
    We don't want the agent to use a lot of resources affecting the performance of
    the production instance.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent could be monitoring a lot of server instances at a time. Since there
    is only one instance of this agent, we do not want it to go down during database
    server maintenance and restart.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The community edition of MongoDB built with SSL or enterprise versions with
    the SSL option used for communication between the client and the mongo server
    must do some additional steps. The first step is to check the **My deployment
    supports SSL for MongoDB connections** flag when we set up the replica set for
    backup (see step 15). Note the check box at the bottom in the screenshot that
    should be checked. Secondly, open the `local.config` file for the MMS configuration
    and look out for these two properties:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first is the fully qualified path of the certifying authority's certificate
    in PEM format. This certificate will be used to verify the certificate presented
    by the mongod instance running over SSL. The second property can be set to `false`
    if certificate verification is to be disabled, this is however not a recommended
    option. As far as the traffic between the backup agent and MMS backup is concerned,
    data sent from the agent to the MMS service over SSL is secure irrespective of
    whether SSL is enabled on your MongoDB instances or not. The data at rest in the
    data center for the backed up data is not encrypted.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'If security is enabled on the mongod instance, a username and password need
    to be provided, which will be used by the MMS backup agent. The username and password
    are provided while setting up backup for the replica set, as in step 15\. Since
    the agent needs to read the oplog, possibly all databases for the initial sync
    and write data to `admin` database the following roles are expected for the user:
    `readAnyDatabase`, `clusterAdmin`, `readWrite` on `admin` and `local` databases,
    and `userAdminAnyDatabase`. This is in case in version 2.4 and above. In versions
    prior to v2.4, we would expect the user to have read access on all the databases
    and read/write access to admin and local databases.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'While setting up a replica set for backup you may get an error like, `Insufficient
    oplog size: The oplog window must be at least 1 hours over the last 24 hours for
    all active replica set members. Please increase the oplog.`. While you may think
    this is always something to do with oplog size, it is also seen when the replica
    set has an instance that is in recovering state. This might feel misleading, so
    do look out for recovering nodes, if any, in the replica set while setting up
    a backup for a replica set. As per the MMS support too, it seems too restrictive
    to not set up a replica set for backup with some recovering nodes, and it might
    be fixed in the future.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Managing backups in MMS Backup service
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we saw how to set up MMS backup service and a simple
    one member replica set was set up for backup. Though a single member replica set
    makes no sense at all, it was needed as a standalone instance cannot be set up
    for backup in MMS. In this recipe, we dive deeper and look at the operations we
    can perform on the server that is set up for backup, such as starting, stopping,
    or terminating a backup; managing exclusion lists; managing backup snapshots and
    retention; and restoring to point in time data.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous recipe is all that is needed for this recipe. The necessary setup
    is expected to be done as we are going to use the same server we had set up for
    backup in this recipe.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the server up and running, let's import some data to it. It can be anything,
    but we chose to use the `countries.geo.json` file that was used in the last chapter.
    It should be available in the bundle downloaded from the Packt site.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the data into a collection called `countries` in the `test`
    database. Use the following command to do it. The following import command was
    executed with the current directory having the `countries.geo.json` file:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have already seen how to exclude namespaces when the replica set backup
    was being set up. We will now see how to exclude namespaces once the backup for
    a replica set is done. Click on the **Backup** menu option on the left and then
    the **Replica Set Status**, which opens by default when **Backup** is clicked.
    Click on the **Gear** button on the right side of the row where the replica set
    is shown. It should look like this:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/B04831_06_24.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: As we see in the preceding image, click on **Edit Excluded Namespaces** and
    type in the name of the collection that we want to exclude. Suppose we want to
    exclude the `applicationLogs` collection in `test` database, type in `test.applicationLogs`.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On saving, you will be asked to enter the token code that is currently displayed
    on your Google Authenticator.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On successful validation of the code, the namespace `test.applicationLogs` will
    be added to the list of namespaces excluded from being backed up.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We now shall see how to manage snapshot scheduling. A snapshot is the state
    of the database as of a particular point in time. To manage the snapshot frequency
    and retention policy, click on the **Gear** button shown in the previous screenshot
    and click on **Edit Snapshot Schedule**.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can see in the following image, we can set the times when the snapshots
    are taken and their retention period. We will discuss more on this in the next
    section. Any changes to it would need multifactor authentication to save the changes.![How
    to do it…](img/B04831_06_25.jpg)
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now see how we go about restoring the data using MMS backup. At any
    point in time whenever we want to restore the data, click on **Backup** and **Replica
    Set Status**/**Shard Cluster Status** click on **set/cluster name**.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On clicking it, we will see the snapshots that are saved against this set.
    It should look something like this:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/B04831_06_26.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: We have encircled some of the portions on the screen which we will see one by
    one.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: To restore to the time when the snapshot was taken, click on the **Restore this
    snapshot** link in the **Actions** column of the grid.![How to do it…](img/B04831_06_27.jpg)
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding image shows us how we can export the data either over HTTPS or
    SCP. We select HTTPS for now, and click **Authenticate**. We will see about SCP
    in the next section.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the token that is received either over SMS or seen on Google Authenticator
    and click **Finalize Request** on entering the auth code.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On successful authentication, click on **Restore Jobs**. This is a one-time
    download that will let you download the `tar.gz` archive. Click on the **download**
    link to download the `tar.gz` archive.![How to do it…](img/B04831_06_28.jpg)
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the archive is downloaded, extract it to get the database files within
    it.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop the mongod instance, replace the database files with the ones that are
    extracted, and restart the server to get the data as of the time when the snapshot
    was taken. Note that the database file will not contain data for the collection
    that was excluded from backup if all.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now see how to get point in time data using MMS backup.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Replica Set Status** / **Shard Cluster Status** and then the cluster/set
    which is to be restored.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the right-hand side of the screen, click on the **Restore** button.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This should give a list of available snapshots or you may enter a custom time.
    Check **Use Custom Point In Time**. Click on the **Date** field and select a date
    and a time to which you want to restore the data in Hours and Minutes and click
    **Next**. Note that the **Point in Time** feature only restores to a point in
    last 24 Hours.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, you will be asked to specify the format as HTTPS or SCP. The subsequent
    steps are similar to what we did on the previous occasion, from step 14 onwards.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After setting up the backup for a replica set, we imported random data into
    the database. Backup for this database would be done by MMS and later on we will
    restore the database using this backup. We saw how to exclude namespaces from
    being backed up in steps 2-5.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the snapshot and retention policy settings, we can see we have the
    choice of the time interval in which the snapshots are to be taken and the number
    of days they are to be retained (step 9). We can see that by default snapshots
    are taken every 6 hours and they are saved for 2 days. The snapshot that is taken
    at the end of the day gets saved for a week. The snapshot taken at the end of
    the week and month are saved for 4 weeks and 13 months respectively. A snapshot
    can be taken once every 6, 8, 12, and 24 hours. However, you need to understand
    the flip side of taking snapshots after long time duration. Suppose the last snapshot
    is taken at 18 hours; getting the data as of that time for restore is very easy
    as it is stored on the MMS backup servers. However, we need the data as of 21:30
    hours for restoration. Since MMS backup supports point in time backup, it would
    use the base snapshot at 18:00 hours and then just replay the changes on it after
    the snapshot is taken at 21:30 hours. This is similar to how an oplog would be
    replayed on the data. There is a cost for this replay and thus getting point in
    time backup is slightly more expensive than getting the data from a snapshot.
    Here, we had to replay the data for 3.5 hours, from 18:00 hours to 21:30 hours.
    Imagine if the snapshots were set to be taken after 12 hours and our first snapshot
    is taken at 00:00 hours, then we would have snapshots at 00:00 hours and 12:00
    hours every day. To restore the data as of 21:30 hours with 12:00 hours as the
    last snapshot, we will have to replay 9.5 hours of data. This is much more expensive.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: More frequent snapshots means more storage space usage but less time needed
    to restore a database to a given point in time. At the same time, less frequent
    snapshots require less storage but at the cost of more time to restore the data
    to a point in time. You need to decide and have a trade-off between these two,
    space and time of restoration. For the daily snapshot, we can choose retention
    of between 3 to 180 days. Similarly, for the weekly and monthly snapshots, the
    retention period can be chosen between 1 to 52 weeks and 1 to 36 months, respectively.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The screenshot in step 9 has a column for the expiry of the snapshot. This,
    for the first snapshot taken, is 1 year, whereas others expire in 2 days. The
    expiration is as per what we discussed in the last paragraph. On changing the
    expiration values, the old snapshots are not affected or adjusted as per the changed
    times. However, the new snapshots taken will be as per the modified settings for
    the retention and frequency.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw how to download the dump (step 10 onwards) and then use it to restore
    the data in the database. It was pretty straightforward and doesn''t need a lot
    of explanation except a couple of things. First, if the data is for a shard, there
    will be multiple folders—one for each shard and each of them will have the database
    files as against what we saw here in case of a replica set where we have a single
    folder with database files in it. Finally, let''s look at the screen when we choose
    SCP as the option:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04831_06_29.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: SCP is short for secure copy. The files will be copied over a secure channel
    to a machine's filesystem. The host that is given needs to have a public IP which
    will be used to SCP. This makes a lot of sense when we want the data from MMS
    to be delivered to a machine running on Unix OS on the cloud, for example, one
    of the AWS virtual instances. Rather than getting the file using HTTPS on our
    local machine and then reuploading it to the server on the cloud, you can specify
    the location on which the data needs to be copied in the Target Directory block,
    the hostname, and the credentials. There are a couple of ways of authentication
    too. A password is an easy way with an additional option to SSH key pair. If you
    have to configure host's firewalls on the cloud to allow incoming traffic over
    the SSH port, the public IP addresses are given at the bottom of the screen (`64.70.114.115/32`
    or `4.71.186.0/24` in our screenshot). You should whitelist them to allow incoming
    secure copy requests over port `22`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen running backups using MMS which uses oplogs for this purpose. There
    is a recipe called *Implementing triggers in Mongo using oplog* in [Chapter 5](ch05.html
    "Chapter 5. Advanced Operations"), *Advanced Operations*, which uses oplog to
    implement trigger-like functionalities. This concept is the backbone of the real-time
    backup used by MMS backup service.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
