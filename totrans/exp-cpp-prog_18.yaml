- en: Multithreading with Distributed Computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributed computing was one of the original applications of multithreaded
    programming. Back when every personal computer just contained a single processor
    with a single core, government and research institutions, as well as some companies
    would have multi-processor systems, often in the form of clusters. These would
    be capable of multithreaded processing; by splitting tasks across processors,
    they could speed up various tasks, including simulations, rendering of CGI movies,
    and the like.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays virtually every desktop-level or better system has more than a single
    processor core, and assembling a number of systems together into a cluster is
    very easy, using cheap Ethernet wiring. Combined with frameworks such as OpenMP
    and Open MPI, it's quite easy to expand a C++ based (multithreaded) application
    to run on a distributed system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Topics in this chapter include:'
  prefs: []
  type: TYPE_NORMAL
- en: Integrating OpenMP and MPI in a multithreaded C++ application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a distributed, multithreaded application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common applications and issues with distributed, multithreaded programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed computing, in a nutshell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to processing large datasets in parallel, it would be ideal if
    one could take the data, chop it up into lots of small parts, and push it to a
    lot of threads, thus significantly shortening the total time spent processing
    the said data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea behind distributed computing is exactly this: on each node in a distributed
    system one or more instances of our application run, whereby this application
    can either be single or multithreaded. Due to the overhead of inter-process communication,
    it''s generally more efficient to use a multithreaded application, as well as
    due to other possible optimizations--courtesy of resource sharing.'
  prefs: []
  type: TYPE_NORMAL
- en: If one already has a multithreaded application ready to use, then one can move
    straight to using MPI to make it work on a distributed system. Otherwise, OpenMP
    is a compiler extension (for C/C++ and Fortran) which can make it relatively painless
    to make an application multithreaded without refactoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, OpenMP allows one to mark a common code segment, to be executed
    on all slave threads. A master thread creates a number of slave threads which
    will concurrently process that same code segment. A basic *Hello World* OpenMP
    application looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: What one can easily tell from this basic sample is that OpenMP provides a C
    based API through the `<omp.h>` header. We can also see the section that will
    be executed by each thread, as marked by a `#pragma omp` preprocessor macro.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of OpenMP over the examples of multithreaded code which we saw
    in the preceding chapters, is the ease with which a section of code can be marked
    as being multithreaded without having to make any actual code changes. The obvious
    limitation that comes with this is that every thread instance will execute the
    exact same code and further optimization options are limited.
  prefs: []
  type: TYPE_NORMAL
- en: MPI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to schedule the execution of code on specific nodes, **MPI** (**Message
    Passing Interface**) is commonly used. Open MPI is a free library implementation
    of this, and used by many high-ranking supercomputers. MPICH is another popular
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: MPI itself is defined as a communication protocol for the programming of parallel
    computers. It is currently at its third revision (MPI-3).
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, MPI offers the following basic concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Communicators**: A communicator object connects a group of processes within
    an MPI session. It both assigns unique identifiers to processes and arranges processes
    within an ordered topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Point-to-point operations**: This type of operation allows for direct communication
    between specific processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collective functions**: These functions involve broadcasting communications
    within a process group. They can also be used in the reverse manner, which would
    take the results from all processes in a group and, for example, sum them on a
    single node. A more selective version would ensure that a specific data item is
    sent to a specific node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Derived datatype**: Since not every node in an MPI cluster is guaranteed
    to have the same definition, byte order, and interpretation of data types, MPI
    requires that it is specified what type each data segment is, so that MPI can
    do data conversion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One-sided communications**: These are operations which allow one to write
    or read to or from remote memory, or perform a reduction operation across a number
    of tasks without having to synchronize between tasks. This can be useful for certain
    types of algorithms, such as those involving distributed matrix multiplication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic process management**: This is a feature which allows MPI processes
    to create new MPI processes, or establish communication with a newly created MPI
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel I/O**: Also called MPI-IO, this is an abstraction for I/O management
    on distributed systems, including file access, for easy use with MPI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of these, MPI-IO, dynamic process management, and one-sided communication are
    MPI-2 features. Migration from MPI-1 based code and the incompatibility of dynamic
    process management with some setups, along with many applications not requiring
    MPI-2 features, means that uptake of MPI-2 has been relatively slow.
  prefs: []
  type: TYPE_NORMAL
- en: Implementations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The initial implementation of MPI was **MPICH**, by **Argonne National Laboratory**
    (**ANL**) and Mississippi State University. It is currently one of the most popular
    implementations, used as the foundation for MPI implementations, including those
    by IBM (Blue Gene), Intel, QLogic, Cray, Myricom, Microsoft, Ohio State University
    (MVAPICH), and others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another very common implementation is Open MPI, which was formed out of the
    merger of three MPI implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: FT-MPI (University of Tennessee)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LA-MPI (Los Alamos National Laboratory)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LAM/MPI (Indiana University)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These, along with the PACX-MPI team at the University of Stuttgart, are the
    founding members of the Open MPI team. One of the primary goals of Open MPI is
    to create a high-quality, open source MPI-3 implementation.
  prefs: []
  type: TYPE_NORMAL
- en: MPI implementations are mandated to support C and Fortran. C/C++ and Fortran
    along with assembly support is very common, along with bindings for other languages.
  prefs: []
  type: TYPE_NORMAL
- en: Using MPI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regardless of the implementation chosen, the resulting API will always match
    the official MPI standard, differing only by the MPI version that the library
    one has picked supports. All MPI-1 (revision 1.3) features should be supported
    by any MPI implementation, however.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the canonical Hello World (as, for example, found on the MPI
    Tutorial site: [http://mpitutorial.com/tutorials/mpi-hello-world/](http://mpitutorial.com/tutorials/mpi-hello-world/))
    for MPI should work regardless of which library one picks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When reading through this basic example of an MPI-based application, it''s
    important to be familiar with the terms used with MPI, in particular:'
  prefs: []
  type: TYPE_NORMAL
- en: '**World**: The registered MPI processes for this job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Communicator**: The object which connects all MPI processes within a session'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rank**: The identifier for a process within a communicator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processor**: A physical CPU, a singular core of a multi-core CPU, or the
    hostname of the system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this Hello World example, we can see that we include the `<mpi.h>` header.
    This MPI header will always be the same, regardless of the implementation we use.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the MPI environment requires a single call to `MPI_Init()`, which
    can take two parameters, both of which are optional at this point.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the size of the world (meaning, number of processes available) is the
    next step. This is done using `MPI_Comm_size()`, which takes the `MPI_COMM_WORLD`
    global variable (defined by MPI for our use) and updates the second parameter
    with the number of processes in that world.
  prefs: []
  type: TYPE_NORMAL
- en: The rank we then obtain is essentially the unique ID assigned to this process
    by MPI. Obtaining this UID is performed with `MPI_Comm_rank()`. Again, this takes
    the `MPI_COMM_WORLD` variable as the first parameter and returns our numeric rank
    as the second parameter. This rank is useful for self-identification and communication
    between processes.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining the name of the specific piece of hardware on which one is running
    can also be useful, particularly for diagnostic purposes. For this we can call
    `MPI_Get_processor_name()`. The returned string will be of a globally defined
    maximum length and will identify the hardware in some manner. The exact format
    of this string is implementation defined.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we print out the information we gathered and clean up the MPI environment
    before terminating the application.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling MPI applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to compile an MPI application, the `mpicc` compiler wrapper is used.
    This executable should be part of whichever MPI implementation has been installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using it is, however, identical to how one would use, for example, GCC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be compared to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This would compile and link our Hello World example into a binary, ready to
    be executed. Executing this binary is, however, not done by starting it directly,
    but instead a launcher is used, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output is from Open MPI running inside a Bash shell on a Windows
    system. As we can see, we launch four processes in total (4 ranks). The processor
    name is reported as the hostname for each process ("PC").
  prefs: []
  type: TYPE_NORMAL
- en: The binary to launch MPI applications with is called mpiexec or mpirun, or orterun.
    These are synonyms for the same binary, though not all implementations will have
    all synonyms. For Open MPI, all three are present and one can use any of these.
  prefs: []
  type: TYPE_NORMAL
- en: The cluster hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The systems an MPI based or similar application will run on consist of multiple
    independent systems (nodes), each of which is connected to the others using some
    kind of network interface. For high-end applications, these tend to be custom
    nodes with high-speed, low-latency interconnects. At the other end of the spectrum
    are so-called Beowulf and similar type clusters, made out of standard (desktop)
    computers and usually connected using regular Ethernet.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the fastest supercomputer (according to the TOP500 listing)
    is the Sunway TaihuLight supercomputer at the National Supercomputing Center in
    Wuxi, China. It uses a total of 40,960 Chinese-designed SW26010 manycore RISC
    architecture-based CPUs, with 256 cores per CPU (divided in 4 64-core groups),
    along with four management cores. The term *manycore* refers to a specialized
    CPU design which focuses more on explicit parallelism as opposed to the single-thread
    and general-purpose focus of most CPU cores. This type of CPU is similar to a
    GPU architecture and vector processors in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of these nodes contains a single SW26010 along with 32 GB of DDR3 memory.
    They are connected via a PCIe 3.0-based network, itself consisting of a three-level
    hierarchy: the central switching network (for supernodes), the supernode network
    (connecting all 256 nodes in a supernode), and the resource network, which provides
    access to I/O and other resource services. The bandwidth for this network between
    individual nodes is 12 GB/second, with a latency of about 1 microsecond.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graphic (from "The Sunway TaihuLight Supercomputer: System and
    Applications", DOI: 10.1007/s11432-016-5588-7) provides a visual overview of this
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/916608cf-a01d-43b6-86c2-af46982d0d28.png)'
  prefs: []
  type: TYPE_IMG
- en: For situations where the budget does not allow for such an elaborate and highly
    customized system, or where the specific tasks do not warrant such an approach,
    there always remains the "Beowulf" approach. A Beowulf cluster is a term used
    to refer to a distributed computing system constructed out of common computer
    systems. These can be Intel or AMD-based x86 systems, with ARM-based processors
    now becoming popular.
  prefs: []
  type: TYPE_NORMAL
- en: It's generally helpful to have each node in a cluster to be roughly identical
    to the other nodes. Although it's possible to have an asymmetric cluster, management
    and job scheduling becomes much easier when one can make broad assumptions about
    each node.
  prefs: []
  type: TYPE_NORMAL
- en: At the very least, one would want to match the processor architecture, with
    a base level of CPU extensions, such as SSE2/3 and perhaps AVX and kin, common
    across all nodes. Doing this would allow one to use the same compiled binary across
    the nodes, along with the same algorithms, massively simplifying the deployment
    of jobs and the maintenance of the code base.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the network between the nodes, Ethernet is a very popular option, delivering
    communication times measured in tens to hundreds of microseconds, while costing
    only a fraction of faster options. Usually each node would be connected to a single
    Ethernet network, as in this graphic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28e8fc11-68fd-4692-9f6e-93250348f221.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There is also the option to add a second or even third Ethernet link to each
    or specific nodes to give them access to files, I/O, and other resources, without
    having to compete with bandwidth on the primary network layer. For very large
    clusters, one could consider an approach such as that used with the Sunway TaihuLight
    and many other supercomputers: splitting nodes up into supernodes, each with their
    own inter-node network. This would allow one to optimize traffic on the network
    by limiting it to only associated nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of such an optimized Beowulf cluster would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a4a767b-6ab9-4f17-bc91-63a04a9452cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Clearly there is a wide range of possible configurations with MPI-based clusters,
    utilizing custom, off-the-shelf, or a combination of both types of hardware. The
    intended purpose of the cluster often determines the most optimal layout for a
    specific cluster, such as running simulations, or the processing of large datasets.
    Each type of job presents its own set of limitations and requirements, which is
    also reflected in the software implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Open MPI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the remainder of this chapter, we will focus on Open MPI. In order to get
    a working development environment for Open MPI, one will have to install its headers
    and library files, along with its supporting tools and binaries.
  prefs: []
  type: TYPE_NORMAL
- en: Linux and BSDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On Linux and BSD distributions with a package management system, it''s quite
    easy: simply install the Open MPI package and everything should be set up and
    configured, ready to be used. Consult the manual for one''s specific distribution,
    to see how to search for and install specific packages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On Debian-based distributions, one would use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command would install the Open MPI binaries, documentation, and
    development headers. The last two packages can be omitted on compute nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On Windows things get slightly complex, mostly because of the dominating presence
    of Visual C++ and the accompanying compiler toolchain. If one wishes to use the
    same development environment as on Linux or BSD, using MinGW, one has to take
    some additional steps.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter assumes the use of either GCC or MinGW. If one wishes to develop
    MPI applications using the Visual Studio environment, please consult the relevant
    documentation for this.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest to use and most up to date MinGW environment is MSYS2, which provides
    a Bash shell along with most of the tools one would be familiar with under Linux
    and BSD. It also features the Pacman package manager, as known from the Linux
    Arch distribution. Using this, it's easy to install the requisite packages for
    Open MPI development.
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing the MSYS2 environment from [https://msys2.github.io/](https://msys2.github.io/),
    install the MinGW toolchain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This assumes that the 64-bit version of MSYS2 was installed. For the 32-bit
    version, select i686 instead of x86_64\. After installing these packages, we will
    have both MinGW and the basic development tools installed. In order to use them,
    start a new shell using the MinGW 64-bit postfix in the name, either via the shortcut
    in the start menu, or by using the executable file in the MSYS2 `install` folder.
  prefs: []
  type: TYPE_NORMAL
- en: With MinGW ready, it's time to install MS-MPI version 7.x. This is Microsoft's
    implementation of MPI and the easiest way to use MPI on Windows. It's an implementation
    of the MPI-2 specification and mostly compatible with the MPICH2 reference implementation.
    Since MS-MPI libraries are not compatible between versions, we use this specific
    version.
  prefs: []
  type: TYPE_NORMAL
- en: Though version 7 of MS-MPI has been archived, it can still be downloaded via
    the Microsoft Download Center at [https://www.microsoft.com/en-us/download/details.aspx?id=49926](https://www.microsoft.com/en-us/download/details.aspx?id=49926).
  prefs: []
  type: TYPE_NORMAL
- en: 'MS-MPI version 7 comes with two installers, `msmpisdk.msi` and `MSMpiSetup.exe`.
    Both need to be installed. Afterwards, we should be able to open a new MSYS2 shell
    and find the following environment variable set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This output for the printenv command shows that the MS-MPI SDK and runtime
    was properly installed. Next, we need to convert the static library from the Visual
    C++ LIB format to the MinGW A format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We first copy the original LIB file into a new temporary folder in our home
    folder, along with the runtime DLL. Next, we use the gendef tool on the DLL in
    order to create the definitions which we will need in order to convert it to a
    new format.
  prefs: []
  type: TYPE_NORMAL
- en: This last step is done with dlltool, which takes the definitions file along
    with the DLL and outputs a static library file which is compatible with MinGW.
    This file we then copy to a location where MinGW can find it later when linking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to copy the MPI header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After copying this header file, we must open it and locate the section that
    starts with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Immediately above that line, we need to add the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This include adds the definition for `__int64`, which we will need for the code
    to compile correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, copy the header file to the MinGW `include` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: With this we have the libraries and headers all in place for MPI development
    with MinGW. allowing us to compile and run the earlier Hello World example, and
    continue with the rest of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Distributing jobs across nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to distribute MPI jobs across the nodes in a cluster, one has to either
    specify these nodes as a parameter to the `mpirun`/`mpiexec` command or make use
    of a host file. This host file contains the names of the nodes on the network
    which will be available for a run, along with the number of available slots on
    the host.
  prefs: []
  type: TYPE_NORMAL
- en: A prerequisite for running MPI applications on a remote node is that the MPI
    runtime is installed on that node, and that password-less access has been configured
    for that node. This means that so long as the master node has the SSH keys installed,
    it can log into each of these nodes in order to launch the MPI application on
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an MPI node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After installing MPI on a node, the next step is to set up password-less SSH
    access for the master node. This requires the SSH server to be installed on the
    node (part of the *ssh* package on Debian-based distributions). After this we
    need to generate and install the SSH key.
  prefs: []
  type: TYPE_NORMAL
- en: One way to easily do this is by having a common user on the master node and
    other nodes, and using an NFS network share or similar to mount the user folder
    on the master node on the compute nodes. This way all nodes would have the same
    SSH key and known hosts file. One disadvantage of this approach is the lack of
    security. For an internet-connected cluster, this would not be a very good approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is, however, a definitely good idea to run the job on each node as the same
    user to prevent any possible permission issues, especially when using files and
    other resources. With the common user account created on each node, and with the
    SSH key generated, we can transfer the public key to the node using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, we can copy the public key into the `authorized_keys` file on
    the node system while we are setting it up. If creating and configuring a large
    number of nodes, it would make sense to use an image to copy onto each node's
    system drive, use a setup script, or possibly boot from an image through PXE boot.
  prefs: []
  type: TYPE_NORMAL
- en: With this step completed, the master node can now log into each compute node
    in order to run jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the MPI host file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, in order to run a job on other nodes, we need to specify
    these nodes. The easiest way to do this is to create a file containing the names
    of the compute nodes we wish to use, along with optional parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To allow us to use names for the nodes instead of IP addresses, we have to
    modify the operating system''s host file first: for example, `/etc/hosts` on Linux:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we create a new file which will be the host file for use with MPI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: With this configuration, a job would be executed on both compute nodes, as well
    as the master node. We can take the master node out of this file to prevent this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without any optional parameter provided, the MPI runtime will use all available
    processors on the node. If it is desirable, we can limit this number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Assuming that both nodes are quad-core CPUs, this would mean that only half
    the cores on node0 would be used, and all of them on node1.
  prefs: []
  type: TYPE_NORMAL
- en: Running the job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Running an MPI job across multiple MPI nodes is basically the same as executing
    it only locally, as in the example earlier in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This command would tell the MPI launcher to use a host file called `my_hostfile`
    and run a copy of the specified MPI application on each processor of each node
    found in that host file.
  prefs: []
  type: TYPE_NORMAL
- en: Using a cluster scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to using a manual command and host files to create and start jobs
    on specific nodes, there are also cluster scheduler applications. These generally
    involve the running of a daemon process on each node as well as the master node.
    Using the provided tools, one can then manage resources and jobs, scheduling allocation
    and keeping track of job status.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most popular cluster management scheduler''s is SLURM, which short
    for Simple Linux Utility for Resource management (though now renamed to Slurm
    Workload Manager with the website at [https://slurm.schedmd.com/](https://slurm.schedmd.com/)).
    It is commonly used by supercomputers as well as many computer clusters. Its primary
    functions consist out of:'
  prefs: []
  type: TYPE_NORMAL
- en: Allocating exclusive or non-exclusive access to resources (nodes) to specific
    users using time slots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The starting and monitoring of jobs such as MPI-based applications on a set
    of nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing a queue of pending jobs to arbitrate contention for shared resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The setting up of a cluster scheduler is not required for a basic cluster operation,
    but can be very useful for larger clusters, when running multiple jobs simultaneously,
    or when having multiple users of the cluster wishing to run their own job.
  prefs: []
  type: TYPE_NORMAL
- en: MPI communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we have a functional MPI cluster, which can be used to execute
    MPI-based applications (and others, as well) in a parallel fashion. While for
    some tasks it might be okay to just send dozens or hundreds of processes on their
    merry way and wait for them to finish, very often it is crucial that these parallel
    processes are able to communicate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the true meaning of MPI (being "Message Passing Interface") comes
    into play. Within the hierarchy created by an MPI job, processes can communicate
    and share data in a variety of ways. Most fundamentally, they can share and receive
    messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'An MPI message has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: A sender
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A receiver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A message tag (ID)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A count of the elements in the message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An MPI datatype
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sender and receiver should be fairly obvious. The message tag is a numeric
    ID which the sender can set and which the receiver can use to filter messages,
    to, for example, allow for the prioritizing of specific messages. The data type
    determines the type of information contained in the message.
  prefs: []
  type: TYPE_NORMAL
- en: 'The send and receive functions look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: An interesting thing to note here is that the count parameter in the send function
    indicates the number of elements that the function will be sending, whereas the
    same parameter in the receive function indicates the maximum number of elements
    that this thread will accept.
  prefs: []
  type: TYPE_NORMAL
- en: The communicator refers to the MPI communicator instance being used, and the
    receive function contains a final parameter which can be used to check the status
    of the MPI message.
  prefs: []
  type: TYPE_NORMAL
- en: MPI data types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MPI defines a number of basic types, which one can use directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **MPI datatype** | **C equivalent** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `MPI_SHORT` | short int |'
  prefs: []
  type: TYPE_TB
- en: '| `MPI_INT` | int |'
  prefs: []
  type: TYPE_TB
- en: '| `MPI_LONG` | long int |'
  prefs: []
  type: TYPE_TB
- en: '| `MPI_LONG_LONG` | long long int |'
  prefs: []
  type: TYPE_TB
- en: '| `MPI_UNSIGNED_CHAR` | unsigned char |'
  prefs: []
  type: TYPE_TB
- en: '| `MPI_UNSIGNED_SHORT` | unsigned short int |'
  prefs: []
  type: TYPE_TB
- en: '| `MPI_UNSIGNED` | unsigned int |'
  prefs: []
  type: TYPE_TB
- en: '| `MPI_UNSIGNED_LONG` | unsigned long int |'
  prefs: []
  type: TYPE_TB
- en: '| `MPI_UNSIGNED_LONG_LONG` | unsigned long long int |'
  prefs: []
  type: TYPE_TB
- en: '| `MPI_FLOAT` | float |'
  prefs: []
  type: TYPE_TB
- en: '| `MPI_DOUBLE` | double |'
  prefs: []
  type: TYPE_TB
- en: '| `MPI_LONG_DOUBLE` | long double |'
  prefs: []
  type: TYPE_TB
- en: '| `MPI_BYTE` | char |'
  prefs: []
  type: TYPE_TB
- en: MPI guarantees that when using these types, the receiving side will always get
    the message data in the format it expects, regardless of endianness and other
    platform-related issues.
  prefs: []
  type: TYPE_NORMAL
- en: Custom types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to these basic formats, one can also create new MPI data types.
    These use a number of MPI functions, including `MPI_Type_create_struct`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'With this function, one can create an MPI type that contains a struct, to be
    passed just like a basic MPI data type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here we see how a new MPI data type called `mpi_car_type` is defined and used
    to message between two processes. To create a struct type like this, we need to
    define the number of items in the struct, the number of elements in each block,
    their byte displacement, and their basic MPI types.
  prefs: []
  type: TYPE_NORMAL
- en: Basic communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A simple example of MPI communication is the sending of a single value from
    one process to another. In order to do this, one needs to use the following listed
    code and run the compiled binary to start at least two processes. It does not
    matter whether these processes run locally or on two compute nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code was gratefully borrowed from [http://mpitutorial.com/tutorials/mpi-hello-world/](http://mpitutorial.com/tutorials/mpi-hello-world/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: There isn't a lot to this code. We work through the usual MPI initialization,
    followed by a check to ensure that our world size is at least two processes large.
  prefs: []
  type: TYPE_NORMAL
- en: The process with rank 0 will then send an MPI message of data type `MPI_INT`
    and value `-1`. The process with rank `1` will wait to receive this message. The
    receiving process specifies for `MPI_Status MPI_STATUS_IGNORE` to indicate that
    the process will not be checking the status of the message. This is a useful optimization
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the expected output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Here we start the compiled demo code with a total of two processes. The output
    shows that the second process received the MPI message from the first process,
    with the correct value.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For advanced MPI communication, one would use the `MPI_Status` field to obtain
    more information about a message. One can use `MPI_Probe` to discover a message's
    size before accepting it with `MPI_Recv`. This can be useful for situations where
    it is not known beforehand what the size of a message will be.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Broadcasting a message means that all processes in the world will receive it.
    This simplifies the broadcast function relative to the send function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The receiving processes would simply use a normal `MPI_Recv` function. All that
    the broadcast function does is optimize the sending of many messages using an
    algorithm that uses multiple network links simultaneously, instead of just one.
  prefs: []
  type: TYPE_NORMAL
- en: Scattering and gathering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scattering is very similar to broadcasting a message, with one very important
    distinction: instead of sending the same data in each message, instead it sends
    a different part of an array to each recipient. Its function definition looks
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Each receiving process will get the same data type, but we can specify how many
    items will be sent to each process (`send_count`). This function is used on both
    the sending and receiving side, with the latter only having to define the last
    set of parameters relating to receiving data, with the world rank of the root
    process and the relevant communicator being provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gathering is the inverse of scattering. Here multiple processes will send data
    that ends up at a single process, with this data sorted by the rank of the process
    which sent it. Its function definition looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: One may notice that this function looks very similar to the scatter function.
    This is because it works basically the same way, only this time around the sending
    nodes have to all fill in the parameters related to sending the data, while the
    receiving process has to fill in the parameters related to receiving data.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note here that the `recv_count` parameter relates to the
    amount of data received from each sending process, not the size in total.
  prefs: []
  type: TYPE_NORMAL
- en: There exist further specializations of these two basic functions, but these
    will not be covered here.
  prefs: []
  type: TYPE_NORMAL
- en: MPI versus threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One might think that it would be easiest to use MPI to allocate one instance
    of the MPI application to a single CPU core on each cluster node, and this would
    be true. It would, however, not be the fastest solution.
  prefs: []
  type: TYPE_NORMAL
- en: Although for communication between processes across a network MPI is likely
    the best choice in this context, within a single system (single or multi-CPU system)
    using multithreading makes a lot of sense.
  prefs: []
  type: TYPE_NORMAL
- en: The main reason for this is simply that communication between threads is significantly
    faster than inter-process communication, especially when using a generalized communication
    layer such as MPI.
  prefs: []
  type: TYPE_NORMAL
- en: 'One could write an application that uses MPI to communicate across the cluster''s
    network, whereby one allocates one instance of the application to each MPI node.
    The application itself would detect the number of CPU cores on that system, and
    create one thread for each core. Hybrid MPI, as it''s often called, is therefore
    commonly used, for the advantages it provides:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Faster communication** – using fast inter-thread communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fewer MPI messages** – fewer messages means a reduction in bandwidth and
    latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoiding data duplication** – data can be shared between threads instead
    of sending the same message to a range of processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing this can be done the way we have seen in previous chapters, by
    using the multithreading features found in C++11 and successive versions. The
    other option is to use OpenMP, as we saw at the very beginning of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The obvious advantage of using OpenMP is that it takes very little effort from
    the developer's side. If all that one needs is to get more instances of the same
    routine running, all it takes is are the small modifications to mark the code
    to be used for the worker threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The above code combines an OpenMP application with MPI. To compile it we would
    run for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, to run the application, we would use mpirun or equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The mpirun command would run two MPI processes using the hellohybrid binary,
    passing the environment variable we exported with the -x flag to each new process.
    The value contained in that variable will then be used by the OpenMP runtime to
    create that number of threads.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming we have at least two MPI nodes in our MPI host file, we would end up
    with two MPI processes across two nodes, each of which running eight threads,
    which would fit a quad-core CPU with Hyper-Threading or an octo-core CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Potential issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When writing MPI-based applications and executing them on either a multi-core
    CPU or cluster, the issues one may encounter are very much the same as those we
    already came across with the multithreaded code in the preceding chapters.
  prefs: []
  type: TYPE_NORMAL
- en: However, an additional worry with MPI is that one relies on the availability
    of network resources. Since a send buffer used for an `MPI_Send` call cannot be
    reclaimed until the network stack can process the buffer, and this call is a blocking
    type, sending lots of small messages can lead to one process waiting for another,
    which in turn is waiting for a call to complete.
  prefs: []
  type: TYPE_NORMAL
- en: This type of deadlock should be kept in mind when designing the messaging structure
    of an MPI application. One can, for example, ensure that there are no send calls
    building up on one side, which would lead to such a scenario. Providing feedback
    messages on, queue depth and similar could be used to the ease pressure.
  prefs: []
  type: TYPE_NORMAL
- en: MPI also contains a synchronization mechanism using a so-called barrier. This
    is meant to be used between MPI processes to allow them to synchronize on for
    example a task. Using an MPI barrier (`MPI_Barrier`) call is similarly problematic
    as a mutex in that if an MPI process does not manage to get synchronized, everything
    will hang at this point.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked in some detail at the MPI standard, along with a
    number of its implementations, specifically Open MPI, and we looked at how to
    set up a cluster. We also saw how to use OpenMP to easily add multithreading to
    existing codes.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the reader should be capable of setting up a basic Beowulf or
    similar cluster, configuring it for MPI, and running basic MPI applications on
    it. How to communicate between MPI processes and how to define custom data types
    should be known. In addition, the reader will be aware of the potential pitfalls
    when programming for MPI.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take all our knowledge of the preceding chapters
    and see how we can combine it in the final chapter, as we look at general-purpose
    computing on videocards (GPGPU).
  prefs: []
  type: TYPE_NORMAL
