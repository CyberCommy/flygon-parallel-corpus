- en: Just Enough Linear Algebra for Machine Learning with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Package imports and initial setup for vectors and matrices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating DenseVector and setup with Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating SparseVector and setup with Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating DenseMatrix and setup with Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using sparse local matrices with Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing vector arithmetic using Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing matrix arithmetic with Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed matrices in Spark 2.0 ML library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring RowMatrix in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring distributed IndexedRowMatrix in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring distributed CoordinateMatrix in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring distributed BlockMatrix in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear algebra is the cornerstone of **machine learning** (**ML**) and **mathematical**
    **programming** (**MP**). When dealing with Spark's machine library, one must
    understand that the Vector/Matrix structures provided by Scala (imported by default)
    are different from the Spark ML, MLlib Vector, Matrix facilities provided by Spark.
    The latter, powered by RDDs, is the desired data structure if you are going to
    use Spark (that is, parallelism) out of the box for large-scale matrix/vector
    computation (for example, SVD implementation alternatives with more numerical
    accuracy, desired in some cases for derivatives pricing and risk analytics). The
    Scala Vector/Matrix libraries provide a rich set of linear algebra operations
    such as dot product, additions, and so on, that still have their own place in
    an ML pipeline. In summary, the key difference between using Scala Breeze and
    Spark or Spark ML is that the Spark facility is backed by RDDs which allows for
    simultaneous distributed, concurrent computing, and resiliency without requiring
    any additional concurrency module or extra effort (for example, Akka + Breeze).
  prefs: []
  type: TYPE_NORMAL
- en: Almost all machine learning algorithms use some form of classification or regression
    mechanism (not necessarily linear) to train a model and then proceed to minimize
    errors by comparing the training output to the actual output. For example, any
    implementation of a recommendation system in Spark will heavily rely on matrix
    decomposition, factorization, approximation, or **Single Value Decomposition**
    (**SVD**). Another machine learning area of interest dealing with dimensionality
    reduction for big datasets is **Principal Component Analysis** (**PCA**), which
    relies heavily on linear algebra, factorization, and matrix manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: When we examined the Spark ML and MLlib algorithms' source code for the first
    time in Spark 1.x.x, we quickly noticed that Vectors and Matrices use RDDs as
    the base for many prominent algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we revisited the source code for Spark 2.0 and machine learning libraries,
    we noticed some interesting changes that need to be considered going forward.
    Here is an example of such changes from Spark 1.6.2 to Spark 2.0.0 that impacted
    some of our linear algebra code with Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous version (Spark 1.6.x), you can convert the `DenseVector` or
    `SparseVector` (refer to [https://spark.apache.org/docs/1.5.2/api/java/org/apache/spark/mllib/linalg/Vectors.html](https://spark.apache.org/docs/1.5.2/api/java/org/apache/spark/mllib/linalg/Vectors.html))
    directly by using the `toBreeze()` function, as shown in the following code base:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In Spark 2.0, the `toBreeze()` function has not only been changed to `asBreeze()`,
    but it has also been demoted to a private function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To remedy this, use one of the following code snippets to convert the preceding
    vector to the commonly used `BreezeVector` instance:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Scala is a concise language in which both object-oriented and functional programming
    paradigms can coexist without conflict. While in the machine learning paradigm,
    functional programming is preferred, there is nothing wrong with using the object-oriented
    approach for initial data collection and presentation at a later stage.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of large-scale distributed matrices, our experience shows that when
    approaching large matrix sets 10⁹ to 10^(13) to 10^(27), and so on, you have to
    take a deeper look at the resulting network operation and shuffling that are inherent
    in a distributed operation. Based on our experience, the combination of local
    and distributed matrix/vector operations (for example, dot product, multiplication,
    and so on) work best when you operate at scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts the categorization of available Spark vectors
    and matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Package imports and initial setup for vectors and matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can program in Spark or use vector and matrix artifacts, we need to
    first import the right packages and then set up `SparkSession` so we can gain
    access to the cluster handle.
  prefs: []
  type: TYPE_NORMAL
- en: In this short recipe, we highlight a comprehensive number of packages that can
    cover most of the linear algebra operations in Spark. The individual recipes that
    follow will include the exact subset required for the specific program.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for vector and matrix manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the logging level to warning and error to cut down on output. See the
    previous step for the package requirement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the Spark context and application parameters so Spark can run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prior to Spark 2.0, the SparkContext and SQLContext had to be initialized separately.
    Refer to the following code snippet if you plan to run the code in previous versions
    of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up the application parameters so Spark can run (using Spark 1.5.2 or Spark
    1.6.1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SparkSession is the new entry point into the cluster in Spark 2.x.x and above.
    SparkSession unifies access entry to the cluster and all things data. It unifies
    access to SparkContext, SQLContext, or HiveContext, while making it easier to
    work with the DataFrame and Dataset APIs. We will revisit the SparkSession with
    a dedicated recipe in [Chapter 4](part0200.html#5UNGG0-4d291c9fed174a6992fd24938c2f9c77),
    *Common Recipes for Implementing a Robust Machine Learning System*.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following figure for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The documentation for method calls can be seen at [https://spark.apache.org/docs/2.0.0/api/scala/#org.apache.spark.sql.SparkSession](https://spark.apache.org/docs/2.0.0/api/scala/#org.apache.spark.sql.SparkSession).
  prefs: []
  type: TYPE_NORMAL
- en: Creating DenseVector and setup with Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore `DenseVectors` using the Spark 2.0 machine library.
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides two distinct types of vector facilities (dense and sparse) for
    storing and manipulating feature vectors that are going to be used in machine
    learning or optimization algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we examine `DenseVector` examples that you would most likely
    use for implementing/augmenting existing machine learning programs. These examples
    also help to better understand Spark ML or MLlib source code and the underlying
    implementation (for example, Single Value Decomposition).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here we look at creating an ML vector feature (with independent variables)
    from arrays, which is a common use case. In this case, we have three almost fully
    populated Scala arrays corresponding to customer and product feature sets. We
    convert these arrays to the corresponding `DenseVectors` in Scala:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the variables to create the vectors from the array. Convert from the array
    to the `DenseVector`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to create a `DenseVector` and to assign values via initialization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is the most cited case and is often used in class constructors that deal
    with batch input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is another example to show on-the-fly conversion from a string
    to a double during the initialization. Here we start with a string and invoke
    `toDouble` inline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The signature for this method constructor is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The method inherits from the following which makes its concrete methods available
    to all routines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'There are several method calls that are of interest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make a deep copy of the vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert to the `SparseVector`. You will do this if your vector is long and
    the density decreases after a number of operations (for example, zero out non-contributing
    members):'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Find the number of non-zero elements. This is useful so you can convert on-the-fly
    to the SparseVector if the density ID is low:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the vector to the array. This is often necessary when dealing with
    distributed operations that require close interactions with RDDs or proprietary
    algorithms that use Spark ML as a subsystem:'
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One must be careful not to mix vector facilities provided by the `Breeze` library
    with Spark ML vectors. To work with ML library algorithms, you are required to
    use its native data structures, but you can always convert from ML vectors to
    `Breeze`, do all your math operations, and then convert to Spark's desired data
    structure when using the ML library algorithms (for example, ALS or SVD).
  prefs: []
  type: TYPE_NORMAL
- en: We need the vector and matrix import statements so we can work with the ML library
    itself, otherwise the Scala vector and matrix will be used by default. This is
    the source of much confusion when the programs fail to scale on cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts a pictorial view which should help clarify the
    subject:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for constructor is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/DenseVector.html#constructor_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/DenseVector.html#constructor_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for method calls is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/DenseVector.html#method_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/DenseVector.html#method_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating SparseVector and setup with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we examine several types of `SparseVector` creation. As the
    length of the vector increases (millions) and the density remains low (few non-zero
    members), then sparse representation becomes more and more advantageous over the
    `DenseVector`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary packages for vector and matrix manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the Spark context and application parameters so Spark can run. See the
    first recipe in this chapter for more details and variations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we look at creating a ML SparseVector that corresponds to its equivalent
    DenseVector. The call consists of three parameters: Size of the vector, indexes
    to non-zero data, and finally, the data itself.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following example, we can compare the dense versus SparseVector creation.
    As you can see, the four elements that are non-zero (5, 3, 8, 9) correspond to
    locations (0, 2, 18, 19) while the number 20 indicates the total size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: To understand the data structure better, we compare the output and some of the
    important attributes that help us, especially with dynamic programming using vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First we take a look at the printout for the DenseVector to see its representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we take a look at the printout for the SparseVector to see its internal
    representation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If we compare and contrast the internal representation and the number of elements
    versus active and non-zero, you will see that the SparseVector only stores non-zero
    elements and indexes to reduce storage requirement.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can convert back and forth between sparse and DenseVectors as needed. The
    reason that you might want to do this is that external math and linear algebra
    do not conform to Spark''s internal representation. We made the variable type
    explicit to make the point, but you can eliminate that extra declaration in actual
    practice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The signature for this method constructor is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The method inherits from the following which makes its concrete methods available
    to all routines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'There are several method calls related to vectors that are of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make a deep copy of the vector:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert to the `SparseVector`. You will do this if your vector is long and
    the density decreases after a number of operations (for example, zero out non-contributing
    members):'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Find the number of non-zero elements. This is useful so you can convert on-the-fly
    to the SparseVector if the density ID is low.
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the vector to an array. This is often necessary when dealing with distributed
    operations that require 1:1 interactions with RDDs or proprietary algorithms that
    use Spark ML as a subsystem:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One must remember that the dense and SparseVectors are local vectors and they
    must not be confused with the distributed facilities (for example, distributed
    matrices such as the RowMatrix class).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The underlying math operations for the vectors on a local machine will be provided
    by two libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Breeze**: [http://www.scalanlp.org/](http://www.scalanlp.org/)'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**JBLAS**: [http://jblas.org/](http://jblas.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is another data structure related directly to Vectors called LabeledPoint,
    which we covered in [Chapter 4](part0200.html#5UNGG0-4d291c9fed174a6992fd24938c2f9c77), *Common
    Recipes for Implementing a Robust Machine Learning System*. In short, it is a
    data structure corresponding to `LIBSVM` and `LIBLINEAR` formats for storing ML
    data consisting of a feature vector plus a label (for example, independent and
    dependent variables in a regression):'
  prefs: []
  type: TYPE_NORMAL
- en: '**LIBSVM**: [http://www.csie.ntu.edu.tw/~cjlin/libsvm/](http://www.csie.ntu.edu.tw/~cjlin/libsvm/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LIBLINEAR**: [http://www.csie.ntu.edu.tw/~cjlin/liblinear/](http://www.csie.ntu.edu.tw/~cjlin/liblinear/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for constructor is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/SparseVector.html#constructor_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/SparseVector.html#constructor_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for method calls is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/SparseVector.html#method_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/SparseVector.html#method_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating dense matrix and setup with Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore matrix creation examples that you most likely would
    need in your Scala programming and while reading the source code for many of the
    open source libraries for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides two distinct types of local matrix facilities (dense and sparse)
    for storage and manipulation of data at a local level. For simplicity, one way
    to think of a matrix is to visualize it as columns of Vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key to remember here is that the recipe covers local matrices stored on
    one machine. We will use another recipe, *D**istributed matrices in the Spark2.0
    ML library*, covered in this chapter, for storing and manipulating distributed
    matrices.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary packages for vector and matrix manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the Spark session and application parameters so Spark can run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we look at creating an ML vector feature from Scala arrays. Let us define
    a 2x2 dense matrix and instantiate it with an array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Constructing a dense matrix and assigning values via initialization in a single
    step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Construct a dense local matrix directly by defining the array inline. This
    is an array of 3x3 and has nine members. You can think of it as three columns
    of three vectors (3x3):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This is another example to show inline instantiation of a dense local matrix
    with vectors. This is a common case in which you collect vectors into a matrix
    (column order) and then perform an operation on the entire set. The most common
    case is to collect the vectors and then use a distributed matrix to do distributed
    parallel operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Scala, we use the `++` operator with arrays to achieve concatenation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The signatures for this method constructor are (Column-major dense matrix):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The method inherits from the following which makes their concrete methods available
    to all routines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: interface class java.lang.Object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: java.io.Serializable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several method calls that are of interest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generate the diagonal matrix from the supplied values in the vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an identity matrix. An identity matrix is a matrix that has diagonals
    as 1 and any other element as 0:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Keep track of whether the matrix is transposed:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a matrix with a set of random numbers - drawn from uniform distribution:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a matrix with a set of random numbers - drawn from gaussian distribution:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Transpose the matrix:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Make a deep copy of the vector:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert to a SparseVector. You will do this if your vector is long and the
    density decreases after a number of operations (for example, zero out non-contributing
    members):'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Find the number of non-zero elements. This is useful so you can convert on-the-fly
    to a SparseVector if the density ID is low:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Get all the values stored in Matrix:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most difficult part of working with matrices in Spark is to getting used
    to column order versus row order. It is key to remember that Spark ML uses underlying
    libraries that work better with column stored mechanisms. Here is an example to
    demonstrate:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a matrix definition which defines a 2x2 matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The matrix is actually stored as :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: You move from left to right in the value set and then from column to column
    for the placement in the Matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the assumption that the matrix is stored row wise is not in
    alignment with the Spark approach. The following order is not correct from Spark''s
    perspective:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for constructor is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/DenseMatrix.html#constructor_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/DenseMatrix.html#constructor_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation For method calls is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/DenseMatrix.html#method_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/DenseMatrix.html#method_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using sparse local matrices with Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we concentrate on SparseMatrix creation. In the previous recipe,
    we saw how a local dense matrix is declared and stored. A good number of machine
    learning problem domains can be represented as a set of features and labels within
    the matrix. In large-scale machine learning problems (for example, progression
    of a disease through large population centers, security fraud, political movement
    modeling, and so on), a good portion of the cells will be 0 or null (for example,
    the current number of people with a given disease versus the healthy population).
  prefs: []
  type: TYPE_NORMAL
- en: To help with storage and efficient operation in real time, sparse local matrices
    specialize in storing the cells efficiently as a list plus an index, which leads
    to faster loading and real time operations.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary packages for vector and matrix manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the Spark context and application parameters so Spark can run - See
    the first recipe in this chapter for more details and variations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The creation of a SparseMatrix is a little bit more complicated due to the way
    we store the sparse presentation as Compressed Column Storage (CCS), also referred
    to as the Harwell-Boeing SparseMatrix format. Please see, *How it works...* for
    a detailed explanation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We declare and create a local 3x2 SparseMatrix with only three non-zero members:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s examine the output so we fully understand what is happening at a lower
    level. The three values will be placed at (0,0),(1,1),(2,1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'To clarify further, here is the code for the SparseMatrix that is illustrated
    on Spark''s documentation pages of the SparseMatrix (see following section titled
    *See also*). This is a 3x3 Matrix with six non-zero values. Note that the order
    of the declaration is: Matrix Size, Column Pointers, Row Indexes, and the Value
    as the last member:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Column Pointers = [0,2,3,6]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Row Indexes = [0,2,1,0,1,2]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-Zero Values = [1.0,2.0,3.0,4.0,5.0,6.0]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our experience, most of the difficulties with SparseMatrices come from a
    lack of understanding of the difference between **Compressed Row Storage** (**CRS**)
    and **Compressed Column Storage** (**CCS**). We highly recommend that the reader
    researches this topic in depth to clearly understand the differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, the CCS format is used by Spark for the transposed target matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two distinct signatures for this method call constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`SparseMatrix (int numRows, int numCols, int[] colPtrs, int[] rowIndices, double[]
    values)`'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SparseMatrix(int numRows, int numCols, int[] colPtrs, int[] rowIndices, double[]
    values, boolean isTransposed)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In option number two, we are indicating that the matrix is declared as transposed
    already, so the matrix will be treated differently.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method inherits from the following which makes their concrete methods available
    to all routines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: interface class java.lang.Object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: java.io.Serializable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several method calls that are of interest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generate the diagonal matrix from the supplied values in the vector:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an identity matrix. An identity matrix is a matrix that has diagonals
    as 1 and any other element as 0:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Keep track of whether the matrix is transposed:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a matrix with a set of random numbers - drawn from uniform distribution:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a matrix with a set of random numbers - drawn from gaussian distribution:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Transpose the matrix:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Make a deep copy of the vector
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert to a SparseVector. You will do this if your vector is long and the
    density decreases after a number of operations (for example, zero out non-contributing
    members):'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Find the number of non-zero elements. This is useful so you can convert on-the-fly
    to the SparseVector if the density ID is low:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Get all the values stored in the Matrix:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'There are other calls corresponding to the specific operation for the SparseMatrix.
    The following is a sample, but we strongly recommend that you familiarize yourself
    with the manual pages (see the *There''s more...* section):'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Get Row Indexes: `int rowIndices()`'
  prefs:
  - PREF_OL
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check for transposition: `booleanisTransposed()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get Column pointers: `int[]colPtrs()`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To reiterate, in a lot of machine learning applications, you end up dealing
    with sparsity due to the large dimensional nature of the feature space that is
    not linearly distributed. To illustrate, we take the simplest case in which we
    have 10 customers indicating their affinity for four themes in the product line:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Theme 1** | **Theme 2** | **Theme 3** | **Theme 4** |'
  prefs: []
  type: TYPE_TB
- en: '| **Cust 1** | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cust 2** | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cust 3** | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cust 4** | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cust 5** | 1 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cust 6** | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cust 7** | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cust 8** | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cust 9** | 1 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cust 10** | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: As you can see, most of the elements are 0 and storing them as a dense matrix
    is not desirable while we increase the number of customers and themes to tens
    of millions (M x N). The SparseVector and matrix help with the storage and operation
    of these sparse structures in an efficient way.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for constructor is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/SparseMatrix.html#constructor_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/SparseMatrix.html#constructor_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for method calls is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/SparseMatrix.html#method_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/SparseMatrix.html#method_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing vector arithmetic using Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore vector addition in the Spark environment using the
    `Breeze` library for underlying operations. Vectors allow us to collect features
    and then manipulate them via linear algebra operations such as add, subtract,
    transpose, dot product, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary packages for vector and matrix manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the Spark session and application parameters so Spark can run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the Vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'We convert the vectors from the Spark public interface to a `Breeze` (library)
    artifact so we can use a rich set of operators provided for Vector manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at the output and understand the results. For an operational understanding
    of vector addition, subtraction, and multiplication, see the *How it works...*
    section in this recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Vector operations using both sparse and dense vectors with the Breeze library
    conversion are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'This is an alternate way, but it has the drawback of using a private function
    (see the actual source code for Spark 2.x.x itself). We recommend the method presented
    previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'We take a look at the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vectors are mathematical artifacts that allow us to express magnitude and direction.
    In machine learning, we collect object/user preferences into vectors and matrices
    in order to take advantage of distributed operations at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Vectors are tuples of numbers usually corresponding to some attributes collected
    for machine learning algorithms. The vectors are usually real numbers (measured
    values), but many times we use binary values to show the presence or absence of
    a preference or bias for a particular topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'A vector can be thought of as either a row vector or a column vector. The column
    vector presentation is more suitable for ML thinking. The column vector is represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The row vector is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Vector addition is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00049.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Vector subtraction is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00050.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Vector multiplication or "dot" product is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00051.jpeg)![](img/00052.gif)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The public interfaces offered by the Spark ML and MLlib library, whether used
    for sparse or dense vectors, currently lacks the necessary operators to do full
    vector arithmetic. We must convert our local vectors to the `Breeze` library vector
    to have the operators available for linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to Spark 2.0, the method for conversion to `Breeze` (`toBreeze`) was available
    to use, but now the method has changed to `asBreeze()` and made private! A quick
    read of the source code is necessary to understand the new paradigm. Perhaps the
    change reflects Spark's core developers' desire to have less dependency on an
    underlying `Breeze` library.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using any version of Spark prior to Spark 2.0 (Spark 1.5.1 or 1.6.1),
    use the following code snippets for conversion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-Spark 2.0 example 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Pre-spark 2.0 example 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Breeze` library documentation is available at [http://www.scalanlp.org/api/breeze/#breeze.package](http://www.scalanlp.org/api/breeze/#breeze.package)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Linalg` library documentation is available at [https://spark.apache.org/docs/latest/api/java/allclasses-noframe.html](https://spark.apache.org/docs/latest/api/java/allclasses-noframe.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing matrix arithmetic using Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore matrix operations such as addition, transpose, and
    multiplication in Spark. The more complex operations such as inverse, SVD, and
    so on, will be covered in future sections. The native sparse and dense matrices
    for the Spark ML library provide multiplication operators so there is no need
    to convert to `Breeze` explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Matrices are the workhorses of distributed computing. ML features that are collected
    can be arranged in a matrix configuration and operated at scale. Many of the ML
    methods such as **ALS** (**Alternating Least Square**) and **SVD** (**Singular
    Value Decomposition**) rely on efficient matrix and vector operations to achieve
    large-scale machine learning and training.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary packages for vector and matrix manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the Spark session and application parameters so Spark can run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Multiply the matrix and vector and print the results. This is an extremely
    useful operation which becomes a common theme in most Spark ML cases. We use a
    `SparseMatrix` to demonstrate the fact that the Dense, Sparse, and Matrix are
    interchangeable and only the density (for example, the percent of non-zero elements)
    and performance should be the criteria for selection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Multiplying a `DenseMatrix` with `DenseVector`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is provided for completeness and will help the user to follow the matrix
    and vector multiplication more easily without worrying about sparsity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: We demonstrate the transposing of a Matrix, which is an operation to swap rows
    with columns. It is an important operation and used almost on a daily basis if
    you are involved in Spark ML or data engineering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here we demonstrate two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transposing a `SparseMatrix` and examining the new resulting matrix via the
    output:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Demonstrating that the transpose of a transpose yields the original matrix:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Transposing a dense matrix and examining the new resulting matrix via the output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes it easier to see how row and column indexes are swapped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: We now look at matrix multiplication and how it would look in code.
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We declare two 2x2 Dense Matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A matrix can be thought of as columns of vectors. Matrices are the power tools
    for distributed computation involving linear algebra transformation. A variety
    of attributes or feature representation can be collected and operated upon via
    matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, matrices are two-dimensional *m x n* arrays of numbers (usually real
    numbers) whose elements can be referenced using a two-element subscript, *i* and
    *j*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A matrix is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00053.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'A matrix transpose is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Matrix multiplication is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00055.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Vector matrix multiplication or "dot" product is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00056.gif)![](img/00057.gif)'
  prefs: []
  type: TYPE_IMG
- en: '*Distributed matrices in the Spark 2.0 ML library*: In the next four recipes,
    we will cover the four types of distributed matrices in Spark. Spark provides
    full support for distributed matrices baked by RDDs right out of the box. The
    fact that Spark supports distributed computing does not relieve the developer
    from planning their algorithms with parallelism in mind.'
  prefs: []
  type: TYPE_NORMAL
- en: The underlying RDDs provide full parallelism and fault tolerance over the underlying
    data that is stored in the matrix. Spark is bundled with MLLIB and LINALG, which
    jointly provide a public interface and support for matrices that are not local
    and need full cluster support due to their size or complexity of chained operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark ML provides four types of distributed matrices to support parallelism:
    `RowMatrix`, `IndexedRowMatrix`, `CoordinateMatrix`, and `BlockMatrix`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RowMatrix`: Represents a row-oriented distributed matrix compatible with ML
    library'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IndexedRowMatrix`: Similar to `RowMatrix` with one additional benefit of indexing
    the rows. This is a specialized version of `RowMatrix` in which the matrix itself
    is created from the RDD of `IndexedRow` (Index, Vector) data structure. To visualize
    it, imagine a matrix where each row is a pair (long, RDD) and the work of pairing
    them (`zip` function) is done for you. This will allow you to carry the Index
    together with the RDD along its computational path in a given algorithm (matrix
    operations at scale)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CoordinateMatrix`: A very useful format which is used for coordinates (for
    example, *x*, *y*, *z* coordinates in a projection space)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BlockMatrix`: A distributed matrix made of blocks of locally maintained matrices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cover the creation of the four types in a brief recipe and then quickly move
    to a more complicated (code and concept) use case involving `RowMatrix` which
    is a typical ML use case involving a massively parallel distributed matrix operation
    (for example, multiplication) with a local matrix.
  prefs: []
  type: TYPE_NORMAL
- en: If you plan to code or design large matrix operations, you must dig into the
    Spark internals such as core Spark and how staging, pipelining, and shuffling
    works in each version of Spark (continuous improvement and optimization in each
    version).
  prefs: []
  type: TYPE_NORMAL
- en: 'We also recommend the following before embarking on a large-scale matrix and
    optimization journey:'
  prefs: []
  type: TYPE_NORMAL
- en: The source for matrix computations and optimization in Apache Spark is available
    at [http://www.kdd.org/kdd2016/papers/files/adf0163-bosagh-zadehAdoi.pdf](http://www.kdd.org/kdd2016/papers/files/adf0163-bosagh-zadehAdoi.pdf)
    and [https://pdfs.semanticscholar.org/a684/fc37c79a3276af12a21c1af1ebd8d47f2d6a.pdf](https://pdfs.semanticscholar.org/a684/fc37c79a3276af12a21c1af1ebd8d47f2d6a.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: The source for efficient large scale distributed matrix computation with Spark
    is available at [https://www.computer.org/csdl/proceedings/big-data/2015/9926/00/07364023.pdf](https://www.computer.org/csdl/proceedings/big-data/2015/9926/00/07364023.pdf)
    and [http://dl.acm.org/citation.cfm?id=2878336&preflayout=flat](http://dl.acm.org/citation.cfm?id=2878336&preflayout=flat)
  prefs: []
  type: TYPE_NORMAL
- en: The source for exploring matrix dependency for efficient distributed matrix
    computation is available at [http://net.pku.edu.cn/~cuibin/Papers/2015-SIGMOD-DMac.pdf](http://net.pku.edu.cn/~cuibin/Papers/2015-SIGMOD-DMac.pdf)
    and [http://dl.acm.org/citation.cfm?id=2723712](http://dl.acm.org/citation.cfm?id=2723712)
  prefs: []
  type: TYPE_NORMAL
- en: Exploring RowMatrix in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the `RowMatrix` facility that is provided by Spark.
    `RowMatrix`, as the name implies, is a row-oriented matrix with the catch being
    the lack of an index that can be defined and carried through the computational
    life cycle of a `RowMatrix`. The rows are RDDs which provide distributed computing
    and resiliency with fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: The matrix is made of rows of local vectors that are parallelized and distributed
    via RDDs. In short, each row will be an RDD, but the total number of columns will
    be limited by the maximum size of a local vector. This is not an issue in most
    cases, but we felt we should mention it for completion.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary packages for vector and matrix manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the Spark context and application parameters so Spark can run. See the
    first recipe in the chapter for more details and variations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'The amount and timing of warning statements returned as output varies due to
    the nature of distributed computing (non-sequential) with distributed matrices.
    The interlacing of messages with actual output varies depending on the execution
    path and that results in hard to read output. In the following statements, we
    elevate the `log4j` messages from warning (WARN - out of the box) to errors (ERROR)
    for clarity. We suggest that the developer follows the warning messages in detail
    to grasp the parallel nature of these operations and to fully understand the concept
    of an RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the level to error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: Originally comes out the box like this
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: We define two sequence data structures of dense vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A Scala sequence of dense local vectors which will be the data for the distributed
    `RowMatrix`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'A Scala sequence of dense local vectors which will be the data for the local
    identity matrix. A quick check of linear algebra shows that any matrix multiplied
    by an identity matrix will yield the same original matrix (that is, *A x I = A*).
    We like to use the identity matrix to prove that the multiplication worked and
    the original statistic computed over the original matrix is the same as the original
    *x* identity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: Create our first distributed matrix by parallelizing the underlying dense vectors
    to RDDs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Going forward, our dense vectors are now rows in the new distributed vectors
    backed by RDD (that is, all RDD operations are fully supported!).
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the original Sequences (made of vectors) and turn them into RDDs. We will
    cover RDDs in detail in the next chapter. In this single statement, we have turned
    a local data structure to a distributed artifact:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'We calculate some basic statistics to verify that the `RowMatrix` is constructed
    properly. The point to remember is that the dense vectors are now rows and not
    columns (which is the source of much confusion):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The statistics calculated (mean, variance, min, max, and so on) are for each
    column and not the entire matrix. This is the reason you see three numbers for
    mean and variance which corresponds to each column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we create our local matrix from the identity vector''s data structure.
    The point to remember is that the multiplication requires a local matrix and not
    a distributed one. Please see the call signature for verification. We use the
    `map`, `toArray`, and `flatten` operators to create a Scala flattened array data
    structure that can be used as one of the parameters to create a local matrix as
    shown in the next step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the local matrix as an identity matrix so we can verify the multiplication
    *A * I = A*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'We multiply the distributed matrix by the local one and create a new distributed
    matrix. This is a typical use case in which you end up multiplying a tall and
    skinny local matrix with a large-scale distributed matrix to achieve scale and
    the inherited dimensionality reduction of the resulting matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: Comparing step 7 and 8, we see that in fact the operation proceeded correctly
    and we can verify via descriptive statistics and the co-variance matrix that *A
    x I = A* using a distributed and local matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The signatures for this method constructor are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`RowMatrix(RDD<Vector> rows)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RowMatrix(RDD<Vector>, long nRows, Int nCols)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The method inherits from the following which makes their concrete methods available
    to all routines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: interface class java.lang.Object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Implements the following interfaces:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several method calls that are of interest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate descriptive statistics such as mean, min, max, variance, and so on:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MultivariateStatisticalSummary`'
  prefs:
  - PREF_OL
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`computeColumnSummaryStatistics()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compute the co-variance matrix from the original:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Matrix computeCovariance()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the Gramian matrix, also referred to as the Gram Matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(*A^TA* ):'
  prefs: []
  type: TYPE_NORMAL
- en: '``Matrix computeGramianMatrix()``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calculate the PCA components:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Matrix computePrincipalComponents(int k)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k* is the number of principal components'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the SVD decomposition of the original matrix:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SingularValueDecomposition<RowMatrix, Matrix> computeSVD(int k, boolean compute,
    double rCond)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k* is the number of leading singular values to keep (*0<k<=n*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiply:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RowMatrix Multiply(Matrix B)`'
  prefs:
  - PREF_OL
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RDD<Vector> rows()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calculate the QR decomposition:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QRDecomposition<RowMatrix, Matrix> tallSkinnyQR(boolean computeQ))`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Find the number of non-zero elements. This is useful so you can convert on-the-fly
    to the SparseVector if the density ID is low:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Int numNonzeros()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Get all the values stored in the matrix:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Double[] Values()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Others:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the column similarities (very useful in document analysis). There
    are two methods available which are covered in the [Chapter 12](part0512.html#F89000-4d291c9fed174a6992fd24938c2f9c77), *Implementing
    Text Analytics with Spark 2.0 ML Library*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of columns and number of rows which we find useful for dynamic programming
  prefs:
  - PREF_OL
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are some additional factors to consider when you use sparse or dense elements
    (vectors or block matrices). Multiplying by a local matrix is usually preferable
    since it doesn't require expensive shuffling.
  prefs: []
  type: TYPE_NORMAL
- en: 'While simplicity and control is preferred when dealing with large matrices,
    the four types of distributed matrices simplify the setup and operation. Each
    of the four types has advantages and disadvantages that have to be considered
    and weighed against these three criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: Sparsity or Density of underlying data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shuffling that will take place when using these facilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network capacity utilization when dealing with edge cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the reasons mentioned, and especially to reduce the shuffling (that is,
    a network bottleneck) required during a distributed matrix operation (for example,
    multiplication of two RowMatrixes), we prefer multiplication with a local matrix
    to reduce shuffle noticeably. While this seems a bit counter-intuitive at first,
    in practice it is fine for the cases we have encountered. The reason for this
    is because when we multiply a large matrix with a vector or tall and skinny matrix,
    the resulting matrix is small enough that fits into the memory.
  prefs: []
  type: TYPE_NORMAL
- en: The other point of caution will be that the returning information (a row or
    local matrix) has to be small enough so it can be returned to the driver.
  prefs: []
  type: TYPE_NORMAL
- en: For imports, we need both local and distributed vector and matrix imports so
    we can work with the ML library. Otherwise, the Scala vector and matrix will be
    used by default.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for constructor is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/RowMatrix.html#constructor_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/RowMatrix.html#constructor_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for method calls is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/RowMatrix.html#method_summary](https://spark.apache.org/docs/1.5.2/api/java/org/apache/spark/mllib/linalg/distributed/RowMatrix.html#method_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Distributed IndexedRowMatrix in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we cover the `IndexRowMatrix`, which is the first specialized
    distributed matrix that we cover in this chapter. The primary advantage of `IndexedRowMatrix`
    is that the index can be carried along with the row (RDD), which is the data itself.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of `IndexRowMatrix`, we have an index defined by the developer which
    is permanently paired with a given row that is very useful for random access cases.
    The index not only helps with random access, but is also used for identifying
    the row itself when performing `join()` operations.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary packages for vector and matrix manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the Spark context and application parameters so Spark can run. See the
    first recipe in the chapter for more details and variations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: We start with our original data vectors and then proceed to construct an appropriate
    data structure (that is, RowIndex) to house the index and vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then proceed to construct the `IndexedRowMatrix` and show the access. For
    those of you who have worked with LIBSVM, this format is close to label and vector
    artifacts with a twist that labels are now indexes (that is, long).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Start with a sequence of vectors as the base data structure for `IndexedRowMatrix`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'Start with a sequence of vectors as the base data structure for `IndexedRowMatrix`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The index is a long data structure which provides a meaningful row index corresponding
    to each row of the `IndexedRowMatrix`. The horsepower underneath the implementation
    are the RDDs which offer all the advantages of a distributed resilient data structure
    in a parallel environment from the get go.
  prefs: []
  type: TYPE_NORMAL
- en: The primary advantage of `IndexedRowMatrix` is that the index can be carried
    along with the row (RDD) which is the data itself. The fact that we can define
    and carry along the index with the data (the actual row of matrix) is very useful
    when we have the `join()` operation that needs a key to select a specific row
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows a pictorial view of the `IndexedRowMatrix` which
    should help clarify the subject:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The definition may be unclear as you are required to repeatedly define the
    index and the data to compose the original matrix. The following code snippet
    shows the inner list with (index, Data) repetition for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: The other operations are similar to the `IndexRow` matrix that was covered in
    the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for constructor is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/IndexedRowMatrix.html#constructor_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/IndexedRowMatrix.html#constructor_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for method calls is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/IndexedRowMatrix.html#method_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/IndexedRowMatrix.html#method_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring distributed CoordinateMatrix in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we cover the second form of specialized distributed matrix.
    This is very handy when dealing with ML implementations that need to deal with
    often large 3D coordinate systems (x, y, z). It is a convenient way to package
    the coordinate data structure into a distributed matrix.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary packages for vector and matrix manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the Spark context and application parameters so Spark can run. See the
    first recipe in the chapter for more details and variations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'We start with a SEQ of `MatrixEntry`, which corresponds to each coordinate
    and will be placed in the `CoordinateMatrix`. Note that the entries cannot be
    real numbers any more (they are x, y, z coordinates after all):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'We instantiate the call and construct the `CoordinateMatrix`. We need an additional
    step to create RDDs which we have shown in the constructor by using the Spark
    context for parallelization (that is, `sc.parallelize`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'We print the first `MatrixEntry` to verify the matrix elements. We will address
    RDDs in the next chapter, but note that `count()` is an action by itself and using
    `collect()` will be redundant:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`CoordinateMatrix` is a specialized matrix in which each entry is a coordinate
    system or a tuple of three numbers (long, long, long corresponding to *x*, *y*,
    *z* coordinates). A related data structure is `MatrixEntry`, in which coordinates
    will be stored and then placed at a location in the `CoordinateMatrix`. The following
    code snippet demonstrates the use of `MaxEntry`, which seems to be a source of
    confusion in itself.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following figure shows a pictorial view of the `CoordinateMatrix`, which
    should help clarify the subject:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The code snippet which holds three coordinates is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '`MaxEntry` is nothing but a required structure to hold the coordinate. Unless
    you need to modify the source code supplied by Spark (see GitHub `CoordinateMatrix.scala`)
    to define a more specialized container (compressed), there is no need to understand
    it any further:'
  prefs: []
  type: TYPE_NORMAL
- en: The `CoordinateMatrix` is also backed by RDDs which lets you leverage parallelism
    from the get go.
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to import `IndexedRow` as well so you can define the row with its index
    prior to instantiating the `IndexedRowMatrix`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This matrix can be converted to `RowMatrix`, `IndexedRowMatrix`, and `BlockMatrix`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is also an added benefit of efficient storage, retrieval, and operation
    that comes with a sparse coordinate system (for example, security threat matrix
    of all devices versus location).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for constructor is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/CoordinateMatrix.html#constructor_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/CoordinateMatrix.html#constructor_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for method calls is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/CoordinateMatrix.html#method_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/CoordinateMatrix.html#method_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for MaxEntry is available at [http://spark.apache.org/docs/latest/api/java/index.html](http://spark.apache.org/docs/latest/api/java/index.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring distributed BlockMatrix in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore `BlockMatrix`, which is a nice abstraction and a
    placeholder for the block of other matrices. In short, it is a matrix of other
    matrices (matrix blocks) which can be accessed as a cell.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an editor of your choice and make sure
    all the necessary JAR files (Scala and Spark) are available to your application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary packages for vector and matrix manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the Spark context and application parameters so Spark can run. See the
    first recipe in this chapter for more details and variations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `CoordinateMatrix` quickly to use as a base for conversion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'We take the `CoordinateMatrix` and convert it into a `BlockMatrix`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a very useful call with this type of matrix. In real life, it is often
    necessary to check the setup before proceeding to compute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A matrix block will be defined as a tuple of (int, int, Matrix). What is unique
    about this matrix is that it has `Add()` and `Multiply()` functions that can take
    another `BlockMatrix` as a second parameter to the distributed matrix. While setting
    it up is a bit confusing at first (especially on-the-fly as data arrives), there
    are helper functions that can help you verify your work and make sure the `BlockMatrix`
    is set up properly. This type of matrix can be converted to a local, `IndexRowMatrix`,
    and `CoordinateMatrix`. One of the most common use cases for the `BlockMatrix`
    is to have a `BlockMatrix` of `CoordinateMatrices`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for constructor is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/BlockMatrix.html#constructor_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/BlockMatrix.html#constructor_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for method calls is available at [https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/BlockMatrix.html#method_summary](https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/linalg/distributed/BlockMatrix.html#method_summary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
