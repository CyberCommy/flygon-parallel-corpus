- en: Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the world focusses more on the web than ever, all of our web applications
    will need to service more requests. In order to face the higher number of requests,
    we might need to scale our applications to support them.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter mainly concentrates on techniques, libraries, and tools that can
    be applied to our regular applications to address scalability concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering and its benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simply put, clustering is nothing but adding more than one server to provide
    the same service. It will help us to avoid interruptions during disasters such
    as system crashes and other unfortunate situations. Clustering can be used as
    a failover system, a load balancing system, or a parallel processing unit.
  prefs: []
  type: TYPE_NORMAL
- en: A failover cluster is a group of servers with the sample applications duplicated
    in all servers to provide the same services to clients to maintain the high availability
    of applications and services. If a server fails for some reason, the rest of the
    servers will take over the load and provide uninterrupted services to consumers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scaling up (vertical scaling)**: This is about adding more resources to our
    servers, for example, increasing the RAM, hard drive capacity, and processors.
    Though it might be a good option, it will only be applicable for certain scenarios,
    not all. In some cases, adding more resources might be expensive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaling out (horizontal scaling)**: Unlike adding more resources inside one
    server, scaling out focuses on adding more servers/nodes to service requests.
    This grouping is called clustering, as all of the servers are doing the same types
    of task, but duplicated on different servers to avoid interruption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benefits of clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is the more preferred solution for scaling services, as it gives
    a quick and flexible option to add more servers whenever needed, without interrupting
    existing services. Uninterrupted service can be provided during scaling. Consumers
    will not need to wait for anything approaching downtime when scaling the application.
    All server loads are balanced properly by a central load balancing server.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A load balancer is the most useful tool in clustering. A load balancer uses
    a variety of algorithms, such as round-robin, least connection, and so on, to
    forward the incoming request to the right backend servers for processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot of third-party load balancers available on the market, such
    as F5 ([https://f5.com](https://f5.com)), HAProxy ([http://www.haproxy.org](http://www.haproxy.org)),
    and so on. Though these load balancing tools behave differently, they focus on
    the main role: distributing the request load to the available backend server and
    maintaining the balance between all the servers. By proper load balancing, we
    prevent a single backend server from being overloaded. Also, most load balancers
    come with health monitoring, such as checks to verify the availability of servicing
    servers.'
  prefs: []
  type: TYPE_NORMAL
- en: Besides the main request distribution among servers, load balancers keep the
    backend servers protected from frontend servers. Frontend servers will have no
    idea about which backend server to sent the request to as load balancers hide
    all details about backend servers.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling the database is one of the challenging parts of architectural design.
    Here, we will discuss some database scaling techniques to scale our application.
  prefs: []
  type: TYPE_NORMAL
- en: Vertical scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed earlier, in the application server level we can also utilize
    the scaling up technique for our database servers. Adding more power, such as
    CPU and RAM, will bring better performance in querying databases. By using vertical
    scaling techniques, we can get consistent performance, and it's also easy to debug
    when things go wrong. Also, vertical scaling offers increased efficiency compared
    to horizontal scaling. However, vertical scaling might require downtime regularly
    to install new hardware, and it is limited by the hardware capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed with horizontal scaling in the application level, we can do
    the same for database servers by adding more machines to our cluster to take care
    of the database load. Compared to vertical scaling, it is significantly cheaper;
    however, this also comes with its own cost structure for cluster configuration,
    maintenance, and management costs.
  prefs: []
  type: TYPE_NORMAL
- en: Read replicas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By keeping multiple slaves that can be accessed for reading purposes, we can
    bring significant improvements to our application. Read replicas help to read
    data in all our slaves that are read-only. However, when we need to send write
    requests, we can use the master database. A master database can be used for both
    writing and reading purposes, and slaves can be used only for reading purposes.
    The more slaves we install, the more read-based queries can be handled. This read
    replica technique is very useful in scenarios where we have minimal write queries
    and maximal read queries to be handled.
  prefs: []
  type: TYPE_NORMAL
- en: Pool connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When an application queries the database, it creates a client connection, sends
    the query, and gets the results. As the client connection to the database is an
    expensive operation, the connections must be reused for further queries. Connection
    pooling will help in this situation by preventing the need establish the connection
    to the database for each request. By keeping better connection pools, such as
    HikariCP, we can improve the performance in our applications.
  prefs: []
  type: TYPE_NORMAL
- en: Use multiple masters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like read replicas, multiple master mechanisms give the option to duplicate
    multiple database servers. Unlike with read replicas duplicating slaves, here
    we duplicate master databases to write and read data. This pattern is very useful
    for specific scenarios such as REST API data transaction-focused applications.
    In the multiple masters pattern, we require our applications to generate **universally
    unique identifier** (**UUID**s), to prevent data collisions during the multi-master
    replication process.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing in DB servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the client connection limit from the application server is based on the database
    vendor, it might be tricky to handle situations when the application servers request
    more connections. By keeping a load balancer, we can distribute the database queries
    to available database servers using their connection pools. With the help of a
    load balancer, we will make sure all database servers are equally loaded; however,
    it depends on the algorithm used in the specific load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: Database partitioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Partitioning the database is very helpful when we deal with large databases
    that require high-end servers and take a lot of time to query. Also, this is useful
    when our application needs to query large amounts of both read and write requests.
    Partitioning can be done both horizontally and vertically. Both horizontal and
    vertical partitioning are described in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Sharding (horizontal partitioning)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A database table can be split into multiple tables based on any specific attribute.
    For example, a user database can be split into two different databases, such as
    `user_1` and `user_2`, where the `user_1` table's username starts with *A*-*N*,
    and the `user_2` table's username starts with *O*-*Z*. By splitting databases
    like earlier, we can reduce the number of rows on each table, and hence we can
    improve the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Vertical partitioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In vertical partitioning, the database table can be split into many tables,
    based on business concepts. For example, One table might have more columns to
    keep other tables to be accessed easily for better performance.
  prefs: []
  type: TYPE_NORMAL
- en: By doing both horizontal and vertical partitioning, querying the database will
    take less time and improve performance. Also, by dividing a big database into
    small chunks, we can avoid requiring high-end computers. These data shards can
    be distributed into low-commodity servers to save money, as well. However, data
    sharing might be a complex process in specific scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributed caching techniques will be helpful to improve the scalability in
    our web services. Unlike in-process caches, distributed caches need not be built
    in the same application space. They can be stored on multiple nodes of a cluster.
    Although distributed caches are deployed on multiple nodes, they offer a single
    state of the cache.
  prefs: []
  type: TYPE_NORMAL
- en: Data-tier caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adding a caching layer in the database will provide better performance. It is
    considered a common strategy for improving performance, especially when read requests
    are heavy in our application. Here, we will discuss Hibernate's levels of caching.
  prefs: []
  type: TYPE_NORMAL
- en: First-level caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A first-level cache is an inbuilt session cache enabled by Hibernate, and it
    is a mandatory cache through all requests. There is no option to disable first-level
    caching in Hibernate. First-level caching is associated with a session object
    and will be lost once the session is expired. When we query the web service for
    the first time, the object is retrieved from the database and stored in the first-level
    cache, which is associated with the Hibernate session. If we request the same
    entity again, it will be retrieved from the cache without querying the database.
  prefs: []
  type: TYPE_NORMAL
- en: Second-level caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second-level cache is an optional cache in Hibernate. The first-level cache
    will be the point of contact before our request reaches the second-level cache.
    The second-level cache can be configured per-class or per-collection, and it is
    responsible for caching objects across sessions.
  prefs: []
  type: TYPE_NORMAL
- en: As only a few classes benefit from caching, by default second-level caching
    is disabled. It can be enabled to service designers.
  prefs: []
  type: TYPE_NORMAL
- en: Application-tier caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like caching in a database, we can also cache any object in the application
    layer to improve the performance of the application. Here, we will talk about
    various object caches, especially key-value caching tools, and check their uniqueness
    in the market.
  prefs: []
  type: TYPE_NORMAL
- en: Memcached
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As most companies use Memcached (`https://memcached.org`) in their applications,
    we consider Memcached to be one of the most powerful distributed caching systems.
    It follows the distributed memory caching mechanism and is very helpful in repeated
    scenarios, for example, when the same service is requested multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: Redis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Redis ([https://redis.io](https://redis.io)) is another in-memory key-value
    store that can be used for caching. Redis supports data structures such as hashes,
    lists, sets, and so on. Redis is considered one of the most popular key-value
    stores, with the support of advanced key-value caches. Redis supports operations
    such as intersection and union. Because of its advanced capabilities and speed,
    it is more to be preferred than Memcached.
  prefs: []
  type: TYPE_NORMAL
- en: Hazelcast
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hazelcast ([https://hazelcast.com](https://hazelcast.com)) is an in-memory data
    grid that supports distributed collections and simplifies distributed computing.
    It provides a simple API with an easy and straightforward deployment strategy.
    As Hazelcast provides the Memcached client library, applications using a Memcached
    cluster might be able to adapt to a Hazelcast cluster. Hazelcast architecture
    supports data distribution and high scalability in a clustered platform. It also
    provides intelligent synchronization and auto-discovery. Hazelcast offers features
    such as distributed data structures, distributed queries, and distributed compute.
    Spring Boot has explicit caching support for Hazelcast in its framework.
  prefs: []
  type: TYPE_NORMAL
- en: Ehcache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ehcache ([http://www.ehcache.org](http://www.ehcache.org))is used mostly in
    small to medium-scale deployments due to its simplified scalable options. It is
    considered one of the most widely-used distributed caches. Also, Ehcache provides
    options to integrate with other popular libraries and frameworks. Ehcache scaling
    starts from in-process caching and goes through mixed in-process and out-of-process
    deployments. Also, Ehcache came up with the Terracotta server to improve performance
    on caching.
  prefs: []
  type: TYPE_NORMAL
- en: Riak
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Riak ([https://github.com/basho/riak](https://github.com/basho/riak)) is an
    Erlang-based key-value data store that is fault-tolerant and gives high availability.
    In Riak, data can be stored in memory, the disk, or both. Riak can be accessed
    through protocols such as the HTTP API or Native Erlang interface. Riak supports
    major languages such as Java, C, and Python. Also, it supports MapReduce, which
    can be flexible in big data-related operations.
  prefs: []
  type: TYPE_NORMAL
- en: Aerospike
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Aerospike ([https://www.aerospike.com](https://www.aerospike.com)) is an open
    source, flash-optimized, in-memory NoSQL database and key-value store. Aerospike
    operates on three layers: flash-optimized data layer, a self-managed distribution
    layer, and a cluster-aware client layer. To ensure consistency, the distribution
    layer is duplicated across all data centers. These duplicates will remain functional
    even when an individual server node fails or is removed from the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: Infinispan
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Infinispan ([http://infinispan.org/](http://infinispan.org/)) is a distributed
    in-memory key-value data store that can be used as a cache or just a data grid.
    It can be accessed as a library or over protocols such as REST. Also, Infinispan
    can be integrated with JPA, JCache, Spring, and Spark. Infinispan supports most
    MapReduce-related operations.
  prefs: []
  type: TYPE_NORMAL
- en: Cache2k
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cache2k ([https://cache2k.org/](https://cache2k.org/)) provides in-memory object
    cache options in Java applications. Cache2k mainly focuses on caching inside JVM.
  prefs: []
  type: TYPE_NORMAL
- en: Other distributed caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Previously, we talked about primary caching tools and their mechanisms. Here,
    we will discuss more about additional distributed caching that is available on
    the market:'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon ElastiCache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ElastiCache is primarily used as an in-memory data store and cache service;
    it was introduced by AWS. With the support of Amazon ElastiCache, we can deploy
    our cache environment quickly, without any complicated installations. It supports
    both Memcached and Redis caching.
  prefs: []
  type: TYPE_NORMAL
- en: Oracle distributed cache (Coherence)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this distributed cache, data is partitioned in all computers in the cluster.
    These partitioned caches will be configured to keep each piece of data on nodes
    in the cluster. Distributed caches are the most commonly used caches in Coherence.
  prefs: []
  type: TYPE_NORMAL
- en: Even though we have plenty of caching solutions available on the market, selecting
    a specific solution depends on many factors, such as business requirements, performance
    requirements, data integrity, fault tolerance, cost, and so on. Adding the right
    distributed caching layer to the application tier and database tier will result
    in better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about different libraries, tools, and techniques
    to scale a RESTful web service. When developing an application, we will have to
    look for loose coupling between components of a system by using well-defined interfaces.
    In the coming chapter, we will talk about microservices and their benefits.
  prefs: []
  type: TYPE_NORMAL
