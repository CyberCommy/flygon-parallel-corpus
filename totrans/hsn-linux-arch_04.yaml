- en: Architecting a Storage Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software-defined storage has changed the way we store our data; with increased
    functionality comes increased requirements when designing the right solution.
    A significant amount of variables need to be considered when architecting a storage
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explores the different design aspects of implementing software-defined
    storage solutions using GlusterFS and its various components.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: GlusterFS compute requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the right storage size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining performance needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding the right approach for high availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establishing how the workload ties everything together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, we''ll use the documentation for GlusterFS available on the
    following URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.gluster.org/](https://www.gluster.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/gluster/glusterfs](https://github.com/gluster/glusterfs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GlusterFS compute requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with any software, GlusterFS has a set of requirements that are defined by
    the developers to ensure that it works as expected. The actual requirements described
    in the documentation are relatively low, and pretty much every computer sold in
    the last 10 years can run GlusterFS. This is probably not at the best possible
    level of performance, but it still shows the flexibility of being able to run
    it in mixed conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For compute requirements, we mainly have the following two resources that we
    need to consider when designing a solution with GlusterFS:'
  prefs: []
  type: TYPE_NORMAL
- en: RAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With memory, the choice is relatively straightforward—use as much as possible.
    Unfortunately, there is no such thing as infinite memory, but the statement of
    using as much as possible couldn't be more real, since GlusterFS uses RAM as a
    read cache for each of the nodes, and at the same time the Linux kernel uses memory
    for the read-ahead cache to speedup reads on frequently accessed files.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the brick layout and filesystem chosen, available memory plays
    a significant role in read performance. As an example of bricks using the advanced ZFS filesystem,
    where it uses RAM for its **Adaptive Replacement Cache** (**ARC**). This adds
    an extra layer of caching sitting on high-speed RAM. The downside is that it consumes
    as much as it has available, so selecting a server that provides considerable
    amount of memory helps a lot.
  prefs: []
  type: TYPE_NORMAL
- en: GlusterFS does not require terabytes of RAM—having 32 GB or more per node assures
    that caches are big enough to allocate frequently accessed files, and if the cluster
    grows in size by adding more bricks to each node, adding more RAM should be considered
    in order to increase the available memory for caching.
  prefs: []
  type: TYPE_NORMAL
- en: Why is cache important?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the following: even old RAM technology such as DDR2 can deliver throughput
    in the GBps and latencies around the several nanoseconds. On the other hand, reading
    from regular spinning media (hard disk drives) throughput peaks at 150 MBps in
    most cases, and latency is in the several hundred milliseconds.'
  prefs: []
  type: TYPE_NORMAL
- en: Reading from cache is always faster than going to disk—waiting for the disk
    to move its head, finding the blocks of data requested, then sending it back to
    the controllers and onto the application.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to keep in mind is that cache needs to be warmed up first; this is
    the process of allowing the system to determine which files are regularly being
    accessed and then moving that data to the cache. While it is warming up, requests
    are slower, as they first have to be fetched from disk.
  prefs: []
  type: TYPE_NORMAL
- en: CPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any software requires CPU cycles, and GlusterFS is no exception. CPU requirements
    are moderately low, and depend on the type of volume used, for example, a **replicated
    volume** requires far less CPU than a **dispersed volume**.
  prefs: []
  type: TYPE_NORMAL
- en: CPU requirements are also affected by the type of filesystem that the bricks
    use and what features they have. Going back to the ZFS example, if compression
    is enabled this adds increased load to the CPU, and not having enough CPU resources
    decreases performance considerably.
  prefs: []
  type: TYPE_NORMAL
- en: For a simple storage server and no advanced features at the brick level, anything
    with four CPUs or more is sufficient. When enabling filesystem features, such
    as compression, eight or more CPUs are required for optimal performance. Additionally,
    more CPU allows for more concurrent I/O to be done to the cluster. This is of
    utmost importance when designing a storage cluster for **high-performance compute**
    (**HPC**) applications, where thousands of users are performing I/O operations
    at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following rules as general rules of thumb:'
  prefs: []
  type: TYPE_NORMAL
- en: For highly concurrent workload, go for higher CPU count, above eight CPUs, depending
    on the concurrency level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For low-performance requirements and a cost-efficient solution, select a lower
    number of CPUs, for example, four CPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many cloud providers have a fixed set of given resources for their virtual machine
    sizes that do not allow for custom vCPU to RAM ratios. Finding the right balance
    depends on which VM size provides the necessary resources.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of GlusterFS in the cloud will be explored in further detail in
    the upcoming chapters. However, get an overview of the concept, let's explore
    VM sizes using Microsoft's Azure offering.
  prefs: []
  type: TYPE_NORMAL
- en: Azure VM families range from general-purpose compute to specific workloads,
    such as GPU. For GlusterFS, we really like the L-series VMs, which are optimized
    for storage workloads. This VM family has a good ratio of vCPU to RAM, and offers
    the highest storage performance to cost ratio of any family.
  prefs: []
  type: TYPE_NORMAL
- en: The general idea can be applied to other cloud vendors. A VM size that provides
    an excellent and cost-effective ratio of vCPU to RAM should be selected.
  prefs: []
  type: TYPE_NORMAL
- en: How much space do you need?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wouldn't it be nice if we could just use as much space as we need? In reality,
    storage has a cost, and unlimited storage does not exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to sizing available storage, the following factors have to be
    taken into consideration:'
  prefs: []
  type: TYPE_NORMAL
- en: GlusterFS volume type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Required space by the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Projected growth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GlusterFS volume type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with some technical considerations. Each GlusterFS volume has characteristics
    when it comes to available space. Depending on the volume type, you can end up
    with less usable space than you initially calculated. We will be exploring the
    space considerations of each volume type we described in [Chapter 2](7a05974e-0554-45d0-a505-921d484942ef.xhtml),
    *Defining GlusterFS Storage*.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This volume type is reasonably straightforward. The sum of the available space
    from each node is the total space on the global namespace (another name for the
    GlusterFS volume mount).
  prefs: []
  type: TYPE_NORMAL
- en: An example is a request of 50 TB volume where the amount of space needed for
    the bricks is precisely 50 TB. This can be divided into five nodes with 10 TB
    each or two nodes with 25 TB each.
  prefs: []
  type: TYPE_NORMAL
- en: Replicated
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With replica volumes, half of the available raw brick space goes into the mirroring
    or replication of the data. This means that when sizing this type of volume, you
    need to at least double the storage capacity of what is requested. This depends
    on the specific configuration of the volume. A general rule of thumb is that the
    available capacity is half of the total space on the bricks.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the request is for 50 TB volume, the node configuration should
    have at least 100 TB available in brick space between two nodes with 50 TB each.
  prefs: []
  type: TYPE_NORMAL
- en: Dispersed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dispersed volumes are trickier to size, as they function similar to a RAID 5,
    where the data is spread across the nodes and a node's worth of capacity is used
    for parity. This depends on the configuration of the volume, but you could expect
    space efficiency to increase with the node count.
  prefs: []
  type: TYPE_NORMAL
- en: To further explain, a request for a 50 TB volume can be configured on six nodes
    with 10 TB each. Note that an extra node was taken into consideration. Selecting
    five nodes with 10 TB each results in a volume of only 40 TB, which falls short
    of the requested size.
  prefs: []
  type: TYPE_NORMAL
- en: Space required by the application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each application has its own set of requirements, and storage requirements are
    as necessary as any other requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Serving media files require considerably more resources than a website with
    few users and not many media files. Knowing precisely what the intended usage
    of the storage system is permits correct sizing of the solution and prevents situations
    where storage estimates fall short of what was needed from the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you go through what minimum requirements the application developers
    recommend and understand how it interacts with the storage, as this helps prevent
    headaches.
  prefs: []
  type: TYPE_NORMAL
- en: Projected growth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your job as an architect is to ask the right questions. When it comes to storage,
    make sure the growth rate or change rate is taken into account.
  prefs: []
  type: TYPE_NORMAL
- en: Consider that data growth happens no matter what, and thinking ahead avoids
    complicated situations where there is not enough space, so leaving some margin
    for future utilization is a good practice. Allowing for 10% or more space should
    be a good starting point, so if 50 TB spaces are requested then add 5 TB more
    space to the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Go for the most cost-effective route. While GlusterFS allows for seamless expansion,
    try to avoid using this feature as an easy solution and make sure that the right
    size is defined from the start and a buffer is considered for future growth.
  prefs: []
  type: TYPE_NORMAL
- en: Performance considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applications that perform poorly are probably worse than applications that don't
    work at all. Having something work half of the time is incredibly frustrating
    and costly to any business.
  prefs: []
  type: TYPE_NORMAL
- en: As an architect, you need to design solutions that perform to the spec or better
    in order to avoid scenarios where problems arise due to poor performance.
  prefs: []
  type: TYPE_NORMAL
- en: The very first place to start is by defining what the performance requirements
    are. Most of the time, the application developers mention the performance requirements
    in their documentation itself. Not meeting these minimum requirements means that
    the application either doesn't work at all or barely works. Neither is acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the things to look out for when designing a performance-oriented
    solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Throughput
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IOPS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I/O size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughput
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughput is a function of a given amount of data over a certain amount of
    time that is typically described in **megabytes per second** (**MBps**). This
    means that every second X amount of data is being sent or received from a storage
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the workload, the highest throughput might not be possible as the
    application is unable to perform I/O big enough or fast enough. There is no hard
    number to recommend here. Try going for the highest possible throughput, and make
    sure that the storage cluster can sustain the transfer rates necessary for the
    desired level of concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Latency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Latency is critical and requires extra care, as some applications are significantly
    sensitive to high latencies or response times.
  prefs: []
  type: TYPE_NORMAL
- en: Latency is a measurement of the amount of time I/O operations take to complete
    that is typically measured in milliseconds (1 second is 1,000 milliseconds). High
    latencies or response times cause applications to take longer to respond and even
    stop working altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Aim for the lowest latency possible. This is a case where getting the lowest
    possible number is always the best approach. With latency, there's no such thing
    as not enough, or, in this case, not too short of response time. Consider the
    type of storage medium you used. Traditional hard disk drives have response times
    (or seek times) ranging in the several hundred milliseconds, while newer solid
    state drives can go past the sub-millisecond mark and into microseconds.
  prefs: []
  type: TYPE_NORMAL
- en: IOPS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Input/output operations per second is a function of a given amount of operations
    over time, in this case, seconds. This is a measurement of how many operations
    can be done over a second, and many applications provide a minimum requirement
    regarding IOPS.
  prefs: []
  type: TYPE_NORMAL
- en: Most applications provide a requirement of the minimum IOPS needed for it to
    work as expected. Make sure that these are met, as otherwise the application might
    not behave as intended.
  prefs: []
  type: TYPE_NORMAL
- en: When designing a storage solution, make sure IOPS is considered as a primary
    deciding factor when taking sizing decisions.
  prefs: []
  type: TYPE_NORMAL
- en: I/O size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I/O size is the amount of data that each operation performs. This is dependent
    on the workload type, as each application interacts with the storage system differently.
    The I/O size impacts directly on the previously mentioned aspects of performance.
  prefs: []
  type: TYPE_NORMAL
- en: Smaller I/O results in lower throughput, but, if done fast enough, it results
    in higher IOPS with lower latencies. Larger I/O, on the other hand, provides a
    higher throughput, but generally produces lower IOPS as fewer operations are done
    in the same amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: There is no solid recommendation regarding I/O size. In an ideal, non-realistic
    world, I/O is done big enough and fast enough, which results in high throughput
    and high IOPS. In reality, it's either one or the other. Small I/O ends up being
    slow regarding throughput, but it completes fast enough so that IOPS seem higher.
    With big I/O, the numbers are inverted, and the throughput becomes high, but since
    it takes longer to complete, IOPS goes down.
  prefs: []
  type: TYPE_NORMAL
- en: GlusterFS performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following aspects need to be taken into consideration when designing a
    GlusterFS storage cluster when it comes to performance:'
  prefs: []
  type: TYPE_NORMAL
- en: Volume type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brick layout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volume type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The volume chosen affects the performance in different ways, since GlusterFS
    allocates data differently for each type.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a replicated volume mirrors data across nodes, while a dispersed
    volume tries to maximize node usage and uses them in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: If performance is the primary aim for a dispersed or distributed volume, consider
    that distributed volumes offer no redundancy, while a dispersed volume does it
    at the expense of performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: Brick layout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having a node with all of its disks in a single large brick does not perform
    in the same way as having disks grouped in smaller numbers with several bricks.
    Brick layout is the highest contributing factor to performance, as this directly
    dictates how disks are used.
  prefs: []
  type: TYPE_NORMAL
- en: If all the disks end up in a single brick, the performance suffers. Generally,
    having more bricks with fewer disks results in better performance and lower latency.
  prefs: []
  type: TYPE_NORMAL
- en: Consider configuring a software RAID0 for the disks that make up the bricks.
    For example, you could have 10 disks available and, for simplicity's sake, configure
    all 10 disks in a RAID0 on a single brick. Alternatively, you could go for a more
    efficient route and configure five bricks where each brick is made of two disks
    in a RAID0.
  prefs: []
  type: TYPE_NORMAL
- en: This also allows smoother growth, since adding more bricks with fewer disks
    is considerably easier than adding a large number of disks. You should aim for
    more bricks with fewer disks grouped in smaller RAID configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see how each brick is made up of two different
    disks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35739515-9a3c-4198-a195-e9f0dcae046d.png)'
  prefs: []
  type: TYPE_IMG
- en: Number of nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Increasing the number of nodes in the cluster allows for higher concurrency.
    While performance gains might not be linear, adding nodes allows for a higher
    number of users and applications accessing the volumes.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to have enough nodes for a balance in the available space and concurrency.
    There is no set number here, but your job as an architect is to define, through
    testing, what is the right number of nodes for a specific solution. During the
    POC phase, test with a smaller number of nodes and check whether the performance
    is acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Filesystem tunables, such as block size can play an important role, and the
    goal is to match the workload I/O size, the GlusterFS volume block size, and the
    filesystem block size to the same amount.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, 4 K is the most used block size that works for general workloads.
    For a large number of small files, go for a smaller block size. For big files,
    aim for a bigger block size, such as 1 M.
  prefs: []
  type: TYPE_NORMAL
- en: The best approach for high availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With GlusterFS, high availability can be delivered through the volume configuration;
    deciding how this is done depends on the application needs, available space, and
    required performance.
  prefs: []
  type: TYPE_NORMAL
- en: Since GlusterFS handles high availability, there is no need to configure any
    form of redundancy at the brick level. This is especially true with cloud instances
    and virtual machines, where there are no physical disks that can go bad. For physical
    installations, it is always better to have an extra layer of redundancy by configuring
    the local disks with RAID5 or RAID6 for a balance in performance and resiliency.
    For now, let's stick to cloud deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'With GlusterFS, only two volume types offer high availability: replicated and
    dispersed. Replicated volumes are reasonably straightforward since data is just
    replicated from one node to another. These offer lower performance, but are considerably
    easier to configure, deploy, and maintain.'
  prefs: []
  type: TYPE_NORMAL
- en: Replicated
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choose a replicated volume when there is no need for extreme performance. Select
    the number of replicas based on how many nodes or bricks the volume should tolerate.
    Consider that using a higher replica number will decrease the amount of available
    space, but increase the availability of the volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows that losing a node in a replicated volume does
    not disrupt volume operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8e4f5f4-104c-438e-b1f5-f75603c9ae7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Dispersed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dispersed volumes offer a good balance between high availability and performance;
    this should be the go-to volume when both are a requirement. Configuring a dispersed
    volume is a more complicated process since the redundancy is handled as in a RAID5
    setup, where a node is used as parity. The redundancy value can be chosen at the
    time of volume creation which allows for greater flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see that losing one node does not disrupt
    the volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c573997-afcc-40f2-bfc0-4256e9e2ed63.png)'
  prefs: []
  type: TYPE_IMG
- en: Plan for high availability when there is a specific requirement. Remember that
    volume types can be mixed and matched. For example, a distributed replicated volume
    will have a good mix of available space and redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: Geo-replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Geo-replication allows for asynchronous replication of data between different
    sites through local networks or the internet. This provides high availability
    by having a copy of the data in a different geo-location, and ensures disaster
    recovery in case of failures.
  prefs: []
  type: TYPE_NORMAL
- en: Consider going the geo-replication route when there is a specific use case where
    the added layer of redundancy is needed. Remember that this is asynchronous replication,
    so, in the case of a disaster, consider the RPO and RTO times explained in the
    previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram gives you a general understanding of how geo-replication
    works—**Site A** replicates to **Site B** through the WAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/527d077d-6b1c-4507-be07-2218b47b1479.png)'
  prefs: []
  type: TYPE_IMG
- en: How the workload defines requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Delivering video files to streaming web servers is not the same as hosting a
    large database. I/O is done in an entirely different way, and knowing exactly
    how the workload interacts with the storage system is crucial to successfully
    size and design a robust storage solution.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The application documentation is your best friend when trying to figure out
    what the storage requirements are. When there's an existing implementation of
    the application, ask the administrators what the software expects for performance
    and how it reacts when it doesn't meet the minimum requirements.
  prefs: []
  type: TYPE_NORMAL
- en: System tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using tools such as `iostat` gives a good understanding of how the application
    interacts with the storage, for example, by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The previous code shows per block device usage, the `areq-sz` column (previously
    known as `avgrq-sz`) shows the average request size in kilobytes, making this
    a good starting point to understand the I/O size the application typically uses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output looks similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72cbb4e3-a020-4996-be85-1b99af3b07ed.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding image, we could appreciate the block devices and their respective
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: File type and size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an example, designing a storage solution for a media streaming server requires
    the use of large block sizes, as media files tend to be bigger than small text
    files. If you use a larger block size for the bricks, the GlusterFS volume will
    not only make more efficient use of the space but will also allow for faster operations,
    as the transaction size matches the file size.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a storage server for sensor logging that usually creates
    a large number of small files containing text requires a smaller block size to
    match the size of the files being created. Using a smaller block size avoids allocating
    an entire block, say 4 K, for a file that is only 1 K in size.
  prefs: []
  type: TYPE_NORMAL
- en: Ask the right questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your goal as an architect is to make sure the workload is very clear. The intended
    use for the storage server defines how many resources need to be allocated. Failing
    to do so could result in resources being wasted that in turn means money being
    wasted, or, in a worst case scenario, could lead to a solution that does not perform
    to spec, which leads to applications failing and users not able to work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember from the [Chapter 1](ef6464b0-95db-45b2-95ce-4f9067e2c6c8.xhtml),
    *Introduction to Design Methodology*: ask the right questions. When sizing a storage
    solution, you can ask the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How much space does the current implementation consume (if there's one already
    in place)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the performance requirements of the application?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many users interact with the application?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is high availability required?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does the application store its data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does it create large files and append data to them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does it create a large number of small files?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Possible answers to these questions could be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Right now, the application consumes 20 TB, but we expect it to increase 5% each
    month and stabilize at 80 TB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application requires at least 100 MB/s of throughput and a latency no higher
    than 10 ms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Currently, about 300 users have access to the application; concurrently, we've
    seen peaks of 150 users, but we expect the user count to increase significantly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can sustain not being able to access the storage for some time, but we do
    need to be able to recover from a failure reasonably quickly, and could possibly
    have a copy of the data off-site.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application primarily saves its information in small files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does not append data, and if more space is needed, it merely creates more
    small files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, we've seen several thousands of files created no bigger than 4 KB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the previous example, you can surmise that the application creates a lot
    of small files, and it can tolerate being down for some time but requires off-site
    replication for smooth disaster recovery. Performance requirements seem to be
    relatively high, so we could opt for a dispersed or distributed volume with geo-replication
    enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of architecting a storage solution requires many variables to be
    known. In this chapter, we defined that deciding how much space is needed depends
    on the GlusterFS volume type, the application requirements, and the estimated
    growth in data utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the volume type, the available space is affected, a distributed
    volume aggregates all of the available space making it the most space efficient,
    while a replicated volume uses half of the available raw space for mirroring.
  prefs: []
  type: TYPE_NORMAL
- en: The application and user base dictate how much space is required. This is because,
    depending on the type of data being served, the storage requirements change. Thinking
    ahead and planning for storage growth avoids the potential to run out of resources,
    and allows for at least a 10% buffer when sizing should fit most situations.
  prefs: []
  type: TYPE_NORMAL
- en: With the performance requirements, we defined the concepts of throughput, latency,
    IOPS, and I/O size and how these interact with each other. We defined what variables
    come into play when configuring GlusterFS for optimal performance, how each volume
    has its performance characteristics, and how the brick layout plays an important
    role when trying to optimize a GlusterFS volume.
  prefs: []
  type: TYPE_NORMAL
- en: We also defined how high availability requirements affect sizing and how each
    volume provides different levels of HA. When disaster recovery is needed, GlusterFS
    geo-replication adds the required level of availability by replicating data to
    another physical region, which allows the smooth recovery of services in case
    of a disaster.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we went through how the workload defines how the solution is designed
    and how using tools to verify how the application interacts with the storage allows
    for the correct configuration of the storage cluster. We also found out how file
    types and sizes define performance behavior and space utilization, and how asking
    the right questions allows for a better understanding of the workload, which results
    in a more efficient and optimized solution.
  prefs: []
  type: TYPE_NORMAL
- en: The main takeaway is to always ask how the application and workload interact
    with its resources. This allows for the most efficient design possible.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll go through the actual configuration needed for GlusterFS.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the compute requirements for GlusterFS?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does GlusterFS use RAM?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a cache?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does concurrency affect CPU sizing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do GlusterFS volumes affect available space?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How much space does the application need?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is projected growth?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is throughput, latency IOPS, and I/O size?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is brick layout?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is geo-replication?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Architecting Data-Intensive Applications* by Anuj Kumar'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Microsoft Azure Storage Essentials* by Chukri Soueidi'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Azure for Architects* by Ritesh Modi'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
