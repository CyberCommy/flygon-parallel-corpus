- en: Deciphering the Process Scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Process scheduling is one of the most crucial executive jobs of any operating
    system, Linux being no different. The heuristics and efficiency in scheduling
    processes is what make any operating system tick and also give it an identity,
    such as a general-purpose operating system, server, or a real-time system. In
    this chapter, we will get under the skin of the Linux scheduler, deciphering concepts
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Linux scheduler design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling policies and priorities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Completely Fair Scheduler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-Time Scheduler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deadline Scheduler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Group scheduling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preemption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process schedulers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The effectiveness of any operating system is proportional to its ability to
    fairly schedule all contending processes. The process scheduler is the core component
    of the kernel, which computes and decides when and for how long a process gets
    CPU time. Ideally, processes require a *timeslice* of the CPU to run, so schedulers
    essentially need to allocate slices of processor time fairly among processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'A scheduler typically has to:'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid process starvation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage priority scheduling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximize throughput of all processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure low turnaround time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure even resource usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid CPU hogging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider process' behavioral patterns for prioritization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elegantly subsidize under heavy load
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handle scheduling on multiple cores efficiently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linux process scheduler design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linux, which was primarily developed for desktop systems, has unassumingly evolved
    into a multi-dimensional operating system with its usage spread across embedded
    devices, mainframes, and supercomputers to room-sized servers. It has also seamlessly
    accommodated the ever-evolving diverse computing platforms such as SMP, virtualization,
    and real-time systems. The diversity of these platforms is brought forth by the
    kind of processes that run on these systems. For instance, a highly interactive
    desktop system may run processes that are I/O bound, and a real-time system thrives
    on deterministic processes. Every kind of process thus calls for a different kind
    of heuristic when it needs to be fairly scheduled, as a CPU-intensive process
    may require more CPU time than a normal process, and a real-time process would
    require deterministic execution. Linux, which caters to a wide spectrum of systems,
    is thus confronted with addressing the varying scheduling challenges that come
    along when managing these diverse processes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00013.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The intrinsic design of Linux's process scheduler elegantly and deftly handles
    this challenge by adopting a simple two-layered model, with its first layer, the
    **Generic Scheduler**, defining abstract operations that serve as entry functions
    for the scheduler, and the second layer, the scheduling class, implementing the
    actual scheduling operations, where each class is dedicated to handling the scheduling
    heuristics of a particular kind of process. This model enables the generic scheduler
    to remain abstracted from the implementation details of every scheduler class.
    For instance, normal processes (I/O bound) can be handled by one class, and processes
    that require deterministic execution, such as real-time processes, can be handled
    by another class. This architecture also enables adding a new scheduling class
    seamlessly. The previous figure depicts the layered design of the process scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generic scheduler defines abstract interfaces through a structure called
    `sched_class`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Every scheduler class implements operations as defined in the `sched_class`
    structure. As of the 4.12.x kernel, there are three scheduling classes: the **Completely
    Fair Scheduling** (**CFS**) class , Real-Time Scheduling class, and Deadline Scheduling
    class, with each class handling processes with specific scheduling requirements.
    The following code snippets show how each class populates its operations as per
    the `sched_class` structure.'
  prefs: []
  type: TYPE_NORMAL
- en: '**CFS class****:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Real-Time Scheduling class****:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Deadline Scheduling class****:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Runqueue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conventionally, the runqueue contains all the processes that are contending
    for CPU time on a given CPU core (a runqueue is per-CPU). The generic scheduler
    is designed to look into the runqueue whenever it is invoked to schedule the next
    best runnable task. Maintaining a common runqueue for all the runnable processes
    would not be a possible since each scheduling class deals with specific scheduling
    policies and priorities.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel addresses this by bringing its design principles to the fore. Each
    scheduling class defined the layout of its runqueue data structure as best suitable
    for its policies. The generic scheduler layer implements an abstract runqueue
    structure with common elements that serves as the runqueue interface. This structure
    is extended with pointers that refer to class-specific runqueues. In other words,
    all scheduling classes embed their runqueues into the main runqueue structure.
    This is a classic design hack, which lets every scheduler class choose an appropriate
    layout for its runqueue data structure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see how the scheduling classes (`cfs`, `rt`, and `dl`) embed themselves
    into the runqueue. Other elements of interest in the runqueue are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nr_running`: This denotes the number of processes in the runqueue'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load`: This denotes the current load on the queue (all runnable processes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`curr` and `idle`: These point to the *task_struct* of the current running
    task and the idle task, respectively. The idle task is scheduled when there are
    no other tasks to run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scheduler's entry point
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of scheduling starts with a call to the generic scheduler, that
    is, the `schedule()` function, defined in `<kernel/sched/core.c>`. This is perhaps
    one of the most invoked routines in the kernel. The functionality of `schedule()`
    is to pick the next best runnable task. The `pick_next_task()` of the `schedule()`
    function iterates through all the corresponding functions contained in the scheduler
    classes and ends up picking the next best task to run. Each scheduler class is
    linked using a single linked list, which enables the `pick_next_task()` to iterate
    through these classes.
  prefs: []
  type: TYPE_NORMAL
- en: Considering that Linux was primarily designed to cater to highly interactive
    systems, the function first looks for the next best runnable task in the CFS class
    if there are no higher-priority runnable tasks in any of the other classes (this
    is done by checking whether the total number of runnable tasks (`nr_running`)
    in the runqueue is equal to the total number of runnable tasks in the CFS class's
    sub-runqueue); else, it iterates through all the other classes and picks the next
    best runnable task. Finally, if no tasks are found, it invokes the idle, background
    tasks (which always returns a non-null value).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block shows the implementation of `pick_next_task()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Process priorities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The decision of which process to run depends on the priority of the process.
    Every process is labelled with a priority value, giving it an immediate position
    in terms of when it will be given CPU time. Priorities are fundamentally classified
    into *dynamic* and *static* priorities on *nix systems. **Dynamic priorities**
    are basically applied to normal processes dynamically by the kernel, considering
    various factors such as the nice value of the process, its historic behavior (I/O
    bound or processor bound), lapsed execution, and waiting time. **Static priorities**
    are applied to real-time processes by the user and the kernel does not change
    their priorities dynamically. Processes with static priorities are thus given
    higher priority when scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: '**I/O bound process:** When the execution of a process is heavily punctuated
    with I/O operations (waiting for a resource or an event), for instance a text
    editor, which almost alternates between running and waiting for a key press, such
    processes are called I/O bound. Due to this nature, the scheduler normally allocates
    short processor time slices to I/O-bound processes and multiplexes them with other
    processes, adding the overhead of context switching and the subsequent heuristics
    of computing the next best process to run.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Processor bound process:** These are processes that love to stick on to CPU
    time slices, as they require maximum utilization of the processor''s computing
    capacity. Processes requiring heavy computations such as complex scientific calculations,
    and video rendering codecs are processor bound. Though the need for a longer CPU
    slice looks desirable, the expectation to run them under fixed time periods is
    not often a requirement. Schedulers on interactive operating systems tend to favor
    more I/O-bound processes than processor-bound ones. Linux, which aims for good
    interactive performance, is more optimized for faster response time, inclining
    towards I/O bound processes, even though processor-bound processes are run less
    frequently they are ideally given longer timeslices to run.'
  prefs: []
  type: TYPE_NORMAL
- en: Processes can also be **multi-faceted**, with an I/O-bound process needing to
    perform serious scientific computations, burning the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: The *nice* value of any normal process ranges between 19 (lowest priority) and
    -20 (highest priority), with 0 being the default value. A higher nice value indicates
    a lower priority (the process is being nicer to other processes). Real-time processes
    are prioritized between 0 and 99 (static priority). All these priority ranges
    are from the perspective of the user.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kernel''s perspective of priorities**'
  prefs: []
  type: TYPE_NORMAL
- en: Linux however looks at process priorities from its own perspective. It adds
    a lot more computation for arriving at the priority of a process. Basically, it
    scales all priorities between 0 to 139, where 0 to 99 is assigned for real-time
    processes and 100 to 139 represents the nice value range (-20 to 19).
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now go deeper into each scheduling class and understand the operations,
    policies, and heuristics it engages in managing scheduling operations adeptly
    and elegantly for its processes. As mentioned earlier, an instance of `struct
    sched_class` must be provided by each scheduling class; let''s look at some of
    the key elements from that structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '`enqueue_task`: Basically adds a new process to the run queue'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dequeue_task`: When the process is taken off the runqueue'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`yield_task`: When the process wants to relinquish CPU voluntarily'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pick_next_task`: The corresponding function of the *pick_next_task* called
    by s*chedule()*. It picks up the next best runnable task from its class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Completely Fair Scheduling class (CFS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All processes with dynamic priorities are handled by the CFS class, and as most
    processes in general-purpose *nix systems are normal (non-realtime), CFS remains
    the busiest scheduler class in the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: CFS relies on maintaining *balance* in allocating processor time to tasks, based
    on policies and dynamic priorities assigned per task. Process scheduling under
    CFS is implemented under the premise that it has an "ideal, precise multi-tasking
    CPU," that equally powers all processes at its peak capacity. For instance, if
    there are two processes, the perfectly multi-tasking CPU ensures that both processes
    run simultaneously, each utilizing 50% of its power. As this is practically impossible
    (achieving parallelism), CFS allocates processor time to a process by maintaining
    proper balance across all contending processes. If a process fails to receive
    a fair amount of time, it is considered out of balance, and thus goes in next
    as the best runnable process.
  prefs: []
  type: TYPE_NORMAL
- en: 'CFS does not rely on the traditional time slices for allocating processor time,
    but rather uses a concept of virtual runtime (*vruntime*): it denotes the amount
    of time a process got CPU time, which means a low `vruntime` value indicates that
    the process is processor deprived and a high `vruntime` value denotes that the
    process acquired considerable processor time. Processes with low `vruntime` values
    get maximum priority when scheduling. CFS also engages *sleeper fairness* for
    processes that are ideally waiting for an I/O request. Sleeper fairness demands
    that waiting processes be given considerable CPU time when they eventually wake
    up, post event. Based on the `vruntime` value, CFS decides what amount of time
    the process is to run. It also uses the nice value to weigh a process in relation
    to all contending processes: a higher-value, low-priority process gets less weight,
    and a lower-value, high-priority task gets more weight. Even handling processes
    with varying priorities is elegant in Linux, as a lower-priority task gets considerable
    factors of delay compared to a higher-priority task; this makes the time allocated
    to a low-priority task dissipate quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: Computing priorities and time slices under CFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Priorities are assigned based on how long the process is waiting, how long the
    process ran, the process's historical behavior, and its nice value. Normally,
    schedulers engage complex algorithms to end up with the next best process to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'In computing the timeslice every process gets, CFS not just relies on the nice
    value of the process but also looks at the load weight of the process. For every
    jump in the nice value of a process by 1, there will be a 10% reduction in the
    CPU timeslice, and for every decrease in the nice value by 1, there will be a
    10% addition in the CPU timeslice, indicating that nice values are multiplicative
    by a 10% change for every jump. To compute the load weight for corresponding nice
    values, the kernel maintains an array called `prio_to_weight`*,* where each nice
    value corresponds to a weight:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The load value of a process is stored in the `weight` field of `struct load_weight`*.*
  prefs: []
  type: TYPE_NORMAL
- en: Like a process's weight, the runqueue of CFS is also assigned a weight, which
    is the gross weight of all the tasks in the runqueue. Now the timeslice is computed
    by factoring the entity's load weight, the runqueue's load weight, and the `sched_period`
    (scheduling period).
  prefs: []
  type: TYPE_NORMAL
- en: CFS's runqueue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CFS sheds the need for a normal runqueue and uses a self-balancing, red-black
    tree instead to get to the next best process to run in the shortest possible time.
    The *RB tree* holds all the contending processes and facilitates easy and quick
    insertion, deletion, and searching of processes. The highest-priority process
    is placed to its leftmost node. The `pick_next_task()` function now just picks
    the leftmost node from the `rb tree` to schedule.
  prefs: []
  type: TYPE_NORMAL
- en: Group scheduling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To ensure fairness when scheduling, CFS is designed to guarantee that every
    runnable process gets at least one run on the processor under a defined time duration,
    called the **scheduling period**. Within a scheduling period, CFS rudimentarily
    ensures fairness or, in other words, ensures that unfairness is kept at a minimum,
    as each process at least runs once. CFS divides the scheduling period into timeslices
    among all threads of execution to avoid process starvation; however, imagine a
    scenario where process A spawns 10 threads of execution and process B spawns 5
    threads of execution: here CFS divides timeslices to all the threads equally,
    leading to process A and its spawned threads getting the maximum time and process
    B to be dealt with unfairly. If process A keeps on spawning more threads, the
    situation may become grave for process B and its spawned threads, as process B
    will have to contend with the minimum scheduling granularity or timeslice (which
    is 1 millisecond). Fairness in this scenario demands process A and B getting equal
    timeslices with spawned threads to share these timeslices internally. For instance,
    if process A and B get 50% of the time each, then process A shall divide its 50%
    time among its spawned 10 threads, with each thread getting 5% time internally.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this issue and to keep up the fairness, CFS introduced **group scheduling**,
    where timeslices are allotted to groups of threads instead of individual threads.
    Continuing the same example, under group scheduling, process A and its spawned
    threads belong to one group and process B and its spawned threads belong to another.
    As scheduling granularity is imposed at a group level and not at a thread level,
    it gives process A and B equal share of processor time, with process A and B dividing
    the timeslice among its group members internally. Here, a thread spawned under
    process A suffers as it is penalized for spawning more threads of execution. To
    ensure group scheduling, `CONFIG_FAIR_GROUP_SCHED` is to be set when configuring
    the kernel. CFS task groups are represented by the structure `sched_entity`*,*
    and every group is referred as a **scheduling entity**. The following code snippet
    shows key elements of the scheduling entity structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`load`: Denotes the amount of load each entity bears on the total load of the
    queue'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vruntime`: Denotes the amount of time the process ran'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling entities under many-core systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Task groups can in a many-core system run on any CPU core, but to facilitate
    this, creating only one scheduling entity will not suffice. Groups thus must create
    a scheduling entity for every CPU core on the system. Scheduling entities across
    CPUs are represented by `struct task_group`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now every task group has a scheduling entity for every CPU core along with a
    CFS runqueue associated with it. When a task from one task group migrates from
    one CPU core (x) to another CPU core (y), the task is dequeued from the CFS runqueue
    of CPU x and enqueued to the CFS runqueue of CPU y.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling policies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scheduling policies are applied to processes, and help in determining scheduling
    decisions. If you recall, in [Chapter 1](part0020.html#J2B80-7300e3ede2f245b0b80e1b18d02a323f),
    *Comprehending Processes, Address Space, and Threads*, we described the `int policy`
    field under the scheduling attributes of struct `task_struct`*.* The `policy field`
    contains the value indicating which policy is to be applied to the process when
    scheduling. The CFS class handles all normal processes using the following two
    policies:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SCHED_NORMAL (0)`: This is used for all normal processes. All non-realtime
    processes can be summarized as normal processes. As Linux aims to be a highly
    responsive and interactive system, most of the scheduling activity and heuristics
    are centered to fairly schedule normal processes. Normal processes are referred
    to as `SCHED_OTHER` as per POSIX.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SCHED_BATCH (3)`: Normally in servers, where processes are non-interactive,
    CPU-bound batch processing is employed. These processes that are CPU intensive
    are given less priority than a `SCHED_NORMAL` process, and they do not preempt
    normal processes, which are scheduled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The CFS class also handles scheduling the idle process, which is specified
    by the following policy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SCHED_IDLE (5)`: When there are no processes to run, the *idle* process (low-priority
    background processes) is scheduled. The *idle* process is assigned the least priority
    among all processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time scheduling class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linux supports soft real-time tasks and they are scheduled by the real-time
    scheduling class. `rt` processes are assigned static priorities and are unchanged
    dynamically by the kernel. As real-time tasks aim at deterministic runs and desire
    control over when and how long they are to be scheduled, they are always given
    preference over normal tasks (`SCHED_NORMAL`). Unlike CFS, which uses `rb tree`
    as its sub-runqueue, the `rt` scheduler, which is less complicated, uses a simple
    `linked list` per priority value (1 to 99). Linux applies two real-time policies,
    `rr` and `fifo`*,* when scheduling static priority processes; these are indicated
    by the `policy` element of `struct task_struct`*.*
  prefs: []
  type: TYPE_NORMAL
- en: '`SCHED_FIFO` (1): This uses the first in, first out method to schedule soft
    real-time processes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SCHED_RR` (2): This is the round-robin policy used to schedule soft real-time
    processes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FIFO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**FIFO** is a scheduling mechanism applied to processes with priorities higher
    than 0 (0 is assigned to normal processes). FIFO processes run sans any timeslice
    allocation; in other words, they invariably run until they block for some event
    or explicitly yield to another process. A FIFO process also gets preempted when
    the scheduler encounters a higher-priority runnable FIFO, RR, or deadline task.
    When scheduler encounters more than one fifo task with the same priority, it runs
    the processes in round robin, starting with the first process at the head of the
    list. On preemption, the process is added back to the tail of the list. If a higher-priority
    process preempts the FIFO process, it waits at the head of the list, and when
    all other high-priority tasks are preempted, it is again picked up to run. When
    a new fifo process becomes runnable, it is added to the tail of the list.'
  prefs: []
  type: TYPE_NORMAL
- en: RR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The round-robin policy is similar to FIFO, with the only exception being that
    it is allocated a timeslice to run. This is kind of an enhancement to FIFO (as
    a FIFO process may run until it yields or waits). Similar to FIFO, the RR process
    at the head of the list is picked for execution (if no other higher-priority task
    is available) and on completion of the timeslice gets preempted and is added back
    to the tail end of the list. RR processes with the same priority run round robin
    until preempted by a high-priority task. When a high-priority task preempts an
    RR task, it waits at the head of the list, and on resumption runs for the remainder
    of its timeslice only.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time group scheduling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to group scheduling under CFS, real-time processes can also be grouped
    for scheduling with `CONFIG_RT_GROUP_SCHED` set. For group scheduling to succeed,
    each group must be assigned a portion of CPU time, with a guarantee that the timeslice
    is enough to run the tasks under each entity, or it fails. So "run time" (a portion
    of how much time a CPU can spend running in a period) is allocated per group.
    The run time allocated to one group will not be used by another group. CPU time
    that is not allocated for real-time groups will be used by normal-priority tasks
    and any time unused by the real-time entities will also be picked by the normal
    tasks. FIFO and RR groups are represented by `struct sched_rt_entity`*:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Deadline scheduling class (sporadic task model deadline scheduling)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deadline** represents the new breed of RT processes on Linux (added since
    the 3.14 kernel). Unlike FIFO and RR, where processes may hog CPU or be bound
    by timeslices, a deadline process, which is based on GEDF (Global Earliest Deadline
    First) and CBS (Constant Bandwidth Server) algorithms, predetermines its runtime
    requirements. A sporadic process internally runs multiple tasks, with each task
    having a relative deadline within which it must complete executing and a computation
    time, defining the time that the CPU needs to complete process execution. To ensure
    that the kernel succeeds in executing deadline processes, the kernel runs an admittance
    test based on the deadline parameters, and on failure returns an error, `EBUSY`.
    Processes with the deadline policy gets precedence over all other processes. Deadline
    processes use `SCHED_DEADLINE` (6) as their policy element.'
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler related system calls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linux provides an entire family of system calls that manage various scheduler
    parameters, policies, and priorities and retrieve a multitude of scheduling-related
    information for the calling threads. It also enables threads to yield CPU explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`nice()` takes an *int* parameter and adds it to the `nice` value of the calling
    thread. On success, it returns the new nice value of the thread. Nice values are
    within the range 19 (lowest priority) to -20 (highest priority). *Nice* values
    can be incremented only within this range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the `nice` value of the thread, group, user, or set of threads
    of a specified user as indicated by its parameters. It returns the highest priority
    held by any of the processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The scheduling priority of the thread, group, user, or set of threads of a
    specified user as indicated by its parameters is set by `setpriority`*.* It returns
    zero on success:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This sets both the scheduling policy and parameters of a specified thread,
    indicated by its `pid`. If the `pid` is zero, the policy of the calling thread
    will be set. The `param` argument, which specifies the scheduling parameters,
    points to a structure `sched_param`, which holds `int sched_priority`. `sched_priority`
    must be zero for normal processes and a priority value in the range 1 to 99 for
    FIFO and RR policies (mentioned in policy argument). It returns zero on success:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'It returns the scheduling policy of a thread (`pid`). If the `pid` is zero,
    the policy of the calling thread will be retrieved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'It sets the scheduling parameters associated with the scheduling policy of
    the given thread (`pid`). If the `pid` is zero, the parameters of the calling
    process are set. On success, it returns zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This sets the scheduling parameters for the specified thread (`pid`). If the
    `pid` is zero, the scheduling parameters of the calling thread will be retrieved.
    On success, it returns zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It sets the scheduling policy and related attributes for the specified thread
    (`pid`). If the `pid` is zero, the policy and attributes of the calling process
    are set. This is a Linux-specific call and is the superset of the functionality
    provided by `sched_setscheduler()` and `sched_setparam()` calls. On success, it
    returns zero.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: It fetches the scheduling policy and related attributes of the specified thread
    (`pid`). If the `pid` is zero the scheduling policy and related attributes of
    the calling thread will be retrieved. This is a Linux-specific call and is a superset
    of the functionality provided by `sched_getscheduler()` and `sched_getparam()`
    calls. On success, it returns zero.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This returns the max and min priority respectively for the specified `policy`.
    `fifo`, `rr`, `deadline`, `normal`, `batch`, and `idle` are supported values of
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: It fetches the time quantum of the specified thread (`pid`) and writes it into
    the `timespec struct`, specified by `tp`*.* If the `pid` is zero, the time quantum
    of the calling process is fetched into `tp`. This is only applicable to processes
    with the `*rr*` policy. On success, it returns zero.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This is called to relinquish the CPU explicitly. The thread is now added back
    to the queue. On success, it returns zero.
  prefs: []
  type: TYPE_NORMAL
- en: Processor affinity calls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linux-specific processor affinity calls are provided, which help the threads
    define on which CPU(s) they want to run. By default, every thread inherits the
    processor affinity of its parent, but it can define its affinity mask to determine
    its processor affinity. On many-core systems, CPU affinity calls help in enhancing
    the performance, by helping the process stick to one core (Linux however attempts
    to keep a thread on one CPU). The affinity bitmask information is contained in
    the `cpu_allowed` field of `struct task_struct`*.* The affinity calls are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: It sets the CPU affinity mask of the thread (`pid`) to the value mentioned by
    `mask`*.* If the thread (`pid`) is not running in one of the specified CPU's queues,
    it is migrated to the specified `cpu`. On success, it returns zero.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This fetches the affinity mask of the thread (`pid`) into the `cpusetsize` structure,
    pointed to by *mask.* If the `pid` is zero, the mask of the calling thread is
    returned. On success, it returns zero.
  prefs: []
  type: TYPE_NORMAL
- en: Process preemption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Understanding preemption and context switching is key to fully comprehending
    scheduling and the impact it has on the kernel in maintaining low latency and
    consistency. Every process must be preempted either implicitly or explicitly to
    make way for another process. Preemption might lead to context switching, which
    requires a low-level architecture-specific operation, carried out by the function
    `context_switch()`*.* There are two primary tasks that need to be done for a processor
    to switch its context: switch the virtual memory mapping of the old process with
    the new one, and switch the processor state from that of the old process to the
    new one. These two tasks are carried out by `switch_mm()` and `switch_to()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Preemption can happen for any of the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: When a high-priority process becomes runnable. For this, the scheduler will
    have to periodically check for a high-priority runnable thread. On return from
    interrupts and system calls, `TIF_NEED_RESCHEDULE` (kernel-provided flag that
    indicates the need for a reschedule) is set, invoking the scheduler. Since there
    is a periodic timer interrupt that is guaranteed to occur at regular intervals,
    invocation of the scheduler is guaranteed. Preemption also happens when a process
    enters a blocking call or on occurrence of an interrupt event.
  prefs: []
  type: TYPE_NORMAL
- en: The Linux kernel historically has been non-preemptive, which means a task in
    kernel mode is non-preemptible unless an interrupt event occurs or it chooses
    to explicitly relinquish CPU. Since the 2.6 kernel, preemption has been added
    (needs to be enabled during kernel build). With kernel preemption enabled, a task
    in kernel mode is preemptible for all the reasons listed, but a kernel-mode task
    is allowed to disable kernel preemption while carrying out critical operations.
    This has been made possible by adding a preemption counter (`preempt_count`) to
    each process's `thread_info` structure*.* Tasks can disable/enable preemption
    through the kernel macros `preempt_disable()` and `preempt_enable()`*,* which
    in turn increment and decrement the `preempt_counter`*.* This ensures that the
    kernel is preemptible only when the `preempt_counter` is zero (indicating no acquired
    locks).
  prefs: []
  type: TYPE_NORMAL
- en: Critical sections in the kernel code are executed by disabling preemption, which
    is enforced by invoking `preempt_disable` and `preempt_enable` calls within kernel
    lock operations (spinlock, mutex).
  prefs: []
  type: TYPE_NORMAL
- en: Linux kernels build with "preempt rt", supporting *fully preemptible kernel*
    option, which when enabled makes all the kernel code including critical sections
    be fully preemptible.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Process scheduling is an ever-evolving aspect of the kernel, and as Linux evolves
    and diversifies further into many computing domains, finer tweaks and changes
    to the process scheduler will be mandated. However, with our understanding established
    over this chapter, gaining deeper insights or comprehending any new changes will
    be quite easy. We are now equipped to go further and explore another important
    aspect of job control and signal management. We will brush through basics of signals
    and move on into signal management data structures and routines of the kernel.
  prefs: []
  type: TYPE_NORMAL
