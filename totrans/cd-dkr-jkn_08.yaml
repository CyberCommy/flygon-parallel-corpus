- en: Clustering with Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already covered all the fundamental aspects of the Continuous Delivery
    pipeline. In this chapter, we will see how to change the Docker environment from
    a single Docker host into a cluster of machines and how to use it all together
    with Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the concept of server clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Docker Swarm and its most important features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Presenting how to build a swarm cluster from multiple Docker hosts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running and scaling Docker images on a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exploring advanced swarm features: rolling updates, draining nodes, multiple
    manager nodes, and tuning the scheduling strategy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the Docker Compose configuration on a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Kubernetes and Apache Mesos as alternatives to Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamically scaling Jenkins agents on a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Server clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have have interacted with each of the machines individually. Even
    when we used Ansible to repeat the same operations on multiple servers, we had
    to explicitly specify on which host the given service should be deployed. In most
    cases, however, if servers share the same physical location, we are not interested
    on which particular machine the service is deployed. All we need is to have it
    accessible and replicated in many instances. How can we configure a set of machines
    to work together so that adding a new one would require no additional setup? This
    is the role of clustering.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will be introduced to the concept of server clustering
    and the Docker Swarm toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing server clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A server cluster is a set of connected computers that work together in a way
    that they can be used similarly to a single system. Servers are usually connected
    through the local network with a connection fast enough to ensure a small influence
    of the fact that services are distributed. A simple server cluster is presented
    in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5932276e-c8df-4777-80c1-69ac1bbccded.png)'
  prefs: []
  type: TYPE_IMG
- en: A user accesses the cluster via a host called the manager, whose interface should
    be similar to a usual Docker host. Inside the cluster, there are multiple worker
    nodes that receive tasks, execute them, and notify the manager of their current
    state. The manager is responsible for the orchestration process, including task
    dispatching, service discovery, load balancing, and worker failure detection.
  prefs: []
  type: TYPE_NORMAL
- en: The manager can also execute tasks, which is the default configuration in Docker
    Swarm. However, for large clusters, the manager should be configured for management
    purposes only.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Swarm is a native clustering system for Docker that turns a set of Docker
    hosts into one consistent cluster, called a swarm. Each host connected to the
    swarm plays the role of a manager or a worker (there must be at least one manager
    in a cluster). Technically, the physical location of the machines does not matter;
    however, it's reasonable to have all Docker hosts inside one local network, otherwise,
    managing operations (or reaching consensus between multiple managers) can take
    a significant amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: Since Docker 1.12, Docker Swarm is natively integrated into Docker Engine as
    swarm mode. In older versions, it was necessary to run the swarm container on
    each of the hosts to provide the clustering functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the terminology, in swarm mode, a running image is called a **service,**
    as opposed to a **container**, which is run on a single Docker host. One service
    runs a specified number of **tasks.** A task is an atomic scheduling unit of the
    swarm that holds the information about the container and the command that should
    be run inside the container. A **replica** is each container that is run on the
    node. The number of replicas is the expected number of all containers for the
    given service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an image presenting the terminology and the Docker Swarm clustering
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/503d41a5-5167-45a9-ac26-547095f5f638.png)'
  prefs: []
  type: TYPE_IMG
- en: We start by specifying a service, the Docker image and the number of replicas.
    The manager automatically assigns tasks to worker nodes. Obviously, each replicated
    container is run from the same Docker image. In the context of the presented flow,
    Docker Swarm can be viewed as a layer on top of the Docker Engine mechanism that
    is responsible for container orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding sample image, we have three tasks, and each of them is run
    on a separate Docker host. Nevertheless, it may also happen that all containers
    would be started on the same Docker host. Everything depends on the manager node
    that allocates tasks to worker nodes using the scheduling strategy. We will show
    how to configure that strategy later, in a separate section.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm features overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Docker Swarm provides a number interesting features. Let''s walk through the
    most important ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load balancing**: Docker Swarm takes care of the load balancing and assigning
    unique DNS names so that the application deployed on the cluster can be used in
    the same way as deployed on a single Docker host. In other words, a swarm can
    publish ports in a similar manner as the Docker container, and then the swarm
    manager distributes requests among the services in the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic role management**: Docker hosts can be added to the swarm at runtime,
    so there is no need for a cluster restart. What''s more, the role of the node
    (manager or worker) can also be dynamically changed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic service scaling**: Each service can be dynamically scaled up or down
    with the Docker client. The manager node takes care of adding or removing containers
    from the nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failure recovery**: Nodes are constantly monitored by the manager and, if
    any of them fails, new tasks are started on different machines so that the declared
    number of replicas would be constant. It''s also possible to create multiple manager
    nodes in order to prevent a breakdown in case one of them fails.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rolling updates**: An update to services can be applied incrementally; for
    example, if we have 10 replicas and we would like to make a change, we can define
    a delay between the deployment to each replica. In such a case, when anything
    goes wrong, we never end up with a scenario where no replica is working correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Two service modes:** There are two modes in which can be run:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replicated services**: The specified number of replicated containers are
    distributed among the nodes based on the scheduling strategy algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global services**: One container is run on every available node in the cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security:** As everything is in Docker, Docker Swarm enforces the TLS authentication
    and the communication encryption. It''s also possible to use CA (or self-signed)
    certificates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see how all of this looks in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Engine includes the Swarm mode by default, so there is no additional
    installation process required. Since Docker Swarm is a native Docker clustering
    system, managing cluster nodes is done by the `docker` command and is therefore
    very simple and intuitive. Let's start by creating a manager node with two worker
    nodes. Then, we will run and scale a service from a Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to set up a Swarm, we need to initialize the manager node. We can
    do this using the following command on a machine that is supposed to become the
    manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A very common practice is to use the `--advertise-addr <manager_ip>` parameter,
    because if the manager machine has more than one potential network interfaces,
    then `docker swarm init` can fail.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the manager machine has the IP address `192.168.0.143` and, obviously,
    it has to be reachable from the worker nodes (and vice versa). Note that the command
    to execute on worker machines was printed to the console. Also note that a special
    token has been generated. From now on, it will be used to connect a machine to
    the cluster and should be kept secret.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check that the Swarm has been created using the `docker node` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When the manager is up and running, we are ready to add worker nodes to the
    Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Adding worker nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to add a machine to the Swarm, we have to log in to the given machine
    and execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check that the node has been added to the Swarm with the `docker node
    ls` command. Assuming that we''ve added two node machines, the output should look
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we have a cluster that consists of three Docker hosts, `ubuntu-manager`,
    `ubuntu-worker1`, and `ubuntu-worker2`. Let's see how we can run a service on
    this cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to run an image on a cluster, we don''t use `docker run` but the Swarm-dedicated
    `docker service` command (which is executed on the manager node). Let''s start
    a single `tomcat` application and give it the name `tomcat`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The command created the service and therefore sent a task to start a container
    on one of the nodes. Let''s list the running services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The log confirms that the `tomcat` service is running, and it has one replica
    (one Docker container is running). We can examine the service even more closely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you are interested in the detailed information about a service, you can use
    the `docker service inspect <service_name>` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the console output, we can see that the container is running on the manager
    node (`ubuntu-manager`). It could have been started on any other node as well;
    the manager automatically chooses the worker node using the scheduling strategy
    algorithm. We can confirm that the container is running using the well-known `docker
    ps` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If we don't want a task to be executed on the manager node, we can constrain
    the service with the `--constraint node.role==worker` option. The other possibility
    is to disable the manager completely from executing tasks with `docker node update
    --availability drain <manager_name>`.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When the service is running, we can scale it up or down so that it will be
    running in many replicas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check that the service has been scaled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that this time, two containers are running on the `manager` node, one on
    the `ubuntu-worker1` node, and the other on the `ubuntu-worker2` node. We can
    check that they are really running by executing `docker ps` on each of the machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to remove the services, it''s enough to execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You can check with the `docker service ls` command that the service has been
    removed, and therefore all related `tomcat` containers were stopped and removed
    from all the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing ports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Docker services, similar to the containers, have a port forwarding mechanism.
    We use it by adding the `-p <host_port>:<container:port>` parameter. Here''s what
    starting a service could look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can open a browser and see the Tomcat's main page under the address
    `http://192.168.0.143:8080/`.
  prefs: []
  type: TYPE_NORMAL
- en: The application is available on the manager host that acts as a load balancer
    and distributes requests to worker nodes. What may sound a little less intuitive
    is the fact that we can access Tomcat using the IP address of any worker, for
    example, if worker nodes are available under `192.168.0.166` and `192.168.0.115`,
    we can access the same running container with `http://192.168.0.166:8080/` and
    `http://192.168.0.115:8080/`. This is possible because Docker Swarm creates a
    routing mesh, in which each node has the information how to forward the published
    port.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about how the load balancing and routing are done by Docker
    Swarm at [https://docs.docker.com/engine/swarm/ingress/](https://docs.docker.com/engine/swarm/ingress/).
  prefs: []
  type: TYPE_NORMAL
- en: By default, the internal Docker Swarm load balancing is used. Therefore, it's
    enough to address all requests to the manager's machine, and it will take care
    of its distribution between nodes. The other option is to configure an external
    load balancer (for example, HAProxy or Traefik).
  prefs: []
  type: TYPE_NORMAL
- en: We have discussed the basic usage of Docker Swarm. Let's now dive into more
    challenging features.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Swarm offers a lot of interesting features that are useful in the Continuous
    Delivery process. In this section, we will walk through the most important ones.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you deploy a new version of your application. You need to update all
    replicas in the cluster. One option would be to stop the whole Docker Swarm service
    and to run a new one from the updated Docker image. Such approach, however, causes
    downtime between the moment when the service is stopped and the moment when the
    new one is started. In the Continuous Delivery process, downtime is not acceptable,
    since the deployment can take place after every source code change, which is simply
    often. Then, how can we provide zero-downtime deployment in a cluster? This is
    the role of rolling updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'A rolling update is an automatic method for replacing a service, replica by
    a replica, in a way that some of the replicas are working all the time. Docker
    Swarm uses rolling updates by default, and they can be steered with two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`update-delay`: Delay between starting one replica and stopping the next one
    (0 seconds by default)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update-parallelism`: Maximum number of replicas updated at the same time (one
    by default)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Docker Swarm rolling update process looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Stop the `<update-parallelism>` number of tasks (replicas).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In their place, run the same number of updated tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If a task returns the **RUNNING** state, then wait for the `<update-delay>`
    period.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If, at any time, any task returns the **FAILED** state, then pause the update.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value of the `update-parallelism` parameter should be adapted to the number
    of replicas we run. If the number is small and booting the service is fast, it's
    reasonable to keep the default value of 1\. The `update-delay` parameter should
    be set to the period longer than the expected boot time of our application so
    that we will notice the failure, and therefore pause the update.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example and change the Tomcat application from version 8
    to version 9\. Suppose we have the `tomcat:8` service with five replicas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check that all replicas are running with the `docker service ps tomcat` command.
    Another useful command that helps examine the service is the `docker service inspect` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the service has five replicas created out of the image `tomcat:8`.
    The command output also includes the information about the parallelism and the
    delay time between updates (as set by the options in the `docker service create` command).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can update the service into the `tomcat:9` image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the first replica of `tomcat:8` has been shut down and the first
    `tomcat:9` is already running. If we kept on checking the output of the `docker
    service ps tomcat` command, we would notice that after every 10 seconds, another
    replica is in the shutdown state and a new one is started. If we also monitored
    the `docker inspect` command, we would see that the value **UpdateStatus: State**
    will change to **updating** and then, when the update is done, to **completed**.'
  prefs: []
  type: TYPE_NORMAL
- en: A rolling update is a very powerful feature that allows zero downtime deployment
    and it should always be used in the Continuous Delivery process.
  prefs: []
  type: TYPE_NORMAL
- en: Draining nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we need to stop a worker node for maintenance, or we would just like to
    remove it from the cluster, we can use the Swarm draining node feature. Draining
    the node means asking the manager to move all tasks out of a given node and exclude
    it from receiving new tasks. As a result, all replicas are running only on the
    active nodes and the drained nodes are idle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look how this works in practice. Suppose we have three cluster nodes
    and a Tomcat service with five replicas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check on which nodes the replicas are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two replicas running on the `ubuntu-worker2` node. Let''s drain that
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The node is put into the **drain** availability, so all replicas should be
    moved out of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that new tasks were started on the `ubuntu-worker1` node and the
    old replicas were shut down. We can check the status of the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the `ubuntu-worker2` node is available (status `Ready`), but its
    availability is set to drain, which means it doesn''t host any tasks. If we would
    like to get the node back, we can check its availability to `active`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: A very common practice is to drain the manager node and, as a result, it will
    not receive any tasks, but do management work only.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative method to draining the node would be to execute the `docker
    swarm leave` command from the worker. This approach, however, has two drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: For a moment, there are fewer replicas than expected (after leaving the swarm
    and before the master starts new tasks on other nodes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The master does not control if the node is still in the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For these reasons, if we plan to stop the worker for some time and then get
    it back, it's recommended to use the draining node feature.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple manager nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having a single manager node is risky because when the manager machine is down,
    the whole cluster is down. This situation is, obviously, not acceptable in the
    case of business-critical systems. In this section, we present how to manage multiple
    master nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to add a new manager node to the system, we need to first execute
    the following command on the (currently single) manager node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The output shows the token and the entire command that needs to be executed
    on the machine that is supposed to become the manager. After executing it, we
    should see that a new manager was added.
  prefs: []
  type: TYPE_NORMAL
- en: Another option to add a manager is to promote it from the worker role using
    the `docker node promote <node>` command. In order to get it back to the worker
    role, we can use the `docker node demote <node>` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have added two additional managers; we should see the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that the new managers have the manager status set to reachable (or left
    empty), while the old manager is the leader. The reason for this is the fact that
    there is always one primary node responsible for all Swarm management and orchestration
    decisions. The leader is elected from the managers using the Raft consensus algorithm,
    and when it is down, a new leader is elected.
  prefs: []
  type: TYPE_NORMAL
- en: Raft is a consensus algorithm that is used to make decisions in distributed
    systems. You can read more about how it works (and see a visualization) at [https://raft.github.io/](https://raft.github.io/).
    A very popular alternative algorithm for the same goal is called Paxos.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we shut down the `ubuntu-manager` machine; let''s have a look at how
    the new leader was elected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note that even when one of the managers is down, the swarm can work correctly.
  prefs: []
  type: TYPE_NORMAL
- en: There is no limit for the number of managers, so it may sound that the more
    managers the better fault tolerance. It's true, however, having a lot of managers
    has an impact on the performance because all Swarm-state-related decisions (for
    example, adding a new node or leader election) have to be agreed between all managers
    using the Raft algorithm. As a result, the number of managers is always a tradeoff
    between the fault tolerance and the performance.
  prefs: []
  type: TYPE_NORMAL
- en: The Raft algorithm itself has a constraint on the number of managers. Distributed
    decisions have to be approved by the majority of nodes, called a quorum. This
    fact implies that an odd number of managers is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: To understand why, let's see what would happen if we had two managers. In that
    case, the majority is two, so if any of the managers is down, then it's not possible
    to reach the quorum and therefore elect the leader. As a result, losing one machine
    makes the whole cluster out of order. We added a manager, but the whole cluster
    became less fault tolerant. The situation would be different in the case of three
    managers. Then, the majority is still two, so losing one manager does not stop
    the whole cluster. This is the fact that even though it's not technically forbidden,
    only odd number of managers makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: The more the managers in the cluster, the more the Raft-related operations are
    involved. Then, the `manager` nodes should be put into the drain availability
    in order to save their resources.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned that the manager automatically assigns a worker node
    to a task. In this section, we dive deeper into what automatically means. We present
    the Docker Swarm scheduling strategy and a way to configure it according to our
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Swarm uses two criteria for choosing the right worker node:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource availability**: Scheduler is aware of the resources available on
    nodes. It uses the so-called **spread strategy** that attempts to schedule the
    task on the least loaded node, provided it meets the criteria specified by labels
    and constraints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Labels and constraints**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label is an attribute of a node. Some labels are assigned automatically, for
    example, `node.id` or `node.hostname`; others can be defined by the cluster admin,
    for example, `node.labels.segment`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constraint is a restriction applied by the creator of the service, for example,
    choosing only nodes with the given label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Labels are divided into two categories, `node.labels` and `engine.labels`. The
    first one is added by the operational team; the second one is collected by Docker
    Engine, for example, operating system or hardware specifics.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, if we would like to run the Tomcat service on the concrete node,
    `ubuntu-worker1`, then we need to use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also add a custom label to the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command added a label, `node.labels.segment`, with the value
    `AA`. Then, we can use it while running the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This command runs the `tomcat` replicas only on the nodes that are labeled with
    the given segment, `AA`.
  prefs: []
  type: TYPE_NORMAL
- en: Labels and constraints give us the flexibility to configure the nodes on which
    service replicas would be run. This approach, even though valid in many cases,
    should not be overused, since it's best to keep the replicas distributed on multiple
    nodes and let Docker Swarm take care of the right scheduling process.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose with Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have described how to use Docker Swarm in order to deploy a service, which
    in turn runs multiple containers from the given Docker image. On the other hand,
    there is Docker Compose, which provides a method to define the dependencies between
    containers and enables scaling containers, but everything is done within one Docker
    host. How do we merge both technologies so that we can specify the `docker-compose.yml`
    file and automatically distribute the containers on a cluster? Luckily, there
    is Docker Stack.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Docker Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Docker Stack is a method to run multiple-linked containers on a Swarm cluster.
    To understand better how it links Docker Compose with Docker Swarm, let''s take
    a look at the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/86ad1636-d244-4a44-9c67-c64b8080eba1.png)'
  prefs: []
  type: TYPE_IMG
- en: Docker Swarm orchestrates which container is run on which physical machine.
    The containers, however, don't have any dependencies between themselves, so in
    order for them to communicate, we would need to link them manually. Docker Compose,
    in contrast, provides linking between the containers. In the example from the
    preceding figure, one Docker image (deployed in three replicated containers) depends
    on another Docker image (deployed as one container). All containers, however,
    run on the same Docker host, so the horizontal scaling is limited to the resources
    of one machine. Docker Stack connects both technologies and allows using the `docker-compose.yml`
    file to run the complete environment of linked containers deployed on a cluster
    of Docker hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As an example, let''s use the `calculator` image that depends on the `redis`
    image. Let''s split the process into four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Specifying `docker-compose.yml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running the Docker Stack command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verifying the services and containers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removing the stack.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specifying docker-compose.yml
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We already defined the `docker-compose.yml` file in the previous chapters and
    it looked similar to the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note that all images must be pushed to the registry before running the `docker
    stack` command so that they would be accessible from all nodes. It is therefore
    not possible to build images inside `docker-compose.yml`.
  prefs: []
  type: TYPE_NORMAL
- en: With the presented docker-compose.yml configuration, we will run three `calculator`
    containers and one `redis` container. The endpoint of the calculator service will
    be exposed on port `8881`.
  prefs: []
  type: TYPE_NORMAL
- en: Running the docker stack command
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s use the `docker stack` command to run the services, which in turn will
    start containers on the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Docker plans to simplify the syntax so that the `stack` word would not be needed,
    for example, `docker deploy --compose-file docker-compose.yml app`. At the time
    of writing, it's only available in the experimental version.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the services and containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The services have started. We can check that they are running with the `docker
    service ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can look even closer at the services and check on which Docker hosts they
    have been deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We can see that one of the `calculator` containers and the `redis` containers
    are started on the `ubuntu-manager` machine. Two other `calculator` containers
    run on the `ubuntu-worker1` and `ubuntu-worker2` machines.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we explicitly specified the port number under which the `calculator`
    web service should be published. Therefore, we are able to access the endpoint
    via the manager's IP address `http://192.168.0.143:8881/sum?a=1&b=2`. The operation
    returns `3` as a result and caches it in the Redis container.
  prefs: []
  type: TYPE_NORMAL
- en: Removing the stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we''re done with the stack, we can remove everything using the convenient
    `docker stack rm` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Using Docker Stack allows running the Docker Compose specification on the Docker
    Swarm cluster. Note that we used the exact `docker-compose.yml` format, which
    is a great benefit, because there is no need to specify anything extra for the
    Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: The merger of both technologies gives us the real power of deploying applications
    on Docker because we don't need to think about individual machines. All we need
    to specify is how our (micro) services are dependent on each other, express it
    in the docker-compose.yml format, and let Docker take care of everything else.
    The physical machines can be then treated simply as a set of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative cluster management systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Swarm is not the only system for clustering Docker containers. Even though
    it's the one available out of the box, there may be some valid reasons to install
    a third-party cluster manager. Let's walk through the most popular alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is an open source cluster management system originally designed by
    Google. Even though it's not Docker-native, the integration is smooth, and there
    are many additional tools that help with this process; for example, **kompose**
    can translate the `docker-compose.yml` files into Kubernetes configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the simplified architecture of Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6ae7e3ed-c6d5-4034-bbb2-e34bae100556.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes is similar to Docker Swarm in a way that it also has master and worker
    nodes. Additionally, it introduces the concept of a **pod** that represents a
    group of containers deployed and scheduled together. Most pods have a few containers
    that make up a service. Pods are dynamically built and removed depending on the
    changing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is relatively young. Its development started in 2014; however, it's
    based on Google's experience, and this is one of the reasons why it's one of the
    most popular cluster management systems available on the market. There are more
    and more organizations that migrated to Kubernetes, such as eBay, Wikipedia, and
    Pearson.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Mesos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Mesos is an open source scheduling and clustering system started at the
    University of California, Berkeley, in 2009, long before Docker emerged. It provides
    an abstraction layer over CPU, disk space, and RAM. One of the great advantages
    of Mesos is that it supports any Linux application, not necessarily (Docker) containers.
    This is why it's possible to create a cluster out of thousands of machines and
    use it for both Docker containers and other programs, for example, Hadoop-based
    calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the figure presenting the Mesos architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8debdc02-d7c6-4f97-948f-e6f51db3e6ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Apache Mesos, similar to other clustering systems, has the master-slave architecture.
    It uses node agents installed on every node for communication, and it provides
    two types of schedulers, Chronos - for cron-style repeating tasks and Marathon
    - providing a REST API to orchestrate services and containers.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Mesos is very mature compared to other clustering systems, and it has
    been adopted in a large number of organizations, such as Twitter, Uber, and CERN.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes, Docker Swarm, and Mesos are all good choices for the cluster management
    system. All of them are free and open source, and all of them provide important
    cluster management features such as load balancing, service discovery, distributed
    storage, failure recovery, monitoring, secret management, and rolling updates.
    All of them can also be used in the Continuous Delivery process without huge differences.
    This is because, in the Dockerized infrastructure, they all address the same issue,
    the clustering of Docker containers. Nevertheless, obviously, the systems are
    not exactly the same. Let''s have a look at a table presenting the differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Docker Swarm** | **Kubernetes** | **Apache Mesos** |'
  prefs: []
  type: TYPE_TB
- en: '| **Docker support** | Native | Supports Docker as one of the container types
    in the pod | Mesos agents (slaves) can be configured to host Docker containers
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Application types** | Docker images | Containerized applications (Docker,
    rkt, and hyper) | Any application that can be run on Linux (also containers) |'
  prefs: []
  type: TYPE_TB
- en: '| **Application definition** | Docker Compose configuration | Pods configuration,
    replica sets, replication controllers, services, and deployments | Application
    groups formed in the tree structure |'
  prefs: []
  type: TYPE_TB
- en: '| **Setup process** | Very simple | Depending on the infrastructure, it may
    require running one command or many complex operations | Fairly involved, it requires
    configuring Mesos, Marathon, Chronos, Zookeeper, and Docker support |'
  prefs: []
  type: TYPE_TB
- en: '| **API** | Docker REST API | REST API | Chronos/Marathon REST API |'
  prefs: []
  type: TYPE_TB
- en: '| **User Interface** | Docker console client, third-party web applications,
    such as Shipyard | Console tools, native Web UI (Kubernetes Dashboard) | Official
    web interfaces for Mesos, Marathon, and Chronos |'
  prefs: []
  type: TYPE_TB
- en: '| **Cloud integration** | Manual installation required | Cloud-native support
    from most providers (Azure, AWS, Google Cloud, and others) | Support from most
    cloud providers |'
  prefs: []
  type: TYPE_TB
- en: '| **Maximum cluster size** | 1,000 nodes | 1,000 nodes | 50,000 nodes |'
  prefs: []
  type: TYPE_TB
- en: '| **Autoscaling** | Not available | Provides horizontal pod autoscaling based
    on the observed CPU usage | Marathon provides autoscaling based on resource (CPU/Memory)
    consumption, number of requests per second, and queue length |'
  prefs: []
  type: TYPE_TB
- en: Obviously, apart from Docker Swarm, Kubernetes, and Apache Mesos, there are
    other clustering systems available in the market. They are, however, not that
    popular and their usage decreases over time.
  prefs: []
  type: TYPE_NORMAL
- en: No matter which system you choose, you can use it not only for the staging/production
    environments but also to scale Jenkins agents. Let's have a look at how to do
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling Jenkins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The obvious use cases for server clustering are the staging and production environments.
    When used, it's enough to attach a physical machine in order to increase the capacity
    of the environment. Nevertheless, in the context of Continuous Delivery, we may
    also want to improve the Jenkins infrastructure by running Jenkins agent (slave)
    nodes on a cluster. In this section, we take a look at two different methods to
    achieve this goal.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic slave provisioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw dynamic slave provisioning in [Chapter 3](f46b0c80-7f67-476c-9a17-8f1238f0b359.xhtml),
    *Configuring Jenkins*. With Docker Swarm, the idea stays exactly the same. When
    the build is started, the Jenkins master runs a container from the Jenkins slave
    Docker image, and the Jenkinsfile script is executed inside the container. Docker
    Swarm, however, makes the solution more powerful since we are not limited to a
    single Docker host machine but can provide real horizontal scaling. Adding a new
    Docker host to the cluster effectively scales up the capacity of the Jenkins infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the Jenkins Docker plugin does not support Docker Swarm.
    One of the solutions is to use Kubernetes or Mesos as the cluster management system.
    Each of them has a dedicated Jenkins plugin: Kubernetes Plugin ([https://wiki.jenkins.io/display/JENKINS/Kubernetes+Plugin](https://wiki.jenkins.io/display/JENKINS/Kubernetes+Plugin))
    and Mesos Plugin ([https://wiki.jenkins.io/display/JENKINS/Mesos+Plugin](https://wiki.jenkins.io/display/JENKINS/Mesos+Plugin)).
  prefs: []
  type: TYPE_NORMAL
- en: No matter how the slaves are provisioned, we always configure them by installing
    the appropriate plugin and adding the entry to the Cloud section in Manage Jenkins
    | Configure System.
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we don't want to use the dynamic slave provisioning, then another solution
    for clustering Jenkins slaves is to use Jenkins Swarm. We described how to use
    it in [Chapter 3](f46b0c80-7f67-476c-9a17-8f1238f0b359.xhtml), *Configuring Jenkins*.
    Here, we add the description for Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s have a look at how to run the Jenkins Swarm slave using the Docker
    image built from the swarm-client.jar tool. There are a few of them available
    on Docker Hub; we can use the csanchez/jenkins-swarm-slave image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This command execution should have exactly the same effect as the on presented in [Chapter
    3](f46b0c80-7f67-476c-9a17-8f1238f0b359.xhtml), *Configuring Jenkins*; it dynamically
    adds a slave to the Jenkins master.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, to get the most of Jenkins Swarm, we can run the slave containers on
    the Docker Swarm cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command starts five slaves on the cluster and attaches them to
    the Jenkins master. Please note that it is very simple to scale Jenkins horizontally
    by executing the docker service scale command.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison of dynamic slave provisioning and Jenkins Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dynamic slave provisioning and Jenkins Swarm can be both run on a cluster that
    results in the architecture presented in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/165c3a7a-c681-4d65-bb26-93517bf1e7e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Jenkins slaves are run on the cluster and therefore are very easily scaled up
    and down. If we need more Jenkins resources, we scale up Jenkins slaves. If we
    need more cluster resources, we add more physical machines to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference between the two solutions is that the dynamic slave provisioning
    automatically adds a Jenkins slave to the cluster before each build. The benefit
    of such approach is that we don''t even have to think about how many Jenkins slaves
    should be running at the moment since the number automatically adapts to the number
    of pipeline builds. This is why, in most cases, the dynamic slave provisioning
    is the first choice. Nevertheless, Jenkins Swarm also carries a few significant
    benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Control of the number of slaves**: Using Jenkins Swarm, we explicitly decide
    how many Jenkins slaves should be running at the moment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stateful slaves**: Many builds share the same Jenkins slave, which may sound
    like a drawback; however, it becomes an advantage when a build requires downloading
    a lot of dependent libraries from the internet. In the case of dynamic slave provisioning,
    to cache the dependencies, we would need to set up a shared volume.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Control of where the slaves are running**: Using Jenkins Swarm, we can decide
    not to run slaves on the cluster but to choose the host machine dynamically; for
    example, for many startups, when the cluster infrastructure is costly, slaves
    can be dynamically run on the laptop of a developer who is starting the build.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering Jenkins slaves bring a lot of benefits and it is what the modern
    Jenkins architecture should look like. This way, we can provide the dynamic horizontal
    scaling of the infrastructure for the Continuous Delivery process.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have covered Docker Swarm and the clustering process in
    detail. In order to enhance this knowledge, we recommend the following exercises:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up a swarm cluster that consists of three nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use one machine as the manager node and two machines as worker nodes
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use physical machines connected to one network, machines from the cloud
    provider, or VirtualBox machines with the shared network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check that the cluster is configured properly using the `docker node` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Run/scale a hello world service on the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The service can look exactly the same as described in the exercises for [Chapter
    2](aa58c16d-41c0-4364-9eae-26b60a05c510.xhtml), *Introducing Docker*
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Publish the port so that it will be accessible from outside of the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale the service to five replicas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a request to the "hello world" service and check which of the containers
    is serving the request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scale Jenkins using slaves deployed on the Swarm cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use Jenkins Swarm or dynamic slave provisioning
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Run a pipeline build and check that it is executed on one of the clustered slaves
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we took a look at the clustering methods for Docker environments
    that enable setting up the complete staging/production/Jenkins environment. Here
    are the key takeaways from the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering is a method of configuring a set of machines in a way that, in many
    respects, can be viewed as a single system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm is the native clustering system for Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm clusters can be dynamically configured using built-in Docker commands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker images can be run and scaled on the cluster using the docker service command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Stack is a method to run the Docker Compose configuration on a Swarm
    cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most popular clustering systems that support Docker are Docker Swarm, Kubernetes,
    and Apache Mesos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jenkins agents can be run on a cluster using the dynamic slave provisioning
    or the Jenkins Swarm plugin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, we will describe the more advanced aspects of the Continuous
    Delivery process and present the best practices for building pipelines
  prefs: []
  type: TYPE_NORMAL
