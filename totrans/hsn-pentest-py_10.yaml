- en: Building a Custom Crawler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we talk of web application scanning, we often come across crawlers that
    are built into the automatic scanning tools we use for web application scanning.
    Tools such as Burp Suite, Acunetix, web inspect, and so on all have wonderful
    crawlers that crawl through web applications and try various attack vectors against
    the crawled URLs. In this chapter, we are going to understand how a crawler works
    and what happens under the hood. The objective of this chapter is to enable the
    user to understand how a crawler collects all the information and forms the attack
    surface for various attacks. The same knowledge can be later used to develop a
    custom tool that may automate web application scanning. In this chapter, we are
    going to create a custom web crawler that will crawl through a website and give
    us a list that contains the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Web pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HTML forms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All input fields within each form
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will see how we can crawl a web application in two modes:'
  prefs: []
  type: TYPE_NORMAL
- en: Without authentication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With authentication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will have a small GUI developed in the Django (a web application framework
    for Python) that will enable the users to conduct crawling on the test applications.
    It must be noted that the main focus of the chapter is on the workings of the
    crawler, and so we will discuss the crawler code in detail. We will not be focusing
    on the workings of Django web applications. For this, there will be reference
    links provided at the end of the chapter. I will be sharing the whole code base
    in my GitHub repository for readers to download and execute in order to get a
    better understanding of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Setup and installations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The operating system to be used is Ubuntu 16.04\. The code is tested on this
    version, but readers are free to use any other version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the prerequisites required for this chapter by running the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It should be noted that the code is tried and tested on Python 2.7\. It is recommended
    for the readers to try the code on the same version of Python, but it should work
    with Python 3 as well. There might be a few syntactic changes with regard to print
    statements.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A typical Django project follows an MVC-based architecture. The user requests
    first hit the URLs configured in the `Urls.py` file, and from there it is forwarded
    to the appropriate view. The view acts as middleware between the backend core
    logic and the template/HTML that is rendered to user. `views.py` has various methods,
    each of which corresponds to the URL mapper in the `Urls.py` file. On receiving
    the request, the logic written in the `views` class or method prepares the data
    from `models.py` and other core business modules. Once all the data is prepared,
    it is rendered back to the user with the help of templates. Thus, the templates
    form the UI layer of the web project.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram represents the Django request-response cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7a2a172-5d51-47c9-ba36-3e0aa1ffbc65.png)'
  prefs: []
  type: TYPE_IMG
- en: Crawler code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, we have a user interface that will collect the user parameters
    for the web application that is to be crawled. Thus, the request is forwarded
    to the `views.py` file and from there we will invoke the crawler driver file, `run_crawler.py`,
    which in turn will call `crawler.py`. The `new_scan` view method takes all the
    user parameters, saves them in a database, and assigns a new project ID to the
    crawl project. It then passes on the project ID to the crawler driver, for it
    to reference and pull the relevant project parameters with the help of the ID
    and then pass them on to `crawler.py` to start the scanning.
  prefs: []
  type: TYPE_NORMAL
- en: Urls.py and Views.py code snippet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the configuration of the `Urls.py` file, which has the mapping
    between the HTTP URL and the `views.py` method mapped to that URL. The path of
    this file is `Chapter8/Xtreme_InjectCrawler/XtremeWebAPP/Urls.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af450762-e9b2-4934-8aac-19e965629f14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding highlighted line represents the mapping between the URL for the
    new crawl project and the `views` method that caters to the request. Thus, we
    will have a method called `new_scan` inside the `views.py` file. The path of the
    file is `Chapter8/Xtreme_InjectCrawler/XtremeWebAPP/xtreme_server/views.py`. The
    method definition is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/825f44cd-a637-4f6f-98e2-ce6e871a44b2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/d4b60bba-4872-40cf-b9d2-a2e654f8624e.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/1aa217d1-0194-4339-84c8-a4e3bde1596c.png)'
  prefs: []
  type: TYPE_IMG
- en: Code explanation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `new_scan` method will receive both `HTTP GET` and `POST` requests from
    the user. The `GET` request will be resolved to serve the page where the user
    can enter the project parameters and the `POST` request will post all the parameters
    to the previous code, which can then be further processed. As highlighted by section
    **(1)** of the code, the project parameters are being retrieved from the user
    request and are placed in Python program variables. Section (2) of the code does
    the same. It also takes a few other parameters from the settings provided by the
    user and places them in a Python dictionary called settings. Finally, when all
    the data is collected, it saves all the details in the backend database table
    called `Project`. As can be seen in line 261, the code initializes a class called `Project()`,
    and then from lines 262 to 279, it assigns the parameters obtained from the user
    to the instance variables of the `Project()` class. Finally, at line 280, the
    `project.save()` code is invoked. This places all the instance variables into
    a database table as a single row.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, Django follows an ORM model of development. **ORM** stands for **object
    relational mapping**. The model layer of a Django project is a set of classes,
    and when the project is compiled using the `python manage.py syncdb` command,
    these classes actually translate into database tables. We actually do not write
    raw SQL queries in Django to push data to database tables or fetch them. Django
    provides us with a models wrapper that we can access as classes and call various
    methods such as `save()`, `delete()`, `update()`, `filter()`, and `get()` in order
    to perform **create, retrieve, update, and delete** (**CRUD**) operations on our
    database tables. For the current case, let''s take a look at the `models.py` file,
    which contains the `Project` model class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61d66aeb-a4ba-4316-9ec6-3cdee9d63aa4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, when the code is compiled or database syncing happens with the `python
    manage.py syncdb` command, a table will be created in the working database called `<project_name>_Project`.
    The schema of the table will be replicated as per the definition of the instance
    variables in the class. Thus, for the preceding case for the projects table, there
    will be 18 columns created. The table will have a primary key of `project_name`, whose
    data type within the Django application is defined as `CharField`, but at the
    backend will be translated to something like `varchar(50)`. The backend database
    in this case is a SQLite database, which is defined in the `settings.py` file
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39093f85-29cf-47bf-8a37-82d16ce9e608.png)'
  prefs: []
  type: TYPE_IMG
- en: Sections **(3)** and **(4)** of the code snippet are interesting, as this is
    where the workflow execution actually begins. It can be seen in section **(3)**
    that we are checking for the OS environment. If the OS is Windows, then we are
    invoking the `crawler_driver` code `run_crawler.py` as a subprocess.
  prefs: []
  type: TYPE_NORMAL
- en: If the underlying environment is Linux-based, then we are invoking the same
    driver file with the command relevant to the Linux environment. As we might have
    observed previously, we are making use of a subprocess call to invoke this code
    as a separate process. The reason behind having this kind of architecture is so
    that we can use asynchronous processing. The HTTP request sent from the user should
    be responded to quickly with a message to indicate that the crawling has started.
    We can't have the same request held on until the whole crawling operation is complete.
    To accommodate this, we spawn an independent process and offload the crawling
    task to that process, and the HTTP request is immediately returned with an HTTP
    response indicating that the crawling has started. We further map the process
    ID and the project name/ID in the backend database to continuously monitor the
    status of the scan. We return control to the user by redirecting control to the
    details URL which in turn returns the template `details.html`.
  prefs: []
  type: TYPE_NORMAL
- en: Driver code – run_crawler.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code is for the `run_crawler.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11d5a06c-c6a0-4c34-8966-c4cd4a0c918a.png)'
  prefs: []
  type: TYPE_IMG
- en: Remember how we invoked this file from our `views.py` code? We invoked it by
    passing a command-line argument that was the name of the project. As highlighted
    in section **(1)**, the preceding code of `run_crawler.py` loads that command-line
    argument into a project_name program variable. In section **(2)**, the code tries
    to read all the parameters from the backend database table project with the `project.objects.get(project_name=project_name)` command.
    As mentioned earlier, Django follows an ORM model and we don't need to write raw
    SQL queries to take data from database tables. The preceding code snippet will
    internally translate to `select * from project where project_name=project_name`.
    Thus, all the project parameters are pulled and passed to local program variables.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in section **(3)**, we initialize the `crawler` class and pass all
    the project parameters to it. Once initialized, we invoke the `c.start()` method
    highlighted as section **(4)**. This is where the crawling starts. In the next
    section, we will see the working of our crawler class.
  prefs: []
  type: TYPE_NORMAL
- en: Crawler code – crawler.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code snippet represents the constructor of the `crawler` class.
    It initializes all the relevant instance variables. `logger` is one of the custom
    classes written to log debug messages, so that if any error occurs during the
    execution of the crawler, which will have been spawned as a subprocess and will
    be running in the background, it can be debugged:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55cc89cf-df18-4999-9384-5e457c77afbd.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/de260b8d-d57a-4452-9f49-d5b450c1253c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now take a look at the `start()` method of the `crawler`, from where
    the crawling actually begins:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3b390e3-51b0-4b62-b8ba-505f7ae87594.png)'
  prefs: []
  type: TYPE_IMG
- en: It can be seen in section **(1)**, which will be true for the second iteration
    (`auth=True`), that we make a `HTTP GET` request to whichever URL is supplied
    as the login URL by the user. We are using the `GET` method from the Python `requests`
    library. When we make the `GET` request to the URL, the response content (web
    page) is placed in the `xx` variable.
  prefs: []
  type: TYPE_NORMAL
- en: Now, as highlighted in section **(2)**, we extract the content of the webpage
    using the `xx.content` command and pass the extracted content to the instance
    of the `Beautifulsoup` module. `Beautifulsoup` is an excellent Python utility
    that makes parsing web pages very simple. From here on, we will represent `Beautifulsoup` with
    an alias, BS.
  prefs: []
  type: TYPE_NORMAL
- en: Section **(3)** uses the `s.findall('form')` method from the BS parsing library.
    The `findall()` method takes the type of the HTML element, which is to be searched
    as a string argument, and it returns a list containing the search matches. If
    a web page contains ten forms, `s.findall('form')` will return a list containing
    the data for the ten forms. It will look as follows: `[<Form1 data>,<Form2 data>,
    <Form3 data> ....<Form10 data>]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In section **(4)** of the code, we are iterating over the list of forms that
    was returned before. The objective here is to identify the login form among multiple
    input forms that might be present on the web page. We also need to figure out
    the action URL of the login form, as that will be be the place where we will `POST`
    the valid credentials and set a valid session as shown in the following screenshots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/386c8822-761c-43e9-bfce-9ac845603173.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/917dce6b-b79d-4a19-bb25-c60251913528.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/bcad6b00-4d83-4be4-8761-17be78c8848e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s try to break down the preceding incomplete code to understand what has
    happened so far. Before we move on, however, let''s take a look at the user interface
    from where the crawling parameters are taken from the user. This will give us
    a good idea about the prerequisites and will help us to understand the code better.
    The following screen shows a representation of the user input parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc67b572-3501-45f3-b936-264b5b4bdfa3.png)'
  prefs: []
  type: TYPE_IMG
- en: As mentioned earlier, the crawler works in two iterations. In the first iteration,
    it tries to crawl the web application without authentication, and in the second
    iteration, it crawls the application with authentication. The authentication information
    is held in the `self.auth` variable, which by default is initialized to `false`.
    Therefore, the first iteration will always be without authentication.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that the purpose of the code mentioned before, which falls
    under the `< if self.auth ==True >` section, is to identify the login form from
    the login web page/URL. Once the login form is identified, the code tried to identify
    all the input fields of that form. It then formulates a data payload with legitimate
    user credentials to submit the login form. Once submitted, a valid user session
    will be returned and saved. That session will be used for the second iteration
    of crawling, which is authentication-based.
  prefs: []
  type: TYPE_NORMAL
- en: 'In section **(5)** of the code, we are invoking the `self.process_form_action()`
    method. Before that, we extract the action URL of the form, so we know where the
    data is to be *posted*. It also combines the relative action URL with the base
    URL of the application, so that we end up sending our request to a valid endpoint
    URL. For example, if the form action is pointing to a location called `/login`,
    and the current URL is `http://127.0.0.1/my_app`, this method will carry out the
    following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Check whether the URL is already added to a list of URLs that the crawler is
    supposed to visit
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine the action URL with the base context URL and return `http://127.0.0.1/my_app/login`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The definition of this method is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ca8d816-05a2-43b1-9257-04195a391ff2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As can be seen, the first thing that is invoked within this method is another
    method, `self.check_and_add_to_visit`. This method checks whether the URL in question
    has already been added to the list of URLs that the crawler is supposed to crawl.
    If it is added, then `no9` action is done. If not, the crawler adds the URL for
    it to revisit later. There are many other things that this method checks, such
    as whether the URL is in scope, whether the protocol is the one permitted, and
    so on. The definition of this method is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8eafa6cb-3962-458a-8816-7d6bc272cc1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As can be seen, if `self.already_seen()` under line 158 returns `false`, then
    a row is created in the backend database `Page` table under the current project.
    The row is created again via Django ORM (model abstraction). The `self.already_seen()` method simply
    checks the `Page` table to see whether the URL in question has been visited under
    the current project name and the current authentication mode by the crawler or
    not. This is verified with the visited `Flag`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a854a93-c9cc-4b7b-a1d5-974ab26aeae7.png)'
  prefs: []
  type: TYPE_IMG
- en: '`Page.objects.filter()` is equivalent to `select * from page where auth_visited=True/False
    and project=''current_project'' and URL=''current_url''`.'
  prefs: []
  type: TYPE_NORMAL
- en: In section **(6)** of the code, we are passing the content of the current form
    to a newly created instance of the BS parsing module. The reason for this is that
    we will parse and extract all the input fields from the form that we are currently
    processing. Once the input fields are extracted, we will compare the name of each
    input field with the name that is supplied by the user under `username_field` and
    `password_field`. The reason why we do this is that there might be occasions where
    there are multiple forms on the login page such as a search form, a sign up form,
    a feedback form, and a login form. We need to be able to identify which of these
    forms is the login form. As we are asking the user to provide the field name for
    **login username/email** and the field name for **Login-password**, our approach
    will be to extract the input fields from all forms and compare them with what
    the user has supplied. If we get a match for both the fields, we set `flag1` and
    `flag2` to `True`. If we get a match within a form, it is very likely that this
    is our login form. This is the form in which we will place our user supplied login
    credentials under the appropriate fields and then submit the form at the action
    URL, as specified under the action parameter. This logic is handled by sections
    **(7)**, **(8)**, **(9)**, **(10)**, **(11)**, **(12)**, **(13)**, and **(14)**.
  prefs: []
  type: TYPE_NORMAL
- en: There is another consideration that is important. There might be many occasions
    in which the login web page also has a signup form in it. Let's suppose that the
    user has specified `username` and `user_pass` as the field names for the username
    and password parameters for our code, to submit proper credentials under these
    field names to obtain a valid session. However, the signup form also contains
    another two fields, also called `username` and `user_pass`, and this also contains
    a few additional fields such as **Address**, **Phone**, **Email**, and so on.
    However, as discussed earlier, our code identifies the login form with these supplied
    field names only, and may end up considering the signup form as the login form.
    In order to address this, we are storing all the obtained forms in program lists.
    When all the forms are parsed and stored, we should have two probable candidates
    as login forms. We will compare the content length of both, and the one with a
    shorter length will be taken as the login form. This is because the signup form
    will usually have more fields than a login form. This condition is handled by
    section **(15)** of the code, which enumerates over all the probable forms and
    finally places the smallest one at index 0 of the `payloadforms[]` list and the `actionform[]`
    list.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in line 448, we post the supplied user credentials to the valid parsed
    login form. If the credentials are correct, a valid session will be returned and
    placed under a session variable, `ss`. The request is made by invoking the `POST`
    method as follows: `ss.post(action_forms[0],data=payload,cookie=cookie)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The user provides the start URL of the web application that is to be crawled.
    Section **(16)** takes that start URL and begins the crawling process. If there
    are multiple start URLs, they should be comma separated. The start URLs are added
    to the `Page()` database table as a URL that the crawler is supposed to visit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c24126aa-1811-425f-8e6f-3eb1393bb9ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In section **(17)**, there is a crawling loop that invokes a `there_are_pages_to_crawl()` method,
    which checks the backend `Page()` database table to see whether there are any
    pages for the current project with the visited flag `set = False`. If there are
    pages in the table that have not been visited by the crawler, this method will
    return `True`. As we just added the start page to the `Page` table in section **(16)**,
    this method will return `True` for the start page. The idea is to make a `GET` request
    on that page and extract all further links, forms, or URLs, and keep on adding
    them to the `Page` table. The loop will continue to execute as long as there are
    unvisited pages. Once the page is completely parsed and all links are extracted,
    the visited flag is `set=True` for that page or URL so that it will not be extracted
    to be crawled again. The definition of this method is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/648bf440-d99e-46be-bbbb-429276042251.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In section **(18)**, we get the unvisited page from the backend `Page` table by
    invoking the `get_a_page_to_visit()` method, the definition of which is given
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bdcd81c-aafd-4cc8-b4da-c6f87e7c81ce.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/4bdcd81c-aafd-4cc8-b4da-c6f87e7c81ce.png)'
  prefs: []
  type: TYPE_IMG
- en: In section **(19)**, we make a HTTP `GET` request to this page, along with the
    session cookies, `ss`, as section **(19)** belongs to the iteration that deals
    with `auth=True`. Once a request is made to this page, the response of the page
    is then further processed to extract more links. Before processing the response,
    we check for the response codes produced by the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are occasions where certain pages will return a redirection (`3XX` response
    codes) and we need to save the URLs and form content appropriately. Let''s say
    that we made a `GET` request to page X and in response we had three forms. Ideally,
    we will save those forms with the URL marked as X. However, let''s say that upon
    making a `GET` request on page X, we got a 302 redirection to page Y, and the
    response HTML actually belonged to the web page where the redirection was set.
    In that case, we will end up saving the response content of three forms mapped
    with the URL X, which is not correct. Therefore, in sections (20) and (21), we
    are handling these redirections and are mapping the response content with the
    appropriate URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bdcd81c-aafd-4cc8-b4da-c6f87e7c81ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Sections (22) and (23) do exactly what the previously mentioned sections (19),
    (20), and (21) do, but (22) and (23) do it for iterations where `authentication
    =False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b6fd524-92c5-423e-96aa-4c230d964ae6.png)'
  prefs: []
  type: TYPE_IMG
- en: If any exceptions are encountered while processing the current page, section
    (24) handles those exceptions, marks the visited flag of the current page as `True`,
    and puts an appropriate exception message in the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'If everything works smoothly, then control passes on to section (26), from
    where the processing of the HTML response content obtained from the `GET` request
    on the current page being visited begins. The objective of this processing is
    to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract all further links from the HTML response (`a href`, `base` tags, `Frame`
    tags, `iframe` tags)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract all forms from the HTML response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract all form fields from the HTML response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section **(26)** of the code extracts all the links and URLs that are present
    under the `base` tag (if any) of the returned HTML response content.
  prefs: []
  type: TYPE_NORMAL
- en: Sections **(27)** and **(28)** parses the content with the BS parsing module
    to extract all anchor tags and their `href` locations. Once extracted, they are
    passed to be added to the `Pages` database table for the crawler to visit later.
    It must be noted that the links are added only after checking they don't exist
    already under the current project and current authentication mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Section **(29)** parses the content with the BS parsing module to extract all
    `iframe` tags and their `src` locations. Once extracted, they are passed to be
    added to the `Pages` database table for the crawler to visit later. Section **(30)** does
    the same for frame tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da96a8bd-6587-478b-a449-c57060376a82.png)'
  prefs: []
  type: TYPE_IMG
- en: Section **(31)** parses the content with the BS parsing module to extract all
    option tags and checks whether they have a link under the `value` attribute. Once
    extracted, they are passed to be added to the `Pages` database table for the crawler
    to visit later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Section **(32)** of the code tries to explore all other options to extract
    any missed links from a web page. The following is the code snippet that checks
    for other possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7405799b-5114-4547-b285-ed249137023b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Sections **(33)** and **(34)** extract all the forms from the current HTML
    response. If any forms are identified, various attributes of the form tag, such
    as action or method, are extracted and saved under local variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/917d3776-7194-4557-b226-ef833763f39d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If any HTML form is identified, the next task is to extract all the input fields,
    text areas, select tags, option fields, hidden fields, and submit buttons. This
    is carried out by sections **(35)**, **(36)**, **(37)**, **(38)**, and **(39)**.
    Finally, all the extracted fields are placed under an `input_field_list` variable in
    a comma-separated manner. For example, let''s say a form, `Form1`, is identified
    with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<input type ="text" name="search">`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<input type="hidden" name ="secret">`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<input type="submit" name="submit_button>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All of these are extracted as `"Form1" : input_field_list = "search,text,secret,hidden,submit_button,submit"`**.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Section **(40)** of the code checks whether there are already any forms saved
    in the database table with the exact same content for the current project and
    current `auth_mode`. If no such form exists, the form is saved in the `Form` table,
    again with the help of the Django ORM (`models`) wrapper:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2ac4a7f-1ce5-4b73-8e65-93295d283408.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Section (41) of the previous code goes ahead and saves these unique forms in
    a JSON file with the name as the current project name. This file can then be parsed
    with a simple Python program to list various forms and input fields present in
    the web application that we crawled. Additionally, at the end of the code, we
    have a small snippet that places all discovered/crawled pages in a text file that
    we can refer to later. The snippet is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Section **(42)** of the code updates the visited flag of the web page whose
    content we just parsed and marks that as visited for the current `auth` mode.
    If any exceptions occur during saving, these are handled by section **(43)**,
    which again marks the visited flag as `true`, but additionally adds an exception
    message.
  prefs: []
  type: TYPE_NORMAL
- en: After sections **(42)** and **(43)**, the control goes back again to section
    **(17)** of the code. The next page that is yet to be visited by the crawler is
    taken from the database and all the operations are repeated. This continues until
    all web pages have been visited by the crawler.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we check whether the current iteration is with or without authentication
    in section (44). If it was without authentication, then the `start()` method of
    the crawler is invoked with the `auth` flag set to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: After both the iterations are successfully finished, the web application is
    assumed to be crawled completely and the project status is marked as **Finished** by
    section (45) of the code.
  prefs: []
  type: TYPE_NORMAL
- en: Execution of code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step we need to do is to convert the model classes into database
    tables. This can be done by executing the `syncdb()` command as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1842a8e9-b78d-44e2-bd63-7ef34f41cdff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the database tables are created, let''s start the Django server as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/067049a1-d5ed-40c7-a280-d8fe41064921.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will be testing our crawler against the famous DVWA application to see what
    it discovers. We need to start the Apache server and serve DVWA locally. The Apache
    server can be started by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s browse the Crawler interface and supply the scan parameters as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/326f3517-bc7e-443c-ad1d-1e96fe5377f0.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/ee6968ff-bcbe-4af0-8085-2e1d9c3246ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the **Start Crawling** button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f087f8b-e2a5-4e9c-bc82-e4bf95fbe61e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now browse the `results` folder of the app, which is at the `<Xtreme_InjectCrawler/results>` path, to
    see the URLs and forms discovered as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01f854c9-3497-4f8c-8024-b2ea90fdbbd2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s open the JSON file first to see the contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c68dfc8-4be7-4ffe-a668-8a77a2e187fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s open the `Pages_Dvwa_test` file to see the discovered URLs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c77b7d22-a667-4a17-a36c-f38777f48b2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It can therefore be verified that the crawler has successfully crawled the
    application and identified the links shown in the previous screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c9bb08e-c24f-4593-a6f1-7f5507a77805.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how we can write a custom crawler from scratch. This
    task is made easier using Python's modules, such as requests, BeautifulSoup, and
    so on. Feel free to download the whole code base and test the crawler with various
    other websites in order to examine its coverage. There may be occasions in which
    the crawler does not give 100% coverage. Take a look and see for yourself the
    limitations of the crawler and how it can be improved.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How can the crawler be improved to cover JavaScript and Ajax calls?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we use the crawler results to automate web application testing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Penetration Testing Automation Using Python and Kali Linux*: [https://www.dataquest.io/blog/web-scraping-tutorial-python/](https://www.dataquest.io/blog/web-scraping-tutorial-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Requests: HTTP for Humans*: [http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Django project*: [https://www.djangoproject.com/](https://www.djangoproject.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Penetration Testing Automation Using Python and Kali Linux*: [https://scrapy.org/](https://scrapy.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
