- en: Chapter 3. Kafka Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start getting our hands dirty by coding Kafka producers and consumers,
    let's quickly discuss the internal design of Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we shall be focusing on the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Kafka design fundamentals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Message compression in Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replication in Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the overheads associated with JMS and its various implementations and
    limitations with the scaling architecture, LinkedIn ([www.linkedin.com](http://www.linkedin.com))
    decided to build Kafka to address its need for monitoring activity stream data
    and operational metrics such as CPU, I/O usage, and request timings.
  prefs: []
  type: TYPE_NORMAL
- en: 'While developing Kafka, the main focus was to provide the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An API for producers and consumers to support custom implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low overheads for network and storage with message persistence on disk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A high throughput supporting millions of messages for both publishing and subscribing—for
    example, real-time log aggregation or data feeds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed and highly scalable architecture to handle low-latency delivery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auto-balancing multiple consumers in the case of failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guaranteed fault-tolerance in the case of server failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka design fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kafka is neither a queuing platform where messages are received by a single
    consumer out of the consumer pool, nor a publisher-subscriber platform where messages
    are published to all the consumers. In a very basic structure, a producer publishes
    messages to a Kafka topic (synonymous with "messaging queue"). A topic is also
    considered as a message category or feed name to which messages are published.
    Kafka topics are created on a Kafka broker acting as a Kafka server. Kafka brokers
    also store the messages if required. Consumers then subscribe to the Kafka topic
    (one or more) to get the messages. Here, brokers and consumers use Zookeeper to
    get the state information and to track message offsets, respectively. This is
    described in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kafka design fundamentals](img/3090OS_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding diagram, a single node—single broker architecture is shown
    with a topic having four partitions. In terms of the components, the preceding
    diagram shows all the five components of the Kafka cluster: Zookeeper, Broker,
    Topic, Producer, and Consumer.'
  prefs: []
  type: TYPE_NORMAL
- en: In Kafka topics, every partition is mapped to a logical log file that is represented
    as a set of segment files of equal sizes. Every partition is an ordered, immutable
    sequence of messages; each time a message is published to a partition, the broker
    appends the message to the last segment file. These segment files are flushed
    to disk after configurable numbers of messages have been published or after a
    certain amount of time has elapsed. Once the segment file is flushed, messages
    are made available to the consumers for consumption.
  prefs: []
  type: TYPE_NORMAL
- en: All the message partitions are assigned a unique sequential number called the
    *offset*, which is used to identify each message within the partition. Each partition
    is optionally replicated across a configurable number of servers for fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Each partition available on either of the servers acts as the *leader* and has
    zero or more servers acting as *followers*. Here the leader is responsible for
    handling all read and write requests for the partition while the followers asynchronously
    replicate data from the leader. Kafka dynamically maintains a set of **in-sync
    replicas** (**ISR**) that are caught-up to the leader and always persist the latest
    ISR set to ZooKeeper. In if the leader fails, one of the followers (in-sync replicas)
    will automatically become the new leader. In a Kafka cluster, each server plays
    a dual role; it acts as a leader for some of its partitions and also a follower
    for other partitions. This ensures the load balance within the Kafka cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka platform is built based on what has been learned from both the traditional
    platforms and has the concept of consumer groups. Here, each consumer is represented
    as a process and these processes are organized within groups called **consumer
    groups**.
  prefs: []
  type: TYPE_NORMAL
- en: A message within a topic is consumed by a single process (consumer) within the
    consumer group and, if the requirement is such that a single message is to be
    consumed by multiple consumers, all these consumers need to be kept in different
    consumer groups. Consumers always consume messages from a particular partition
    sequentially and also acknowledge the message offset. This acknowledgement implies
    that the consumer has consumed all prior messages. Consumers issue an asynchronous
    pull request containing the offset of the message to be consumed to the broker
    and get the buffer of bytes.
  prefs: []
  type: TYPE_NORMAL
- en: In line with Kafka's design, brokers are stateless, which means the message
    state of any consumed message is maintained within the message consumer, and the
    Kafka broker does not maintain a record of what is consumed by whom. If this is
    poorly implemented, the consumer ends up in reading the same message multiple
    times. If the message is deleted from the broker (as the broker doesn't know whether
    the message is consumed or not), Kafka defines the time-based SLA (service level
    agreement) as a message retention policy. In line with this policy, a message
    will be automatically deleted if it has been retained in the broker longer than
    the defined SLA period. This message retention policy empowers consumers to deliberately
    rewind to an old offset and re-consume data although, as with traditional messaging
    systems, this is a violation of the queuing contract with consumers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss the message delivery semantic Kafka provides between producer
    and consumer. There are multiple possible ways to deliver messages, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Messages are never redelivered but may be lost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Messages may be redelivered but never lost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Messages are delivered once and only once
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When publishing, a message is committed to the log. If a producer experiences
    a network error while publishing, it can never be sure if this error happened
    before or after the message was committed. Once committed, the message will not
    be lost as long as either of the brokers that replicate the partition to which
    this message was written remains available. For guaranteed message publishing,
    configurations such as getting acknowledgements and the waiting time for messages
    being committed are provided at the producer's end.
  prefs: []
  type: TYPE_NORMAL
- en: From the consumer point-of-view, replicas have exactly the same log with the
    same offsets, and the consumer controls its position in this log. For consumers,
    Kafka guarantees that the message will be delivered at least once by reading the
    messages, processing the messages, and finally saving their position. If the consumer
    process crashes after processing messages but before saving their position, another
    consumer process takes over the topic partition and may receive the first few
    messages, which are already processed.
  prefs: []
  type: TYPE_NORMAL
- en: Log compaction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Log compaction is a mechanism to achieve finer-grained, per-message retention,
    rather than coarser-grained, time-based retention. It ensures that the last known
    value for each message key within the log for a topic partition must be retained
    by removing the records where a more recent update with the same primary key is
    done. Log compaction also addresses system failure cases or system restarts, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Kafka cluster, the retention policy can be set on a per-topic basis
    such as time based, size-based, or log compaction-based. Log compaction ensures
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Ordering of messages is always maintained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The messages will have sequential offsets and the offset never changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reads progressing from offset 0, or the consumer progressing from the start
    of the log, will see at least the final state of all records in the order they
    were written
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log compaction is handled by a pool of background threads that recopy log segment
    files, removing records whose key appears in the head of the log.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following points summarize important Kafka design facts:'
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental backbone of Kafka is message caching and storing on the fiesystem.
    In Kafka, data is immediately written to the OS kernel page. Caching and flushing
    of data to the disk are configurable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka provides longer retention of messages even after consumption, allowing
    consumers to re-consume, if required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka uses a message set to group messages to allow lesser network overhead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unlike most messaging systems, where metadata of the consumed messages are
    kept at the server level, in Kafka the state of the consumed messages is maintained
    at the consumer level. This also addresses issues such as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Losing messages due to failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple deliveries of the same message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, consumers store the state in Zookeeper but Kafka also allows storing
    it within other storage systems used for **Online Transaction Processing** (**OLTP**)
    applications as well.
  prefs: []
  type: TYPE_NORMAL
- en: In Kafka, producers and consumers work on the traditional push-and-pull model,
    where producers push the message to a Kafka broker and consumers pull the message
    from the broker.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka does not have any concept of a master and treats all the brokers as peers.
    This approach facilitates addition and removal of a Kafka broker at any point,
    as the metadata of brokers are maintained in Zookeeper and shared with consumers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Producers also have an option to choose between asynchronous or synchronous
    mode to send messages to a broker.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Message compression in Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the cases where network bandwidth is a bottleneck, Kafka provides a message
    group compression feature for efficient message delivery. Kafka supports efficient
    compression by allowing recursive message sets where the compressed message may
    have infinite depth relative to messages within itself. Efficient compression
    requires compressing multiple messages together rather than compressing each message
    individually. A batch of messages is compressed together and sent to the broker.
    There is a reduced network overhead for the compressed message set and decompression
    also attracts very little additional overhead.
  prefs: []
  type: TYPE_NORMAL
- en: In an earlier version of Kafka, 0.7, compressed batches of messages remained
    compressed in the log files and were presented as a single message to the consumer
    who later decompressed it. Hence, the additional overhead of decompression was
    present only at the consumer's end.
  prefs: []
  type: TYPE_NORMAL
- en: In Kafka 0.8, changes were made to the broker in the way it handles message
    offsets; this may also cause a degradation in broker performance in the case of
    compressed messages.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Kafka 0.7, messages were addressable by physical byte offsets in the partition's
    log whereas in Kafka 0.8 each message is addressable by a non-comparable, increasingly
    logical offset that is unique per partition—that is, the first message has an
    offset of `1`, the tenth message has an offset of `10`, and so on. In Kafka 0.8,
    changes to offset management simplify the consumer capability to rewind the message
    offset.
  prefs: []
  type: TYPE_NORMAL
- en: In Kafka 0.8, the lead broker is responsible for serving the messages for a
    partition by assigning unique logical offsets to every message before it is appended
    to the logs. In the case of compressed data, the lead broker has to decompress
    the message set in order to assign offsets to the messages inside the compressed
    message set. Once offsets are assigned, the leader again compresses the data and
    then appends it to the disk. The lead broker follows this process for every compressed
    message sets it receives, which causes CPU load on a Kafka broker.
  prefs: []
  type: TYPE_NORMAL
- en: In Kafka, data is compressed by the message producer using either the **GZIP**
    or **Snappy** compression protocols. The following producer configurations need
    to be provided to use compression at the producer's end.
  prefs: []
  type: TYPE_NORMAL
- en: '| Property name | Description | Default value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `compression.codec` | This parameter specifies the compression codec for
    all data generated by this producer. Valid values are `none`, `gzip`, and `snappy`.
    | `none` |'
  prefs: []
  type: TYPE_TB
- en: '| `compressed.topics` | This parameter allows you to set whether compression
    should be turned on for particular topics. If the compression codec is anything
    other than `none`, enable compression only for specified topics, if any. If the
    list of compressed topics is empty, then enable the specified compression codec
    for all topics. If the compression codec is `none`, compression is disabled for
    all topics. | `null` |'
  prefs: []
  type: TYPE_TB
- en: The `ByteBufferMessageSet` class representing message sets may consist of both
    uncompressed as well as compressed data. To differentiate between compressed and
    uncompressed messages, a compression-attributes byte is introduced in the message
    header. Within this compression byte, the lowest two bits are used to represent
    the compression codec used for compression and the value 0 of these last two bits
    represents an uncompressed message.
  prefs: []
  type: TYPE_NORMAL
- en: Message compression techniques are very useful for mirroring data across datacenters
    using Kafka, where large amounts of data get transferred from active to passive
    datacenters in the compressed format.
  prefs: []
  type: TYPE_NORMAL
- en: Replication in Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we talk about replication in Kafka, let's talk about message partitioning.
    In Kafka, a message partitioning strategy is used at the Kafka broker end. The
    decision about how the message is partitioned is taken by the producer, and the
    broker stores the messages in the same order as they arrive. The number of partitions
    can be configured for each topic within the Kafka broker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka replication is one of the very important features introduced in Kafka
    0.8\. Though Kafka is highly scalable, for better durability of messages and high
    availability of Kafka clusters, replication guarantees that the message will be
    published and consumed even in the case of broker failure, which may be caused
    by any reason. Both producers and consumers are replication-aware in Kafka. The
    following diagram explains replication in Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Replication in Kafka](img/3090OS_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's discuss the preceding diagram in detail.
  prefs: []
  type: TYPE_NORMAL
- en: In replication, each partition of a message has *n* replicas and can afford
    *n-1* failures to guarantee message delivery. Out of the *n* replicas, one replica
    acts as the lead replica for the rest of the replicas. Zookeeper keeps the information
    about the lead replica and the current follower **in-sync replicas** (**ISR**).
    The lead replica maintains the list of all in-sync follower replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Each replica stores its part of the message in local logs and offsets, and is
    periodically synced to the disk. This process also ensures that either a message
    is written to all the replicas or to none of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka supports the following replication modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Synchronous replication**: In synchronous replication, a producer first identifies
    the lead replica from ZooKeeper and publishes the message. As soon as the message
    is published, it is written to the log of the lead replica and all the followers
    of the lead start pulling the message; by using a single channel, the order of
    messages is ensured. Each follower replica sends an acknowledgement to the lead
    replica once the message is written to its respective logs. Once replications
    are complete and all expected acknowledgements are received, the lead replica
    sends an acknowledgement to the producer. On the consumer''s side, all the pulling
    of messages is done from the lead replica.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Asynchronous replication**: The only difference in this mode is that, as
    soon as a lead replica writes the message to its local log, it sends the acknowledgement
    to the message client and does not wait for acknowledgements from follower replicas.
    But, as a downside, this mode does not ensure message delivery in case of a broker
    failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If any of the follower in-sync replicas fail, the leader drops the failed follower
    from its ISR list after the configured timeout period and writes will continue
    on the remaining replicas in ISRs. Whenever the failed follower comes back, it
    first truncates its log to the last checkpoint (the offset of the last committed
    message) and then starts to catch up with all messages from the leader, starting
    from the checkpoint. As soon as the follower becomes fully synced with the leader,
    the leader adds it back to the current ISR list.
  prefs: []
  type: TYPE_NORMAL
- en: If the lead replica fails, either while writing the message partition to its
    local log or before sending the acknowledgement to the message producer, a message
    partition is resent by the producer to the new lead broker.
  prefs: []
  type: TYPE_NORMAL
- en: The process of choosing the new lead replica involves all the followers' ISRs
    registering themselves with Zookeeper. The very first registered replica becomes
    the new lead replica and its **log end offset** (**LEO**) becomes the offset of
    the last committed message (also known as **high watermark** (**HW**)). The rest
    of the registered replicas become the followers of the newly elected leader. Each
    replica registers a listener in Zookeeper so that it will be informed of any leader
    change. Whenever the new leader is elected and the notified replica is not the
    leader, it truncates its log to the offset of the last committed message and then
    starts to catch up from the new leader. The new elected leader waits either until
    the time configured is passed or until all live replicas get in sync and then
    the leader writes the current ISR to Zookeeper and opens itself up for both message
    reads and writes.
  prefs: []
  type: TYPE_NORMAL
- en: Replication in Kafka ensures stronger durability and higher availability. It
    guarantees that any successfully published message will not be lost and will be
    consumed, even in the case of broker failures.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more insight on Kafka replication implementation, visit [https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3](https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned the design concepts used to build a solid foundation
    for Kafka. You also learned how message compression and replication are done in
    Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be focusing on how to write Kafka producers using
    the API provided.
  prefs: []
  type: TYPE_NORMAL
