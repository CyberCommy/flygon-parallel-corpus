- en: Text Wrangling and Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing NLTK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing sentence splitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing stemming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing lemmatization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying and removing stop words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating the frequency distribution of words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying and removing rare words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying and removing short words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing punctuation marks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Piecing together n-grams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scraping a job listing from StackOverflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading and cleaning the description in the job listCreating a word cloud from
    a StackOverflow job listing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mining the data is often the most interesting part of the job, and text is one
    of the most common data sources. We will be using the NLTK toolkit to introduce
    common natural language processing concepts and statistical models. Not only do
    we want to find quantitative data, such as numbers within data that we have scraped,
    we also want to be able to analyze various characteristics of textual information.
    This analysis of textual information is often lumped into a category known as
    natural language processing (NLP).  There exists a library for Python, NLTK, that
    provides rich capabilities.  We will investigate several of it's capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Installing NLTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe we learn to install NTLK, the natural language toolkit for Python.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The core of NLTK can be installed using pip:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Some processes, such as those we will use, require an additional download of
    various data sets that they use to perform various analyses. They can be downloaded
    by executing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'On a Mac, this actually pops up the following window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/75aae6dd-a071-42ea-a6be-8ccd5f961b95.png)The NTLK GUI'
  prefs: []
  type: TYPE_NORMAL
- en: Select install all and press the Download button.  The tools will begin to download
    a number of data sets.  This can take a while, so grab a coffee or beer and check
    back every now and then.  When completed, you are ready to progress to the next
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Performing sentence splitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many NLP processes require splitting a large amount of text into sentences. 
    This may seem to be a simple task, but for computers it can be problematic.  A
    simple sentence splitter can look just for periods (.), or use other algorithms
    such as predictive classifiers.  We will examine two means of sentence splitting
    with NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use a sentence stored in thee `07/sentence1.txt` file.  It has the
    following content, which was pulled from a random job listing on StackOverflow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are seeking developers with demonstrable experience in: ASP.NET, C#, SQL
    Server, and AngularJS. We are a fast-paced, highly iterative team that has to
    adapt quickly as our factory grows. We need people who are comfortable tackling
    new problems, innovating solutions, and interacting with every facet of the company
    on a daily basis. Creative, motivated, able to take responsibility and support
    the applications you create. Help us get rockets out the door faster!'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first example of sentence splitting is in the `07/01_sentence_splitting1.py`
    file.  This uses the built-in sentence splitter in NLTK, which uses an internal
    boundary detection algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First we import the sentence tokenizer from NLTK:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then load the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the sentence is split using `sent_tokenize`, and the sentences are reported:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to create your own tokenizer and train it yourself, then you can
    use the `PunktSentenceTokenizer` class.  `sent_tokenize` is actually a derived
    class of this class that implements sentence splitting in English by default. 
    But there are 17 different language models you can pick from:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can select the desired language by using the language parameter.  As an
    example, the following would split based on using German:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about this algorithm, you can read the source paper available
    at [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.5017&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.5017&rep=rep1&type=pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Performing tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tokenization is the process of converting text into tokens.  These tokens can
    be paragraphs, sentences, and common individual words, and are commonly based
    at the word level.  NLTK comes with a number of tokenizers that will be demonstrated
    in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for this example is in the `07/02_tokenize.py` file.  This extends
    the sentence splitter to demonstrate five different tokenization techniques. 
    The first sentence in the file will be the only one tokenized so that we keep
    the amount of output to a reasonable amount:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to simply use the built-in Python string `.split()` method. 
    This results in the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The sentence is split on space boundaries.  Note that punctuation such as ":"
    and "," are included in the resulting tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following demonstrates using the tokenizers built into NLTK.  First, we
    need to import them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following demonstrates using the `word_tokenizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The result now has also split the punctuation into their own tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following uses the regex tokenizer, which allows you to apply any regex
    expression as a tokenizer.  It uses a  `''\w+''`  regex and has the following
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `wordpunct_tokenizer` has the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'And `blankline_tokenize` produces the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen, this is not quite a simple problem as might be thought.  Depending
    upon the type of text being tokenized, you can come out with quite different results.
  prefs: []
  type: TYPE_NORMAL
- en: Performing stemming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stemming is the process of cutting down a token to its *stem*. Technically,
    it is the process or reducing inflected (and sometimes derived) words to their
    word stem - the base root form of the word.  As an example, the words *fishing*,
    *fished*, and *fisher* stem from the root word *fish*.  This helps to reduce the
    set of words being processed into a smaller base set that is more easily processed.
  prefs: []
  type: TYPE_NORMAL
- en: The most common algorithm for stemming was created by Martin Porter, and NLTK
    provides an implementation of this algorithm in the PorterStemmer.  NLTK also
    provides an implementation of a Snowball stemmer, which was also created by Porter,
    and designed to handle languages other than English.  There is one more implementation
    provided by NLTK referred to as a Lancaster stemmer. The Lancaster stemmer is
    considered the most aggressive stemmer of the three.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NLTK provides an implementation of the Porter stemming algorithm in its PorterStemmer
    class. An instance of this can easily be created by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The script in the `07/03_stemming.py` file applies the Porter and Lancaster
    stemmers to the first sentence of our input file.  The primary section of the
    code performing the stemming is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And this results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the results, it can be seen that the Lancaster stemmer is indeed
    more aggressive than the Porter stemmer, as several of the words have been cut
    down further with the latter stemmer.
  prefs: []
  type: TYPE_NORMAL
- en: Performing lemmatization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lemmatization is a more methodical process of converting words to their base. 
    Where stemming generally just chops off the ends of words, lemmatization takes
    into account the morphological analysis of words, evaluating the context and part
    of speech to determine the inflected form, and makes a decision between different
    rules to determine the root.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lemmatization can be utilized in NTLK using the `WordNetLemmatizer`. This class
    uses the WordNet service, an online semantic database to make its decisions.  The
    code in the `07/04_lemmatization.py` file extends the previous stemming example
    to also calculate the lemmatization of each word.  The code of importance is the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'And it results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: There is a small amount of variance in the results using the lemmatization process. 
    The point of this is that, depending upon your data, one of these may be more
    suitable for your needs than the other, so give all of them a try if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Determining and removing stop words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stop words are common words that, in a natural language processing situation,
    do not provide much contextual meaning.  These words are often the most common
    words in a language.  These tend to, at least in English, be articles and pronouns,
    such as *I*, *me*, *the*, *is*, *which*, *who*, *at*, among others.  Processing
    of meaning in documents can often be facilitated by removal of these words before
    processing, and hence many tools support this ability.  NLTK is one of these,
    and comes with support for stop word removal for roughly 22 languages.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Proceed with the recipe as follows (code is available in `07/06_freq_dist.py`):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following demonstrates stop word removal using NLTK.  First, start with
    importing stop words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then select the stop words for your desired language. The following selects
    English:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The English stop list has 153 words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s not too many that we can''t show them all here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The removal of stop words from a list of words can be performed easily with
    a simple python statement.  This is demonstrated in the `07/05_stopwords.py` file. 
    The script starts with the required imports and readies the sentence we want to
    process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following output, which we are familiar with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we tokenize that sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'With the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can remove tokens that are in the stop list with the following statements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stop word removal has its purposes.  It is helpful, as we will see in a later
    recipe where we create a word cloud (stop words don't give much information in
    a word cloud), but can also be detrimental.  Many other NLP processes that deduce
    meaning based upon sentence structure can be greatly hampered by their removal.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the frequency distributions of words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A frequency distribution counts the number of occurrences of distinct data values. 
    These are of value as we can use them to determine which words or phrases within
    a document are most common, and from that infer those that have greater or lesser
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Frequency distributions can be calculated using several different techniques. 
    We will examine them using the facilities built into NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NLTK provides a class, `ntlk.probabilities.FreqDist`, that allow us to very
    easily calculate the frequency distribution of values in a list.  Let''s examine
    using this class (code is in `07/freq_dist.py`):'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a frequency distribution using NLTK, start by importing the feature
    from NTLK (and also tokenizers and stop words):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can use the `FreqDist` function to create a frequency distribution
    given a list of words.  We will examine this by reading in the contents of `wotw.txt`
    (The War of the Worlds - courtesy of Gutenberg), tokenizing, and removing stop
    words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then calculate the frequency distribution of the remaining words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`freq_dist` is a dictionary of words to the counts of those words.  The following
    prints all of them (only a few lines of output shown as there are thousands of
    unique words):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the frequency distribution to identify the most common words. The
    following reports the 10 most common words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: I was hoping that martians was in the top 5\. It's number 4.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also use this to identify the least common words, by slicing the result
    of `.most_common()` with a negative value.  As an example, the following finds
    the 10 least common words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'There are quite a few words with only one occurrence, so this only gets a subset
    of those values.  The number of words with only one occurrence can be determined
    by the following (truncated due to there being 3,224 words):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Identifying and removing rare words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can remove words with low occurences by leveraging the ability to find words
    with low frequency counts, that fall outside of a certain deviation of the norm,
    or just from a list of words considered to be rare within the given domain.  But
    the technique we will use works the same for either.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rare words can be removed by building a list of those rare words and then removing
    them from the set of tokens being processed.  The list of rare words can be determined
    by using the frequency distribution provided by NTLK. Then you decide what threshold
    should be used as a rare word threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The script in the `07/07_rare_words.py` file extends that of the frequency
    distribution recipe to identify words with two or fewer occurrences and then removes
    those words from the tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Through these two steps, removing stop words and then words with 2 or fewer
    occurrences, we have moved the total number of words from 6,613 to 2,252, which 
    is roughly one third.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying and removing rare words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Removal of short words can also be useful in removing noise words from the content. 
    The following examines removing words of a certain length or shorter.  It also
    demonstrates the opposite by selecting the words not considered short (having
    a length of more than the specified short word length).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can leverage the frequency distribution from NLTK to efficiently calculate
    the short words.  We could just scan all of the words in the source, but it is
    simply more efficient to scan the lengths of all of the keys in the resulting
    distribution as it will be a significantly smaller set of data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The script in the `07/08_short_words.py` file exemplifies this process.  It
    starts by loading the content of `wotw.txt` and then calculating the word frequency
    distribution (after short word removal).  Then it identifies the words of thee
    characters or less:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The words not considered short can be found by changing the logic operator
    in the list comprehension:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'And results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Removing punctuation marks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending upon the tokenizer used, and the input to those tokenizers, it may
    be desired to remove punctuation from the resulting list of tokens.  The `regexp_tokenize`
    function with `'\w+'` as the expression removes punctuation well, but `word_tokenize`
    does not do it very well and will return many punctuation marks as their own tokens.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Removing punctuation marks from our tokens is done similarly to the removal
    of other words within our tokens by using a list comprehension and only selecting
    those items that are not punctuation marks. The script  `07/09_remove_punctuation.py`
    file demonstrates this.  Let''s walk through the process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start with the following, which will `word_tokenize` a string from a
    job listing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can remove the punctuation with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This process can be encapsulated in a function.  The following is in the `07/punctuation.py`
    file, and will remove punctuation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Removal of punctuation and symbols can be a difficult problem.  While they
    don''t add value to many searches, punctuation can also be required to be kept
    as part of a token.  Take the case of searching a job site and trying to find
    C# programming positions, such as in the example in this recipe. The tokenization
    of C# gets split into two tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We actually have two problems here.  By having C and # separated, we lost knowledge
    of C# being in the source content.  And then if we removed the # from the tokens,
    then we lose that information as we also cannot reconstruct C# from adjacent tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: Piecing together n-grams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Much has been written about NLTK being used to identify n-grams within text. 
    An n-gram is a set of words, *n* words in length, that are common within a document/corpus
    (occurring 2 or more times).  A 2-gram is any two words commonly repeated, a 3-gram
    is a three word phrase, and so on.  We will not look into determining the n-grams
    in a document.  We will focus on reconstructing known n-grams from our token streams,
    as we will consider those n-grams to be more important to a search result than
    the 2 or 3 independent words found in any order.
  prefs: []
  type: TYPE_NORMAL
- en: In the domain of parsing job listings, important 2-grams can be things such
    as  **Computer Science**, **SQL Server**, **Data Science**, and **Big Data**. 
    Additionally, we could consider C# a 2-gram of `'C'` and `'#'`, and hence why
    we might not want to use the regex parser or `'#'` as punctuation when processing
    a job listing.
  prefs: []
  type: TYPE_NORMAL
- en: We need to have a strategy to recognize these known combinations from out token
    stream.  Let's look at how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, this example does not intend to make an exhaustive examination or one
    that is optimally performant.  Just one that is simple to understand and can be
    easily applied and extended to our example of parsing job listings:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will examine this process using the following sentences from a `StackOverflow`
    job listing for SpaceX:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*We are seeking developers with demonstrable experience in: ASP.NET, C#, SQL
    Server, and AngularJS. We are a fast-paced, highly iterative team that has to
    adapt quickly as our factory grows.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of high value 2-grams in these two sentences (and I think
    job listings are a great place to look for 2-grams).  Just looking at it, I can
    pick out the following as being important:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ASP.NET
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: C#
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL Server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fast-paced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: highly iterative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: adapt quickly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: demonstrable experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, while these may not be 2-grams in the technical definition, when we parse
    them, they will all be separated into independent tokens.  This can be shown in
    the `07/10-ngrams.py` file, and in the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We want to remove punctuation from this set, but we would like to do it after
    constructing some 2-grams, specifically so that we can piece "C#" back into a
    single token.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script in the `07/10-reconstruct-2grams.py` file demonstrates a function
    to facilitate this.  First, we need to describe the 2-grams that we want to reconstruct. 
    In this file, they are defined as the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '`grams` is a dictionary, where the keys specify the `"Left"` side of the 2-gram. 
    Each key has a list of dictionaries, where each dictionary key can be the right
    side of the 2-gram, and the value is a string that will be placed between the
    left and right.'
  prefs: []
  type: TYPE_NORMAL
- en: With this definition, we are able to see `"C"` and  `"#"` in our tokens be reconstructed
    to "C#".  `"SQL"` and `"Server"` will be `"SQL Server"`.  `"fast"` and  `"paced"`
    will result in `"faced-paced"`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So we just need a function to make this all work.  This function is defined
    in the `07/buildgrams.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This function, given a set of tokens and a dictionary in the format described
    earlier, will return a revised set of tokens with any matching 2-grams put into
    a single token.  The following demonstrates some simple cases of its use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s apply it to our input.  The complete script for this is in the `07/10-reconstruct-2grams.py`
    file (and adds a few 2-grams):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Perfect!
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are providing a dictionary to the `build_2grams()` function that defines
    rules for identifying 2-grams.  In this example, we predefined these 2-grams. 
    It is possible to use NLTK to find 2-grams (and n-grams in general), but with
    this small sample of one job positing, it's likely that none will be found.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping a job listing from StackOverflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's pull a bit of this together to scrape information from a StackOverflow
    job listing.  We are going to look at just one listing at this time so that we
    can learn the structure of these pages and pull information from them.  In later
    chapters, we will look at aggregating results from multiple listings.  Let's now
    just learn how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: StackOverflow actually makes it quite easy to scrape data from their pages. 
    We are going to use content from a posting at[ https://stackoverflow.com/jobs/122517/spacex-enterprise-software-engineer-full-stack-spacex?so=p&sec=True&pg=1&offset=22&cl=Amazon%3b+](https://stackoverflow.com/jobs/122517/spacex-enterprise-software-engineer-full-stack-spacex?so=p&sec=True&pg=1&offset=22&cl=Amazon%3b+). 
    This likely will not be available at the time you read it, so I've included the
    HTML of this page in the `07/spacex-job-listing.html` file, which we will use
    for the examples in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'StackOverflow job listings pages are very structured.  It''s probably because
    they''re created by programmers and for programmers.  The page (at the time of
    writing) looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dfb46fdc-eb94-4b80-93a8-885f9ce8a756.png)A StackOverflow job listing'
  prefs: []
  type: TYPE_NORMAL
- en: 'All of this information is codified within the HTML of the page.  You can see
    for yourself by analyzing the page content.  But what StackOverflow does that
    is so great is that it puts much of its page data in an embedded JSON object. 
    This is placed within a `<script type="application/ld+json">` HTML tag, so it''s
    really easy to find.  The following shows a truncated section of this tag (the
    description is truncated, but all the tags are shown):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5fecad0b-3acf-4cb6-90d9-4da452fd469b.png)The JSON embedded in a
    job listing'
  prefs: []
  type: TYPE_NORMAL
- en: This makes it very easy to get the content, as we can simply retrieve the page,
    find this tag, and then convert this JSON into a Python object with the `json`
    library.  In addition to the actual job description, is also included  much of
    the "metadata" of the job posting, such as skills, industries, benefits, and location
    information.  We don't need to search the HTML for the information - just find
    this tag and load the JSON.  Note that if we want to find items, such as Job Responsibilities**,**
    we still need to parse the description.  Also note that the description contains
    full HTML, so when parsing that, we would need to still deal with HTML tags.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's go and get the job description from this page.  We will simply retrieve
    the contents in this recipe.  We will clean it up in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code for this example is in the `07/12_scrape_job_stackoverflow.py`
    file.  Let''s walk through it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First we read the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we load the content into a `BeautifulSoup` object, and retrieve the `<script
    type="application/ld+json">` tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have that tag, we can load its contents into a Python dictionary
    using the `json` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this looks like the following (this is truncated for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This is great because we can now do some simple tasks with this without involving
    HTML parsing.  As an example, we can retrieve the skills required for the job
    with just the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The description is still stored in HTML within the description property of this
    JSON object.  We will examine the parsing of that data in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Reading and cleaning the description in the job listing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The description of the job listing is still in HTML.  We will want to extract
    the valuable content out of this data, so we will need to parse this HTML and
    perform tokenization, stop word removal, common word removal, do some tech 2-gram
    processing, and in general all of those different processes.  Let's look at doing
    these.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have collapsed the code for determining tech-based 2-grams into the `07/tech2grams.py`
    file.  We will use the `tech_2grams` function within the file.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for this example is in the `07/13_clean_jd.py` file.  It continues
    on where the `07/12_scrape_job_stackoverflow.py` file ends:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by creating a `BeautifulSoup` object from the description key of the
    description we loaded.  We will also print this to see what it looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to go through this and remove all of the HTML and only be left with
    the text of the description.  That will be what we then tokenize.  Fortunately,
    throwing out all the HTML tags is easy with `BeautifulSoup`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Just super!  We now have this, and it is already broken down into what can be
    considered sentences!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s join these all together, word tokenize them, get rid of stop words,
    and also apply common tech job 2-grams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'And this has the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: I think that's a very nice and refined set of keywords pulled out of that job
    listing.
  prefs: []
  type: TYPE_NORMAL
