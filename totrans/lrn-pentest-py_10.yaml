- en: Chapter 10. Adding Permanency to Python Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python has enormous capabilities, and we have only scratched the surface of
    the tools and techniques available for us as assessors. We are going to cover
    a few of the more advanced features of the Python language that can be helpful
    to us. Specifically, we are going to highlight how we can build logging into our
    scripts and then develop multithreaded and multiprocessing tools. Adding in these
    more advanced capabilities means that the tools you develop will be more resilient
    to the test of time and stand apart from other solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding logging within Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you write your own modules, such as the one highlighted in [Chapter 9](ch09.html
    "Chapter 9. Automating Reports and Tasks with Python"), *Automating Reports and
    Tasks with Python*, you would want to be able to track errors, warnings, and debug
    messages easily. The logger library allows you to track events and output them
    to **Standard Error** (**STDERR**), files, and **Standard Output** (**STDOUT**).
    The benefit to using logger is that the format can be easily defined and sent
    to the relevant output using specific message types. The messages are similar
    to syslog messages, and they mimic the same logging levels.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More details about the logger library can be found at [https://docs.python.org/2/library/logging.html](https://docs.python.org/2/library/logging.html).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the difference between multithreading and multiprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two different ways in which simultaneous requests can be executed
    within Python: multithreading and multiprocessing. Often, these two items are
    confused with each other, and when you read about them, you will see similar responses
    on blogs and newsgroups. If you are speaking about using multiple processors and
    processing cores, you are talking about multiprocessing. If you are staying within
    the same memory block but not using multiple cores or processes, then you are
    talking about multithreading. Multithreading, in turn, runs concurrent code but
    does not execute tasks in parallel due to the Python interpreter''s design.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you review [Chapter 8](ch08.html "Chapter 8. Exploit Development with Python,
    Metasploit, and Immunity"), *Exploit Development with Python, Metasploit, and
    Immunity*, and look at the defined areas of the Windows memory, you will gain
    a better understanding of how threads and processes work within the Windows memory
    structure. Keep in mind that the manner in which other **Operating Systems** (**OS**)
    handle these memory locations is different.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a multithreaded script in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand the limitations of multithreading, you have to understand the
    Python interpreter. The Python interpreter uses a **Global Interpreter Lock**
    (**GIL**), which means that when byte code is executed by a thread, it is done
    by a thread at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To better understand GIL, view the documentation at [https://docs.python.org/2/glossary.html#term-global-interpreter-lock](https://docs.python.org/2/glossary.html#term-global-interpreter-lock).
  prefs: []
  type: TYPE_NORMAL
- en: This prevents problems related to data structure manipulation by more than one
    thread at a time. Think about data being written to a dictionary and you referencing
    different pieces of data by the same key in concurrent threads. You would clobber
    some of the data that you intended to write to the dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For multithreaded Python applications, you will hear a term called **thread
    safe**. This means, "Can something be modified by a thread without impacting the
    integrity or availability of the data or not?" Even if something is not considered
    **thread safe**, you can use locks, which is described later, to control the data
    entry as necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to use the `head_request.py` script we previously created in [Chapter
    6](ch06.html "Chapter 6. Assessing Web Applications with Python"), *Assessing
    Web Applications with Python*, and we are going to mature it as a new script.
    This script will use a queue to hold all the tasks that need to be processed,
    which will be assigned dynamically during execution. This queue is built by reading
    values from a file and storing them for later processing. We will incorporate
    the new logger library to output the details to a `results.log` file as the script
    executes. The following screenshot shows the results of this new script after
    execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a multithreaded script in Python](img/B04315_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Additionally, the following highlighted log file contains the detailed execution
    of the script and the concurrent thread''s output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a multithreaded script in Python](img/B04315_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This script can be found at [https://raw.githubusercontent.com/funkandwagnalls/pythonpentest/master/multi_threaded.py](https://raw.githubusercontent.com/funkandwagnalls/pythonpentest/master/multi_threaded.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, with the goal in sight, we begin with what libraries need to be imported
    and configure two global variables. The first variable holds our queued workload,
    and the second is used to lock the thread for a moment so that data can be printed
    on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Remember the following: concurrent processing means that items are processed.
    The details are provided as executed, and displaying this can come out garbled
    at the console. To combat this, we use a lock to pause the execution sufficiently
    to return the necessary details. The logger is a thread-safe library, but print
    is not and other libraries may not be either. As such, use locks where appropriate.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we need to create the class that will spawn threads, with the only
    new constructor concept being `threading.Thread.__init__(self)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to create a function that will process the actual data in each
    of these threads. The function starts off by defining the initial values, and
    as you can see, these values are extracted from the queue. They represent an **Internet
    Protocol** (**IP**) address that was loaded into the queue from a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we are going to process both insecure and secure versions of the
    host''s potential websites. The following code, which is for the insecure portion
    of the website, does a job similar to the script highlighted in [Chapter 6](ch06.html
    "Chapter 6. Assessing Web Applications with Python"), *Assessing Web Applications
    with Python*. The only difference is that we have added the new logger functions
    to print the details to a results log file. As you can see in following code,
    writing the details to the logger is almost identical to writing a print statement.
    You will also notice that we have used the `with` statement to lock the thread
    processes so that the details can be printed. This is not necessary for I**/O**,
    but it would be difficult to read otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The secure portion of the request-response instructions is almost identical
    to the non-secure portion of the code, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, this function lists the task that was provided as done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As highlighted before, the arguments and options are configured very similarly
    to other scripts. So, for the sake of brevity, these have been omitted, but they
    can be found in the aforementioned link. What has changed, however, is the configuration
    of the logger. We set up a variable that can have a log file''s name passed by
    argument. We then configure the logger so that it is at the appropriate level
    for outputting to a file, and the format stamps the output of the thread to include
    the time, thread name, logging level, and actual message. Finally, we configure
    the object that will be used as a reference for all logging operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'With the logger all set up, we can actually set up the final lines of code
    necessary to make the script multithreaded. We load all the targets into a list
    from the file, then parse the list into the queue. We could have done this a little
    tighter, but the following format is easier to read. We then generate workers
    and set `setDaemon` to `True` so that the script terminates after the main thread
    completes, which prevents the script from hanging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The preceding details create a functional multithreaded Python script, but there
    are problems. Python multithreading is very error-prone. Even with a well-written
    script, you can have different errors returned on each iteration. Additionally,
    it takes a significant amount of code to accomplish relatively minute tasks, as
    shown in the preceding code. Finally, depending on the situation and the OS that
    your script is being executed on, threading may not improve the processing performance.
    Another solution is to use multiprocessing instead of multithreading, which is
    easier to code, is less error-prone, and (again) can use more than one core or
    processor.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python has a number of libraries that can support concurrency to make coding
    easier. As an example, handling URLs with currency can be done with simple-requests
    ([http://pythonhosted.org/simple-requests/](http://pythonhosted.org/simple-requests/)),
    which has been built at [http://www.gevent.org/](http://www.gevent.org/). The
    preceding code example was for showing how a concurrent script can be modified
    to include multithreaded support. When maturing a script, you should see whether
    other libraries can enable better functionality directly so as to improve your
    personal knowledge and create scripts that remain relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a multiprocessing script in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before getting into creating a multiprocessing script in Python, you should
    understand the pitfalls that most people run into. This will help you in the future
    as you attempt to mature your tool sets. There are four major issues that you
    will run into with multiprocessing scripts in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: Serialization of objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel writing or reading of data and dealing with locks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating system nuances with relevant parallelism **Application Program Interfaces**
    (**APIs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translation of a current script (threaded or unthreaded script) into a script
    that takes advantage of parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When writing a multiprocessing script in Python, the biggest hurdle is dealing
    with serialization (known as pickling) and deserialization (known as unpickling)
    of objects. When you are writing your own code related to multiprocessing, you
    may see reference errors to the pickle library. This means that you have run into
    an issue related to the way your data is being serialized.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some objects in Python cannot be serialized, so you have to find ways around
    that. The most common way that you will see referenced is by using the `copy_reg`
    library. This library provides a means of defining functions so that they can
    be serialized.
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, just like concurrent code, writing and reading of data to
    a singular file or some other **Input/Output** (**I/O**) resource will cause issues.
    This is because each core or processor is crunch data at the same time, and for
    the most part, this is handled without the other processes being aware of it.
    So, if you are writing code that needs to output the details, you can lock the
    processes so that the details can be handled appropriately. This capability is
    handled through the use of the `multiprocessing.Lock()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Besides I/O, there is also an additional problem of shared memory used between
    processes. Since these processes run relatively independently (depending on the
    implementation), malleable data that would be referenced in memory can be problematic.
    Thankfully, the `multiprocessing` library provides a number of tools to help us.
    The basic solution is to use `multiprocessing.Values()` and `multiprocessing.Arrays()`,
    which can be shared across processes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Additional details about shared memory and multiprocessing can be found at [https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.sharedctypes](https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.sharedctypes).
  prefs: []
  type: TYPE_NORMAL
- en: All OSes are not created equal when it comes to process and memory management.
    Understanding how these different operating systems work at these levels is necessary
    for system engineers and developers alike. As assessors, we have the same need
    when developing more advanced tools and creating exploits, as previously highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: Think about how many times you see a new tool or script come out of and it has
    only been tested on one OS or distribution; when you use it, the product does
    not work elsewhere. Multiprocessing scripts are no different, and when you are
    writing these scripts, keep the final goal in mind. If you have no intention of
    making your script run anywhere other than on Kali, then make sure you test there.
    If you are going to run it on Windows, you need to verify that the same method
    of script design works there as well. Specifically, the entry point for the multiprocessing
    code needs to be within the `main()` function or, in essence, below the check
    to see whether `__name__` is equal to `'__main__':`. If it is not, you may be
    creating a fork bomb, or an infinite loop of spawning processes that eventually
    crashes the system.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To gain a better understanding of Windows' restrictions on the forking of processes
    and Python multiprocessing, you can refer to [https://docs.python.org/2/library/multiprocessing.html#windows](https://docs.python.org/2/library/multiprocessing.html#windows).
  prefs: []
  type: TYPE_NORMAL
- en: The final consideration is the translation of established scripts into multiprocessing
    scripts. Though there are a large number of demos on the Internet that show a
    user taking a threaded or nonthreaded script and translating it into a multiprocessing
    script, they are usually good for demos only. Translating functional code into
    a multiprocessing script that is both stable and useful typically requires rewriting.
    This is because of the points noted earlier, which highlight the challenges you
    will have to overcome.
  prefs: []
  type: TYPE_NORMAL
- en: So what did you learn from all this?
  prefs: []
  type: TYPE_NORMAL
- en: The function that will be executed in parallel must be pickable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locks may need to be incorporated while dealing with I/O, and shared memory
    requires specific functions from the multiprocessing library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main entry point to parallel processes needs to be protected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scripts do not easily translate from threaded or unthreaded formats to multiprocessing
    formats, and as such, some thought should go into redesigning them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The details of the arguments and options have been removed for brevity, but
    the full details can be found at [https://raw.githubusercontent.com/funkandwagnalls/pythonpentest/master/multi_process.py](https://raw.githubusercontent.com/funkandwagnalls/pythonpentest/master/multi_process.py).
  prefs: []
  type: TYPE_NORMAL
- en: With all of this in mind, we can now rewrite the `head_request.py` script so
    as to accommodate multiple multiprocessing. The `run()` function's code is largely
    rewritten in order to accommodate the objects so that they can be pickled. This
    is because the `host_request` function is what is run by each subprocess. The
    `urllib2` request and responses are objects that are not picklable, and as such,
    the data needs to be converted to a string prior to passing. Additionally, with
    multiprocessing scripts, a logger has to be handled instead of being called directly.
    In this way, the subprocesses know what to write to, using a universal filename
    reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'This format prevents the file from being written to at the same time by multiple
    processes. To begin with, we create a timestamp, which will be used for reference
    when the log handler is grabbed. The following code highlights the configuration
    of the initial values and the insecure service request and response instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Following the insecure request and response instructions are the secure service
    request and response instructions, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After the request and response details have been captured, the details are
    returned and logged appropriately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned earlier, the logger uses a handler and we accomplish this by creating
    a function that defines the logger''s design. This function will then be called
    by each subprocess using the `initializer` parameter within `multiprocessing.map`.
    This means that we have full control over the logger across processes, and this
    prevents problems with unpickable objects requiring to be passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, with all of these details in the `main()` function, we define the **Command-line
    Interface** (**CLI**) for the arguments and options. Then we generate the data
    that will be tested from the target''s file and the argument variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the following code uses the `map` function, which calls the `host_request`
    function as it iterates through the list of targets. The `map` function allows
    a multiprocessing script to queue work in a manner similar to the previous multithreaded
    script. We can then use the processes variable loaded by the CLI argument to define
    the number of subprocesses to spawn, which allows us to dynamically control the
    number of processes that are forked. This is a very much guess-and-check method
    of process control.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you wanted to be more specific, another manner would be to determine the
    number of CPU and double it to determine the number of processes. This could be
    accomplished as follows: `processes = multiprocessing.cpu_count() *2`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'With the code generated, we can output the help file to decide how the script
    needs to be run, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a multiprocessing script in Python](img/B04315_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When the script is run, the output itemizes the request successes, failures,
    and relevant processes, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a multiprocessing script in Python](img/B04315_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the `results.log` file contains the details related to the activity
    produced by the script as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a multiprocessing script in Python](img/B04315_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We have now finished our multiprocessing script, which can handle logging in
    a controlled manner. This is the step in the right direction for creating industry-standard
    tools. With additional time, we could attach this script to the `nmap_parser.py`
    script that we created in the last chapter and even generate detailed reports
    using the `nmap_doc_generator.py` script as an example. The combination of these
    capabilities would make the tool even more useful.
  prefs: []
  type: TYPE_NORMAL
- en: Building industry-standard tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python is a fantastic language and these advanced techniques, which highlight
    controlling threads, processes, I/O, and logging, are pivotal to adding permanency
    to your scripts. There are a number of examples in the industry that help assess
    security, such as Sulley. This is a tool that automates the fuzzing of applications
    in an effort to help identify security weaknesses, the results of which can later
    be used to write Frameworks such as Metasploit. Other tools help harden security
    by improving a code base, such as **Open Web Application Security Project's**
    (**OWASP**) Python Security Project. These are examples of tools that started
    out to fit a missing need and gained strong followings. These tools are mentioned
    here as to highlight what your tools could become with the right focus.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you develop your own tools, keep in mind what your goals are, start small,
    and add capabilities. This will help you make the project manageable and successful,
    and the little rewards related to small successes will push you to engage in bigger
    innovations. Finally, never fear starting over. Many times, code will lead you
    in the right direction once you realize that the manner in which you were doing
    something may not be the right fit.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From [Chapter 2](ch02.html "Chapter 2. The Basics of Python Scripting"), *The
    Basics of Python Scripting* to [Chapter 10](ch10.html "Chapter 10. Adding Permanency
    to Python Tools"), *Adding Permanency to Python Tools*, we highlighted incremental
    ways of improving penetration testing scripts. This organic growth of knowledge
    showed how to improve code to meet the evaluation needs of today's environments.
    It also highlighted the fact that there are specific places where scripts fit
    the need that an assessor has, and that there are established tools or projects
    currently in place that can do the intended task. In this chapter, we witnessed
    a culmination of the previous examples to develop tools that are able run concurrent
    code and parallel processes, effectively logging data all the while. I hope you
    have enjoyed this read as much as I have enjoyed writing it.
  prefs: []
  type: TYPE_NORMAL
