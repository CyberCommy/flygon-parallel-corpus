- en: Running Kubernetes on Multiple Clouds and Cluster Federation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we''ll take it to the next level, and look at running Kubernetes
    on multiple clouds and cluster federation. A Kubernetes cluster is a close-knit
    unit in which all the components run in relative proximity and are connected by
    a fast network (a physical data center or cloud provider availability zone). This
    is great for many use cases, but there are several important use cases where systems
    need to scale beyond a single cluster. Kubernetes federation is a methodical way
    to combine multiple Kubernetes clusters and interact with them as a single entity.
    The topics we will cover include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A deep dive into what cluster federation is all about
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to prepare, configure, and manage a cluster federation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to run a federated workload across multiple clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding cluster federation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster federation is conceptually simple. You aggregate multiple Kubernetes
    clusters and treat them as a single logical cluster. There is a federation control
    plane that presents to clients a single unified view of the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates the big picture of Kubernetes cluster federation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/20ec9f25-71e9-4c28-b26c-bbb521b2dd32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The federation control plane consists of a federation API server and a federation
    controller manager that collaborate. The federated API server forwards requests
    to all the clusters in the federation. In addition, the federated controller manager
    performs the duties of the controller manager across all clusters by routing requests
    to the individual federation cluster members'' changes. In practice, cluster federation
    is not trivial and can''t be totally abstracted away. Cross-pod communication
    and data transfer may suddenly incur a massive latency and cost overhead. Let''s
    look at the use cases for cluster federation first, understand how the federated
    components and resources work, and then examine the hard parts: location affinity,
    cross-cluster scheduling, and federated data access.'
  prefs: []
  type: TYPE_NORMAL
- en: Important use cases for cluster federation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are four categories of use cases that benefit from cluster federation.
  prefs: []
  type: TYPE_NORMAL
- en: Capacity overflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The public cloud platforms, such as AWS, GCE, and Azure, are great and provide
    many benefits, but they are not cheap. Many large organizations have invested
    a lot in their own data centers. Other organizations work with private service
    providers such as OVS, Rackspace, or Digital Ocean. If you have the operational
    capacity to manage and operate infrastructure on your own, it makes a lot of economic
    sense to run your Kubernetes cluster on your infrastructure rather than in the
    cloud. But what if some of your workloads fluctuate and require a lot more capacity
    for a relatively short amount of time?
  prefs: []
  type: TYPE_NORMAL
- en: For example, your system may be hit especially hard on the weekends or maybe
    during holidays. The traditional approach is to just provision extra capacity.
    But in many dynamic situations, it is not easy. With capacity overflow, you can
    run the bulk of your work in a Kubernetes cluster running on an on-premise data
    center or with a private service provider and have a secondary cloud-based Kubernetes
    cluster running on one of the big platform providers. Most of the time, the cloud-based
    cluster will be shut down (stopped instances), but when the need arises you can
    elastically add capacity to your system by starting some stopped instances. Kubernetes
    cluster federation can make this configuration relatively straightforward. It
    eliminates a lot of headaches about capacity planning and paying for hardware
    that's not used most of the time.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is sometimes called **cloud bursting**.
  prefs: []
  type: TYPE_NORMAL
- en: Sensitive workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is almost the opposite of capacity overflow. Maybe you've embraced the
    cloud-native lifestyle and your entire system runs on the cloud, but some data
    or workloads deal with sensitive information. Regulatory compliance or your organization's
    security policies may dictate that the data and workloads must run in an environment
    that's fully controlled by you. Your sensitive data and workloads may be subject
    to external auditing. It may be critical to ensure no information ever leaks from
    the private Kubernetes cluster to the cloud-based Kubernetes cluster. But it may
    be desirable to have visibility into the public cluster and the ability to launch
    non-sensitive workloads from the private cluster to the cloud-based cluster. If
    the nature of a workload can change dynamically from non-sensitive to sensitive
    then it needs to be addressed by coming up with a proper policy and implementation.
    For example, you may prevent workloads from changing their nature. Alternatively,
    you may migrate a workload that suddenly became sensitive and ensure that it doesn't
    run on the cloud-based cluster anymore. Another important instance is national
    compliance, where certain data is required by law to remain in and be accessed
    only from a designated geographical region (typically a country). In this case,
    a cluster must be created in that geographical region.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding vendor lock-in
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large organizations often prefer to have options and not be tied to a single
    provider. The risk is often too great, because the provider may shut down or be
    unable to provide the same level of service. Having multiple providers is often
    good for negotiating prices, too. Kubernetes is designed to be vendor-agnostic.
    You can run it on different cloud platforms, private service providers, and on-premises
    data centers.
  prefs: []
  type: TYPE_NORMAL
- en: However, this is not trivial. If you want to be sure that you are able to switch
    providers quickly or shift some workloads from one provider to the next, you should
    already be running your system on multiple providers. You can do it yourself or
    there are some companies that provide the service of running Kubernetes transparently
    on multiple providers. Since different providers run different data centers, you
    automatically get some redundancy and protection from vendor-wide outages.
  prefs: []
  type: TYPE_NORMAL
- en: Geo-distributing high availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: High availability means that a service will remain available to users even when
    some parts of the system fail. In the context of a federated Kubernetes cluster,
    the scope of failure is an entire cluster, which is typically due to problems
    with the physical data center hosting the cluster, or perhaps a wider issue with
    the platform provider. The key to high availability is redundancy. Geo-distributed
    redundancy means having multiple clusters running in different locations. It may
    be different availability zones of the same cloud provider, different regions
    of the same cloud provider, or even different cloud providers altogether (see
    the *Avoiding vendor lock-in* section). There are many issues to address when
    it comes to running a cluster federation with redundancy. We'll discuss some of
    these issues later. Assuming that the technical and organizational issues have
    been resolved, high availability will allow the switching of traffic from a failed
    cluster to another cluster. This should be transparent to the users up to a point
    (delay during switchover, and some in-flight requests or tasks may disappear or
    fail). The system administrators may need to take extra steps to support the switchover
    and to deal with the original cluster failure.
  prefs: []
  type: TYPE_NORMAL
- en: The federation control plane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The federation control plane consists of two components that together enable
    a federation of Kubernetes clusters to appear and function as a single unified
    Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Federation API server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The federation API server is managing the Kubernetes clusters that together
    comprise the federation. It manages the federation state (that is, which clusters
    are part of the federation) in an `etcd` database the same as a regular Kubernetes
    cluster, but the state it keeps is just which clusters are members of the federation.
    The state of each cluster is stored in the `etcd` database of that cluster. The
    main purpose of the federation API server is to interact with the federation controller
    manager and route requests to the federation member clusters. The federation members
    don''t need to know they are part of a federation: they work in just the same
    way.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates the relationships between the federation
    API server, the federation replication controllers, and the Kubernetes clusters
    in the federation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/e89605d4-6f49-4385-87e2-6bf824bfb661.png)'
  prefs: []
  type: TYPE_IMG
- en: Federation controller manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The federation controller manager makes sure the federation''s desired state
    matches the actual state. It forwards any necessary changes to the relevant cluster
    or clusters. The federated controller manager binary contains multiple controllers
    for all the different federated resources we''ll cover later in the chapter. The
    control logic is similar, though: it observes changes and brings the clusters''
    states to the desired state when they deviate. This is done for each member in
    the cluster federation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates this perpetual control loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/6c1f2b1b-0bfc-4849-939c-3659eb76d1ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Federated resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes federation is still a work in progress. As of Kubernetes 1.10, only
    some of the standard resources can be federated. We'll cover them here. To create
    a federated resource, you use the `--context=federation-cluster` command-line
    argument from Kubectl. When you use `--context=federation-cluster`, the command
    goes to the federation API server, which takes care of sending it to all the member
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Federated ConfigMap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Federated ConfigMaps are very useful because they help centralize the configuration
    of applications that may be spread across multiple clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a federated ConfigMap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is an example of creating a federated ConfigMap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the only difference when creating a ConfigMap in a single Kubernetes
    cluster is the context. When a federated ConfigMap is created, it is stored in
    the control plane's `etcd` database, but a copy is also stored in each member
    cluster. This way, each cluster can operate independently and doesn't need to
    access the control plane.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing a federated ConfigMap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can view a ConfigMap by accessing the control plane or by accessing a member
    cluster. To access a ConfigMap in a member cluster, specify the federation cluster
    member name in the context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Updating a federated ConfigMap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's important to note that, when created through the control plane, the ConfigMap
    will be identical across all member clusters. However, since it is stored separately
    in each cluster in addition to the control plane cluster, there is no single source
    of `true`. It is possible (although not recommended) to later modify the ConfigMap
    of each member cluster independently. That leads to non-uniform configuration
    across the federation. There are valid use cases for different configurations
    for different clusters in the federation, but in those cases I suggest just configuring
    each cluster directly. When you create a federated ConfigMap, you make a statement
    that means whole clusters should share this configuration. However, you would
    usually want to update the ConfigMap across all the federation clusters by specifying
    `--context=federation-cluster.`
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a federated ConfigMap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'That''s right, you guessed it. You delete as usual, but specify the context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'There is just one little twist. As of Kubernetes 1.10, when you delete a federated
    ConfigMap, the individual ConfigMaps that were created automatically in each cluster
    remain. You must delete them separately in each cluster. That is, if you have
    three clusters in your federation called `cluster-1`, `cluster-2`, and `cluster-3`,
    you''ll have to run these extra three commands to get rid of the ConfigMap across
    the federation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This will be rectified in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Federated DaemonSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A federated DaemonSet is pretty much the same as a regular Kubernetes DaemonSet.
    You create it and interact with it through the control plane (by specifying `--context=federation-cluster`),
    and the control plane propagates it to all the member clusters. At the end of
    the day, you can be sure that your Daemons run on every node in every cluster
    of the federation.
  prefs: []
  type: TYPE_NORMAL
- en: Federated Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Federated deployments are a little smarter. When you create a federated deployment
    with X replicas and you have *N* clusters, the replicas will be distributed evenly
    between the clusters by default. If you have 3 clusters and the federated deployment
    has 15 pods, then each cluster will run 5 replicas. As with other federated resources,
    the control plane will store the federated deployment with 15 replicas and then
    create 3 deployments (1 for each cluster) with 5 replicas each. You can control
    the number of replicas per cluster by adding an annotation: `federation.kubernetes.io/deployment-preferences`.
    Federated deployment is still in Alpha as of Kubernetes 1.10\. In the future,
    the annotation will become a proper field in federated deployment configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: Federated Events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Federated events are different than the other federated resources. They are
    only stored in the control plane and are not propagated to the underlying Kubernetes
    member clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can query the federation events with `--context=federation-cluster` as
    usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Federated Horizontal Pod Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Federated **Horizontal Pod Scaling** (**HPA**) was added recently as an Alpha
    feature in Kubernetes 1.9\. In order to use it, you must provide the following
    flag when starting the API server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It is a major feature because one of the primary motivations of cluster federation
    is fluidly shifting workloads across multiple clusters without manual intervention.
    Federated HPA utilizes the in-cluster HPA controllers. The federated HPA evenly
    distributes the load across member clusters based on the requested max and min
    replicas. In the future, users will be able to specify more advanced HPA policies.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider a federation with 4 clusters; we want to ensure there
    are always at least 6 pods and at most 16 pods running. The following manifest
    will get the work done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following command to initiate the federated HPA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: What happens now? The federation control plane will create a standard HPA in
    each of the 4 clusters with a maximum of 4 replicas and a minimum of 2 replicas.
    The reason is that this is the setting that most economically accomplishes the
    federated requirement. Let's understand why. If each cluster has at most 4 replicas,
    then we'll have at most 4 x 4 = 16 replicas, which meets our requirements. The
    guarantee of at least 2 replicas means that we will have at least 4 x 2 = 8 replicas.
    This meets the requirement that we will have at least 6 replicas. Note that even
    if there is zero load on the system, we will always have at least 8 replicas even
    though we specified that 6 is fine. There is no way around it, given the constraint
    of even distribution across clusters. If the cluster HPAs had `minReplicas=1`
    then the total number of replicas in the cluster could be 4 x 1 = 4, which is
    less than the federated minimum of 6 that's required. In the future, users may
    be able to specify more sophisticated distribution schemes.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible, to use cluster selectors (introduced in Kubernetes 1.7) to restrict
    federated object to a subset of members. So, if we want to a minimum of 6 and
    maximum of 15, it is possible to evenly distribute it among 3 clusters instead
    of 4, and each cluster will have a minimum of 2 and maximum of 5.
  prefs: []
  type: TYPE_NORMAL
- en: Federated ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The federated ingress does more than just create matching ingress objects in
    each cluster. One of the main features of federated ingress is that if a whole
    cluster goes down it can direct traffic to other clusters. As of Kubernetes 1.4,
    federated ingress is supported on Google Cloud Platform, both on GKE and GCE.
    In the future, hybrid cloud support for federated ingress will be added.
  prefs: []
  type: TYPE_NORMAL
- en: 'The federated ingress performs the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Creates Kubernetes ingress objects in each cluster member of the federation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides a one-stop logical L7 load balancer with a single IP address for all
    the cluster ingress objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitors the health and capacity of the service backend pods behind the ingress
    object in each cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makes sure to route client connections to a healthy service endpoint in the
    face of various failures, such as the failure of pod, cluster, availability zone,
    or a whole region, as long as there is one healthy cluster in the federation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating federated ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You create federated ingress by addressing the federation control plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The federation control plane will create the corresponding ingress in each
    cluster. All the clusters will share the same namespace and name for the `ingress`
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Request routing with federated ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The federated ingress controller will route requests to the closest cluster.
    Ingress objects expose one or more IP address (through the `Status.Loadbalancer.Ingress`
    field) that remain static for the lifetime of the ingress object. When an internal
    or external client connects to an IP address of a cluster-specific ingress object,
    it will be routed to one of the pods in that cluster. However, when a client connects
    to the IP address of a federated ingress object it will be automatically routed,
    via the shortest network path, to a healthy pod in the closest cluster to the
    origin of the request. So, for example, HTTP(S) requests from internet users in
    Europe will be routed directly to the closest cluster in Europe that has available
    capacity. If there are no such clusters in Europe, the request will be routed
    to the next closest cluster (often in the US).
  prefs: []
  type: TYPE_NORMAL
- en: Handling failures with federated ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two broad categories of failure:'
  prefs: []
  type: TYPE_NORMAL
- en: Pod failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods might fail for many reasons. In a properly configured Kubernetes cluster
    (a cluster federation member or not), pods will be managed by services and ReplicaSets
    that can automatically handle pod failures. It shouldn't impact cross-cluster
    routing and load balancing done by the federated ingress. A whole cluster might
    fail due to problems with the data center or global connectivity. In this case,
    the federated services and federated ReplicaSets will ensure that the other clusters
    in the federation run enough pods to handle the workload, and the federated ingress
    will take care of routing client requests away from the failed cluster. To benefit
    from this auto-healing capability, clients must always connect to the federation
    ingress object and not to individual cluster members.
  prefs: []
  type: TYPE_NORMAL
- en: Federated job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Federated jobs work similarly to in-cluster jobs. The federation control plane
    creates jobs in the underlying clusters and evenly divides the load as far as
    the parallelism of tasks and keeping track of completions is concerned. For example,
    if the federation has 4 clusters and you create a federated job spec with parallelism=8
    and completions=24, then a job will be created in each cluster with parallelism=2
    and completions=6.
  prefs: []
  type: TYPE_NORMAL
- en: Federated namespace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes namespaces are used within a cluster to isolate independent areas
    and support multi-tenant deployments. Federated namespaces provide the same capabilities
    across a cluster federation. The API is identical. When a client is accessing
    the federation control plane, they will only get access to the namespaces they
    requested and are authorized to access across all the clusters in the federation.
  prefs: []
  type: TYPE_NORMAL
- en: 'You use the same commands and add `--context=federation-cluster`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Federated ReplicaSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is best to use deployments and federated deployments to manage the replicas
    in your cluster or federation. However, if for some reason you prefer to work
    directly with ReplicaSets, then Kubernetes supports a federated `ReplicaSet`.
    There is no federated replication controller because ReplicaSets supersede replication
    controllers.
  prefs: []
  type: TYPE_NORMAL
- en: When you create federated ReplicaSets, the job of the control plane is to ensure
    that the number of replicas across the cluster matches your federated ReplicaSets
    configuration. The control plane will create a regular ReplicaSet in each federation
    member. Each cluster will get, by default, an equal (or as close to equal as possible)
    number of replicas so that the total will add up to the specified number of replicas.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can control the number of replicas per cluster by using the following annotation:
    `federation.kubernetes.io/replica-set-preferences`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The corresponding data structure is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If `Rebalance` is `true`, then running replicas may be moved between clusters
    as necessary. The clusters map determines the ReplicaSets preferences per cluster.
    If `*` is specified as the key, then all unspecified clusters will use that set
    of preferences. If there is no `*` entry, then replicas will only run on clusters
    that show up in the map. Clusters that belong to the federation but don't have
    an entry will not have pods scheduled (for that pod template).
  prefs: []
  type: TYPE_NORMAL
- en: 'The individual ReplicaSets preferences per cluster are specified using the
    following data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`MinReplicas` is `0` by default. `MaxReplicas` is unbounded by default. Weight
    expresses the preference to add an additional replica to this ReplicaSets and
    defaults to `0`.'
  prefs: []
  type: TYPE_NORMAL
- en: Federated secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Federated secrets are simple. When you create a federated secret as usual through
    the control plane it gets propagated to the whole cluster. That's it.
  prefs: []
  type: TYPE_NORMAL
- en: The hard parts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, federation seems almost straightforward. You group a bunch of clusters
    together, access them through the control plane, and everything just gets replicated
    to all the clusters. But there are hard and difficult factors and basic concepts
    that complicate this simplified view. Much of the power of Kubernetes is derived
    from its ability to do a lot of work behind the scenes. Within a single cluster
    deployed fully in a single physical data center or availability zone, where all
    the components are connected to a fast network, Kubernetes is very effective on
    its own. In a Kubernetes cluster federation, the situation is different. Latency,
    data transfer costs, and moving pods between clusters all have different trade-offs.
    Depending on the use case, making federation work may require extra attention,
    planning, and maintenance on the part of the system designers and operators. In
    addition, some of the federated resources are not as mature as their local counterparts,
    and that adds more uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: Federated unit of work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The unit of work in a Kubernetes cluster is the pod. You can't break a pod in
    Kubernetes. The entire pod will always be deployed together and will be subject
    to the same life cycle treatment. Should the pod remain the unit of work for a
    cluster federation? Maybe it makes more sense to be able to associate a bigger
    unit, such as a whole ReplicaSet, deployment, or service with a specific cluster.
    If the cluster fails, the entire ReplicaSet, deployment, or service is scheduled
    to a different cluster. How about a collection of tightly coupled ReplicaSets?
    The answers to these questions are not always easy and may even change dynamically
    as the system evolves.
  prefs: []
  type: TYPE_NORMAL
- en: Location affinity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Location affinity is a major concern. When can pods be distributed across clusters?
    What is the relationships between those pods? Are there any requirements for affinity
    between pods or pods and other resources, such as storage? There are several major
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Strictly-coupled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loosely-coupled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preferentially-coupled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strictly-decoupled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uniformly-spread
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When designing the system and how to allocate and schedule services and pods
    across the federation, it's important to make sure the location affinity requirements
    are always respected.
  prefs: []
  type: TYPE_NORMAL
- en: Strictly-coupled
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The strictly-coupled requirement applies to applications where the pods must
    be in the same cluster. If you partition the pods, the application will fail (perhaps
    due to real-time requirements that can't be met when networking across clusters)
    or the cost may be too high (pods may be accessing a lot of local data). The only
    way to move such tightly coupled applications to another cluster is to start a
    complete copy (including data) on another cluster and then shut down the application
    on the current cluster. If the amount of data is too large, the application may
    practically be immovable and sensitive to catastrophic failure. This is the most
    difficult situation to deal with, and if possible you should architect your system
    to avoid the strictly-coupled requirement.
  prefs: []
  type: TYPE_NORMAL
- en: Loosely-coupled
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Loosely-coupled applications are best when the workload is embarrassingly parallel
    and each pod doesn't need to know about the other pods or access a lot of data.
    In these situations, pods can be scheduled to clusters just based on capacity
    and resource utilization across the federation. If necessary, pods can be moved
    from one cluster to another without problems. For example, a stateless validation
    service that performs some calculation and gets all its input in the request itself
    and doesn't query or write any federation-wide data. It just validates its input
    and returns a valid/invalid verdict to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: Preferentially-coupled
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preferentially-coupled applications perform better when all the pods are in
    the same cluster or the pods and the data are co-located, but it is not a hard
    requirement. For example, it could work with applications that require only eventual
    consistency, where some federation-wide application periodically synchronizes
    the application state across all clusters. In these cases, allocation is done
    explicitly to one cluster, but leaves a safety hatch for running or migrating
    to other clusters under stress.
  prefs: []
  type: TYPE_NORMAL
- en: Strictly-decoupled
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some services have fault isolation or high availability requirements that force
    partitioning across clusters. There is no point running three replicas of a critical
    service if all replicas might end up scheduled to the same cluster, because that
    cluster just becomes an ad hoc **single point of failure** (**SPOF**).
  prefs: []
  type: TYPE_NORMAL
- en: Uniformly-spread
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Uniformly-spread is when an instance of a service, ReplicaSet, or pod must run
    on each cluster. It is similar to DaemonSet, but instead of ensuring there is
    one instance on each node, it's one per cluster. A good example is a Redis cache
    backed up by some external persistent storage. The pods in each cluster should
    have their own cluster-local Redis cache to avoid accessing the central storage
    that may be slower or become a bottleneck. On the other hand, there is no need
    for more than one Redis service per cluster (it could be distributed across several
    pods in the same cluster).
  prefs: []
  type: TYPE_NORMAL
- en: Cross-cluster scheduling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-cluster scheduling goes hand in hand with location affinity. When a new
    pod is created or an existing pod fails and a replacement needs to be scheduled,
    where should it go? The current cluster federation doesn't handle all the scenarios
    and options for location affinity we mentioned earlier. At this point, cluster
    federation handles the loosely-coupled (including weighted distribution) and strictly-coupled
    (by making sure the number of replicas matches the number of clusters) categories
    well. Anything else will require that you don't use cluster federation. You'll
    have to add your own custom federation layer that takes more specialized concerns
    into account and can accommodate more intricate scheduling use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Federated data access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is a tough problem. If you have a lot of data and pods running in multiple
    clusters (possibly on different continents) and need to access it quickly, then
    you have several unpleasant options:'
  prefs: []
  type: TYPE_NORMAL
- en: Replicate your data to each cluster (slow to replicate, expensive to transfer,
    expensive to store, and complicated to sync and deal with errors)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access the data remotely (slow to access, expensive on each access, and can
    be a SPOF)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a sophisticated hybrid solution with per-cluster caching of some of the
    hottest data (complicated/stale data, and you still need to transfer a lot of
    data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Federated auto-scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is currently no support for federated auto-calling. There are two dimensions
    of scaling that can be utilized, as well as a combination:'
  prefs: []
  type: TYPE_NORMAL
- en: Per-cluster scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding/removing clusters from the federation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider the relatively simple scenario of a loosely coupled application running
    on three clusters with five pods in each cluster. At some point, 15 pods can't
    handle the load anymore. We need to add more capacity. We can increase the number
    of pods per cluster, but if we do it at the federation level than we will have
    six pods running in each cluster. We've increased the federation capacity by three
    pods, when only one pod is needed. Of course, if you have more clusters the problem
    gets worse. Another option is to pick a cluster and just change its capacity.
    This is possible with annotations, but now we're explicitly managing capacity
    across the federation. It can get complicated very quickly if we have lots of
    clusters running hundreds of services with dynamically changing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a whole new cluster is even more complicated. Where should we add the
    new cluster? There is no requirement for extra availability that can guide the
    decision. It is just about extra capacity. Creating a new cluster also often requires
    complicated first-time setup and it may take days to approve various quotas on
    public cloud platforms. The hybrid approach increases the capacity of existing
    clusters in the federation until some threshold is reached and then starts adding
    new clusters. The benefit of this approach is that when you're getting closer
    to your capacity limit per cluster you start preparing new clusters that will
    be ready to go when necessary. Other than that, it requires a lot of effort and
    you pay in increased complexity for the flexibility and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Managing a Kubernetes cluster federation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing a Kubernetes cluster federation involves many activities above and
    beyond managing a single cluster. There are two ways to set up the federation.
    Then, you need to consider cascading resource deletion, load balancing across
    clusters, fail over across clusters, federated service discovery, and federated
    discovery. Let's go over each one in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up cluster federation from the ground up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Note: this approach is now deprecated in favor of using `Kubefed`. I describe
    it here for the benefit of readers using older versions of Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up a Kubernetes cluster federation, we need to run the components of
    the control plane, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the easiest ways to do that is to use the all-in-one hyperkube image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/kubernetes/kubernetes/tree/master/cluster/images/hyperkube](https://github.com/kubernetes/kubernetes/tree/master/cluster/images/hyperkube)'
  prefs: []
  type: TYPE_NORMAL
- en: The federation API server and the federation controller manager can be run as
    pods in an existing Kubernetes cluster, but as discussed earlier, it is better
    from a fault tolerance and high availability point of view to run them in their
    own cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Initial setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, you must have Docker running and get a Kubernetes release that contains
    the scripts we will use in this guide. The current release is 1.5.3\. You can
    download the latest available version instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to create a directory for the federation config files and set the `FEDERATION_OUTPUT_ROOT`
    environment variable to that directory. For easy cleanup, it''s best to create
    a new directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can initialize the federation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Using the official Hyperkube image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As part of every Kubernetes release, official release images are pushed to `gcr.io/google_containers`.
    To use the images in this repository, you can set the container image fields in
    the config files in `${FEDERATION_OUTPUT_ROOT}` to point to the `gcr.io/google_containers/hyperkube`
    image, which includes both the `federation-apiserver` and `federation-controller-manager`
    binaries.
  prefs: []
  type: TYPE_NORMAL
- en: Running the federation control plane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''re ready to deploy the federation control plane by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The command will launch the control plane components as pods and create a service
    of type `LoadBalancer` for the federation API server and a persistent volume claim
    backed up by a dynamic persistent volume for `etcd`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify everything was created correctly in the federation namespace, type
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You can also check your `kubeconfig` file for new entries using the Kubectl
    config view. Note that dynamic provisioning works only for AWS and GCE at the
    moment.
  prefs: []
  type: TYPE_NORMAL
- en: Registering Kubernetes clusters with the federation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To register a cluster with the federation, we need a secret to talk to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Let's create the secret in the host Kubernetes cluster. Suppose `kubeconfig`
    of the target cluster is at `|cluster-1|kubeconfig`. You can run the following
    command
  prefs: []
  type: TYPE_NORMAL
- en: 'to create the `secret`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration for the cluster looks the same as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We need to set `<client-cidr>`, `<apiserver-address>`, and `<secret-name>`.
    `<secret-name>` here is name of the secret that you just created. `serverAddressByClientCIDRs`
    contains the various server addresses that clients can use as per their CIDR.
    We can set the server's public IP address with `CIDR 0.0.0.0/0`, which all clients
    will match. In addition, if you want internal clients to use the server's `clusterIP`,
    you can set that as `serverAddress`. The client CIDR in that case will be a CIDR
    that only matches the IPs of pods running in that cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s register the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see if the cluster has been registered properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Updating KubeDNS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The cluster is registered with the federation. It''s time to update `kube-dns`
    so that your cluster can route federation service requests. As of Kubernetes 1.5
    or later, it''s done by passing the `--federations` flag to `kube-dns` through
    the `kube-dns ConfigMap` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what the `ConfigMap` looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Replace the `federation-name` and the `federation-domain-name` with the correct
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Shutting down the federation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to shut down the federation, just run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Setting up cluster federation with Kubefed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes 1.5 introduced a new Alpha command-line tool called `Kubefed` to
    help you administrate your federated clusters. The job of `Kubefed` is to make
    it easy to deploy a new Kubernetes cluster federation control plane, and to add
    or remove clusters from an existing federation control plane. It has been in beta
    since Kubernetes 1.6.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Kubefed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Until Kubernetes 1.9, Kubefed was part of the Kubernetes client binaries. You''ll
    get Kubectl and Kubefed. Here are the instructions for downloading and installing
    on Linux:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Make the necessary adjustments if you''re using a different OS or want to install
    a different version. Since Kubernetes 1.9 Kubefed has been available in the dedicated
    federation repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You can install Kubectl separately following the instructions here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Choosing a host cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The federation control plane can be its own dedicated cluster or hosted with
    an existing cluster. You need to make this decision. The host cluster hosts the
    components that make up your federation control plane. Ensure that you have a
    `kubeconfig` entry in your local `kubeconfig` that corresponds to the host cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify that you have the required `kubeconfig` entry, type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The context name, `cluster-1`, will be provided later when deploying the federation
    control plane.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a federation control plane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s time to start using Kubefed. The `kubefed init` command requires three
    arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: The federation name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The host cluster context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A domain name suffix for your federated services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following example command deploys a federation control plane with the
  prefs: []
  type: TYPE_NORMAL
- en: 'name federation; a host cluster context, `cluster-1`; a coredns DNS provider
    (`google-clouddns` and `aes-route53` are also valid); and the domain suffix, `kubernetes-ftw.com`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The DNS suffix should be for a DNS domain you manage, of course.
  prefs: []
  type: TYPE_NORMAL
- en: '`kubefed init` sets up the federation control plane in the host cluster and
    adds an entry for the federation API server in your local `kubeconfig`. Kubernetes
    might not create the default namespace due to a bug. In this case, you''ll have
    to do it yourself. Type the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Don''t forget to set the current context to federation, so that Kubectl targets
    the federation control plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Federated service discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Federated service discovery is tightly coupled with federated load balancing.
    A pragmatic setup includes a global L7 load balancer that distributes requests
    to federated ingress objects in the federation clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of this approach is that the control stays with the Kubernetes federation,
    which over time will able to work with more cluster types (currently just AWS
    and GCE) and understand cluster utilization and other constraints.
  prefs: []
  type: TYPE_NORMAL
- en: The alternative of having a dedicated lookup service and let clients connect
    directly to services on individual clusters loses all these benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a cluster to a federation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the control plane has been deployed successfully, we should add some Kubernetes
    clusters to the federation. Kubefed provides the `join` command exactly for this
    purpose. The `kubefed join` command requires the following arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: The name of the cluster to add
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Host cluster context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, to add a new cluster called `cluster-2` to the federation, type
  prefs: []
  type: TYPE_NORMAL
- en: 'the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Naming rules and customization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cluster name you supply to `kubefed join` must be a valid RFC 1035 label.
    RFC 1035 allows only letters, digits, and hyphens, and the label must start with
    a letter.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the federation control plane requires the credentials of the joined
    clusters to operate on them. These credentials are obtained from the local `kubeconfig`.
    The `Kubefed join` command uses the cluster name specified as the argument to
    look for the cluster's context in the local `kubeconfig`. If it fails to find
    a matching context, it exits with an error.
  prefs: []
  type: TYPE_NORMAL
- en: 'This might cause issues in cases where context names for each cluster in the
    federation don''t follow RFC 1035 label naming rules. In such cases, you can specify
    a cluster name that conforms to the RFC 1035 label naming rules and specify the
    cluster context using the `--cluster-context` flag. For example, if the context
    of the cluster you are joining is `cluster-3` (an underscore is not allowed),
    you can join the cluster by running this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Secret name
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster credentials required by the federation control plane as described in
    the previous section are stored as a secret in the host cluster. The name of the
    secret is also derived from the cluster name.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the name of a `secret` object in Kubernetes should conform to the
    DNS subdomain name specification described in RFC 1123\. If this isn''t the case,
    you can pass the `secret name` to `kubefed join` using the `--secret-name` flag.
    For example, if the cluster name is `cluster-4` and the `secret name` is `4secret`
    (starting with a letter is not allowed), you can join the cluster by running this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The `kubefed join` command automatically creates the secret for you.
  prefs: []
  type: TYPE_NORMAL
- en: Removing a cluster from a federation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To remove a cluster from a federation, run the `kubefed unjoin` command with
    the cluster name and the federation''s host cluster context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Shutting down the federation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Proper cleanup of the federation control plane is not fully implemented in
    this beta release of Kubefed. However, for the time being, deleting the federation
    system namespace should remove all the resources except the persistent storage
    volume dynamically provisioned for the federation control plane''s `etcd`. You
    can `delete` the federation namespace by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Cascading delete of resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubernetes cluster federation often manages a federated object in the control
    plane, as well as corresponding objects in each member Kubernetes cluster. A cascading
    delete of a federated object means that the corresponding objects in the member
    Kubernetes clusters will also be deleted.
  prefs: []
  type: TYPE_NORMAL
- en: 'This doesn''t happen automatically. By default, only the federation control
    plane object is deleted. To activate cascading delete, you need to set the following
    option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In Kuberentes 1.5, only the following federated objects supported cascading
    delete:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DaemonSets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Namespaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReplicaSets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secrets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For other objects, you have to go into each cluster and delete them explicitly.
    Fortunately, starting from Kubernetes 1.6, all the federated objects support cascading
    delete.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing across multiple clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dynamic load balancing across clusters is not trivial. The simplest solution
    is to just say that it is not Kubernetes's responsibility. Load balancing will
    be performed outside the Kubernetes cluster federation. But given the dynamic
    nature of Kubernetes, even an external load balancer will have to gather a lot
    of information about which services and backend pods are running on each cluster.
    An alternative solution is for the federation control plane to implement an L7
    load balancer that serves as traffic director for the entire federation. In one
    of the simpler use cases, each service runs on a dedicated cluster and the load
    balancer simply routes all traffic to that cluster. In case of cluster failure,
    the service is migrated to a different cluster and the load balancer now routes
    all traffic to the new cluster. This provides a coarse fail over and high-availability
    solution at the cluster level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimal solution will be able to support federated services and take into
    account additional factors, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Geo-location of client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource utilization of each cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource quotas and auto-scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows how an L7 load balancer on GCE distributes client
    requests to the closest cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/1add4f66-0a86-4972-a9b5-cc887d731c95.png)'
  prefs: []
  type: TYPE_IMG
- en: Failing over across multiple clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Federated failover is tricky. Suppose a cluster in the federation fails; one
    option is to just have other clusters pick up the slack. Now, the question is,
    how do you distribute the load across other clusters?
  prefs: []
  type: TYPE_NORMAL
- en: Uniformly?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launch a new cluster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pick an existing cluster as close to this one as possible (maybe in the same
    region)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these solutions has subtle interactions with federated load balancing,
  prefs: []
  type: TYPE_NORMAL
- en: geo-distributed high availability, cost management across different clusters,
  prefs: []
  type: TYPE_NORMAL
- en: and security.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the failed cluster comes back online. Should it gradually take over its
    original workload again? What if it comes back but with reduced capacity or sketchy
    networking? There are many combinations of failure modes that could make recovery
    complicated.
  prefs: []
  type: TYPE_NORMAL
- en: Federated migration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Federated migration is related to several topics we discussed, such as location
    affinity, federated scheduling, and high availability. At its core, federated
    migration means moving a whole application or some part of it from one cluster
    to another (and more generally from M clusters to N clusters). Federation migration
    can happen in response to various events, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A low capacity event in a cluster (or a cluster failure)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A change of scheduling policy (we no longer use cloud provider X)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A change of resource pricing (cloud provider Y dropped their prices, so let's
    migrate there)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new cluster was added to or removed from the federation (let's rebalance the
    pods of the application)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strictly-coupled applications can be trivially moved, in part or in whole, one
    pod at a time, to one or more clusters (within applicable policy constraints,
    such as `PrivateCloudOnly`).
  prefs: []
  type: TYPE_NORMAL
- en: For preferentially-coupled applications, the federation system must first locate
    a single cluster with sufficient capacity to accommodate the entire application,
    then reserve that capacity and incrementally move the application, one (or more)
    resource at a time, over to the new cluster within some bounded time period (and
    possibly within a predefined maintenance window).
  prefs: []
  type: TYPE_NORMAL
- en: 'Strictly-coupled applications (with the exception of those deemed completely
    immovable) require the federation system to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Start up an entire replica application in the destination cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy persistent data to the new application instance (possibly before
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: starting pods)
  prefs: []
  type: TYPE_NORMAL
- en: Switch user traffic across
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tear down the original application instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering a federated service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes provides KubeDNS as a built-in core component. KubeDNS uses a
  prefs: []
  type: TYPE_NORMAL
- en: '`cluster-local` DNS server as well as naming conventions to compose well-qualified'
  prefs: []
  type: TYPE_NORMAL
- en: (by namespace) DNS names conventions. For example, `the-service` is resolved
    to the `the-service` service in the default `namespace`, while `the-service.the-namespace`
    is resolved to the service called `the-service` in the `the-namespace namespace`,
    which is separate from the default `the-service`. Pods can find and access internal
    services easily with KubeDNS. Kubernetes cluster federation extends the mechanism
    to multiple clusters. The basic concept is the same, but another level of a federation
    is added. The DNS name of a service now consists of `<service name>.<namespace
    name>.<federation name>`. This way, internal service access is still usable using
    the original `<service name>.<namepace name>` naming convention. However, clients
    that want to access a federated service use the federated name that will be forwarded
    eventually to one of the federation member clusters to handle the request.
  prefs: []
  type: TYPE_NORMAL
- en: This federation-qualified naming convention also helps prevent internal cluster
    traffic from reaching across to other clusters by mistake.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the preceding NGINX example service, and the federated service DNS name
    form just described, let''s consider an example: a pod in a cluster in the cluster-1
    availability zone needs to access the NGINX service. Rather than use the service''s
    traditional cluster-local DNS name (`nginx.the-namespace`, which is automatically
    expanded to `nginx.the-namespace.svc.cluster.local`), it can now use the service''s
    federated DNS name, which is `nginx.the-namespace.the-federation`. This will be
    automatically expanded and resolved to the closest healthy shard of the NGINX
    service, wherever in the world that may be. If a healthy shard exists in the local
    cluster, that service''s cluster-local (typically `10.x.y.z`) IP address will
    be returned (by the cluster-local KubeDNS). This is almost exactly equivalent
    to non-federated service resolution (almost because KubeDNS actually returns both
    a `CNAME` and an A record for local federated services, but applications will
    be oblivious to this minor technical difference).'
  prefs: []
  type: TYPE_NORMAL
- en: However, if the service doesn't exist in the local cluster (or doesn't have
    healthy backend pods) the DNS query is expanded automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Running federated workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Federated workloads are workloads that are processed on multiple Kubernetes
    clusters at the same time. This is relatively easy to do for loosely-coupled and
    embarrassingly-distributed applications. However, if most of the processing can
    be done in parallel, often there is a join point at the end, or at least a central
    persistent store that needs to be queried and updated. It gets more complicated
    if multiple pods of the same service need to cooperate across clusters, or if
    a collection of services (each one of them may be federated) must work together
    and be synchronized to accomplish something.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes federation supports federated services that provide a great foundation
    for such federated workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Some key points for federated services are service discovery, cross-cluster
  prefs: []
  type: TYPE_NORMAL
- en: load-balancing, and availability zone fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a federated service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A federated service creates a corresponding service in the federation's member
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to create a federated NGINX service (assuming you have the service
    configuration in `nginx.yaml`), type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify a service was created in each cluster (for example, in `cluster-2`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: All the created services in all the clusters will share the same namespace and
    service name, which makes sense since they are a single logical service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The status of your federated service will automatically reflect the real-time
    status of the underlying Kubernetes services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Adding backend pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As of Kubernetes 1.10, we still need to add backend pods to each federation
    member cluster. This can be done with the `kubectl run` command. In a future release,
    the Kubernetes federation API server will be able to do it automatically. This
    will save one more step. Note that when you use the `kubectl run` command, Kubernetes
    automatically adds the run label to the pod based on the image name. In the following
    example, which launches an NGINX backend pod on five Kubernetes clusters, the
    image name is `nginx` (ignoring the version), so the following label is added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This is necessary because the service uses that label to identify its pods.
    If you use another label, you need to add it explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Verifying public DNS records
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the preceding pods have successfully started and are listening for connections,
    Kubernetes will report them as healthy endpoints of the service in that cluster
    (through automatic health checks). The Kubernetes cluster federation will in turn
    consider each of these service shards to be healthy, and place them in service
    by automatically configuring corresponding public DNS records. You can use your
    preferred interface from your configured DNS provider to verify this. For example,
    your federation may be configured to use Google Cloud DNS and a managed DNS domain,
    `example.com`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow up with the following command to see the actual DNS records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'If your federation is configured to use the `aws route53` DNS service, use
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Then use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'You can, of course, use standard DNS tools such as `nslookup` or `dig` to verify
    DNS records were updated properly. You may have to wait a little for your changes
    to propagate. Alternatively, you can point directly to your DNS provider:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: However, I always prefer to observe DNS changes in the wild after they were
    properly propagated, so I can inform users that everything is ready to go.
  prefs: []
  type: TYPE_NORMAL
- en: DNS expansion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the service does not exist in the local cluster (or it exists but has no
    healthy backend pods), the DNS query is automatically expanded to find the external
    IP address closest to the requestor's availability zone. KubeDNS performs this
    automatically and returns the corresponding `CNAME`. That will get further resolved
    to the IP address of one of the service's backing pods.
  prefs: []
  type: TYPE_NORMAL
- en: You don't have to rely on automatic DNS expansion. You can also provide the
    `CNAME` of a service in a particular cluster directly or in a particular region.
    For example, on GCE/GKE you can specify `nginx.the-namespace.svc.europe-west1.example.com`.
    That will get resolved to a backing pod of the service in one of the clusters
    in Europe (assuming there are clusters and healthy backing pods there).
  prefs: []
  type: TYPE_NORMAL
- en: 'External clients can''t utilize DNS expansion, but if they want to target some
    restricted subset of the federation (such as a particular region) then they can
    provide the service''s fully qualified `CNAME` just as the example. Since those
    names tend to be long and cumbersome, a good practice is to add some static convenience
    `CNAME` records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows how a federated lookup works across multiple clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/07c14135-6427-484a-93c4-08f0e73fd168.png)'
  prefs: []
  type: TYPE_IMG
- en: Handling failures of backend pods and whole clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes will take non-responsive pods out of service within a few seconds.
    The federation control plane monitors the health of clusters and the endpoints
    behind all of the shards of your federated service in the different clusters.
    It will take them in and out of service as required, for example, when all of
    the endpoints behind a service, an entire cluster, or a whole availability zone
    go down. The inherent latency of DNS caching (3 minutes for Federated Service
    DNS records by default), may send failover of clients to an alternative cluster
    in the case of catastrophic failure. However, given the number of discrete IP
    addresses that can be returned for each regional service endpoint (see, for example,
    `us-central1`, which has three alternatives), many clients will fail over automatically
    to one of the alternative IPs in less time than that, given the appropriate configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When things go south, you need to be able to figure out what's wrong and how
    to fix it. Here are a few common problems and how to diagnose/fix them.
  prefs: []
  type: TYPE_NORMAL
- en: Unable to connect to federation API server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Verify the federation API server is running
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify the client (Kubectl) is configured correctly with proper API endpoints
    and credentials
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Federated service is created successfully but no service is created in the underlying
    clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Verify the clusters are registered with federation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify the federation API server was able to connect and authenticate against
    all clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check quotas are sufficient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Check the logs for other problems:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've covered the important topic of Kubernetes cluster federation.
    Cluster federation is still in beta and is a little raw, but it is already usable.
    There aren't a lot of deployments and the officially supported target platforms
    are currently AWS and GCE/GKE, but there is a lot of momentum behind cloud federation.
    It is a very important piece for building massively scalable systems on Kubernetes.
    We've discussed the motivation and use cases for Kubernetes cluster federation,
    the federation control plane components, and the federated Kubernetes objects.
    We also looked into the less supported aspects of federation, such as custom scheduling,
    federated data access, and auto-scaling. We then looked at how to run multiple
    Kubernetes clusters, which includes setting up a Kubernetes cluster federation,
    adding and removing clusters to the federation along with load balancing, federated
    failover when something goes wrong, service discovery, and migration. Then, we
    dived into running federated workloads across multiple clusters with federated
    services and the various challenges associated with this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should have a clear understanding of the current state of
    federation, what it takes to utilize the existing capabilities provided by Kubernetes,
    and what pieces you'll have to implement yourself to augment incomplete or immature
    features. Depending on your use case, you may decide that it's still too early
    or that you want to take the plunge. The developers working on Kubernetes federation
    are moving fast, so it's very likely that it will be much more mature and battle-tested
    by the time you need to make your decision.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll dig into Kubernetes internals and how to customize
    it. One of the prominent architectural principles of Kubernetes is that it is
    accessible through a full-fledged REST API. The Kubectl command-line tool is built
    on top the Kubernetes API and provides interactivity to the full spectrum of Kubernetes.
    However, the programmatic API access gives you a lot of flexibility to enhance
    and extend Kubernetes. There are client libraries in many languages that allow
    you to leverage Kubernetes from the outside and integrate it into existing systems.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to its REST API, Kubernetes is a very modular platform by design.
    Many aspects of its core operation can be customized and/or extended. In particular,
    you can add user-defined resources and integrate them with the Kubernetes object
    model and benefit from the management services of Kubernetes, storage in `etcd`,
    exposure through the API, and uniform access to built-in and custom objects.
  prefs: []
  type: TYPE_NORMAL
- en: We've already seen various aspects that are extremely extensible, such as networking
    and access control through CNI plugins and custom storage classes. However, Kubernetes
    goes even further and lets you customize the scheduler itself, which controls
    pod assignment to nodes.
  prefs: []
  type: TYPE_NORMAL
