- en: Chapter 4. Real-time Trend Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce you to trend analysis techniques using Storm
    and Trident. Real-time trend analysis involves identifying patterns in data streams,
    such as recognizing when the occurrence rate or count of certain events reaches
    a certain threshold. Common examples include trending topics in social media,
    such as when a specific hashtag becomes popular on Twitter or identifying trending
    search terms in a search engine. Storm originated as a project to perform real-time
    analytics on Twitter data, and it provides many of the core primitives required
    for analytical computation.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, the spout implementations used were primarily simulations
    that used static sample data or randomly generated data. In this chapter, we will
    introduce an open source spout that emits data from a queue (Apache Kafka) and
    supports all three types of the Trident spout transaction (Non-transaction, Repeat
    Transaction, and Opaque Transactional). We will also implement a simple, generic
    method to populate the Kafka queue using a popular logging framework that will
    enable you to quickly begin real-time analysis of the existing applications and
    data with little or no source code modifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Logging data to Apache Kafka and streaming it to Storm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming an existing application's log data to Storm for analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing an exponentially weighted moving average Trident function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the XMPP protocol with Storm to send alerts and notifications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our use case, we have an application or set of applications (websites, enterprise
    applications, and so on) that use the popular logback framework ([http://logback.qos.ch](http://logback.qos.ch))
    for logging structured messages to disk (access logs, errors, and so on). Currently,
    the only way to perform analytics on that data is to process the files in batches
    using something like Hadoop. The latency introduced by that process dramatically
    slows down our reaction time; patterns gleaned from the log data only emerge hours,
    sometimes days, after a particular event occurred and the opportunity to take
    responsive action has passed. It is much more desirable to be actively notified
    of patterns as they emerge, rather than after the fact.
  prefs: []
  type: TYPE_NORMAL
- en: 'This use case represents a common theme and has a broad range of applications
    across many business scenarios, including the following applications:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Application Monitoring: For example, to notify system administrators when certain
    network errors reach a certain frequency'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Intrusion Detection: For example, to detect suspicious activity such as an
    increase in failed login attempts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Supply Chain Management: For example, to identify spikes in sales of specific
    products and adjusting just-in-time delivery accordingly'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Online Advertising: For example, to recognize popular trends and dynamically
    changing ad delivery'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The architecture of our application is depicted in the following diagram, and
    it will include the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture](img/8294OS_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The source application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The source application component is any application that uses the logback framework
    for logging arbitrary log messages. For our purposes, we will create a simple
    application that logs structured messages at certain intervals. However, as you'll
    see, any existing application that uses either the logback or slf4j frameworks
    can be substituted with a simple configuration change.
  prefs: []
  type: TYPE_NORMAL
- en: The logback Kafka appender
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The logback framework has an extension mechanism that allows you to add additional
    appenders to its configuration. A logback appender is simply a Java class that
    receives logging events and does something with them. The most commonly used appenders
    are one of several `FileAppender` subclasses that simply format and write log
    messages to a file on disk. Other appender implementations write log data to network
    sockets, relational databases, and to SMTP for e-mail notifications. For our purposes,
    we will implement an appender that writes log messages to an Apache Kafka queue.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Kafka ([http://kafka.apache.org](http://kafka.apache.org)) is an open
    source distributed publish-subscribe messaging system. Kafka is specifically designed
    and optimized for high-throughput, persistent real-time streams. Like Storm, Kafka
    is designed to scale horizontally on commodity software to support hundreds of
    thousands of messages per second.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka spout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Kafka spout reads data from a Kafka queue and emits it to a Storm or Trident
    topology. The Kafka spout was originally authored by Nathan Marz, and it is still
    a part of the storm-contrib project on GitHub ([https://github.com/nathanmarz/storm-contrib](https://github.com/nathanmarz/storm-contrib)).
    Prebuilt binaries of the Kafka spout are available from the `clojars.org` Maven
    repository ([https://clojars.org/storm/storm-kafka](https://clojars.org/storm/storm-kafka)).
    We will use the Kafka spout to read messages from the Kafka queue and stream them
    into our topology.
  prefs: []
  type: TYPE_NORMAL
- en: Our topology will consist of a collection of both built-in and custom Trident
    components (functions, filters, state, and so on) that detect patterns in the
    source data stream. When a pattern is detected, the topology will emit a tuple
    to a function that will send an XMPP message to an XMPP server to notify end users
    via an **instant message** (**IM**).
  prefs: []
  type: TYPE_NORMAL
- en: The XMPP server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Extensible Messaging and Presence Protocol** (**XMPP**) ([http://xmpp.org](http://xmpp.org))
    is an XML-based standard for instant messaging, presence information, and contact
    list maintenance. Many IM clients such as Adium (for OSX) ([http://adium.im](http://adium.im))
    and Pidgin (for OSX, Linus, and Windows) ([http://www.pidgin.im](http://www.pidgin.im))
    support the XMPP protocol, and if you have ever used Google Talk for instant messaging,
    you have used XMPP.'
  prefs: []
  type: TYPE_NORMAL
- en: We will use the open source OpenFire XMPP server ([http://www.igniterealtime.org/projects/openfire/](http://www.igniterealtime.org/projects/openfire/))
    for its ease of setup and compatibility with OSX, Linux, and Windows.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the required software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll begin by installing the necessary software: Apache Kafka and OpenFire.
    Although Kafka is a distributed messaging system, it will work just fine installed
    as a single node, or even locally as part of a development environment. In a production
    environment, you will need to set up a cluster of one or more machines depending
    on your scaling requirements. The OpenFire server is not a clustered system and
    can be installed on a single node or locally.'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Kafka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kafka depends on ZooKeeper for storing certain state information, much like
    Storm. Since Storm imposes a relatively light load on ZooKeeper, in many cases
    it is acceptable to share the same ZooKeeper cluster between both Kafka and Storm.
    Since we've already covered ZooKeeper installation in [Chapter 2](ch02.html "Chapter 2. Configuring
    Storm Clusters"), *Configuring Storm Clusters*, here we'll just cover the running
    of the local ZooKeeper server that ships with Kafka and is suitable for a development
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by downloading the 0.7.x release of Apache Kafka from the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://kafka.apache.org/downloads.html](http://kafka.apache.org/downloads.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, unpack the source distribution and change the existing directory to the
    following directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Kafka is written in the Scala JVM language ([http://www.scala-lang.org](http://www.scala-lang.org))
    and uses `sbt` (**Scala Build Tool**) ([http://www.scala-sbt.org](http://www.scala-sbt.org))
    for compiling and packaging. Fortunately, the Kafka source distribution includes
    `sbt` and can be built with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Before starting Kafka, unless you already have a ZooKeeper service running,
    you will need to start the ZooKeeper service bundled with Kafka using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, in a separate terminal window, start the Kafka service with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The Kafka service is now ready to use.
  prefs: []
  type: TYPE_NORMAL
- en: Installing OpenFire
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenFire is available as an installer for OSX and Windows as well as a package
    for various Linux distributions, and it can be downloaded from the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.igniterealtime.org/downloads/index.jsp](http://www.igniterealtime.org/downloads/index.jsp)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install OpenFire, download the installer for your operating system and follow
    the appropriate installation instructions that can be found at the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.igniterealtime.org/builds/openfire/docs/latest/documentation/index.html](http://www.igniterealtime.org/builds/openfire/docs/latest/documentation/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the sample application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The application component is a simple Java class that uses the **Simple Logging
    Facade for Java** (**SLF4J**) ([http://www.slf4j.org](http://www.slf4j.org)) to
    log messages. We will simulate an application that begins by generating warning
    messages at a relatively slow rate, then switches to a state where it generates
    warning messages at a much faster rate, and finally returns to the slow state
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Log a warning message every 5 seconds for 30 seconds (slow state)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log a warning message every second for 15 seconds (rapid state)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log a warning message every 5 seconds for 30 seconds (slow state)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The goal of the application is to generate a simple pattern that our storm
    topology can recognize and react to by sending notifications when certain patterns
    emerge and state changes occur as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Sending log messages to Kafka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The logback framework provides a simple extension mechanism that allows you
    to plug in additional appenders. In our case, we want to implement an appender
    that can write log message data to Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logback includes the `ch.qos.logback.core.AppenderBase` abstract class that
    makes it easy to implement the `Appender` interface. The `AppenderBase` class
    defines a single abstract method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `eventObject` parameter represents a logging event and includes properties
    such as the date of the event, the log level (`DEBUG`, `INFO`, `WARN`, and so
    on), as well as the log message itself. We will override the `append()` method
    to write the `eventObject` data to Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the `append()` method, the `AppenderBase` class defines two
    additional lifecycle methods that we will need to override:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `start()` method is called during the initialization of the logback framework,
    and the `stop()` method is called upon deinitialization. We will override these
    methods to set up and tear down our connection to the Kafka service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code for the `KafkaAppender` class is listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you will see, the JavaBean-style accessors in this class allow us to configure
    the associated values via dependency injection at runtime when the logback framework
    initializes. The setters and getters for the `zookeeperHosts` property are used
    to initialize the `KafkaProducer` client, configuring it to discover Kafka hosts
    that have registered with ZooKeeper. An alternative method would be to supply
    a static list of Kafka hosts, but for simplicity's sake it is easier to use an
    auto-discovery mechanism. The `topic` property is used to tell the `KafkaConsumer`
    client from which Kafka topic it should read.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Formatter` property is somewhat special. It is an interface we''ve defined
    that provides an extension point for handling structured (that is, parseable)
    log messages as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'A `Formatter` implementation''s job is to take an `ILoggingEvent` object and
    turn it into a machine-readable string that can be processed by a consumer. A
    simple implementation listed in the following code snippet simply returns the
    log message, discarding any additional metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following logback configuration file illustrates the usage of the appender.
    This example does not define a custom `Formatter` implementation, so the `KafkaAppender`
    class will default to using the `MessageFormatter` class and just write the log
    message data to Kafka and discard any additional information contained in the
    logging event, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The Storm application we''re building is time sensitive: if we''re tracking
    the rate at which each event occurs, we need to know exactly when an event occurs.
    A naïve approach would be to simply assign the event a time using the `System.currentTimeMillis()`
    method when the data enters our topology. However, Trident''s batching mechanism
    doesn''t guarantee that tuples will be delivered to a topology at the same rate
    with which they were received.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to account for this situation, we need to capture the time of the event
    when it occurs and include it in the data when we write to the Kafka queue. Fortunately,
    the `ILoggingEvent` class includes a timestamp, in milliseconds since the epoch,
    that the event occurred.
  prefs: []
  type: TYPE_NORMAL
- en: 'To include the metadata included in `ILoggingEvent`, we''ll create a custom
    `Formatter` implementation that encodes the log event data in JSON format as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The bulk of the `JsonMessageFormatter` class code uses a `java.lang.StringBuilder`
    class to create JSON from the `ILoggingEvent` object. While we could have used
    a JSON library to do the work, the JSON data we're generating is simple and adding
    an additional dependency just to generate JSON would be overkill.
  prefs: []
  type: TYPE_NORMAL
- en: The one JavaBean property exposed by `JsonMessageFormatter` is the `expectJson`
    Boolean used to specify whether the log message passed to the `Formatter` implementation
    should be treated as JSON. If set to `False`, the log message will be treated
    as a string and wrapped in double quotes, otherwise the message will be treated
    as a JSON object (`{...}`) or array (`[...]`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a sample logback configuration file that illustrates the usage
    of the `KafkaAppender` and `JsonFormatter` classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Since the analytics topology we are building is more concerned with event timing
    than message content, the log messages we generate will be strings, so we set
    the `expectJson` property to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the log analysis topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the means to write our log data to Kafka, we''re ready to turn our attention
    to the implementation of a Trident topology to perform the analytical computation.
    The topology will perform the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Receive and parse the raw JSON log event data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract and emit necessary fields.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update an exponentially-weighted moving average function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine if the moving average has crossed a specified threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filter out events that do not represent a state change (for example, rate moved
    above/below threshold).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Send an instant message (XMPP) notification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The topology is depicted in the following diagram with the Trident stream operations
    at the top and stream processing components at the bottom:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing the log analysis topology](img/8294OS_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Kafka spout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step in creating the log analysis topology is to configure the Kafka
    spout to stream data received from Kafka into our topology as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This code first creates a new `TridentTopology` instance, and then uses the
    Kafka Java API to create a list of Kafka hosts with which to connect (since we''re
    running a single, unclustered Kafka service locally, we specify a single host:
    `localhost`). Next, we create the `TridentKafkaConfig` object, passing it the
    host list and a unique identifier.'
  prefs: []
  type: TYPE_NORMAL
- en: The data our application writes to Kafka is a simple Java string, so we use
    Storm-Kafka built-in `StringScheme` class. The `StringScheme` class will read
    data from Kafka as a string and output it in a tuple field named `str`.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, upon deployment the Kafka spout will attempt to read from the Kafka
    queue where it last left off by querying ZooKeeper for state information. This
    behavior can be overridden by calling the `forceOffsetTime(long time)` method
    of the `TridentKafkaConfig` class. The time parameter can be one of the following
    three values:'
  prefs: []
  type: TYPE_NORMAL
- en: '**-2 (earliest offset)**: The spout will *rewind* and start reading from the
    beginning of the queue'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**-1 (latest offset)**: The spout will *fast forward* and read from the end
    of the queue'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time in milliseconds**: Given a specific date in milliseconds (for example,
    `java.util.Date.getTime()`), the spout will attempt to begin reading from that
    point in time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After setting up the spout configuration, we create an instance of the *Opaque
    Transactional* Kafka spout and set up a corresponding Trident stream.
  prefs: []
  type: TYPE_NORMAL
- en: The JSON project function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The data stream coming from the Kafka spout will contain a single field (`str`)
    containing the JSON data from the log event. We''ll create a Trident function
    to parse the incoming data and output, or project requested fields as tuple values
    as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `JsonProjectFunction` constructor takes a `Fields` object parameter that
    will determine what values to emit as a list of key names to look up from the
    JSON. When the function receives a tuple, it will parse the JSON in the tuple's
    `str` field, iterate the `Fields` object's values, and emit the corresponding
    value from the input JSON.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code creates a `Fields` object with a list of field names to
    extract from the JSON. It then creates a new `Stream` object from the spout stream,
    selects the `str` tuple field as the input to the `JsonProjectFunction` constructor,
    constructs the `JsonProjectFunction` constructor, and specifies that the fields
    selected from the JSON will also be output from the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider that the following JSON message is received from the Kafka spout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This would mean that the function would output the following tuple values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Calculating a moving average
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to calculate the rate at which log events occur, without the need to
    store an inordinate amount of state, we will implement a function that performs
    what is known in statistics as an **exponentially weighted moving average**.
  prefs: []
  type: TYPE_NORMAL
- en: 'A moving average calculation is often used to smooth out short-term fluctuations
    and expose long-term trends in time series data. One of the most common examples
    of a moving average is its use in graphing the fluctuation of prices in the stock
    market, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Calculating a moving average](img/8294OS_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The smoothing effect of a moving average is achieved by taking into account
    historical values in the calculation. A moving average calculation can be performed
    with a very minimal amount of state. For a time series, we need only keep the
    time of the last event and the last calculated average.
  prefs: []
  type: TYPE_NORMAL
- en: 'In pseudo code, the calculation would look something like the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `alpha` value in the preceding calculation is a constant value between `0`
    and `1`. The `alpha` value determines the amount of smoothing that occurs over
    time. The closer the `alpha` value is to `1`, the more the historical values affect
    the current average. In other words, an `alpha` value closer to `0` will result
    in less smoothing and the moving average will be closer to the current value.
    An `alpha` value closer to `1` will have the opposite effect. The current average
    will be less affected by wild fluctuations and the historical values will have
    more weight in determining the current average.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a sliding window
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some cases, we may want to discount historical values to reduce their effects
    on the moving average, for example, to reset the smoothing effect if a large amount
    of time has passed between receiving events. In case of a low alpha value, this
    may not be necessary since the smoothing effect is minimal. In the event of a
    high alpha, however, it may be desirable to counteract the smoothing effect.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the following example.
  prefs: []
  type: TYPE_NORMAL
- en: We have an event (such as a network error and so on) that occurs infrequently.
    Occasionally, small spikes in frequency occur, but that's usually okay. So, we
    want to smooth out the small spikes. What we want to be notified of is if a *sustained*
    spike occurs.
  prefs: []
  type: TYPE_NORMAL
- en: If the event occurs once a week on average (well below our notification threshold),
    but one day spikes to many occurrences within an hour (above our notification
    threshold), the smoothing effect of the high alpha may negate the spike such that
    a notification is never triggered.
  prefs: []
  type: TYPE_NORMAL
- en: 'To counteract this effect, we can introduce the concept of a **sliding window**
    into our moving average calculation. Since we''re already keeping track of both
    the time of the last event, and the current average, implementing a sliding window
    is simple as illustrated in the following pseudo code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'An implementation of an exponentially weighted moving average is listed as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `EWMA` implementation defines three useful constant `alpha` values: `ONE_MINUTE_ALPHA`,
    `FIVE_MINUTE_ALPHA`, and `FIFTEEN_MINUTE_ALPHA`. These correspond to the standard
    `alpha` values used to calculate load averages in UNIX. The `alpha` value can
    also be specified manually, or as a function of an *alpha* window.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation uses a fluent-style *builder* API. For example, you can
    create an `EWMA` instance with a sliding window of one minute and an `alpha` value
    equivalent to the UNIX one-minute interval, as shown in use the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `mark()` methods are used to update the moving average. Without arguments,
    the `mark()` method will use the current time to calculate the average. Because
    we want to use the original timestamp from the log event, we overload the `mark()`
    method to allow the specification of a specific time.
  prefs: []
  type: TYPE_NORMAL
- en: The `getAverage()` method returns the average time between calls to `mark()`
    in milliseconds. We also added the convenient `getAverageIn()`method, which will
    return the average in the specified time unit of measure (seconds, minutes, hours,
    and so on). The `getAverageRatePer()` method returns the rate of calls to `mark()`
    in a specific time measurement.
  prefs: []
  type: TYPE_NORMAL
- en: As you'll probably notice, using an exponentially weighted moving average can
    be somewhat tricky. Finding the right set of values for an alpha as well as the
    optional sliding window varies quite a bit depending on the specific use case,
    and finding the right value is largely a matter of trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the moving average function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use our `EWMA` class in a Trident topology, we''ll create a subclass of
    Trident''s `BaseFunction` abstract class named `MovingAverageFunction` that wraps
    an instance of `EWMA,` as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `MovingAverage.execute()` method gets the `Long` value of the incoming
    tuple''s first field, uses the value to call the `mark()` method to update the
    current average, and emits the current average rate. Functions in Trident are
    additive, meaning they add values to the tuples in a stream. So, for example,
    consider that the tuple coming into our function looks like the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that after processing, the tuple might look like the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, the new value represents the new average rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the function, we create an instance of the `EWMA` class and pass it
    to the `MovingAverageFunction` constructor. We apply the function to the stream
    with the `each()` method, selecting the `timestamp` field as the input, as shown
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Filtering on thresholds
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our use case, we want to be able to define a rate threshold that triggers
    a notification when exceeded. We also want notifications when the average rate
    falls back below that threshold (that is, returns to normal). We can accomplish
    this functionality using a combination of an additional function and a simple
    Trident filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The job of the function will be to determine whether the new value of the average
    rate field crosses a threshold, and if that represents a change from the previous
    value (that is, whether it has changed from *below threshold* to *above threshold*
    or vice versa). If the new average represents a state change, the function will
    emit the Boolean value `True`, otherwise it will emit `False`. We will leverage
    that value to filter out events that do not represent a state change. We''ll implement
    the threshold tracking function in the `ThresholdFilterFunction` class as shown
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `ThresholdFilterFunction` class defines an inner enumeration to represent
    the state (above threshold or below). The constructor takes a double argument
    that establishes the threshold we compare against. In the `execute()` method,
    we get the current rate value and determine whether it is below or above the threshold.
    We then compare it to the last state to see if it has changed and emit that value
    as a Boolean. Finally, we update the internal above/below state to the newly calculated
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'After passing through the `ThresholdFilterFunction` class, tuples in the input
    stream will contain a new Boolean value that we can use to easily filter out events
    that don''t trigger a state change. To filter out non-state-change events, we''ll
    use a simple `BooleanFilter` class as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `BooleanFilter.isKeep()` method simply reads a field from a tuple as a Boolean
    value and returns that value. Any tuples containing `False` for the input value
    will be filtered out of the resulting stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code fragment illustrates the usage of the `ThresholdFilterFuncation`
    class and the `BooleanFilter` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The first line creates a `ThresholdFilterFunction` instance with a threshold
    of `50.0`. We then create a new stream using the `averageStream` as input to the
    threshold function, and select the `average` tuple field as input. We also assign
    names (`change` and `threshold`) to the fields added by the function. Finally,
    we apply the `BooleanFilter` class to create a new stream that will only contain
    tuples that represent a change in threshold comparison.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have everything necessary to implement notifications. The
    `filteredStream` we've created will only contain tuples that represent a threshold
    state change.
  prefs: []
  type: TYPE_NORMAL
- en: Sending notifications with XMPP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The XMPP protocol provides all the typical features you would expect in an
    instant messaging standard:'
  prefs: []
  type: TYPE_NORMAL
- en: Rosters (contact lists)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Presence (knowing when others are online and their availability status)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User-to-user instant messaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Group chats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The XMPP protocol uses an XML format for its communication protocol, but there
    are numerous high-level client libraries that handle most of the low-level details
    with a simple API. We will use the Smack API ([http://www.igniterealtime.org/projects/smack/](http://www.igniterealtime.org/projects/smack/))
    as it is one of the most straightforward XMPP client implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet demonstrates the usage of the Smack API to send
    a simple instant message to another user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The code connects to the XMPP server at [jabber.org](http://jabber.org) and
    logs in with a username and password. Behind the scenes, the Smack library handles
    the low-level communications with the server. When the client connects and authenticates,
    it also sends a presence message to the server. This allows a user's contacts
    (other users listed in their XMPP roster) to receive a notification that the person
    is now connected. Finally, we create and send a simple message addressed to `"myfriend@jabber.org"`.
  prefs: []
  type: TYPE_NORMAL
- en: Based on this simple example, we will create a class named `XMPPFunction` that
    sends XMPP notifications when it receives a Trident tuple. The class will establish
    a long-lived connection to an XMPP server in the `prepare()` method. Also, in
    the `execute()` method it will create an XMPP message based on the tuple received.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the `XMPPFunction` class more reusable, we''ll introduce the `MessageMapper`
    interface that defines a method to format the data from a Trident tuple to a string
    suitable for an instant message notification, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll delegate message formatting to an instance of `MessageMapper` in the
    `XMPPFunction` class as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `XMPPFunction` class begins by defining several string constants that are
    used to look up values from the Storm configuration passed to the `prepare()`
    method, and it follows with the declaration of the instance variables that we'll
    populate when the function becomes active. The class' constructor takes a `MessageMapper`
    instance as a parameter that will be used in the `execute()` method to format
    the body of the notification message.
  prefs: []
  type: TYPE_NORMAL
- en: In the `prepare()` method, we look up the configuration parameters (`server`,
    `username`, `to address`, and so on) for the `XMPPConnection` class and open the
    connection. When a topology that uses this function is deployed, the `XMPP` client
    will send a presence packet and other users who have the configured user in their
    roster (buddy list) will receive a notification indicating that the user is now
    online.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final necessary piece of our notification mechanism is to implement a `MessageMapper`
    instance to format the contents of a tuple into a human-readable message body
    as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The final topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now have all the components necessary to build our log analysis topology
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the `buildTopology()` method creates all the stream connections between
    the Kafka spout and our Trident functions and filters. The `main()` method then
    submits the topology to a cluster: a local cluster if the topology is being run
    in the local mode or a remote cluster when run in the distributed mode.'
  prefs: []
  type: TYPE_NORMAL
- en: We begin by configuring the Kafka spout to read from the same topic that our
    application is configured to write log events. Because Kafka persists all the
    messages it receives, and because our application may have been running for some
    time (and thus logging many events), we tell the spout to fast-forward to the
    end of the Kafka queue by calling the `forceStartOffsetTime()` method with a value
    of `-1`. This will avoid the replay of all the old messages that we may not be
    interested in. Using a value of `-2` will force the spout to rewind to the beginning
    of the queue, and using a specific date in milliseconds will force it to rewind
    to a specific point in time. If the `forceFromStartTime()` method is not called,
    the spout will attempt to resume where it last left off by looking up an offset
    in ZooKeeper.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we set up the `JsonProjectFunction` class to parse the raw JSON received
    from Kafka and emit the values that we're interested in. Recall that the Trident
    functions are additive. This means that our tuple stream, in addition to all the
    values extracted from the JSON, will also contain the original unparsed JSON string.
    Since we no longer need that data, we call the `Stream.project()` method with
    a list of fields we want to keep. The `project()` method is useful for paring
    down tuple streams to just the essential fields, and it is especially important
    while repartitioning streams that have large amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting stream now contains just the data we need. We set up an `EWMA`
    instance with a sliding window of one minute and configure the `MovingAverageFunction`
    class to emit the current rate in minutes. We create the `ThresholdFunction` class
    with a value of `50.0`, so we'll receive a notification any time the average rate
    goes above or falls below 50 events per minute.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we apply the `BooleanFilter` class and connect the resulting stream
    to the `XMPPFunction` class.
  prefs: []
  type: TYPE_NORMAL
- en: The `main()` method of the topology simply populates a `Config` object with
    the properties needed by the `XMPPFunction` class and submits the topology.
  prefs: []
  type: TYPE_NORMAL
- en: Running the log analysis topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To run the analysis topology, first make sure that ZooKeeper, Kafka, and OpenFire
    are all up and running by using the procedures outlined earlier in the chapter.
    Then, run the `main()` method of the topology.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the topology activates, the *storm* XMPP user will connect to the XMPP
    server and trigger a presence event. If you are logged into the same server with
    an XMPP client and have the *storm* user in your buddy list, you will see it become
    available. This is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running the log analysis topology](img/8294OS_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, run the `RogueApplication` class and wait for a minute. You should receive
    an instant message notification indicating that the threshold has been exceeded,
    which will be followed by one indicating a return to normal (below threshold),
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running the log analysis topology](img/8294OS_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've introduced you to real-time analytics by creating a simple
    yet powerful topology that can be adapted to a wide range of applications. The
    components we've built are generic and can easily be reused and extended in other
    projects. Finally, we introduced a real-world spout implementation that can be
    used for a multitude of purposes.
  prefs: []
  type: TYPE_NORMAL
- en: While the topic of real-time analytics is very broad, and admittedly we've only
    been able to scratch the surface in this chapter, we encourage you to explore
    the techniques presented in other chapters of this book and consider how they
    may be incorporated into your analytics toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll introduce you to Trident's distributed state mechanism
    by building an application that continuously writes Storm-processed data to a
    graph database.
  prefs: []
  type: TYPE_NORMAL
