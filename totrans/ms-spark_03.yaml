- en: Chapter 3. Apache Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Apache Streaming module is a stream processing-based module within Apache
    Spark. It uses the Spark cluster to offer the ability to scale to a high degree.
    Being based on Spark, it is also highly fault tolerant, having the ability to
    rerun failed tasks by checkpointing the data stream that is being processed. The
    following areas will be covered in this chapter after an initial section, which
    will provide a practical overview of how Apache Spark processes stream-based data:'
  prefs: []
  type: TYPE_NORMAL
- en: Error recovery and checkpointing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TCP-based Stream Processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: File Streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flume Stream source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Stream source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each topic, I will provide a worked example in Scala, and will show how
    the stream-based architecture can be set up and tested.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When giving an overview of the Apache Spark streaming module, I would advise
    you to check the [http://spark.apache.org/](http://spark.apache.org/) website
    for up-to-date information, as well as the Spark-based user groups such as `<[user@spark.apache.org](mailto:user@spark.apache.org)>`.
    My reason for saying this is because these are the primary places where Spark
    information is available. Also the extremely fast (and increasing) pace of change
    means that by the time you read this new Spark functionality and versions, will
    be available. So, in the light of this, when giving an overview, I will try to
    generalize.
  prefs: []
  type: TYPE_NORMAL
- en: '![Overview](img/B01989_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The previous figure shows potential data sources for Apache Streaming, such
    as **Kafka**, **Flume**, and **HDFS**. These feed into the Spark Streaming module,
    and are processed as discrete streams. The diagram also shows that other Spark
    module functionality, such as machine learning, can be used to process the stream-based
    data. The fully processed data can then be an output for **HDFS**, **databases**,
    or **dashboards**. This diagram is based on the one at the Spark streaming website,
    but I wanted to extend it for both—expressing the Spark module functionality,
    and for dashboarding options. The previous diagram shows a MetricSystems feed
    being fed from Spark to Graphite. Also, it is possible to feed Solr-based data
    to Lucidworks banana (a port of kabana). It is also worth mentioning here that
    Databricks (see [Chapter 8](ch08.html "Chapter 8. Spark Databricks"), *Spark Databricks*
    and [Chapter 9](ch09.html "Chapter 9. Databricks Visualization"), *Databricks
    Visualization*) can also present the Spark stream data as a dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: '![Overview](img/B01989_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When discussing Spark discrete streams, the previous figure, again taken from
    the Spark website at [http://spark.apache.org/](http://spark.apache.org/), is
    the diagram I like to use. The green boxes in the previous figure show the continuous
    data stream sent to Spark, being broken down into a **discrete stream** (**DStream**).
    The size of each element in the stream is then based on a batch time, which might
    be two seconds. It is also possible to create a window, expressed as the previous
    red box, over the DStream. For instance, when carrying out trend analysis in real
    time, it might be necessary to determine the top ten Twitter-based Hashtags over
    a ten minute window.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, given that Spark can be used for Stream processing, how is a Stream created?
    The following Scala-based code shows how a Twitter stream can be created. This
    example is simplified because Twitter authorization has not been included, but
    you get the idea (the full example code is in the *Checkpointing* section). The
    Spark stream context, called `ssc`, is created using the spark context `sc`. A
    batch time is specified when it is created; in this case, five seconds. A Twitter-based
    DStream, called `stream`, is then created from the `Streamingcontext` using a
    window of 60 seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The stream processing can be started with the stream context start method (shown
    next), and the `awaitTermination` method indicates that it should process until
    stopped. So, if this code is embedded in a library-based application, it will
    run until the session is terminated, perhaps with a *Crtl* + *C*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This explains what Spark streaming is, and what it does, but it does not explain
    error handling, or what to do if your stream-based application fails. The next
    section will examine Spark streaming error management and recovery.
  prefs: []
  type: TYPE_NORMAL
- en: Errors and recovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally, the question that needs to be asked for your application is; is it
    critical that you receive and process all the data? If not, then on failure you
    might just be able to restart the application and discard the missing or lost
    data. If this is not the case, then you will need to use checkpointing, which
    will be described in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth noting that your application's error management should be robust
    and self-sufficient. What I mean by this is that; if an exception is non-critical,
    then manage the exception, perhaps log it, and continue processing. For instance,
    when a task reaches the maximum number of failures (specified by `spark.task.maxFailures`),
    it will terminate processing.
  prefs: []
  type: TYPE_NORMAL
- en: Checkpointing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is possible to set up an HDFS-based checkpoint directory to store Apache
    Spark-based streaming information. In this Scala example, data will be stored
    in HDFS, under `/data/spark/checkpoint`. The following HDFS file system `ls` command
    shows that before starting, the directory does not exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The Twitter-based Scala code sample given next, starts by defining a package
    name for the application, and by importing Spark, streaming, context, and Twitter-based
    functionality. It then defines an application object named `stream1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, a method is defined called `createContext`, which will be used to create
    both the spark, and streaming contexts. It will also checkpoint the stream to
    the HDFS-based directory using the streaming context checkpoint method, which
    takes a directory path as a parameter. The directory path being the value (`cpDir`)
    that was passed into the `createContext` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the main method is defined, as is the `HDFS` directory, as well as Twitter
    access authority and parameters. The Spark streaming context `ssc` is either retrieved
    or created using the HDFS `checkpoint` directory via the `StreamingContext` method—`getOrCreate`.
    If the directory doesn''t exist, then the previous method called `createContext`
    is called, which will create the context and checkpoint. Obviously, I have truncated
    my own Twitter auth. keys in this example for security reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Having run this code, which has no actual processing, the HDFS `checkpoint`
    directory can be checked again. This time it is apparent that the `checkpoint`
    directory has been created, and the data has been stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This example, taken from the Apache Spark website, shows how checkpoint storage
    can be set up and used. But how often is checkpointing carried out? The Meta data
    is stored during each stream batch. The actual data is stored with a period, which
    is the maximum of the batch interval, or ten seconds. This might not be ideal
    for you, so you can reset the value using the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Where `newRequiredInterval` is the new checkpoint interval value that you require,
    generally you should aim for a value which is five to ten times your batch interval.
  prefs: []
  type: TYPE_NORMAL
- en: Checkpointing saves both the stream batch and metadata (data about the data).
    If the application fails, then when it restarts, the checkpointed data is used
    when processing is started. The batch data that was being processed at the time
    of failure is reprocessed, along with the batched data since the failure.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to monitor the HDFS disk space being used for check pointing. In the
    next section, I will begin to examine the streaming sources, and will provide
    some examples of each type.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I will not be able to cover all the stream types with practical examples in
    this section, but where this chapter is too small to include code, I will at least
    provide a description. In this chapter, I will cover the TCP and file streams,
    and the Flume, Kafka, and Twitter streams. I will start with a practical TCP-based
    example.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter examines stream processing architecture. For instance, what happens
    in cases where the stream data delivery rate exceeds the potential data processing
    rate? Systems like Kafka provide the possibility of solving this issue by providing
    the ability to use multiple data topics and consumers.
  prefs: []
  type: TYPE_NORMAL
- en: TCP stream
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is a possibility of using the Spark streaming context method called `socketTextStream`
    to stream data via TCP/IP, by specifying a hostname and a port number. The Scala-based
    code example in this section will receive data on port `10777` that was supplied
    using the `netcat` Linux command. The code sample starts by defining the package
    name, and importing Spark, the context, and the streaming classes. The object
    class named `stream2` is defined, as it is the main method with arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of arguments passed to the class is checked to ensure that it is
    the hostname and the port number. A Spark configuration object is created with
    an application name defined. The Spark and streaming contexts are then created.
    Then, a streaming batch time of 10 seconds is set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: A DStream called `rawDstream` is created by calling the `socketTextStream` method
    of the streaming context using the host and port name parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'A top-ten word count is created from the raw stream data by splitting words
    by spacing. Then a (key,value) pair is created as `(word,1)`, which is reduced
    by the key value, this being the word. So now, there is a list of words and their
    associated counts. Now, the key and value are swapped, so the list becomes (`count`
    and `word`). Then, a sort is done on the key, which is now the count. Finally,
    the top 10 items in the `rdd`, within the DStream, are taken and printed out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The code closes with the Spark Streaming start, and `awaitTermination` methods
    being called to start the stream processing and await process termination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The data for this application is provided, as I stated previously, by the Linux
    `netcat` (`nc`) command. The Linux `cat` command dumps the contents of a log file,
    which is piped to `nc`. The `lk` options force `netcat` to listen for connections,
    and keep on listening if the connection is lost. This example shows that the port
    being used is `10777`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from this TCP-based stream processing is shown here. The actual
    output is not as important as the method demonstrated. However, the data shows,
    as expected, a list of 10 log file words in descending count order. Note that
    the top word is empty because the stream was not filtered for empty words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This is interesting if you want to stream data using Apache Spark streaming,
    based upon TCP/IP from a host and port. But what about more exotic methods? What
    if you wish to stream data from a messaging system, or via memory-based channels?
    What if you want to use some of the big data tools available today like Flume
    and Kafka? The next sections will examine these options, but first I will demonstrate
    how streams can be based upon files.
  prefs: []
  type: TYPE_NORMAL
- en: File streams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I have modified the Scala-based code example in the last section, to monitor
    an HDFS-based directory, by calling the Spark streaming context method called
    `textFileStream`. I will not display all of the code, given this small change.
    The application class is now called `stream3`, which takes a single parameter—the
    `HDFS` directory. The directory path could be on NFS or AWS S3 (all the code samples
    will be available with this book):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The stream processing is the same as before. The stream is split into words,
    and the top-ten word list is printed. The only difference this time is that the
    data must be put into the `HDFS` directory while the application is running. This
    is achieved with the HDFS file system `put` command here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the `HDFS` directory used is `/data/spark/stream/`, and the
    text-based source log file is `anaconda.storage.log` (under `/var/log/`). As expected,
    the same word list and count is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: These are simple streaming methods based on TCP, and file system data. But what
    if I want to use some of the built-in streaming functionality within Spark streaming?
    This will be examined next. The Spark streaming Flume library will be used as
    an example.
  prefs: []
  type: TYPE_NORMAL
- en: Flume
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Flume is an Apache open source project and product, which is designed to move
    large amounts of data at a big data scale. It is highly scalable, distributed,
    and reliable, working on the basis of data source, data sink, and data channels,
    as the diagram here, taken from the [http://flume.apache.org/](http://flume.apache.org/)
    website, shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Flume](img/B01989_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Flume uses agents to process data streams. As can be seen in the previous figure,
    an agent has a data source, a data processing channel, and a data sink. A clearer
    way to describe this is via the following figure. The channel acts as a queue
    for the sourced data and the sink passes the data to the next link in the chain.
  prefs: []
  type: TYPE_NORMAL
- en: '![Flume](img/B01989_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Flume agents can form Flume architectures; the output of one agent's sink can
    be the input to a second agent. Apache Spark allows two approaches to using Apache
    Flume. The first is an Avro push-based in-memory approach, whereas the second
    one, still based on Avro, is a pull-based system, using a custom Spark sink library.
  prefs: []
  type: TYPE_NORMAL
- en: 'I installed Flume via the Cloudera CDH 5.3 cluster manager, which installs
    a single agent. Checking the Linux command line, I can see that Flume version
    1.5 is now available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The Flume-based Spark example that I will initially implement here, is the
    Flume-based push approach, where Spark acts as a receiver, and Flume pushes the
    data to Spark. The following figure represents the structure that I will implement
    on a single node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Flume](img/B01989_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The message data will be sent to port `10777` on a host called `hc2r1m1` using
    the Linux `netcat` (`nc`) command. This will act as a source (`source1`) for the
    Flume agent (`agent1`), which will have an in-memory channel called `channel1`.
    The sink used by `agent1` will be Apache Avro based, again on a host called `hc2r1m1`,
    but this time, the port number will be `11777`. The Apache Spark Flume application
    `stream4` (which I will describe shortly) will listen for Flume stream data on
    this port.
  prefs: []
  type: TYPE_NORMAL
- en: 'I start the streaming process by executing the `netcat` (`nc`) command next,
    against the `10777` port. Now, when I type text into this window, it will be used
    as a Flume source, and the data will be sent to the Spark application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In order to run my Flume agent, `agent1`, I have created a Flume configuration
    file called `agent1.flume.cfg`, which describes the agent's source, channel, and
    sink. The contents of the file are as follows. The first section defines the `agent1`
    source, channel, and sink names.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The next section defines `source1` to be netcat based, running on the host
    called `hc2r1m1`, and `10777` port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `agent1` channel, `channel1`, is defined as a memory-based channel with
    a maximum event capacity of 1000 events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `agent1` sink, `sink1`, is defined as an Apache Avro sink on the
    host called `hc2r1m1`, and `11777` port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'I have created a Bash script called `flume.bash` to run the Flume agent, `agent1`.
    It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The script calls the Flume executable `flume-ng`, passing the `agent1` configuration
    file. The call specifies the agent named `agent1`. It also specifies the Flume
    configuration directory to be `/etc/flume-ng/conf/`, the default value. Initially,
    I will use a `netcat` Flume source with a Scala-based example to show how data
    can be sent to an Apache Spark application. Then, I will show how an RSS-based
    data feed can be processed in a similar way. So initially, the Scala code that
    will receive the `netcat` data looks like this. The class package name and the
    application class name are defined. The necessary classes for Spark and Flume
    are imported. Finally, the main method is defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The host and port name arguments for the data stream are checked and extracted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The Spark and streaming contexts are created. Then, the Flume-based data stream
    is created using the stream context host and port number. The Flume-based class
    `FlumeUtils` has been used to do this by calling it''s `createStream` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, a stream event count is printed, and (for debug purposes while we
    test the stream) the stream content is dumped. After this, the stream context
    is started and configured to run until terminated via the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Having compiled it, I will run this application using `spark-submit`. In the
    other chapters of this book, I will use a Bash-based script called `run_stream.bash`
    to execute the job. The script looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'So, this script sets some Spark-based variables, and a JAR library path for
    this job. It takes which Spark class to run, as its first parameter. It passes
    all the other variables, as parameters, to the Spark application class job. So,
    the execution of the application looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that the Spark application is ready, and is running as a Flume sink
    on port `11777`. The Flume input is ready, running as a netcat task on port `10777`.
    Now, the Flume agent, `agent1`, can be started using the Flume script called `flume.bash`
    to send the netcat source-based data to the Apache Spark Flume-based sink:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when the text is passed to the netcat session, it should flow through
    Flume, and be processed as a stream by Spark. Let''s try it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Three simple pieces of text have been added to the netcat session, and have
    been acknowledged with an `OK`, so that they can be passed to Flume. The debug
    output in the Flume session shows that the events (one per line ) have been received
    and processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, in the Spark `stream4` application session, three events have been
    received and processed. In this case, dumped to the session to prove the point
    that the data arrived. Of course, this is not what you would normally do, but
    I wanted to prove data transit through this configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This is interesting, but it is not really a production-worthy example of Spark
    Flume data processing. So, in order to demonstrate a potentially real data processing
    approach, I will change the Flume configuration file source details so that it
    uses a Perl script, which is executable as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The Perl script, which is referenced previously, `rss.perl`, just acts as a
    source of Reuters science news. It receives the news as XML, and converts it into
    JSON format. It also cleans the data of unwanted noise. First, it imports packages
    like LWP and `XML::XPath` to enable XML processing. Then, it specifies a science-based
    Reuters news data source, and creates a new LWP agent to process the data, similar
    to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then an infinite while loop is opened, and an HTTP `GET` request is carried
    out against the URL. The request is configured, and the agent makes the request
    via a call to the request method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'If the request is successful, then the XML data returned, is defined as the
    decoded content of the request. Title information is extracted from the XML, via
    an XPath call using the path called `/rss/channel/item/title`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'For each node in the extracted title data title XML string, data is extracted.
    It is cleaned of unwanted XML tags, and added to a Perl-based array called `titles`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The same process is carried out for description-based data in the request response
    XML. The XPath value used this time is `/rss/channel/item/description/`. There
    are many more tags to be cleaned from the description data, so there are many
    more Perl searches, and line replacements that act on this data (`s///g`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the XML-based title and description data is output in the RSS JSON
    format using a `print` command. The script then sleeps for 30 seconds, and requests
    more RSS news information to process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'I have created a second Scala-based stream processing code example called `stream5`.
    It is similar to the `stream4` example, but it now processes the `rss` item data
    from the stream. A case class is defined next to process the category, title,
    and summary from the XML `rss` information. An html location is defined to store
    the resulting data that comes from the Flume channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The `rss` stream data from the Flume-based event is converted into a string.
    It is then formatted using the case class called `RSSItem`. If there is event
    data, it is then written to an HDFS directory using the previous `hdfsdir` path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code sample, it is possible to see that the Perl `rss` script
    is producing data, because the Flume script output indicates that 80 events have
    been accepted and received:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The Scala Spark application `stream5` has processed 80 events in two batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'And the events have been stored on HDFS, under the expected directory, as the
    Hadoop file system `ls` command shows here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, using the Hadoop file system `cat` command, it is possible to prove that
    the files on HDFS contain rss feed news-based data as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This Spark stream-based example has used Apache Flume to transmit data from
    an rss source, through Flume, to HDFS via a Spark consumer. This is a good example,
    but what if you want to publish data to a group of consumers? In the next section,
    I will examine Apache Kafka—a publish subscribe messaging system, and determine
    how it can be used with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Kafka ([http://kafka.apache.org/](http://kafka.apache.org/)) is a top
    level open-source project in Apache. It is a big data publish/subscribe messaging
    system that is fast and highly scalable. It uses message brokers for data management,
    and ZooKeeper for configuration, so that data can be organized into consumer groups
    and topics. Data in Kafka is split into partitions. In this example, I will demonstrate
    a receiver-less Spark-based Kafka consumer, so that I don't need to worry about
    configuring Spark data partitions when compared to my Kafka data.
  prefs: []
  type: TYPE_NORMAL
- en: In order to demonstrate Kafka-based message production and consumption, I will
    use the Perl RSS script from the last section as a data source. The data passing
    into Kafka and onto Spark will be Reuters RSS news data in the JSON format.
  prefs: []
  type: TYPE_NORMAL
- en: As topic messages are created by message producers, they are then placed in
    partitions in message order sequence. The messages in the partitions are retained
    for a configurable time period. Kafka then stores the offset value for each consumer,
    which is that consumer's position (in terms of message consumption) in that partition.
  prefs: []
  type: TYPE_NORMAL
- en: 'I am currently using Cloudera''s CDH 5.3 Hadoop cluster. In order to install
    Kafka, I need to download a Kafka JAR library file from: [http://archive.cloudera.com/csds/kafka/](http://archive.cloudera.com/csds/kafka/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having downloaded the file, and given that I am using CDH cluster manager,
    I then need to copy the file to the `/opt/cloudera/csd/` directory on my NameNode
    CentOS server, so that it will be visible to install:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'I then need to restart the Cloudera cluster manager server on my NameNode,
    or master server, so that the change will be recognized. This was done as root
    using the service command, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the Kafka parcel should be visible within the CDH manager under **Hosts**
    | **Parcels**, as shown in the following figure. You can follow the usual download,
    distribution, and activate cycle for the CDH parcel installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kafka](img/B01989_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: I have installed Kafka message brokers on each Data Node, or Spark Slave machine
    in my cluster. I then set the Kafka broker ID values for each Kafka broker server,
    giving them a `broker.id` number of 1 through 4\. As Kafka uses ZooKeeper for
    cluster data configuration, I wanted to keep all the Kafka data in a top level
    node called `kafka` in ZooKeeper. In order to do this, I set the Kafka ZooKeeper
    root value, called `zookeeper.chroot`, to `/kafka`. After making these changes,
    I restarted the CDH Kafka servers for the changes to take effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Kafka installed, I can check the scripts available for testing. The following
    listing shows Kafka-based scripts for message producers and consumers, as well
    as scripts for managing topics, and checking consumer offsets. These scripts will
    be used in this section in order to demonstrate Kafka functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: In order to run the installed Kafka servers, I need to have the broker server
    ID's (`broker.id`) values set, else an error will occur. Once Kafka is installed
    and running, I will need to prepare a message producer script. The simple Bash
    script given next, called `kafka.bash`, defines a comma-separated broker list
    of hosts and ports. It also defines a topic called `rss`. It then calls the Perl
    script `rss.perl` to generate the RSS-based data. This data is then piped into
    the Kafka producer script called `kafka-console-producer` to be sent to Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that I have not mentioned Kafka topics at this point. When a topic is
    created in Kafka, the number of partitions can be specified. In the following
    example, the `kafka-topics` script has been called with the `create` option. The
    number of partitions have been set to `5`, and the data replication factor has
    been set to `3`. The ZooKeeper server string has been defined as `hc2r1m2-4` with
    a port number of `2181`. Also note that the top level ZooKeeper Kafka node has
    been defined as `/kafka` in the ZooKeeper string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'I have also created a Bash script called `kafka_list.bash` for use during testing,
    which checks all the Kafka topics that have been created, and also the Kafka consumer
    offsets. It calls the `kafka-topics` commands with a `list` option, and a `ZooKeeper`
    string to get a list of created topics. It then calls the Kafka script called
    `kafka-consumer-offset-checker` with a `ZooKeeper` string—the topic name and a
    group name to get a list of consumer offset values. Using this script, I can check
    that my topics are created, and the topic data is being consumed correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, I need to create the Apache Spark Scala-based Kafka consumer code. As
    I said, I will create a receiver-less example, so that the Kafka data partitions
    match in both, Kafka and Spark. The example is called `stream6`. First, the package
    is defined, and the classes are imported for Kafka, spark, context, and streaming.
    Then, the object class called `stream6`, and the main method are defined. The
    code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the class parameters (broker''s string, group ID, and topic) are checked
    and processed. If the class parameters are incorrect, then an error is printed,
    and execution stops, else the parameter variables are defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The Spark context is defined in terms of an application name. Again the Spark
    URL has been left as the default. The streaming context has been created using
    the Spark context. I have left the stream batch interval at 10 seconds, which
    is the same as the last example. However, you can set it using a parameter of
    your choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the broker list and group ID are set up as parameters. These values are
    then used to create a Kafka-based Spark stream called `rawDStream`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'I have again printed the stream event count for debug purposes, so that I know
    when the application is receiving and processing the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The HDSF location for the Kafka data has been defined as `/data/spark/kafka/rss/`.
    It has been mapped from the DStream into the variable lines. Using the `foreachRDD`
    method, a check on the data count is carried out on the `lines` variable, before
    saving the data into HDFS using the `saveAsTextFile` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the Scala script closes by starting the stream processing, and setting
    the application class to run until terminated with `awaitTermination`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'With all of the scripts explained and the Kafka CDH brokers running, it is
    time to examine the Kafka configuration, which if you remember is maintained by
    Apache ZooKeeper (all of the code samples that have been described so far will
    be released with the book). I will use the `zookeeper-client` tool, and connect
    to the `zookeeper` server on the host called `hc2r1m2` on the `2181` port. As
    you can see here, I have received a connected message from the `client` session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'If you remember, I specified the top level ZooKeeper directory for Kafka to
    be `/kafka`. If I examine this now via a client session, I can see the Kafka ZooKeeper
    structure. I will be interested in `brokers` (the CDH Kafka broker servers), and
    `consumers` (the previous Spark Scala code). The ZooKeeper `ls` commands show
    that the four Kafka servers have registered with ZooKeeper, and are listed by
    their `broker.id` configuration values one to four:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'I will create the topic that I want to use for this test using the Kafka script
    `kafka-topics` with a `create` flag. I do this manually, because I can demonstrate
    the definition of the data partitions while I do it. Note that I have set the
    partitions in the Kafka `topic rss` to five as shown in the following piece of
    code. Note also that the ZooKeeper connection string for the command has a comma-separated
    list of ZooKeeper servers, terminated by the top level ZooKeeper Kafka directory
    called `/kafka`. This means that the command puts the new topic in the proper
    place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when I use the ZooKeeper client to check the Kafka topic configuration,
    I can see the correct topic name, and the expected number of the partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'This describes the configuration for the Kafka broker servers in ZooKeeper,
    but what about the data consumers? Well, the following listing shows where the
    data will be held. Remember though, at this time, there is no consumer running,
    so it is not represented in ZooKeeper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: In order to start this test, I will run my Kafka data producer, and consumer
    scripts. I will also check the output of the Spark application class and need
    to check the Kafka partition offsets and HDFS to make sure that the data has arrived.
    This is quite complicated, so I will add a diagram here in the following figure
    to explain the test architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The Perl script called `rss.perl` will be used to provide a data source for
    a Kafka data producer, which will feed data into the CDH Kafka broker servers.
    The data will be stored in ZooKeeper, in the structure that has just been examined,
    under the top level node called `/kafka`. The Apache Spark Scala-based application
    will then act as a Kafka consumer, and read the data that it will store under
    HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: '![Kafka](img/B01989_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to try and explain the complexity here, I will also examine my method
    of running the Apache Spark class. It will be started via the `spark-submit` command.
    Remember again that all of these scripts will be released with this book, so that
    you can examine them in your own time. I always use scripts for server test management,
    so that I encapsulate complexity, and command execution is quickly repeatable.
    The script, `run_stream.bash`, is like many example scripts that have already
    been used in this chapter, and this book. It accepts a class name and the class
    parameters, and runs the class via spark-submit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'I then used a second script, which calls the `run_kafka_example.bash` script
    to execute the Kafka consumer code in the previous `stream6` application class.
    Note that this script sets up the full application class name—the broker server
    list. It also sets up the topic name, called `rss`, to use for data consumption.
    Finally, it defines a consumer group called `group1`. Remember that Kafka is a
    publish/subscribe message brokering system. There may be many producers and consumers
    organized by topic, group, and partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'So, I will start the Kafka consumer by running the `run_kafka_example.bash`
    script, which in turn will run the previous `stream6` Scala code using spark-submit.
    While monitoring Kafka data consumption using the script called `kafka_list.bash`,
    I was able to get the `kafka-consumer-offset-checker` script to list the Kafka-based
    topics, but for some reason, it will not check the correct path (under `/kafka`
    in ZooKeeper) when checking the offsets as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'By starting the Kafka producer rss feed using the script `kafka.bash`, I can
    now start feeding the rss-based data through Kafka into Spark, and then into HDFS.
    Periodically checking the `spark-submit` session output it can be seen that events
    are passing through the Spark-based Kafka DStream. The following output comes
    from the stream count in the Scala code, and shows that at that point, 28 events
    were processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'By checking HDFS under the `/data/spark/kafka/rss/` directory, via the Hadoop
    file system `ls` command, it can be seen that there is now data stored on HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'By checking the contents of this directory, it can be seen that an HDFS part
    data file exists, which should contain the RSS-based data from Reuters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Using the Hadoop file system `cat` command below, I can dump the contents of
    this HDFS-based file to check its contents. I have used the Linux `head` command
    to limit the data to save space. Clearly this is RSS Reuters science based information
    that the Perl script `rss.perl` has converted from XML to RSS JSON format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: This ends this Kafka example. It can be seen that Kafka brokers have been installed
    and configured. It shows that an RSS data-based Kafka producer has fed data into
    the brokers. It has been proved, using the ZooKeeper client, that the Kafka architecture,
    matching the brokers, topics, and partitions has been set up in ZooKeeper. Finally,
    it has been shown using the Apache Spark-based Scala code, in the `stream6` application,
    that the Kafka data has been consumed and saved to HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I could have provided streaming examples for systems like Kinesis, as well as
    queuing systems, but there was not room in this chapter. Twitter streaming has
    been examined by example in the checkpointing section.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has provided practical examples of data recovery via checkpointing
    in Spark streaming. It has also touched on the performance limitations of checkpointing
    and shown that that the checkpointing interval should be set at five to ten times
    the Spark stream batch interval. Checkpointing provides a stream-based recovery
    mechanism in the case of Spark application failure.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has provided some stream-based worked examples for TCP, File, Flume,
    and Kafka-based Spark stream coding. All the examples here are based on Scala,
    and are compiled with `sbt`. All of the code will be released with this book.
    Where the example architecture has become over-complicated, I have provided an
    architecture diagram (I'm thinking of the Kafka example here).
  prefs: []
  type: TYPE_NORMAL
- en: It is clear to me that the Apache Spark streaming module contains a rich source
    of functionality that should meet most of your needs, and will grow as future
    releases of Spark are delivered. Remember to check the Apache Spark website ([http://spark.apache.org/](http://spark.apache.org/)),
    and join the Spark user list via `<[user@spark.apache.org](mailto:user@spark.apache.org)>`.
    Don't be afraid to ask questions, or make mistakes, as it seems to me that mistakes
    teach more than success.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will examine the Spark SQL module, and will provide worked
    examples of SQL, data frames, and accessing Hive among other topics.
  prefs: []
  type: TYPE_NORMAL
