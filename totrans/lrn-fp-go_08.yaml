- en: Increasing Performance Using Pipelining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, we feel the need to work on some data and pass it along a series of steps,
    transforming it along the way before it arrives at its destination. We come across
    these sort of processes occurring in real-life scenarios, especially in factory
    assembly line environments.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will see how the pipeline patterns can be used to build
    component-based applications. We'll see how we can use function composition data
    flow programming techniques to create flexible solutions that are not only robust,
    but also performant in today's distributed processing environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal in this chapter is to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Be able to identify when to use the pipeline pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to build a pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand how we can leverage buffering to increase throughput
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Goroutines and channels to process data faster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve API readability using interfaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement useful filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a flexible pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See what happens when you change the order of filters and submit invalid data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the pipeline pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pipeline software design pattern is used in cases where data flows through
    a sequence of stages where the output of the previous stage is the input of the
    next. Each step can be thought of as a filter operation that transforms the data
    in some way. Buffering is frequently implemented between filters to prevent deadlock
    or data loss when one filter runs faster than another filter connected to it.
    Connecting the filters into a pipeline is analogous to function composition.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts the flow of data from a data source, for example,
    a file. The data is transformed as it passes from one filter to the next, until
    the result is finally displayed on standard out in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8ba339a-0300-4012-8a27-280779c6fc81.png)'
  prefs: []
  type: TYPE_IMG
- en: Grep sort example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `/etc/group` file is the data source. Grep is the first filter whose input
    is all the lines from the `/etc/group` file. The `grep` command removes all lines
    that do not begin with `"com"`, and then sends its output to the Unix pipe, which
    sends that data to the `sort` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let's be clear. What we're covering in this chapter behaves like Unix pipes,
    but what we'll study are pipelines that are implemented in Go, mainly using Go
    channels and Goroutines. Similarly, we will not discuss Go Pipes ([https://golang.org/pkg/os/#Pipe](https://golang.org/pkg/os/#Pipe))
    other than to say that they are unbuffered, unstructured streams of bytes.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline characteristics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The pipeline pattern affords a number of valuable benefits that are desirable
    in properly engineered applications; these benefits are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Provides the structure for a system that processes data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Divides tasks into sequential steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encapsulates each step in a filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Independent filters (run in isolation) with a set of inputs and outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data passes through a pipeline in one direction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configurable modularity (read, write, split, and merge operations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High cohesion, where filter logic is self-contained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low coupling, where filters communicate through connecting pipes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distinction between batch and online processing disappears
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pipeline pattern has many characteristics that make it appealing for a variety
    of use cases. We see it in use in technologies ranging from constant integration
    and deployment pipelines, to batch and stream data processing. If there is a need
    to handle the flow of data in an assembly line fashion, then we should consider
    using this pipeline pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extensibility**: Add another filter to the pipeline'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: Function composition by connecting filters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: Utilizes multi-processor systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testability**: Easy to analyze, evaluate, and test pipe filter systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As with any pattern, we must consider its potential issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Potential data transformation overhead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential deadlock and buffer overflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential reliability issues if infrastructure loses the data flowing between
    filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential reprocessing of data if a filter fails after sending results downstream,
    but before indicating that processing was successfully completed (design filters
    in a pipeline to be idempotent)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potentially large context, since each filter must be provided with sufficient
    context to perform its work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some high-level use cases, which if applicable, make this pipeline
    pattern an attractive design solution candidate:'
  prefs: []
  type: TYPE_NORMAL
- en: Processing requirements can be decomposed into a set of independent steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter operations can take advantage of multi-core processors or distributed
    computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each filter has different scalability requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A system that must accommodate reordering of processing steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's look at some examples to help appreciate the value and applicability
    of this pipeline pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Website order processing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following diagram depicts the flow of an order from the website that displays
    the order form to the user. The filters along the way perform various tasks, such
    as decrypting the request payload, authenticating the user credentials, charging
    the customer''s credit card, sending the customer a confirmation email, and finally,
    displaying the thank you page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3164bd3c-f56b-47f9-a36d-43f5ca76cd34.png)'
  prefs: []
  type: TYPE_IMG
- en: Boss worker pattern
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the boss worker pattern, the **Boss** filter pushes data down to the workers
    that process the data and merge the results into the **Product**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e2256fb-62d4-484e-a677-2699938dbd25.png)'
  prefs: []
  type: TYPE_IMG
- en: Load balancer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following example shows a **Load Balancer** that takes requests from clients
    and sends them to the server that has the smallest backlog and is most available
    to handle the request information packet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/641536af-3cf2-4026-8356-fffa971bad36.png)'
  prefs: []
  type: TYPE_IMG
- en: Data flow types
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The data flow types can be viewed as **Read**, **Split**, **Merge**, and **Write**
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Filter type** | **Image** | **Receive** | **Send** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| **Read** | ![](img/581df9d3-ea69-4bba-b5fd-98816201ed16.png) |  | ![](img/e065c36e-7b8c-472b-b2df-22c69774644d.png)
    | A **Read** filter reads data from the data source and sends the information
    packet downstream. |'
  prefs: []
  type: TYPE_TB
- en: '| **Split** | ![](img/beb61560-0dff-4ba2-9537-2ed634a6c356.png) | ![](img/e065c36e-7b8c-472b-b2df-22c69774644d.png)
    | ![](img/e065c36e-7b8c-472b-b2df-22c69774644d.png)![](img/e065c36e-7b8c-472b-b2df-22c69774644d.png)
    | Multiple functions read from the same channel until that channel is closed.
    It improves the performance by distributing work among a group of workers to parallelize
    CPU usage. |'
  prefs: []
  type: TYPE_TB
- en: '| **Transform** | ![](img/906ea89c-b1e8-400a-9abf-f37907bf6d16.png) | ![](img/e065c36e-7b8c-472b-b2df-22c69774644d.png)
    | ![](img/e065c36e-7b8c-472b-b2df-22c69774644d.png) | This filter receives data
    from upstream, transforms it, and sends it downstream. |'
  prefs: []
  type: TYPE_TB
- en: '| **Merge** | ![](img/b3515be9-b909-4b4a-a4bd-f256d0697213.png) | ![](img/e065c36e-7b8c-472b-b2df-22c69774644d.png)![](img/e065c36e-7b8c-472b-b2df-22c69774644d.png)
    | ![](img/e065c36e-7b8c-472b-b2df-22c69774644d.png) | This function reads from
    multiple input channels onto a single channel that''s closed when all the inputs
    are closed. Work can be distributed to multiple Goroutines that all read from
    the same input channel. |'
  prefs: []
  type: TYPE_TB
- en: '| **Write** | ![](img/9f22ce75-8545-450e-a029-0f3e09e1cb25.png) | ![](img/e065c36e-7b8c-472b-b2df-22c69774644d.png)
    |  | This filter receives data from upstream and writes it to the sink.  |'
  prefs: []
  type: TYPE_TB
- en: Building blocks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'These are the basic building blocks of a flow-based programming system. With
    these basic operations, we can build any component-based system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b2a5616-80dd-4653-b1e9-cc4658fbf4c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Flow-based programming is a component-based programming model that defines applications
    as a network of asynchronous processing operations (aka filters) that exchange
    streams ([https://en.wikipedia.org/wiki/Stream_(computing)](https://en.wikipedia.org/wiki/Stream_(computing))) of
    structured information packets with defined lifetimes, named ports, and separate
    definitions of connections.
  prefs: []
  type: TYPE_NORMAL
- en: Generalized business application design
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following diagram depicts the component composition diagram for a generalized
    business application that processes input requests and routes the requests to
    backend servers. Responses from the servers are subsequently handled, processed,
    and returned. A few alternate data flows exist for responses that need to be re-routed
    or re-processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74baa648-b3ea-4ec7-9eb3-ac89c57c0342.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that each operation can be swapped, as long as its input and output sets
    are identical, without impacting the flow of data or overall operation of the
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Example implementations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we see the value in the pipeline pattern, let's start planning a Go
    implementation of one.
  prefs: []
  type: TYPE_NORMAL
- en: In Go, pipelines are implemented using a series of stages connected by Go channels.
    A Go pipeline begins with a data source (aka producer), has stages that are connected
    via channels, and ends with a data sink (aka consumer).
  prefs: []
  type: TYPE_NORMAL
- en: The data source can be a generator function that sends data to the first stage
    and then closes the initial outbound channel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each filter (step or stage) in the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Consists of one or more Goroutines that run the same function (aka filter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Receives upstream data via one or more inbound channels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforms the data in some way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sends data downstream via one or more outbound channels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Closes its outbound channels when all the send operations are completed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeps receiving values from inbound channels until those channels are closed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example transformer functions include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Accumulator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delta (to calculate the change between two sample data points of a resource)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arithmetic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example data sinks include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: File storage (for example, NFS and CIFS/SMB protocol access to NAS or DAS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Message broker (for example, Kafka, NATS, and RabbitMQ)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database (for example, PostgreSQL, MongoDB, and DynamoDB)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud storage (for example, S3, OpenStack Swift and Ceph)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imperative implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's start our coding examples with the simplest form of a pipeline, which
    of course is implemented using the imperative style of programming.
  prefs: []
  type: TYPE_NORMAL
- en: Decrypt, authenticate, charge flow diagram
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We''ll base our coding examples on the following flow diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae1c9d2f-d295-494b-8b17-ad4d5d8c8567.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We''ll be passing order data from stage to stage until the entire process has
    been completed. The order data can be transformed along the way, for example,
    when the **Decrypt** step converts the credit card number into plain text. We''ll
    refer to each stage or step as a filter. In our example, each filter will receive
    one order from the upstream and send one order downstream. The flow is unidirectional.
    It starts at the data source and moves to the **Decrypt** filter, then to the
    **Authenticate** filter, and ends in the **Charge Credit Card** filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll import the `go_currency` package, which will help us handle the prices
    in the order line items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `GetOrders()` function will be our order generating data source. Note that
    the credit card numbers are stored in an encrypted format. We''ll need to decrypt
    them later in order to charge the credit card:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that our credit card number is encrypted and the last field is a slice
    of `LineItem` structs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In our example, we'll only process two orders. We return them from the `GetOrders()`
    function as a slice of the `Order` structs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We call the `GetOrder()` function to generate our orders. Next, we range over
    our orders, running each one in turn through our order processing pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Our pipeline has three steps. Each step is a function that we''ll refer to
    as a filter. There are three sequential filters that our order runs through as
    it is processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we''re starting with the simplest example possible, in each filter is
    output which filter action is occurring and we pass the order along, in this simple
    example without transforming it in any way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is the basic idea of a pipeline. We take a data packet, for example, an
    order, and pass it from step to step, where each step is a filter function with
    a specific speciality. The data can be transformed along the way and travels in
    one direction from the data source to the sink, which ends the process.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to increase performance, we should consider running things concurrently.
    Go has a few concurrency constructs that we can use: Goroutines and channels.
    Let''s give that a try:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We created an input channel and an output channel for our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we created an immediately executable Goroutine function. Note the open/close
    parenthesis at the end of the Goroutine block: `}()` . This Goroutine won''t exit
    until we close the input channel in the last line of our main function.'
  prefs: []
  type: TYPE_NORMAL
- en: We generate an order, just as in our imperative example. Then, we process each
    order by passing the next order to the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The output is identical to the imperative example and it runs slower. So, we
    have reduced performance and increased code complexity. We can do better.
  prefs: []
  type: TYPE_NORMAL
- en: Buffered implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's try using input/output buffers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, each stage of the pipeline reads from its input buffer
    and writes to its output buffer. For example, the **Decrypt** filter reads from
    its instream buffer, coming from the data source and writes its output buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ddbc0de-b007-4794-ab6b-88901d9a9605.png)'
  prefs: []
  type: TYPE_IMG
- en: Since there are two orders, the buffer size is two. Since concurrent queues'
    buffer shared inputs and outputs, if we had four orders, then all filters in the
    pipeline could execute at the same time. If we had four CPU cores available, then
    all filters could run concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: As long as there is room in its output buffer, a stage of the pipeline can add
    the value it produces to its output queue. If the output buffer is full, the producer
    of the new value waits until space becomes available.
  prefs: []
  type: TYPE_NORMAL
- en: Filters can block, waiting for orders to arrive in its instream buffer or until
    its input channel has been closed.
  prefs: []
  type: TYPE_NORMAL
- en: Buffers can be effectively used that hold more than one order at a time and
    this can compensate for variability in the time it takes each filter to process
    each order.
  prefs: []
  type: TYPE_NORMAL
- en: In the best case scenario, each filter along the pipeline would process its
    input order in about the same time as the other filters. However, if the **Decrypt**
    filter takes substantially longer to process an order than the **Authenticate**
    filter, the **Authenticate** filter will block, waiting on **Decrypt** to send
    the decrypted order into its input buffer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how we would modify our program to include buffered channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This is great, right? We increased performance by adding buffered channels.
    Our solution runs filters concurrently on multiple cores at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: That's great, but what if we process a large number of orders?
  prefs: []
  type: TYPE_NORMAL
- en: Leverage all CPU cores
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We could increase the number of buffers by the number of CPU cores available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The use of I/O buffers is an improvement on our design, but there is actually
    a better solution.
  prefs: []
  type: TYPE_NORMAL
- en: Improved implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s take another look at our order processing pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/050180d4-ba4a-40a3-bc59-4d2f2f328d4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's implement the **Decrypt, Authenticate**, and **Charge Credit Card** filters
    with a closer to real life example.
  prefs: []
  type: TYPE_NORMAL
- en: The `Order` and `LineItem` structs will remain the same and so will the `GetOrders()`
    generator.
  prefs: []
  type: TYPE_NORMAL
- en: Imports
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We have more imports. We''ll use `go_utils` for its `Dashes` function to anonymize
    the credit card number. Also, we''ll import a number of `crypto` packages for
    decrypting the credit card number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: BuildPipeline
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We have a new function, `BuildPipeline()`, which takes a list of filters and
    connects them using each filter''s input and output channels. The `BuildPipeline()`
    function lays the pipe, starting with the data source and ending with the sink,
    that is, the `Charge` filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Immediately executable Goroutine
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next, is the immediately executable Goroutine that iterates over the orders
    that it generates and sends each order to the input of that filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: When all of the orders have been sent into the pipeline, it's time to close
    the pipeline's input channel.
  prefs: []
  type: TYPE_NORMAL
- en: Receive order
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next, we execute the pipeline''s `Receive()` function to wait for the orders
    to arrive on the output channel, and then we print out the order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Filterer interface
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our pipeline API is constructed around the `Filterer` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: A Filterer object
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A Filterer object has one method, `Filter`, which has an input channel of type
    `Order` and returns an output channel of type `Order`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/524bf1d3-8f54-4431-acd1-da49e4472a9b.png)'
  prefs: []
  type: TYPE_IMG
- en: We define types to act as receivers of `Filter` executions. The first filter
    encountered in the pipeline is the Authenticate filter. The following Authenticate
    filter has a single input parameter of type `Order` channel and it returns a single
    value of type `Order` channel.
  prefs: []
  type: TYPE_NORMAL
- en: Authenticate filter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our authentication logic is hardcoded and simple, that is, not what I''d call
    production ready. The password `secret` will work for any username. If `Authenticate`
    encounters `secret` in the `Credentials` field, the order will flow unchanged
    to the next step in the pipeline. However, if the password is not `secret`, then
    the order''s `isValid` field will be set to `false`. The behavior or subsequent
    filters in the pipeline can be affected by this value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Decrypt filter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following `Decrypt` filter has a single input parameter of type `Order`
    channel and it returns a single value of type `Order` channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note that we handle errors by logging the error. Even though we are told that
    the `IsDecrypted` field value is always false when it arrives from the source,
    we play it safe and set `order.IsDecrypted = false` if we encounter an error.
  prefs: []
  type: TYPE_NORMAL
- en: We only process this order if the order is valid. The order can be invalid if
    the decrypt function fails, refer to the the preceding code. The order can also
    be invalidated in a previous step in the flow, for example, if the order's `Authenticate`
    filter failed.
  prefs: []
  type: TYPE_NORMAL
- en: Complete processing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'When this filter''s processing is complete, we close its output channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The ChargeCard helper function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The `ChargeCard` function is a helper function used by the `Charge` filter
    to charge the credit card number found in the order. This implementation simply
    prints that the credit card was charged. It''s a good placeholder for a real charge
    credit card logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Charge filter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Like all the other filters in the API, `Charge` accepts an input channel of
    type `Order` and returns an output channel of type `Order`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the order is valid, then we initialize the total to $0.00 using the  `total
    := gc.USD{0, 0}`  statement and iterate over the order''s line items, executing
    the `Add` function to arrive at the order''s total amount. We then pass that amount
    to the `ChargeCard` helper function to collect our money:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The encrypt and decrypt helper functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `decrypt` helper function in the following code is used by the `Decrypt`
    filter. We also have the `encrypt` helper function, though not in our pipeline,
    can be nice to have, to encrypt plain text and for testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `decrypt` function accepts the encrypted string value. The `aes.NewCipher`
    accepts our 32-byte long AES encryption key and returns an AES-256 cipher block,
    which is passed to `NewCBCDecrypter`. The `NewCBCDecrypter` function also accepts
    an initialization vector (`iv`), which it uses to decrypt the block in cipher
    block chaining mode. Its `CryptBlocks` function is used to decrypt the value,
    and `RightTrim` is used to slice off the trailing `\x00`. Voila! we''ve got our
    decrypted string value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Testing how the application handles invalid data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's see how our application handles bad data.
  prefs: []
  type: TYPE_NORMAL
- en: Invalid credit card cipher text
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Note the XXX that has been appended to the encrypted credit card number value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The order that had the invalid credit card number was not fully processed. Note
    the error message in the log.
  prefs: []
  type: TYPE_NORMAL
- en: Invalid password
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Note the XXX that has been appended to the credentials field value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The order that had the invalid password was not fully processed. Note the error
    message in the log.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the order of authenticate and decrypt filters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Previously, the order was `Decrypt{},Authenticate{}, Charge{}`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: There was difference. In both cases, both invoices were fully processed.
  prefs: []
  type: TYPE_NORMAL
- en: Attempting to charge before decrypting credit card number and authentication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We start by building our pipeline of functions: Charge,Decrypt and Authenticate.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Attempting to charge before authentication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'No surprise here either. If we attempt to charge the credit card before we
    authenticate the request, the charge will not be processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Further reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An entire book could be written on the topic of the pipeline pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the topics not covered in this chapter, but you should research on
    your own, include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing and implementing the `Split` and `Merge` filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how the `sync.WaitGroup` type helps you manage synchronization
    of channel communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add branching and conditional workflow patterns to the pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Good reads: *Go Concurrency Patterns: Pipelines and cancellation* ([https://blog.golang.org/pipelines](https://blog.golang.org/pipelines)) and *Go
    by Example: Channels* ([https://gobyexample.com/channels](https://gobyexample.com/channels))'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building applications that have high cohesion and low coupling is a major goal
    in software engineering. In this chapter, we explored the pipeline pattern and
    you learned how to build component-based systems using **flow-based programming**
    (**FPB**) techniques. We studied FPB patterns and use cases that would benefit
    from applying the pipeline pattern.
  prefs: []
  type: TYPE_NORMAL
- en: We studied an example order processing flow. We progressed from an imperative
    implementation to a concurrent one using Goroutines and channels. We learned how
    I/O buffers can effectively be used to hold more than one order at a time and
    how this can compensate for variability in the time it takes each filter to process
    each order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our last implementation was an improvement upon the prior attempts. We created
    an elegant API based on the `Filterer` interface. We were able to define and control
    our entire order processing flow with this one command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, we implemented various FPB error handling techniques and tested their
    effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we''ll look at another technique used to improve performance:
    being lazy.'
  prefs: []
  type: TYPE_NORMAL
