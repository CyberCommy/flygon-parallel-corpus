- en: Using GlusterFS on the Cloud Infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With a good understanding of the core concepts of GlusterFS, we can now dive
    into the installation, configuration, and optimization of a storage cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We will be installing GlusterFS on a three-node cluster using Azure as the cloud
    provider for this example. However, the concepts can also be applied to other
    cloud providers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring GlusterFS backend storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and configuring GlusterFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up volumes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s the list of technical resources for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A detailed view of Azure **virtual machine** (**VM**) sizes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-storage](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-storage)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A detailed view of Azure disk sizes and types:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://azure.microsoft.com/en-us/pricing/details/managed-disks/](https://azure.microsoft.com/en-us/pricing/details/managed-disks/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main page for the ZFS on Linux project:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/zfsonlinux/zfs/wiki/RHEL-and-CentOS](https://github.com/zfsonlinux/zfs/wiki/RHEL-and-CentOS)'
  prefs: []
  type: TYPE_NORMAL
- en: 'GlusterFS installation guide for CentOS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://wiki.centos.org/HowTos/GlusterFSonCentOS](https://wiki.centos.org/HowTos/GlusterFSonCentOS)'
  prefs: []
  type: TYPE_NORMAL
- en: 'GlusterFS quick start guide on the Gluster website:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.gluster.org/en/latest/Quick-Start-Guide/Quickstart/](https://docs.gluster.org/en/latest/Quick-Start-Guide/Quickstart/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'GlusterFS setting up volumes on the administrators guide:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/](https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'GlusterFS tuning volumes for better performance:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.gluster.org/en/latest/Administrator%20Guide/Managing%20Volumes/#tuning-options](https://docs.gluster.org/en/latest/Administrator%20Guide/Managing%20Volumes/#tuning-options)'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the bricks used for backend storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the list of components that we''ll be using:'
  prefs: []
  type: TYPE_NORMAL
- en: Azure L4s VM with 4vCPUs and 32 GB of RAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Four S10 128 GB Disks per VM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CentOS 7.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZFS on Linux as the filesystem for the bricks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A single RAID 0 group with four disks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GlusterFS 4.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before going into the details of how to configure the bricks, we first need
    to deploy the nodes in Azure. For this example, we are using the storage optimized
    VM series, or L-series. One thing that is worth mentioning is that Azure has a
    30-day free trial that can be used for testing before committing to any deployment.
  prefs: []
  type: TYPE_NORMAL
- en: In Azure, performance is defined on several levels. The first level is the VM
    limit, which is the maximum performance that the VM allows. The L-series family
    provides the correct balance of price versus performance as these VMs are optimized
    to deliver higher **input/output operations per second** (**IOPS**)and throughput
    rather than delivering high compute or memory resources. The second level on which
    performance is defined is through the disks that are attached to the VM. For this
    example, we will be using standard **hard disk drives** (**HDD**) for a cost-effective
    solution. If more performance is needed, the disks can always be migrated to premium
    **solid-state drives** (**SSD**) storage.
  prefs: []
  type: TYPE_NORMAL
- en: The exact VM size for this example will be L4s, which provides four vCPUs and
    32 GB of RAM, and is enough for a small storage cluster for general purposes.
    With a maximum of 125 MB/s and 5k IOPS, it still retains respectable performance
    when correctly configured.
  prefs: []
  type: TYPE_NORMAL
- en: A new generation of storage optimized VMs has been recently released, offering
    a locally-accessible NVMe SSD of 2 TB. Additionally, it provides increased core
    count and memory, making these new VMs ideal for a GlusterFS setup with **Z file
    system** (**ZFS**). The new L8s_v2 VM can be used for this specific setup, and
    the sizes and specifications can be seen on the product page ([https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-storage#lsv2-series](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-storage#lsv2-series)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the Availability set, Current fault domain,
    and Current update domain settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b322180-550e-4559-ba84-4edffa7ad8cd.png)'
  prefs: []
  type: TYPE_IMG
- en: When deploying a GlusterFS setup in Azure, make sure that each node lands on
    a different update and fault domain. This is done through the use of availability
    sets (refer to the preceding screenshot). Doing so ensures that if the platform
    restarts a node, the others remain up and serving data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, for the Azure setup, we need **512 GB** per node for a total of 1.5
    TB raw, or 1 TB usable space. The most cost-effective way to achieve this is by
    using a single **S20 512 GB** disk, since the price per gigabyte per month is
    approximately **$0.04**. Going down the route of a single disk will impact on
    performance, as a single standard disk only provides a maximum of 500 IOPS and
    60 MB/s. Considering performance and accepting the fact that we will lose a bit
    of efficiency in the cost department, we will be using four **S10 128** GB disks
    in a single RAID0 group. The price per month per gigabyte of an **S10** disk is
    **$0.05**, compared to **$0.04** per month for an **S20** disk. You can refer
    to the following table, where the calculation is done based on the cost of the
    managed disk divided by its respective size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bf1ef53-b1a8-4381-8d20-0db862ac1bc6.png)'
  prefs: []
  type: TYPE_IMG
- en: Make sure that all three nodes are deployed on the same region and the same
    resource group for consistency.
  prefs: []
  type: TYPE_NORMAL
- en: ZFS as the backend for the bricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We spoke about ZFS in a [Chapter 3](cb87a072-b90e-4e96-9ba1-54424c2a0231.xhtml), *Architecting
    a Storage Cluster*. ZFS is a filesystem that was developed by Sun Microsystems
    and was later acquired by Oracle. The project was later made open source and was
    ported to Linux. Although the project is still in beta, most of the features work
    fine and the majority of the problems have been ruled out—the project is now focused
    on adding new features.
  prefs: []
  type: TYPE_NORMAL
- en: ZFS is a software layer that combines disk management, logical volumes, and
    a filesystem all in one. Advanced features such as compression, **adaptive replacement
    cache** (**ARC**), deduplication, and snapshots make it ideal to work with GlusterFS
    as the backend for the bricks.
  prefs: []
  type: TYPE_NORMAL
- en: Installing ZFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start by installing ZFS; there are some dependencies, such as **dynamic
    kernel modules** (**DKMS**), that live in the EPEL repository.
  prefs: []
  type: TYPE_NORMAL
- en: Note that most of the commands that run here are assumed to be running as root;
    the commands can be run as the non-root account by prefacing `sudo` before each.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the required components, we can use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following commands are used to enable the ZFS components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Configuring the zpools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With ZFS installed and enabled, we can now create the zpools. Zpool is the name
    given to volumes that are created within ZFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we will be using a single RAID 0 group consisting of four disks, we can
    create a zpool named `brick1`; this needs to be done on all three nodes. Additionally,
    let''s create a directory named `bricks` that lives under the root (`/`); this
    directory houses the bricks under a directory with the brick name. The command
    required to do this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates the directory tree, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To further explain the command, `brick1` is the name of the zpool. Then, we
    indicate the path to the disks. In this example, we are using the ID of the disks
    since this avoids problems if the disks change order. While ZFS is not affected
    for disks in a different order, it is better to avoid problems by using an ID
    that will never change.
  prefs: []
  type: TYPE_NORMAL
- en: ZFS can use the entire disk because it creates the required partitions automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the `zpool` instance created, we can check whether it has completed correctly
    by using the `zpool status` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c86f5d0e-9ca7-4f0d-ad2c-804cbee4ff84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s enable compression and change the mount point of the pool to the previously
    created directory. To do this, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You will also need to run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The first command enables compression with the `lz4` algorithm, which has a
    low CPU overhead. The second command changes the mount point of the zpool. Make
    sure that you use the correct name of the pool when changing the settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'After doing this, we should have the ZFS volume mounted under `/bricks/brick1`,
    as shown in the `df` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8102aba-efc0-42ba-95a7-62420a947695.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We need to create a directory on the recently added mount point to use as the
    brick; the consensus is to use the name of the volume. In this case, we''ll name
    the volume `gvol1`, and simply create the directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This needs to be done on all the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the ZFS cache to the pool (optional)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With Azure, every single VM has a temporary resource drive. The performance
    of this temporary resource drive is considerably higher than the data disks that
    are added to it. This drive is ephemeral, meaning the data is wiped once the VM
    is deallocated; this should work very well as a read cache drive since there is
    no need to persistently keep the data across reboots.
  prefs: []
  type: TYPE_NORMAL
- en: Since the drive is wiped with every `stop/deallocate/start` cycle, we need to
    tweak some things with the unit files for ZFS to allow the disk to be added on
    every reboot. The drive will always be `/dev/sdb`, and since there is no need
    to create a partition on it, we can simply tell ZFS to add it as a new disk each
    time the system boots.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be achieved by editing the `systemd` unit for `zfs-mount.service`,
    which is located under `/usr/lib/systemd/system/zfs-mount.service`. The problem
    with this approach is that the ZFS updates will overwrite the changes made to
    the preceding unit. One solution to this problem is to run `sudo systemctl edit
    zfs-mount` and add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply the changes, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have ensured that the cache drive will be added after every reboot,
    we need to change an Azure-specific configuration with the Linux agent that runs
    on Azure VMs. This agent is in charge of creating the temporary resource drive,
    and since we''ll be using it for another purpose, we need to tell the agent not
    to create the ephemeral disk. To achieve this, we need to edit the file located
    in `/etc/waagent.conf` and look for the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You will then need to change it to the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After doing this, we can add the cache drive to the pool by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `-f` option must only be used the first time because it removes the previously
    created filesystem. Note that the `stop/deallocate/start` cycle of the VM is required
    to stop the agent from formatting the resource disk, as it gets an `ext4` filesystem
    by default.
  prefs: []
  type: TYPE_NORMAL
- en: The previous process can also be applied to the newer Ls_v2 VMs, which use the
    much faster NVMe drives, such as the L8s_v2; simply replace `/dev /sdb` with `/dev/nvme0n1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can verify that the cache disk was added as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/abbd1b75-a9b2-4338-92ea-3291dfa89a29.png)'
  prefs: []
  type: TYPE_IMG
- en: As we'll be using a single RAID group, this will be used as a read cache for
    the entire brick, allowing better performance when reading the files of the GlusterFS
    volume.
  prefs: []
  type: TYPE_NORMAL
- en: Installing GlusterFS on the nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With each node having the bricks already configured, we can finally install
    GlusterFS. The installation is relatively straightforward and requires just a
    couple of commands.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll be using the packages provided by CentOS. To install GlusterFS, we first
    install the repository as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we install the `glusterfs-server` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We then make sure the `glusterd` service is enabled and started:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c41c81ee-4ef5-449c-8fe0-53dbf9fc8167.png)'
  prefs: []
  type: TYPE_IMG
- en: These commands need to be run on each of the nodes that will be part of the
    cluster; this is because each node requires the packages and services to be enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the trusted pool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we need to create a trusted pool. A trusted pool is a list of nodes
    that will be part of the cluster, where each Gluster node trusts the other, thus
    allowing for the creation of volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the trusted pool, run the following code from the first node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify that the nodes show up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/469305e9-615d-45a6-9031-742db5231517.png)'
  prefs: []
  type: TYPE_IMG
- en: The command can be run from any node, and the hostnames or IPs need to be modified
    to include the others. In this case, I have added the IP addresses of each of
    the nodes onto the `/etc/hosts` file to allow for easy configuration. Ideally,
    the hostnames should be registered with the DNS server for the name resolution
    within the network.
  prefs: []
  type: TYPE_NORMAL
- en: After the installation, the `gluster` nodes should allow volumes to be created.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now reached the point where we can create the volumes; this is because
    we have the bricks configured and the necessary packages for GlusterFS to work.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a dispersed volume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll be using a dispersed volume type across three nodes, giving a good balance
    of high availability and performance. The raw space of all of the nodes combined
    will be around 1.5 TB; however, the distributed volume will have a usable space
    of approximately 1 TB.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a dispersed volume, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, start the volume using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure that it starts correctly by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The volume should show up now as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a87c65a6-747f-4151-92ce-23dc2caf1991.png)'
  prefs: []
  type: TYPE_IMG
- en: Mounting the volume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The volume is now created and can be mounted on the clients; the preferred method
    for doing this is by using the native `glusterfs-fuse` client, which allows for
    automatic failovers in the event that one of the nodes goes down.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the `gluster-fuse` client, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s create a directory under root called `gvol1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can mount the GlusterFS volume on the client as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: It doesn't matter which node you specify, as the volume can be accessed from
    any of them. In the event that one of the nodes goes down, the client will automatically
    redirect I/O requests to the remaining nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the volume created and mounted, we can tweak some parameters to get the
    best performance. Mainly, performance tuning can be done on the filesystem level
    (in this case, ZFS), and on the GlusterFS volume level.
  prefs: []
  type: TYPE_NORMAL
- en: GlusterFS tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, the main variable is `performance.cache-size`. This setting specifies
    the amount of RAM to be allocated as a read cache for the GlusterFS volume. By
    default, it is set to 32 MB, which is fairly low. Given that the selected VM has
    enough RAM, this can be bumped to 4 GB using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Another essential parameter once the cluster starts growing is `performance.io-thread-count`.
    This controls how many I/O threads are spawned by the volume. The default is `16`
    threads, which are enough for small-to-medium clusters. However, once the cluster
    size starts growing, this can be doubled. To change the setting, use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This setting should be tested to check whether increasing the count improves
    the performance or not.
  prefs: []
  type: TYPE_NORMAL
- en: ZFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll be primarily changing two settings: ARC and the L2ARC feed performance.'
  prefs: []
  type: TYPE_NORMAL
- en: ARC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The primary setting for ZFS is its read cache, called ARC. Allowing more RAM
    to be allocated to ZFS increases read performance substantially. Since we have
    already allocated 4 GB to the Gluster volume read cache and the VM has 32 GB available,
    we can allocate 26 GB of RAM to ZFS, which will leave approximately 2 GB for the
    OS.
  prefs: []
  type: TYPE_NORMAL
- en: 'To change the maximum size that is allowed for ARC, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the number is the amount of RAM in bytes, in this case, 26 GB. Doing
    this changes the setting on the fly but does not make it boot persistent. To have
    the settings applied on boot, create a file named `/etc/modprobe.d/zfs.conf` and
    add the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: By doing this, you can make the changes persist across boots.
  prefs: []
  type: TYPE_NORMAL
- en: L2ARC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'L2ARC refers to a second level of read cache; this is the cache disk that was
    previously added to the zpools. Changing the speed in which data is fed to the
    cache helps by decreasing the amount of time it takes to warm or fill up the cache
    with constantly accessed files. The setting is specified in bytes per second.
    To change it you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As with the previous setting, this is applied to the running kernel. To make
    it boot-persistent, add the following line to the `/etc/modprobe.d/zfs.conf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This setting allows a maximum of 256 MB/s of L2ARC feed; the setting should
    be increased to at least double if the VM size is changed to a higher tier.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, you should end up with a file on each node that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1e838b2-e494-45bf-a92e-b0fa4e54513d.png)'
  prefs: []
  type: TYPE_IMG
- en: Regarding ZFS, on other types of filesystems, changing the block size helps
    to gain some performance. ZFS has a variable block size, allowing for small and
    big files to achieve similar results, so there is no need to change this setting.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After installing ZFS, creating the zpools, installing GlusterFS, and creating
    the volumes, we have ended up with a solution with respectable performance that
    can sustain a node failure and still serve data to its clients.
  prefs: []
  type: TYPE_NORMAL
- en: For the setup, we used Azure as the cloud provider. While each provider has
    their own set of configuration challenges, the core concepts can be used on other
    cloud providers as well.
  prefs: []
  type: TYPE_NORMAL
- en: However, this design has a disadvantage. When adding new disks to the zpools,
    the stripes don't align, causing new reads and writes to yield lower performance.
    This problem can be avoided by adding an entire set of disks at once; lower read
    performance is mostly covered by the read cache on RAM (ARC) and the cache disk
    (L2ARC).
  prefs: []
  type: TYPE_NORMAL
- en: For GlusterFS, we used a dispersed layout that balances performance with high
    availability. In this three-node cluster setup, we can sustain a node failure
    without holding I/O from the clients.
  prefs: []
  type: TYPE_NORMAL
- en: The main takeaway is to have a critical mindset when designing a solution. In
    this example, we worked with the resources that we had available to achieve a
    configuration that would perform to specification and utilize what we provided.
    Make sure that you always ask yourself how this setting will impact the result,
    and how you can change it to be more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll go through testing and validating the performance
    of the setup.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are GlusterFS bricks?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is ZFS?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a zpool?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a cache disk?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is GlusterFS installed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a trusted pool?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is a GlusterFS volume created?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is performance.cache-size?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is ARC?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Learning Microsoft Azure* by Geoff Webber-Cross: [https://www.packtpub.com/networking-and-servers/learning-microsoft-azure](https://www.packtpub.com/networking-and-servers/learning-microsoft-azure)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Implementing Azure Solutions* by Florian Klaffenbach, Jan-Henrik Damaschke,
    and Oliver Michalski: [https://www.packtpub.com/virtualization-and-cloud/implementing-azure-solutions](https://www.packtpub.com/virtualization-and-cloud/implementing-azure-solutions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Azure for Architects* by Ritesh Modi: [https://www.packtpub.com/virtualization-and-cloud/azure-architects](https://www.packtpub.com/virtualization-and-cloud/azure-architects)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
