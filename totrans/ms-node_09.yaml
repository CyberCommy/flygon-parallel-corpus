- en: Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let everyone sweep in front of his own door, and the whole world will be clean.
  prefs: []
  type: TYPE_NORMAL
- en: – Goethe
  prefs: []
  type: TYPE_NORMAL
- en: As software has grown more complex, it has become impossible for any one person,
    or even a single team, to maintain total awareness of an entire architecture.
    The rise of the internet promoted the concepts of *frontend* (a browser on one
    computer processing JavaScript, CSS, HTML) and *backend* (another computer running
    a database and an HTTP server) unified on a single server to deliver one product—the
    *web page*. A user might click on a button, a call is made to a server, that server
    might check a database, and will ultimately deliver an HTML page.
  prefs: []
  type: TYPE_NORMAL
- en: The pace has picked up. The modern user expects powerful and highly-interactive
    mobile apps to entertain them or drive their business, at a low cost, with regular
    updates. Now, one person can create an app that gains millions of users within
    months. To scale from one person to a company supporting millions of concurrent
    users in a few months—even a few years—requires efficient teams and engineering
    management.
  prefs: []
  type: TYPE_NORMAL
- en: Today's network-based applications are composed of several independent subsystems
    that must cooperate to fulfil the business or other requirements of the larger
    system. For example, many web applications will present browser-based interfaces
    composed of one or several libraries and/or UI frameworks translating user actions
    against JavaScript controllers running on phones, microcontrollers, and laptops,
    into formalized network requests issued across several web protocols, ultimately
    communicating with any number of servers executing units of business logic programmed
    in different languages, all sharing one or several databases, maybe across several
    data centers, themselves initiating and coordinating even longer chains of requests
    to a cloud API or other servers, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Software of any complexity is today rarely contained on one machine or within
    a single code base. In this chapter, we will look into the recently popular technique
    of composing distributed architectures out of independent actors, each in the
    form of a small, delineated, hot-reloadable service, or microservices. Microservices
    allow you to rewire, rewrite, reuse, and redeploy modular parts of your application,
    making change easier.
  prefs: []
  type: TYPE_NORMAL
- en: Why microservices?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building larger systems out of smaller, dedicated units is not a new idea. Object-oriented
    programming follows the same principle. Unix is built this way. Architectures
    facilitating composable networked software (CORBA, WebObjects, NetBeans) are decades-old
    ideas. What is new is the scale of profits networked software generates. Customers
    across nearly every business sector require new software and new features, and
    software developers are constantly delivering and/or refining those features in
    response to changing market conditions. Microservices are really a management
    idea whose goal is to decrease the time it takes to reflect changing business/customer
    needs in code. The goal is to reduce the cost of change.
  prefs: []
  type: TYPE_NORMAL
- en: 'As there is no absolute *right way* to build software, every language design
    is biased toward one or a few key principles, in particular, principles guiding
    how a system should scale, which normally affects how it is deployed. Some of
    the key principles informing the Node community—modular systems composed of small
    programs that do one thing well, event-driven, I/O focused, network focused—align
    closely with those underpinning microservices:'
  prefs: []
  type: TYPE_NORMAL
- en: A system should be broken out into many small services where each does one thing,
    and no more. This helps with clarity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code powering services should be short and simple. A common guideline in
    the Node community is to limit programs to somewhere near 100 lines of code. This
    helps with maintainability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No service should depend on the existence of another service—or even know of
    the existence of other services. Services are decoupled. This helps with scalability,
    clarity, and maintainability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data models should be decentralized, with a common (but not required) microservice
    pattern being each service maintaining its own database or similar model. Services
    are stateless. This reinforces (3).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Independent services are easy to replicate (or cull). Scaling (in both directions)
    is a natural feature of microservice architectures as new *nodes* can be added
    or removed as necessary. This also enables easy experimentation, where prototype
    services can be tested, new features can be tested or deployed temporarily, and
    so forth.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Independent, stateless, services can be replaced or upgraded (or downgraded)
    independently, regardless of the state of any system they form a part of. This
    opens the possibility of more focused, discreet deployments and refactors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Failure is unavoidable, so systems should be designed to fail gracefully. Localize
    points of failure (1, 2), isolate failure (3, 4), and implement recovery mechanisms
    (easier when error boundaries are clearly defined, small, and non-critical), promoting
    robustness by reducing the scope of unreliability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing is essential to any non-trivial system. Unambiguous and simple stateless
    services are easy to test. A key aspect of testing is simulation—the *stubbing* or
    *mocking* of services in order to test service interoperability. Clearly delineated
    services are also easy to simulate, and can, therefore, be intelligently composed
    into testable systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The idea is simple: smaller services are easy to reason about individually,
    encouraging correctness of specification (little or no grey area) and clarity
    of API (constrained sets of outputs follow constrained sets of inputs). Being
    stateless and decoupled, services promote system composability, helping with scaling
    and maintainability, making them easier to deploy. Also, very precise, discrete
    monitoring of these sorts of systems is possible.'
  prefs: []
  type: TYPE_NORMAL
- en: With that rough sketch in mind, let's go back in time and survey some foundational
    architectural patterns such as "3-Tier" architectures, and how their character
    led to the idea of a *microservice*. Bringing that progression up to the present,
    we'll then take a look at how the incredible scale of modern networked applications
    has forced a reimagining of the classic client->server->database setup, a new
    world often best composed with microservices.
  prefs: []
  type: TYPE_NORMAL
- en: When building web-based APIs using microservices, it will be useful to have
    tools that give you precise control over handling calls, headers, POST bodies,
    responses and so on, especially when debugging. I recommend installing **Postman**
    ([https://www.getpostman.com/](https://www.getpostman.com/)), and an extension
    for your browser that "prettifies" JSON objects. For Chrome, a good one is **JSON
    Formatter** ([https://chrome.google.com/webstore/detail/json-formatter/bcjindcccaagfpapjjmafapmmgkkhgoa?hl=en](https://chrome.google.com/webstore/detail/json-formatter/bcjindcccaagfpapjjmafapmmgkkhgoa?hl=en)).
  prefs: []
  type: TYPE_NORMAL
- en: From 3-Tiers to 4-Tiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand how microservices can improve your Node application, you must
    understand the problems they are designed to solve, and how those problems were
    solved previously. It is important to know *where* a microservice-oriented architecture
    might apply, and *why* such a change will help you. Let's look at how multitiered,
    distributed network architectures have developed over time.
  prefs: []
  type: TYPE_NORMAL
- en: Monoliths
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is a monolith:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/474961f2-296d-4989-890e-c8d86d0d938f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It is big, of one piece, and grows vertically. It would probably be difficult
    to reshape, or otherwise modify without great effort, great danger, and great
    cost. When someone describes an architecture as *monolithic*, they are using the
    preceding metaphor to suggest something very large and immovable and so massive
    as to discourage those attempting to improve it or to survey its total constituent
    parts in a comprehensive way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a simple application, like a *to-do* list. A list manager requires
    functionality to `create`, `add`, `delete`, and otherwise change lists. The code
    for that application might resemble this pseudo-code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This example shows monolithic design thinking. The data is on the same machine
    as the UI controllers as the process logic, functionality resides within the same
    context (the enclosing Node module), the same file, and the same OS process. You
    need not be pro-microservices to understand that as you add user accounts, drafts,
    and media attachments, sharing, multi-device synchronization, and other features
    to your to-do app, the original singular, monolithic repository for all application
    logic has grown too dense, and will need to be broken up into pieces.
  prefs: []
  type: TYPE_NORMAL
- en: What if you broke each of those functions off into an independent process, running
    in its own memory space, so pure and unattached to any other process that it can
    be updated, shut down, duplicated, deployed, tested, or even replaced with no
    impact on any other part of the system? Microservices come out of that kind of
    thinking.
  prefs: []
  type: TYPE_NORMAL
- en: It is perfectly okay to construct your software using standard OO, or a list
    of functions or structures all in one file or a small collection of files, and
    to expect your software to be running on a single machine. That architectural
    model probably works for most people; on modern hardware, a single-core machine
    running a simple Node server can likely handle several thousand concurrent users
    performing non-trivial, database-driven tasks. It is perfectly okay to scale a
    growing application by adding more cores or more memory to scale an architecture
    vertically. It's also okay to scale an architecture by standing up several already
    vertically-scaled servers and balancing the load between them. That strategy is
    still used by some billion-dollar companies.
  prefs: []
  type: TYPE_NORMAL
- en: It is okay to build a monolith *if it's the right choice for your needs*. At
    other times, microservices will be the right choice. You may not need to use decentralized
    data sources; you might have no need to *hot-reload* your services when they change.
    The widely used database MYSQL is commonly scaled vertically. When limits are
    being pushed, one simply adds more processing cores, memory, and storage space
    to your database server, or creates multiple copies of the same database and balances
    requests between them. This sort of monolithic architecture is easy to reason
    about and is typically resilient.
  prefs: []
  type: TYPE_NORMAL
- en: 'What are the advantages of vertically scaled architectures (monoliths)?:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Testing and debugging**: What happens in the application begins and ends
    with the application itself, independent of random network effects. This can be
    helpful when testing and debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strong consistency**: A persistent local database connection can help guarantee
    transactional integrity, including rollbacks. Distributed databases, especially
    those being accessed concurrently by many thousands of clients, are much more
    difficult to keep synchronized, and are typically described as *eventually consistent*,
    which may be a problem, especially if you''re a bank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplicity**: A well-designed application with, for example, a single REST
    API locally bounded within the same logical space as a single database can be
    easily described, and is predictable. Often, an individual can understand the
    entire system and even run it singlehandedly! This is a non-trivial advantage,
    especially in terms of the increased velocity of employee onboarding, and the
    opportunity for individual entrepreneurship.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear scaling**: If you can, hypothetically, double your capacity by doubling
    memory on a single machine, that is a very easy upgrade. At some point, this solution
    won''t suffice, but that point is probably much farther out than you might think.
    It is relatively easy to predict the cost of increased load and the steps needed
    to scale the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A few companies, or developers, will encounter scales absolutely requiring a
    distributed architecture. Some smart data object design and componentized UI related
    through a single database, well-designed and maintained, can be enough for a very
    long time, or even forever. In many ways, the popular Ruby on Rails framework
    continues to champion the monolith and the value of integrated systems, a position
    strongly argued for by its creator *David Heinemeier Hansson* at: [http://rubyonrails.org/doctrine/#integrated-systems](http://rubyonrails.org/doctrine/#integrated-systems).
  prefs: []
  type: TYPE_NORMAL
- en: From monoliths to 3-Tiered architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You might say with some precision that few people are truly building monolithic
    applications anymore. What people call *monolith* nowadays is typically a 3-Tiered
    application, which concretizes the following conceptual layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Presentation tier**: Interfaces for clients to request, view, and modify
    information. Typically communicates with the Application tier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application tier:** Logic to connect the Presentation and Data tiers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data-tier**: Where information is persisted and organized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Node developer will likely recognize applications composed of a client framework
    such as React (Presentation), *served* by an application layer built using Express,
    communicating with a MongoDB database via some sort of connector, such as Mongoose.
    These are the **LAMP** stacks, the **MEAN** stacks. System architects have known
    for a long time that the separation of an application into distinct systems is
    a smart strategy. In many ways, this architecture reflects the **Model View Controller**(**MVC**)
    model, where  M=Data, V=Presentation, and C=Application.
  prefs: []
  type: TYPE_NORMAL
- en: How did this architecture come about?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Firstly, it was recognized that there are very clearly distinct parts of systems
    that should be understood separately. A browser-based UI has nothing to do with
    your database or with your web server. It may, through various layers of abstraction,
    come to *reflect* your database structure (the display of profiles linked by interests,
    for example), but that characteristic of your system is ultimately a design decision,
    not a necessary condition. Maintaining and updating your database independent
    of your layout grid or interactive components just makes sense. Unfortunately,
    these distinct things can end up entangled, through lazy design or the vicissitudes
    of fast-paced business environments, and for other reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, rapid, continuous deployment and integration is much more difficult
    when changing one part of one of the tiers ultimately requires retesting the entire
    system. Integration tests must either touch on real systems or create artificial
    simulations, neither of which are dependable, and both of which can lead to destructive
    results. Similarly, deployments are holistic—each part, even if conceptually distinct,
    is in reality intimately bound with the others, where the integrity of each must
    be validated by validating the integrity of the whole. Enormous test suites reflect
    the enormity and density of the application designs they attempt to cover.
  prefs: []
  type: TYPE_NORMAL
- en: The focus on having exactly three tiers makes elasticity difficult. New data
    models, functions, services, even UIs that might be *one-off* additions (a completely
    independent login system from Facebook, for instance) must be linked across the
    three tiers, and its integration with many or all the existing data models, functions,
    business logic, UIs must be done carefully (and somewhat artificially) both initially
    and on every change. As new caching mechanisms (CDNs) and API-driven development
    has taken hold, the artificiality of the 3-tier system has begun to frustrate
    developers.
  prefs: []
  type: TYPE_NORMAL
- en: Service-Oriented Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The idea of microservices is largely a refinement and recontextualization of
    the ideas around **Service-Oriented Architectures (SOA)**, which Wikipedia defines
    this way:'
  prefs: []
  type: TYPE_NORMAL
- en: <q>"A [SOA] is a style of software design where services are provided to the
    other components by application components, through a communication protocol over
    a network. ... A service is a discrete unit of functionality that can be accessed
    remotely and acted upon and updated independently, such as retrieving a credit
    card statement online."</q>
  prefs: []
  type: TYPE_NORMAL
- en: An SOA makes a lot of sense when a distinct piece of functionality can be delineated.
    If you are running an online store, you'll probably want to separate the search
    functionality and the payments functionality from the signup system and the client
    UI server. We can see that the foundational idea here is to create functionality
    that is logically self-contained and accessible via a network—useful services
    that other system components (including services) can work with without colliding
    with each other.
  prefs: []
  type: TYPE_NORMAL
- en: Separating like functionality into individual services is a common adjustment
    to 3-Tier architectures, where the business logic on a server might delegate its
    responsibilities to a third-party API. For example, one might use an identity
    management service like **Auth0** to manage user accounts, rather than store them
    locally in a database. This means the business logic for logins functions as a
    proxy to an external service. Financial transactions, like sales, are often delegated
    to external providers, as are log collection and storage. For companies that might
    offer up their services as an API, the entirety of the API management might be
    delegated to a cloud service such as Swagger or Apiary.
  prefs: []
  type: TYPE_NORMAL
- en: Possibly as a result of architectural trend toward services, driven by third-party
    services managing once *on-site* functionality such as caching and other APIs,
    a new basket of ideas generally referred to as "4-Tier architecture" has attracted
    the attention of system architects.
  prefs: []
  type: TYPE_NORMAL
- en: 4-Tiers and microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last several years of modern distributed application development has led
    to somewhat of a consensus of patterns advantageous to scaling. Let's consider
    first what is generally meant by a "4-Tier Architecture", and then how microservices
    have come to define the design of these types of systems.
  prefs: []
  type: TYPE_NORMAL
- en: '4-Tier architectures extend and expand 3-Tier architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tier 1:** The Data tier in a 3-Tier architecture is replaced by a **Services**
    tier. The thinking is straightforward: data is stored at such scale, in so many
    different ways, across so many different technologies, and changes so often in
    quality and type that the idea of a "single source of truth", like a single database,
    is no longer workable. Data is exposed through abstract interfaces whose internal
    design (calling a Redis database and/or pulling your inbox from Gmail and/or reading
    weather data from government databases) is a "black box" that need simply return
    data in the expected format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tier 2:** 4-Tier architectures introduce the concept of an **Aggregation**
    tier. Just as data is now broken into services (1), business logic is also being
    isolated into individual services. As we''ll see later when discussing Lambda
    architectures, the distinction between the way you fetch data or call a *subroutine* has
    blurred into a general API driven model, where individual services with consistent
    interfaces generate protocol-aware data. This tier assembles and transforms data,
    augmenting and filtering aggregated source data into data modeled in structured,
    predictable ways. This is the layer that might have been called the *backend*,
    or the Application tier. This is where developers program the channels through
    which data flows as agreed upon (programmed) protocols. Generally, we want to
    produce structured data here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The remaining tiers are created by splitting the Presentation tier into two:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tier 3:** The **Delivery** Tier: This tier, aware of client profile (mobile,
    desktop, IOT, and so on), transforms data delivered by the Aggregation tier into
    client-specific formats. Cached data would be fetched here, via CDN or otherwise.
    Selection of ads to insert into a *webpage* might be done here. This tier is responsible
    for optimizing data received from the Aggregation tier for an individual user.
    This layer can often be fully automated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tier 4:** The **Client** Tier: This tier customizes what is generally returned
    from the Delivery tier for the specific clients. This can be as simple as rendering
    a data stream for a mobile device (perhaps a responsive CSS structure or device
    specific native format) or the reflection of a personalized view (only images,
    or language translation). Here''s where the same data source can be aligned with
    a specific business partnership, made to conform with an **SLA (Service Level
    Agreement)** or other business function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The notable change is the splitting of the Presentation tier into two. Node
    often appears in the Delivery tier, querying the Aggregation tier on behalf of
    the Client, customizing the data response it receives for the Client.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, we have moved to an architecture where individual services are
    not expected to reflect the needs of the caller in any specific way, like a browser-focused
    templating engine in an Express server might have. Services need not share the
    same technology or programming language, or even the same OS version or kind.
    Architects instead declare a certain type of topology, with clearly defined communication
    points and protocols, generally distributed into: 1) data sources, 2) data aggregators,
    3) data shapers, and 4) data displayers.'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll consider several variations on microservices, looking
    at a few common ways in which developers are using microservices with Node. We'll
    start with **Seneca**, a microservices framework for Node. Then, we'll move on
    to developing cloud-based microservices using **Amazon Lambda**. From there, we
    will attempt to model a **Kubernetes** cluster out of **Docker** containers, exploring
    modern containerized microservice orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices with Seneca
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Seneca is a Node-based microservice construction kit that helps you organize
    your code into distinct actions triggered by patterns. Seneca applications are
    composed of services that can accept JSON messages and optionally return some
    JSON. Services register an interest in messages with certain characteristics.
    For example, a service might run whenever a JSON message displaying the `{ cmd:
    "doSomething" }` pattern is broadcast.'
  prefs: []
  type: TYPE_NORMAL
- en: To start, let’s create a service that responds to three patterns, one pattern
    returning “Hello!”, and the other two different ways of saying “Goodbye!”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `hellogoodbye.js` file containing the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Seneca works on the idea of service clients listening for certain command patterns,
    and routing to the right handler based on pattern matching. Our first job is to
    set up two Seneca service clients, listening on ports `8080` and `8081`. Already,
    we can see how services are being organized into two groups, the "hello service"
    with one method, and the "goodbye service" with another. We now need to add actions
    to those services. To do that, we need to tell Seneca how to act when a service
    call matching a certain pattern is made, here defined using certain object keys.
    It is left open how to define your service objects, but the "cmd" and "role" pattern
    is common—it helps you create logical groups and a standard command call signature.
    We'll be using that pattern in the examples that follow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering the preceding code, we see that when a JSON object is received
    with the `cmd` field set to `sayHello` and a `role` of `hello`, the service handler
    should return  `{ message: "Hello!" }`. The "goodbye" role methods are similarly
    defined. At the bottom of the file, you see how we can directly call those services
    via Node. It is easy to imagine how these service definitions can be broken out
    into several module exports in individual files, dynamically imported as needed,
    and otherwise used to compose applications in an organized way (a goal of microservice-based
    architectures).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get rid of the log data being displayed, you can initialize your Seneca
    instance with `require(''seneca'')({ log: ''silent'' })`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the Seneca service is listening on HTTP by default, you can achieve the
    same result by making a direct call over HTTP, operating against the `/act` route:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This automatic HTTP interface gets us network-discoverable services for free,
    which is handy. We can already get a sense of the microservice pattern: simple,
    independent, small bits of functionality that communicate using standard network
    data patterns. Seneca gives us both a programmatic and networked interface for
    free, and that''s a bonus.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you start creating a lot of services, it will become difficult to keep
    track of which service group operates out of which port. Service discovery is
    a difficult, new problem that microservice architectures introduce. Seneca solves
    this via its **mesh** plugin, which adds service discovery to your Seneca cluster.
    Let's create a simple calculator service to demonstrate. We’ll create two services,
    each listening on a distinct port, one that performs addition and the other subtraction,
    as well as a base service to instantiate the mesh. Finally, we'll create a simple
    script that performs addition/subtraction operations using services whose location
    it need not know, via the mesh.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this example is in the `/seneca` folder in your code bundle. To
    start, you will need to install two modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create a base node that will enable the mesh:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Once this node is started, other services will be automatically discovered once
    connected to the mesh.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `add` service block looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: (The **subtract** service looks exactly the same, changing only the math operator
    it uses, and of course its `cmd` will be "subtract").
  prefs: []
  type: TYPE_NORMAL
- en: Using the familiar role/cmd pattern, we attach the `add` command to the `calculator`
    group, similar to how we defined the "hello" service in our earlier example, with
    a handler to perform the addition operation.
  prefs: []
  type: TYPE_NORMAL
- en: We also instruct our service to `listen` for calls on localhost to a specific
    port, as we normally do. What is new is that we `use` the mesh network, using
    the `pin` attribute to indicate the role and cmd that this service will respond
    to, making it discoverable in the mesh.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jump into the `/seneca` folder in your code bundle and start the following
    three files in separate terminals, in the following order: `base.js` -> `add.js `-> `subtract.js`.
    The logical units for our calculator are set up and running independently, which
    is the general goal of microservices. The last step is to interact with them,
    for which we''ll use the following `calculator.js` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Apart from running our actions from within the `ready` handler for Seneca (a
    useful practice), and of course our `use` of `mesh`, the `seneca.act` statements
    look the same as the "hello" acts we used earlier, don''t they? They are the same,
    except for one important detail: we are not using the `.listen(<port>)` method!
    There is no need to create new Seneca clients bound to certain ports as there
    were in the `hellogoodbye.js` example, because the mesh network services are autodiscovered.
    We can simply make calls without needing to know which port services exist on.
    Go ahead and run the preceding code. You should see the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This allows great flexibility. By building out your calculator in this way,
    each operation can be isolated into its own service, and you can add or remove
    functionality as needed, without affecting the overall program. Should a service
    develop bugs, you can fix and replace it without stopping the general calculator
    application. If one operation requires more powerful hardware or more memory,
    you can shift it to its own server without stopping the calculator application
    or altering your application logic. It is easy to see how stringing together database,
    authentication, transaction, mapping, and other services can be more easily modeled,
    deployed, scaled, monitored, and maintained than if they were all coupled to a
    centralized service manager.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The abstractions that emerge from the design of these distributed systems,
    largely built on microservices, suggest a natural next step. Why have servers
    in the traditional sense at all? Servers are big, powerful machines designed in
    the age of monoliths. If our thinking is in terms of small, resource sipping,
    independent actors indifferent to the world around them, shouldn''t we be deploying
    microservices to "microservers"? This line of thought has led a revolutionary
    ideas: AWS Lamda.'
  prefs: []
  type: TYPE_NORMAL
- en: AWS Lambda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The introduction of Amazon''s AWS Lambda technology boostrapped the serverless
    movement we have today. Amazon describes Lambda like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '"AWS Lambda lets you run code without provisioning or managing servers...With
    Lambda, you can run code for virtually any type of application or backend service
    - all with zero administration. Just upload your code and Lambda takes care of
    everything required to run and scale your code with high availability. You can
    set up your code to automatically trigger from other AWS services or call it directly
    from any web or mobile app."'
  prefs: []
  type: TYPE_NORMAL
- en: Lambda is a technology allowing you to create an infinitely scalable computation
    cloud composed out of microservices written in JavaScript. You no longer manage
    servers, only functions (Lambda functions). The cost of scaling is measured on *usage*,
    not *counts*. It costs more to call 1 Lambda service 10 times than to call each
    of 9 Lambda services once. Similarly, your services can sit idle, never being
    called, without incurring any charges.
  prefs: []
  type: TYPE_NORMAL
- en: Lambda functions are functional virtual machines. A Lambda function is essentially
    a containerized Node application which builds and deploys automatically, including
    security updates and further maintenance of the underlying services and infrastructure.
    You will never need to manage Lambda functions beyond writing the code they execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, you trade off some of the flexibility developing on a server
    architecture provides. At the time of writing this, these are the limits for each
    Lambda function:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Resource | Limits |'
  prefs: []
  type: TYPE_TB
- en: '| Memory allocation range | Minimum = 128 MB / Maximum = 1536 MB (with 64 MB
    increments). If the maximum memory use is exceeded, function invocation will be
    terminated. |'
  prefs: []
  type: TYPE_TB
- en: '| Ephemeral disk capacity ("/tmp" space) | 512 MB |'
  prefs: []
  type: TYPE_TB
- en: '| Number of file descriptors | 1,024 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of processes and threads (combined total) | 1,024 |'
  prefs: []
  type: TYPE_TB
- en: '| Maximum execution duration per request | 300 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Invoke request body payload size (RequestResponse/synrchronous invocation)
    | 6 MB |'
  prefs: []
  type: TYPE_TB
- en: '| Invoke request body payload size (Event/asynchronous invocation) | 128 K
    |'
  prefs: []
  type: TYPE_TB
- en: These limits need to be kept in mind when designing your application. Generally,
    Lambda functions should not depend on persistence, do one thing well, and do it
    quickly. These limits also imply that you cannot spin up a local database, or
    other in-process applications, within your Lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: When it was released, Lambda was exclusively designed for Node; you wrote Lambda
    functions in JavaScript via the Node runtime. This fact is at least some indication
    of just how important Node is to modern application development. While other languages
    are now supported, Lambda continues to treat Node as a first-class citizen. In
    this section, we'll develop an application using the Lambda compute cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the process of setting up with Lambda is much easier now than it was
    when the project was first released, there is still a lot of automatic boilerplate
    you will need to build, and a lot of manual work to make changes. For this reason
    a number of very high quality Lambda-focused "serverless" frameworks have sprung
    up in the Node ecosystem. Some of the leading frameworks are listed below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Serverless: [https://github.com/serverless/serverless](https://github.com/serverless/serverless)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apex: [https://github.com/apex/apex](https://github.com/apex/apex)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Claudia: [https://github.com/claudiajs/claudia](https://github.com/claudiajs/claudia)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the following examples, we''ll use `claudia`, which is well-designed, documented,
    and maintained, and is easy to use.  The `claudia` developer puts it this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '"...if you want to build simple services and run them with AWS Lambda, and
    you''re looking for something low-overhead, easy to get started with, and you
    only want to use the Node.js runtime, Claudia is a good choice. If you want to
    export SDKs, need fine-grained control over the distribution, allocation or discovery
    of services, need support for different runtimes and so on, use one of the alternative
    tools."'
  prefs: []
  type: TYPE_NORMAL
- en: '**API Gateway** is a fully-managed AWS service "that makes it easy for developers
    to create, publish, maintain, monitor, and secure APIs at any scale". We will
    now assemble a scalable web server out of Lambda-powered microservices using Claudia
    and AWS API Gateway.'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling with Claudia and API Gateway
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To begin, you will need to create a developer account with Amazon Web Services
    (AWS) [https://aws.amazon.com](https://aws.amazon.com). This account setup is
    free. Additionally, most AWS services have very generous free usage tiers, within
    which limits you may use AWS while learning and developing without incurring any
    cost. With Lambda, the first one million requests per month are free.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a developer account, log in to your dashboard, and from the Services
    tab, select IAM. You will now add a user that we'll use for these examples. Claudia needs
    permission to communicate with your AWS account. You generally do not want to
    use your root account privileges in applications, which should be understood as
    "sub-users" of your account. AWS provides an **Identity and Access Management
    (IAM)** service to help with that. Let's create an AWS profile with IAM full access,
    Lambda full access, and API Gateway Administrator privileges.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the side panel, select Users and click on Add user:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c928b713-abb6-44ae-8d50-80fa5265a5a4.png)'
  prefs: []
  type: TYPE_IMG
- en: As indicated, create a new user `claudia`, affording this user programmatic
    access.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''re done, click on the Next: Permissions button. We now need to attach
    this IAM account to the Lambda and API Gateway services and give it administrative
    privileges:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0224d5e-a147-4472-8fdc-491c73f9d712.png)'
  prefs: []
  type: TYPE_IMG
- en: After selecting Attach existing policies directly, you will see a long checklist
    of options appear below. Select the following three permissions for the `claudia` user: AdministratorAccess,
    AmazonAPIGatewayAdministrator, and of course, AWSLambdaFullAccess.
  prefs: []
  type: TYPE_NORMAL
- en: 'After clicking on Review, you should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e87983e-5f8c-4036-947a-d62012181666.png)'
  prefs: []
  type: TYPE_IMG
- en: Great. Click on Create User and copy the provided Access key ID and Secret access
    key (you'll need these later). You're now ready to start deploying Lambda functions
    using `claudia`.
  prefs: []
  type: TYPE_NORMAL
- en: Installing claudia and deploying a service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To begin installing the `claudia` module, type the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you should store the credentials you just created for the `claudia` user.
    A good pattern here is to store an AWS configuration file in your home directory
    (on OSX, this would be` /Users/<yoursystemusername>`). Once you''re in your home
    directory, create the `.aws/credentials` directory and file with your IAM user
    keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are indicating that `claudia` is the AWS profile name, targeting these
    IAM credentials. When we run our deployment, AWS will be informed of this profile
    and credentials.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's create a web-accessible HTTP endpoint that returns the string "Hello
    from AWS!".
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new directory and initialize an `npm` package with `npm init`, using
    any name that you''d like for the package. To work with AWS API Gateway, we''ll
    also need to install an extension to `claudia`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, add the following `app.js` file to this directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `claudia` `ApiBuilder`, we attach a Lambda function to handle a GET on
    the `/hello` route. Surprisingly, we''re done! To deploy, enter the following
    into your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `AWS_PROFILE` environment variable is referencing the `[claudia]` profile
    identifier in our credentials file, and we are using the `--region` flag to establish
    the deployment region.
  prefs: []
  type: TYPE_NORMAL
- en: 'If everything goes right, your endpoint will be deployed, and information similar
    to the following will be returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned URL points to our API gateway. Now, we need to add the name of
    our Lambda function, which was set as `''hello''` in the GET handler we defined
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy and paste the returned URL in your browser and add the name of your Lambda
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'That was easy. Updating the function is just as easy. Return to the code and
    change the string message your function return''s to something else, then run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: A JSON object will be returned on success, indicating code size and other useful
    information about your function. Go ahead and reload the endpoint in your browser
    and you will see the updated message. These are zero-downtime updates—your service
    never stops working while the new code is deployed. Here, we satisfy the key goal
    of creating "independent, stateless, services [that] can be replaced or upgraded
    (or downgraded) independently, regardless of the state of any system they form
    a part of."
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now validate the existence of your Lambda function by returning to
    the AWS dashboard and visiting your Lambda services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5137fa18-5bb4-4a06-bb8a-3753e1ae10e4.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see the package name listed (`claudiaapi`) and the Node runtime we are
    using (the highest available version on AWS at the time of this writing). If you
    click on the function, you will see the management page for your Lambda function,
    including its code, and interfaces for managing maximum execution times and memory
    limits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Change your handler function in `app.js` to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: You will see three new arguments are passed to the `handler`, `request`, `context`,
    and `callback`. The `context` argument contains useful information about the Lambda
    context for this call, such as the invocation ID, the name of the called function,
    and more. Usefully, `claudia` mirrors Lambda context in the passed `request` object
    at key `lambdaContext`. For this reason, when using `claudia`, you need to only
    work with the `request` argument, which simplifies things.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about Lambda event contexts, refer to: [http://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-context.html](http://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-context.html).
  prefs: []
  type: TYPE_NORMAL
- en: Now, update your Lambda function using `claudia update`, and check the URL.
    You should see a large amount of JSON data returned, the totality of the request
    event information available to you. For more on this data object, visit: [https://github.com/claudiajs/claudia-api-builder/blob/master/docs/api.md#the-request-object](https://github.com/claudiajs/claudia-api-builder/blob/master/docs/api.md#the-request-object).
  prefs: []
  type: TYPE_NORMAL
- en: An interesting collection of serverless development information and links can
    be found at: [https://github.com/anaibol/awesome-serverless](https://github.com/anaibol/awesome-serverless).
  prefs: []
  type: TYPE_NORMAL
- en: Containerized microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Amazon AWS infrastructure is able to create services like Lambda because
    their engineers no longer provision hardware (ie. new physical servers) when customers
    create another cloud function or API. Instead, they provision lightweight **VM
    (Virtual Machines)**. Nobody is lifting a big new metal box onto a rack when you
    sign up. Software is the new hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '**Containers** aim for the same general architectural idea and advantages that
    virtualized servers provide —to mass produce virtualized, independent, machines.
    The main difference is that while a VM provides its own OS (typically called a
    **Hypervisor**), a container requires a host OS to provide actual kernel services
    (such as a filesystem, other devices, and resource management and scheduling)
    as they do not need to carry around their own OS but operate parasitically on
    a host OS containers are very light, using fewer (host) resources and able to
    be started up much more quickly. In this section, we''ll go over how any developer
    can use **Docker**, a leading container technology, to cheaply manufacture and
    manage many virtualized servers.'
  prefs: []
  type: TYPE_NORMAL
- en: This is a good StackOverflow discussion on the distinctions between virtual
    environments at: [https://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine](https://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine).
  prefs: []
  type: TYPE_NORMAL
- en: 'This image from the Docker website ([http://www.docker.com/](http://www.docker.com/))
    gives some information on how, and why, the Docker team feels their technology
    fits into the future of application development:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b059cb0b-cc0d-4ed2-b400-361e6bc1b0c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Recalling our discussion of 4-Tier architectures, here we can see that developers
    asked themselves a question: if my application is composed out of many independently
    developed, tested, deployed, and managed communicating services in the cloud running
    on their own infrastructure, couldn''t we just do the same with our "local" services
    and give each its own isolated container so that those can be independently developed,
    tested, deployed, and so forth? Reducing the cost of implementing change is the
    goal of containerization, and of microservices. A container generating one localized,
    independent service with protected local memory that can be started and restarted
    quickly, tested individually, and fail quietly fits right into a microservices
    architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Clearly defined areas of responsibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isolated dependencies and state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes are disposable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lightweight and easy to start and replicate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graceful termination with zero application downtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be independently tested
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be independently monitored
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Docker ecosystem has three main components. Here''s what the documentation
    says:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Docker containers**. A Docker container holds everything that is needed for
    an application to run. Each container is created from a Docker image. Docker containers
    can be run, started, stopped, moved, and deleted. Each container is an isolated
    and secure application platform. You can consider Docker containers as the run
    portion of the Docker framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker images**. The Docker image is a template, for example, an Ubuntu operating
    system with Apache and your web application installed. Docker containers are launched
    from images. Docker provides a simple way to build new images or update the existing
    images. You can consider Docker images to be the build portion of the Docker framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker Registries**. Docker registries hold images. These are public (or
    private!) stores that you can upload or download images to and from. These images
    can be images you create yourself or you can also make use of images that others
    have previously created. You can consider Docker registries to be the share portion
    of the Docker framework. You create images of applications to be run in any number
    of isolated containers, sharing those images with others if you’d like. The most
    popular is **Docker Hub** ([https://hub.docker.com/](https://hub.docker.com/)),
    but you can also operate your own.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concept of composing Node applications out of many independent processes
    naturally aligns with the philosophy behind Docker. Docker containers are sandboxed,
    unable to without your knowledge execute instructions on their host. They can
    expose a port to their host OS, however, allowing many independent virtual containers
    to be linked together into a larger application.
  prefs: []
  type: TYPE_NORMAL
- en: It will be a good idea to learn a little about how to find information about
    your OS, which ports are being used, by which processes, and so on. We've mentioned
    HTOP previously, and you should familiarize yourself with at least gathering network
    statistics—most OS's offer a `netstat` utility, useful to discover which ports
    are open and who is listening on them. For example, `netstat -an | grep -i "listen"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download and install **Docker Community Edition** ([https://www.docker.com/community-edition](https://www.docker.com/community-edition))
    or the **Docker Toolbox** ([https://docs.docker.com/toolbox/overview/](https://docs.docker.com/toolbox/overview/)). A
    comparison between the two can be found at: [https://docs.docker.com/docker-for-mac/docker-toolbox/](https://docs.docker.com/docker-for-mac/docker-toolbox/).
    If you''re using the toolbox, select the Docker Quickstart Terminal when prompted,
    which will spawn a terminal and install the necessary components on your system. 
    The installation process might take while, so don''t panic! When done, you should
    see something like the following in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note that the name of the Docker machine is "default".
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a sense of how images work, run the `docker run hello-world` command.
    You should see the machine pull an image and containerize it—full details of what
    is happening will be printed. If you now run the command docker images, you''ll
    see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16dc8107-f6a1-46e7-9566-86da2260fe2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This command will tell you something about your Docker installation: `docker
    info`.'
  prefs: []
  type: TYPE_NORMAL
- en: A Docker container runs an image of your application. You can create these images
    yourself, of course, but there does exist a large ecosystem of the existing images.
    Let’s create our own image of a Node server running Express.
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll need to build an application to run. Create a folder to put your
    application files into. Within that folder, create an `/app` folder; this is where
    we will put our server files. As with all Node applications, we’ll need to create
    a `package.json` file. Enter the `/app` folder and `run npm init`, giving this
    package a name of "docker-example". Then, install Express with `npm i express`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, create a simple Express server and save it to `app/index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Go ahead and start the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now point your browser to your host on port `8087` and see a unique
    message similar to `Service #1513534970093 responding` displayed. Great. There
    is a reason for creating a unique message (via `Date.now()`) that will make more
    sense later when we discuss scaling services. For now, let''s build these files
    into a container using Docker.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Dockerfile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our goal is to describe the environment this application executes within, now,
    so that Docker can reproduce that environment in a container. Also, we want to
    add the source files of our application to run in this newly virtualized environment.
    Docker can act as a builder, in other words, following the instructions you provide
    on how to build an image of your application.
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, you should have a folder containing your application files. This
    is your source code repository, which your docker image will build within. As
    mentioned, a Dockerfile is a list of instructions for building an application.
    A Dockerfile describes a build process. What you will normally declare in a Dockerfile
    is the OS version the container will run, and any OS installations you might need
    done, such as Node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `Dockerfile` file (no extension):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You see in the this file various directives, and there are several others you
    might use for more complex builds. We'll keep it simple to start. To go deeper
    into Dockerfiles, you can run through the full documentation at: [https://docs.docker.com/engine/reference/builder/](https://docs.docker.com/engine/reference/builder/).
  prefs: []
  type: TYPE_NORMAL
- en: The `FROM` directive is used to set the base image you will build upon. We will
    be building upon `node:9`, an image containing the latest Node. More complex images
    are usually included here, typically built around common patterns. For example,
    this image implements the **MEAN (Mongo Express Angular Node)** stack: [https://hub.docker.com/r/meanjs/mean/](https://hub.docker.com/r/meanjs/mean/).
    `FROM` should be the very first directive in a Dockerfile.
  prefs: []
  type: TYPE_NORMAL
- en: You can set (optional) metadata for your image via `LABEL`. There can be multiple
    `LABEL` declarations. This is useful for version management, credits, and so forth.
    We also set some environment variables (`ENV`) for the Node process (exposed as
    you would expect in `process.env`).
  prefs: []
  type: TYPE_NORMAL
- en: We state our working directory (`WORKDIR`) for the application, and `COPY` all
    the local files on our machine into the filesystem of the container; the container
    is isolated and has no access to a filesystem outside of itself, so we need to
    build its filesystem from ours.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we establish startup directives. `RUN npm i` to install `package.json`,
    `EXPOSE` the port our server runs on (`8087`) to the outside world (again, containers
    are isolated and cannot expose internal ports without permission), and run the
    command (`CMD`) `npm start`. You may set multiple `RUN` and `CMD` directives,
    whatever is necessary to start your application.
  prefs: []
  type: TYPE_NORMAL
- en: We're now ready to build and run a container.
  prefs: []
  type: TYPE_NORMAL
- en: Running containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From within the directory containing your Dockerfile, run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker build -t mastering-docker .` (note trailing dot).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker will now fetch all base dependencies and build your image according
    to your directives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90224448-516e-4071-b6f7-c67b93255a3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You just created your first Docker image! To see your image, use `docker images`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fedf510f-94b0-49f4-940e-54b251fbb325.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we see the image we created—`mastering-docker`—and the image our image
    was based on—`node:9`. Note how the colon is used to created tagged versions of
    images -- we are ultimately using the **node** image tagged **9**. More on versioning
    later.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to containerize and run the image. Use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If all goes well, you will be able to list your running Docker process with
    the `docker ps` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ccc76ea-dc15-4093-b26e-094bd36abb94.png)'
  prefs: []
  type: TYPE_IMG
- en: Recall the `EXPOSE 8087` directive? We needed to map the port our container
    exposed to the local OS network interface, which we flagged in the run command
    with `-p 8088:8087`, and which mapping we can see in the screenshot above under
    `PORTS`.
  prefs: []
  type: TYPE_NORMAL
- en: The `-d` flag indicates to Docker that we'd like to run the container in detached
    mode. This is probably what you want to do, running your container in the background.
    Without this flag, you will terminate the container when you terminate your terminal
    session.
  prefs: []
  type: TYPE_NORMAL
- en: You are now running a Node server in a container completely isolated from your
    local machine. Try it out by navigating a browser to `localhost:8088`. It's pretty
    great to be able to construct entirely isolated builds, with completely different
    operating systems, databases, software versions and so forth all on you local
    machine, knowing that you can take that exact same container and deploy it into
    a data center without changing a thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some more useful commands:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Delete a container: `docker rm <containerid>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete all containers: `docker rm $(docker ps -a -q)`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Delete an image: `docker rmi <imageid>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete all images: `docker rmi $(docker images -q)`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stop or start a container: `docker stop (or start) <containerid>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orchestrating Containers with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A microservices-driven architecture is composed of independent services. We've
    just looked at how containers can be used to isolate distinct services. Now, the
    question is how to manage and coordinate these 10, 20, 100, 1,000 service containers?
    "By hand" doesn't seem like the right approach. **Kubernetes** automates container
    orchestration, helping you deal with questions of deployment and scaling and fleet
    health. Developed by Google, it is a mature technology used in Google's own incomprehensibly
    massive data centers to orchestrate millions of containers.
  prefs: []
  type: TYPE_NORMAL
- en: We'll install an application, **Minikube**, that runs a single-node Kubernetes
    cluster inside a VM on your local machine, so you can test develop a Kubernetes
    cluster locally before deploying. As the configuration of the cluster you do locally
    mirrors a "real" Kubernetes cluster, once you are satisfied, you can deploy your
    definitions to Kubernetes in production without any changes.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a basic Kubernetes cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You'll need some sort of VM driver for Minikube, and by default, Minikube uses
    **VirtualBox**. You can find installation instructions at: [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads).
    VirtualBox stands on its own as a free hypervisor, which is used to power other
    useful developer tools, like **Vagrant**.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we'll install `kubectl` (think of it as "Kube Control"), the Kubernetes
    command-line interface. Follow the instructions at: [https://kubernetes.io/docs/tasks/tools/install-kubectl/](https://kubernetes.io/docs/tasks/tools/install-kubectl/).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we install Minikube: [https://kubernetes.io/docs/tasks/tools/install-minikube/](https://kubernetes.io/docs/tasks/tools/install-minikube/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the cluster up with `minikube start` (this can take a while, so be patient).
    The output is sufficiently descriptive: you will be starting a VM, getting an
    IP address, and building a Kubernetes cluster. The output should end with something
    like Kubectl is now configured to use the cluster. You can always check its status
    with `minikube status`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: To see that kubectl is configured to communicate with Minikube, try `kubectl
    get nodes`, which should show that the Minkube machine 'minikube' is 'Ready'.
  prefs: []
  type: TYPE_NORMAL
- en: This VM is being run via VirtualBox. Open the Virtualbox Manager on your machine.
    You should see a machine named "minikube" listed. If you do, great; a Kubernetes
    cluster is running on your machine!
  prefs: []
  type: TYPE_NORMAL
- en: You can test different Kubernetes versions with Minikube. To get available versions,
    run `minikube get-k8s-versions`. Once you have a version, start Minikube on that
    version with `minikube start --kubernetes-version v1.8.0`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we'll use Kubernetes to deploy the "hello world" server we earlier containerized
    with Docker. Usefully, Minikube manages its own Docker daemon and local repository.
    We'll use that to build what follows. First, link into Minikube's Docker with
    `eval $(minikube docker-env)`. When you want to return control back to your host
    Docker daemon, try `eval $(minikube docker-env -u)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return to the folder containing our server and build our Docker image (note
    trailing dot):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'When that process has completed, you should see something like this displayed
    in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'One thing that you may have noted is the `:v1` suffix on our image name. We
    saw that earlier when we declared Node in our Dockerfile (remember the `FROM Node:9`
    directive)? If you run `docker images`, you''ll see that tags are applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/190bd58c-f298-4e14-9430-2cb4be0b265f.png)'
  prefs: []
  type: TYPE_IMG
- en: Later on, if we want to release a new version of mastering-kube, we can simply
    build with a new tag, which creates an independent image. This is what you should
    do over time to manage versioning of your container images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Great, now let''s start a container with that image and **deploy** it into
    our Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we declare a new deployment with the name `kubernetes-demo` that should
    import the `mastering-kube` image at version `v1`. If everything works, you should
    see deployment "kubernetes-demo" created in your terminal. You can list deployments
    with `kubectl get deployments`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25b518d0-d72b-4619-92dd-3650d4d7a9cf.png)'
  prefs: []
  type: TYPE_IMG
- en: We've just deployed a single **Pod** into a Kubernetes cluster. Pods are the
    basic organizational unit of Kubernetes, and they are an abstract wrapper around
    containers. Pods may contain one or more containers. Kubernetes manages pods,
    and Pods manage their own containers. Each Pod receives its own IP address and
    is isolated from other pods, but containers within pods are not isolated from
    each other (they can all communicate via `localhost`, for example).
  prefs: []
  type: TYPE_NORMAL
- en: A Pod presents the abstraction of a single machine running somewhere (locally,
    on AWS, in a data center) and all the containers running on that single machine.
    In this way you can have a single Kubernetes cluster running Pods located in different
    locations in the cloud. Kubernetes is an abstraction across differently located
    machine hosts that lets you orchestrate their behavior indifferent to whether
    one is a VM hosted on AWS or a laptop in your office, in the same way that you
    might use an ORM to abstract away database details, leaving you free to change
    the technical makeup of a deployment without altering configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the `kubectl get pods` command, you should now see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbda1b0e-1fa8-4d9b-a7e0-33b86dbdb59c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final step is to expose this deployed pod as a service. Run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If successful, you should see the message service "kubernetes-demo" exposed.
    To view services, use `kubectl get services`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/311e19b0-e547-40f3-9ab4-bbacd6ad240e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note how we have created a load-balanced type of deployment, exposing a Kubernetes
    service mapped to our mastering-kube service (container), accessible through the
    unique IP this deployed Pod will be assigned. Let''s find that URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get back a URL (note how Kubernetes is running its own DNS), and
    browsing to that URL, you should see something like this message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Via Minikube, you can start your service in a browser in just one step: `minikube
    service kubernetes-demo`.'
  prefs: []
  type: TYPE_NORMAL
- en: Great. However, the real magic with Kubernetes has to do with how deployments
    can be scaled and otherwise respond to network conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recalling that this deployment is load balanced, let''s create multiple containers
    within the same pod sharing load (not unlike the way you might use Node''s Cluster
    module to balance load). Run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the message deployment "kubernetes-demo" scaled. Let''s make
    sure that''s true. Run `kubectl get pods` again. You should see that our deployment
    has autoscaled the number of pods that it balances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9aa52dae-635a-44a4-a28a-fe1636e5fe9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'That was easy. Let''s do a quick test to demonstrate that load is being balanced
    across more than one container. We''ll use **AB (Apache Bench)** for a quick benchmarking
    and display of responses. Use the following command against our service URL (replace
    the URL for your local service):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'All that matters with the above is that we''ve simulated 100 calls to our server,
    which was done to check if it is responding as expected. We''ll receive output
    similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that the server we''ve scaled across 4 containers has a constant message
    with a unique timestamp?:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The variance in the responses `ab` returned proves that calls to one endpoint
    are being balanced across multiple servers/containers.
  prefs: []
  type: TYPE_NORMAL
- en: If you ever find Minikube in a strange or unbalanced state, just clear its home
    directory and reinstall. For example: `rm -rf ~/.minikube; minikube start`. You
    can also delete the Kubernetes cluster entirely with `minikube delete`.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the command-line tools are extremely useful, you also have access to
    a dashboard for your Kubernetes cluster. You can start up a dashboard to monitor
    your cluster by typing `kubectl proxy` into your terminal. You''ll see something
    similar to this displayed: Starting to serve on 127.0.0.1:8001\. This points to
    the dashboard server. Open the `/ui` path on this server (`127.0.0.1:8001/ui`)
    in a browser. You should see a UI with your cluster fully described:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd87bab9-33d3-4452-b267-d953131fef11.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we see all our pods, statuses, and so forth, in particular the 4/4 scaling
    of our Pod container. Later in the chapter, we'll go a little deeper into how
    you can use the dashboard to inspect your running clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Minikube provides a shortcut that will automatically open this dashboard: `minikube
    dashboard`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at how to use **YAML (Yet Ain't Markup Language)** to create
    Pod declarations, avoiding the manual configuration we've been doing, and simplifying
    deployments later.
  prefs: []
  type: TYPE_NORMAL
- en: Declaring Pod deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll create one Pod with three containers, demonstrating both
    how managing configuration declarations using YAML files simplifies the deployment
    process, and how containers within the same Pod can communicate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'In your code bundle, there will be a directory `/kubernetes` with this layout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Each directory defines a Docker container which defines an Express server which
    will become containers in this Pod. We will treat each of these as an individual
    service, and demonstrate how they might communicate with each other via `localhost`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at the YAML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This manifest is of a `kind` (`Pod`) with a specification (`spec`) defining
    three `containers`, with a shared `volume` (more on that in a second), and a `restartPolicy`
    indicating that containers should only be restarted if they exit with a failure
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Volumes are used when containers need to share data. Within a container, data
    storage is ephemeral -- if your container is restarted, that data is lost. A shared
    volume is held external to the containers within a Pod, and therefore can persist
    data through container restarts and crashes. More importantly, many containers
    in a single Pod can write to and read from a shared volume, creating a shared
    data space. Our services will use this volume as a shared filesystem, and containers
    who wish to use it can add a mount path -- we'll see how that works in just a
    second. For more on volumes, visit: [https://kubernetes.io/docs/concepts/storage/volumes/](https://kubernetes.io/docs/concepts/storage/volumes/).
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, navigate into the `/rerouter` folder and build the docker image: `docker
    build -t rerouter:v1 .`. Note how in the above Pod manifest this image is listed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The `name` for this container is `service-rerouter`, and it provides an Express
    server handling two routes:'
  prefs: []
  type: TYPE_NORMAL
- en: When the root route is called (`/`), it will look for an `index.html` file in
    the `/public` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When `/rerouter` is called, it will redirect the user to another service in
    this Pod, the service listening on port `8086`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If you look at the declaration for `service-rerouter`, you''ll see that it
    has mounted to the shared volume on the path `/app/public`. Any container in this
    Pod can now write to the shared volume and what it writes will end up in this
    container''s `/public` folder (making it available as a static file for serving).
    We create a container service that does just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The `service-os` container will contain a Debian operating system, and will
    mount the shared volume at the path `/pod-data`. Now, any OS operation that writes
    to the filesystem will actually be writing to this shared volume. Using the system
    shell (`/bin/sh`), when this container starts up, it will `echo` an `index.html`
    file to the shared volume with the contents "Another service wrote this!". Since
    this container has nothing else to do after echoing, it will terminate. It is
    for this reason that we set our restart policy to only restart on failure -- we
    don't want this container to be endlessly restarted. This pattern of adding terminating
    "helper" services that contribute to the construction of Pod containers and then
    exit is a common one for Kubernetes deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recalling that `service-rerouter` also declared that its volume mounts `shared-data` on
    path `/app/public`, the `index.html` file generated by `service-os` will now appear
    in that folder, making it available for serving:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Go ahead and build the docker image for the application in folder `/responder` like
    you did for `/rerouter`. The `service-responder` container resolves a single route,
    `/oneroute`, returning a simple message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This container will be used to demonstrate how `service-rerouter` can redirect
    HTTP requests across the (shared) `localhost` Kubernetes has set up for this Pod.
    Since `service-responder` is bound on port `8086`, `service-rerouter` (running
    on port `8087`) can route to it via localhost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we have shown how containers within a Pod can share a common network and
    data volume. Assuming that you''ve successfully built the `rerouter:v1` and `responder:v1`
    Docker images, execute the Pod manifest with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see pod "three-containers" created displayed. Open up the dashboard
    with `minikube dashboard`. You should see the three-containers Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a053b0b1-2e87-4362-a8bb-fedf0e1dff03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on three-containers to bring up a description:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/452ed631-cd9c-473f-a382-9938e77695d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Great, everything is running. Now, let's verify that things are working by connecting
    to our containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Get a shell to `service-router`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Install curl:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Your working directory should have a `/public` folder. There should be an `index.html`
    file in there, created by the `service-os` container. Fetch that file's contents: `cat
    public/index.html`. If everything is working you should see the message "Another
    service wrote this!", which you'll recall is what the `service-os` service created,
    via the shared volume -- a `public/index.html` file that `service-rerouter` will
    serve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s call the `/rerouter` route, which should redirect to the `service-responder`
    server on `localhost:8086/oneroute/` and receive its response "routing service
    works!":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This demonstrates how containers in the same Pod can communicate via localhost
    across port ranges, just as if their containing Pod was a single host machine.
  prefs: []
  type: TYPE_NORMAL
- en: Mesos is another option for orchestration ([http://mesos.apache.org/](http://mesos.apache.org/)),
    as is CoreOS: [https://coreos.com/](https://coreos.com/)
  prefs: []
  type: TYPE_NORMAL
- en: This just scratches the surface of how Docker and Kubernetes can be deployed
    to simplify scaling, especially on a microservice architecture. You can further 
    orchestrate your entire fleet through declarative manifests for services and deployments.
    For example, it is easy to see how the Seneca microservices we designed earlier
    might fit into Pods. It should be clear that you are now able to abstract away
    the implementation details of individual servers and begin to think declaratively,
    simply describing your desired deployment topology (replicas, volumes, recovery
    behavior, and so on) and letting Kubernetes make that a reality, which is much
    nicer than imperatively micromanaging hundreds of thousands of services.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we did a deep dive into various architectural patterns, from
    monoliths to 4-Tier. In the process, we started to consider how we might start
    building highly dynamic applications out of microservices, exploring their general
    advantages in terms of scalability, flexibility, and maintainability. We took
    a look at the Seneca framework for microservices, whose pattern-based execution
    model is easy to construct against and follow, particularly when combined with
    the benefits of autodiscovered mesh services. Jumping into full distributed mode,
    we deployed serverless Lambda functions with Claudia, using API-Gateway to push
    a RESTful API into the AWS cloud, with always-on availability and nearly infinite
    scale at a low cost. With Docker and Kubernetes (with an assist from Minikube),
    we went deep into how to construct clusters of independent virtual machines, declaring
    and deploying Pods of containers that scale on demand.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next and final chapter of this book, we will study possibly the most
    important skill a software developer can develop: how to test and debug your code.
    Now that we have learned to separate the logical parts of our applications out
    into independent pieces, we can start to explore the advantages of such a design
    in terms of testing, both in abstract test harnesses and live code situations.'
  prefs: []
  type: TYPE_NORMAL
