- en: '*Chapter 8*: Concurrency in C++'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The purpose of this chapter is to describe the features for concurrent programming
    that were added to the language recently: in the C++17 and C++20 standards. While
    it is too early to talk about the best practices in using these features for optimum
    performance, we can describe what they do, as well as the current state of the
    compiler support.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction of concurrency into the C++ language in C++11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel STL algorithms in C++17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coroutines in C++20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After reading this chapter, you will know the features that C++ offers to help
    write concurrent programs. The chapter is not meant to be a comprehensive manual
    for C++ concurrency features. Rather, it's an overview of the available language
    facilities, a starting point from which you can further explore the subjects that
    interest you.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you want to experiment with the language features offered by the recent C++
    versions, you will need a very modern compiler. For some features, you may also
    need additional tools installed; we will point this out when we describe a specific
    language feature. The code accompanying this chapter can be found at [https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter08](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency support in C++11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before C++11, the C++ standard made no mention of concurrency. Of course, in
    practice, programmers wrote multi-threaded and distributed programs in C++ long
    before 2011\. What made that possible was the fact that the compiler writers have
    voluntarily adopted additional restrictions and guarantees, usually by way of
    complying with both the C++ standard (for the language) and another standard,
    such as POSIX, for concurrency support.
  prefs: []
  type: TYPE_NORMAL
- en: C++11 has changed that by introducing the **C++ memory model**. The memory model
    describes how threads interact through memory. For the first time, the C++ language
    was on a solid foundation about concurrency. The immediate practical impact, however,
    was rather muted since the new C++ memory model was quite similar to the memory
    models already supported by most compiler writers. There were some subtle differences
    between those models, and the new standard finally guaranteed the portable behavior
    of the programs that encounter these dark corners.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of more immediate practical use were several language features that directly
    supported multi-threading. First of all, the standard introduced the notion of
    a thread. There were notably few guarantees about the behavior of threads, but
    most implementations simply use the system threads to support C++ threads. This
    is fine at the lowest level of the implementation but insufficient for any but
    the simplest program. For instance, a naïve attempt to create a new thread for
    every independent task the program has to perform is almost guaranteed to fail:
    launching new threads takes time, and very few operating systems can handle millions
    of threads efficiently. On the other hand, for the programmers who implemented
    their thread schedulers, the C++ thread interface does not offer sufficient control
    over thread behavior (most thread attributes are OS-specific).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the standard introduced several synchronization primitives for controlling
    concurrent accesses to memory. The language provides `std::mutex`, which is usually
    implemented using the regular system mutex: on POSIX platforms, this is typically
    the POSIX mutex. The standard provides timed and recursive variants of the mutex
    (again, following POSIX). To simplify exception handling, the locking and unlocking
    of mutexes directly should be avoided in favor of the RAII template `std::lock_guard`.'
  prefs: []
  type: TYPE_NORMAL
- en: For locking multiple mutexes safely, without the risk of a deadlock, the standard
    provides the `std::lock()` function (while it guarantees no deadlocks, the algorithm
    it uses is unspecified, and the performance of specific implementations varies
    widely). The other commonly used synchronization primitive is a condition variable,
    `std::condition_variable`, and the respective waiting and signaling operations.
    This functionality also follows the corresponding POSIX features quite closely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, there is support for low-level atomic operations: `std::atomic`, atomic
    operations such as compare-and-swap, and memory order specifiers. We have covered
    their behavior and applications in [*Chapter 5*](B16229_05_Epub_AM.xhtml#_idTextAnchor084),
    *Threads, Memory, and Concurrency*, [*Chapter 6*](B16229_06_Epub_AM.xhtml#_idTextAnchor103),
    *Concurrency and Performance*, and [*Chapter 7*](B16229_07_Epub_AM.xhtml#_idTextAnchor117),
    *Data Structures for Concurrency*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the language added support for asynchronous execution: where a function
    can be invoked asynchronously (possibly on another thread) using `std::async`.
    While this might enable concurrent programming, in practice, this feature is almost
    entirely useless for high-performance applications. Most implementations will
    either provide very limited parallelism or execute each asynchronous function
    call on its own thread. Most operating systems have a rather high overhead for
    creating and joining threads (the only OS I have seen that makes concurrent programming
    as simple as *fire up a thread for every task, millions of them if you need to*
    was AIX, on every other OS I know this is a recipe for chaos).'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we can say that, when it comes to concurrency, C++11 was a major step
    forward conceptually but offered modest immediate practical gains. C++14 improvements
    were focused elsewhere, so nothing of note changed with regard to concurrency.
    Next, we will see what new developments were brought in C++17.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency support in C++17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: C++17 brought with it one major advance and several minor tweaks to concurrency-related
    features. Let us quickly cover the latter first. The `std::lock()` function that
    was introduced in C++11 now has a corresponding RAII object, `std::scoped_lock`.
    A shared mutex, `std::shared_mutex`, otherwise known as a **read-write mutex**,
    was added (again, matching the corresponding POSIX feature). This mutex allows
    multiple threads to proceed as long as they do not need exclusive access to the
    locked resource. Usually, such threads perform read-only operations, while a writer
    thread needs exclusive access, hence the name **read-write lock**. It's a clever
    idea in theory, but most implementations offer dismal performance.
  prefs: []
  type: TYPE_NORMAL
- en: Of note is a new feature that allows portably determining the cache line size
    for L1 cache, `std::hardware_destructive_interference_size`, and `std::hardware_constructive_interference_size`.
    These constants help create cache-optimal data structures that avoid false sharing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we come to the major new feature in C++17 – `std::for_each`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In C++17, we can ask the library to do this computation in parallel on all
    available processors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The parallel versions of STL algorithms have a new first argument: the execution
    policy. Note that the execution policy is not a single type but rather a template
    parameter. The standard provides several execution policies; the parallel policy
    `std::execution::par` that we used earlier allows the algorithm to execute on
    multiple threads. The number of threads and the way the computations are partitioned
    within threads are unspecified and depend on the implementation. The sequential
    policy `std::execution::seq` executes the algorithm on a single thread, the same
    way it''s executed without any policies (or before C++17).'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a parallel unsequenced policy, `std::execution::par_unseq`. The
    difference between the two parallel policies is subtle but important to understand.
    The standard says that the unsequenced policy allows computations to be interleaved
    within a single thread, which allows additional optimizations such as vectorization.
    But an optimizing compiler can use vector instructions like AVX when generating
    machine code, and it''s done without any help from the source C++ code: the compiler
    just finds vectorization opportunities and replaces regular single-word instructions
    with vector ones. So what is different here?'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the nature of the unsequenced policies, we have to consider a
    more complex example. Let us say that, instead of simply operating on every element,
    we want to do some computation that uses shared data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we do some computations on each vector element, then accumulate the sum
    of the results. The computations themselves can be done in parallel, but the accumulation
    must be protected by a lock since all threads increment the same shared variable
    `res`. The parallel execution policy is safe to use, thanks to the lock. However,
    we cannot use an unsequenced policy here: if the same thread were to process multiple
    vector elements at the same time (interleaved), it could attempt to acquire the
    same lock multiple times. This is a guaranteed deadlock: if a thread is holding
    the lock and tries to lock it again, the second attempt will block, and the thread
    cannot proceed to the point where it would have unlocked the lock. The standard
    calls code such as our last example **vectorization-unsafe** and states that such
    code should not be used with unsequenced policies.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen how parallel algorithms work in theory, how about in practice?
    The short answer is *quite well, with some caveats*. Read on for the long version.
  prefs: []
  type: TYPE_NORMAL
- en: Before you can check out parallel algorithms in practice, you have to do some
    work to prepare your built environment. Usually, to compile C++ programs, you
    just need to install the desired compiler version, such as GCC, and you are ready
    to go. Not so with parallel algorithms. At the time this book is written, the
    installation process is somewhat cumbersome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent enough versions of GCC and Clang include parallel STL headers (in some
    installations, Clang requires GCC to be installed because it uses GCC-provided
    parallel STL). The problem appears at the lower level. The runtime threading system
    used by both compilers is Intel **Threading Building Blocks** (**TBB**), which
    comes as a library with its own set of headers. Neither compiler includes TBB
    in its installation. To complicate matters even more, each version of the compiler
    requires the corresponding version of TBB: neither an older nor a more recent
    version will work (the failures can manifest themselves at both compile and link-time).
    To run the programs linked with TBB, you will likely need to add the TBB libraries
    to your library path.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have resolved all these problems and configured a working installation
    of the compiler and necessary libraries, using parallel algorithms is no harder
    than using any STL code. So, how well does it scale? We can run some benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us start with `std::for_each` without any locks and with a lot of computations
    for each element (function `work()` is expensive, the exact operations don''t
    really matter for our current focus on scaling):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the performance of the sequential versus the parallel version running
    on 2 threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Benchmark of parallel std::foreach on 2 CPUs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.1_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Benchmark of parallel std::foreach on 2 CPUs
  prefs: []
  type: TYPE_NORMAL
- en: 'The scaling is not bad. Note that the vector size `N` is fairly large, 32K
    elements. The scaling does improve for larger vectors. However, for relatively
    small amounts of data, the performance of parallel algorithms is very poor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Benchmark of parallel std::foreach for short sequences'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.2_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – Benchmark of parallel std::foreach for short sequences
  prefs: []
  type: TYPE_NORMAL
- en: The parallel version is slower than the sequential version for vectors of 1024
    elements. The reason is that the execution policy starts all the threads at the
    beginning of each parallel algorithm and joins them at the end. Launching new
    threads takes significant time, so when the computation is short, the overhead
    overwhelms any speedup we can get from parallelism. This is not a requirement
    imposed by the standard, but the way the current implementation of parallel STL
    in GCC and Clang manages its interactions with the TBB system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the size for which parallel algorithms improve performance depends
    on the hardware, the compiler and its implementation of parallelism, and the amount
    of computation per element. For example, we can try a very simple per-element
    computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now processing the same 32K-element vector shows no benefit of parallelism:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Benchmark of parallel std::foreach for cheap per-element computations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.3_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Benchmark of parallel std::foreach for cheap per-element computations
  prefs: []
  type: TYPE_NORMAL
- en: For much larger vector sizes, the parallel algorithm may get ahead unless memory
    access speed limits the performance of both single- and multi-threaded versions
    (this is a very much memory-bound computation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps more impressive is the performance of algorithms that are more difficult
    to parallelize, such as `std::sort`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Benchmark of parallel std::sort'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.4_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – Benchmark of parallel std::sort
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we need a sufficiently large amount of data before the parallel algorithm
    becomes effective (for 1024 elements, single-threaded sort is faster). This is
    quite a remarkable achievement: sort is not the easiest algorithm to parallelize,
    and per-element computations on doubles (comparison and swap) are very cheap.
    Nonetheless, the parallel algorithm shows very good speedup, and it gets better
    if the element comparison is more expensive.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You might wonder how parallel STL algorithms interact with your threads, that
    is, what happens if you run two parallel algorithms on two threads simultaneously?
    First of all, like with any code running on multiple threads, you have to ensure
    thread safety (running two sorts on the same container in parallel is a bad idea
    no matter which sort you use). Other than that, you will find that multiple parallel
    algorithms coexist just fine, but you have no control over job scheduling: each
    of them tries to run on all available CPUs, so they compete for the resources.
    Depending on how well each algorithm scales, you may or may not get higher overall
    performance by running several algorithms in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we can conclude that the parallel versions of STL algorithms deliver
    very good performance when they operate on large enough data volumes, although
    what is *large enough* depends on the particular computation. Additional libraries
    may be needed to compile and run programs that use parallel algorithms, and configuring
    these libraries may require some effort, as well as experimentation. Also, not
    all STL algorithms have their parallel equivalents (for example, `std::accumulate`
    does not).
  prefs: []
  type: TYPE_NORMAL
- en: ….
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to flip a few more pages on the calendar and jump forward to
    C++20.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency support in C++20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'C++20 added a few enhancements here and there to the existing concurrency support,
    but we are going to focus on the major new addition: coroutines. Coroutines, in
    general, are functions that can be interrupted and resumed. They are useful in
    several major applications: they can greatly simplify writing event-driven programs,
    they are almost unavoidable for work-stealing thread pools, and they make writing
    asynchronous I/O and other asynchronous code much easier.'
  prefs: []
  type: TYPE_NORMAL
- en: The foundations of coroutines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two styles of coroutines: **stackful** and **stackless**. Stackful
    coroutines are also sometimes called **fibers**; they are similar to functions
    wherein their state is allocated on the stack. Stackless coroutines have no corresponding
    stack allocations, their state is stored on the heap. In general, stackful coroutines
    are more powerful and flexible, but stackless coroutines are significantly more
    efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will focus on stackless coroutines since this is what C++20
    supports. This is a sufficiently unusual concept that we need to explain before
    we show C++-specific syntax and examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'A regular C++ function always has a corresponding stack frame. This stack frame
    exists for as long as the function is running, and that is where all the local
    variables and other states are stored. Here is a simple function `f()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'It has a corresponding stack frame. The function `f()` may call another function,
    `g()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The function `g()` also has a stack frame that exists while the function is
    running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Stack frames of regular functions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.5_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – Stack frames of regular functions
  prefs: []
  type: TYPE_NORMAL
- en: When function `g()` exits, its stack frame is destroyed, and only the frame
    of the function `f()` remains.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, the state of the stackless coroutine is not stored on the stack
    but on the heap: this allocation is called the **activation frame**. The activation
    frame is associated with a coroutine handle, which is an object that acts as a
    smart pointer. Function calls can be made and returned from, but the activation
    frame persists as long as the handle is not destroyed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The coroutine also needs stack space, for example, if it calls other functions.
    This space is allocated on the stack of the caller. Here is how it works (the
    real C++ syntax is different, so think of the coroutine-related lines as pseudocode
    for now):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding memory allocations are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Coroutine call'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.6_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6 – Coroutine call
  prefs: []
  type: TYPE_NORMAL
- en: 'The function `f()` creates a coroutine handle object, which owns the activation
    frame. Then it calls the coroutine function `coro()`. There is some stack allocation
    at this point, in particular, the coroutine stores on the stack the address where
    it would return if it is suspended (remember that coroutines are functions that
    can suspend themselves). The coroutine can call another function `g()`, which
    allocates the stack frame of `g()` on the stack. At this point, the coroutine
    can no longer suspend itself: it is possible to suspend only from the top level
    of the coroutine function. Function `g()` runs the same way no matter who called
    it and eventually returns, which destroys its stack frame. The coroutine can suspend
    itself now, so let us assume that it does.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the key difference between stackful and stackless coroutines: a stackful
    coroutine can be suspended anywhere, at an arbitrary depth of function calls,
    and will resume from that point. But this flexibility has a high cost in memory
    and especially runtime: stackless coroutines, with their limited state allocations,
    are much more efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: When a coroutine suspends itself, parts of the state that are necessary to resume
    it are stored in the activation frame. The stack frame of the coroutine is then
    destroyed, and the control returns to the caller, to the point where the coroutine
    was called. The same happens if the coroutine runs to completion, but there is
    a way for the caller to find out whether the coroutine is suspended or done.
  prefs: []
  type: TYPE_NORMAL
- en: 'The caller continues its execution and may call other functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The memory allocations now look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Coroutine is suspended, execution continues'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.7_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.7 – Coroutine is suspended, execution continues
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that there is no stack frame corresponding to the coroutine, only the
    heap-allocated activation frame. The coroutine can be resumed as long as the handle
    object is alive. It does not have to be the same function that calls and resumes
    the coroutine; for example, our function `h()` can resume it if it has access
    to the handle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The coroutine resumes from the point where it was suspended. Its state is restored
    from the activation frame, and any necessary stack allocations will happen as
    usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Coroutine is resumed from a different function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.8_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.8 – Coroutine is resumed from a different function
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, the coroutine completes, and the handle is destroyed; this deallocates
    all the memory associated with the coroutine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of what is important to know about C++20 coroutines:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Coroutines are functions that can suspend themselves. This is different from
    the OS suspending a thread: suspending a coroutine is done explicitly by the programmer
    (cooperative multitasking).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike regular functions, which are associated with stack frames, coroutines
    have handle objects. Coroutine state persists as long as the handle is alive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the coroutine is suspended, the control is returned to the caller, which
    continues to run the same way as if the coroutine had completed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The coroutine can be resumed from any location; it does not have to be the caller
    itself. Furthermore, the coroutine can even be resumed from a different thread
    (we will see an example later in this section). The coroutine is resumed from
    the point of suspension and continues to run *as if nothing happened* (but may
    be running on a different thread).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let us see how all of this is done in real C++.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutine C++ syntax
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us now see the C++ language constructs that are used for programming with
    coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: The first order of business is getting a compiler that supports this feature.
    Both GCC and Clang have coroutine support in their latest versions, but, unfortunately,
    not in the same way. For GCC, you need version 11 or later. For Clang, partial
    support was added in version 10 and was improved in later versions, although it
    still remains "experimental."
  prefs: []
  type: TYPE_NORMAL
- en: First of all, in order to compile coroutine code, you need a compiler option
    on the command line (merely enabling C++20 with the `--std=c++20` option is not
    enough). For GCC, the option is `–fcoroutines`. For Clang, the options are `-stdlib=libc++
    -fcoroutines-ts`. No options except `/std:c++20` are needed for the latest Visual
    Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you need to include the coroutines header. In GCC and Visual Studio (and
    according to the standard), the header is `#include <coroutine>` and all the classes
    it declares are in namespace `std`. Unfortunately, in Clang, the header is `#include
    <experimental/coroutine>` and the namespace is `std::experimental`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no special syntax for declaring a coroutine: coroutines are, syntactically,
    just regular C++ functions. What makes them into coroutines is the use of the
    suspend operator `co_await` or its variant, `co_yield`. However, it''s not enough
    to call one of these operators in the body of the function: coroutines in C++
    have strict requirements for their return types. The standard library offers no
    help in declaring these return types and other classes necessary for working with
    coroutines. The language provides only a framework for programming with coroutines.
    As a result, the coroutine code that uses C++20 constructs directly is very verbose,
    repetitive, and contains a lot of boilerplate code. In practice, everybody who
    uses coroutines does so using one of several available coroutine libraries.'
  prefs: []
  type: TYPE_NORMAL
- en: For practical programming, so should you. However, in this book, we show you
    examples written in *bare* C++. We do it because we do not want to direct you
    toward any particular library and because doing so would obscure the understanding
    of what is really going on. The support for coroutines is very recent, and the
    libraries are rapidly evolving; it is unlikely that your library of choice will
    stay the same. We would like you to understand the coroutine code at the C++ level
    instead of at the level of abstractions presented by a particular library. Then
    you should choose a library based on your needs and use its abstractions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A thorough description of the syntax constructs related to coroutines would
    be remarkably non-intuitive: it is a framework, not a library. For that reason,
    we do the rest of the presentation using examples. If you really want to know
    all the syntax requirements for coroutines, you have to look up a very recent
    publication (or read the standard). But the examples should give you enough understanding
    of what coroutines can do that you can read the documentation for your favorite
    coroutine library instead and use it in your programs.'
  prefs: []
  type: TYPE_NORMAL
- en: Coroutine examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first example is probably the most common use of coroutines in C++ (and
    the one for which the standard provides some explicitly designed syntax). We are
    going to implement a lazy generator. Generators are functions that generate sequences
    of data; every time you call the generator, you get a new element of the sequence.
    A lazy generator is a generator that computes elements on demand, as it is called.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a lazy generator based on C++20 coroutines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As promised, this is very low-level C++, you rarely see code like this, but
    it allows us to explain all the steps. First of all, the coroutine `coro()` looks
    like any other function, except for the `co_yield` operator. This operator suspends
    the coroutine and returns the value `i` to the caller. Because the coroutine is
    suspended, not terminated, the operator can be executed multiple times. Just like
    any other function, the coroutine terminates when the control reaches the closing
    brace; at this point, it cannot be resumed. It is possible to exit the coroutine
    at any point by calling operator `co_return` (the regular `return` operator should
    not be used).
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, the return type of the coroutine – `generator` – is a special type
    that we are about to define. It has a lot of requirements on it, which results
    in lengthy boilerplate code (any coroutine library will have such types predefined
    for you). We can already see that `generator` contains a nested data member `h_`;
    that is the coroutine handle. The creation of this handle also creates the activation
    frame. The handle is associated with a `promise` object; this has absolutely nothing
    to do with C++11 `std::promise`. In fact, it is not one of the standard types
    at all: we have to define it according to a set of rules listed in the standard.
    At the end of the execution, the handle is destroyed, which destroys the coroutine
    state as well. The handle is, thus, similar to a pointer.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the handle is a callable object. Calling it resumes the coroutine,
    which generates the next value and promptly suspends itself again because the
    `co_yield` operator is in the loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of this is magically tied together by defining the appropriate return type
    for the coroutine. Just like the STL algorithms, the entire system is bound by
    convention: there are expectations on all types involved in this process, and
    something somewhere will not compile if these expectations are not met. Let us
    see the `generator` type now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'First of all, the `return` type does not have to be generated from a template.
    We could have just declared a generator for integers. Usually, it is a template
    parameterized on the type of the elements in the generated sequence. Second, the
    name *generator* is in no way special: you can call this type anything you want
    (most libraries provide a similar template and call it `generator`). On the other
    hand, the nested type `generator::promise_type` *must* be called `promise_type`,
    otherwise, the program will not compile. Often, the nested type itself is called
    something else, and a type alias is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `promise_type` type must be a nested type of the `generator` class (or,
    in general, any type returned by the coroutine). But the `promise` class does
    not have to be a nested class: usually, it is, but it could be declared outside
    as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What is mandatory is the set of required member functions of the `promise`
    type, including their signatures. Note that some of the member functions are declared
    `noexcept`. This is part of the requirement, too: the program will not compile
    if you omit this specification. Of course, any function that is not required to
    be `noexcept` can be declared as such if it doesn''t throw.'
  prefs: []
  type: TYPE_NORMAL
- en: The body of these required functions may be more complex for different generators.
    We will describe briefly what each of them does.
  prefs: []
  type: TYPE_NORMAL
- en: The first non-empty function, `get_return_object()`, is part of the boilerplate
    code and usually looks exactly like the one earlier; this function constructs
    a new generator from a handle that is, in turn, constructed from a promise object.
    It is called by the compiler to get the result of the coroutine.
  prefs: []
  type: TYPE_NORMAL
- en: The second non-empty function, `yield_value()`, is invoked every time the operator
    `co_yield` is called; its argument is the `co_yield` value. Storing the value
    in the promise object is how the coroutine usually passes the results to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: The `initial_suspend()` function is called by the compiler the first time `co_yield`
    is encountered. The `final_suspend()` function is called after the coroutine produces
    its last result via `co_return`; it cannot be suspended afterward. If the coroutine
    ends without `co_return`, the `return_void()` method is called. Finally, if the
    coroutine throws an exception that escapes from its body, the `unhandled_exception()`
    method is called. You can customize these methods for special handling of each
    of these situations, although this is seldom used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we see how it all ties together to provide us with a lazy generator. First,
    the coroutine handle is created. In our example, we do not keep the `generator`
    object, only the handle. This is not required: we could have kept the `generator`
    object and destroyed the handle in its destructor. The coroutine runs until it
    hits `co_yield` and suspends itself; the control is returned by the caller while
    the return value of `co_yield` is captured in the promise. The calling program
    retrieves this value and resumes the coroutine by invoking the handle. The coroutine
    picks up from the point where it was suspended and runs until the next `co_yield`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our generator can run forever (or until we reach the maximum integer value
    on our platform, anyway): the sequence never ends. If we needed a sequence of
    finite length, we can execute `co_return` or just exit the loop after the sequence
    is over. Refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now we have a sequence of 10 elements. The caller must check the result of the
    handle member function `done()` before trying to resume the coroutine.
  prefs: []
  type: TYPE_NORMAL
- en: 'We mentioned before that a coroutine can be resumed from anywhere in the code
    (after it was suspended, of course). It can even be resumed from a different thread.
    In this case, the coroutine starts to execute on one thread, is suspended, and
    then runs the rest of its code on another thread. Let us see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let us get one detail out of the way: `std::jthread` is a C++20 addition,
    it is just a joinable thread – it is joined in the destructor of the object (almost
    anyone who worked with threads wrote a class for that, but now we have a standard
    one). Now we can move to the important part – the coroutine itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us see the return type of the coroutine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This is actually the smallest possible return type of a coroutine: it contains
    all the required boilerplate and nothing else. Specifically, the return type is
    a class that defines a nested type `promise_type`. That nested type must define
    several member functions, as shown in this code. Our generator type from the previous
    example has all of that plus some data used to return the results to the caller.
    Of course, the task can also have an internal state as needed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second change from the previous example is the way the task is suspended:
    we do it with `co_await` instead of `co_yield`. Operator `co_await` is actually
    the most general way to suspend a coroutine: just like `co_yield`, it suspends
    the function and returns the control to the caller. The difference is in the argument
    type: while `co_yield` returns a result, `co_await`''s argument is an awaiter
    object with very general functionality. There are, again, specific requirements
    on the type of this object. If the requirements are met, the class is called an
    `awaitable`, and an object of this type is a valid awaiter (if not, something
    somewhere will not compile). Here is our `awaitable`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The required interface of an `awaitable` is the three methods we see here.
    The first is `await_ready()`: it is called after the coroutine is suspended. If
    it returns `true`, then the result of the coroutine is ready, and it is not really
    necessary to suspend it. In practice, it almost always returns `false`, which
    leads to suspension of the coroutine: the state of the coroutine, such as local
    variables and the suspension point, is stored in the activation frame, and the
    control is returned to the caller or resumer. The second function is `await_resume()`,
    it is called just before the coroutine continues to execute after it is resumed.
    If it returns the result, that is the result of the entire `co_await` operator
    (no result in our example). The most interesting function is `await_suspend()`.
    It is called with the handle of the current coroutine when this coroutine is suspended
    and can have several different return types and values. If it returns `void`,
    as it does in our example, the coroutine is suspended, and the control is returned
    to the caller or resumer. Don''t be fooled by the content of `await_suspend()`
    in our example: it does not resume the coroutine. Instead, it creates a new thread
    that will execute a callable object, and it is this object that resumes the coroutine.
    The coroutine may be resumed after `await_suspend()` is done or while it is still
    running: this example demonstrates the use of coroutines of asynchronous operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting all of this together, we get this sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: The main thread calls a coroutine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The coroutine is suspended by operator `co_await`. This process involves several
    calls to the member functions of the `awaitable` object, one of which creates
    a new thread whose payload resumes the coroutine (the game with move-assigning
    thread objects is done so we delete the new thread in the main program and avoid
    some nasty race conditions).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Control is returned to the caller of the coroutine, so the main thread continues
    to run from the line after the coroutine call. It will block in the destructor
    of the thread object `t` if it gets there before the coroutine completes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The coroutine is resumed by the new thread and continues to execute on that
    thread from the line after `co_await`. The `awaitable` object that was constructed
    by `co_await` is destroyed. The coroutine runs to the end, all on the second thread.
    Reaching the end of the coroutine means it's done, just like any other function.
    The thread that runs the coroutine now can be joined. If the main thread was waiting
    for the destructor of thread `t` to complete, it now unblocks and joins the thread
    (if the main thread has not yet reached the destructor, it won't block when it
    does).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The sequence is confirmed by the output of our program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the coroutine `coro()` was running on one thread first, then
    changed to a different thread in the middle of the execution. If it had any local
    variables, they would be preserved through this transition.
  prefs: []
  type: TYPE_NORMAL
- en: 'We mentioned that `co_await` is the general operator for suspending coroutines.
    Indeed, the `co_yield x` operator is equivalent to a particular invocation of
    `co_await` as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here `promise` is the `promise_type` object associated with the current coroutine
    handle. The reason for the separate operator `co_yield` is that accessing your
    own promise from inside the coroutine results in a quite verbose syntax, so the
    standard added a shortcut.
  prefs: []
  type: TYPE_NORMAL
- en: These examples demonstrate the capabilities of coroutines in C++. The situations
    where coroutines are thought to be useful are work stealing (you have seen how
    easy it is to transfer execution of a coroutine to another thread), lazy generators,
    and asynchronous operations (I/O and event handling). Nonetheless, the C++ coroutines
    have not been around long enough for any patterns to emerge, so the community
    is yet to come up with the best practices for using coroutines. Similarly, it
    is too early to talk about the performance of the coroutines; we have to wait
    for the compiler support to mature and for larger-scale applications to be developed.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, after neglecting concurrency for years, the C++ standard is rapidly
    catching up, so let us summarise the recent advances.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: C++11 was the first version of the standard to acknowledge the existence of
    threads. It laid the foundation for documenting the behavior of C++ programs in
    concurrent environments and provided some useful functionality in the standard
    library. Out of this functionality, the basic synchronization primitives and the
    threads themselves are the most useful. Subsequent versions extended and completed
    these features with relatively minor enhancements.
  prefs: []
  type: TYPE_NORMAL
- en: C++17 brought a major advancement in the form of parallel STL. The performance
    is, of course, determined by the implementation. The observed performance is quite
    good as long as the data corpus is sufficiently large, even on hard-to-parallelize
    algorithms like search and partition. However, if the sequences of data are too
    short, parallel algorithms actually degrade the performance.
  prefs: []
  type: TYPE_NORMAL
- en: C++20 added coroutine support. You have seen how stackless coroutines work,
    in theory and on some basic examples. However, it is too early to talk about the
    performance and best practices for the use of C++20 coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter concludes our exploration of concurrency. Next, we go on to learn
    how the use of the C++ language itself influences the performance of our programs.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why is the foundation of concurrent programming laid in C++11 important?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we use parallel STL algorithms?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are coroutines?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
