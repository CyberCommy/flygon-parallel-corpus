- en: Race Conditions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the concept of race conditions and their potential
    causes in the context of concurrency. The definition of critical section, which
    is a concept highly relevant to race conditions and concurrent programming, will
    also be covered. We will use some example code in Python to simulate race conditions
    and the solutions commonly used to address them. Finally, real-life applications
    that commonly deal with race conditions will be discussed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The basic concept of a race condition, and how it occurs in concurrent applications,
    along with the definition of critical sections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A simulation of a race condition in Python and how to implement race condition
    solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The real-life computer science concepts that commonly interact and work with
    race conditions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Following is the list of prerequisites needed for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you have Python 3 installed on your computer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the GitHub repository at [https://github.com/PacktPublishing/Mastering-Concurrency-in-Python](https://github.com/PacktPublishing/Mastering-Concurrency-in-Python)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During this chapter, we will be working with the subfolder titled `Chapter14`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action: [http://bit.ly/2AdYWRj](http://bit.ly/2AdYWRj)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concept of race conditions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A race condition is typically defined as a phenomenon during which the output
    of a system is indeterminate and dependent on the scheduling algorithm and the
    order in which tasks are scheduled and executed. When the data becomes mishandled
    and corrupted during this process, a race condition becomes a bug in the system.
    Given the nature of this problem, it is quite common for a race condition to occur
    in concurrent systems, which emphasize scheduling and coordinating independent
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A race condition can occur in both an electronic hardware system and a software
    application; in this chapter, we will only be discussing race conditions in the
    context of software development—specifically, concurrent software applications.
    This section will cover the theoretical foundations of race conditions and their
    root causes and the concept of critical sections.
  prefs: []
  type: TYPE_NORMAL
- en: Critical sections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Critical sections indicate shared resources that are accessed by multiple processes
    or threads in a concurrent application, which can lead to unexpected, and even
    erroneous, behavior. We have seen that there are multiple methods to protect the
    integrity of the data contained in these resources, and we call these protected
    sections **critical sections**.
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, the data in these critical sections, when interacted with
    and altered concurrently or in parallel, can become mishandled or corrupted. This
    is especially true when the threads and processes interacting with it are poorly
    coordinated and scheduled. The logical conclusion, therefore, is to not allow
    multiple agents to go into a critical section at the same time. We call this concept
    **mutual exclusion**.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss the relationship between critical sections and the causes of
    race conditions in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: How race conditions occur
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's consider a simple concurrent program, in order to understand what can
    give rise to a race condition. Suppose that the program has a shared resource
    and two separate threads (thread 1 and thread 2) that will access and interact
    with that resource. Specifically, the shared resource is a number and, as per
    their respective execution instructions, each thread is to read in that number,
    increment it by 1, and finally, update the value of the shared resource with the
    incremented number.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that the shared number is originally 2, and then, thread 1 accesses
    and interacts with the number; the shared resource then becomes 3\. After thread
    1 successfully alters and exits the resource, thread 2 begins to execute its instructions,
    and the shared resource that is a number is updated to 4\. Throughout this process,
    the number was originally 2, was incremented twice (each time by a separate thread),
    and held a value of 4 at the end. The shared number was not mishandled and corrupted
    in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine, then, a scenario in which the shared number is still 2 at the beginning,
    yet both of the threads access the number at the same time. Now, each of the threads
    reads in the number 2 from the shared resource, each increments the number 2 to
    3 individually, and then, each writes the number 3 back to the shared resource.
    Even though the shared resource was accessed and interacted with by a thread twice,
    it only held a value of 3 at the end of the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an example of a race condition occurring in a concurrent program: since
    the second thread to access a shared resource does it before the first thread
    finishes its execution (in other words, writing the new value to the shared resource),
    the second thread fails to take in the updated resource value. This leads to the
    fact that, when the second thread writes to the resource, the value that is processed
    and updated by the first thread is overwritten. At the end of the execution of
    the two threads, the shared resource has technically only been updated by the
    second thread.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram further illustrates the contrast between a correct data
    handling process and a situation with a race condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cba8d804-7fd7-4729-95db-a8ca8f5647e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Mishandling shared data
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, we can see that a race condition can result in the mishandling
    and corruption of data. In the preceding example, we can see that a race condition
    can occur with only two separate threads accessing a common resource, causing
    the shared resource to be updated incorrectly and hold an incorrect value at the
    end of the program. We know that most real-life concurrent applications contain
    significantly more threads and processes and more shared resources, and the more
    threads/processes that interact with the shared resource, the more likely it is
    that a race condition will occur.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating race conditions in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we discuss a solution that we can implement to solve the problem of
    race conditions, let''s try to simulate the problem in Python. If you have already
    downloaded the code for this book from the GitHub page, go ahead and navigate
    to the `Chapter14` folder. Let''s take a look at the `Chapter14/example1.py` file—specifically,
    the `update()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The goal of the preceding `update()` function is to increment a global variable
    called `counter`, and it is to be called by a separate thread in our script. Inside
    the function, we are interacting with a shared resource—in this case, `counter`.
    We then assign the value of `counter` to another local variable, called `current_counter`
    (this is to simulate the process of reading data from more complex data structures
    for the shared resources).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will pause the execution of the function by using the `time.sleep()`
    method. The length of the period during which the program will pause is pseudo-randomly
    chosen between `0` and `1`, generated by the function call, `random.randint(0,
    1)`, so the program will either pause for one second or not at all. Finally, we
    assign the newly computed value of `current_counter` (which is its one-increment)
    to the original shared resource (the `counter` variable).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can move on to our main program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are initializing the `counter` global variable with a set of `threading.Thread`
    objects, in order to execute the `update()` function concurrently; we are initializing
    twenty thread objects, to increment our shared counter twenty times. After starting
    and joining all of the threads that we have, we can finally print out the end
    value of our shared `counter` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Theoretically, a well-designed concurrent program will successfully increment
    the share counter twenty times in total, and, since its original value is `0`,
    the end value of the counter should be `20` at the end of the program. However,
    as you run this script, the `counter` variable that you obtain will most likely
    not hold an end value of `20`. The following is my own output, obtained from running
    the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This output indicates that the counter was only successfully incremented nine
    times. This is a direct result of a race condition that our concurrent program
    has. This race condition occurs when a specific thread spends time reading in
    and processing the data from the shared resource (specifically, for one second,
    using the `time.sleep()` method), and another thread reads in the current value
    of the `counter` variable, which, at this point, has not been updated by the first
    thread, since it has not completed its execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, if a thread does not spend anytime processing the data (in other
    words, when `0` is chosen by the pseudo-random `random.randint()` method), the
    value of the shared resource can potentially be updated just in time for the next
    thread to read and process it. This phenomenon is illustrated by the fact that
    the end value of the counter varies within different runs of the program. For
    example, the following is the output that I obtained after running the script
    three times. The output from the first run is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the second run is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the third run is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Again, the final value of the counter is dependent on the number of threads
    that spend one second pausing and the number of threads not pausing at all. Since
    these two numbers are, in turn, dependent on the `random.randint()` method, the
    final value of the counter changes between different runs of the program. We will
    still have a race condition in our program, except for when we can ensure that
    the final value of the counter is always `20` (the counter being successfully
    incremented twenty times, in total).
  prefs: []
  type: TYPE_NORMAL
- en: Locks as a solution to race conditions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will discuss the most common solution to race conditions:
    locks. Intuitively, since the race conditions that we observed arose when multiple
    threads or processes accessed and wrote to a shared resource simultaneously, the
    key idea to solving race conditions is to isolate the executions of different
    threads/processes, especially when interacting with a shared resource. Specifically,
    we need to make sure that a thread/process can only access the shared resource
    after any other threads/processes interacting with the resource have finished
    their interactions with that resource.'
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of locks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With locks, we can turn a shared resource in a concurrent program into a critical
    section, whose integrity of data is guaranteed to be protected. A critical section
    guarantees the mutual exclusion of a shared resource, and cannot be accessed concurrently
    by multiple processes or threads; this will prevent any protected data from being
    updated or altered with conflicting information, resulting from race conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, **Thread B** is blocked from accessing the shared
    resource—the critical section, named `var`—by a mutex (mutual exclusion) lock,
    because **Thread A** is already accessing the resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d4e4feb0-9ba3-46f0-89c3-3b4125b58095.png)'
  prefs: []
  type: TYPE_IMG
- en: Locks prevent simultaneous access to a critical section
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will specify that, in order to gain access to a critical section in
    a concurrent program, a thread or process needs to acquire a lock object that
    is associated with the critical section; similarly, that thread or process also
    needs to release that lock upon leaving the critical section. This setup will
    effectively prevent multiple accesses to the critical section, and will therefore
    prevent race conditions. The following diagram illustrates the execution flow
    of multiple threads interacting with multiple critical sections, with the implementation
    of locks in place:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5b3de2b6-dac3-43e0-a00f-4c30702a0763.png)'
  prefs: []
  type: TYPE_IMG
- en: Locks and critical sections in multiple threads
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the diagram, threads **T1** and **T2** both interact with
    three critical sections in their respective execution instructions: **CS1**, **CS2**,
    and **CS3**. Here, **T1** and **T2** attempt to access **CS1** at almost the same
    time, and, since **CS1** is protected with lock **L1**, only **T1** is able to
    acquire lock **L1**, and hence, access/interact with the critical section, while
    **T2** has to spend time waiting for **T1** to exit out of the critical section
    and release the lock before accessing the section itself. Similarly, for the critical
    sections, **CS2** and **CS3**, although both threads require access to a critical
    section at the same time, only one can process it, while the other has to wait
    to acquire the lock associated with the critical section.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s implement the specification in the preceding example, in order
    to solve the problem of race conditions. Navigate to the `Chapter14/example2.py`
    file and consider our corrected `update()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that all of the execution instructions of a thread specified in
    the `update()` function are under the context manager of a lock object named `count_lock`.
    So, every time a thread is called to run the function, it will have to first acquire
    the lock object, before any instructions can be executed. In our main program,
    we simply create the lock object in addition to what we already had, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the program, and your output should look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the counter was successfully incremented twenty times and held
    the correct value at the end of the program. Furthermore, no matter how many times
    the script is executed, the final value of the counter will always be **20**.
    This is the advantage of using locks to implement critical sections in your concurrent
    programs.
  prefs: []
  type: TYPE_NORMAL
- en: The downside of locks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 12](43dfc569-005f-416d-8492-c0814e403b02.xhtml), *Deadlock*, we
    covered an interesting phenomenon, in which the use of locks can lead to undesirable
    results. Specifically, we found out that, with enough locks implemented in a concurrent
    program, the whole program can become sequential. Let''s analyze this concept
    with our current program. Consider the `Chapter14/example3.py` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Turning a concurrent program sequential
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of this script is to compare the speed of our current concurrent program
    with its sequential version. Here, we are still using the same `update()` function,
    with locks, and we are running it twenty times, both sequentially and concurrently,
    like we did earlier. We are also creating a list of determined periods of pausing,
    so that these periods are consistent between when we simulate the sequential version
    and when we simulate the concurrent version (for this reason, the `update()` function
    now takes in a parameter that specifies the period of pausing each time it is
    called):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'During the next step of the program, we simply call the `update()` function
    inside a `for` loop, with twenty iterations, keeping track of the time it takes
    for the loop to finish. Note that, even though this is to simulate the sequential
    version of the program, the `update()` function still needs the lock object to
    be created prior, so we are initializing it here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step is to reset the counter and run the concurrent version of the
    program that we already implemented. Again, we need to pass in the corresponding
    pause period while initializing each of the threads that run the `update()` function.
    We are also keeping track of the time it takes for this concurrent version of
    the program to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, after you have run the script, you will observe that both the sequential
    version and the concurrent version of our program took the same amount of time
    to run. Specifically, the following is the output that I obtained; in this case,
    they both took approximately 12 seconds. The actual time that your program takes
    might be different, but the speed of the two versions should still be equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'So, our concurrent program is taking just as much time as its sequential version,
    which negates one of the biggest purposes of implementing concurrency in a program:
    improving speed. But why would concurrent and traditional sequential applications with
    the same sets of instructions and elements also have the same speed? Should the
    concurrent program always produce a faster speed than the sequential one?'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that, in our program, the critical section is being protected by a lock
    object, and no multiple threads can access it at the same time. Since all of the
    execution of the program (incrementing the counter for twenty times) depends on
    a thread accessing the critical section, the placement of the lock object on the
    critical section means that only one thread can be executing at a given time.
    With this specification, the executions of any two threads cannot overlap with
    each other, and no additional speed can be gained from this implementation of
    concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the phenomenon that we encountered when analyzing the problem of deadlock:
    if enough locks are placed in a concurrent program, that program will become entirely
    sequential. This is a reason why locks are sometimes undesirable solutions to
    problems in concurrent programming. However, this situation only happens if all
    of the execution of the concurrent program is dependent upon interacting with
    the critical section. Most of the time, reading and manipulating the data of a
    shared resource is only a portion of the entire program and, therefore, concurrency
    still provides the intended additional speed for our program.'
  prefs: []
  type: TYPE_NORMAL
- en: Locks do not lock anything
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An additional aspect of locks is the fact that they do not actually lock anything.
    The only way that a lock object is utilized, with respect to a specific shared
    resource, is for the threads and processes interacting with that resource to also
    interact with the lock. In other words, if those threads and processes choose
    to not check with the lock before accessing and altering the shared resource,
    the lock object itself cannot stop them from doing so.
  prefs: []
  type: TYPE_NORMAL
- en: In our examples, you have seen that, to implement the acquiring/releasing process
    of a lock object, the instructions of a thread or process will be wrapped around
    by a lock context manager; this specification is dependent on the implementation
    of the thread/process execution logic and not the resource. That is because the
    lock objects that we have seen are not connected to the resources that they are
    supposed to protect in any way. So, if the thread/process execution logic does
    not require any interaction with the lock object associated with the shared resource,
    that thread or process can simply gain access to the resource without difficulty,
    potentially resulting in the mismanipulation and corruption of data.
  prefs: []
  type: TYPE_NORMAL
- en: This is not only true in the scope of having multiple threads and processes
    in a single concurrent program. Suppose that we have a concurrent system consisting
    of multiple components that all interact and manipulate the data of a resource
    shared across the system, and this resource is associated with a lock object;
    it follows that, if any of these components fail to interact with that lock, it
    can simply bypass the protection implemented by the lock and access the shared
    resource. More importantly, this characteristic of locks also has implications
    regarding the security of a concurrent program. If an outside, malicious agent
    is connected to the system (say, a malicious client interacting with a server)
    and intends to corrupt the data shared across the system, that agent can be instructed
    to simply ignore the lock object and access that data in an intrusive way.
  prefs: []
  type: TYPE_NORMAL
- en: The view that locks don't lock anything was popularized by Raymond Hettinger,
    a Python core developer who worked on the implementation of various elements in
    Python concurrent programming. It is argued that using lock objects alone does
    not guarantee a secure implementation of concurrent data structures and systems.
    Locks need to be concretely linked to the resources that they are to protect,
    and nothing should be able to access a resource without first acquiring the lock
    that is associated with it. Alternatively, other concurrent synchronization tools,
    such as atomic message queues, can provide a solution to this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Race conditions in real life
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have now learned about the concept of race conditions, how they are caused
    in concurrent systems, and how to effectively prevent them. In this section, we
    will provide an overarching view of how race conditions can occur in real-life
    examples, within the various sub-fields of computer science. Specifically, we
    will be discussing the topics of security, file management, and networking.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrent programming can have significant implications in terms of the security
    of the system in question. Recall that a race condition arises between the process
    of reading and altering the data of a resource; a race condition in an authenticating
    system can cause the corruption of data between the **time of check** (when the
    credentials of an agent are checked) and the **time of use** (when the agent can
    utilize the resource). This problem is also known as a **Time-Of-Check-To-Time-Of-Use**
    (**TOCTTOU**) bug, which is undoubtedly detrimental to security systems.
  prefs: []
  type: TYPE_NORMAL
- en: Careless protection of shared resources when handling race conditions, as we
    briefly touched upon during the last section, can provide external agents with access
    to those supposedly protected resources. Those agents can then change the data
    of the resources to create **privilege escalation** (in simple terms, to give
    themselves more illegal access to more shared resources), or they can simply corrupt
    the data, causing the whole system to malfunction.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, race conditions can also be used to implement computer security.
    As race conditions result from the uncoordinated access of multiple threads/processes
    to a shared resources, the specification in which a race condition occurs is significantly
    random. For example, in our own Python example, you saw that, when simulating
    a race condition, the final value of the counter varies between different executions
    of the program; this is (partly) because of the unpredictable nature of the situation,
    in which multiple threads are running and accessing the shared resources. (I say
    partly, since the randomness also results from the random pausing periods that
    we generate in each execution of the program.) So, race conditions are sometimes
    intentionally provoked, and the information obtained when the race condition occurs
    can be used to generate digital fingerprints for security processes—this information,
    again, is significantly random, and is therefore valuable for security purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Operating systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Race conditions can occur in the context of file and memory management in an
    operating system, when two separate programs attempt to access the same resource,
    such as memory space. Imagine a situation where two processes from different programs
    have been running for a significant amount of time, and, even though they were
    originally initialized apart from each other in terms of memory space, enough
    data has been accumulated and the stack of execution of one process now collides
    with that of the other process. This can lead to the two processes sharing the
    same portion of memory space and can ultimately result in unpredictable consequences.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect of the complexity of race conditions is illustrated by the Unix
    version 7 operating system—specifically, in the `mkdir` command. Typically, the
    `mkdir` command is used to create a new directory in the Unix operating system;
    this is done by calling the `mknod` command to create the actual directory and
    the `chown` command to specify the owner of that directory. Because there are
    two separate commands to be run and a definite gap exists between when the first
    command is finished and the second is called, this can cause a race condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'During the gap between the two commands, if someone can delete the new directory
    created by the `mknod` command and link the reference to another file, when the
    `chown` command is run, the ownership of that file will be changed. By exploiting
    this vulnerability, someone can theoretically change the ownership of any file
    in an operating system so that someone can create a new directory. The following
    diagram further illustrates this exploitation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/66861344-20e7-4db5-9f43-57592c8b370b.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of mkdir race condition
  prefs: []
  type: TYPE_NORMAL
- en: Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In networking, race conditions can take the form of giving multiple users unique
    privileges in a network. Specifically, say a given server should only have exactly
    one user with admin privileges. If two users, who are both eligible to become
    the server admin, request access to those privileges at the same time, then it
    is possible for both of them to gain that access. This is because, at the point
    when both of the user requests are received by the server, neither of the users
    have been granted admin privileges yet, and the server thinks that admin privileges
    can still be given out.
  prefs: []
  type: TYPE_NORMAL
- en: This form of a race condition is quite common when a network is highly optimized
    for parallel processing (for example, non-blocking sockets), without a careful
    consideration of the resources shared across the network.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A race condition is defined as a phenomenon during which the output of a system
    is indeterminate and is dependent on the scheduling algorithm and the order in
    which tasks are scheduled and executed. Critical sections indicate shared resources
    that are accessed by multiple processes or threads in a concurrent application,
    which can lead to unexpected, and even erroneous, behavior. A race condition occurs
    when two or more threads/processes access and alter a shared resource simultaneously,
    resulting in mishandled and corrupted data. Race conditions also have significant
    implications in real-life applications, such as security, operating systems, and
    networking.
  prefs: []
  type: TYPE_NORMAL
- en: Since the race conditions that we observed arose when multiple threads or processes
    accessed and wrote to a shared resource simultaneously, the key idea for solving
    race conditions is to isolate the execution of different threads/processes, especially
    when interacting with a shared resource. With locks, we can turn a shared resource
    in a concurrent program into a critical section, whose integrity of data is guaranteed
    to be protected. However, there are a number of disadvantages to using locks: with
    enough locks implemented in a concurrent program, the whole program might become
    sequential; locks don't lock anything.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will consider one of the biggest problems in Python
    concurrent programming: the infamous **Global Interpreter Lock (GIL)**. You will
    learn about the basic idea behind the GIL, its purposes, and how to effectively
    work with it in concurrent Python applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a critical section?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a race condition and why is it undesirable in a concurrent program?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the underlying cause of race conditions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can locks solve the problem of race conditions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are locks sometimes undesirable in a concurrent program?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the significance of race conditions in real-life systems and applications?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information, you can refer to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Parallel Programming with Python*, by Jan Palach, Packt Publishing Ltd, 2014'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Python Parallel Programming Cookbook*, by Giancarlo Zaccone, Packt Publishing
    Ltd, 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Race Conditions and Critical Sections* ([tutorials.jenkov.com/java-concurrency/race-conditions-and-critical-sections](http://tutorials.jenkov.com/java-concurrency/race-conditions-and-critical-sections.html)),
    by Jakob Jenkov'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Race conditions, files, and security flaws; or the tortoise and the hare redux*,
    by Matt Bishop, Technical Report CSE-95-98(1995)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Computer and Information Security, Chapter 11, Software Flaws and Malware
    1 Illustration* ([slideplayer.com/slide/10319860/](https://slideplayer.com/slide/10319860/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
