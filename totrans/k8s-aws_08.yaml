- en: Sorry My App Ate the Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Kubernetes to run our applications allows us to achieve much higher utilization
    of resources on the machines in our clusters. The Kubernetes scheduler is very
    effective at packing different applications onto your cluster in a way that will
    maximize the use of the resources on each machine. You can schedule a mix of lower-priority
    jobs that can be restarted if needed, for example, batch jobs, and high-priority
    jobs, such as web servers or databases. Kubernetes will help you make use of the
    idle CPU cycles that occur when your web server is waiting for requests.
  prefs: []
  type: TYPE_NORMAL
- en: This is great news if you want to reduce the amount that you are paying AWS,
    for your EC2 instances to run your applications. It is important to learn how
    to configure your pods, so Kubernetes can account for the resource use of your
    applications. If you don't configure your pods correctly, then the reliability
    and performance of your application could be impacted as Kubernetes may need to
    evict your pods from a node because it is running out of resources.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you are going to start by learning how to account for the memory
    and CPU that your pods will use. We will learn how to configure pods with a different
    quality of service so important workloads are guaranteed the resources they need,
    but less important workloads can make use of idle resources when they are available
    without needing dedicated resources. You will also learn how to make use of Kubernetes
    autoscaling facilities to add additional pods to your applications when they are
    under increased load, and to add additional nodes to your cluster when resources
    run low.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Configure container resource requests and limits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure your pods for a desired **Quality of Service** (**QoS**) class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set quotas on the use of resources per namespace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the horizontal pod autoscaler to automatically scale your applications to
    match the demand for them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the cluster autoscaler to automatically provision and terminate EC2 instances
    as the use of your cluster changes over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource requests and limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes allows us to achieve high utilization of our cluster by scheduling
    multiple different workloads to a single pool of machines. Whenever we ask Kubernetes
    to schedule a pod, it needs to consider which node to place it on. The scheduler
    can make much better decisions about where to place a pod if we can give it some
    information about the resources that the pod will need; it then can calculate
    the current workload on each node and choose the node that fits the expected resource
    usage of our pod. We can optionally give Kubernetes this information with resource
    **requests**. Requests are considered at the time when a pod is scheduled to a
    node. Requests do not provide any limit to the amount of resources a pod may consume
    once it is running on a particular node, they just represent an accounting of
    the requests that we, the cluster operator, made when we asked for a particular
    pod to be scheduled to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In order to prevent pods from using more resources than they should, we can
    set resource **limits**. These limits can be enforced by the container runtime,
    to ensure that a pod doesn't use more of a particular resource than required.
  prefs: []
  type: TYPE_NORMAL
- en: We can say that the CPU use of a container is compressible because if we limit
    it, it might result in our processes running more slowly, but typically won't
    cause any other ill effects, whereas the memory use of a container is uncompressible,
    because the only remedy available if a container uses more than its memory limit
    is to kill the container in question.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is very simple to add the configuration for resource limits and requests
    to a pod specification. In our manifests, each container specification can have
    a `resources` field that contains requests and limits. In this example, we request
    that an Nginx web server container is allocated 250 MiB of RAM and a quarter of
    a CPU core. Because the limit is set higher than the request, this allows the
    pod to use up to half a CPU core, and the container will only be killed if its
    memory use exceeds 128 Mi:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Resource units
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever we specify CPU requests or limits, we specify them in terms of CPU
    cores. Because often we want to request or limit the use of a pod to some fraction
    of a whole CPU core, we can either specify this fraction of a CPU as a decimal
    or as a millicore value. For example, a value of 0.5 represents half of a core.
    It is also possible to configure requests or limits with a millicore value. As
    there are 1,000 millicores to a single core, we could specify half a CPU as 500
    m. The smallest amount of CPU that can be specified is 1 m or 0.001.
  prefs: []
  type: TYPE_NORMAL
- en: I find that it can be more readable to use the millicore units in your manifests.
    When using `kubectl` or the Kubernetes dashboard, you will also notice that CPU
    limits and requests are formatted as millicore values. But if you are creating
    manifests with an automated process, you might use the floating point version.
  prefs: []
  type: TYPE_NORMAL
- en: Limits and requests for memory are measured in bytes. But specifying them in
    this way in your manifests would be quite unwieldy and difficult to read. So,
    Kubernetes supports the standard prefixes for referring to multiples of bytes;
    you can choose to use either a decimal multiplier such as M or G, or one of the
    binary equivalents, such as Mi or Gi, which are more commonly used as they reflect
    the actual size of the physical RAM.
  prefs: []
  type: TYPE_NORMAL
- en: The binary versions of these units are actually what most people really mean
    when they are talking about megabytes or gigabytes, even though more correctly
    they are talking about mebibytes and gibibytes!
  prefs: []
  type: TYPE_NORMAL
- en: In practice, you should just always remember to use the units with an **i**
    on the end, or you will end up with slightly less memory than you expected. This
    notation was introduced in the ISO/IEC 80000 standard in 1998, in order to avoid
    confusion between the decimal and binary units.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Decimal** | **Binary** |'
  prefs: []
  type: TYPE_TB
- en: '| **Name** | **Bytes** | **Suffix** | **Name** | **Bytes** | **Suffix** |'
  prefs: []
  type: TYPE_TB
- en: '| kilobyte | 1000 | K | kibibyte | 1024 | Ki |'
  prefs: []
  type: TYPE_TB
- en: '| megabyte | 1000² | M | mebibyte | 1024² | Mi |'
  prefs: []
  type: TYPE_TB
- en: '| gigabyte | 1000³ | G | gibibyte | 1024³ | Gi |'
  prefs: []
  type: TYPE_TB
- en: '| terabyte | 1000⁴ | T | tebibyte | 1024⁴ | Ti |'
  prefs: []
  type: TYPE_TB
- en: '| petabyte | 1000⁵ | P | pebibyte | 1024⁵ | Pi |'
  prefs: []
  type: TYPE_TB
- en: '| exabyte | 1000⁶ | E | exbibyte | 1024⁶ | Ei |'
  prefs: []
  type: TYPE_TB
- en: The memory units supported by Kubernetes
  prefs: []
  type: TYPE_NORMAL
- en: How pods with resource limits are managed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the Kubelet starts a container, the CPU and memory limits are passed to
    the container runtime, which is then responsible for managing the resource usage
    of that container.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using Docker, the CPU limit (in milicores) is multiplied by 100 to
    give the amount of CPU time the container will be allowed to use every 100 ms.
    If the CPU is under load, once a container has used its quota it will have to
    wait until the next 100 ms period before it can continue to use the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: The method used to share CPU resources between different processes running in
    cgroups is called the **Completely Fair Scheduler** or **CFS**; this works by
    dividing CPU time between the different cgroups. This typically means assigning
    a certain number of slices to a cgroup. If the processes in one cgroup are idle
    and don't use their allocated CPU time, these shares will become available to
    be used by processes in other cgroups.
  prefs: []
  type: TYPE_NORMAL
- en: This means that a pod might perform well even if the limit is set too low, but
    could then grind to a halt only later, when another pod begins to take its fair
    share of allocated CPU. You may find that if you begin to set CPU limits on your
    pods on an empty cluster and add additional workloads, the performance of your
    pods begins to suffer.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter, we discuss some basic tooling that can give us an idea
    of how much CPU each pod is using.
  prefs: []
  type: TYPE_NORMAL
- en: If memory limits are reached, the container runtime will kill the container
    (and it might be restarted). If a container is using more memory than the requested
    amount, it becomes a candidate for eviction if and when the node begins to run
    low on memory.
  prefs: []
  type: TYPE_NORMAL
- en: Quality of Service (QoS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When Kubernetes creates a pod, it is assigned one of three QoS classes. These
    classes are used to decide how Kubernetes schedules and evicts pods from nodes.
    Broadly, pods with a guaranteed QoS class will be subject to the least amount
    of disruption from evictions, and pods with the BestEffort QoS class are the most
    likely to be disrupted:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Guaranteed**: This is for high-priority workloads that benefit from avoiding
    being evicted from a node wherever possible and have priority over pods in the
    lower QoS classes for CPU resources, with the container runtime guaranteeing that
    the full amount of the CPU specified in the limit will be available when needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Burstable**: This is for less important workloads, for example, background
    jobs that can take advantage of greater CPU when available but are only guaranteed
    the level specified in the CPU request. Burstable pods are more likely to be evicted
    from a node than those in the Guaranteed QoS class when the node is running low
    on resources, especially if they are using more than the requested amount of memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BestEffort**: Pods with this class are the most likely to be evicted if a
    node is running low on resources. Pods in this QoS class can also only use whatever
    CPU and memory are free on the node at that time, so if other pods running on
    the node are making heavy use of the CPU, these pods may end up completely starved
    of resources. If you schedule Pods in this class, you should ensure that your
    application behaves as expected when subject to resource starvation and frequent
    restarts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, it is always best to avoid using pods with a BestEffort QoS class,
    as these pods will be subject to very unusual behavior when the cluster is under
    heavy load.
  prefs: []
  type: TYPE_NORMAL
- en: When we set the resource and request limits on the containers in our pod, the
    combination of the values decides the QoS class the pod will be in.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be given a QoS class of BestEffort, none of the containers in the pod should
    have any CPU or memory requests or limits set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A pod with no resource limits or requests will be assigned the BestEffort QoS
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be given a QoS class of Guaranteed, a pod needs to have both CPU and memory
    requests and limits set on each container in the pod. The limits and requests
    must match each other. As a shortcut, if a container only has its limits set,
    Kubernetes automatically assigns equal values to the resource requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A pod that will be assigned the Guaranteed QoS class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Anything that falls between these two cases will be given a QoS class of Burstable.
    This applies to any pod where any CPU or memory limits or requests have been set
    on any pods. But where they do not meet the criteria for the Guaranteed class,
    for example by not setting both limits on each container, or by having requests
    and limits that do not match:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A pod that will be assigned the Burstable QoS class.
  prefs: []
  type: TYPE_NORMAL
- en: Resource quotas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Resource quotas allow you to place limits on how many resources a particular
    namespace can use. Depending on how you have chosen to use namespaces in your
    organization, they can give you a powerful way to limit the resources that are
    used by a particular team, application, or group of applications, while still
    giving developers the freedom to tweak the resource limits of each individual
    container.
  prefs: []
  type: TYPE_NORMAL
- en: Resource quotas are a useful tool when you want to control the resource costs
    of different teams or applications, but still want to achieve the utilization
    benefits of scheduling multiple workloads to the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes, resource quotas are managed by an admission controller. This
    controller tracks the use of resources such as pods and services, and if a limit
    is exceeded, it prevents new resources from being created.
  prefs: []
  type: TYPE_NORMAL
- en: The resource quota admission controller is configured by one or more `ResourceQuota`
    objects created in the namespace. These objects would typically be created by
    a cluster administrator, but you could integrate creating them into a wider process
    in your organization for allocating resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example of how a quota can be used to limit the use of CPU
    resources in a cluster. As quotas will affect all the pods within a namespace,
    we will start by creating a new namespace using `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will start by creating a simple example that will ensure that every new
    pod that is created has the CPU limit set, and that the total limits do not exceed
    two cores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Create the `ResourceQuota` by submitting the manifest to the cluster using `kubectl`.
  prefs: []
  type: TYPE_NORMAL
- en: Once a `ResourceQuota` specifying resource requests or limits has been created
    in a namespace, it becomes mandatory for all pods to specify a corresponding request
    or limit before they can be created.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see this behavior in action, let''s create an example deployment in our
    namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have submitted the deployment manifest to Kubernetes with `kubectl`,
    check that the pod is running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, scale up the deployment and observe that additional pods are created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Because we specified a CPU limit of `500m`, there is no problem scaling our
    deployment to four replicas, which uses the two cores that we specified in our
    quota.
  prefs: []
  type: TYPE_NORMAL
- en: 'But if you now try to scale the deployment so it uses more resources than specified
    in the quota, you will find that additional pods are not scheduled by Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Running `kubectl get events` will show you a message where the scheduler failed
    to create the additional pod required to meet the replica count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Default limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you are using quotas on a namespace, one requirement is that every container
    in the namespace must have resource limits and requests defined. Sometimes this
    requirement can cause complexity and make it more difficult to work quickly with
    Kubernetes. Specifying resource limits correctly, while an essential part of preparing
    an application for production, does add additional overhead when, for example,
    using Kubernetes as a platform for development or testing workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides the facility for default requests and limits to be provided
    at the namespace level. You could use this to provide some sensible defaults to
    namespaces used by a particular application or team.
  prefs: []
  type: TYPE_NORMAL
- en: We can configure default limits and requests for the containers in a namespace
    using the `LimitRange` object. This object allows us to provide defaults for the
    CPU or memory, or both. If a `LimitRange` object exists in a namespace, then any
    container created without the resource requests or limits configured in `LimitRange`
    will inherit these values from the limit range.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two situations where `LimitRange` will affect the resource limits
    or requests when a pod is created:'
  prefs: []
  type: TYPE_NORMAL
- en: Containers that have no resource limits or requests will inherit the resource
    limit and requests from the `LimitRange` object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers that have no resource limits but do have requests specified will
    inherit the resource limit from the `LimitRange` object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If a container already has limits and requests defined, then `LimitRange` will
    have no effect. Because containers that specify only limits default the request
    field to the same value, they will not inherit the request value from `LimitRange`.
    Let''s look at a quick example of this in action. We start by creating a new namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a manifest for the limit range object, and submit it to the cluster
    with `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If you create a pod in this namespace without resource limits, it will inherit
    from the limit range object when they are created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`deployment.apps/` example created.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the limits by running `kubectl describe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Horizontal Pod Autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some applications can be scaled up to handle an increased load by adding additional
    replicas. Stateless web applications are a great example of this, as adding additional
    replicas provides the additional capacity required to handle increased requests
    to your application. Some other applications are also designed to operate in such
    a way that adding additional pods can handle increased loads; many systems that
    are architected around processing messages from a central queue can also handle
    an increased load in this way.
  prefs: []
  type: TYPE_NORMAL
- en: When we use Kubernetes deployments to deploy our pod workloads, it is simple
    to scale the number of replicas used by our applications up and down using the
    `kubectl scale` command. However, if we want our applications to automatically
    respond to changes in their workloads and scale to meet demand, then Kubernetes
    provides us with Horizontal Pod Autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal Pod Autoscaling allows us to define rules that will scale the numbers
    of replicas up or down in our deployments based on CPU utilization and optionally
    other custom metrics. Before we are able to use Horizontal Pod Autoscaling in
    our cluster, we need to deploy the Kubernetes metrics server; this server provides
    endpoints that are used to discover CPU utilization and other metrics generated
    by our applications.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the metrics server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can make use of Horizontal Pod Autoscaling, we need to deploy the
    Kubernetes metrics server to our cluster. This is because the Horizontal Pod Autoscaling
    controller makes use of the metrics provided by the `metrics.k8s.io` API, which
    is provided by the metrics server.
  prefs: []
  type: TYPE_NORMAL
- en: While some installations of Kubernetes may install this add-on by default, in
    our EKS cluster we will need to deploy it ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of ways to deploy add-on components to your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: If you have followed the advice in [Chapter 7](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml), *A
    Production-Ready Cluster*, and are provisioning your cluster with Terraform, you
    could provision the required manifests with `kubectl` as we did in [Chapter 7](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml), *A
    Production-Ready Cluster*, when we provisioned kube2iam.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are using helm to manage applications on your cluster, you could use
    the stable/metrics server chart.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, for simplicity we are just going to deploy the metrics server
    manifests using `kubectl.`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I like to integrate deploying add-ons such as the metrics server and kube2iam
    with the process that provisions the cluster, as I see them as integral parts
    of the cluster infrastructure. But if you are going to use a tool like helm to
    manage deploying applications to your cluster, then you might prefer to manage
    everything running on your cluster with the same tool. The decision you take really
    depends on the processes you and your team adopt for managing your cluster and
    the applications that run on it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metrics server is developed in the GitHub repository found at [https://github.com/kubernetes-incubator/metrics-server](https://github.com/kubernetes-incubator/metrics-server)
    You will find the manifests required to deploy it in the deploy directory of that
    repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start by cloning the configuration from GitHub. The metrics server began supporting
    the authentication methods provided by EKS in version 0.0.3 so make sure the manifests
    you have use at least that version.
  prefs: []
  type: TYPE_NORMAL
- en: You will find a number of manifests in the `deploy/1.8+` directory. The `auth-reader.yaml`
    and `auth-delegator.yaml` files configure the integration of the metrics server
    with the Kubernetes authorization infrastructure. The `resource-reader.yaml` file
    configures a role to give the metrics server the permissions to read resources
    from the API server, in order to discover the nodes that pods are running on.
    Basically, `metrics-server-deployment.yaml` and `metrics-server-service.yaml`
    define the deployment used to run the service itself and a service to be able
    to access it. Finally, the `metrics-apiservice.yaml` file defines an `APIService`
    resource that registers the metrics.k8s.io API group with the Kubernetes API server
    aggregation layer; this means that requests to the API server for the metrics.k8s.io
    group will be proxied to the metrics server service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploying these manifests with `kubectl` is simple, just submit all of the
    manifests to the cluster with `kubectl apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You should see a message about each of the resources being created on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a tool like Terraform to provision your cluster, you might
    use it to submit the manifests for the metrics server when you create your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the metrics server and troubleshooting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we continue, we should take a moment to check that our cluster and the
    metrics server are correctly configured to work together.
  prefs: []
  type: TYPE_NORMAL
- en: After the metrics server is running on your cluster and has had a chance to
    collect metrics from the cluster (give it a minute or so), you should be able
    to use the `kubectl top` command to see the resource usage of the pods and nodes
    in your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by running `kubectl top nodes`. If you see output like this, then the
    metrics server is configured correctly and is collecting metrics from your nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: If you see an error message, then there are a number of troubleshooting steps
    you can follow.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should start by describing the metrics server deployment and checking that
    one replica is available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: If it is not, you should debug the created pod by running `kubectl -n kube-system
    describe pod`. Look at the events to see why the server is not available. Make
    sure that you are running at least version 0.0.3 of the metrics server, as previous
    versions didn't support authenticating with the EKS API server.
  prefs: []
  type: TYPE_NORMAL
- en: If the metrics server is running correctly and you still see errors when running
    `kubectl top`, the issue is that the APIservice registered with the aggregation
    layer is not configured correctly. Check the events output at the bottom of the
    information returned when you run `kubectl describe apiservice v1beta1.metrics.k8s.io`.
  prefs: []
  type: TYPE_NORMAL
- en: One common issue is that the EKS control plane cannot connect to the metrics
    server service on port `443`. If you followed the instructions in [Chapter 7](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml),
    *A Production-Ready Cluster*, you should already have a security group rule allowing
    this traffic from the control plane to the worker nodes, but some other documentation
    can suggest more restrictive rules, which might not allow traffic on port `443`.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling pods based on CPU usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the metrics server has been installed into our cluster, we will be able
    to use the metrics API to retrieve information about CPU and memory usage of the
    pods and nodes in our cluster. Using the `kubectl top` command is a simple example
    of this.
  prefs: []
  type: TYPE_NORMAL
- en: The Horizontal Pod Autoscaler can also use this same metrics API to gather information
    about the current resource usage of the pods that make up a deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an example of this; we are going to deploy a sample application
    that uses a lot of CPU under load, then configure a Horizontal Pod Autoscaler
    to scale up extra replicas of this pod to provide extra capacity when CPU utilization
    exceeds a target level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The application we will be deploying as an example is a simple Ruby web application
    that can calculate the nth number in the Fibonacci sequence, this application
    uses a simple recursive algorithm, and is not very efficient (perfect for us to
    experiment with autoscaling). The deployment for this application is very simple.
    It is important to set resource limits for CPU because the target CPU utilization
    is based on a percentage of this limit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We are not specifying a number of replicas in the deployment spec; when we first
    submit this deployment to the cluster, the number of replicas will therefore default
    to 1\. This is good practice when creating a deployment where we intend the replicas
    to be adjusted by a Horizontal Pod Autoscaler, because it means that if we use
    `kubectl apply` to update the deployment later, we won't override the replica
    value the Horizonal Pod Autoscaler has set (inadvertently scaling the deployment
    down or up).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s deploy this application to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: You could run `kubectl get pods -l app=fib` to check that the application started
    up correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a service, so we are able to access the pods in our deployment,
    requests will be proxied to each of the replicas, spreading the load:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Submit the service manifest to the cluster with `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We are going to configure a Horizonal Pod Autoscaler to control the number of
    replicas in our deployment. The `spec` defines how we want the autoscaler to behave;
    we have defined here that we want the autoscaler to maintain between 1 and 10
    replicas of our application and achieve a target average CPU utilization of 60,
    across those replicas.
  prefs: []
  type: TYPE_NORMAL
- en: 'When CPU utilization falls below 60%, then the autoscaler will adjust the replica
    count of the targeted deployment down; when it goes above 60%, replicas will be
    added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the autoscaler with `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `kubectl autoscale` command is a shortcut to create a `HorizontalPodAutoscaler`.
    Running `kubectl autoscale deployment fib --min=1 --max=10 --cpu-percent=60` would
    create an equivalent autoscaler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have created the Horizontal Pod Autoscaler, you can see a lot of interesting
    information about its current state with `kubectl describe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have set up our Horizontal Pod Autoscaler, we should generate some load
    on the pods in our deployment to illustrate how it works. In this case, we are
    going to use the `ab` (Apache benchmark) tool to repeatedly ask our application
    to compute the thirtieth Fibonacci number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This job uses `ab` to make 1,000 requests to the endpoint (with a concurrency
    of 4). Submit the job to the cluster, then observe the state of the Horizontal
    Pod Autoscaler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the load job has started to make requests, the autoscaler will scale up
    the deployment in order to handle the load:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Autoscaling pods based on other metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The metrics server provides APIs that the Horizontal Pod Autoscaler can use
    to gain information about the CPU and memory utilization of pods in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to target a utilization percentage like we did for the CPU metric,
    or to target the absolute value as we have here for the memory metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The Horizonal Pod Autoscaler also allows us to scale on other metrics provided
    by more comprehensive metrics systems. Kubernetes allows for metrics APIs to be
    aggregated for custom and external metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Custom metrics are metrics other than CPU and memory that are associated with
    a pod. You might for example use an adapter that allows you to use metrics that
    a system like Prometheus has collected from your pods.
  prefs: []
  type: TYPE_NORMAL
- en: This can be very beneficial if you have more detailed metrics available about
    the utilization of your application, for example, a forking web server that exposes
    a count of busy worker processes, or a queue processing application that exposes
    metrics about the number of items currently enqueued.
  prefs: []
  type: TYPE_NORMAL
- en: External metrics adapters provide information about resources that are not associated
    with any object within Kubernetes, for example, if you were using an external
    queuing system, such as the AWS SQS service.
  prefs: []
  type: TYPE_NORMAL
- en: On the whole, it is simpler if your applications can expose metrics about resources
    that they depend on that use an external metrics adapter, as it can be hard to
    limit access to particular metrics, whereas custom metrics are tied to a particular
    Pod, so Kubernetes can limit access to only those users and processes that need
    to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The capabilities of Kubernetes Horizontal Pod Autoscaler allow us to add and
    remove pod replicas from our applications as their resource usage changes over
    time. However, this makes no difference to the capacity of our cluster. If our
    pod autoscaler is adding pods to handle an increase in load, then eventually we
    might run out of space in our cluster, and additional pods would fail to be scheduled.
    If there is a decrease in the load on our application and the pod autoscaler removes
    pods, then we are paying AWS for EC2 instances that will sit idle.
  prefs: []
  type: TYPE_NORMAL
- en: When we created our cluster in [Chapter 7](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml),
    *A Production-Ready Cluster*, we deployed the cluster nodes using an autoscaling
    group, so we should be able to use this to grow and shrink the cluster as the
    needs of the applications deployed to it change over time.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling groups have built-in support for scaling the size of the cluster,
    based on the average CPU utilization of the instances. This, however, is not really
    suitable when dealing with a Kubernetes cluster because the workloads running
    on each node of our cluster might be quite different, so the average CPU utilization
    is not really a very good proxy for the free capacity of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, in order to schedule pods to nodes effectively, Kubernetes keeps
    track of the capacity of each node and the resources requested by each pod. By
    utilizing this information, we can automate scaling the cluster to match the size
    of the workload.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes autoscaler project provides a cluster autoscaler component for
    some of the main cloud providers, including AWS. The cluster autoscaler can be
    deployed to our cluster quite simply. As well as being able to add instances to
    our cluster, the cluster autoscaler is also able to drain the pods from and then
    terminate instances when the capacity of the cluster can be reduced.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the cluster autoscaler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying the cluster autoscaler to our cluster is quite simple as it just requires
    a simple pod to be running. All we need for this is a simple Kubernetes deployment,
    just as we have used in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order for the cluster autoscaler to update the desired capacity of our autoscaling
    group, we need to give it permissions via an IAM role. If you are using kube2iam,
    as we discussed in [Chapter 7](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml), *A
    Production-Ready Cluster*, we will be able to specify this role for the cluster
    autoscaler pod via an appropriate annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In order to deploy the cluster autoscaler to our cluster, we will submit a deployment
    manifest using `kubectl`, in a similar way to how we deployed kube2iam in [Chapter
    7](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml), *A Production-Ready Cluster*.
    We will use Terraform's templating system to produce the manifest.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a service account that is used by the autoscaler to connect to the
    Kubernetes API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The cluster autoscaler needs to read information about the current resource
    usage of the cluster, and needs to be able to evict pods from nodes that need
    to be removed from the cluster and terminated. Basically, `cluster-autoscalerClusterRole`
    provides the required permissions for these actions. The following is the code
    continuation for `cluster_autoscaler.tpl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `cluster-autoscaler` stores state information in a config map, so
    needs permissions to be able to read and write from it. This role allows that.
    The following is the code continuation for `cluster_autoscaler.tpl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Finally, let's consider the manifest for the cluster autoscaler deployment itself.
    The cluster autoscaler pod contains a single container running the cluster autoscaler
    control loop. You will notice that we are passing some configuration to the cluster
    autoscaler as command-line arguments. Most importantly, the `--node-group-auto-discovery`
    flag allows the autoscaler to operate on autoscaling groups with the `kubernetes.io/cluster/<cluster_name>`
    tag that we set on our autoscaling group when we created the cluster in [Chapter
    7](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml), *A Production-Ready Cluster*.
    This is convenient because we don't have to explicitly configure the autoscaler
    with our cluster autoscaling group.
  prefs: []
  type: TYPE_NORMAL
- en: If your Kubernetes cluster has nodes in more than one availability zone and
    you are running pods that rely on being scheduled to a particular zone (for example,
    pods that are making use of EBS volumes), it is recommended to create an autoscaling
    group for each availability zone that you plan to use. If you use one autoscaling
    group that spans several zones, then the cluster autoscaler will be unable to
    specify the availability zone of the instances that it launches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code continuation for `cluster_autoscaler.tpl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we render the templated manifest by passing in the variables for the
    AWS region, cluster name and IAM role, and submitting the file to Kubernetes using
    `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code continuation for `cluster_autoscaler.tpl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is a powerful tool; it is very effective at achieving much higher
    usage of compute resources than would ever be possible by manually scheduling
    applications to machines. It is important that you learn how to allocate resources
    to your pods by setting the correct resource limits and requests; if you don't,
    your applications can become unreliable or starved of resources.
  prefs: []
  type: TYPE_NORMAL
- en: By understanding how Kubernetes assigns Quality of Service classes to your pods
    based on the resource requests and limits that you assign them, you can have precisely
    control how your pods are managed. By ensuring your critical applications, such
    as web servers and databases, run with the Guaranteed class, you can ensure that
    they will perform consistently and suffer minimal disruption when pods need to
    be rescheduled. You can improve the efficiency of your cluster by setting limits
    on lower-priority pods that will result in them being scheduled with the Burstable
    QoS class. Burstable pods can use extra resources when they are available but
    won't need extra capacity to be added to the cluster when load increases.
  prefs: []
  type: TYPE_NORMAL
- en: Resource quotas can be invaluable when managing a large cluster that is used
    to run several applications, and even by different teams in your organization,
    especially if you are trying to control the cost of non-production workloads,
    such as testing and staging environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS calls its machines elastic for a reason: they can be scaled up or down
    in a matter of minutes to meet the demands of your applications. If you run workloads
    on a cluster where the load is variable, then you should make use of these properties
    and the tools that Kubernetes provides to scale your deployments to match the
    load that your applications are receiving, and your cluster to the size of the
    pods that need to be scheduled.'
  prefs: []
  type: TYPE_NORMAL
