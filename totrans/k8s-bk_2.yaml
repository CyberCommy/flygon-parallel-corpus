- en: '2: Kubernetes principles of operation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we’ll learn about the major components needed to build a Kubernetes
    cluster and deploy an app. The aim of the game is to give you an overview of the
    major concepts. But don’t worry if you don’t understand everything straight away,
    we’ll cover most things again as we progress through the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll divide the chapter as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes from 40K feet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Masters and nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packaging apps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Declarative configuration and desired state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes from 40K feet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the highest level, Kubernetes is an orchestrator of cloud-native microservices
    apps. This is just a fancy name for an application that’s made from lots of small
    independent services that work together to form a useful app.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a quick analogy.
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, a football (soccer) team is made of individuals. No two are
    the same, and each has a different role to play in the team - some defend, some
    attack, some are great at passing, some are great at shooting…. Along comes the
    coach, and he or she gives everyone a position and organizes them into a team
    with a purpose. We go from Figure 2.1 to Figure 2.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1](Image00005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2](Image00006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2
  prefs: []
  type: TYPE_NORMAL
- en: The coach also makes sure that the team maintains its formation, sticks to the
    plan, and deals with any injuries. Well guess what… microservices apps in the
    Kubernetes world are the same!
  prefs: []
  type: TYPE_NORMAL
- en: Stick with me on this…
  prefs: []
  type: TYPE_NORMAL
- en: We start out with lots of individual specialised services - some serve web pages,
    some do authentication, some do searches, others persist data. Kubernetes comes
    along - a bit like the coach in the football analogy – and organizes everything
    into a useful app and keeps things running smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: In the sports world we call this *coaching* . In the application world we call
    it *orchestration* .
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is an *orchestrator* .
  prefs: []
  type: TYPE_NORMAL
- en: To make this happen, we start out with an app, package it up and give it to
    the cluster (Kubernetes). The cluster is made up of one or more *masters* , and
    a bunch of *nodes* .
  prefs: []
  type: TYPE_NORMAL
- en: The masters are in-charge of the cluster and make all the scheduling decisions.
    They also monitor the cluster, implement changes, and respond to events. For this
    reason, we often refer to the masters as the *control plane* .
  prefs: []
  type: TYPE_NORMAL
- en: The nodes are where application services run, and we sometimes call them the
    *data plane* . They have a reporting line back to the masters, and constantly
    watch for new work assignments.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run applications on a Kubernetes cluster we follow this simple pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: Write the application as small independent services in our favourite languages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Package each service in its own container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrap each container in its own Pod.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy Pods to the cluster via higher-level objects such as; *Deployments, DaemonSets,
    StafeulSets, CronJobs etc.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’re still near the beginning of the book and you’re not expected to know what
    all of these terms mean yet. But at a high-level, *Deployments* offer scalability
    and rolling updates, *DaemonSets* run one instance of a Pod on every node in the
    cluster, *StatefulSets* are for stateful components of your app, and *CronJobs*
    are for work that needs to run at set times. There are more options, but these
    will do for now.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes likes to manage applications *declaratively* . This is a pattern
    where we describe how we want our application to look and feel in a set of YAML
    files, `POST` these files to Kubernetes, then sit back while Kubernetes makes
    it all happen.
  prefs: []
  type: TYPE_NORMAL
- en: But it doesn’t stop there. Kubernetes constantly watches the different parts
    of our application to make sure it’s running exactly the way it should. If something
    isn’t as it should be, Kubernetes tries to fix it.
  prefs: []
  type: TYPE_NORMAL
- en: That’s the big picture. Let’s dig a bit deeper.
  prefs: []
  type: TYPE_NORMAL
- en: Masters and nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Kubernetes cluster is made of masters and nodes. These are Linux hosts that
    can be VMs, bare metal servers in your data center, or instances on a private
    or public cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Masters (control plane)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A Kubernetes master is a collection of system services that make up the control
    plane of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest setups run all the master *services* on a single host. However,
    multi-master HA is becoming more and more important, and is a **must have** for
    production environments. This is why the major cloud providers implement highly
    available masters as part of their Kubernetes-as-a-Service platforms such as AKS,
    EKS, and GKE.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also considered a good practice **not** to run application workloads on
    masters. This allows masters to concentrate entirely on managing the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a quick look at the major pieces of a Kubernetes master that make
    up the control plane.
  prefs: []
  type: TYPE_NORMAL
- en: The API server
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The API server is the front door into Kubernetes. It exposes a RESTful API that
    we `POST` YAML configuration files to. These YAML files, which we sometimes call
    *manifests* , contain the desired state of our application. This includes things
    like; which container image to use, which ports to expose, and how many Pod replicas.
  prefs: []
  type: TYPE_NORMAL
- en: All requests to the API Server are subject to authentication and authorization
    checks, but once these are done, the config in the YAML file is validated, persisted
    to the cluster store, and deployed to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of the API server as the brains of the cluster - where the smarts
    are implemented.
  prefs: []
  type: TYPE_NORMAL
- en: The cluster store
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If the API server is the brains of the cluster, the *cluster store* is its memory.
    It’s the only stateful part of the control plane, and it persistently stores the
    entire configuration and state of the cluster. As such, it’s a vital component
    of the cluster - no cluster store, no cluster!
  prefs: []
  type: TYPE_NORMAL
- en: The cluster store is based on **etcd** , a popular distributed database. As
    it’s the *single source of truth* for the cluster, you should take care to protect
    it and provide adequate ways to recover when things go wrong.
  prefs: []
  type: TYPE_NORMAL
- en: The controller manager
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The controller manager is a *controller of controllers* and is a bit of a monolith.
    Although it runs as a single process, it implements several control loops that
    watch the cluster and respond to events. Some of these control loops include;
    the node controller, the endpoints controller, and the namespace controller. Each
    one generally runs as a background watch-loop that is constantly watching the
    API Server for changes – the aim of the game is to ensure the *current state*
    of the cluster matches the *desired state* (more on this shortly).
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** Throughout the book we’ll use terms like *control loop, watch loop,*
    and *reconciliation loop* to mean the same thing.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The scheduler
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: At a high level, the scheduler watches for new work and assigns it to nodes.
    Behind the scenes, it evaluates affinity and anti-affinity rules, constraints,
    and resource management.
  prefs: []
  type: TYPE_NORMAL
- en: The cloud controller manager
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you’re running your cluster on a supported public cloud platform, such as
    AWS, Azure, or GCP, your control plane will be running a *cloud controller manager*
    . It’s job is to manage integrations with your underlying cloud platform, such
    as nodes, load-balancers, and storage.
  prefs: []
  type: TYPE_NORMAL
- en: Control Plane summary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Kubernetes masters run all of the cluster’s control plane services. Think of
    it as brains of the cluster - where all the control and scheduling decisions are
    made. Behind the scenes, a master is made up of lots of small specialized services.
    These include the API server, the cluster store, the controller manager, and the
    scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: The API Server is the front-end into the control plane and the only component
    in the control plane that we interact with directly. By default, it exposes a
    RESTful endpoint on port 443.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 shows a high-level view of a Kubernetes master (control plane).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 - Kubernetes Master](Image00007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 - Kubernetes Master
  prefs: []
  type: TYPE_NORMAL
- en: Nodes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Nodes* are the workers of a Kubernetes cluster. At a high-level they do three
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: Watch the API Server for new work assignments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute new work assignments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Report back to the control plane
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can see from Figure 2.4, they’re are a bit simpler than *masters* . Let’s
    look at the three major components of a node.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 - Kubernetes Node (formerly Minion)](Image00008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 - Kubernetes Node (formerly Minion)
  prefs: []
  type: TYPE_NORMAL
- en: Kubelet
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Kubelet is the main Kubernetes agent that runs on all nodes in the cluster.
    In fact, it’s common to use the terms *node* and *kubelet* interchangeably. You
    install the kubelet on a Linux host, which registers the host with the cluster
    as a *node* . It then watches the API server for new work assignments. Any time
    it sees one, it carries out the task and maintains a reporting channel back to
    the master.
  prefs: []
  type: TYPE_NORMAL
- en: If the kubelet can’t run a particular work task, it reports back to the master
    and lets the control plane decide what actions to take. For example, if a Pod
    fails on a node, the kubelet is **not** responsible for finding another node to
    run it on. It simply reports back to the control plane and the control plane decides
    what to do.
  prefs: []
  type: TYPE_NORMAL
- en: Container runtime
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Kubelet needs a container runtime to perform container-related tasks – things
    like pulling images, and starting and stopping containers.
  prefs: []
  type: TYPE_NORMAL
- en: In the early days, Kubernetes had native support for a few container runtimes
    such as Docker. More recently, it has moved to a plugin model called the Container
    Runtime Interface (CRI). This is an abstraction layer for external (3rd-party)
    container runtimes to plug in to. Basically, the CRI masks the internal machinery
    of Kubernetes and exposes a clean documented interface for 3rd-party container
    runtimes to interface with.
  prefs: []
  type: TYPE_NORMAL
- en: The CRI is the supported method for integrating runtimes into Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of container runtimes available for Kubernetes. `cri-containerd`
    is a community-based open-source project porting the CNCF `containerd` runtime
    to the CRI interface. It has a lot of support and is replacing Docker as the most
    popular container runtime used in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** `containerd` (pronounced “container-dee”) is the container supervisor
    and runtime logic stripped out of the Docker Engine. It was donated to the CNCF
    by Docker, Inc. and has a lot of community support. Other CRI-compliant container
    runtimes exist.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Kube-proxy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The last piece of the *node* puzzle is the kube-proxy. This runs on every node
    in the cluster and is responsible for local networking. For example, it makes
    sure each node gets its own unique IP address, and implements local IPTABLES or
    IPVS rules to handle routing and load-balancing of certain traffic types.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the fundamentals of masters and nodes, let’s switch gears
    and look at how we package applications to run on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Packaging apps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order for an application to run on a Kubernetes cluster it needs to tick
    a few boxes. These include:'
  prefs: []
  type: TYPE_NORMAL
- en: Packaged as a container
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrapped in a Pod
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deployed via a manifest file (Pod, Deployment. DaemonSet…)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It goes like this… We write our code in our favourite language. We build that
    into a container image and store it in a registry. At this point, our code is
    *containerized* .
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define a Kubernetes Pod to hold our containerized app. At the kind
    of high level we’re at, a Pod is just a wrapper that allows containers to run
    on a Kubernetes cluster. Once we’ve defined a Pod for our container, we’re ready
    to deploy it on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes offers several objects for deploying and managing Pods. The most
    common is the *Deployment* , which offers scalability, self-healing, and rolling
    updates. They’re defined in a YAML files and specify things like - which Pod to
    deploy and how many replicas to deploy.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 shows application code packaged as a *container* , running inside
    a *Pod* , managed by a *Deployment* .
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 - Kubernetes Node (formerly Minion)](Image00009.gif)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 - Kubernetes Node (formerly Minion)
  prefs: []
  type: TYPE_NORMAL
- en: Once everything is defined in the *Deployment* YAML file, we `POST` it to the
    cluster as the *desired state* of our application and let Kubernetes implement
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of desired state…
  prefs: []
  type: TYPE_NORMAL
- en: The declarative model and desired state
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *declarative model* and the concept of *desired state* are two things at
    the very heart of Kubernetes. Take them away and Kubernetes crumbles!
  prefs: []
  type: TYPE_NORMAL
- en: 'In Kubernetes, the declarative model works like this:'
  prefs: []
  type: TYPE_NORMAL
- en: We declare the desired state of an application (microservice) in a manifest
    file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We POST it to the Kubernetes API server
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kubernetes stores this in the cluster store as the application’s *desired state*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kubernetes implements the desired state on the cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kubernetes implements watch loops to make sure the *current state* of the application
    doesn’t vary from *desired state*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s look at each step in a bit more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Manifest files are written in simple YAML, and they tell Kubernetes how we want
    an application to look. We call this is the *desired state* . It includes things
    such as; which image to use, how many replicas to have, which network ports to
    listen on, and how to perform updates.
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve created the manifest, we `POST` it to the API server. The most common
    way of doing this is with the `kubectl` command-line utility. This POSTs the manifest
    as a request to the control plane, usually on port 443.
  prefs: []
  type: TYPE_NORMAL
- en: Once the request is authenticated and authorized, Kubernetes inspects the manifest,
    identifies which controller to send it to (e.g. the *Deployments controller* ),
    and records the config in the cluster store as part of the cluster’s overall *desired
    state* . Once this is done, the work gets scheduled on the cluster. This includes
    the hard work of pulling images, starting containers, and building networks.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Kubernetes sets up background reconciliation loops that constantly
    monitor the state of the cluster. If the *current state* of the cluster varies
    from the *desired state* , Kubernetes will perform whatever tasks are necessary
    to reconcile the issue.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6](Image00010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to understand that what we’ve described is the opposite of the
    traditional *imperative model* . The imperative model is where we issue long lists
    of platform-specific commands to build things.
  prefs: []
  type: TYPE_NORMAL
- en: Not only is the declarative model a lot simpler than long lists of imperative
    commands, it also enables self-healing, scaling, and lends itself to version control
    and self-documentation! It does this by telling the cluster *how things should
    look* . If they stop looking like this, the cluster notices the discrepancy and
    does all of the hard work to reconcile the situation (self-heals).
  prefs: []
  type: TYPE_NORMAL
- en: But the declarative story doesn’t end there - things go wrong, and things change.
    When they do, the ***current state*** of the cluster no longer matches the ***desired
    state*** . As soon as this happens, Kubernetes kicks into action and attempts
    to bring the two back into harmony.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: Assume we have an app with a desired state that includes 10 replicas of a web
    front-end Pod. If a node that was running two replicas fails, the *current state*
    will be reduced to 8 replicas, but the *desired state* will still be 10\. This
    will be observed by a reconciliation loop and Kubernetes will schedule two new
    replicas on other nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The same thing will happen if we intentionally scale the desired number of replicas
    up or down. We could even change the image we want to use. For example, if the
    app is currently using `v2.00` of an image, and we update the desired state to
    use `v2.01` , Kubernetes will notice the discrepancy and go through the process
    of updating all replicas so that they are using the new image specified in the
    *desired state* .
  prefs: []
  type: TYPE_NORMAL
- en: To be clear. Instead of writing a long list of commands that will update every
    replica to the new version, we simply tell Kubernetes we want the new version,
    and Kubernetes does the hard work for us.
  prefs: []
  type: TYPE_NORMAL
- en: Despite how simple this might seem, it’s extremely powerful! It’s also at the
    very heart of how Kubernetes operates. We give Kubernetes a declarative manifest
    that describes how we want an application to look. This forms the basis of the
    application’s desired state. The Kubernetes control plane records it, implements
    it, and runs background reconciliation loops that constantly check what is running
    is what we’ve asked for. When current state matches desired state, the world is
    a happy place. When it doesn’t, Kubernetes gets busy and fixes it.
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the VMware world, the atomic unit of scheduling is the virtual machine (VM).
    In the Docker world, it’s the container. Well… in the Kubernetes world, it’s the
    ***Pod*** .
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 - Atomic units of scheduling](Image00011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 - Atomic units of scheduling
  prefs: []
  type: TYPE_NORMAL
- en: Pods and containers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It’s true that Kubernetes runs containerized apps. But you cannot run a container
    directly on a Kubernetes cluster - containers must **always** run inside of Pods!
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest model is to run a single container per Pod. However, there are
    advanced use-cases that run multiple containers inside of a single Pod. These
    *multi-container Pods* are beyond the scope of what we’re discussing here, but
    powerful examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Service meshes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web containers supported by a *helper* container that pulls the latest content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers with a tightly coupled log scraper tailing the logs off to a logging
    service somewhere else.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just three simple examples. Figure 2.8 shows a multi-container Pod.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8](Image00012.gif)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8
  prefs: []
  type: TYPE_NORMAL
- en: Pod anatomy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At the highest-level, a *Pod* is a ring-fenced environment to run containers.
    The Pod itself doesn’t actually run anything, it’s just a sandbox for hosting
    containers. Keeping it high level, you ring-fence an area of the host OS, build
    a network stack, create a bunch of kernel namespaces, and run one or more containers
    in it - that’s a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re running multiple containers in a Pod, they all share the **same environment**
    - things like the IPC namespace, shared memory, volumes, network stack etc. As
    an example, this means that all containers in the same Pod will share the same
    IP address (the Pod’s IP). This is shown in Figure 2.8.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9](Image00013.gif)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9
  prefs: []
  type: TYPE_NORMAL
- en: If two containers in the same Pod need to talk to each other (container-to-container
    within the Pod) they can use the Pod’s `localhost` interface as shown in Figure
    2.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10](Image00014.gif)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10
  prefs: []
  type: TYPE_NORMAL
- en: Multi-container Pods are ideal when you have requirements for tightly coupled
    containers that may need to share memory and storage etc. However, if you don’t
    **need** to tightly couple your containers, you should put them in their own Pods
    and loosely couple them over the network - this keeps things clean by each container
    only performing a single task.
  prefs: []
  type: TYPE_NORMAL
- en: Pods as the atomic unit
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pods are also the minimum unit of scheduling in Kubernetes. If you need to scale
    your app, you add or remove Pods. You **do not** scale by adding more containers
    to an existing Pod! Multi-container Pods are only for situations where two different,
    but complimentary, containers need to share resources. Figure 2.11 shows how to
    scale the `nginx` front-end of an app using multiple Pods as the unit of scaling.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 - Scaling with Pods](Image00015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 - Scaling with Pods
  prefs: []
  type: TYPE_NORMAL
- en: The deployment of a Pod is an atomic operation. This means that a Pod is either
    entirely deployed, or not deployed at all. There is never a situation where you
    have a partially deployed Pod servicing requests. The entire Pod either comes
    up and is put into service, or it doesn’t, and it fails.
  prefs: []
  type: TYPE_NORMAL
- en: A Pod can only exist on a single node. This is also true of multi-container
    Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Pod lifecycle
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pods are mortal. They’re created, they live, and they die. If they die unexpectedly,
    we don’t bring them back to life! Instead, Kubernetes starts a new one in its
    place. But despite the fact that the new Pod looks, smells, and feels like the
    old one, it’s not! It’s a shiny new Pod with a shiny new ID and IP address.
  prefs: []
  type: TYPE_NORMAL
- en: This has implications on how we should build our applications. Don’t build them
    so that they are tightly coupled to a particular instance of a Pod. Instead, build
    them so that when Pods fail, a totally new one (with a new ID and IP address)
    can pop up somewhere else in the cluster and seamlessly take its place.
  prefs: []
  type: TYPE_NORMAL
- en: Deployments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We normally deploy Pods indirectly as part of something bigger. Examples include;
    *Deployments* , *DaemonSets* , and *StatefulSets* .
  prefs: []
  type: TYPE_NORMAL
- en: For example, a Deployment is a higher-level Kubernetes object that wraps around
    a set of Pods and adds features such as scaling, zero-downtime updates, and versioned
    rollbacks.
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, they implement a controller and a watch loop that is always
    watching the cluster to make sure that the current state matches the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: Deployments have existed in Kubernetes since version 1.2, and were promoted
    to GA (stable) in 1.9\. You’ll see them a lot.
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve just learned that Pods are mortal and can die. However, if they’re deployed
    via Deployments or DaemonSets, they get replaced when they fail. But the new ones
    come with totally different IPs! This also happens when we perform scaling operations
    - scaling up adds new Pods with new IP addresses, whereas scaling down takes existing
    Pods away. Events like these cause a lot of IP churn.
  prefs: []
  type: TYPE_NORMAL
- en: The take-home point is that **Pods are unreliable** . But this poses a challenge…
    Assume we’ve got a microservices app with a bunch of Pods performing video rendering.
    How will this work if other parts of the app that need to use the rendering service
    can’t rely on the Pods being there when they need them?
  prefs: []
  type: TYPE_NORMAL
- en: This is where *Services* come in to play. **Services provide reliable networking
    for a set of Pods.**
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.12 shows the uploader microservice talking to the renderer microservice
    via a Service. The Service is providing a reliable name and IP, and is load-balancing
    requests across the two renderer Pods behind it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12](Image00016.gif)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12
  prefs: []
  type: TYPE_NORMAL
- en: Digging in to a bit more detail. Services are fully-fledged objects in the Kubernetes
    API - just like Pods and Deployments. They have a front-end that consists of a
    stable DNS name, IP address, and port. On the back-end, they load-balance across
    a dynamic set of Pods. Pods come and go, the Service observes this, automatically
    updates itself, and continues to provide that stable networking endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: The same applies if we scale the number of Pods up or down. New Pods are seamlessly
    added to the Service, whereas terminated Pods are seamlessly removed.
  prefs: []
  type: TYPE_NORMAL
- en: So that’s the job of a Service – it’s a stable network abstraction point that
    load-balances traffic across a dynamic set of Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting Pods to Services
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Services use *labels* and a *label selector* to know which set of Pods to load-balance
    requests to. The Service has a *label selector* that contains all of the *labels*
    a Pod must have in order for it to receive traffic from the Service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.13 shows a Service configured to send traffic to all Pods on the cluster
    with the following three labels:'
  prefs: []
  type: TYPE_NORMAL
- en: zone=prod
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: env=be
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ver=1.3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both Pods in the diagram have all three labels, so the Service will load-balance
    traffic to them both.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13](Image00017.gif)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.14 shows a similar setup. However, an additional Pod, on the right,
    does not match the set of labels configured in the Service’s label selector. This
    means the Service will not load balance requests to it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14](Image00018.gif)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14
  prefs: []
  type: TYPE_NORMAL
- en: One final thing about Services. They only send traffic to **healthy Pods** .
    This means a Pod that is failing health-checks will not receive traffic from the
    Service.
  prefs: []
  type: TYPE_NORMAL
- en: That’s the basics - Services bring stable IP addresses and DNS names to the
    unstable world of Pods!
  prefs: []
  type: TYPE_NORMAL
- en: Chapter summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we introduced some of the major components of a Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The masters is where the control plane components run. Under-the-hood, they’re
    a combination of several system-services, including the API Server that exposes
    the public REST interface to the control plane. Masters make all of the deployment
    and scheduling decisions, and multi-master HA is important for production-grade
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Nodes are where user applications run. Each node runs a service called the `kubelet`
    that registers the node with the cluster and communicates with the control plane.
    This includes receiving new work tasks and reporting back about them. Nodes also
    have a container runtime and the `kube-proxy` service. The container runtime,
    such as Docker or containerd, is responsible for all container-related operations.
    The `kube-proxy` service is responsible for networking on the node.
  prefs: []
  type: TYPE_NORMAL
- en: We also talked about some of the major Kubernetes API objects such as Pods,
    Deployments, and Services. The Pod is the basic building-block. Deployments add
    self-healing, scaling and updates. Services add stable networking and load-balancing.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the basics, we’re going to start getting into the detail.
  prefs: []
  type: TYPE_NORMAL
