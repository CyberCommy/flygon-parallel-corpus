- en: Pain Points of Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to feedforward networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequential workings of RNNs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paint point #1 – the vanishing gradient problem'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pain point #2 – the exploding gradient problem'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequential workings of LSTMs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recurrent neural networks have proven to be incredibly efficient at tasks involving
    the learning and prediction of sequential data. However, when it comes to natural
    language, the question of long-term dependencies comes into play, which is basically
    remembering the context of a particular conversation, paragraph, or sentence in
    order to make better predictions in the future. For example, consider a sentence
    that says:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '*Last year, I happened to visit China. Not only was Chinese food different
    from the Chinese food available everywhere else in the world, but the people were
    extremely warm and hospitable too. In my three years of stay in this beautiful
    country, I managed to pick up and speak very good....*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: If the preceding sentence were fed into a recurrent neural network to predict
    the next word in the sentence (such as Chinese), the network would find it difficult
    since it has no memory of the context of the sentence. This is what we mean by
    long-term dependencies. In order to predict the word Chinese correctly, the network
    needs to know the context of the sentence as well as remember the fact that I
    happened to visit China last year. Recurrent neural networks therefore become
    inefficient at performing such tasks. However, this problem is overcome by **Long
    Short-Term Memory Units** (**LSTMs**), which are capable of remembering long-term
    dependencies and storing information in the cell state. LSTMs will be discussed
    later on, but the bulk of this chapter will focus on a basic introduction to Neural
    Networks, activation functions, Recurrent Networks, some of the main pain points
    or drawbacks of Recurrent Networks, and finally how these drawbacks may be overcome
    by the use of LSTMs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to feedforward networks
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand recurrent networks, first you have to understand the basics of
    feedforward networks. Both of these networks are named after the way they move
    information through a series of mathematical operations performed at the nodes
    of the network. One feeds information in only one direction through every node
    (never touching a given node twice), while the other cycles it through a loop
    and feeds it back to the same node (kind of like a feedback loop). It is easily
    understood how the first kind is called a **feedforward network,** while the latter
    is recurrent.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most important concept while understanding any neural network diagram is
    the concept of computational graphs. Computational graphs are nothing but the
    nodes of the neural network connected to each other, and each node performs a
    particular mathematical function.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Feedforward neural networks channel the inputs (to the input layer) through
    a set of computational nodes which are nothing but mathematical operators and
    activation functions arranged in layers to calculate the network outputs. The output
    layer is the final layer of the neural network and usually contains linear functions.
    The layers between the input layer and the output layer are called **hidden layers** and
    usually contain nonlinear elements or functions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram (a) shows how nodes are interconnected in feedforward
    neural networks with many layers:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00095.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: FeedForward Neural Network
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Feedforward neural networks mainly differ from each other by the type of functions
    (activation functions) that are used in the hidden-layer nodes. They also differ
    from each other by the algorithms that are used to optimize the other parameters
    of the network during training.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The relationships between nodes shown in the preceding diagram need not be fully
    populated for every node; optimization strategies usually start with a large number
    of hidden nodes and tune the network by eliminating connections, and possibly
    nodes, as training progresses. It may not be necessary to utilize every node during
    the training process.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的图中显示的节点之间的关系不需要对每个节点进行完全填充；优化策略通常从大量的隐藏节点开始，并通过消除连接和可能的节点来调整网络，随着训练的进行。在训练过程中可能不需要利用每个节点。
- en: How it works...
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The neuron is the basic structural element of any neural network. A neuron
    can be thought of as a simple mathematical function or operator that operates
    on the input flowing through it to produce an output flowing out of it. The inputs
    to a neuron are multiplied by the node''s weight matrix, summed over all the inputs,
    translated, and passed through an activation function. These are basically matrix
    operations in mathematics as described here:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元是任何神经网络的基本结构元素。神经元可以被看作是一个简单的数学函数或运算符，它对通过它流动的输入进行操作，以产生从它流出的输出。神经元的输入与节点的权重矩阵相乘，对所有输入求和，进行平移，并通过激活函数传递。这基本上是数学中的矩阵运算，如下所述：
- en: The computational graph representation of a neuron is shown in the preceding
    diagram (b).
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经元的计算图表示如前图(b)所示。
- en: 'The transfer function for a single neuron or node is written as follows:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单个神经元或节点的传递函数如下所示：
- en: '![](img/00096.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00096.jpeg)'
- en: Here, *x*[ *i* ]is the input to the ith node, *w*[ *i* ]is the weight term associated
    with the *i*^(th) node, *b* is the bias which is generally added to prevent overfitting, *f*(⋅)
    is the activation function operating over the inputs flowing into the node, and *y* is
    the output from the node.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x*[ *i* ]是第i个节点的输入，*w*[ *i* ]是与第i个节点相关的权重项，*b*是通常添加的偏差，以防止过拟合，*f*(⋅)是作用于流入节点的输入的激活函数，*y*是节点的输出。
- en: Neurons with sigmoidal activation functions are commonly used in the hidden
    layer(s) of the neural network, and the identity function is usually used in the
    output layer.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有S形激活函数的神经元通常用于神经网络的隐藏层，并且恒等函数通常用于输出层。
- en: The activation functions are generally chosen in a manner to ensure the outputs
    from the node are strictly increasing, smooth (continuous first derivative), or
    asymptotic.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活函数通常被选择为确保节点的输出严格增加、平滑（连续的一阶导数）或渐近的方式。
- en: 'The following logistic function  is used as a sigmoidal activation function:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下的逻辑函数被用作S形激活函数：
- en: '![](img/00097.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00097.jpeg)'
- en: A neural trained using backpropagation algorithm may learn faster if the activation
    function is antisymmetric, that is, *f*(-*x*) = -*f*(*x*) as in the case of the
    sigmoidal activation function. The backpropagation algorithm will be discussed
    in detail in the following sections of this chapter.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用反向传播算法训练的神经元，如果激活函数是反对称的，即*f*(-*x*) = -*f*(*x*)，可能会学习得更快，就像S形激活函数的情况一样。反向传播算法将在本章的后续部分中详细讨论。
- en: 'The logistic function, however, is not antisymmetric, but can be made antisymmetric
    by a simple scaling and shift, resulting in the hyperbolic tangent function which
    has  a first derivative described by *f *''(*x*) = 1 - *f *²(*x*),  as shown in
    the following mathematical function:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逻辑函数不是反对称的，但可以通过简单的缩放和移位来使其成为反对称，从而得到具有由*f*(*x*) = 1 - *f*²(*x*)描述的一阶导数的双曲正切函数，如下数学函数所示：
- en: '![](img/00098.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00098.jpeg)'
- en: The simple form of the sigmoidal function and its derivative allows for the
    quick and accurate calculation of the gradients needed to optimize the selection
    of the weights and biases and carry out second-order error analysis.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: S形函数及其导数的简单形式允许快速准确地计算梯度，以优化权重和偏差的选择，并进行二阶误差分析。
- en: There's more...
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'At every neuron/node in the layers of a neural network, a series of matrix
    operations are performed. A more mathematical way of visualizing the feedforward
    network is given in the following diagram, which will help you to better understand
    the operations at each node/neuron:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的各层中的每个神经元/节点上执行一系列矩阵运算。下图以更数学化的方式展示了前馈网络，这将帮助您更好地理解每个节点/神经元的操作：
- en: '![](img/00099.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00099.jpeg)'
- en: Intuitively, we can see that the inputs (which are vectors or matrices) are
    first multiplied by weight matrices. A bias is added to this term and then activated
    using an activation function (such as ReLU, tanh, sigmoid, threshold, and so on)
    to produce the output. Activation functions are key in ensuring that the network
    is able to learn linear as well as non-linear functions.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '直观地，我们可以看到输入（向量或矩阵）首先被权重矩阵相乘。然后添加一个偏差项，然后使用激活函数（如ReLU、tanh、sigmoid、阈值等）激活以产生输出。激活函数是确保网络能够学习线性和非线性函数的关键。 '
- en: 'This output then flows into the next neuron as its input, and the same set
    of operations are performed all over again. A number of such neurons combine together
    to form a layer (which performs a certain function or learns a certain feature
    of the input vector), and many such layers combine together to form a feedforward
    neural network that can learn to recognize inputs completely, as shown in the
    following diagram:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，这个输出作为下一个神经元的输入，然后再次执行相同的一系列操作。许多这样的神经元组合在一起形成一个层（执行输入向量的某个功能或学习某个特征），许多这样的层组合在一起形成一个前馈神经网络，可以完全学会识别输入，如下图所示：
- en: '![](img/00100.gif)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00100.gif)'
- en: 'Let''s suppose our feedforward network has been trained to classify images
    of dogs and images of cats. Once the network is trained, as shown in the following
    diagram, it will learn to label images as dog or cat when presented with new images:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们的前馈网络已经训练好，可以对狗和猫的图像进行分类。一旦网络训练好，如下图所示，它将学会在呈现新图像时将图像标记为狗或猫：
- en: '![](img/00101.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00101.jpeg)'
- en: In such networks, there is no relation between the present output and the previous
    or future outputs.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这样的网络中，当前输出与先前或未来的输出之间没有关系。
- en: This means the feedforward network can basically be exposed to any random collection
    of images and the first image it is exposed to will not necessarily alter how
    it classifies the second or third images. Therefore, we can say that the output
    at time step *t* is independent of the output at time step  *t - 1*.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这意味着前馈网络基本上可以暴露给任何随机的图像集合，它暴露给的第一张图像不一定会改变它对第二张或第三张图像的分类方式。因此，我们可以说在时间步*t*的输出与时间步*t-1*的输出是独立的。
- en: Feedforward networks work well in such cases as image classification, where
    the data is not sequential. Feedforward networks also perform well when used on
    two related variables such as temperature and location, height and weight, car
    speed and brand, and so on.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前馈网络在图像分类等情况下效果很好，其中数据不是顺序的。前馈网络在使用两个相关变量时也表现良好，比如温度和位置、身高和体重、汽车速度和品牌等。
- en: However, there may be cases where the current output is dependent on the outputs
    at previous time steps (the ordering of data is important).
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，可能存在当前输出依赖于先前时间步的输出的情况（数据的顺序很重要）。
- en: Consider the scenario of reading a book. Your understanding of the sentences
    in the book is based on your understanding of all the words in the sentence. It
    wouldn't be possible to use a feedforward network to predict the next word in
    a sentence, as the output in such a case would depend on the previous outputs.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑阅读一本书的情景。你对书中句子的理解基于你对句子中所有单词的理解。使用前馈网络来预测句子中的下一个单词是不可能的，因为在这种情况下输出取决于先前的输出。
- en: 'Similarly, there are many cases where the output requires the previous output
    or some information from the previous outputs (for example, stock market data,
    NLP, voice recognition, and so on). The feedforward network may be modified as
    in the following diagram to capture information from previous outputs:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，有许多情况下，输出需要先前的输出或一些先前输出的信息（例如，股市数据、自然语言处理、语音识别等）。前馈网络可以被修改如下图所示，以捕获先前输出的信息：
- en: '![](img/00102.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00102.jpeg)'
- en: At time step *t*, the input at *t* as well as the information from *t-1* is
    both provided to the network to obtain the output at time *t*.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在时间步*t*，输入*t*以及*t-1*的信息都提供给网络，以获得时间*t*的输出。
- en: Similarly, the information from *t* as well as the new input is fed into the
    network at time step *t+1* to produce the output at *t+1*. The right-hand side
    of the preceding diagram is a generalized way of representing such a network where
    the output of the network flows back in as input for future time steps. Such a
    network is called a **recurrent neural network** (**RNN**).
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，从时间步*t*以及新输入都被输入到网络中的时间步*t+1*，以产生*t+1*的输出。前面图表的右侧是表示这样一个网络的一般方式，其中网络的输出会作为未来时间步的输入。这样的网络被称为**循环神经网络**（**RNN**）。
- en: See also
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: '**Activation Function**: In artificial neural networks, the activation function of
    a node decides the kind of output that node produces, given an input or set of
    inputs. The output *y[k]* is given by the input *u[k]* and bias *b[k]*, which
    are passed through the activation function *φ(.)*  as shown in the following expression:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数**：在人工神经网络中，节点的激活函数决定了节点在给定输入或一组输入时产生的输出类型。输出*y[k]*由输入*u[k]*和偏置*b[k]*通过激活函数*φ(.)*得到，如下式所示：'
- en: '![](img/00103.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00103.jpeg)'
- en: 'There are various types of activation functions. The following are the commonly
    used ones:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种类型的激活函数。以下是常用的几种：
- en: '**Threshold function**:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**阈值函数**：'
- en: '![](img/00104.gif)![](img/00105.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00104.gif)![](img/00105.jpeg)'
- en: It is clear from the preceding diagram that this kind of  function restricts
    the output values of neurons to between 0 and 1\. This may be useful in many cases.
    However, this function is non-differentiable, which means it cannot be used to
    learn non-linearities, which is vital when using the backpropagation algorithm.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表可以清楚地看出，这种函数限制了神经元的输出值在0和1之间。在许多情况下，这可能是有用的。然而，这个函数是不可微的，这意味着它不能用于学习非线性，而在使用反向传播算法时，这是至关重要的。
- en: '**Sigmoid function**:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Sigmoid函数**：'
- en: '![](img/00106.jpeg)![](img/00107.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00106.jpeg)![](img/00107.jpeg)'
- en: The sigmoid function is a logistic function with a lower limit of 0 and an upper
    limit of 1, as with the threshold function. This activation function is continuous
    and therefore, also differentiable. In the sigmoid function, the slope parameter
    of the preceding function is given by α. The function is nonlinear in nature,
    which is critical in increasing the performance since it is able to accommodate
    non linearities in the input data unlike regular linear functions. Having non
    linear capabilities ensure that small changes in the weights and bias causes significant
    changes in the output of the neuron.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数是一个具有下限为0和上限为1的逻辑函数，与阈值函数一样。这个激活函数是连续的，因此也是可微的。在Sigmoid函数中，前面函数的斜率参数由α给出。这个函数是非线性的，这对于提高性能至关重要，因为它能够容纳输入数据中的非线性，而常规线性函数不能。具有非线性能力确保权重和偏置的微小变化会导致神经元输出的显著变化。
- en: '**Hyperbolic Tangent function (tanh)**:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**双曲正切函数（tanh）**：'
- en: '![](img/00108.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00108.jpeg)'
- en: This function enables activation functions to range from -1 to +1 instead of
    between 0 and 1 as in the previous cases.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数使激活函数的范围从0到1变为-1到+1。
- en: '**Rectified Linear Unit (ReLU) function**: ReLUs are the smooth approximation
    to the sum of many logistic units, and produce sparse activity vectors. The following
    is the equation of the function:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**修正线性单元（ReLU）函数**：ReLU是许多逻辑单元的平滑近似，产生稀疏的活动向量。以下是该函数的方程：'
- en: '![](img/00109.jpeg)![](img/00110.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00109.jpeg)![](img/00110.jpeg)'
- en: ReLU function graph
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU函数图
- en: In the preceding diagram, softplus ![](img/00111.jpeg)(x) = log ( 1 + e^x) is
    the smooth approximation to the rectifier.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，softplus ![](img/00111.jpeg)(x) = log ( 1 + e^x)是整流器的平滑近似。
- en: '**Maxout function**: This function utilizes a technique known as **"dropout"** and
    improves the accuracy of the dropout technique''s fast approximate model averaging
    in order to facilitate optimization.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Maxout函数**：该函数利用一种称为**“dropout”**的技术，并改进了快速近似模型平均的准确性，以便促进优化。'
- en: 'Maxout networks learn not just the relationship between hidden units, but also
    the activation function of each hidden unit. By actively dropping out hidden units,
    the network is forced to find other paths to get to the output from a given input
    during the training process. The following diagram is the graphical depiction
    of how this works:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Maxout网络不仅学习隐藏单元之间的关系，还学习每个隐藏单元的激活函数。通过主动丢弃隐藏单元，网络被迫在训练过程中找到其他路径以从给定输入到输出。以下图表是这个过程如何工作的图形描述：
- en: '![](img/00112.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00112.jpeg)'
- en: Maxout Network
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Maxout网络
- en: 'The preceding diagram shows the Maxout network with five visible units, three
    hidden units, and two neurons for each hidden unit. The Maxout function is given
    by the following equations:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了具有五个可见单元、三个隐藏单元和每个隐藏单元两个神经元的Maxout网络。Maxout函数由以下方程给出：
- en: '![](img/00113.jpeg)![](img/00114.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00113.jpeg)![](img/00114.jpeg)'
- en: 'Here  W..[ij ] is the mean vector of the size of the input obtained by accessing
    the matrix W ∈  ![](img/00115.jpeg) at the second coordinate *i* and third coordinate *j*.
    The number of intermediate units (*k) *is called the number of pieces used by
    the Maxout nets. The following diagram shows how the Maxout function compares
    to the ReLU and **Parametric Rectified Linear Unit** (**PReLU**) functions:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 W..[ij ]是通过访问矩阵W ∈  ![](img/00115.jpeg)的第二坐标 *i* 和第三坐标 *j*获得的输入的大小的均值向量。中间单元的数量（*k）*称为Maxout网络使用的片数。以下图表显示了Maxout函数与ReLU和**参数修正线性单元**（**PReLU**）函数的比较：
- en: '![](img/00116.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00116.jpeg)'
- en: Graphical comparison of Maxout, ReLU and PReLU function
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Maxout、ReLU和PReLU函数的图形比较
- en: Sequential workings of RNNs
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN的顺序工作
- en: 'Recurrent neural networks are a type of artificial neural network designed
    to recognize and learn patterns in sequences of data. Some of the examples of
    such sequential data are:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络是一种人工神经网络，旨在识别和学习数据序列中的模式。此类序列数据的一些示例包括：
- en: Handwriting
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手写
- en: Text such as customer reviews, books, source code, and so on
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诸如客户评论、书籍、源代码等文本
- en: Spoken word / Natural Language
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 口语/自然语言
- en: Numerical time series / sensor data
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数值时间序列/传感器数据
- en: Stock price variation data
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 股价变动数据
- en: Getting ready
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In recurrent neural networks, the hidden state from the previous time step
    is fed back into the network at the next time step, as shown in the following
    diagram:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在递归神经网络中，来自上一个时间步的隐藏状态被反馈到下一个时间步的网络中，如下图所示：
- en: '![](img/00117.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00117.jpeg)'
- en: Basically, the upward facing arrows going into the network represent the inputs
    (matrices/vectors) to the RNN at each time step, while the upward-facing arrows
    coming out of the network represent the output of each RNN unit. The horizontal
    arrows indicate the transfer of information learned in a particular time step
    (by a particular neuron) onto the next time step.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，进入网络的朝上箭头代表RNN在每个时间步的输入（矩阵/向量），而从网络中出来的朝上箭头代表每个RNN单元的输出。水平箭头表示在特定时间步（由特定神经元）学习的信息传递到下一个时间步。
- en: 'More information about using RNNs can be found at :'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用RNN的更多信息，请访问：
- en: '[https://deeplearning4j.org/usingrnns](https://deeplearning4j.org/usingrnns)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://deeplearning4j.org/usingrnns](https://deeplearning4j.org/usingrnns)'
- en: How to do it...
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'At every node/neuron of a recurrent network, a series of matrix multiplication
    steps are carried out. The input vector/matrix is multiplied by a weight vector/matrix
    first, a bias is added to this term, and this is finally passed through an activation
    function to produce the output (just as in the case of feedforward networks):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在递归网络的每个节点/神经元上，进行一系列矩阵乘法步骤。首先将输入向量/矩阵乘以权重向量/矩阵，然后添加偏差项，最后通过激活函数产生输出（就像前馈网络的情况一样）：
- en: 'The following diagram shows an intuitive and mathematical way of visualizing
    RNNs in the form of a computational graph:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下图表显示了一种直观和数学化的方式来可视化RNNs，以计算图的形式：
- en: '![](img/00118.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00118.jpeg)'
- en: At the first time step (which is *t=0*), *h*[*0* ]is calculated using the first
    formula on the right-hand side of the preceding diagram. Since *h*^(*-1* )does
    not exist, the middle term becomes zero.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一个时间步骤（即*t=0*），使用前面图表右侧的第一个公式计算*h*[*0*]。由于*h*^(*-1*)不存在，中间项变为零。
- en: The input matrix *x*[*0* ]is multiplied by the weight matrix *w[i]* and a bias
    *b[h]* is added to this term.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入矩阵*x*[*0*]乘以权重矩阵*w[i]*，并且将偏差*b[h]*添加到这个项。
- en: The two preceding matrices are added and then passed through an activation function
    *g[h]* to obtain *h[0]*.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将前面的两个矩阵相加，然后通过激活函数*g[h]*获得*h[0]*。
- en: Similarly, *y[0]* is calculated using the second equation on the right-hand
    side of the preceding diagram by multiplying *h[0]* with the weight matrix *w[y]*,
    adding a bias *b[y]* to it, and passing it through an activation function *g[y]*.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，*y[0]*使用前面图表右侧的第二个方程计算，方法是将*h[0]*与权重矩阵*w[y]*相乘，加上偏差*b[y]*，并通过激活函数*g[y]*传递。
- en: At the next time step (which is *t=1*), *h^((t-1))* does exist. It is nothing
    but *h[0]*. This term, multiplied with the weight matrix *w[R]*, is also provided
    as the input to the network along with the new input matrix *x[1]*.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个时间步（即*t=1*），*h^((t-1))*存在。它就是*h[0]*。这个项乘以权重矩阵*w[R]*，也作为网络的输入与新的输入矩阵*x[1]*一起提供。
- en: This process is repeated over a number of time steps, and the weights, matrices,
    and biases flow through the entire network over different time steps.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程在多个时间步骤中重复进行，权重、矩阵和偏差在不同的时间步骤中通过整个网络流动。
- en: This entire process is executed over one single iteration, which constitutes
    the forward pass of the network.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整个过程在一个迭代中执行，这构成了网络的前向传递。
- en: How it works...
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'To train feedforward neural networks the most commonly used technique is backpropagation
    through time. It is a supervised learning method used to reduce the loss function
    by updating weights and biases in the network after every time step. A number
    of training cycles (also known as epochs) are executed where the error determined
    by the loss function is backward propagated by a technique called gradient descent.
    At the end of each training cycle, the network updates its weights and biases
    to produce an output which is closer to the desired output, until a sufficiently
    small error is achieved :'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 训练前馈神经网络最常用的技术是通过时间的反向传播。这是一种监督学习方法，用于通过在每个时间步之后更新网络中的权重和偏差来减少损失函数。执行多个训练周期（也称为时代），其中由损失函数确定的误差通过梯度下降的技术进行反向传播。在每个训练周期结束时，网络更新其权重和偏差，以产生接近期望输出的输出，直到达到足够小的误差：
- en: 'The backpropagation algorithm basically implements the following three fundamental
    steps during every iteration:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每次迭代期间，反向传播算法基本上实现以下三个基本步骤：
- en: The forward pass of the input data and calculating the loss function
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据的前向传递和计算损失函数
- en: The computation of gradients and errors
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度和误差的计算
- en: Backpropagation through time and adjustment of weights and biases accordingly
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过时间的反向传播和相应地调整权重和偏差
- en: After the weighted sum of inputs (passed through an activation function after
    adding a bias) is fed into the network and an output is obtained, the network
    immediately compares how different the predicted output is from the actual case
    (correct output).
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在通过激活函数加上偏差后的输入的加权和被馈送到网络中并获得输出后，网络立即比较预测输出与实际情况（正确输出）的差异有多大。
- en: Next, the error is calculated by the network. This is nothing but the network
    output subtracted from the actual/correct output.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，网络计算误差。这实际上就是网络输出减去实际/正确的输出。
- en: The next step involves backpropagation through the entire network based on the
    calculated error. The weights and biases are then updated to notice whether the
    error increases or decreases.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步涉及根据计算的误差在整个网络中进行反向传播。然后更新权重和偏差以观察误差是增加还是减少。
- en: The network also remembers whether the error increases by increasing the weights
    and biases or by decreasing the weights and biases.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络还记得，增加权重和偏差会增加误差，或者减少权重和偏差会减少误差。
- en: Based on the preceding inferences, the network continues to update the weights
    and biases during every iteration in a manner such that the error becomes minimal.
    The following example will make things clearer.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据前述推论，网络在每次迭代期间继续以使误差最小的方式更新权重和偏差。下面的例子将使事情更清楚。
- en: 'Consider a simple case of teaching a machine how to double a number, as shown
    in the following table:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑一个简单的情况，教会机器如何将一个数字加倍，如下表所示：
- en: '![](img/00119.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00119.jpeg)'
- en: As you can see, by initializing the weights randomly (*W = 3*), we obtain outputs
    of 0, 3, 6, and 9.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如你所看到的，通过随机初始化权重（*W = 3*），我们得到了0、3、6和9的输出。
- en: The error is calculated by subtracting the column of correct outputs from the
    column of model outputs. The square error is nothing but each error term multiplied
    by itself. It is usually a better practice to use square error as it eliminates
    negative values from error terms.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 误差是通过将正确输出的列减去模型输出的列来计算的。平方误差实际上就是每个误差项与自身相乘。通常最好使用平方误差，因为它消除了误差项中的负值。
- en: The model then realizes that in order to minimize the error, the weight needs
    to be updated.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型随后意识到，为了最小化误差，需要更新权重。
- en: 'Let''s suppose the model updates its weight to *W = 4* during the next iteration.
    This would result in the following output:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设在下一次迭代中，模型将其权重更新为*W = 4*。这将导致以下输出：
- en: '![](img/00120.jpeg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00120.jpeg)'
- en: The model now realizes that the error actually increased by increasing the weight
    to *W = 4*. Therefore, the model updates its weight by reducing it to *W = 2*
    in its next iteration, which results in the actual/correct output.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型现在意识到，通过增加权重到*W = 4*，实际上误差增加了。因此，在下一次迭代中，模型通过将权重减小到*W = 2*来更新权重，从而得到实际/正确的输出。
- en: 'Note that, in this simple case, the error increases when the weight is increased
    and reduces when the weight is decreased, as follows:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，在这个简单的情况下，当增加权重时，误差增加，当减少权重时，误差减少，如下所示：
- en: '![](img/00121.jpeg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00121.jpeg)'
- en: In an actual neural network, a number of such weight updates are performed during
    every iteration until the model converges with the actual/correct output.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实际的神经网络中，每次迭代期间都会执行多次这样的权重更新，直到模型收敛到实际/正确的输出。
- en: There's more...
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'As seen in the preceding case, the error increased when the weight was increased
    but decreased when the weight was decreased. But this may not always be the case.
    The network uses the following graph to determine how to update weights and when
    to stop updating them:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的情况所示，当增加权重时，误差增加，但当减少权重时，误差减少。但这并不总是成立。网络使用以下图表来确定如何更新权重以及何时停止更新它们：
- en: '![](img/00122.jpeg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00122.jpeg)'
- en: Let the weights be initialized to zero at the beginning of the first iteration.
    As the network updates its weights by increasing them from point A to B, the error
    rate begins to decrease.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让权重在第一次迭代开始时初始化为零。当网络通过从点A到B增加权重时，误差率开始减少。
- en: Once the weights reach point B, the error rate becomes minimal. The network
    constantly keeps track of the error rate.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦权重达到B点，误差率就变得最小。网络不断跟踪误差率。
- en: On further increasing the weights from point B towards point C, the network
    realizes that the error rate begins to increase again. Thus, the network stops
    updating its weights and reverts back to the weights at point B, as they are optimal.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next scenario, consider a case where the weights are randomly initialized
    to some value (let''s say, point C), as shown in the following graph:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00123.jpeg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: On further increasing these random weights, the error also increases ( starting
    at point C and moving away from point B, indicated by the small arrow in the graph).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The network realizes that the error increased and begins to decrease the weights
    from point C so that the error decreases (indicated by the long arrow from point
    C moving towards point B in the graph). This decrease of weights happens until
    the error reaches a minimal value (point B on the graph).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The network continues to further update its weights even after reaching point
    B (indicated by the arrow moving away from point B and towards point A on the
    graph). It then realizes that the error is again increasing. As a result, it stops
    the weight update and reverts back to the weights that gave the minimal error
    value (which are the weights at point B).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is how neural networks perform weight updates after backpropagation. This
    kind of weight update is momentum-based. It relies on the computed gradients at
    each neuron of the network during every iteration, as shown in the following diagram:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00124.jpeg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Basically, the gradients are computed for each input with respect to the output
    every time an input flows into a neuron. The chain rule is used to compute the
    gradients during the backward pass of backpropagation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A detailed explanation of the math behind backpropagation can be found at the
    following links:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5](https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andrej Karpathy''s blog has tons of useful information about recurrent neural
    networks. Here is a link explaining their unreasonable effectiveness:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pain point #1 – The vanishing gradient problem'
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent neural networks are great for tasks involving sequential data. However,
    they do come with their drawbacks. This section will highlight and discuss one
    such drawback, known as the **vanishing gradient problem**.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The name vanishing gradient problem stems from the fact that, during the backpropagation
    step, some of the gradients vanish or become zero. Technically, this means that
    there is no error term being propagated backward during the backward pass of the
    network. This becomes a problem when the network gets deeper and more complex.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will describe how the vanishing gradient problem occurs in recurrent
    neural networks:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: While using backpropagation, the network first calculates the error, which is
    nothing but the model output subtracted from the actual output squared (such as
    the square error).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using this error, the model then computes the change in error with respect to
    the change in weights (de/dw).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The computed derivative multiplied by the learning rate ![](img/00125.jpeg) gives ![](img/00126.jpeg)w,
    which is nothing but the change in weights. The term ![](img/00127.jpeg)w is added
    to the original weights to update them to the new weights.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose the value of de/dw (the gradient or rate of change of error with respect
    to weights) is much less than 1, then that term multiplied by the learning rate ![](img/00128.jpeg) (which
    is always much less than 1) gives a very small, negligible, number which is negligible.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This happens because the weight updates during backpropagation are only accurate
    for the most recent time step, and this accuracy reduces while backpropagating
    through the previous time steps and becomes almost insignificant when the weight
    updates flow through many steps back in time.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There may be certain cases where sentences may be extremely long and the neural
    network is trying to predict the next word in a sentence. It does so based on
    the context of the sentence, for which it needs information from many previous
    time steps (these are called **long-term dependencies**). The number of previous
    times steps the network needs to backpropagate through increases with the increasing
    length of sentences. In such cases, the recurrent networks become incapable of
    remembering information from many time steps in the past and therefore are unable
    to make accurate predictions.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When such a scenario occurs, the network requires many more complex calculations,
    as a result of which the number of iterations increases substantially and during
    which the change in error term vanishes (by reducing over time) and changes in
    weight (![](img/00129.jpeg)w) become negligibly small. As a result, the new or
    updated weight is almost equal to the previous weight.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since there is no weight update occurring, the network stops learning or being
    able to update its weights, which is a problem as this will cause the model to
    overfit the data.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This entire process is illustrated in the following diagram:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00130.jpeg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: How it works...
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will describe some of the repercussions of the vanishing gradient
    problem:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: This problem occurs when we train a neural network model using some sort of
    optimization techniques which are gradient based.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generally, adding more hidden layers tends to make the network able to learn
    more complex arbitrary functions, and thus do a better job in predicting future
    outcomes. Deep Learning makes a big difference due to the large number of hidden
    layers it has, ranging from 10 to 200\. It is now possible to make sense of complicated
    sequential data, and perform tasks such as Speech Recognition, Image Classification,
    Image Captioning, and more.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The problem caused by the preceding steps is that, in some cases, the gradients
    become so small that they almost vanish, which in turn prevents the weights from
    updating their values during future time steps.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the worst case, it could result in the training process of the network being
    stopped, which means that the network stops learning the different features it
    was intended to learn through the training steps.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The main idea behind backpropagation is that it allows us, as researchers, to
    monitor and understand how machine learning algorithms process and learn various
    features. When the gradients vanish, it becomes impossible to interpret what is
    going on with the network, and hence identifying and debugging errors becomes
    even more of a challenge.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some of the ways in which the problem of vanishing gradients
    can be solved:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: One method to overcome this problem to some extent by using the ReLU activation
    function. It computes the function *f(x)=max(0,x) (i.e., t*he activation function
    simply thresholds the lower level of outputs at zero) and prevents the network
    from producing negative gradients.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another way to overcome this problem is to perform unsupervised training on
    each layer separately and then fine-tune the entire network through backpropagation,
    as done by Jürgen Schmidhuber in his study of multi-level hierarchy in neural
    networks. The link to this paper is provided in the following section.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A third solution to this problem is the use of **LSTM** (**Long Short-Term Memory**)
    units or **GRUs (Gated Recurrent Units)**, which are special types of RNNs.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following links provide a more in-depth description of the vanishing gradient
    problem and also some ways to tackle the issue:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b)'
- en: '[http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf)'
- en: '[http://people.idsia.ch/~juergen/cvpr2012.pdf](http://people.idsia.ch/~juergen/cvpr2012.pdf)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://people.idsia.ch/~juergen/cvpr2012.pdf](http://people.idsia.ch/~juergen/cvpr2012.pdf)'
- en: 'Pain point #2 – The exploding gradient problem'
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 痛点＃2 - 爆炸梯度问题
- en: Another drawback of recurrent neural networks is the problem of exploding gradients.
    This is similar to the vanishing gradient problem but the exact opposite. Sometimes,
    during backpropagation, the gradients explode to extraordinarily large values.
    As with the vanishing gradient problem, the problem of exploding gradients occurs
    when network architectures get deeper.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络的另一个缺点是爆炸梯度问题。这与梯度消失问题类似，但完全相反。有时在反向传播过程中，梯度会爆炸成异常大的值。与梯度消失问题一样，爆炸梯度问题发生在网络架构变得更深时。
- en: Getting ready
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The name exploding gradient problem stems from the fact that, during the backpropagation
    step, some of the gradients vanish or become zero. Technically, this means that
    there is no error term being propagated backward during the backward pass of the
    network. This becomes a problem when the network gets deeper and more complex.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 爆炸梯度问题的名称源于反向传播步骤中一些梯度消失或变为零的事实。从技术上讲，这意味着在网络的反向传播过程中没有误差项向后传播。当网络变得更深更复杂时，这就成为了一个问题。
- en: How to do it...
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'This section will describe the exploding gradient problem in recurrent neural
    networks:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述递归神经网络中的爆炸梯度问题：
- en: The exploding gradient problem is very similar to the vanishing gradient problem,
    but just the opposite.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爆炸梯度问题与梯度消失问题非常相似，但完全相反。
- en: When long-term dependencies arise in recurrent neural networks, the error term
    is propagated backward through the network sometimes explodes or becomes very
    large.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当递归神经网络中出现长期依赖时，误差项向后传播时有时会爆炸或变得非常大。
- en: This error term multiplied by the learning rate results in an extremely large ![](img/00131.jpeg)w.
    This gives rise to new weights that look very different from the previous weights.
    It is called the exploding gradient problem because the value of the gradient
    becomes too large.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个误差项乘以学习率的结果是一个极端大的![](img/00131.jpeg)w。这导致产生的新权重看起来与以前的权重非常不同。这被称为爆炸梯度问题，因为梯度的值变得太大。
- en: 'The problem of exploding gradients is illustrated in an algorithmic fashion,
    in the following diagram:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爆炸梯度问题以算法方式在以下图表中进行了说明：
- en: '![](img/00132.jpeg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00132.jpeg)'
- en: How it works...
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'Since neural networks use a gradient-based optimization technique to learn
    features present in data, it is essential that these gradients are preserved in
    order for the network to calculate an error based on the change in gradients.
    This section will describe how the exploding gradient problem occurs in recurrent
    neural networks:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络使用基于梯度的优化技术来学习数据中存在的特征，因此必须保留这些梯度，以便网络根据梯度的变化计算误差。本节将描述爆炸梯度问题在递归神经网络中是如何发生的：
- en: While using backpropagation, the network first calculates the error, which is
    nothing but the model output subtracted from the actual output squared (such as
    the square error).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用反向传播时，网络首先计算误差，这只是模型输出减去实际输出的平方（如平方误差）。
- en: Using this error, the model then computes the change in error with respect to
    the change in weights (de/dw).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这个误差，模型然后计算了相对于权重变化的误差变化（de/dw）。
- en: The computed derivative multiplied by the learning rate ![](img/00133.jpeg) gives ![](img/00134.jpeg)w,
    which is nothing but the change in weights. The term ![](img/00135.jpeg)w is added
    to the original weights to update them to the new weights.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算得到的导数乘以学习率![](img/00133.jpeg)得到![](img/00134.jpeg)w，这只是权重的变化。项![](img/00135.jpeg)w被添加到原始权重上，以将它们更新为新的权重。
- en: Suppose the value of de/dw (the gradient or rate of change of error with respect
    to weights) is greater than 1, then that term multiplied by the learning rate ![](img/00136.jpeg) gives
    a very, very large number that is of no use to the network while trying to optimize
    weights further, since the weights are no longer in the same range.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设de/dw（误差相对于权重的梯度或变化率）的值大于1，那么该项乘以学习率![](img/00136.jpeg)将得到一个非常非常大的数字，对于网络在进一步优化权重时是毫无用处的，因为权重已不再处于相同的范围内。
- en: This happens because the weight updates during backpropagation are only accurate
    for the most recent time step, and this accuracy reduces while backpropagating
    through the previous time steps and becomes almost insignificant when the weight
    updates flow through many steps back in time.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是因为在反向传播过程中，权重更新仅对最近的时间步准确，而在通过以前的时间步进行反向传播时，这种准确性会降低，并且当权重更新通过许多时间步回溯时几乎变得无关紧要。
- en: The number of previous times steps the network needs to backpropagate through
    increases with the increase in the number of sequences in the input data. In such
    cases, the recurrent networks become incapable of remembering information from
    many time steps in the past and therefore are unable to make accurate predictions
    of future time steps.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络需要通过的以前时间步数随着输入数据中序列数量的增加而增加。在这种情况下，递归网络无法记住过去许多时间步的信息，因此无法准确预测未来时间步。
- en: When such a scenario occurs, the network requires many more complex calculations,
    as a result of which the number of iterations increases substantially and during
    which the change in error term increases beyond 1 and changes in weight (![](img/00137.jpeg)w)
    explode. As a result, the new or updated weight is completely out of range when
    compared to the previous weight.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since there is no weight update occurring, the network stops learning or being
    able to update its weights within a specified range, which is a problem as this
    will cause the model to overfit the data.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some of the ways in which the problem of exploding gradients
    can be solved:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Certain gradient clipping techniques can be applied to solve this issue of exploding
    gradients.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another way to prevent this is by using truncated Backpropagation Through Time,
    where instead of starting the backpropagation at the last time step (or output
    layer), we can choose a smaller time step (say, 15) to start backpropagating.
    This means that the network will backpropagate through only the last 15 time steps
    at one instance and learn information related to those 15-time steps only. This
    is similar to feeding in mini batches of data to the network as it would become
    far too computationally expensive to compute the gradient over every single element
    of the dataset in the case of very large datasets.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final option to prevent the explosion of gradients is by monitoring them
    and adjusting the learning rate accordingly.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A more detailed explanation of the vanishing and exploding gradient problems
    can be found at the following links:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[http://neuralnetworksanddeeplearning.com/chap5.html](http://neuralnetworksanddeeplearning.com/chap5.html)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/](https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://machinelearningmastery.com/exploding-gradients-in-neural-networks/](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequential working of LSTMs
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Long Short-Term Memory Unit** (**LSTM**) cells are nothing but slightly more
    advanced architectures compared to Recurrent Networks. LSTMs can be thought of
    as a special kind of Recurrent Neural Networks with the capabilities of learning
    long-term dependencies that exist in sequential data. The main reason behind this
    is the fact that LSTMs contain memory and are able to store and update information
    within their cells unlike Recurrent Neural Networks.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main components of a Long Short-Term Memory unit are as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: The input gate
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The forget gate
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The update gate
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these gates is made up of a sigmoid layer followed by a pointwise multiplication
    operation. The sigmoid layer outputs numbers between zero and one. These values
    describe  how much information of each component is allowed to pass through the
    respective gate. A value of zero means the gate will allow nothing to pass through
    it, while a value of one means the gate allows all the information to pass through.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: The best way to understand LSTM cells is through computational graphs, just
    like in the case of recurrent neural networks.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTMs were originally developed by Sepp Hochreiter and Jurgen Schmidhuber in
    1997\. The following is, link to their published paper:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will describe the inner components of a single LSTM cell, primarily,
    the three different gates present inside the cell. A number of such cells stacked
    together form an LSTM network:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs also have a chain-like structure like RNNs. Standard RNNs are basically
    modules of repeating units like a simple function (for example, tanh).
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LSTMs have the capability to retain information for long periods of time as
    compared to RNNs owing to the presence of memory in each unit. This allows them
    to learn important information during the early stages in a sequence of inputs
    and also gives it the ability to have a significant impact on the decisions made
    by the model at the end of each time step.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By being able to store information right from the early stages of an input sequence, LSTMs
    are actively able to preserve the error that can be backpropagated through time
    and layers instead of letting that error vanish or explode.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LSTMs are capable of learning information over many time steps and thus have
    denser layer architectures by preserving the error which is backpropagated through
    those layers.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cell structures called **"gates"** give the LSTM the ability to retain information,
    add information or remove information from the **cell state**.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the structure of an LSTM. The key feature
    while trying to  understand LSTMs is in understanding the LSTM network architecture
    and cell state, which can be visualized here:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00138.gif)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, *x[t]* and *h[t-1]* are the two inputs to the cell.
    *x*[*t* ]is the input from the current time step, while h[t-1 ]is the input from
    the previous time step (which is the output of the preceding cell during the previous
    time step). Besides these two inputs, we also have *h*[, ]which is the current
    output (i.e., time step t) from the LSTM cell after performing its operations
    on the two inputs through its gates.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the preceding diagram, r[t ]represents the output emerging from the input
    gate, which takes in inputs *h*[*t-1* ]and *x[t]*, performs multiplication of
    these inputs with its weight matrix *W[z]*, and passes them through a sigmoid
    activation function.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, the term *z[t]* represents the output emerging from the forget gate.
    This gate has a set of weight matrices (represented by *W[r]*) which are specific
    to this particular gate and govern how the gate functions.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, there is ![](img/00139.jpeg)[t], which is the output emerging from
    the update gate. In this case, there are two parts. The first part is a sigmoid
    layer which is also called the **input gate layer**, and its primary function
    is deciding which values to update. The next layer is a tanh layer . The primary
    function of this layer is to create a vector or array containing new values that  could
    be added to the cell state.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A combination of a number of LSTM cells/units forms an LSTM network. The architecture
    of such a network is shown in the following diagram:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00140.jpeg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, the full LSTM cell is represented by ***"A"***. The
    cell takes the current input (*x**[i]*) of a sequence of inputs, and produces
    (*h**[i]*) which is nothing but the output of the current hidden state. This output
    is then sent to the next LSTM cell as its input.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An LSTM cell is slightly more complicated than an RNN cell. While the RNN cell
    has just one function/layer acting on a current input, the LSTM cell has three
    layers which are the three gates controlling the information flowing through the
    cell at any given instance in time.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cell behaves a lot like the hard disk memory in a computer. The cell, therefore,
    has the capability to allow the writing, reading and storing of information within
    its cell state. The cell also makes decisions about what information to store,
    and when to allow reading, writing, and erasing information. This is facilitated
    by the gates that open or close accordingly.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gates present in LSTM cells are analog in contrast to the digital storage
    systems in today's computers. This means that the gates can only be controlled
    through an element-wise multiplication through sigmoids, yielding probability
    values between 0 and 1\. A high value will cause the gate to remain open while
    a low value will cause the gate to remain shut.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analog systems have an edge over digital systems when it comes to neural network
    operations since they are differentiable. This makes analog systems more suitable
    for tasks like backpropagation which primarily rely on the gradients.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gates pass on information or block information or let only a part of the
    information flow through them based on its strength and importance. The information
    is filtered at every time step through the sets of weight matrices specific to
    each gate. Therefore, each gate has complete control over how to act on the information
    it receives.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The weight matrices associated with each gate, like the weights that modulate
    input and hidden states, are adjusted based on the recurrent network's learning
    process and gradient descent.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first gate is called the **forget gate** and it controls what information
    is maintained from the previous state. This gate takes the previous cell output
    (*h**[t]** - 1*) as its input along with the current input (*x**[t]*), and applies
    a sigmoid activation (![](img/00141.jpeg)) in order to produce and output value
    between 0 and 1 for each hidden unit. This is followed by the element-wise multiplication
    with the current state (illustrated by the first operation in the preceding diagram).
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second gate is called the **update gate** and its primary function is to
    update the cell state based on the current input. This gate passes the same input
    as the forget gate's inputs (*h**[t-1]* and *x**[t]*) into a sigmoid activation
    layer (![](img/00141.jpeg)) followed by a tanh activation layer and then performs
    an element-wise multiplication between these two results. Next, element-wise addition
    is performed with the result and the current state (illustrated by the second
    operation in the preceding diagram).
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, there is an output gate which controls what information and how much
    information gets transferred to the adjoining cell to act as its inputs during
    the next time step. The current cell state is passed through a tanh activation
    layer and multiplied element-wise with the cell input (*h**[t-1]* and *x**[t]*)
    after being passed through a sigmoid layer (![](img/00141.jpeg)) for this operation.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The update gate behaves as the filter on what the cell decides to output to
    the next cell. This output, h[t], is then passed on to the next LSTM cell as its
    input, and also to the above layers if many LSTM cells are stacked on top of each
    other.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LSTMs were a big leap forward when compared to what could be accomplished with
    feedforward networks and Recurrent Neural Networks. One might wonder what the
    next big step in the near future is, or even what that step might be. A lot researchers
    do believe "attention" is the next big step when it comes to the field of artificial
    intelligence. With the amount of data growing vastly with each day it becomes
    impossible to process every single bit of that data. This is where attention could
    be a potential game-changer, causing the networks to give their attention only
    to data or areas which are of high priority or interest and disregard useless
    information. For example, if an RNN is being used to create an image captioning
    engine, it will only pick a part of the image to to give its attention to for
    every word it outputs.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'The recent (2015) paper by Xu, et al. does exactly this. They explore adding
    attention to LSTM cells. Reading this paper can be a good place to start learning
    about the use of attention in neural networks. There have been some good results
    with using attention for various tasks, and more research is currently being conducted
    on the subject. The paper by Xu, et al. can be found using the following link:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1502.03044v2.pdf](https://arxiv.org/pdf/1502.03044v2.pdf)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Attention isn't the only variant to LSTMs. Some of the other active research
    is based on the utilization of grid LSTMs, as used in the paper by Kalchbrenner,
    et al., for which the link is at: [https://arxiv.org/pdf/1507.01526v1.pdf](https://arxiv.org/pdf/1507.01526v1.pdf).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力并不是LSTM的唯一变体。一些其他活跃的研究是基于格子LSTM的利用，正如Kalchbrenner等人在其论文中使用的那样，链接在：[https://arxiv.org/pdf/1507.01526v1.pdf](https://arxiv.org/pdf/1507.01526v1.pdf)。
- en: See also
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Some other useful information and papers related to RNNs and LSTMs in generative
    networks can be found by visiting the following links:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 关于生成网络中的RNN和LSTM的其他有用信息和论文可以通过访问以下链接找到：
- en: '[http://www.deeplearningbook.org/contents/rnn.html](http://www.deeplearningbook.org/contents/rnn.html)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.deeplearningbook.org/contents/rnn.html](http://www.deeplearningbook.org/contents/rnn.html)'
- en: '[https://arxiv.org/pdf/1502.04623.pdf](https://arxiv.org/pdf/1502.04623.pdf)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1502.04623.pdf](https://arxiv.org/pdf/1502.04623.pdf)'
- en: '[https://arxiv.org/pdf/1411.7610v3.pdf](https://arxiv.org/pdf/1411.7610v3.pdf)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1411.7610v3.pdf](https://arxiv.org/pdf/1411.7610v3.pdf)'
- en: '[https://arxiv.org/pdf/1506.02216v3.pdf](https://arxiv.org/pdf/1506.02216v3.pdf)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1506.02216v3.pdf](https://arxiv.org/pdf/1506.02216v3.pdf)'
