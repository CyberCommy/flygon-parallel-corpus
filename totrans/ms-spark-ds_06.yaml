- en: Chapter 6. Scraping Link-Based External Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter aims to explain a common pattern for enhancing local data with
    external content found at URLs or over APIs. Examples of this are when URLs are
    received from GDELT or Twitter. We offer readers a tutorial using the GDELT news
    index service as a source of news URLs, demonstrating how to build a web scale
    news scanner that scrapes global breaking news of interest from the Internet.
    We explain how to build this specialist web scraping component in a way that overcomes
    the challenges of scale. In many use cases, accessing the raw HTML content is
    not sufficient enough to provide deeper insights into emerging global events.
    An expert data scientist must be able to extract entities out of that raw text
    content to help build the context needed track broader trends.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a scalable web content fetcher using the *Goose* library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leverage the Spark framework for Natural Language Processing (NLP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: De-duplicate names using the double metaphone algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make use of GeoNames dataset for geographic coordinates lookup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a web scale news scanner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What makes data science different from statistics is the emphasis on scalable
    processing to overcome complex issues surrounding the quality and variety of the
    collected data. While statisticians work on samples of clean datasets, perhaps
    coming from a relational database, data scientists in contrast, work at scale
    with unstructured data coming from a variety of sources. While the former focuses
    on building models having high degrees of precision and accuracy, the latter often
    focuses on constructing rich integrated datasets that offer the discovery of less
    strictly defined insights. The data science journey usually involves torturing
    the initial sources of data, joining datasets that were theoretically not meant
    to be joined, enriching content with publicly available information, experimenting,
    exploring, discovering, trying, failing, and trying again. No matter the technical
    or mathematical skills, the main difference between an average and an expert data
    scientist is the level of curiosity and creativity employed in extracting the
    value latent in the data. For instance, you could build a simple model and provide
    business teams with the minimum they asked for, or you could notice and leverage
    all these URLs mentioned in your data, then scrape that content, and use these
    extended results to discover new insights that exceed the original questions business
    teams asked.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the web content
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unless you have been working really hard in early 2016, you will have heard
    about the death of the singer *David Bowie*, aged 69, on January 10, 2016\. This
    news has been widely covered by all media publishers, relayed on social networks,
    and followed by lots of tributes paid from the greatest artists around the world.
    This sadly is a perfect use case for the content of this book, and a good illustration
    for this chapter. We will use the following article from the BBC as a reference
    in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Accessing the web content](img/image_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: BBC article about David Bowie, Source: http://www.bbc.co.uk/news/entertainment-arts-35278872'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the HTML source code behind this article, the first thing to notice
    is that most of the content does not contain any valuable information. This includes
    the header, footer, navigation panels, sidebar, and all the hidden JavaScript
    code. While we are only interested in the title, some references (such as the
    publishing date), and at most, really only a dozens lines for the article itself,
    analyzing the page will require parsing more than 1500 lines of HTML code. Although
    we can find plenty of libraries designed for parsing HTML file content, creating
    a parser generic enough that can work with unknown HTML structures from random
    articles might become a real challenge on its own.
  prefs: []
  type: TYPE_NORMAL
- en: The Goose library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We delegate this logic to the excellent Scala library **Goose** ([https://github.com/GravityLabs/goose](https://github.com/GravityLabs/goose)).
    This library opens a URL connection, downloads the HTML content, cleanses it from
    all its junk, scores the different paragraphs using some clustering of English
    stop words, and finally returns the pure text content stripped of any of the underlying
    HTML code. With a proper installation of *imagemagick*, this library can even
    detect the most representative picture of a given website (out of the scope here).
    The `goose` dependency is available on Maven central:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Interacting with the Goose API is as pleasant as the library itself. We create
    a new Goose configuration, disable the image fetching, modify some optional settings
    such as the user agent and time out options, and create a new `Goose` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling the `extractContent` method returns an Article class with the following
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Using such a library, opening a connection and parsing the HTML content did
    not take us more than a dozen lines of code, and the technique can be applied
    to a random list of articles' URLs regardless of their source or HTML structure.
    The final output is a cleanly parsed dataset that is consistent, and highly useable
    in downstream analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Integration with Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next logical step is to integrate such a library and make its API available
    within a scalable Spark application. Once integrated, we will explain how to efficiently
    retrieve the remote content from a large collection of URLs and how to make use
    of non-serializable classes inside of a Spark transformation, and in a way that
    is performant.
  prefs: []
  type: TYPE_NORMAL
- en: Scala compatibility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Goose library on Maven has been compiled for Scala 2.9, and therefore is
    not compatible with Spark distribution (requires Scala 2.11 for version 2.0+ of
    Spark). To use it, we had to recompile the Goose distribution for Scala 2.11 and,
    for your convenience, we made it available on our main GitHub repository. This
    can be quickly installed using the commands below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note, you will have to modify your project `pom.xml` file using this new dependency.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Serialization issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Any Spark developer working with third-party dependencies should have experienced
    a `NotSerializableException` at least once. Although it might be challenging to
    find the exact root cause on a large project with lots of transformations, the
    reason is quite simple. Spark tries to serialize all its transformations before
    sending them to the appropriate executors. Since the `Goose` class is not serializable,
    and since we built an instance outside of a closure, this code is a perfect example
    of a `NotSerializableException` being thrown.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We simply overcome this constraint by creating an instance of a `Goose` class
    inside of a `map` transformation. By doing so, we avoid passing any reference
    to a non-serializable object we may have created. Spark will be able to send the
    code *as-is* to each of its executors without having to serialize any referenced
    object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Creating a scalable, production-ready library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Improving performance of a simple application that runs on a single server is
    sometimes not easy; but doing so on a distributed application running on several
    nodes that processes a large amount of data in parallel is often vastly more difficult,
    as there are so many additional factors to consider that affect performance. We
    show next, the principles we used to tune the content fetching library, so it
    can be confidently run on clusters at any scale without issues.
  prefs: []
  type: TYPE_NORMAL
- en: Build once, read many
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is worth mentioning that in the previous example, a new Goose instance was
    created for each URL, making our code particularly inefficient when running at
    scale. As a naive example to illustrate this point, it may take around 30 ms to
    create a new instance of a `Goose` class. Doing so on each of our millions of
    records would require 1 hour on a 10 node clusters, not to mention the garbage
    collection performance that would be significantly impacted. This process can
    be significantly improved using a `mapPartitions` transformation. This closure
    will be sent to the Spark executors (just like a `map` transformation would be)
    but this pattern allows us to create a single Goose instance per executor and
    call its `extractContent` method for each of the executor's records.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Exception handling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Exception handling is a cornerstone of proper software engineering. This is
    especially true in distributed computing, where we are potentially interacting
    with a large number of external resources and services that are out of our direct
    control. If we were not handling exceptions properly, for instance, any error
    occurring while fetching external website content would make Spark reschedule
    the entire task on other nodes several times before throwing a final exception
    and aborting the job. In a production-grade, lights-out web scraping operation,
    this type of issue could compromise the whole service. We certainly do not want
    to abort our whole web scraping content handling process because of a simple 404
    error.
  prefs: []
  type: TYPE_NORMAL
- en: 'To harden our code against these potential issues, any exceptions should be
    properly caught, and we should ensure that all returned objects should consistently
    be made optional, being undefined for all the failed URLs. In this respect, the
    only bad thing that could be said about the Goose library is the inconsistency
    of its returned values: null can be returned for titles and dates, while an empty
    string is returned for missing descriptions and bodies. Returning null is a really
    bad practice in Java/Scala as it usually leads to `NullPointerException` – despite
    the fact most developers usually write a This should not happen comment next to
    it. In Scala, it is advised to return an option instead of null. In our example
    code, any field we harvest from the remote content should be returned optionally,
    as it may not exist on the original source page. Additionally, we should address
    other areas of consistency too when we harvest data, for example we can convert
    dates into strings as it might lead to serialization issues when calling an action
    (such as **collect**). For all these reasons, we should redesign our `mapPartitions`
    transformation as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: We test for the existence of each object and return optional results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We wrap the article content into a serializable case class `Content`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We catch any exception and return a default object with undefined values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The revised code is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Performance tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although most of the time, the performance of a Spark application can greatly
    be improved from changes to the code itself (we have seen the concept of using
    `mapPartitions` instead of a `map` function for that exact same purpose), you
    may also have to find the right balance between the total number of executors,
    the number of cores per executor, and the memory allocated to each of your containers.
  prefs: []
  type: TYPE_NORMAL
- en: When doing this second kind of application tuning, the first question to ask
    yourself is whether your application is I/O bound (lots of read/write access),
    network bound (lots of transfer between nodes), memory, or CPU bound (your tasks
    usually take too much time to complete).
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to spot the main bottleneck in our web scraper application. It takes
    around 30 ms to create a `Goose` instance, and fetching the HTML of a given URL
    takes around 3 seconds to complete. We basically spend 99% of our time waiting
    for a chunk of content to be retrieved, mainly because of the Internet connectivity
    and website availability. The only way to overcome this issue is to drastically
    increase the number of executors used in our Spark job. Note that since executors
    usually sit on different nodes (assuming a correct Hadoop setup), a higher degree
    of parallelism will not hit the network limit in terms of bandwidth (as it would
    certainly do on a single node with multiple threads).
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, it is key to note that no reduce operation (no shuffle) is involved
    at any stage of this process as this application is a *map-only* job, making it
    linearly scalable by nature. Logically speaking, two times more executors would
    make our scraper two times more performant. To reflect these settings on our application,
    we need to make sure our data set is partitioned evenly with at least as many
    partitions as the number of executors we have defined. If our dataset were to
    fit on a single partition only, only one of our many executors would be used,
    making our new Spark setup both inadequate and highly inefficient. Repartitioning
    our collection is a one-off operation (albeit an expensive one) assuming we properly
    cache and materialize our RDD. We use parallelism of `200` here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The last thing to remember is to thoroughly cache the returned RDD, as this
    eliminates the risk that all its lazily defined transformations (including the
    HTML content fetching) might be re-evaluated on any further action we might call.
    To stay on the safe side, and because we absolutely do not want to fetch HTML
    content over the internet twice, we force this caching to take place explicitly
    by persisting the returned dataset to `DISK_ONLY`.
  prefs: []
  type: TYPE_NORMAL
- en: Named entity recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a web scraper that enriches an input dataset containing URLs with external
    web-based HTML content is of great business value within a big data ingestion
    service. But while an average data scientist should be able to study the returned
    content by using some basic clustering and classification techniques, an expert
    data scientist will bring this data enrichment process to the next level, by further
    enriching and adding value to it in post processes. Commonly, these value-added,
    post processes include disambiguating the external text content, extracting entities
    (like People, Places, and Dates), and converting raw text into its simplest grammatical
    form. We will explain in this section how to leverage the Spark framework in order
    to create a reliable **Natural Language Processing** (**NLP**) pipeline that includes
    these valuable post-processed outputs, and which handles English language-based
    content at any scale.
  prefs: []
  type: TYPE_NORMAL
- en: Scala libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**ScalaNLP** ([http://www.scalanlp.org/](http://www.scalanlp.org/)) is the
    parent project of breeze (among others), and is a numerical computational framework
    heavily used in Spark MLlib. This library would have been the perfect candidate
    for NLP on Spark if it was not causing such a number of dependency issues between
    the different versions of breeze and epic. To overcome these core dependency mismatches,
    we would have to recompile either the entire Spark distribution or the full ScalaNLP
    stack, neither of them being a walk in the park. Instead, our preferred candidate
    is thus a suite of Natural Language processors from the Computational Language
    Understanding Lab ([https://github.com/clulab/processors](https://github.com/clulab/processors)).
    Written in Scala 2.11, it provides three different APIs: A Stanford **CoreNLP**
    processor, a fast processor, and one for processing biomedical text. Within this
    library, we can use `FastNLPProcessor` that is both accurate enough for basic
    **Named Entity Recognition** (**NER**) functionalities and licensed under Apache
    v2.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: NLP walkthrough
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A NLP processor annotates a document and returns a list of lemma (words in their
    simplest grammatical form), a list of named entities types such as `[ORGANIZATION]`,
    `[LOCATION]`, `[PERSON]` and a list of normalized entities (such as actual date
    values).
  prefs: []
  type: TYPE_NORMAL
- en: Extracting entities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the following example, we initialize a `FastNLPProcessor` object, annotate
    and tokenize the document into a list of `Sentence`, zip both the lemma and NER
    types, and finally return an array of recognized entities for each given sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the above output, you may notice that all the retrieved entities
    are not linked together, both `David` and `Bowie` being two distinct entities
    of a type `[PERSON]`. We recursively aggregate consecutive similar entities using
    the following methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Printing out the same content now gives us a much more consistent output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a functional programming context, try to limit the use of any mutable object
    (such as using `var`). As a rule of thumb, any mutable object can always be avoided
    using preceding recursive functions.
  prefs: []
  type: TYPE_NORMAL
- en: Abstracting methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We appreciate that working on an array of sentences (sentences being themselves
    an array of entities) might sound quite blurry. By experience, this will be much
    more confusing when running at scale, when several `flatMap` functions will be
    required for a simple transformation on a RDD. We wrap the results into a class
    `Entities` and expose the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Building a scalable code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have now defined our NLP framework and abstracted most of the complex logic
    into a set of methods and convenient classes. The next step is to integrate this
    code within a Spark context and to start processing text content at scale. In
    order to write scalable code, one needs to take extra care addressing the following
    points:'
  prefs: []
  type: TYPE_NORMAL
- en: Any use of a non-serializable class within a Spark job must be carefully declared
    inside of a closure in order to avoid a `NotSerializableException` being raised.
    Please refer to the Goose library serialization issues we have been discussing
    in the previous section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whenever we create a new instance of `FastNLPProcessor` (whenever we first hit
    its `annotate` method because of lazy defined), all the required models will be
    retrieved from classpath, deserialized, and loaded into memory. This process takes
    around 10 seconds to complete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the instantiation process being quite slow, it is worth mentioning
    that the models can be very large (around a gigabyte), and that keeping all these
    models in memory will be incrementally consuming our available Heap space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build once, read many
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For all these reasons, embedding our code *as-is* within a `map` function would
    be terribly inefficient (and would probably blow all our available heap space).
    As per the below example, we leverage the `mapPartitions` pattern in order to
    optimize both the overhead time of loading and deserializing the models, as well
    as reducing the amount of memory used by our executors. Using `mapPartitions`
    forces the processing of the first record of each partition to evaluate the models
    inducing the model loading and deserializing process, and all subsequent calls
    on that executor will reuse those models within that partition, helping to limit
    the expensive model transfer and initialization costs to once per executor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The ultimate goal of this NLP scalability problem is to load the least possible
    number of models while processing as many records as possible. With one executor,
    we would load the models only once but would totally lose the point of parallel
    computing. With lots of executors, we will spend much more time deserializing
    models than actually processing our text content. This is discussed in the performance
    tuning section.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability is also a state of mind
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because we designed our code locally before integrating it into Spark, we kept
    in mind writing things in the most convenient way. It is important because scalability
    is not only how fast you code works in a big data environment, but also how people
    feel about it, and how efficiently developers interact with your API. As a developer,
    if you need to chain nested `flatMap` functions in order to perform what should
    be a simple transformation, your code simply does not scale! Thanks to our data
    structure being totally abstracted inside of an `Entities` class, deriving the
    different RDDs from our NLP extraction can be done from a simple map function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is key to note the use of `persist` here. As previously done on the HTML
    fetcher process, we thoroughly cache the returned RDD to avoid situations where
    all its underlying transformations will be re-evaluated on any further action
    we might be calling. NLP processing being quite an expensive process, you have
    to make sure it won't be executed twice, hence the `DISK_ONLY` cache here.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to bring this application to scale, you need to ask yourself the same
    key questions: Is this job I/O, memory, CPU, or network bound? NLP extraction
    is an expensive task, and loading a model is memory intensive. We may have to
    reduce the number of executors while allocating much more memory to each of them.
    To reflect these settings, we need to make sure our dataset will be evenly partitioned
    using at least as many partitions as the number of executors. We also need to
    enforce this repartitioning by caching our RDD and calling a simple `count` action
    that will evaluate all our previous transformations (including the partitioning
    itself).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: GIS lookup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we were covering an interesting use case, how to extract
    location entities from unstructured data. In this section, we will make our enrichment
    process even smarter by trying to retrieve the actual geographical coordinate
    information (such as latitude and longitude) based on the locations of entities
    we were able to identify. Given an input string `London`, can we detect the city
    of London - UK together with its relative latitude and longitude? We will be discussing
    how to build an efficient geo lookup system that does not rely on any external
    API and which can process location data of any scale by leveraging the Spark framework
    and the *Reduce-Side-Join* pattern. When building this lookup service, we will
    have to bear in mind many places around the world might be sharing the same name
    (there are around 50 different places called Manchester in the US alone), and
    that an input record may not use the official name of the place it would be referring
    to (the official name of commonly used Geneva/Switzerland is Geneva)**.**
  prefs: []
  type: TYPE_NORMAL
- en: GeoNames dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**GeoNames** ([http://www.geonames.org/](http://www.geonames.org/)) is a geographical
    database that covers all countries, contains over 10 million place names with
    geographic coordinates, and is available for download free of charge. In this
    example, we will be using the `AllCountries.zip` dataset (1.5 GB) together with
    `admin1CodesASCII.txt` reference data in order to turn our location strings into
    valuable location objects with geo coordinates. We will be keeping only the records
    related to continents, countries, states, districts, and cities together with
    major oceans, seas, rivers, lakes, and mountains, thus reducing by half, the entire
    dataset. Although the admin codes dataset easily fits in memory, the Geo names
    must be processed within an RDD and need to be converted into the following case
    classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We will not be describing the process of parsing a flat file into a `geoNameRDD`
    here. The parser itself is quite straightforward, processing a tab delimited records
    file and converting each value as per the above case class definition. We expose
    the following static method instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Building an efficient join
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main lookup strategy will rely on a `join` operation to be executed against
    both our Geo names and our input data. In order to maximize the chance of getting
    a location match, we will be expanding our initial data using a `flatMap` function
    over all the possible alternative names, hence drastically increasing the initial
    size of 5 million to approximately 20 million records. We also make sure to clean
    names from any accents, dashes, or fuzzy characters they might contain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'And voila, the remaining process is a simple `join`  operation between both
    a cleaned input and a cleaned `geoNameRDD`. Finally, we can group all the matching
    places into a simple set of `GeoName` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'An interesting pattern can be discussed here. How does Spark perform a `join`
    operation on large datasets? Called the *Reduce-Side-Join* pattern in legacy MapReduce,
    it requires the framework to hash all the keys from both RDDs and send all elements
    with a same key (same hash) on a dedicated node in order to locally `join` their
    values. The principle of *Reduce-Side-Join* is illustrated in *Figure 2* as follows.
    Because a *Reduce-Side-Join* is an expensive task (network bound), we must take
    special care addressing the following two points:'
  prefs: []
  type: TYPE_NORMAL
- en: '*GeoNames* dataset is much larger than our input RDD*.*We will be wasting lots
    of effort shuffling data that wouldn''t match anyway, making our `join` not only
    inefficient, but mainly useless.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*GeoNames* dataset does not change over time*.*It wouldn''t make sense to re-shuffle
    this immutable dataset on a pseudo real-time system (such as Spark Streaming)
    where location events are received in batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can build two different strategies, an offline and an online strategy. The
    former will make use of a *Bloom filter* to drastically reduce the amount of data
    to be shuffled while the latter will partition our RDD by key in order to reduce
    the network cost associated to a `join` operation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building an efficient join](img/image_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The Reduce-Side-Join'
  prefs: []
  type: TYPE_NORMAL
- en: Offline strategy - Bloom filtering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Bloom filter** is a space efficient probabilistic data structure that is
    used to test whether an element is a member of a set with a limited probability
    of false positives. Heavily used in legacy MapReduce, some implementations have
    been compiled for Scala. We will use the Bloom filter of breeze library, available
    on maven central (breeze itself can be used without much of dependency mismatches
    compared to the ScalaNLP models we were discussing earlier).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Because our input dataset is much smaller than our `geoNameRDD`, we will train
    a Bloom filter against the former by leveraging the `mapPartitions` function.
    Each executor will build its own Bloom filter that we can aggregate, thanks to
    its associative property, into a single object using a bitwise operator within
    a `reduce` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We test our filter against the full `geoNameRDD` in order to remove the places
    we know will not match, and finally execute our same `join` operation, but this
    time with much less data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'By reducing the size of our `geoNameRDD`, we have been able to release a lot
    of pressure from the shuffling process, making our `join` operation much more
    efficient. The resulting *Reduce-Side-Join* is reported on following *Figure 3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Offline strategy - Bloom filtering](img/image_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Reduce-Side-Join with Bloom filter'
  prefs: []
  type: TYPE_NORMAL
- en: Online strategy - Hash partitioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In an offline process, we were reducing the amount of data to be shuffled by
    pre-processing our `geoNameRDD`. In a streaming process, because any new batch
    of data is different, it wouldn''t be worth filtering our reference data over
    and over. In such a scenario, we can greatly improve the `join` performance by
    pre-partitioning our `geoNameRDD` data by key, using a `HashPartitioner` with
    the number of partitions being at least the number of executors. Because the Spark
    framework knows about the repartitioning used, only the input RDD would be sent
    to the shuffle, making our lookup service significantly faster. This is illustrated
    in *Figure 4*. Note the `cache` and `count` methods used to enforce the partitioning.
    Finally, we can safely execute our same `join` operation, this time with much
    less pressure on the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![Online strategy - Hash partitioning](img/image_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Reduce-Side-Join with Hash partitioning'
  prefs: []
  type: TYPE_NORMAL
- en: Content deduplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With cities like Manchester being found 100 times in our dataset, we need to
    work on a deduplication strategy for similar names, taking into account some cities
    might not be as important as others in terms of probability of being found within
    a random text content.
  prefs: []
  type: TYPE_NORMAL
- en: Context learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most accurate method for de-duplicating locations content would probably
    be to study location records in their contexts, similar to Apple – the company
    – being to Google and Yahoo! what Apple – the fruit – is to banana and orange.
    By machine learning locations in their context, we would probably discover that
    words *beavers* and *bears* are contextually close to the city of London in Ontario,
    Canada. As far as we know, the risk of bumping into a wild bear in London, UK
    is pretty small. Assuming one can access the text content, training a model shouldn't
    be difficult, but accessing the geo coordinates would require building an indexed
    dictionary of every single place with both its geographical values and its most
    describing topics. Because we do not have access to such a dataset (we could be
    scraping *Wikipedia* though), and that we do not want to assume one gets access
    to text content, we will simply be ranking places as an order of importance.
  prefs: []
  type: TYPE_NORMAL
- en: Location scoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given the different codes we pulled from the GeoNames website, we assume a continent
    will be more important than a country, a country will be more important than a
    state or a capital, and so on. This naive approach will make sense for 80% of
    the time, but might return irrelevant results in some edge cases. Given the Manchester
    example, we will find Manchester as being the parish of Manchester, a major state
    in Jamaica, instead of Manchester city, a *simple* city in the UK. We can fix
    this issue by being less restrictive in term of scoring and by sorting places
    of a same score by descending order of population. Returning the most important
    and relevant place makes sense, and such an approach is done by most online APIs
    anyway, but is that fair for the less important cities? We improve our scoring
    engine by adding a unique reference ID to a context where several locations may
    be mentioned together. If a document is only focused on cities in Canada, and
    if nothing is mentioning the United Kingdom, then *London* would most likely be
    the place in Canada. If no country or state is mentioned, or if both Canada and
    United Kingdom are found, we take the most important city of London in our dataset
    being London in the UK. The de-duplication occurs by sorting all our matching
    records by similar continent/country/states mentioned in the context, then by
    importance, and finally by population. The first result will be returned as our
    best candidate.
  prefs: []
  type: TYPE_NORMAL
- en: Names de-duplication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we were pulling entities from an NLP extraction process without any validation,
    the name we were able to retrieve may be written in many different ways. They
    can be written in different order, might contain middle names or initials, a salutation
    or a nobility title, nicknames, or even some typos and spelling mistakes. Although
    we do not aim to fully de-duplicate the content (such as learning that both *Ziggy
    Stardust* and *David Bowie* stand for the same person), we will be introducing
    two simple techniques used to de-duplicate a large amount of data at a minimal
    cost by combining the concept MapReduce paradigm and functional programming.
  prefs: []
  type: TYPE_NORMAL
- en: Functional programming with Scalaz
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is all about enriching data as part of an ingestion pipeline.
    We are therefore less interested in building the most accurate system using advanced
    machine learning techniques, but rather the most scalable and efficient one. We
    want to keep a dictionary of alternative names for each record, to merge and update
    them really fast, with the least possible code, and at very large scale. We want
    these structures to behave like monoids, algebraic associative structures properly
    supported on **Scalaz** ([https://github.com/scalaz/scalaz](https://github.com/scalaz/scalaz)),
    a library used for doing pure functional programming:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Our de-duplication strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use a simple example below to justify the need of using Scalaz programming
    for building a scalable, deduplication pipeline made of multiple transformations.
    Using a RDD of person, `personRDD`, as a test dataset shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we first count the number of occurrences for each entry. This is in fact
    a simple Wordcount algorithm, the *101* of MapReduce programming:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we apply a first transformation, such as `lowercase`, and produce an
    updated report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we then apply a second transformation that removes any special character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We now have reduced our list of six entries to only two, but since we've lost
    the original records across our transformations, we cannot build a dictionary
    in the form of [original value] -> [new value].
  prefs: []
  type: TYPE_NORMAL
- en: Using the mappend operator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instead, using the Scalaz API, we initialize a names'' frequency dictionary
    (as a Map, initialized to 1) upfront for each original record and merge these
    dictionaries using the `mappend` function (accessed through the `|+|` operator).
    The merge occurs after each transformation, within a `reduceByKey` function, taking
    the result of the transformation as a key and the term frequency map as a value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'For each de-duplication entry, we find the most frequent item and build our
    dictionary RDD as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In order to fully de-duplicate our person RDD, one needs to replace all `david
    bowie` and `david#bowie` occurrences with `David Bowie`. Now that we have explained
    the de-duplication strategy itself, let us dive deeply into the set of transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Simple clean
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first deduplication transformation is obviously to clean names from all
    their fuzzy characters or extra spaces. We replace accents with their matching
    ASCII characters, handle camel case properly, and remove any stop words such as
    [mr, miss, sir]. Applying this function to the prime minister of Tonga, [Mr. Sialeʻataongo
    Tuʻivakanō], we return [siale ataongo tu ivakano], a much cleaner version of it,
    at least in the context of string deduplication. Executing the deduplication itself
    will be as simple as a few lines of code using both the MapReduce paradigm and
    the monoids concept introduced earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: DoubleMetaphone
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**DoubleMetaphone** is a useful algorithm that can index names by their English
    pronunciation. Although it does not produce an exact phonetic representation of
    a name, it creates a simple hash function that can be used to group names with
    similar phonemes.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more information about DoubleMetaphone algorithm, please refer to: *Philips,
    L. (1990). Hanging on the Metaphone (Vol. 7). Computer Language.)*'
  prefs: []
  type: TYPE_NORMAL
- en: We turn to this algorithm for performance reasons, as finding potential typos
    and spelling mistakes in large dictionaries is usually an expensive operation;
    it often requires a candidate name to be compared with each of the others we are
    tracking. This type of comparison is challenging in a big data environment as
    it usually requires a Cartesian `join` which can generate excessively large intermediate
    datasets. The metaphone algorithm offers a greater, and much faster alternative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `DoubleMetaphone` class from the Apache commons package, we simply
    leverage the MapReduce paradigm by grouping names sharing a same pronunciation.
    `[david bowie]`, `[david bowi]` and `[davide bowie]`, for example, are all sharing
    the same code `[TFT#P]` and will all be grouped together. In the example below,
    we compute the double metaphone hash for each record and call a `reduceByKey`
    that merges and updates all our names'' frequency maps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also greatly improve this simple technique by keeping a list of common
    English nicknames (bill, bob, will, beth, al, and so on) and their associated
    primary names, so that we can match across non-phonetic synonyms. We can do this
    by pre-processing our name RDD by replacing the hash codes for known nicknames
    with the hash codes of the associated primary names, and then we can run the same
    deduplication algorithm to resolve duplicates across both phonetic and synonym
    based matches. This will detect both spelling mistakes and alternative nicknames
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, we want to highlight the fact this algorithm (and the simple cleansing
    routine shown above) will not be as accurate as a proper, fuzzy string matching
    approach that would, for example, compute a *Levenshtein* distance between each
    possible pair of names. By sacrificing accuracy, we do however create a method
    that is highly scalable, and that finds most common spelling mistakes at a minimal
    cost, especially spelling mistakes made on silent consonants. Once all the alternative
    names have been grouped on the resulting hash codes, we can output the best alternative
    to the presented name as the most frequent name we return from our term frequency
    objects. This best alternate is applied through a `join` with the initial name
    RDD in order to replace any record with its preferred alternative (if any):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: News index dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we were able to enrich the content found at input URLs with valuable information,
    the natural next step is to start visualizing our data. Although the different
    techniques of Exploratory Data Analysis have been thoroughly discussed within
    [Chapter 4](ch04.xhtml "Chapter 4. Exploratory Data Analysis"), *Exploratory Data
    Analysis*, we believe it is worth wrapping up what we have covered so far using
    a simple dashboard in Kibana. From around 50,000 articles, we were able to fetch
    and analyze on January 10-11, we filter any record mentioning *David Bowie* as
    a NLP entity and containing the word *death.* Because all our text content is
    properly indexed in Elasticsearch, we can pull 209 matching articles with their
    content in just a few seconds.
  prefs: []
  type: TYPE_NORMAL
- en: '![News index dashboard](img/B05261_06_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: News Index Dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: We can quickly get the top ten persons mentioned alongside **David Bowie**,
    including his stage name *Ziggy Stardust*, his son *Duncan Jones*, his former
    producer *Tony Visconti*, or the British prime minister *David Cameron*. Thanks
    to the *GeoLookup* service we built, we display all the different places mentioned,
    discovering a clique around the Vatican City state where the cardinal **Gianfranco
    Ravasi**, head of the pontifical council of culture, tweeted about *David Bowie's*
    famous lyrics of *Space Oddity*.
  prefs: []
  type: TYPE_NORMAL
- en: '![News index dashboard](img/image_06_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Vatican paid tribute from Twitter'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the race to be the first news publishing company covering breaking
    news, finding the first one who published about *David Bowie's* death is as easy
    as a simple click!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data science is not just about machine learning. In fact, machine learning is
    only a small portion of it. In our understanding of what modern data science is,
    the science often happens exactly here, at the data enrichment process. The real
    magic occurs when one can transform a meaningless dataset into a valuable set
    of information and get new insights out of it. In this section, we have been describing
    how to build a fully functional data insight system using nothing more than a
    simple collection of URLs (and a bit of elbow grease).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we demonstrated how to create an efficient web scraper with
    Spark using the Goose library and how to extract and de-duplicate features out
    of raw text using NLP techniques and the GeoNames database. We also covered some
    interesting design patterns such as *mapPartitions* and *Bloom filters* that will
    be discussed further in [Chapter 14](ch14.xhtml "Chapter 14. Scalable Algorithms"),
    *Scalable Algorithms*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be focusing on the people we were able to extract
    from all these news articles. We will be describing how to create connections
    among them using simple contact chaining techniques, how to efficiently store
    and query a large graph from a Spark context, and how to use *GraphX* and *Pregel*
    to detect communities.
  prefs: []
  type: TYPE_NORMAL
