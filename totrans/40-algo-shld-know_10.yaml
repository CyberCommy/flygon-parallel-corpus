- en: Neural Network Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A combination of various factors has made **Artificial Neural Networks** (**ANNs**)
    one of the most important machine learning techniques available today. These factors
    include the need to solve increasingly complex problems, the explosion of data,
    and the emergence of technologies, such as readily available cheap clusters, that
    provide the computing power necessary to design very complex algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, this is the research area that is rapidly evolving and is responsible
    for most of the major advances claimed by leading-edge tech fields such as robotics,
    natural language processing, and self-driving cars.
  prefs: []
  type: TYPE_NORMAL
- en: Looking into the structure of an ANN, its basic unit is a neuron. The real strength
    of the ANN lies in its ability to use the power of multiple neurons by organizing
    them in a layered architecture. An ANN creates a layered architecture by chaining
    neurons together in various layers. A signal passes through these layers and is
    processed in different ways in each of the layers until the final required output
    is generated. As we will see in this chapter, the hidden layers used by ANNs act
    as layers of abstraction, enabling deep learning, which is extensively used in
    realizing powerful applications such as Amazon's Alexa, Google's image search,
    and Google Photos.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter first introduces the main concepts and components of a typical
    neural network. Then, it presents the various types of neural networks and explains
    the different kinds of activation functions used in these neural networks. Then,
    the backpropagation algorithm is discussed in detail, which is the most widely
    used algorithm for training a neural network. Next, the transfer learning technique
    is explained, which can be used to greatly simplify and partially automate the
    training of models. Finally, how to use deep learning to flag fraudulent documents
    is looked at by way of a real-world example application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the main concepts discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding ANNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evolution of ANNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools and frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case study: using deep learning for fraud detection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start by looking at the basics of ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding ANNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inspired by the working of neurons in the human brain, the concept of neural
    networks was proposed by Frank Rosenblatt in 1957\. To understand the architecture
    fully, it is helpful to briefly look at the layered structure of neurons in the
    human brain. (Refer to the following diagram to get an idea of how the neurons
    in the human brain are chained together.)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the human brain, **dendrites**act as sensors that detect a signal. The signal
    is then passed on to an **a****xon**, which is a long, slender projection of a
    nerve cell. The function of the axon is to transmit this signal to muscles, glands,
    and other neurons. As shown in the following diagram, the signal travels through
    interconnecting tissue called a **synapse** before being passed on to other neurons.  Note
    that through this organic pipeline, the signal keeps traveling until it reaches
    the target muscle or gland, where it causes the required action. It typically
    takes seven to eight milliseconds for the signal to pass through the chain of
    neurons and reach its destination:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0c34e87b-05f8-4248-a1d3-bfecc9d58c0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Inspired by this natural architectural masterpiece of signal processing, Frank
    Rosenblatt devised a technique that would mean digital information could be processed
    in layers to solve a complex mathematical problem. His initial attempt at designing
    a neural network was quite simple and looked similar to a linear regression model.
    This simple neural network did not have any hidden layers and was named a *perceptron*.
    The following diagram illustrates it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/88d873d7-e3fb-4de6-a442-85e1cd5539c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s try to develop the mathematical representation of this perceptron. In
    the preceding diagram, the input signals are shown on the left-hand side. It is
    a weighted sum of inputs because each of the inputs *(x[1], x[2]..x[n])* gets
    multiplied by a corresponding weight *(w[1],w[2]… w[n])* and then summed up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3f4de7d2-6950-4b64-a0c5-3f96e3b1f24e.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that it is a binary classifier because the final output from this perceptron
    is true or false depending on the output of the aggregator (shown as **∑** in
    the diagram).  The aggregator will produce a true signal if it can detect a valid
    signal from at least one of the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look into how neural networks have evolved over time.
  prefs: []
  type: TYPE_NORMAL
- en: The Evolution of ANNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding section, we looked into a simple neural network without any
    layers called a perceptron. The perceptron was found to have serious limitations,
    and in 1969, Marvin Minsky and Seymour Papert worked on research that led to the
    conclusion that a perceptron is incapable of learning any complex logic.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, they showed that it would be a struggle to learn even logical functions
    as simple as XOR. That led to a decrease in interest in machine learning in general,
    and neural networks in particular, and started an era that is now known as the
    **AI winter**. Researchers around the world would not take AI seriously, thinking
    that it was incapable of solving any complex problems.
  prefs: []
  type: TYPE_NORMAL
- en: One of the primary reasons for the so-called AI winter was the limitation of
    the hardware capabilities available at that time. Either the necessary computing
    power was not available or it was prohibitively expensive. Toward the end of the
    1990s, advances in distributed computing provided easily available and affordable
    infrastructure, which resulted in the thaw of the AI winter. The thaw reinvigorated
    research in AI. This eventually resulted in turning the current era into an era
    that can be called the **AI spring**, where there is so much interest in AI in
    general and neural networks in particular.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more complex problems, researchers have developed a multilayer neural network
    called a **multilayer perceptron**. A multilayer neural network has a few different
    layers, as shown in the following diagram. These layers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden layer(s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output layer:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A deep neural network is a neural network with one or more hidden layers. Deep
    learning is the process of training an ANN.![](assets/4ec6b21a-e4bb-4c00-b243-d7b438d0f3d9.png)
  prefs: []
  type: TYPE_NORMAL
- en: An important thing to note is that the neuron is the basic unit of this network,
    and  each neuron of a layer is connected to all neurons of the next layer. For
    complex networks, the number of these interconnections explodes, and we will explore
    different ways of reducing these interconnections without sacrificing too much
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's try to formulate the problem we are trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: The input is a feature vector, *x*, of dimensions *n*.
  prefs: []
  type: TYPE_NORMAL
- en: We want the neural network to predict values. The predicted values are represented
    by *ý*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, we want to determine, given a particular input, the probability
    that a transaction is fraudulent. In other words, given a particular value of
    *x*, what is the probability that *y* = 1? Mathematically, we can represent this
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a68e1e70-b03b-423e-8eb2-b6e57787c0d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that x is an *n[x]*-dimensional vector, where *n*[*x*] is the number of
    input variables.
  prefs: []
  type: TYPE_NORMAL
- en: This neural network has four layers. The layers between the input and the output
    are the hidden layers. The number of neurons in the first  hidden layer is denoted
    by ![](assets/11fb6e2b-3c01-4cb3-b314-0d3f70d33ea8.png). The links between various
    nodes are multiplied by parameters called *weights*. Training a neural network
    is all about finding the right values for the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how we can train a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of building a neural network using a given dataset is called training
    a neural network*.* Let's look into the anatomy of a typical neural network. When
    we talk about training a neural network, we are talking about calculating the
    best values for the weights. The training is done iteratively by using a set of
    examples in the form of training data. The examples in the training data have
    the expected values of the output for different combinations of input values.
    The training process for neural networks is different from the way traditional
    models are trained (which were discussed in [Chapter 7](e3df232d-9571-4514-a5f1-2789965492e1.xhtml),
    *Traditional Supervised Learning Algorithms*).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Anatomy of a Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see what a neural network consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Layers:** Layers are the core building blocks of a neural network. Each layer
    is a data-processing module that acts as a filter. It takes one or more inputs,
    processes it in a certain way, and then produces one or more outputs. Each time
    data passes through a layer, it goes through a processing phase and shows patterns
    that are relevant to the business question we are trying to answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function:** A loss function provides the feedback signal that is used
    in the various iterations of the learning process. The loss function provides
    the deviation for a single example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost function:** The cost function is the loss function on a complete set
    of examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizer:**  An optimizer determines how the feedback signal provided by
    the loss function will be interpreted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input data:**  Input data is the data that is used to train the neural network.
    It specifies the target variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weights:** The weights are calculated by training the network. Weights roughly
    correspond to the importance of each of the inputs. For example, if a particular
    input is more important than other inputs, after training, it is given a greater
    weight value, acting as a multiplier. Even a weak signal for that important input
    will gather strength from the large weight value (that acts as a multiplier).
    Thus weight ends up turning each of the inputs according to their importance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation function:** The values are multiplied by different weights and
    then aggregated. Exactly how they will be aggregated and how their value will
    be interpreted will be determined by the type of the chosen activation function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now have a look at a very important aspect of neural network training.
  prefs: []
  type: TYPE_NORMAL
- en: While training neural networks, we take each of the examples one by one. For
    each of the examples, we generate the output using our under-training model. We
    calculate the difference between the expected output and the predicted output.
    For each individual example, this difference is called the **loss**. Collectively,
    the loss across the complete training dataset is called the **cost**. As we keep
    on training the model, we aim to find the right values of weights that will result
    in the smallest loss value. Throughout the training, we keep on adjusting the
    values of the weights until we find the set of values for the weights that results
    in the minimum possible overall cost. Once we reach the minimum cost, we mark
    the model as trained.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of training a neural network model is to find the right values for
    weights. We start training a neural network with random or default values for
    the weights. Then, we iteratively use an optimizer algorithm, such as gradient
    descent, to change the weights in such a way that our predictions improve.
  prefs: []
  type: TYPE_NORMAL
- en: The starting point of a gradient descent algorithm is the random values of weights
    that need to be optimized as we iterate through the algorithm. In each of the
    subsequent iterations, the algorithm proceeds by changing the values of the weights
    in such a way that the cost is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram explains the logic of the gradient descent algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/47b9db15-d8f5-4fa7-b88f-044a27d2cb3c.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, the input is the feature vector **X**. The actual
    value of the target variable is **Y** and the predicted value of the target variable
    is **Y'**. We determine the deviation of the actual value from the predicted values.
    We update the weights and repeat the steps until the cost is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'How to vary the weight in each iteration of the algorithm will depend on the
    following two factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Direction:** Which direction to go in to get the minimum of the loss function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning Rate:**  How big the change should be in the direction we have chosen'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A simple iterative process is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/116bf2a0-a40d-45f4-aa58-cd0c7ddec8ba.png)'
  prefs: []
  type: TYPE_IMG
- en: The diagram shows how, by varying the weights, gradient descent tries to find
    the minimum cost. The learning rate and chosen direction will determine the next
    point on the graph to explore.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the right value for the learning rate is important. If the learning
    rate is too small, the problem may take a lot of time to converge. If the learning
    rate is too high, the problem will not converge. In the preceding diagram, the
    dot representing our current solution will keep oscillating between the two opposite
    lines of the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how to minimize a gradient. Consider only two variables, *x*
    and *y*. The gradient of *x* and *y* is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9c045c39-cac5-4e9d-976d-7e77da01c018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To minimize the gradient, the following approach can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This algorithm can also be used to find the optimal or near-optimal values of
    weights for a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the calculation of gradient descent proceeds backward throughout the
    network. We start by calculating the gradient of the final layer first, and then
    the second-to-last one, and then the one before that, until we reach the first
    layer. This is called backpropagation, which was introduced by Hinton, Williams,
    and Rumelhart in 1985\.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's look into activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An activation function formulates how the inputs to a particular neuron will
    be processed to generate an output.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, each of the neurons in a neural network
    has an activation function that determines how inputs will be processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/536fd811-bb85-40d2-9ac6-28e8cf18bc20.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we can see that the results generated by an activation
    function are passed on to the output. The activation function sets the criteria
    that how the values of the inputs are supposed to be interpreted to generate an
    output.
  prefs: []
  type: TYPE_NORMAL
- en: For exactly the same input values, different activation functions will produce
    different outputs. Understanding how to select the right activation function is
    important when using neural networks to solve problems.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look into these activation functions one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Threshold Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest possible activation function is the threshold function. The output
    of the threshold function is binary: 0 or 1\. It will generate 1 as the output
    if any of the input is greater than 1\. This can be explained in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3b653d1d-c86d-45e7-8d19-366b8fb9dd19.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that as soon as there are any signs of life detected in the weighted sums
    of inputs, the output (*y*) becomes 1\. This makes the threshold activation function
    very sensitive. It is quite vulnerable to being wrongly triggered by the slightest
    signal in the input due to a glitch or some noise.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The sigmoid function can be thought of as an improvement of the threshold function.
    Here, we have control over the sensitivity of the activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/581c93c0-8b11-4abf-8e29-2923349c1a40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sigmoid function, *y*, is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cf23e5ef-72a7-42de-a05e-71c0595f48bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It can be implemented in Python as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that by reducing the sensitivity of the activation function, we make glitches
    in the input  less disruptive. Note that the output of the sigmoid activation
    function is still binary, that is, 0 or 1.
  prefs: []
  type: TYPE_NORMAL
- en: Rectified linear unit (ReLU)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The output for the first two activation functions presented in this chapter
    was binary. That means that they will take a set of input variables and convert
    them into binary outputs. ReLU is an activation function that takes a set of input
    variables as input and converts them into a single continuous output. In neural
    networks, ReLU is the most popular activation function and is usually used in
    the hidden layers, where we do not want to convert continuous  variables into
    category variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram summarizes the ReLU activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f99737ce-4ece-44ae-afc6-35e51f95a0e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that when *x≤ 0*, that means *y = 0*. This means that any signal from
    the input that is zero or less than zero is translated into a zero output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c36cfe68-1975-4d9b-98f9-1e404b75399e.png) for ![](assets/9ab31cd7-0b6a-4412-b32f-72ec6a44227f.png)![](assets/ecea8529-3f94-4351-84bb-3841d873a580.png)
    for ![](assets/0b063a5e-03ee-44f1-8a19-664c6536e069.png)'
  prefs: []
  type: TYPE_IMG
- en: As soon as *x* becomes more than zero, it is *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ReLU function is one of the most used activation functions in neural networks.
    It can be implemented in Python as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now let's look into Leaky ReLU, which is based on ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: Leaky ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In ReLU, a negative value for *x* results in a zero value for *y*. It means
    that some information is lost in the process, which makes training cycles longer,
    especially at the start of training. The Leaky ReLU activation function resolves
    this issue. The following applies for Leaky ReLu:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/10f3d67b-45d7-44b6-8455-576d8bae3079.png) ; for ![](assets/2fd2b88a-3be6-49d4-ae6b-fbb781c0c34e.png)![](assets/6aab91fb-9ecf-489a-a7d1-f74144c4dcb0.png)
    for![](assets/01238af1-5c97-438f-9c4d-6eeb770fc177.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/75ed9e52-7fbb-4b34-9e63-3bd0fff166ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *ß* is a parameter with a value less than one.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be implemented in Python as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three ways of specifying the value for ß:'
  prefs: []
  type: TYPE_NORMAL
- en: We can specify a default value of ß.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can make ß a parameter in our neural network and we can let the neural network
    decide the value (this is called **parametric ReLU**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can make ß a random value (this is called **randomized ReLU**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperbolic tangent (tanh)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The tanh function is similar to the sigmoid function, but it has the ability
    to give a negative signal as well. The following diagram illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f43bf042-f802-41dd-8990-e192757ad7c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The *y* function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/88736b33-6ea8-4e3e-ab82-887e5f487fe8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It can be implemented by the following Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now let's look at the softmax function.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes we need more than two levels for the output of the activation function.
    Softmax is an activation function that provides us with more than two levels for
    the output. It is best suited to multiclass classification problems.  Let''s assume
    that we have *n* classes. We have input values. The input values map the classes
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x = {x^((1)),x^((2)),....x^((n))}*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Softmax operates on probability theory. The output probability of the *e*^(th)
    class of the softmax is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bb2a0fea-2795-46ca-8749-aafbd462c707.png)For binary classifiers,
    the activation function in the final layer will be sigmoid, and for multiclass
    classifiers it will be softmax.'
  prefs: []
  type: TYPE_NORMAL
- en: Tools and Frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at the frameworks and tools available for implementing
    neural networks in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Over time, many different frameworks have been developed to implement neural
    networks. Different frameworks have their own strengths and weaknesses. In this
    section, we will focus on Keras with a TensorFlow backend.
  prefs: []
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras is one of the most popular and easy-to-use neural network libraries and
    is written in Python. It was written with ease of use in mind and provides the
    fastest way to implement deep learning. Keras only provides high-level blocks
    and is considered at the model level.
  prefs: []
  type: TYPE_NORMAL
- en: Backend Engines of Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keras needs a lower-level deep learning library to perform tensor-level manipulations.
    This lower level deep-learning library is called the *backend engine*. Possible
    backend engines for Keras include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow** ([www.tensorflow.org](https://www.tensorflow.org/)):This is
    the most popular framework of its kind and is open sourced by Google.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Theona**  ([deeplearning.net/software/theano](http://deeplearning.net/software/theano)):
    This was developed at the MILA lab at Université de Montréal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microsoft Cognitive Toolkit** (**CNTK**): This was developed by Microsoft.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The format of this modular deep learning technology stack is shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d57a079b-0263-474a-9c6f-a419ed2ecb88.png)'
  prefs: []
  type: TYPE_IMG
- en: The advantage of this modular deep learning architecture is that the backend
    of Keras can be changed without rewriting any code. For example, if we find TensorFlow
    better than Theona for a particular task, we can simply change the backend to
    TensorFlow without rewriting any code.
  prefs: []
  type: TYPE_NORMAL
- en: Low-level layers of the deep learning stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The three backend engines we just mentioned can all run both on CPUs and GPUs
    using the low-level layers of the stack. For CPUs, a low-level library of tensor
    operations called **Eigen** is used. For GPUs, TensorFlow uses NVIDIA's **CUDA
    Deep Neural Network** (**cuDNN**) library.
  prefs: []
  type: TYPE_NORMAL
- en: Defining hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed in [Chapter 6](ce14ecc1-8ad5-406e-88d0-7f3acb3e4569.xhtml), *Unsupervised
    Machine Learning Algorithms*, a hyperparameter is a parameter whose value is chosen
    before the learning process starts. We start with common-sense values and then
    try to optimize them later. For neural networks, the important hyperparameters
    are these:'
  prefs: []
  type: TYPE_NORMAL
- en: The activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of neurons in each hidden layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look into how we can define a model using Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a Keras model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three steps involved in defining a complete Keras model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define the layers**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can build a model using Keras in two possible ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The** **Sequential API:**  This allows us to architect models for a linear
    stack of layers. It is used for relatively simple models and is the usual choice
    for building models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/d836d917-46b6-4525-b030-bd434e2ee4cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, here, we have created three layers – the first two layers have the
    ReLU activation function and the third layer has softmax as the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Functional API:** This allows us to architect models for acyclic graphs
    of layers. More complex models can be created using the Functional API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/7caf80b6-fee4-4173-be0d-5a6efd584932.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we can define the same neural network using both the Sequential and
    Functional APIs. From the point of view of performance, it does not make any difference
    which approach you take to define the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Define the learning process**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this step, we define three things:'
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The metrics that will quantify the quality of the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/c540f046-60b4-4740-8d6a-f164581aed1e.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we use the `model.compile` function to define the optimizer, loss
    function, and metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Train the model**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the architecture is defined, it is time to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/96bff123-20b0-49ed-9739-c22f3e8ddff5.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that parameters such as `batch_size` and `epochs` are configurable parameters,
    making them hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing sequential or functional model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The sequential model creates the ANN as a simple stack of layer. The sequential
    model is easy and straightforward to understand and implement, but its simplistic
    architecture also has a major restriction. Each layer is connected to exactly
    one input and output tensor. This means that if our model has multiple inputs
    or multiple outputs either at the input or output or at any of the hidden layers,
    then we cannot use a sequential model. In this case, we will have to use the functional
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is one of the most popular libraries for working with neural networks.
    In the preceding section, we saw how we can use it as the backend engine of Keras.
    It is an open source, high-performance library that can actually be used for any
    numerical computation. If we look at the stack, we can see that we can write TensorFlow
    code in a high-level language such as Python or C++, which gets interpreted by
    the TensorFlow distributed execution engine. This makes it quite useful for and
    popular with developers.
  prefs: []
  type: TYPE_NORMAL
- en: The way TensorFlow works is that you create a **Directed Graph** (**DG**) to
    represent your computation.  Connecting the nodes are the edges, the input, and
    the output of the mathematical operations. Also, they represent arrays of data.
  prefs: []
  type: TYPE_NORMAL
- en: Presenting TensorFlow's Basic Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a brief look at TensorFlow concepts such as scalars, vectors, and
    matrices. We know that a simple number, such as three or five, is called a **scalar**
    in traditional mathematics. Moreover, in physics, a **vector** is something with
    magnitude and direction. In terms of TensorFlow, we use a vector to mean one-dimensional
    arrays. Extending this concept, a two-dimensional array is a **matrix**. For a
    three-dimensional array, we use the term **3D tensor**. We use the term **rank**
    to capture the dimensionality of a data structure. As such, a **scalar** is a
    **rank 0** data structure, a **vector** is a **rank** **1** data structure, and
    a **matrix** is a **rank** **2** data structure. These multi-dimensional structures
    are known as **tensors** and are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1eceea7d-8244-4156-90ee-d4b739fe0fe1.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see in the preceding diagram, the rank defines the dimensionality
    of a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now look at another parameter, `shape`. `shape` is a tuple of integers
    specifying the length of an array in each dimension. The following diagram explains
    the concept of `shape`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/909e6fb6-e3be-42fb-aa53-48010ffe319a.png)'
  prefs: []
  type: TYPE_IMG
- en: Using `shape` and rank, we can specify the details of tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Tensor Mathematics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now look at different mathematical computations using tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define two scalars and try to add and multiply them using TensorFlow:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/5a659402-0713-4684-9f2a-82b06e36c24c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can add and multiply them and display the results:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/ced54688-183d-4fb6-b225-0363341a3a76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also create a new scalar tensor by adding the two tensors:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/fcdc1e92-3893-494a-ba31-d64e09f19618.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also perform complex tensor functions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/d8ad73e8-25c0-4028-b542-2da93252e716.png)'
  prefs: []
  type: TYPE_IMG
- en: Understanding the Types of Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is more than one way that neural networks can be built. If every neuron
    in each layer is connected to each of the neurons in another layer, then we call
    it a dense or fully connected neural network. Let's look at some other forms of
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolution Neural Networks** (**CNNs**) are typically used to analyze multimedia
    data. In order to learn more about how a CNN is used to analyze image-based data,
    we need to have a grasp of the following processes:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's explore them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of convolution emphasizes a pattern of interest in a particular
    image by processing it with another smaller image called a **filter** (also called
    a **kernel**). For example, if we want to find the edges of objects in an image,
    we can convolve the image with a particular filter to get them. Edge detection
    can help us in object detection, object classification, and other applications.
    So, the process of convolution is about finding characteristics and features in
    an image.
  prefs: []
  type: TYPE_NORMAL
- en: The approach to finding patterns is based on finding patterns that can be reused
    on different data. The reusable patterns are called filters or kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An important part of processing multimedia data for the purpose of machine
    learning is downsampling it. This provides two benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: It reduces the overall dimensionality of the problem, decreasing the time needed
    to train the model in a major way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through aggregation, we abstract the unnecessary details in the multimedia data,
    making it more generic and more representative of similar problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Downsampling is performed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e0c416f9-1cf3-4056-9e3d-26d427784c53.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we have replaced every block of four pixels with one pixel, choosing
    the highest value of the four pixels to be the value of that one pixel. This means
    that we have downsampled by a factor of four. As we have chosen the maximum value
    in each block, this process is called **max** **pooling**. We could have chosen
    the average value; in that case, it would be average pooling.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNNs**) are a special type of neural network
    that are based on looped architecture. This is why they are called *recurrent*.
    The important thing to note is that RNNs have memory. This means that they have
    the ability to store information from recent iterations. They are used in areas
    such as analyzing sentence structures to predict the next word in a sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative Adversarial Networks** (**GANs**) are a type of neural network
    that generate synthetic data. They were created in 2014 by Ian Goodfellow and
    his colleagues. They can be used to generate photographs of people that have never
    existed. More importantly, they are used to generate synthetic data to augment
    training datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming section, we will see what transfer learning is.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Over the years, many organizations, research groups, and individuals within
    the open source community have perfected some complex models trained using gigantic
    amounts of data for generic use cases. In some cases, they have invested years
    of effort in optimizing these models. Some of these open source models can be
    used for the following applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Object detection in video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection in images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transcription for audio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis for text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whenever we start working on training a new machine learning model, the question
    to ask ourselves is this: instead of starting from scratch, can we simply customize
    a well-established pre-trained model for our purposes? In other words, can we
    transfer the learning of existing models to our custom model so that we can answer
    our business question? If we can do that, it will provide three benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Our model training efforts will be given a jump-start.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using a well-tested and well-established model, the overall quality of our
    model is likely to improve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we do not have enough data for the problem we are working on, using a pre-trained
    model through transfer learning may help.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look into two actual examples where this would be useful:'
  prefs: []
  type: TYPE_NORMAL
- en: When training a robot, we could first use a simulation game to train a neural
    network model. In that simulation, we could create all those rare events that
    are very hard to find in the real world. Once trained, we could use transfer learning
    ;to train the model for the real world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's assume that we want to train a model that can classify Apple And Windows
    laptops from a video feed. There are already well-established object detection
    models available as open source that can accurately classify various objects in
    a video feed. We can use these models as a starting point and identify objects
    as laptops. Once we have identified the objects as laptops, we can further train
    the model to differentiate between Apple and Windows laptops.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will apply the concepts that we have covered in this
    chapter to building a fraudulent document classifying neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – using deep learning for fraud detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using **Machine Learning** (**ML**) techniques to identify fraudulent documents
    is an active and challenging field of research. Researchers are investigating
    to what extent the pattern recognition power of neural networks can be exploited
    for this purpose. Instead of manual attribute extractors, raw pixels can be used
    for several deep learning architectural structures.
  prefs: []
  type: TYPE_NORMAL
- en: Methodology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The technique presented in this section uses a type of neural network architecture
    called **Siamese neural networks**, which features two branches that share identical
    architectures and parameters. The use of Siamese neural networks to flag fraudulent
    documents is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/46b8db83-bcc9-4c6a-9bbd-bb19de3a84ac.png)'
  prefs: []
  type: TYPE_IMG
- en: When a particular document needs to be verified for authenticity, we first classify
    the document based on its layout and type, and then we compare it against its
    expected template and pattern. If it deviates beyond a certain threshold, it is
    flagged as a fake document; otherwise, it is considered an authentic or true document.
    For critical use cases, we can add a manual process for borderline cases where
    the algorithm cannot conclusively classify a document as authentic or fake.
  prefs: []
  type: TYPE_NORMAL
- en: To compare a document against its expected template, we use two identical CNNs
    in our Siamese architecture. CNNs have the advantage of learning optimal shift-invariant
    local feature detectors and can build representations that are robust to geometric
    distortions of the input image. This is well suited to our problem since we aim
    to pass authentic and test documents through a single network, and then compare
    their outcomes for similarity. To achieve this goal, we implement the following
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that we want to test a document. For each class of document,
    we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Get the stored image of the authentic document. We call it the **true document**.
    The test document should look like the true document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The true document is passed through the neural network layers to create a feature
    vector, which is the mathematical representation of the patterns of the true document.
    We call it F**eature Vector 1**, as shown in the preceding diagram.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The document that needs to be tested is called the **test document**. We pass
    this document through a neural network similar to the one that was used to create
    the feature vector for the true document. The feature vector of the test document
    is called F**eature Vector 2**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use the Euclidean distance between feature vector 1 and feature vector 2
    to calculate the similarity score between the true document and the test document.
    This similarity score is called the **Measure Of Similarity** (**MOS**). The MOS
    is a number between 0 and 1\. A higher number represents a lower distance between
    the documents and a greater likelihood that the documents are similar.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the similarity score calculated by the neural network is below a pre-defined
    threshold, we flag the document as fraudulent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s see  how we can implement Siamese neural networks using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the Python packages that are required:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define the neural network that will be used to process each of
    the branches of the Siamese network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that, in order to reduce overfitting, we have also specified  a dropout
    rate of `0.15`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement Siamese networks, we will use MNIST images. MNIST images are ideal
    for testing the effectiveness of our approach. Our approach entails preparing
    the data in such a way that each sample will have two images and a binary similarity
    flag. This flag is an indicator that they are from the same class. Let''s now
    implement the function named  `prepareData`, which can prepare the data for us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that  `prepareData()`  will result in an equal number of samples across
    all digits.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now prepare the training and testing datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create the two halves of the Siamese system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will implement the  **measure of similarity**  (**MOS**), which will
    quantify the distance between two documents that we want to compare:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s train the model. We will use 10 epochs to train this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/df9a4124-9b34-4f4e-86ad-464cdf028948.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we reached an accuracy of 97.49% using 10 epochs. Increasing the number
    of epochs will further improve the level of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first looked at the details of neural networks. We started
    by looking at how neural networks have evolved over the years. We studied different
    types of neural networks. Then, we looked at the various building blocks of neural
    networks. We studied in depth the gradient descent algorithm, which is used to
    train neural networks. We discussed various activation functions and studied the
    applications of activation functions in a neural network. We also looked at the
    concept of transfer learning. Finally, we looked at a practical example of how
    a neural network can be used to train a machine learning model that can be deployed
    to flag forged or fraudulent documents.
  prefs: []
  type: TYPE_NORMAL
- en: Looking ahead, in the next chapter, we will look into how we can use such algorithms
    for natural language processing. We will also introduce the concept of web embedding
    and will look into the use of recurrent networks for natural language processing.
    Finally, we will also look into how to implement sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
