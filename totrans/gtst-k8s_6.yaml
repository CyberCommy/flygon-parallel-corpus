- en: Chapter 6. Monitoring and Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will cover the usage and customization of both built-in and third-party
    monitoring tools on our Kubernetes cluster. We will cover how to use the tools
    to monitor health and performance of our cluster. In addition, we will look at
    built-in logging, the **Google Cloud Logging** service, and **Sysdig**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How Kuberentes uses cAdvisor, Heapster, InfluxDB, and Grafana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to customize the default Grafana dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How FluentD and Grafana are used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to install and use logging tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to work with popular third-party tools, such as StackDriver and Sysdig,
    to extend our monitoring capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real-world monitoring goes far beyond checking whether a system is up and running.
    Although health checks, like those you learned in [Chapter 2](part0022_split_000.html#KVCC1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 2. Kubernetes – Core Concepts and Constructs"), *Kubernetes – Core Concepts
    and Constructs*, under the *Health checks* section, can help us isolate problem
    applications. Operation teams can best serve the business when they can anticipate
    the issues and mitigate them before a system goes offline.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices in monitoring are to measure the performance and usage of core
    resources and watch for trends that stray from the normal baseline. Containers
    are not different here, and a key component to managing our Kubernetes cluster
    is having a clear view into performance and availability of the OS, network, system
    (CPU and memory), and storage resources across all nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will examine several options to monitor and measure the
    performance and availability of all our cluster resources. In addition, we will
    look at a few options for alerting and notifications when irregular trends start
    to emerge.
  prefs: []
  type: TYPE_NORMAL
- en: Built-in monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you recall from [Chapter 1](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 1. Kubernetes and Container Operations"), *Kubernetes and Container Operations*,
    we noted that our nodes were already running a number of monitoring services.
    We can see these once again by running the `get pods` command with the `kube-system`
    namespace specified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Built-in monitoring](../images/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1\. System pod listing
  prefs: []
  type: TYPE_NORMAL
- en: Again, we see a variety of services, but how does this all fit together? If
    you recall the *Node (formerly minions)* section from [Chapter 2](part0022_split_000.html#KVCC1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 2. Kubernetes – Core Concepts and Constructs"), *Kubernetes – Core Concepts
    and Constructs*, each node is running a kublet. The kublet is the main interface
    for nodes to interact and update the API server. One such update is the **metrics**
    of the node resources. The actual reporting of the resource usage is performed
    by a program named cAdvisor.
  prefs: []
  type: TYPE_NORMAL
- en: '**cAdvisor** is another open source project from Google, which provides various
    metrics on container resource use. Metrics include CPU, memory, and network statistics.
    There is no need to tell cAdvisor about individual containers; it collects the
    metrics for all containers on a node and reports this back to the kublet, which
    in turn reports to Heapster.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Google''s open source projects**'
  prefs: []
  type: TYPE_NORMAL
- en: Google has a variety of open source projects related to Kubernetes. Check them
    out, use them, and even contribute your own code!
  prefs: []
  type: TYPE_NORMAL
- en: 'cAdvisor and Heapster are mentioned in the following section:'
  prefs: []
  type: TYPE_NORMAL
- en: '**cAdvisor**: [https://github.com/google/cadvisor](https://github.com/google/cadvisor)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heapster**: [https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contrib** is a catch-all for a variety of components that are not part of
    core Kubernetes. It is found at [https://github.com/kubernetes/contrib](https://github.com/kubernetes/contrib).'
  prefs: []
  type: TYPE_NORMAL
- en: '**LevelDB** is a key store library that was used in the creation of InfluxDB.
    It is found at [https://github.com/google/leveldb](https://github.com/google/leveldb).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Heapster** is yet another open source project from Google; you may start
    to see a theme emerging here (see the preceding information box). Heapster runs
    in a container on one of the minion nodes and aggregates the data from kublet.
    A simple REST interface is provided to query the data.'
  prefs: []
  type: TYPE_NORMAL
- en: When using the GCE setup, a few additional packages are set up for us, which
    saves us time and gives us a complete package to monitor our container workloads.
    As we can see from Figure 6.1, there is another pod with `influx-grafana` in the
    title.
  prefs: []
  type: TYPE_NORMAL
- en: '**InfluxDB** is described at it''s official website as follows¹:'
  prefs: []
  type: TYPE_NORMAL
- en: '*An open-source distributed time series database with no external dependencies.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is based on a key store package (see the previous *Google's open source projects*
    information box) and is perfect to store and query event or time-based statistics
    such as those provided by Heapster.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have **Grafana**, which provides a dashboard and graphing interface
    for the data stored in InfluxDB. Using Grafana, users can create a custom monitoring
    dashboard and get immediate visibility into the health of their Kubernetes cluster
    and therefore their entire container infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Heapster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s quickly look at the REST interface by SSH''ing to the node with the
    Heapster pod. First, we can list the pods to find the one running Heapster as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The name of the pod should start with `monitoring-heapster`. Run a `describe`
    command to see which node it is running on as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: From the output in the following figure (Figure 6.2), we can see that the pod
    is running in `kubernetes-minion-merd`. Also note the IP for the pod, a few lines
    down, as we will need that in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring Heapster](../images/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2\. Heapster pod details
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can SSH to this box with the familiar `gcloud ssh` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: From here, we can access the `Heapster` REST API directly using the pod's IP
    address. Remember that pod IPs are routable not only in the containers but also
    on the nodes themselves. The `Heapster` API is listening on port `8082`, and we
    can get a full list of metrics at `/api/v1/metric-export-schema/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the list now by issuing a `curl` command to the pod IP address we
    saved from the `describe` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see a listing that is quite long. The first section shows all the metrics
    available. The last two sections list fields by which we can filter and group.
    For your convenience, I''ve added the following tables that are a little bit easier
    to read:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric | Description | Unit | Type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| uptime | The number of milliseconds since the container was started | ms
    | cumulative |'
  prefs: []
  type: TYPE_TB
- en: '| cpu/usage | Cumulative CPU usage on all cores | ns | cumulative |'
  prefs: []
  type: TYPE_TB
- en: '| cpu/limit | CPU limit in millicores | - | gauge |'
  prefs: []
  type: TYPE_TB
- en: '| memory/usage | Total memory usage | bytes | gauge |'
  prefs: []
  type: TYPE_TB
- en: '| memory/working_set | Total working set usage. Working set is the memory being
    used and not easily dropped by the kernel | bytes | gauge |'
  prefs: []
  type: TYPE_TB
- en: '| memory/limit | Memory limit | bytes | gauge |'
  prefs: []
  type: TYPE_TB
- en: '| memory/page_faults | The number of page faults | - | cumulative |'
  prefs: []
  type: TYPE_TB
- en: '| memory/major_page_faults | The number of major page faults | - | cumulative
    |'
  prefs: []
  type: TYPE_TB
- en: '| network/rx | Cumulative number of bytes received over the network | bytes
    | cumulative |'
  prefs: []
  type: TYPE_TB
- en: '| network/rx_errors | Cumulative number of errors while receiving over the
    network | - | cumulative |'
  prefs: []
  type: TYPE_TB
- en: '| network/tx | Cumulative number of bytes sent over the network | bytes | cumulative
    |'
  prefs: []
  type: TYPE_TB
- en: '| network/tx_errors | Cumulative number of errors while sending over the network
    | - | cumulative |'
  prefs: []
  type: TYPE_TB
- en: '| filesystem/usage | Total number of bytes consumed on a filesystem | bytes
    | gauge |'
  prefs: []
  type: TYPE_TB
- en: '| filesystem/limit | The total size of filesystem in bytes | bytes | gauge
    |'
  prefs: []
  type: TYPE_TB
- en: '*Table 6.1\. Available Heapster metrics*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| Field | Description | Label type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `hostname` | The hostname where the container ran | Common |'
  prefs: []
  type: TYPE_TB
- en: '| `host_id` | An identifier specific to a host, which is set by cloud provider
    or user | Common |'
  prefs: []
  type: TYPE_TB
- en: '| `container_name` | The user-provided name of the container or full container
    name for system containers | Common |'
  prefs: []
  type: TYPE_TB
- en: '| `pod_name` | The name of the pod | Pod |'
  prefs: []
  type: TYPE_TB
- en: '| `pod_id` | The unique ID of the pod | Pod |'
  prefs: []
  type: TYPE_TB
- en: '| `pod_namespace` | The namespace of the pod | Pod |'
  prefs: []
  type: TYPE_TB
- en: '| `namespace_id` | The unique ID of the namespace of the pod | Pod |'
  prefs: []
  type: TYPE_TB
- en: '| `labels` | A comma-separated list of user-provided labels | Pod |'
  prefs: []
  type: TYPE_TB
- en: '*Table 6.2\. Available Heapster fields*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Customizing our dashboards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have the fields, we can have some fun. Recall the Grafana page
    we looked at in [Chapter 1](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 1. Kubernetes and Container Operations"), *Kubernetes and Container Operations*.
    Let''s pull that up again by going our cluster''s monitoring URL. Note that you
    may need to log in with your cluster credentials. Refer to the following format
    of the link you need to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '`https://`**`<your master IP>`**`/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana`'
  prefs: []
  type: TYPE_NORMAL
- en: We'll see the default Kubernetes dashboard, and now we can add our own statistics
    to the board. Scroll all the way to the bottom and click on **Add a Row**. This
    should create a space for a new row and present a green tab on the left-hand side
    of the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by adding a view into the filesystem usage for each node (minion).
    Click on the *green* tab to expand and then choose **Add Panel** and then **graph**.
    An empty graph should appear on the screen. If we click on the **graph** where
    it says **no title (click here)**, a context menu will appear. We can then click
    on **Edit**, and we'll be able to set up the query for our custom dashboard panel.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **series** box allows us to use any of the Heapster metrics we saw in the
    previous tables. In the **series** box, enter `filesystem/usage_bytes_gauge` and
    select to **max(value)**. Then, enter `5s` for **group by time** and **hostname**
    in the box marked column next to the plus sign, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customizing our dashboards](../images/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3\. Heapster pod details
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's click on the **Axes & Grid** tab, so that we can set the units and
    legend. Under **Left Y Axis**, set **Format** to **bytes** and **Label** to **Disk
    Space Used**. Under **Right Y Axis**, set **Format** to **none**. Next, under
    **Legend** styles, make sure to check **Show values, and table**. A **Legend Values**
    section should appear, and we can check the box for **Max** here.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's quickly go to the **General** tab and choose a title. In my case,
    I named mine `Filesystem Disk Usage by Node (max)`.
  prefs: []
  type: TYPE_NORMAL
- en: We don't want to lose this nice new graph we've created, so let's click on the
    save icon in the top right corner. It looks like a *floppy disk* (you can do a
    Google image search if you don't know what those are).
  prefs: []
  type: TYPE_NORMAL
- en: After we click on the save icon, a dropdown will appear with several options.
    The first item should have the default dashboard title, which is **Kubernetes
    Cluster!** at the time of this writing. Also, click on the save icon on the right-hand
    side.
  prefs: []
  type: TYPE_NORMAL
- en: It should take us back to the main dashboard where we will see our new graph
    at the bottom. Let's add another panel to that row. Again use the *green* tab
    and then select **Add Panel** and **singlestat**. Once again, an empty panel will
    appear, and we can click it where it says **no title (click here)** for the context
    menu and then click on **Edit**.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say, we want to watch a particular node and monitor memory usage. We can
    easily do this by setting the where clause in our query. First, choose **network/rx_bytes_cumulative**
    for **series** and **mean(value)** for **select**. Then, we can specify the hostname
    in the `where` clause with `hostname=kubernetes-minion-35ao` and **group by time**
    to `5s`. (Use one of your own hostnames if you are following along).
  prefs: []
  type: TYPE_NORMAL
- en: '![Customizing our dashboards](../images/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4\. Singlestat options
  prefs: []
  type: TYPE_NORMAL
- en: Under the **Options** tab, make sure that **Unit format** is set to **bytes**
    and check the **Spark line** box under **Spark lines**. The **sparkline** gives
    us a quick history view of the recent variation in the value. We can use the **Background**
    mode to take up the entire background; by default, it uses the area below the
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Under **Coloring**, we can optionally check the **Value** box. A **Thresholds**
    and **Colors** section will appear. This will allow us to choose different colors
    for the value based on the threshold tier we specify. Note that an unformatted
    version of the number must be used for threshold values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s go back to the **General** tab and choose a title as **Network
    bytes received (Node 35ao)**. Once again, let''s save our work and return to the
    dashboard. We should now have a row that looks like the following figure (Figure
    6.5):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Customizing our dashboards](../images/00062.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5\. Custom dashboard panels
  prefs: []
  type: TYPE_NORMAL
- en: A third type of panel we didn't cover is **text**. It's pretty straightforward
    and allows us to place a block of text on the dashboard using HTML, markdown,
    or just plain text.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, it is pretty easy to build a custom dashboard and monitor the
    health of our cluster at a glance.
  prefs: []
  type: TYPE_NORMAL
- en: FluentD and Google Cloud Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking back at Figure 6.1, you may have noted a number of pods starting with
    the words **fluentd-cloud-logging-kubernetes**. These pods appear when using the
    GCE provider for your K8s cluster. A pod like this exists on every node in our
    cluster and its sole purpose to handle the processing of Kubernetes logs.
  prefs: []
  type: TYPE_NORMAL
- en: If we log in to our Google Cloud Platform account, we can see some of the logs
    processed there. Simply navigate to our project page, and on the left, under **Monitoring**,
    click on **Logs**. (If you are using the beta console, it will be under **Operations**
    and then **Logging**.) This will take us to a log listing page with a number of
    drop-down menus on the top. If this is your first time visiting the page, you
    should see a log selection dropdown with the value **All Logs**.
  prefs: []
  type: TYPE_NORMAL
- en: In this dropdown, we'll see a number of Kubernetes-related entries, including
    **kublet** and some entries with **kubernetes** at the beginning of the label.
    We can also filter by date and use the *play* button to watch events stream in
    live.
  prefs: []
  type: TYPE_NORMAL
- en: '![FluentD and Google Cloud Logging](../images/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6\. The Google Cloud Logging filter
  prefs: []
  type: TYPE_NORMAL
- en: FluentD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we know that the `fluentd-cloud-logging-kubernetes` pods are sending the
    data to the Google Cloud, but why do we need FluentD? Simply put, **FluentD**
    is a collector. It can be configured to have multiple sources to collect and tag
    logs, which are then sent to various output points for analysis, alerting, or
    archiving. We can even transform data using plugins before it is passed on to
    its destination.
  prefs: []
  type: TYPE_NORMAL
- en: Not all provider setups have FluentD installed by default, but it is one of
    the recommended approaches to give us greater flexibility for future monitoring
    operations. The AWS Kubernetes setup also uses FluentD, but instead forwards events
    to **Elasticsearch**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Exploring FluentD**'
  prefs: []
  type: TYPE_NORMAL
- en: If you are curious about the inner workings of the FluentD setup or just want
    to customize the log collection, we can explore quite easily using the `kubectl
    exec` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s see if we can find the FluentD `config` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Usually, we would look in the `etc` folder for a `ta-agent` or `fluent` subfolder.
    However, if we run an `ls` command, we''ll see that there is no `ta-agent` or
    `fluent` subfolder, but there is a `google-fluentd` subfolder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'While searching in this directory, we should see a `google-fluentd.conf` file.
    We can view that file with a simple `cat` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We should see a number of sources including the `kublet`, `containers`, `etcd`,
    and various other Kubernetes components.
  prefs: []
  type: TYPE_NORMAL
- en: Note that while we can make changes here, remember that is a running container
    and our changes won't be saved if the pod dies or is restarted. If we really want
    to customize, it's best to use this container as a base and build a new container
    that we can push to a repository for later use.
  prefs: []
  type: TYPE_NORMAL
- en: Maturing our monitoring operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Grafana gives us a great start to monitor our container operations, it
    is still a work in progress. In the real world of operations, having a complete
    dashboard view is great once we know there is a problem. However, in everyday
    scenarios, we'd prefer to be proactive and actually receive notifications when
    issues arise. This kind of alerting capability is a must to keep the operations
    team ahead of the curve and out of *reactive mode*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many solutions available in this space, and we will take a look at
    two in particular: GCE monitoring (StackDriver) and Sysdig.'
  prefs: []
  type: TYPE_NORMAL
- en: GCE (StackDriver)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**StackDriver** is a great place to start for infrastructure in the public
    cloud. It is actually owned by Google, so it''s integrated as the Google Cloud
    Platform monitoring service. Before your lock-in alarm bells start ringing, StackDriver
    also has solid integration with AWS. In addition, StackDriver has alerting capability
    with support for notification to a variety of platforms and webhooks for anything
    else.'
  prefs: []
  type: TYPE_NORMAL
- en: Sign-up for GCE monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the GCE console, under the **Monitoring** section, there is a **Dashboard
    & alerts** link (or just the **Monitoring** link under **Operations** in the beta
    console). This will open a new window where we can enable the monitoring functionality
    (still in beta at the time of this writing). Once enabled, we'll be taken to a
    screen that has install instructions for each operating system (this will be under
    **Set up and monitor an endpoint** in the beta console). It will also show your
    API key, which is necessary for the installation.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to do something similar in AWS, you can simply sign up for account
    at StackDriver''s main website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.stackdriver.com/](https://www.stackdriver.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: Installation instructions for the more common installs can be found at [http://support.stackdriver.com/customer/en/portal/articles/1491726-what-is-the-stackdriver-agent](http://support.stackdriver.com/customer/en/portal/articles/1491726-what-is-the-stackdriver-agent).
  prefs: []
  type: TYPE_NORMAL
- en: We can find our API key under **Account Settings** and **API Keys**.
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Go to Monitoring** to proceed. We'll be taken to the main dashboard
    page where we will see some basic statistics on our node in the cluster. If we
    go to **Infrastructure** and then **Instances**, we'll be taken to a page with
    all our nodes listed. By clicking on the individual node, we can again see some
    basic information even without an agent installed.
  prefs: []
  type: TYPE_NORMAL
- en: Configure detailed monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have seen, simply enabling monitoring will give us basic stats for all
    our machines in GCE, but if we want to get detailed results, we'll need the agent
    on each node. Let's walk through an install.
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, we''ll want to use the `gcloud compute ssh` command to get a shell
    on one of our minion nodes. Then, we can download and install the agent. If you
    need your API key, this can be found by clicking your user icon in the top-right
    corner and going to **Account Settings** and then on the next page, click on **API
    Keys** in the menu on the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything goes well, we should have an agent installed and ready. We can
    check this by running the `info` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We should see a lot of information in the form of JSON on the screen. After
    you finish, give the agent a few minutes before going back to **Infrastructure**
    and **Instances**.
  prefs: []
  type: TYPE_NORMAL
- en: On the summary instance page, we'll note that all our GCE instances are showing
    CPU usage. However, only the instance with the agent installed will show the **Memory
    usage** statistic.
  prefs: []
  type: TYPE_NORMAL
- en: Click on the node with the agent installed, so we can inspect it a bit further.
    If we click on each one and look at the details page, we should note that the
    instance with the agent installed has a lot more information. Although all instances
    report CPU usage, Disk I/O, and network traffic, the instance with the agent has
    much more.
  prefs: []
  type: TYPE_NORMAL
- en: '![Configure detailed monitoring](../images/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7\. Google Cloud Monitoring with agent installed
  prefs: []
  type: TYPE_NORMAL
- en: In Figure 6.7, we can see a variety of additional charts including Open TCP
    connections and processes as well as CPU steal (not pictured). We also have better
    visibility into the machine details such as network interfaces, file systems,
    and operating system information.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we see how much information is available, we can install the agent
    on the remaining instances. You may also wish to install an agent on the master
    as it is a critical piece of your Kubernetes infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Alerts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we can look at the alerting policies available as part of the monitoring
    service. From the instance details page, click on the **Create Alerting Policy**
    button in the **Incidents** section at the top of the page.
  prefs: []
  type: TYPE_NORMAL
- en: We'll name the policy as `Excessive CPU Load` and set a metric threshold. Under
    the section, in the **Metric Threshold** area, click on **Next** and then in the
    **TARGET** section, set **Resource Type** to **Instances**. Then, set **Applies
    To** to **Group** and **kubernetes**. Leave **Condition Triggers If** set to **Any
    Member Violates**.
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Next** and leave **IF METRIC** as **CPU (agent)** and **CONDITION**
    as **above**. Now set **THRESHOLD (PERCENT)** to `80` and leave the time under
    **FOR** to **5 minutes**. Click on **Save Condition**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Alerts](../images/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8\. Google Cloud Monitoring alert policy
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will add a notification. Under that section, leave **Method** as
    **Email** and click on **Add Notification**. Enter your e-mail address and then
    click on **Save Policy**.
  prefs: []
  type: TYPE_NORMAL
- en: Now whenever the CPU from one of our instances goes above 80 percent, we will
    receive an e-mail notification. If we ever need to review our policies, we can
    find them under the **Alerting** dropdown and **Policies Overview** at the menu
    on the top of the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond system monitoring with Sysdig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monitoring our cloud systems is a great start, but what about visibility into
    the containers themselves? Although there are a variety of cloud monitoring and
    visibility tools, Sysdig stands out for its ability to dive deep not only into
    system operations but specifically containers.
  prefs: []
  type: TYPE_NORMAL
- en: Sysdig is open source and is billed as *a universal system visibility tool with
    native support for containers**²*. It is a command-line tool, which provides insight
    into the areas we've looked at earlier such as storage, network, and system processes.
    What sets it apart is the level of detail and visibility it offers for these process
    and system activities. Furthermore, it has native support for containers, which
    gives us a full picture of our container operations. This is a highly recommended
    tool for your container operations arsenal. Their main website is [http://www.sysdig.org/](http://www.sysdig.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Sysdig Cloud
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will take a look at the Sysdig tool and some of the useful command-line-based
    UIs in a moment. However, the team at Sysdig has also built a commercial product,
    named **Sysdig Cloud**, which provides the advanced dashboard, alerting, and notification
    services we discussed earlier in the chapter. Also, the differentiator here has
    high visibility into containers, including some nice visualizations of our application
    topology.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you'd rather skip the *Sysdig Cloud* section and just try out the command-line
    tool, simply skip to the *Sysdig command line* section later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: If you have not done so already, sign up for Sysdig Cloud at [http://www.sysdigcloud.com](http://www.sysdigcloud.com).
  prefs: []
  type: TYPE_NORMAL
- en: After activating and logging in for the first time, we'll be taken to a welcome
    page. Clicking on **Next**, we are shown a page with various options to install
    the `sysdig` agents. For our example environment, we will use a Linux agent. The
    **Next** button will be disabled until we install at least one agent. The page
    should show the following command with our *access key* filled in.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We'll need to SSH into our master and each node to run the installer. It will
    take a few minutes to install several packages and then set up the connection
    to the Sysdig Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: After our first install completes, the page should update with the text **You
    have one agent connected!** and the **Next** button will become active. Go ahead
    and install the rest of the agents and then come back to this page and click on
    **Next**.
  prefs: []
  type: TYPE_NORMAL
- en: We can skip the AWS setup for now and then click on **Let's Get Started** on
    the final screen.
  prefs: []
  type: TYPE_NORMAL
- en: We'll be taken to the main **sysdig cloud** dashboard screen. **kubernetes-master**
    and our various minion nodes should appear under the **Explore** tab. We should
    see something similar to Figure 6.9 with our cluster master and all four minion
    nodes (or the nodes we have already installed agents on).
  prefs: []
  type: TYPE_NORMAL
- en: '![Sysdig Cloud](../images/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9\. Sysdig Cloud Explore page
  prefs: []
  type: TYPE_NORMAL
- en: This page shows us a table view and the links on the left let us explore some
    key metrics for CPU, memory, networking, and so on. Although this is a great start,
    the detailed views will give us a much deeper look at each node.
  prefs: []
  type: TYPE_NORMAL
- en: Detailed views
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s take a look at these views. Select **kubernetes-master** and then scroll
    down to the detail section that appears below. By default, we should see the **System:
    Overview by Process** view (If it''s not selected, just click on it in the list
    on the left.) If the chart is hard to read, simply use the maximize icon in the
    top-left corner of each graph for a larger view.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a variety of interesting views to explore. Just to call out a few
    others, **Application: HTTP** and **System: Overview** by container give us some
    great charts for inspection. In the later view, we can see stats for CPU, memory,
    network, and file usage by container.'
  prefs: []
  type: TYPE_NORMAL
- en: Topology views
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In addition, there are three topology views at the bottom. These views are
    perfect for helping us understand how our application is communicating. Click
    on **Topology: Network Traffic** and wait a few seconds for the view to fully
    populate. It should look similar to Figure 6.10:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Topology views](../images/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10\. Sysdig Cloud network topology view
  prefs: []
  type: TYPE_NORMAL
- en: We note the view maps out the flow of communication between the minion nodes
    and the master in the cluster. On the right-hand side, there may be connections
    to servers with a **1e100.net** name and also **169.254.169.254**, which are both
    part of Google infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may also note a **+** symbol in the top corner of the node boxes. Click
    on that in **kubernetes-master** and use the zoom tools at the top of the view
    area to zoom into the details, as you see in Figure 6.11:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Topology views](../images/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11\. The Sysdig Cloud network topology detailed view
  prefs: []
  type: TYPE_NORMAL
- en: Note that we can now see all the components of Kubernetes running inside the
    master. We can see how the various components work together. We will see **kubectl**
    and the **kublet** process running, as well as a number of boxes with the Docker
    whale, which indicate that they are containers. If we zoom in and use the plus
    icon, we will see that these are the containers for core Kubernetes process, as
    we saw in the services running on the master section in [Chapter 1](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 1. Kubernetes and Container Operations"), *Kubernetes and Container Operations*.
  prefs: []
  type: TYPE_NORMAL
- en: Also, if we pan over to the minion, we can also see **kublet**, which initiates
    communication, and follow it all the way through the `kube-apiserver` container
    in the master.
  prefs: []
  type: TYPE_NORMAL
- en: We can even see the instance probing for GCE metadata on **169.254.169.254**.
    This view is great in order to get a mental picture of how our infrastructure
    and underlying containers are talking to one another.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Next, let's switch over to the **Metrics** tab in the left-hand menu next to
    **Views**. Here, there are also a variety of helpful views.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at **capacity.estimated.request.total.count (avg)** under **System**.
    This view shows us an estimate of how many requests a node is capable of handling
    when fully loaded. This can be really useful for infrastructure planning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Metrics](../images/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12\. Sysdig Cloud capacity estimate view
  prefs: []
  type: TYPE_NORMAL
- en: Alerting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have all this great information, let's create some notifications.
    Scroll back up to the top of the page and find the bell icon next to one of your
    minion entries. This will open a **New Alert** dialog. Here, we can set manual
    alerts similar to what we did earlier in the chapter. However, there is also the
    option to use **Baselines** and **Host comparison**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the **Baseline** option is extremely helpful as Sysdig will watch the
    historical patterns of the node and alert us whenever one of the metrics strays
    outside the expected metric thresholds. No manual settings are required, so this
    can really save time for the notification setup and help our operations team to
    be proactive before issues arise. Refer to the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alerting](../images/00070.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13\. Sysdig Cloud new alert
  prefs: []
  type: TYPE_NORMAL
- en: The **Host Comparison** option is also a great help as it allows us to compare
    metrics with other hosts and alert whenever one host has a metric that differs
    significantly from the group. A great use case for this is monitoring resource
    usage across minion nodes to ensure that our scheduling constraints are not creating
    a bottleneck somewhere in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: You can choose whichever option you like, give it a name and description and
    choose a notification method. Sysdig supports e-mail, **SNS** (short for **Simple
    Notification Service**), and **PagerDuty** as notification methods. Once you have
    everything set, just click on **Create** and you will start to receive alerts
    as issues come up.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An exciting new feature that has been recently released is support for integrating
    directly with the Kubernetes API. The agents make calls to K8s so that it is aware
    of metadata and the various constructs, such as pods and RCs.
  prefs: []
  type: TYPE_NORMAL
- en: We can check this out easily on the main dashboard by clicking the gear icon
    next to the word Show on the top bar. We should see some filter options as in
    the following figure (Figure 6.14). Click on the **Apply** button next to **Logical
    Apps Hierarchy - Kubernetes**. This will set a number of filters that organizes
    our list in order of namespace, RC, pods, and finally container ID.
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes support](../images/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14\. Sysdig Cloud Kubernetes filters
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then select a default namespace from the list and use the detail views
    later, as we did before. By selecting the **Topology: Network Traffic** view,
    we can drill into the namespace and get a visual for each RC and the pods running
    within (see Figure 6.15):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes support](../images/00072.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15\. Sysdig Cloud Kubernetes-aware topology view
  prefs: []
  type: TYPE_NORMAL
- en: The Sysdig command line
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether you only use the open source tool or you are trying out the full Sysdig
    Cloud package, the command-line utility is a great companion to have to track
    down issues or get a deeper understanding of your system.
  prefs: []
  type: TYPE_NORMAL
- en: In the core tool, there is the main `sysdig` utility and also a command-line
    style UI named `csysdig`. Let's take a look at a few useful commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll need to SSH to the master or one of the minion nodes where we installed
    the Sysdig Cloud agents. It''s a single command to install the CLI tools as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can find instructions for other OSes at [http://www.sysdig.org/install/](http://www.sysdig.org/install/).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can see the process with the most network activity by issuing the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Sysdig command line](../images/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16\. A Sysdig top process by network activity
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an interactive view that will show us a top process in terms of network
    activity. Also, there are a plethora of commands to use with `sysdig`. A few other
    useful commands to try out include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More examples can be found at [http://www.sysdig.org/wiki/sysdig-examples/](http://www.sysdig.org/wiki/sysdig-examples/).
  prefs: []
  type: TYPE_NORMAL
- en: The csysdig command-line UI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Because we are in a shell on one of our nodes doesn''t mean we can''t have
    a UI. Csysdig is a customizable UI to explore all the metrics and insight that
    Sysdig provides. Simply type `csysdig` at the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: After entering csysdig, we see a real-time listing of all processes on the machine.
    At the bottom of the screen, you'll note a menu with various options. Click on
    **Views** or *F2* if you love to use your keyboard. On the left-hand menu, there
    are a variety of options, but we'll look at threads. Double-click to select **Threads**.
  prefs: []
  type: TYPE_NORMAL
- en: We can see all the threads currently running on the system and some information
    about the resource usage. By default, we see a big list that is updating often.
    If we click on the **Filter**, *F4* for the mouse challenged, we can slim down
    the list.
  prefs: []
  type: TYPE_NORMAL
- en: Type `kube-apiserver`, if you are on the master, or `kube-proxy`, if you are
    on a (minion) node, in the filter box and press enter. The view now filters for
    only the threads in that command.
  prefs: []
  type: TYPE_NORMAL
- en: '![The csysdig command-line UI](../images/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17\. Csysdig threads
  prefs: []
  type: TYPE_NORMAL
- en: If we want to inspect a little further, we can simply select one of the threads
    in the list and click on **Dig** or *F6*. Now we see a detail listing of system
    calls from the command in real time. This can be a really useful tool to gain
    deep insight into the containers and processing running on our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Press **Back** or the *backspace* key to go back to the previous screen. Then,
    go to **Views** once more. This time, we will look at the **Containers** view.
    Once again, we can filter and also use the **Dig** view to get more in-depth visibility
    into what is happening at a system call level.
  prefs: []
  type: TYPE_NORMAL
- en: Another menu item you might note here is **Actions**, which is available in
    the newest release. These features allow us to go from process monitoring to action
    and response. It gives us the ability to perform a variety of actions from the
    various process views in csysdig. For example, the container view has actions
    to drop into a bash shell, kill containers, inspect logs, and more. It's worth
    getting to know the various actions and hotkeys and even add you own custom hotkeys
    for common operations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We took a quick look at monitoring and logging with Kubernetes. You should now
    be familiar with how Kubernetes uses cAdvisor and Heapster to collect metrics
    on all the resources in a given cluster. Furthermore, we saw how Kubernetes saves
    us time by providing InfluxDB and Grafana set up and configured out of the box.
    Dashboards are easily customizable for our everyday operational needs.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we looked at the built-in logging capabilities with FluentD and
    the Google Cloud Logging service. Also, Kubernetes gives us great time savings
    by setting up the basics for us.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned about the various third-party options available to monitor
    our containers and clusters. Using these tools will allow us to gain even more
    insight into the health and status of our applications. All these tools combine
    to give us a solid toolset to manage day-to-day operations.
  prefs: []
  type: TYPE_NORMAL
- en: Footnotes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ¹[http://stackdriver.com/](http://stackdriver.com/)
  prefs: []
  type: TYPE_NORMAL
- en: ²[http://www.sysdig.org/wiki/](http://www.sysdig.org/wiki/)
  prefs: []
  type: TYPE_NORMAL
