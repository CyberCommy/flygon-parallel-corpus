- en: Chapter 6. Using Concurrency and Parallelism with Schedulers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern processors have multiple cores and enable many time-consuming operations
    to be processed faster simultaneously. The Java concurrency API (which includes
    threads and much more) makes it possible to do just that.
  prefs: []
  type: TYPE_NORMAL
- en: RxJava's `Observable` chains seem a good match for the threads. It would be
    great if we could *subscribe* to our source and do all the transforming, combining,
    and filtering in the background and, when everything is done, have the result
    to be passed to the main threads. Yes, this sounds wonderful, but RxJava is single-threaded
    by default. This means that, in the most cases, when the `subscribe` method is
    called on an `Observable` instance, the current thread blocks until everything
    is emitted. (This is not true for the `Observable` instances created by the `interval`
    or `timer` factory methods, for example.). This is a good thing because working
    with threads is not so easy. They are powerful, but they need to be synchronized
    with each other; for example, when one depends on the result of another.
  prefs: []
  type: TYPE_NORMAL
- en: One of the hardest things to manage in a multi-threaded environment is the shared
    data between the threads. One thread could read from a data source while another
    is modifying it, which leads to different versions of the same data being used
    by the different threads. If an `Observable` chain is constructed the right way,
    there is no shared state. This means that synchronization is not so complex.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will talk about executing things in parallel and look at
    what concurrency means. Additionally, we''ll learn some techniques for handling
    the situation when too many items are emitted by our `Observable` instances (a
    situation which is not so rare in the multi-threaded environment). The topics
    covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Using `Scheduler` instances to achieve *concurrency*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Buffering**, **throttling**, and **debouncing** with `Observable` instances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RxJava's schedulers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The schedulers are the RxJava's way of achieving concurrency. They are in charge
    of creating and managing the threads for us (internally relying on Java's threadpool
    facilities). We won't be dealing with Java's concurrency API and its quirks and
    complexities. We've been using the schedulers all along, implicitly with timers
    and intervals, but the time has come to master them.
  prefs: []
  type: TYPE_NORMAL
- en: Let's recall the `Observable.interval` factory method, which we introduced back
    in [Chapter 3](ch03.html "Chapter 3. Creating and Connecting Observables, Observers,
    and Subjects"), *Creating and Connecting Observables, Observers, and Subjects*.
    As we saw before, RxJava is *single-threaded* by *default*, so in most cases,
    calling the `subscribe` method on the `Observable` instance will block the current
    thread. But that is not the case with the `interval Observable` instances. If
    we look at the JavaDoc of the `Observable<Long> interval(long interval, TimeUnit
    unit)` method, we'll see that it says that the `Observable` instance created by
    it operates on something called '*the computation Scheduler*'.
  prefs: []
  type: TYPE_NORMAL
- en: In order to inspect the behavior of the `interval` method (as well as other
    things in this chapter) we will need a powerful debugging utility. That's why
    the first thing we'll be doing in this chapter is implementing it.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Observables and their schedulers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, we've introduced the `doOnNext()` operator, which could
    be used for logging the emitted items directly from within the `Observable` chain.
    We mentioned that there are `doOnError()` and `doOnCompleted()` operators too.
    But there is one that combines all three of them—the `doOnEach()` operator. We
    can log everything from it because it receives all the notifications emitted,
    regardless of their type. We can put it halfway through the chain of operators
    and use it to log, say, the state there. It takes a `Notification -> void` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the source of a higher order *debug* function returning a `lambda`
    result, which is capable of logging the emissions of an `Observable` instance
    labeled, using the passed description:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Depending on the passed *description* and *offset*, the returned method logs
    each notification. The important thing, however, is that it logs the current active
    thread's name before everything else. `<value>` marks the *OnNext notifications*;
    `X`, the *OnError notifications*; and `|`, the *OnCompleted notifications*, and
    the `nextOffset` variable is used to show the values in time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of using this new method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This example will generate five sequential numbers, beginning with the number
    five. We pass a call to our `debug(String, String)` method to the `doOnEach()`
    operator to log everything after the call of the `range()` method. With a subscribe
    call without parameters, this little chain will be triggered. The output is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The first thing logged is the name of the current thread (the main one), then
    we have the description of the `Observable` instance passed to the `debug()` method,
    and after that, a colon and dashes forming arrows, representing the time. Finally
    we have the symbol of the type of the notification—the value itself for values
    and `|` for completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define one overload to the `debug()` helper method so that we don''t
    need to pass a second parameter to it with an additional offset, if it is not
    needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The code for the preceding methods can be viewed/downloaded at: [https://github.com/meddle0x53/learning-rxjava/blob/master/src/main/java/com/packtpub/reactive/common/Helpers.java](https://github.com/meddle0x53/learning-rxjava/blob/master/src/main/java/com/packtpub/reactive/common/Helpers.java).'
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to debug what's happening with the `Observable` instances,
    created by the interval method!
  prefs: []
  type: TYPE_NORMAL
- en: The interval Observable and its default scheduler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s examine the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates an `interval Observable` instance, emitting every half second.
    We use the `take()` method to get only the first five *notifications* and to complete.
    We''ll use our `debug()` helper method to log the values, emitted by the `Observable`
    instance, created by the interval method and use the call to `subscribe()`, which
    will trigger the logic. The output should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Everything should be familiar here, except the thread that the `Observable`
    instance executes on! This thread is not the *main* one. It seems it is created
    by a RxJava-managed pool of reusable `Thread` instances, judging by its name (`RxComputationThreadPool-1`).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall, the `Observable.interval` factory method had the following overload:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that we can specify a scheduler on which it will operate. It was
    mentioned previously, that the overload with only two parameters operates on the
    *computation* scheduler. So, now let''s try passing another scheduler and see
    what''s going to happen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the same as before, but with one little difference. We pass a scheduler
    called *immediate*. The idea is to execute the work immediately on the currently
    running thread. The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: By specifying this scheduler, we made the `interval Observable` instance run
    on the current, *main* thread.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The source code for the preceding example can be found at [https://github.com/meddle0x53/learning-rxjava/blob/master/src/main/java/com/packtpub/reactive/chapter06/IntervalAndSchedulers.java](https://github.com/meddle0x53/learning-rxjava/blob/master/src/main/java/com/packtpub/reactive/chapter06/IntervalAndSchedulers.java).
  prefs: []
  type: TYPE_NORMAL
- en: With the help of the schedulers, we can instruct our operators to run on a particular
    thread or to use a particular pool of threads.
  prefs: []
  type: TYPE_NORMAL
- en: Everything we just covered leads us to the conclusion that the schedulers spawn
    new threads, or reuse already spawned ones on which the *operations*, part of
    the `Observable` instance chain, execute. Thus, we can achieve concurrency (operators
    making progress at the same time) by using only them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to have *multi-threaded* logic, we''ll have to learn just these two
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: The types of schedulers we can chose from
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use these schedulers with an arbitrary `Observable` chain of *operations*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of schedulers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several types of `schedulers` dedicated for certain kinds of actions.
    In order to learn more about them, let's take a look at the `Scheduler` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that the class is quite simple. It has only two methods, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`long now()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`abstract Worker createWorker()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first one returns the current time in milliseconds, and the second creates
    a `Worker` instance. These `Worker` instances are used for executing actions on
    a single thread or event loop (depending on the implementation). Scheduling actions
    for execution is done using the Worker's `schedule*` methods. The `Worker` class
    implements the `Subscription` interface, so it has an `unsubscribe()` method.
    *Unsubscribing* the `Worker` *unschedules* all outstanding work and allows a resource
    cleanup.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the workers to perform scheduling outside the `Observable` context.
    For every `Scheduler` type, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This will schedule the passed action and execute it. In most cases, this method
    shouldn't be used directly for scheduling work, we just pick the right scheduler
    and schedule actions on it instead. In order to understand what they do, we can
    use the method to inspect the various types of schedulers available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define a testing method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The method uses the passed `Scheduler` instance to do some work. There is an
    option to specify whether it should use the same `Worker` instance for every task,
    or spawn a new one for every sub-task. Basically, the dummy work consists of filling
    up a list with random numbers and then removing these numbers one by one. Every
    *add operation* and *remove operation* are scheduled via the worker created by
    the passed `Scheduler` instance as a sub-task. And before and after every sub-task
    the current thread and some additional information is logged.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a real-world scenario, once all the work has been done, we should always
    invoke the `worker.unsubscribe()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Turning to the predefined `Scheduler` instances. They can be retrieved via a
    set of static methods contained in the `Schedulers` class. We will be using the
    debugging method defined previously to inspect their behavior in order to learn
    their differences and usefulness.
  prefs: []
  type: TYPE_NORMAL
- en: The Schedulers.immediate scheduler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `Schedulers.immediate` scheduler executes work here and now. When an action
    is passed to its worker''s `schedule(Action0)` method, it is just called. Let''s
    suppose we run our test method with it, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In both the cases, the result will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In other words, everything is executed on the caller thread—the main one and
    nothing is in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: This scheduler can be used to execute methods, such as `interval()` and `timer()`,
    in the foreground.
  prefs: []
  type: TYPE_NORMAL
- en: The Schedulers.trampoline scheduler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The scheduler, retrieved by the `Schedulers.trampoline` method *enqueues* sub-tasks
    on the current `thread`. The enqueued work is executed after the work currently
    in progress completes. Say we were to run this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first case, the result will be the same as with the immediate scheduler,
    because all the tasks are executed in their own `Worker` instances and, therefore,
    there is only one task to be enqueued for execution in every worker. But when
    we use the same `Worker` instance for scheduling every sub-task, we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In other words, it will first execute the entire main action and after that,
    the sub-tasks; thus, the `List` instance will be filled in (the sub-tasks were
    enqueued) but never emptied. That's because, while executing the main task, the
    `List` instance was still empty and the `while` loop was not triggered.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *trampoline* scheduler is useful for avoiding a `StackOverflowError` exception
    while running many tasks recursively. For example, let's assume a task completes
    and then calls itself to perform some new work. In the case of a single-threaded
    environment, this would lead to stack overflow due to the recursion; however,
    if we use the *trampoline* scheduler, it will serialize all scheduled activities
    and the stack depth will remain normal. However, the *trampoline* scheduler is
    usually slower than the *immediate* one. So, using the correct one depends on
    the use case.
  prefs: []
  type: TYPE_NORMAL
- en: The Schedulers.newThread scheduler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This schedule creates a *new* `Thread` instance (a single-threaded `ScheduledThreadPoolExecutor`
    instance to be precise) for every new `Worker` instance. Additionally, each worker
    enqueues the actions it receives through its `schedule()` method, much like the
    trampoline scheduler does. Let''s look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: It will have the same behavior as the *trampoline* but it will run in a new
    `thread:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead, if we call the testing method like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will spawn a new `Thread` instance for every *sub-task*, which will produce
    output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: By using the *new thread* `Scheduler` instance, you can execute background tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A very important requirement here is that its workers need to be *unsubscribed*
    to avoid leaking threads and OS resources. Note that it is expensive to create
    new threads each time, so in most cases, the *computation* and the *IO* `Scheduler`
    instances should be used.
  prefs: []
  type: TYPE_NORMAL
- en: The Schedulers.computation scheduler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The computation scheduler is very similar to the *new thread* one, but it takes
    into account the number of processors/cores that the machine on which it runs
    has, and uses a thread pool that can reuse a limited number of threads. Every
    new `Worker` instance schedules sequential actions on one of these `Thread` instances.
    If the thread is not used at the moment they are executed, and if it is active,
    they are enqueued to execute on it later.
  prefs: []
  type: TYPE_NORMAL
- en: If we use the same `Worker` instance, we'll just enqueue all the actions on
    its thread and the result will be the same as scheduling with one `Worker` instance,
    using the *new thread* `Scheduler` instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'My machine has four cores. Say I call the testing method on it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'I''d get output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Everything is executed using only four `Thread` instances from a pool (note
    that there is a way to limit the number of `Thread` instances to be less than
    the available processor count).
  prefs: []
  type: TYPE_NORMAL
- en: The *computation* `Scheduler` instance is your real choice for doing background
    work—computations or processing thus its name. You can use it for everything that
    should run in the background and is not an *IO* related or blocking operation.
  prefs: []
  type: TYPE_NORMAL
- en: The Schedulers.io scheduler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Input-Output (IO) scheduler uses a `ScheduledExecutorService` instance to
    retrieve the threads from a *thread pool* for its workers. Unused threads are
    cached and reused on demand. It can spawn an arbitrary number of threads if it
    is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Again, if we run our example with only one `Worker` instance, the actions will
    be enqueued on its thread, and it will behave like the *computation* and *new
    thread* schedulers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we run it with multiple `Worker` instances, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'It would produce `Thread` instances on demand from its *pool*. The result looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The *IO* scheduler is reserved for blocking *IO operations*. Use it for requests
    to servers, reading from files and sockets, and other similar blocking tasks.
    Note that its thread pool is unbounded; if its workers are not unsubscribed, the
    pool will grow indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The source code for all the preceding code is located at [https://github.com/meddle0x53/learning-rxjava/blob/master/src/main/java/com/packtpub/reactive/chapter06/SchedulersTypes.java](https://github.com/meddle0x53/learning-rxjava/blob/master/src/main/java/com/packtpub/reactive/chapter06/SchedulersTypes.java).
  prefs: []
  type: TYPE_NORMAL
- en: The Schedulers.from(Executor) method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This can be used to create a custom `Scheduler` instance. If none of the predefined
    schedulers work for you, use this method, passing it to a `java.util.concurrent.Executor`
    instance, to implement the behavior you need.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've learned about how and when the predefined `Scheduler` instances
    should be used, is time to see how to integrate them with our `Observable` sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Combining Observables and schedulers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to execute our observable logic on other threads, we can use the schedulers.
    There are two special operators, which receive `Scheduler` as a parameter and
    produce `Observable` instances, capable of performing operations on `Thread` instances
    different from the current one.
  prefs: []
  type: TYPE_NORMAL
- en: The Observable<T> subscribeOn(Scheduler) method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `subscribeOn()` method creates an `Observable` instance, whose `subscribe`
    method causes the subscription to occur on a thread retrieved from the passed
    scheduler. For example, we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll get this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This is normal; calling the `subscribe` method executes the observable logic
    on the main thread, and only after all this is done, we see `'Hey!'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s modify the code to look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output changes to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This means that the *caller* thread doesn't block printing '`Hey!'` first or
    in between the the numbers, and all the `Observable` instance observable logic
    is executed on a *computation* thread. This way, you can use every scheduler you
    like to decide where to execute the work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we need to mention something important about the `subscribeOn()` method.
    If you call it multiple times throughout the chain like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The call to it that is *the closest* to the beginning of the chain matters.
    Here we *subscribe* on the *computation* scheduler first, then on the *IO* scheduler,
    and then on the *new thread* scheduler, but our code will be executed on the *computation*
    scheduler because this is specified *first* in the chain.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In conclusion, don't specify a scheduler in methods producing `Observable` instances;
    leave this choice to the callers of your methods. Alternatively, make your methods
    receive a `Scheduler` instance as a parameter; like the `Observable.interval`
    method, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `subscribeOn()` operator is usable with `Observable` instances that block
    the caller thread when one subscribes to them. Using the `subscribeOn()` method
    with such sources lets the caller thread progress concurrently with the `Observable`
    instance logic.
  prefs: []
  type: TYPE_NORMAL
- en: And what about the other operator, which helps us doing work on other threads?
  prefs: []
  type: TYPE_NORMAL
- en: The Observable<T> observeOn(Scheduler) operator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `observeOn()` operator is similar to the `subscribeOn()` operator, but
    instead of executing the entire chain on the passed `Scheduler` instances, it
    executes the part of the chain from its place within it, onwards. The easiest
    way to understand this is through an example. Let''s use the previous one, after
    slightly modifying it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we tell the `Observable` chain to execute on the *main* thread after
    subscribing until it reaches the `observeOn()` operator. At this point, it is
    moved on the *computation* scheduler. The output of this is something similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the part of the chain before the call to the operator blocks
    the *main* thread, preventing printing `Hey!`. However, after all the notifications
    pass through the `observeOn()` operator, `'Hey!'` is printed and the execution
    continues on the *computation* thread.
  prefs: []
  type: TYPE_NORMAL
- en: If we move the `observeOn()` operator up the `Observable` chain, a greater part
    of the logic will be executed using the *computation* scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the `observeOn()` operator can be used together with the `subscribeOn()`
    operator. That way, part of the chain could be executed on one thread and the
    rest of it on another (in most cases). This is especially useful if you code a
    client-side application because, normally, these applications run on one *event
    enqueueing* thread. You can read from files/servers using the *IO* scheduler with
    `subscribeOn()`/`observeOn()` operator and then observe the result on the *event*
    thread.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is an Android module for RxJava that is not covered by this book, but
    it is getting quite a lot of attention. You can read more about it here: [https://github.com/ReactiveX/RxJava/wiki/The-RxJava-Android-Module](https://github.com/ReactiveX/RxJava/wiki/The-RxJava-Android-Module).'
  prefs: []
  type: TYPE_NORMAL
- en: If you are an Android developer don't miss it!
  prefs: []
  type: TYPE_NORMAL
- en: There are similar modules for **Swing** and **JavaFx** as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example using both the `subscribeOn()` and `observeOn()`
    operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we use one call for the `subsribeOn()` operator at the beginning of the
    chain (actually, it doesn''t matter where we put it, because it is a sole call
    to that operator) and two calls for the `observeOn()` operator. The result of
    executing this code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the chain passes through three threads. If we do this with more
    elements, some of the code will be executed seemingly in *parallel*. The conclusion
    is that, using the `observeOn()` operator, we can change the threads multiple
    times; using the `subscribeOn()` operator, we can do this one time—*on subscription*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The source for the preceding examples with the `observeOn()`/`subscribeOn()`
    operators can be found at [https://github.com/meddle0x53/learning-rxjava/blob/master/src/main/java/com/packtpub/reactive/chapter06/SubscribeOnAndObserveOn.java](https://github.com/meddle0x53/learning-rxjava/blob/master/src/main/java/com/packtpub/reactive/chapter06/SubscribeOnAndObserveOn.java).
  prefs: []
  type: TYPE_NORMAL
- en: With these two operators, we can have `Observable` instances and *multi-threading*
    working together. But being *concurrent* doesn't really mean that we can do things
    in *parallel*. It means that our program has multiple threads, making some progress
    independently. True parallelism is when our program uses the CPU (cores) of the
    machine it runs on at their maximum and its threads run literally at the same
    time.
  prefs: []
  type: TYPE_NORMAL
- en: All of our examples up until now just moved the chain logic onto another threads.
    Although, some of the examples really did part of their operations in *parallel*,
    but a true *parallelism* example looks different.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can achieve *parallelism* only by using the operators that we already know.
    Think about the `flatMap()` operator; it creates an `Observable` instance for
    each item emitted by the source. If we call the `subscribeOn()` operator with
    a `Scheduler` instance on these `Observable` instances, each one of them will
    be *scheduled* on a new `Worker` instance, and they''ll work in *parallel* (if
    the host machine allows that). Here is an example of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We can see by the names of the threads that the `Observable` instances defined
    through the `flatMap()` operator are executed in *parallel*. And that's really
    the case—the four threads are using the four cores of my processor.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ll provide another example, this time for *parallel* requests to a remote
    server. We''ll be using the `requestJson()` method we defined in the previous
    chapter. The idea is this:'
  prefs: []
  type: TYPE_NORMAL
- en: We'll retrieve information about the followers of a GitHub user (for this example
    we'll be using my account).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For every follower, we'll get the URL to its profile.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will request the profiles of the followers in *parallel*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll print the number of the followers and the number of their followers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s see how this is implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what''s happening in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: First we perform a request to the followers data of my user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The request returns the followers as *JSON* strings, which are converted into
    `Map` objects (see the implementation of the `requestJson` method). From each
    of the *JSON* files, the URL to the profile of the follower it represents is read.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A new request is executed for each of these URLs. The requests run in *parallel*
    on *IO* threads, because we use the same technique as in the previous example.
    It is worth mentioning that the `flatMap()` operator has an overload that takes
    a `maxConcurrent` integer parameter. We can limit the concurrent requests using
    it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After user data for a follower is fetched, the information for his/her followers
    is generated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This information is printed as a side effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We count my followers using the `count()` operator (which is the same as the
    `scan(0.0, (sum, element) -> sum + 1).last()` call). Then we print them. The order
    of the printed data is not guaranteed to be the same as the order in which the
    followers were traversed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The source code for the preceding example can be found at [https://github.com/meddle0x53/learning-rxjava/blob/master/src/main/java/com/packtpub/reactive/chapter06/ParallelRequestsExample.java](https://github.com/meddle0x53/learning-rxjava/blob/master/src/main/java/com/packtpub/reactive/chapter06/ParallelRequestsExample.java).
  prefs: []
  type: TYPE_NORMAL
- en: That's all about *concurrency* and *parallelism*. Everything is pretty simple,
    but powerful. There are a few rules (such as using the `Subscribers.io` instance
    for blocking operations, using the *computation* one for background tasks, and
    so on) that you must follow to ensure nothing goes wrong, even with *multi-threaded*
    observable chains of actions.
  prefs: []
  type: TYPE_NORMAL
- en: It is very possible using this *parallelism* technique to flood the `Observable`
    instance chain with data, and that's a problem. That's why we'll have to deal
    with it. Through the rest of this chapter, we'll learn how to handle too many
    elements coming from an *upstream* observable chains of actionse.
  prefs: []
  type: TYPE_NORMAL
- en: Buffering, throttling, and debouncing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is one interesting example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This goes through all the files in a folder and reads all of them in parallel
    if they are not folders themselves. For the example, while I''m running it, there
    are five text files in the folder, and one of them is quite large. While printing
    the content of these files with our `subscribePrint()` method, we get something
    that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The output is cropped, but the important thing is that we get this `MissingBackpressureException`
    exception.
  prefs: []
  type: TYPE_NORMAL
- en: The threads reading each of the files are trying to push their data into the
    `merge()` operator (the `flatMap()` operator is implemented as `merge(map(func))`).
    The operator is struggling with a large amount of data, so it will try to notify
    the overproducing `Observable` instances to slow down (this ability to notify
    the upstream that the amount of data can't be handled is called *backpressure*).
    The problem is that they don't implement such a mechanism (*backpressure*), so
    the `MissingBackpressureException` exception is encountered.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with such a situation is achieved through implementing *backpressure*
    into the upstream observables, using one of the special `onBackpressure*` methods
    or by trying to avoid it by packaging the large amount of incoming items into
    a smaller set of emissions. This packaging is done through *buffering*, *dropping*
    some of the incoming items, *throttling* (buffering using time intervals or events),
    and *debouncing* (buffering using the intervals between emissions of items).
  prefs: []
  type: TYPE_NORMAL
- en: Let's examine some of them.
  prefs: []
  type: TYPE_NORMAL
- en: Throttling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using this mechanism, we can regulate the emission rate of an `Observable` instance.
    We can specify time intervals or another flow-controlling `Observable` instance
    to achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `sample()` operator, we can control the emissions of an `Observable`
    instance using another one, or a time interval.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The *sampling* `Observable` instance emits every 100 milliseconds for the first
    two seconds and then begins emitting every 200 milliseconds. The *data* `Observable`
    instance drops all of its items until the *sampling* emits. When this happens,
    the last item emitted by the *data* `Observable` instance is passed through. So
    we have great data loss, but it's harder to encounter the `MissingBackpressureException`
    exception (it is possible to get it, though).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sample()` operator has two additional overloads to which you can pass
    time intervals, a `TimeUnit` metric and, optionally, a `Scheduler` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Using the `sample()` operator with the `Observable` instance gives us more detailed
    control over the data flow. The `throttleLast()` operator is just an alias for
    the different versions of the `sample()` operator that receive the time interval.
    The `throttleFirst()` operator is the same as the `throttleLast()` operator, but
    the *source* `Observable` instance will emit the first item it emitted at the
    beginning of the interval, instead of the last. These operators are running on
    the *computation* scheduler by default.
  prefs: []
  type: TYPE_NORMAL
- en: These techniques are useful (as well as most of the others in this section)
    when you have multiple, similar events. For example, if you want to capture and
    react to *mouse-move events*, you don't need all the events, containing all the
    pixel positions; you need only some of them.
  prefs: []
  type: TYPE_NORMAL
- en: Debouncing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our previous example, *debouncing* won't work. Its idea is to emit only items
    that are not followed by other items for a given time interval. Therefore, some
    time must pass between emissions in order to propagate something. Because all
    of the items in our *data* `Observable` instances are emitted seemingly at once,
    there is no interval between them to use. So we need to change the example a bit
    in order to demonstrate this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Here we are using the `sample()` operator with a special *sampling* `Observable`
    instance in order to reduce the emissions to occur on 100, 200, and 150 milliseconds.
    By using the `repeat()` operator, we create an *infinite* `Observable` instance,
    repeating the source, and set it to execute on the *computation* scheduler. Now
    we can use the `debounce()` operator to emit only this set of items with time
    gaps between their emissions of 150 or more milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: '*Debouncing*, like *throttling*, can be used to filter similar events from
    an over-producing source. A good example of this is an auto-complete search. We
    don''t want to trigger searches on every letter inputted by the user; we need
    to wait for him/her to stop typing and then trigger the search. We can use the
    `debounce()` operator for that and set a reasonable *time interval*. The `debounce()`
    operator has an overload that takes a `Scheduler` instance as its third argument.
    Additionally, there is one more overload with a selector returning an `Observable`
    instance for more fine-grained control over the *data flow*.'
  prefs: []
  type: TYPE_NORMAL
- en: The buffer and window operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These two sets of operators are *transforming* operators much like the `map()`
    or `flatMap()` operators. They *transform* a series of elements in a collection—a
    sequence of these elements to be emitted as one.
  prefs: []
  type: TYPE_NORMAL
- en: 'This book will not cover these operators in detail, but it''s worth mentioning
    that the `buffer()` operator has overloads that are able to collect emissions
    based on *time intervals*, *selectors*, and other `Observable` instances. It can
    be configured to skip items too. Here is an example with the `buffer(int count,
    int skip)` method, a version of the `buffer()` operator that collects *count*
    items and skips *skip* items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output something similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The `window()` operator has exactly the same set of overloads as the `buffer()`
    operator. The difference is that instead of arrays of the buffered elements, the
    `Observable` instance created by the `window()` operator emits `Observable` instances
    emitting the collected elements.
  prefs: []
  type: TYPE_NORMAL
- en: In order to demonstrate a different overload, we'll present an example using
    the `window(long timespan, long timeshift, TimeUnit units)` method. This operator
    collects elements emitted within the *timespan* interval and skips all the elements
    emitted within the *timeshift* interval. This is repeated until the source `Observable`
    instance is complete.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We use the `flatMap()` operator to flatten the `Observable` instances. The result
    consists of all the items emitted in the first three milliseconds of the *subscription*,
    plus the ones emitted for three milliseconds after a 200-millisecond gap, and
    this is repeated while the source is emitting.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All the examples introduced in the preceding section can be found at [https://github.com/meddle0x53/learning-rxjava/blob/master/src/main/java/com/packtpub/reactive/chapter06/BackpressureExamples.java](https://github.com/meddle0x53/learning-rxjava/blob/master/src/main/java/com/packtpub/reactive/chapter06/BackpressureExamples.java).
  prefs: []
  type: TYPE_NORMAL
- en: The backpressure operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last set of operators preventing the `MissingBackpressureException` exception
    actually activate automatically when there is an overproducing *source* `Observable`
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `onBackpressureBuffer()` operator buffers the items emitted by the faster
    than its `Observer` instance''s *source* `Observable`. The buffered items are
    then emitted in a way that the subscribers can handle them. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Here we used a big capacity for the buffer because of the large number of elements,
    but note that overflowing this buffer will get the `MissingBackpressureException`
    exception back.
  prefs: []
  type: TYPE_NORMAL
- en: The `onBackpressureDrop()` operator drops all the incoming items from the *source*
    `Observable` instance that can not be handled by the subscribers.
  prefs: []
  type: TYPE_NORMAL
- en: There is a way to establish *backpressure* by implementing smart Observables
    or Subscribers, but this topic is beyond the scope of this book. There is an excellent
    article about *backpressure* and observables on the RxJava wiki page—[https://github.com/ReactiveX/RxJava/wiki/Backpressure](https://github.com/ReactiveX/RxJava/wiki/Backpressure).
    Many of the operators mentioned in this section are described there in depth,
    and there are marble diagrams available to help you understand the more complex
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've learned how to execute our observable logic on other
    threads that are different from the *main* one. There are some simple rules and
    techniques for doing this, and if everything is followed accordingly, there should
    be no dangers. Using these techniques, we are able to write *concurrent* programs.
    We've also learned how to achieve *parallel* execution using the schedulers and
    the `flatMap()` operator, and we saw a real-world example of doing that.
  prefs: []
  type: TYPE_NORMAL
- en: Another useful thing that we've examined was how to handle *overproducing* sources
    of data. There are a lot of operators that are able to do that by different means,
    and we introduced some of them, and talked about their usefulness.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have the knowledge to write arbitrary RxJava programs capable
    of working with data from different sources. We know how to do this using multiple
    threads. Using RxJava, its operators, and *constructions* is almost like coding
    using a new language. It has its rules and flow control methods.
  prefs: []
  type: TYPE_NORMAL
- en: In order to write stable applications, we'll have to learn how to *unit test*
    them. Testing *asynchronous* code is not an easy task. The good news is that there
    are some operators and classes provided by RxJava that are going to help us do
    that. You can read more about them in the next chapter.
  prefs: []
  type: TYPE_NORMAL
