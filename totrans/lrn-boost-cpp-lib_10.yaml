- en: Chapter 10. Concurrency with Boost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Threads** represent concurrent streams of execution within a process. They
    are a low-level abstraction for **concurrency** and are exposed by the system
    programming libraries or system call interfaces of operating systems, for example,
    POSIX threads, Win32 Threads. On multiprocessor or multicore systems, operating
    systems can schedule two threads from the same process to run in parallel on two
    different cores, thus achieving true **parallelism**.'
  prefs: []
  type: TYPE_NORMAL
- en: Threads are a popular mechanism to abstract concurrent tasks that can potentially
    run in parallel with other such tasks. Done right, threads can simplify program
    structure and improve performance. However, concurrency and parallelism introduce
    complexities and nondeterministic behavior unseen in single-threaded programs,
    and doing it right can often be the biggest challenge when it comes to threads.
    A wide variance in the native multithreading libraries or interfaces across operating
    systems makes the tasks of writing portable concurrent software using threads
    even more difficult. The Boost Thread library eases this problem by providing
    a portable interface to create threads and higher level abstractions for concurrent
    tasks. The **Boost Coroutine** library provides a mechanism to create cooperative
    *coroutines* or functions which can be exited and resumed, retaining states of
    automatic objects between such calls. Coroutines can express event-driven logic
    in a simpler way, and avoid the overhead of threads in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is a hands-on introduction to using the Boost Thread library and
    also features a short account of the Boost Coroutine library. It is divided into
    the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating concurrent tasks with Boost Thread
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency, signaling, and synchronization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boost Coroutines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if you have never written multithreaded programs or concurrent software,
    this would be a good starting point. We will also touch upon the thread library
    in the C++11 Standard Library, which is based on the Boost Thread library and
    introduces additional refinements.
  prefs: []
  type: TYPE_NORMAL
- en: Creating concurrent tasks with Boost Thread
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider a program that prints greetings in different languages. There is one
    list of greetings in Anglo-Saxon languages, such as English, German, Dutch, Danish,
    and so on. There is a second list of greetings in Romance languages, such as Italian,
    Spanish, French, Portuguese, and so on. Greetings from both language groups need
    to be printed, and we do not want to delay printing the greetings from one group
    because of the other, that is, we want to print greetings from both the groups
    *concurrently*. Here is one way to print both the groups of greetings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.1: Interleaved tasks**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, we have two vectors of greetings, and printing the
    greetings in each is an independent task. We interleave these two tasks by printing
    one greeting from each array, and thus the two tasks progress concurrently. From
    the code, we can tell that a Latin and an Anglo-Saxon greeting will be printed
    alternately in the exact order as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: While the two tasks were run interleaved, and in that sense concurrently, the
    distinction between them in code was totally muddled to the extent that they were
    coded in a single function. By separating them into separate functions and running
    them in separate threads, the tasks can be totally decoupled from each other yet
    be run concurrently. In addition, threads would allow for their parallel execution.
  prefs: []
  type: TYPE_NORMAL
- en: Using Boost Threads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Every running process has at least one thread of execution. A traditional "hello
    world" program with a `main` function also has a single thread, often called the
    **main thread**. Such programs are called **single-threaded**. Using Boost Threads,
    we can create programs with multiple threads of execution that run concurrent
    tasks. We can rewrite the listing 10.1 using Boost Threads so that the code for
    an individual task is cleanly factored out, and the tasks potentially run in parallel
    when parallel hardware is available. Here is how we can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.2: Concurrent tasks as threads**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We define a function `printGreets` that takes a vector of greetings and prints
    all the greetings in the vector (lines 8-13). This is the code for the task, simplified
    and factored out. This function is invoked once each on the two greeting vectors.
    It is called once from the `main` function, which executes in the main thread
    (line 17), and once from a second thread of execution that we spawn by instantiating
    an object of type `boost::thread`, passing it the function to invoke and its arguments
    (line 16). The header file `boost/thread.hpp` provides types and functions needed
    for using Boost Threads (line 1).
  prefs: []
  type: TYPE_NORMAL
- en: The object `t1` of type `boost::thread` wraps a native thread, for example,
    `pthread_t`, Win32 thread `HANDLE`, and so on. For conciseness, we simply refer
    to "the thread `t1`" to mean the underlying thread as well as the `boost::thread`
    object wrapping it, unless it is necessary to distinguish between the two. The
    object `t1` is constructed by passing a function object (the initial function
    of the thread) and all the arguments to pass to the function object (line 16).
    Upon construction, the underlying native thread starts running immediately by
    calling the passed function with the arguments provided. The thread terminates
    when this function returns. This happens concurrently with the `printGreets` function
    called from the `main` function (line 17).
  prefs: []
  type: TYPE_NORMAL
- en: 'One possible output from this program is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The Latin greets are printed in the order they appear in the `romance` vector,
    and the Anglo-Saxon greets are printed in the order they appear in the `angloSaxon`
    vector. But there is no predictable order in which they are interleaved. This
    lack of determinism is a key feature in concurrent programming and a source of
    some of the difficulty. What is possibly more unnerving is that even the following
    output is possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the two greets `Buenos dias!` (Spanish) and `Godmorgen!` (Dutch)
    are interleaved, and `Good morning!` was printed before the new line following
    `Bom dia!` could be printed.
  prefs: []
  type: TYPE_NORMAL
- en: We call the `join` member function on `t1` to wait for the underlying thread
    to terminate (line 18). Since the main thread and the thread `t1` run concurrently,
    either can terminate before the other. If the `main` function terminated first,
    it would terminate the program and the `printGreets` function running in the thread
    `t1` would be terminated before it finished execution. By calling `join`, the
    main function ensures that it does not exit while `t1` is still running.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Linking against Boost Thread Library**'
  prefs: []
  type: TYPE_NORMAL
- en: Boost Thread is not a header-only library but has to be built from the sources.
    [Chapter 1](ch01.html "Chapter 1. Introducing Boost"), *Introducing Boost*, describes
    the details of building the Boost libraries from their sources, their **name layout
    variants**, and naming conventions.
  prefs: []
  type: TYPE_NORMAL
- en: To build a running program from listing 10.2, you need to link your compiled
    objects with these libraries. To build the preceding example, you must link with
    Boost Thread and Boost System libraries. On Linux, you must also link against
    `libpthread`, which contains the Pthreads library implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming the source file is `Listing9_2.cpp`, here is the g++ command line
    on Linux to compile and link the source to build a binary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Linking to `libboost_chrono` is necessary only if we use the Boost Chrono library.
    The option `-pthread` sets the necessary preprocessor and linker flags to enable
    compiling a multithreaded application and linking it against `libpthread`. If
    you did not use your native package manager to install Boost on Linux, or if you
    are trying to build on another platform, such as Windows, then refer to the detailed
    build instructions in [Chapter 1](ch01.html "Chapter 1. Introducing Boost"), *Introducing
    Boost*.
  prefs: []
  type: TYPE_NORMAL
- en: If you are on C++11, you can use the Standard Library threads instead of Boost
    Threads. For this, you have to include the Standard Library header `thread`, and
    use `std::thread` instead of `boost::thread`. Boost Thread and `std::thread` are
    not drop-in replacements of each other, and therefore some changes would be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Moving threads and waiting on threads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An object of `std::thread` is associated with and manages exactly one thread
    in a process. Consider the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: When the `boost::thread` object `thr` is created (line 4), it gets associated
    with a new native thread (`pthread_t`, handle to a Windows thread, and so on),
    which executes the function pointed to by `thrFunc`. Now `boost::thread` is a
    movable but not a copyable type. When the `makeThread` function returns `thr`
    by value (line 7), the ownership of the underlying native thread handle is moved
    from the object `thr` in `makeThread` to `thr1` in the `main` function (line 11).
    Thus you can create a thread in one function and return it to the calling function,
    *transferring ownership* in the process.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately though, we wait for the thread to finish execution inside the `main`
    function by calling `join` (line 13). This ensures that the `main` function does
    not exit until the thread `thr1` terminates. Now it is entirely possible that
    by the time `makeThread` returned `thr`, the underlying thread had already completed
    execution. In this case, `thr1.join()` (line 13) returns immediately. On the other
    hand, the underlying thread could well continue to execute while the control on
    the main thread is transferred to the `main` function, and even as `join` was
    called on `thr1` (line 13). In this case, `thr1.join()` would block, waiting for
    the thread to exit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, we may want a thread to run its course and exit, and we would never
    bother to check on it again. Moreover, it may not matter whether the thread terminated
    or not. Imagine a personal finance desktop application that features a nifty stock
    ticker thread that keeps displaying stock prices of a configurable set of companies
    in one corner of the window. It is started by the main application and keeps doing
    its job of fetching the latest prices of stocks and showing them until the application
    exits. There is little point for the main thread to wait on this thread before
    exiting. When the application terminates, the stock ticker thread is also terminated
    and cleaned up in its wake. We can explicitly request this behavior for a thread
    by calling `detach` on the `boost::thread` object, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: When we call `detach` on a `boost::thread` object, the ownership of the underlying
    native thread is passed to the C++ runtime, which continues to execute the thread
    until either the thread terminates or the program terminates, killing the thread.
    After the call to `detach`, the `boost::thread` object no longer refers to a valid
    thread, and the program can no longer check the status of the thread or interact
    with it in any way.
  prefs: []
  type: TYPE_NORMAL
- en: A thread is said to be joinable if and only if neither `detach` nor `join` has
    been called on the `boost::thread` object. The `joinable` method on the `boost::thread`
    returns `true` if and only if the thread is joinable. If you call `detach` or
    `join` on a `boost::thread` object that is not joinable, the calls return immediately
    with no other effect. If we do not call `join` on a `boost::thread` object, then
    `detach` is called in its destructor, when the thread goes out of scope.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Differences between `boost::thread` and `std::thread`
  prefs: []
  type: TYPE_NORMAL
- en: You must call either `join` or `detach` on a `std::thread` object; otherwise,
    the destructor of `std::thread` calls `std::terminate` and aborts the program.
    Moreover, calling `join` or `detach` on a `std::thread` that is not joinable will
    result in a `std::system_error` exception being thrown. Thus you call any one
    of `join` and `detach` on `std::thread`, and you do so once and only once. This
    is in contrast to the behavior of `boost::thread` we just described.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get `boost::thread` to emulate this behavior of `std::thread` by defining
    the following preprocessor macros, and it is a good idea to emulate the behavior
    of `std::thread` in any new code that you write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Thread IDs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At any time, each running thread in a process has a unique identifier. This
    identifier is represented by the type `boost::thread::id` and can be obtained
    from a `boost::thread` object by calling the `get_id` method. To get the ID of
    the current thread, we must use `boost::this_thread::get_id()`. A string representation
    of the ID can be printed to an `ostream` object, using an overloaded insertion
    operator (`operator<<`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thread IDs can be ordered using an `operator<` so they can easily be stored
    in ordered associative containers (`std::set` / `std::map`). Thread IDs can be
    compared using an `operator==` and can be stored in unordered associative containers
    too (`std::unordered_set` / `std::unordered_map`). Storing threads in associative
    containers indexed by their IDs is an effective means of supporting lookups on
    threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.3: Using thread IDs**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we create five threads and each runs the function
    `doStuff`. The function `doStuff` is passed an assigned name of the thread it
    runs; we name the threads `thread1` through `thread5`, and put them in a `std::map`
    indexed by their IDs (lines 26). Because `boost::thread` is movable but not copyable,
    we move the thread objects into the map. The `doStuff` function simply prints
    the ID of the current thread using the method `boost::this_thread::get_id` (line
    12), as part of some diagnostic message, and then sleeps for 2 seconds using `boost::this_thread::sleep_for`,
    which is passed a duration of type `boost::chrono::duration` (see [Chapter 8](ch08.html
    "Chapter 8. Date and Time Libraries"), *Date and Time Libraries*). We can also
    use duration types provided by Boost Date Time, that is, `boost::posix_time::time_duration`
    and its subtypes, instead of `boost::chrono`, but for that we would need to use
    the `boost::this_thread::sleep` function rather than `sleep_for`.
  prefs: []
  type: TYPE_NORMAL
- en: Cores and threads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many modern computers have multiple CPU cores on a single die and there might
    be multiple dice in a processor package. To get the number of physical cores on
    the computer, you can use the static function `boost::thread::physical_concurrency`.
  prefs: []
  type: TYPE_NORMAL
- en: Modern Intel CPUs support Intel's HyperThreading technology, which maximizes
    utilization of a single core by using two sets of registers allowing two threads
    to be multiplexed on the core at any given point and reducing the costs of context
    switching. On an Intel system with eight cores and supporting HyperThreading,
    the maximum number of threads that can be scheduled to run in parallel at any
    given time is then 8x2 = 16\. The static function `boost::thread::hardware_concurrency`
    returns this number for the local machine.
  prefs: []
  type: TYPE_NORMAL
- en: These numbers are useful in deciding the optimal number of threads in your program.
    However, it is possible for these functions to return 0 if the numbers are not
    available from the underlying system. You should test these functions thoroughly
    on each platform where you plan to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Managing shared data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All threads in a process have access to the same global memory, so the results
    of computations performed in one thread are relatively easy to share with other
    threads. Concurrent read-only operations on shared memory do not require any coordination,
    but any write to shared memory requires synchronization with any read or write.
    Threads that share *mutable data* and other resources need mechanisms to *arbitrate
    access* to shared data and signal each other about events and state changes. In
    this section, we explore the mechanisms for coordination between multiple threads.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and coordinating concurrent tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider a program that generates the difference between two text files à la
    the Unix `diff` utility. You need to read two files, and then apply an algorithm
    to identify the parts that are identical and the parts that have changed. For
    most text files, reading both the files and then applying a suitable algorithm
    (based on the Longest Common Subsequence problem) works perfectly well. The algorithm
    itself is beyond the scope of this book and not germane to the present discussion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the tasks we need to perform:'
  prefs: []
  type: TYPE_NORMAL
- en: 'R1: Read complete contents of the first file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'R2: Read complete contents of the second file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'D: Apply the diff algorithm to the contents of the two files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The tasks R1 and R2 conceivably produce two arrays of characters containing
    the file content. The task D consumes the content produced by R1 and R2 and produces
    the diff as another array of characters. There is no ordering required between
    R1 and R2, and we can read the two files concurrently in separate threads. For
    simplicity, D commences only once both R1 and R2 are complete, that is, both R1
    and R2 must happen before D. Let us start by writing the code to read a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.4a: Reading contents of a file**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Given a file name, the function `readFromFile` reads the contents of the entire
    file and returns it in a `vector<char>`. We read the file contents into the underlying
    array of the `vector`, to get at which we call the `data` member function introduced
    in C++11 (line 11). We open the file for reading (line 8), and obtain the size
    of the file using the `boost::filesystem::size` function (line 9). We also define
    a stub of a method `diffContent` to compute the diff between the contents of two
    files.
  prefs: []
  type: TYPE_NORMAL
- en: How can we employ the `readFromFile` function to read a file in a separate thread
    and return the vector containing the contents of the file to the calling thread?
    The calling thread needs a way to wait for the read to complete in the reader
    thread, and then get at the content read. In other words, the calling thread needs
    to wait for the future result of an asynchronous operation. The `boost::future`
    template provides an easy way to enforce such ordering between tasks.
  prefs: []
  type: TYPE_NORMAL
- en: boost::future and boost::promise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `boost::future<>` template is used to represent the result of a computation
    that potentially happens in the future. An object of type `boost::future<T>` represents
    a proxy for an object of type `T` that will potentially be produced in the future.
    Loosely speaking, `boost::future` enables a calling code to wait or block for
    an event to happen—the event of producing a value of a certain type. This mechanism
    can be used to signal events and pass values from one thread to another.
  prefs: []
  type: TYPE_NORMAL
- en: 'The producer of the value or the source of the event needs a way to communicate
    with the future object in the calling thread. For this, an object of type `boost::promise<T>`,
    associated with the future object in the calling thread, is used to signal events
    and send values. Thus `boost::future` and `boost::promise` objects work in pairs
    to signal events and pass values across threads. We will now see how we can guarantee
    that the two file read operations in two threads precede the diff operation using
    Boost futures and promises:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.4b: Returning values from a thread using futures and promises**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To be able to use `boost::future` and `boost::promise`, we need to include `boost/thread/future.hpp`
    (line 3). If we did not define the preprocessor symbol `BOOST_THREAD_PROVIDES_FUTURE`
    (line 1), then we would need to use `boost::unique_future` instead of `boost::future`.
    This example would work unchanged if we replaced `boost::future` with `boost::unique_future`,
    but in general there are differences in the capabilities of the two facilities,
    and we stick to `boost::future` throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: The function `diffFiles` (lines 6 and 7) takes two file names and returns their
    diff. It reads the first file synchronously (line 23) using the `readFromFile`
    function in listing 10.4a, and creates a thread called `reader` to read the second
    file concurrently (line 13). In order to be notified, when the `reader` thread
    is done reading and gets the content read, we need to set up a future-promise
    pair. Since we want to return a value of type `std::vector<char>` from the `reader`
    thread, we define a promise called `promised_value` of type `boost::promise<std::vector<char>>`
    (line 9). The `get_future` member of the promise object returns the associated
    future object and is used to move-construct `future_result` (lines 10-11). This
    sets up `promised_value` and `future_result` as the promise-future pair we work
    with.
  prefs: []
  type: TYPE_NORMAL
- en: To read contents of `file2`, we create the `reader` thread passing a lambda
    (lines 14-20). The lambda captures `promised_value` and the name of the file to
    read (line 14). It reads the contents of the file and calls `set_value` on the
    promise object, passing in the content read (line 17). It then prints a diagnostic
    message and returns. Concurrently, with this, the calling thread also reads in
    the other file `file1` into the buffer `content1` and then calls `get` on `future_result`
    (line 26). This call blocks until the associated promise is set via the call to
    `set_value` (line 17). It returns the `vector<char>` set in the promise and this
    is used to move-construct `content2`. If the promise was already set, when `get`
    is called on the future, it returns the value without blocking the calling thread.
  prefs: []
  type: TYPE_NORMAL
- en: We now have the data needed to compute the diff, and we do so by passing the
    buffers `content1` and `content2` to the `diffContent` function (line 27). Note
    that we call `join` on the `reader` thread before returning `diff` (line 28).
    This would be necessary only if we wanted to ensure that the `reader` thread exited
    before returning from the function. We could also call `detach` instead of `join`
    to not wait for the reader thread to exit.
  prefs: []
  type: TYPE_NORMAL
- en: Waiting for future
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `get` member function of `boost::future<>` blocks the calling thread until
    the associated promise is set. It returns the value set in the promise. Sometimes,
    you might want to block for a short duration and go ahead if the promise is not
    set. To do this, you have to use the `wait_for` member function and specify the
    duration to wait using `boost::chrono::duration` (see [Chapter 8](ch08.html "Chapter 8. Date
    and Time Libraries"), *Date and Time Libraries*):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.5: Waiting and timing out on a future**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This example demonstrates how we can wait for a fixed duration on a future object.
    We create a promise-future pair (lines 11-12), but the template argument for `boost::future<>`
    and `boost::promise<>` is void. This means that we can use this pair purely for
    signaling/waiting, but not for transferring any data across threads.
  prefs: []
  type: TYPE_NORMAL
- en: We create a thread `thr` (line 16) passing it a lambda, which captures the promise
    object. This thread simply sleeps for a random duration between 10 and 19 seconds
    by passing a random duration to `boost::this_thread::sleep_for` (line 22) and
    then exits. The duration is constructed using the `boost::chrono::seconds` function
    (line 23) and passed a random interval `secs` computed using the `rand` function
    (line 18). We use `rand` for brevity, although more reliable and robust facilities
    are available in Boost and C++11\. To use `rand`, we need to call `srand` to seed
    the random number generator. On Windows, we must call `srand` in each thread that
    calls `rand` as we have shown here (line 17), while on POSIX, we should call `srand`
    once per process, which could be at the start of `main`.
  prefs: []
  type: TYPE_NORMAL
- en: After sleeping for a specific duration, the thread `thr` calls `set_value` on
    the promise and returns (line 24). Since the promise is of type `boost::promise<void>`,
    `set_value` does not take any parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In the main thread, we run a loop calling `wait_for` on the future associated
    with `promise`, passing a duration of 2 seconds each time (line 30). The function
    `wait_for` returns a value of the enum type `boost::future_state`. Each time `wait_for`
    times out, it returns `boost::future_state::timeout`. Once the promise is set
    (line 24), the `wait_for` call returns `boost::future_state::ready` and the loop
    breaks. The `is_ready` member function of `boost::future` returns `true` (line
    35), and the future's state as returned by the `get_state` member function is
    `boost::future_state::ready` (line 36).
  prefs: []
  type: TYPE_NORMAL
- en: Throwing exceptions across threads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If the initial function passed to the `boost::thread` constructor allows any
    exceptions to propagate, then the program is immediately aborted by a call to
    `std::terminate`. This creates a problem if we need to throw an exception from
    one thread to indicate a problem to another thread, or propagate an exception
    we caught in one thread to another. The promise/future mechanism comes in handy
    for this purpose too. Consider how, in Listing 10.4a and 10.4b, you would handle
    the case when a file does not exist or is not readable:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.6: Transporting exceptions across threads**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If `file2` is the name of a file that does not exist or is not readable (line
    25), then the function `readFromFile` throws an exception (line 10) that is caught
    by the `reader` thread (line 27). The `reader` thread sets the exception in the
    promise object by using the `set_exception` member function (lines 28-29). Notice
    that we create a copy of the exception object using `boost::copy_exception` and
    set it in the promise object (line 29). Once an exception is set in the promise,
    the call to `get` on the future object (line 35) throws that exception, which
    needs to be caught and handled (line 38).
  prefs: []
  type: TYPE_NORMAL
- en: shared_future
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `boost::future` object can only be waited upon by one thread. It is not
    copyable but is movable; thus, its ownership can be transferred from one thread
    to another and one function to another, but never shared. If we want multiple
    threads to wait on the same condition using the future mechanism, we need to use
    `boost::shared_future`. In the following example, we create a publisher thread
    that waits for a fixed duration before setting a promise with its thread ID. We
    also create three subscriber threads, which poll a `boost::shared_future` object
    associated with the promise object at different periodicities until it is ready,
    and then retrieves the thread ID of the publisher object from the `shared_future`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.7: Using shared_future**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Following the familiar pattern, we create a promise (line 11) and a `boost::future`
    (line 12). Using the future object, we move-initialize a `shared_future` object
    `shfut` (line 13). The `publisher` thread captures the promise (line 14) and sleeps
    for 15 seconds (line 21) before setting its ID string into the promise (line 22).
  prefs: []
  type: TYPE_NORMAL
- en: For the subscriber threads, we store the function object generated by the lambda
    expression in a variable called `thrFunc` (line 24) so that it can be reused multiple
    times. The initial function for the subscriber thread takes a `shared_future`
    parameter by value, and also the `waitFor` parameter, which specifies the frequency
    of polling the `shared_future` in seconds. The subscriber spins in a loop calling
    `wait_for` on the shared future, timing out after `waitFor` seconds. It comes
    out of the loop once the promise is set (line 22) and retrieves the value set
    in the promise (the publisher's thread ID) by calling `get` on the `shared_future`
    (line 35).
  prefs: []
  type: TYPE_NORMAL
- en: Three subscriber threads are spawned (lines 38-40). Note how the arguments to
    their initial function, the `shared_future` object, and the wait period in seconds,
    are passed as additional arguments to `boost::thread` object's variadic constructor
    template. Note that `shared_future` is copyable and the same `shared_future` object
    `shfut` is copied into the three subscriber threads.
  prefs: []
  type: TYPE_NORMAL
- en: std::future and std::promise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The C++11 Standard Library provides `std::future<>`, `std::shared_future<>`,
    and `std::promise<>` templates that are pretty much identical in behavior to their
    Boost library counterparts. The Boost version''s additional member functions are
    experimental, but leaving those aside, they mirror their Standard Library counterparts.
    For example, we can rewrite listing 10.5 and 10.7 by replacing the following symbols
    in the program text:'
  prefs: []
  type: TYPE_NORMAL
- en: Replace `boost::thread` with `std::thread`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace `boost::future` with `std::future`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace `boost::promise` with `std::promise`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace `boost::shared_promise` with `std::shared_promise`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace `boost::chrono` with `std::chrono`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, we would need to replace the included headers `boost/thread.hpp`,
    `boost/thread/future.hpp`, and `boost/chrono.hpp` with the Standard Library headers
    `thread`, `future`, and `chrono` respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In listing 10.6, we used the `set_exception` member function of `boost::promise`
    to enable passing an exception across thread boundaries. This would require some
    changes to work with `std::promise`. C++11 introduces `std::exception_ptr`, a
    special smart pointer type with shared ownership semantics that must wrap exception
    objects so that they can be passed across functions and threads (see [Appendix](apa.html
    "Appendix A. C++11 Language Features Emulation"), *C++11 Language Features Emulation*).
    The `set_exception` member function of `std::promise` takes a parameter of type
    `std::exception_ptr` instead of a `std::exception`. The following snippet shows
    how you would change listing 10.6 to use the Standard Library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, we call `std::current_exception` (line 29), which returns a `std::exception_ptr`
    object that wraps the currently active exception in the catch block. This `exception_ptr`
    is passed to the `set_exception` member function of `std::promise` (line 28).
    These type and function declarations are available from the Standard Library header
    `exception` (line 2).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also create a `std::exception_ptr` object from an exception object using
    `std::make_exception_ptr`, as shown in the following snippet (line 29):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: std::packaged_task and std::async
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While threads are powerful constructs, the full generality and control that
    they provide comes at the cost of simplicity. In a lot of cases, it works best
    to operate at a higher level of abstraction than creating explicit threads to
    run tasks. The Standard Library provides the `std::async` function template and
    `std::packaged_task` class template that provide different levels of abstractions
    for creating concurrent tasks, freeing the programmer from having to write a lot
    of boilerplate code in the process. They have counterparts in the Boost library
    (`boost::async` and `boost::packaged_task`) that are incompletely implemented
    and less portable to use as of this writing (Boost version 1.57), especially in
    pre-C++11 environments.
  prefs: []
  type: TYPE_NORMAL
- en: std::packaged_task
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `std::packaged_task<>` class template is used to create asynchronous tasks.
    You need to explicitly create a thread that runs the task or calls the task manually
    using the overloaded `operator()` in `packaged_task`. But you do not need to manually
    set up promise-future pairs or deal with promises in any way. Here is listing
    10.6, rewritten using `std::packaged_task`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.8: Using std::packaged_task**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we read two files and compute their diff. To read the files,
    we use the function `readFromFile`, which returns the file contents in a `vector<char>`
    or throws an exception if the file is not readable. We read one of the two files
    by a blocking call to `readFromFile` (line 25), and read the other file on a separate
    thread.
  prefs: []
  type: TYPE_NORMAL
- en: To read the second file concurrently with the first one, we wrap the `readFromFile`
    function in a `std::packaged_task` called `readerTask` (lines 19-20) and run it
    in a separate thread. The specific type of `readerTask` is `std::packaged_task<buffer_t(const
    std::string&)>`. The template argument to `packaged_task` is the wrapped function
    type. Before starting this task on a separate thread, we must first get a reference
    to the associated future object. We get this reference to the future object by
    calling the `get_future` member function of `packaged_task` (line 21). Next, we
    create a thread and move the packaged task to this thread (line 24). This is necessary
    because `packaged_task` is movable but not copyable, which is why the `get_future`
    method must be called on the `packaged_task` object before it is moved.
  prefs: []
  type: TYPE_NORMAL
- en: The thread `thread2` reads `file2` by calling the `readFromFile` function passed
    to it in a `packaged_task`. The `vector<char>` returned by `readFromFile` can
    be obtained from the future object associated with `readerTask` by a call to the
    `get` member function of the future (line 28). The `get` call will throw any exception
    originally thrown by `readFromFile`, such as when the named file does not exist.
  prefs: []
  type: TYPE_NORMAL
- en: std::async
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `std::async` function template creates a task from a function object that
    can potentially run concurrently in a separate thread. It returns a `std::future`
    object, which can be used to block on the task or wait for it. It is available
    through the Standard Library header file `future`. With `std::async`, we no longer
    need to explicitly create threads. Instead, we pass to `std::async` the function
    to execute, the arguments to pass, and an optional launch policy. `std::async`
    runs the function either asynchronously in a different thread or synchronously
    on the calling thread based on the launch policy specified. Here is a simple rewrite
    of listing 10.5 using `std::async`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.9: Using std::async to create concurrent tasks**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: While `packaged_task` abstracts promises, `std::async` abstracts threads themselves,
    and we no longer deal with objects of `std::thread`. Instead, we call `std::async`,
    passing it a launch policy `std::launch::async` (line 16), a function object (line
    17), and any number of arguments that the function object takes. It returns a
    future object and runs the function passed to it asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: Like the constructor of `thread`, `std::async` is a variadic function and is
    passed all the arguments that need to be forwarded to the function object. The
    function object is created using a lambda expression and does little, besides
    sleeping for a duration passed to it as a parameter. The `duration` is a random
    value between 10 and 19 seconds and is passed to the `async` call as the sole
    argument for the function object (line 24). The function object returns the duration
    of sleep (line 23). We call the `wait_for` member function on the future object
    to wait for short periods till the future is set (line 28). We retrieve the return
    value of the task from the future object by calling its `get` member function
    (line 34).
  prefs: []
  type: TYPE_NORMAL
- en: Launch policy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We used the launch policy `std::launch::async` to indicate that we want the
    task to run on a separate thread. This would launch the task immediately in a
    separate thread. Using the other standard launch policy `std::launch::deferred`
    , we can launch the task lazily, when we first call `get` or `wait` (non-timed
    wait functions) on the associated future object. The task would run synchronously
    in the thread that calls `get` or `wait`. This also means that the task would
    never be launched if one used the `deferred` policy and did not call `get` or
    `wait`.
  prefs: []
  type: TYPE_NORMAL
- en: We could not have used `std::launch::deferred` in the listing 10.10\. This is
    because we wait for the future to be ready (line 28) before calling `get` in the
    same thread (line 34). The task would never be launched until we called `get`,
    but the future could never be ready unless the task was launched and returned
    a value; so we would spin eternally in the `while` loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'While creating a task using `std::async`, we may also omit the launch policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In such cases, the behavior is equivalent to the following call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: It is up to the implementation to choose the behavior conforming to either `std::launch::async`
    or `std::launch::deferred`. Moreover, the implementation would only create a new
    thread if the runtime libraries needed to support multithreading are linked to
    the program. With the default policy, when multithreading is enabled, `std::async`
    either launches new tasks in new threads or posts them to an internal thread pool.
    If there are no free threads in the pool or free cores, the tasks would be launched
    synchronously.
  prefs: []
  type: TYPE_NORMAL
- en: Lock-based thread synchronization methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we saw how we can delegate functions to be run on separate threads using
    `boost::thread` and `std::thread`. We saw the use of `boost::future` and `boost::promise`
    to communicate results and exceptions between threads and to impose order between
    tasks through blocking calls. Sometimes, you can break down your program into
    independent tasks that can be run concurrently, producing a value, a side effect,
    or both, which is then consumed by another part of the program. Launching such
    tasks and waiting on them using futures is an effective strategy. Once the tasks
    have returned, you can start on the next phase of computations that consume the
    results of the first phase.
  prefs: []
  type: TYPE_NORMAL
- en: Often though, multiple threads need to access and modify the same data structures
    concurrently and repeatedly. These accesses need to be ordered reliably and isolated
    from each other to prevent inconsistencies from creeping into the underlying data
    structure due to uncoordinated, concurrent accesses. In this section, we look
    at the Boost libraries that help us take care of these concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Data races and atomic operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider the following code snippet. We create two threads, and each thread
    increments a shared integer variable a fixed number of times in a loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: What value of `x` would be printed at the end of the program? Since each thread
    increments `x` a million times and there are two threads, one could expect it
    to be `2000000`. You can verify for yourself that the increment operator is called
    on `x` no less and no more than `N*max` times, where `N=2` is the number of threads
    and `max` is a million. Yet I saw `2000000` being printed not for once; each time
    it was a smaller number. This behavior might vary depending on the OS and hardware,
    but it is common enough. Clearly, some increments are not taking effect.
  prefs: []
  type: TYPE_NORMAL
- en: The reason becomes clear when you realize that the operation `++x` involves
    reading the value of `x`, adding one to the value, and writing this result back
    into `x`. Say the value of `x` is `V` and two threads perform the operation `++x`
    on `V`. Each of the two threads can read V as the value of `x`, perform the increment,
    and write back V+1\. Therefore, after two threads, each incrementing `x` once,
    the value of `x` could still be as if it was incremented only once. Depending
    on the machine architecture, for some "primitive" data types, it may require two
    CPU instructions to update the value of a variable. Two such operations executing
    concurrently could end up setting the value to what neither intended due to *partial
    writes*.
  prefs: []
  type: TYPE_NORMAL
- en: Interleaved operations like these represent a **data race**—the threads performing
    them are said to race against each other in performing the operation steps and
    their exact sequence, and therefore, the results are unpredictable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us use the notation [r=v1, w=v2] to indicate that a thread *read* the value
    v1 from the variable `x` and *wrote* back v2\. Note that there can be an arbitrary
    duration between the time a thread reads the value of `x` and the time when it
    writes back a value. So the notation [r=v1, … is used to indicate that a value
    v1 was read but the write back is yet to happen, and the notation … w=v2] indicates
    that the pending write happened. Now consider two threads each incrementing `x`
    a million times, as shown in the following sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data races and atomic operations](img/1217OT_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For simplicity, assume that partial writes cannot happen. At time **t1**, both
    Thread 1 and Thread 2 read the value of `x` as 0\. Thread 2 increments this value
    and writes back the value 1\. Thread 2 continues reading and incrementing the
    value of `x` for 999998 more iterations until it writes back the value 999999
    at time **t999999**. Following this, Thread 1 increments the value 0 that it had
    read at **t1** and writes back the value 1\. Next, both Thread 1 and Thread 2
    read the value 1, and Thread 1 writes back 2 but Thread 2 hangs on. Thread 1 goes
    on for 999998 more iterations, reading and incrementing the value of `x`. It writes
    the value 1000000 to `x` at time **t1999999** and exits. Thread 2 now increments
    the value 1 that it had read at **t1000001** and writes back. For two million
    increments, the final value of `x` could well be 2\. You can change the number
    of iterations to any number greater than or equal to 2, and the number of threads
    to any number greater than or equal to 2, and this result would still hold—a measure
    of the nondeterminism and nonintuitive aspects of concurrency. When we see the
    operation `++x`, we intuitively think of it as an indivisible or *atomic operation,*
    when it really is not.
  prefs: []
  type: TYPE_NORMAL
- en: An **atomic operation** runs without any observable intermediate states. Such
    operations cannot interleave. Intermediate states created by an atomic operation
    are not visible to other threads. Machine architectures provide special instructions
    for performing atomic read-modify-write operations, and operating systems often
    provide library interfaces for atomic types and operations that use these primitives.
  prefs: []
  type: TYPE_NORMAL
- en: The increment operation `++x` is clearly nonatomic. The variable `x` is a shared
    resource and between a read, increment, and a subsequent write to `x` by one thread,
    any number of read-modify-writes to `x` can take place from other threads—the
    operations can be interleaved. For such nonatomic operations, we must find means
    of making them **thread-safe,** that is, by preventing interleaving of operations,
    such as `++x`, across multiple threads.
  prefs: []
  type: TYPE_NORMAL
- en: Mutual exclusion and critical sections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One way to make the `++x` operation thread-safe is to perform it in a **critical
    section**. A critical section is a section of code that cannot be executed simultaneously
    by two different threads. Thus, two increments of `x` from different threads can
    be interleaved. Threads must adhere to this protocol and can use a **mutex** to
    do so. A mutex is a primitive used for synchronizing concurrent access to shared
    resources, such as the variable `x`. We use the `boost::mutex` class for this
    purpose, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.10: Using mutexes**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We declare a mutex object of type `boost::mutex` (line 9), capture it in the
    lambda that generates the initial function for the threads (line 11), and then
    protect the increment operation on the variable `x` by locking the mutex before
    performing it (line 13) and unlocking it afterwards (line 15). The increment operation
    on `x` (line 14) is the critical section. This code prints the following each
    and every time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'How does this work? The mutex object has two states: **locked** and **unlocked**.
    The first thread to call the `lock` member function on a mutex that is unlocked,
    locks it and the call to `lock` returns. Other threads that call `lock` on the
    already-locked mutex simply **block**, which means the OS scheduler does not schedule
    these threads to run, unless some event (like the unlocking of the mutex in question)
    takes place. The thread with the lock then increments `x` and calls the `unlock`
    member function on the mutex to relinquish the lock it is holding. At this point,
    one of the threads that is blocked in the `lock` call is woken up, the call to
    `lock` in that thread returns, and the thread is scheduled to run. Which waiting
    thread is woken up depends on the underlying native implementation. This goes
    on until all the threads (in our example, just two) have run to completion. The
    lock ensures that at any point in time, only one thread exclusively holds the
    lock and is thus free to increment `x`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The section we choose to protect with the mutex is critical. We could have
    alternatively protected the entire for-loop, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The final value of `x` would still be the same (`2000000`) as with listing 10.10,
    but the critical section would be bigger (lines 13-15). One thread would run its
    entire loop before the other thread could even increment `x` once. By limiting
    the extent of the critical section and the time a thread holds the lock, multiple
    threads can make more equitable progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'A thread may choose to probe and see whether it can acquire a lock on a mutex
    but not block if it cannot. To do so, the thread must call the `try_lock` member
    function instead of the `lock` member function. A call to `try_lock` returns `true`
    if the mutex was locked and `false` otherwise, and does not block if the mutex
    was not locked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'A thread may also choose to block for a specified duration while waiting to
    acquire a lock, using the `try_lock_for` member function. The call to `try_lock_for`
    returns `true` if it succeeds in acquiring the lock and as soon as it does. Otherwise,
    it blocks for the entire length of the specified duration and returns false once
    it times out without acquiring the lock:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mutexes should be held for as short a duration as possible over as small a section
    of code as necessary. Since mutexes serialize the execution of critical sections,
    holding a mutex over longer durations delays the progress of other threads waiting
    to lock the mutex.
  prefs: []
  type: TYPE_NORMAL
- en: boost::lock_guard
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Acquiring a lock on a mutex and failing to release it is disastrous, as any
    other thread waiting on the mutex will never make any progress. The bare `lock`
    / `try_lock` and `unlock` calls on the mutex are not a good idea, and we need
    some means of locking and unlocking mutexes in an exception-safe way. The `boost::lock_guard<>`
    template uses the **Resource Acquisition Is Initialization** (**RAII**) idiom
    to lock and unlock mutexes in its constructor and destructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.11: Using boost::lock_guard**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Using a `boost::lock_guard` object (line 13), we lock the section of code following
    the instantiation of the lock guard till the end of the scope. The `lock_guard`
    acquires the lock in the constructor and releases it in the destructor. This ensures
    that even in the face of an exception arising in the critical section, the mutex
    is always unlocked once the scope is exited. You pass the type of the lock as
    a template argument to `lock_guard`. `boost::lock_guard` can be used not only
    with `boost::mutex` but with any type that conforms to the **BasicLockable** concept,
    that is, has accessible `lock` and `unlock` member functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use `boost::lock_guard` to encapsulate a mutex that is already
    locked. To do so we need to pass a second argument to the `lock_guard` constructor
    indicating that it should assume ownership of the mutex without trying to lock
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`boost::lock_guard` either locks the underlying mutex in its constructor or
    adopts an already-locked mutex. The only way to release the mutex is to let the
    `lock_guard` go out of scope. `lock_guard` is neither copyable nor movable, so
    you cannot pass them around from one function to the next, nor store them in containers.
    You cannot use `lock_guard` to wait on a mutex for specific durations.'
  prefs: []
  type: TYPE_NORMAL
- en: boost::unique_lock
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `boost::unique_lock<>` template is a more flexible alternative that still
    uses RAII to manage mutex-like locks but provides an interface to manually lock
    and unlock as required. For this additional flexibility, `unique_lock` has to
    maintain an additional data member to keep track of whether the mutex is owned
    by the thread or not. We can use `unique_lock` to manage any class conforming
    to the **Lockable** concept. A class conforms to the Lockable concept if it conforms
    to BasicLockable and additionally, defines an accessible `try_lock` member function—just
    as `boost::mutex` does.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `boost::unique_lock` as a drop-in replacement for `boost::lock_guard`,
    but `unique_lock` should not be used if `lock_guard` suffices for a purpose. `unique_lock`
    is typically useful when we want to mix manual locking with exception-safe lock
    management. For example, we can rewrite listing 10.11 to use `unique_lock`, as
    shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Unlike in listing 10.11, we do not create a new `lock_guard` object in each
    iteration of the loop. Instead, we create a single `unique_lock` object encapsulating
    the mutex before the loop begins (line 12). The `boost::defer_lock` argument passed
    to the `unique_lock` constructor tells the constructor not to lock the mutex immediately.
    The mutex is locked before incrementing the shared variable by calling the `lock`
    member function of `unique_lock` (line 16) and unlocked after the operation by
    calling the `unlock` member function of `unique_lock` (line 21). In the event
    of an exception, the `unique_lock` destructor unlocks the mutex only if it is
    locked.
  prefs: []
  type: TYPE_NORMAL
- en: The `owns_lock` member function of `unique_lock` returns `true` if the `unique_lock`
    owns a lock on the mutex, and `false` otherwise (lines 13 and 18). The `mutex`
    member function of `unique_lock` returns a pointer to the stored mutex (line 19)
    or `nullptr` if `unique_lock` does not wrap a valid mutex.
  prefs: []
  type: TYPE_NORMAL
- en: Deadlocks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mutexes provide for exclusive ownership of shared resources and many real-world
    problems deal with multiple shared resources. Take the case of a multiplayer first-person
    shooting game. It maintains and updates two lists in real time. There is a set
    A of shooters who are players with ammunition of some sort, and a second set U
    of players that are unarmed. When a player exhausts her ammo, she is moved from
    A to U. When her ammo is replenished, she is moved back from U to A. Thread 1
    handles moving elements from A to U and thread 2 handles moving elements from
    U to A.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a new player joins the game, she is added to either U or A, depending
    on whether she has ammo. When a player is killed in the game, she is removed from
    whichever set (U or A) she was part of. But when ammo is either exhausted or replenished,
    the player is moved between U and A; so both U and A need to be edited. Consider
    the following code in which one thread is responsible for moving players from
    A to U when ammo is exhausted, and another thread is responsible for the movement
    back (U to A) when ammo is replenished:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.12: Deadlock example**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Each time a player's ammo is exhausted, the `onAmmoExhausted` (line 42) function
    is called with the ID of the player. This function creates a thread that runs
    the function `a2u` (line 18) to move this player from set A (`armed`) to set U
    (`unarmed`). Similarly, when player's ammo is replenished, the `onAmmoReplenished`
    (line 47) function is called and this, in turn, runs the function `u2a` in a separate
    thread to move the player from the set U (`unarmed`) to the set A (`armed`).
  prefs: []
  type: TYPE_NORMAL
- en: The mutexes `amtx` and `umtx` control access to the sets `armed` and `unarmed`.
    To move a player from A to U, the function `a2u` first acquires a lock on `amtx`
    (line 19) and looks up the player in `armed` (line 20). If the player is found,
    the thread acquires a lock on `umtx` (line 23), puts the player in `unarmed` (line
    23), releases the lock on `umtx` (line 24), and removes the player from `armed`
    (line 25).
  prefs: []
  type: TYPE_NORMAL
- en: The function `u2a` has essentially the same logic but acquires the lock on `umtx`
    first, followed by `amtx`, and this leads to a fatal flaw. If one player exhausts
    ammo and another replenishes ammo at around the same time, two threads could run
    `a2u` and `u2a` concurrently. Perhaps rarely, it could happen that the `exhausted`
    thread locks `amtx` (line 19), but before it can lock `umtx` (line 23), the `replenished`
    thread locks `umtx` (line 31). Now the exhausted thread waits for `umtx`, which
    is held by the `replenished` thread, and the `replenished` thread waits for `amtx`,
    which is held by the `exhausted` thread. There is no conceivable way for the two
    threads to proceed from this state, and they are locked in a deadlock.
  prefs: []
  type: TYPE_NORMAL
- en: A **deadlock** is a state in which two or more threads vying for shared resources
    are blocked, waiting on some resources while holding others, such that it is *impossible*
    for any of the threads to progress from that state.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, only two threads were involved, and it is relatively easy to
    debug and fix the problem. The gold standard for fixing deadlocks is to ensure
    **fixed lock-acquisition order**—any thread acquires two given locks in the same
    order. By rewriting `u2a`, as shown in the following snippet, we can ensure that
    a deadlock is not possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we make sure that `u2a` locks `amtx` first before locking
    `umtx`, just like `a2u` does. We could have manually acquired the locks in this
    order but instead, we demonstrate the use of `boost::lock` to do this. We create
    the `unique_lock` objects, `lka` and `lku`, with the `defer_lock` flag to indicate
    we do not want to acquire the locks yet. We then call `boost::lock`, passing the
    `unique_lock`s in the order we would like to acquire them, and `boost::lock` ensures
    that order is observed.
  prefs: []
  type: TYPE_NORMAL
- en: There are two reasons for using `boost::unique_lock` instead of `boost::lock_guard`
    in this example. First, we can create `unique_lock`s without immediately locking
    the mutex. Second, we can call `unlock` to release the `unique_lock` early (line
    40) and increase lock granularity, which promotes concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Besides fixed lock-acquisition order, another way to avoid deadlocks is for
    threads to probe locks (using `try_lock`) and backtrack if they fail to acquire
    a particular lock. This typically makes code more complex, but may be necessary
    sometimes.
  prefs: []
  type: TYPE_NORMAL
- en: There are many real-world examples of code with deadlocks, like the one in our
    example, which might be working correctly for years but with deadlocks lurking
    in them. Sometimes, the probability of hitting the deadlock could be very low
    when run on one system, and you might immediately hit it when you run the same
    code on another system, all purely because of variances in thread scheduling on
    the two systems.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronizing on conditions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mutexes serialize access to shared data by creating critical sections. A critical
    section is like a room with a lock and a waiting area outside. One thread acquires
    the lock and occupies the room while others arrive outside, wait for the occupant
    to vacate the room, and then take its place in some defined order. Sometimes,
    threads need to wait on a condition becoming true, such as some shared data changing
    state. Let us look at the producer-consumer problem to see examples of threads
    waiting on conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Condition variables and producer-consumer problem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Unix command-line utility **grep** searches files for text patterns specified
    using regular expressions. It can search through a whole list of files. To search
    for a pattern in a file, its complete contents must be read and searched for the
    pattern. Depending on the number of files to search, one or more threads can be
    employed to concurrently read contents of files into buffers. The buffers can
    be stored in some data structure that indexes them by file and offset. Multiple
    threads can then process these buffers and search them for the pattern.
  prefs: []
  type: TYPE_NORMAL
- en: What we just described is an example of a producer-consumer problem in which
    a set of threads generates some content and puts them in a data structure, and
    a second set of threads reads the content off the data structure, and performs
    computations on it. If the data structure is empty, the consumers must wait until
    a producer adds some content. If data fills up the data structure, then the producers
    must wait for consumers to process some data and make room in the data structure
    before trying to add more content. In other words, consumers wait on certain conditions
    to fulfill and these are fulfilled as a result of the actions of the producers,
    and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: One way to model such conditions, wait on them, and signal them, is by using
    `boost::condition_variable` objects. A **condition variable** is associated with
    a testable runtime condition or predicate in the program. A thread tests the condition
    and if it is not true, the thread waits for that condition to become true using
    a `condition_variable` object. Another thread that causes the condition to become
    true signals the condition variable, and this wakes up one or more waiting threads.
    Condition variables are inherently associated with shared data and represent some
    condition being fulfilled for the shared data. In order for the waiting thread
    to first test a condition on the shared data, it must acquire the mutex. In order
    for the signaling thread to change the state of shared data, it too needs the
    mutex. In order for the waiting thread to wake up and verify the result of the
    change, it again needs the mutex. Thus we need to use `boost::mutex` in conjunction
    with a `boost::condition_variable`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now solve the producer-consumer problem for a fixed-sized queue using
    condition variables. There is a queue of a fixed size, which means the maximum
    number of elements in the queue is bounded. One or more threads produce content
    and **enqueue** them (append them to the queue). One or more threads **dequeue**
    content (remove content from the head of the queue) and perform computations on
    the content. We use a circular queue implemented on top of a fixed size `boost::array`
    rather than any STL data structure, such as `std::list` or `std::deque`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.13: Using condition variables for a thread-safe, fixed-size queue**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In this listing, we define the `CircularQueue<>` template and its member functions,
    including the `pop` (line 11) and `push` (line 32) member functions, which are
    of particular interest. A call to `push` blocks until there is space in the queue
    to add a new element. A call to `pop` blocks until it is able to read and remove
    an element from the top of the queue. The utility function `top` (line 21) blocks
    until it is able to read an element from the top of the queue, a copy of which
    it returns.
  prefs: []
  type: TYPE_NORMAL
- en: To implement the necessary synchronization, we define the mutex `qlock` (line
    58) and two condition variables, `canRead` (line 59) and `canWrite` (line 60).
    The `canRead` condition variable is associated with a predicate that checks whether
    there are any elements in the queue which could thus be read. The `canWrite` condition
    variable is associated with a predicate that checks whether there is any space
    left in the queue where a new element can be added. The mutex `qlock` needs to
    be locked to edit the queue and to check the state of the queue in any way.
  prefs: []
  type: TYPE_NORMAL
- en: The `pop` method first acquires a lock on `qlock` (line 12) and then checks
    whether the queue is empty (line 13). If the queue is empty, the call must block
    until there is an item available to read. To do this, `pop` calls the `wait` method
    on the `canRead` condition variable, passing it the lock `lock` and a lambda predicate
    to test (line 14). The call to `wait` unlocks the mutex in `lock` and blocks.
    If a call to the `push` method from another thread succeeds and thus data is available,
    the `push` method unlocks the mutex (line 39) and signals the `canRead` condition
    variable by calling the `notify_one` method (line 40). This wakes up exactly one
    thread blocked in the `wait` call inside a `pop` method call. The `wait` call
    atomically locks the mutex, checks whether the predicate (`size() > 0`) is true
    and if so, returns (line 14). If the predicate is not true, it once again unlocks
    the mutex and goes back to waiting.
  prefs: []
  type: TYPE_NORMAL
- en: The `pop` method is either woken up from its wait, and verifies that there is
    an element to read after reacquiring the mutex lock, or it never has to wait because
    there were elements to read already. Thus, `pop` proceeds to remove the element
    at the head of the list (line 16). After removing the element, it unlocks the
    mutex (line 17) and calls `notify_one` on the `canWrite` condition (line 18).
    In case it popped an element from a queue that was full, and there were threads
    blocked in `push`, waiting for some room in the queue, the call to `notify_one`
    wakes up exactly one thread blocked in `canWrite.wait(...)` inside `push` (line
    35) and gives it the chance to add an item to the queue.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of `push` is really symmetrical and uses the same concepts
    we described for `pop`. We pass the mutex to the `wait` method on the condition
    variable, wrapped in a `unique_lock` and not a `lock_guard` because the wait method
    needs to access the underlying mutex to unlock it manually. The underlying mutex
    is retrieved from a `unique_lock` by calling the `mutex` member function of `unique_lock`;
    `lock_guard` does not provide such a mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: To test our implementation, we create a `CircularQueue` of 200 elements of type
    `int` (line 65), a producer thread that pushes 10,000 elements into the queue
    (line 67), and four consumer threads that pop 2,500 elements each (lines 82-85).
  prefs: []
  type: TYPE_NORMAL
- en: The consumer threads are not created individually but as part of a **thread
    group**. A thread group is an object of type `boost::thread_group`, which provides
    an easy way to manage multiple threads together. Since we want to create four
    consumer threads using the same initial function and join them all, it is easy
    to create a `thread_group` object (line 82), create four threads in a loop using
    its `create_thread` member function (line 84), and wait on all the threads in
    the group by calling the `join_all` method (line 88).
  prefs: []
  type: TYPE_NORMAL
- en: Condition variable nuances
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We call `notify_one` to signal the `canRead` condition variable and wake up
    exactly one thread waiting to read (line 39). Instead, we could have called `notify_all`
    to *broadcast* the event and wake up all waiting threads, and it would still have
    worked. However, we only put one new element in the queue in each call to `push`,
    so exactly one of the threads woken up would read the new element off the queue.
    The other threads would check the number of elements in the queue, find it empty,
    and go back to waiting, resulting in unnecessary context switches.
  prefs: []
  type: TYPE_NORMAL
- en: But if we added a load of elements to the queue, calling `notify_all` might
    be a better alternative than `notify_one`. Calling `notify_one` would wake up
    only one waiting thread, which would process the elements serially in a loop (lines
    63-65). Calling `notify_all` would wake up all the threads, and they would process
    the elements concurrently much quicker.
  prefs: []
  type: TYPE_NORMAL
- en: One common conundrum is whether to call `notify_one`/`notify_all` while holding
    the mutex, as we have done in our examples earlier, or after releasing it. Both
    options work equally well, but there might be some difference in the performance.
    If you signal a condition variable while holding the mutex, the woken up threads
    would immediately block, waiting for the mutex until you release it. So there
    are two additional context switches per thread and these can have an impact on
    the performance. Therefore, if you unlock the mutex first before signaling the
    condition variable, you could see some performance benefits. Therefore, signaling
    *after* unlocking is the often preferred approach.
  prefs: []
  type: TYPE_NORMAL
- en: The Readers-Writers problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Take the case of an online catalog of a library. The library maintains a look-up
    table of books. For simplicity, let us imagine that the books can only be looked
    up by titles, and titles are unique. Multiple threads representing various clients
    perform look-ups on the library concurrently. From time to time, the librarian
    adds new books to the catalog and rarely, takes a book off the catalog. A new
    book can be added only if a book with the same title is not already present, or
    if an older edition of the title is present.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following snippet, we define a type representing a book entry and the
    public interface of the `LibraryCatalog` class that represents the library catalog:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.14a: Library catalog types and interfaces**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The member function `find_book` is used to look up a single title and returns
    it as `book_t` object wrapped in `boost::optional`. Using `boost::optional`, we
    can return an empty value if a title is not found (see [Chapter 2](ch02.html "Chapter 2. The
    First Brush with Boost's Utilities"), *The First Brush with Boost's Utilities*).
    The member function `find_books` looks up a list of titles passed to it as a `vector`
    and returns a vector of `book_t` objects. The member function `add_book` adds
    a title to the catalog and `remove_book` removes a title from the catalog.
  prefs: []
  type: TYPE_NORMAL
- en: We want to implement the class to allow multiple threads to look up titles concurrently.
    We also want to allow the librarian to add and remove titles concurrently with
    the reads, without hurting correctness or consistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'As long as data in the catalog does not change, multiple threads can concurrently
    look up titles without the need for any synchronization; because read-only operations
    cannot introduce inconsistencies. But since the catalog does allow the librarian
    to add and remove titles, we must make sure that these operations do not interleave
    with read operations. In thus formulating our requirements, we just stated the
    classic concurrency problem known as the Readers-Writers problem. The Readers-Writers
    problem lays down the following constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: Any writer thread must have exclusive access to a data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any reader thread can share access to the data structure with other reader threads,
    in the absence of a writer thread
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the above statements, *reader thread* refers to threads performing only read-only
    operations like looking up titles, and *writer thread* refers to threads that
    modify the contents of the data structure in some way, such as adding and removing
    titles. This is sometimes referred to as **Multiple Readers Single Writer** (**MRSW**)
    model, as it allows either multiple concurrent readers or a single exclusive writer.
  prefs: []
  type: TYPE_NORMAL
- en: 'While `boost::mutex` allows a single thread to acquire an exclusive lock, it
    does not allow multiple threads to share a lock. We need to use `boost::shared_mutex`
    for this purpose. `boost::shared_mutex` conforms to the *SharedLockable* concept,
    which subsumes the Lockable concept, and additionally, defines `lock_shared` and
    `unlock_shared` member functions, which should be called by reader threads. Because
    `shared_mutex` also conforms to Lockable, it can be locked for exclusive access
    using `boost::lock_guard` or `boost::unique_lock`. Let us now look at the implementation
    of `LibraryCatalog`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.14b: Library catalog implementation**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The method `find_book` performs read-only operations on the catalog and therefore
    acquires a shared lock using the `boost::shared_lock` template (line 17). It releases
    the lock after retrieving a matching book, if any (line 23). The method `find_books`
    is implemented in terms of `find_book`, which it calls in a loop for each title
    in the list passed to it. This allows for better overall concurrency between reader
    threads at the cost of a slight performance hit, due to repeated locking and unlocking
    of the `shared_mutex`.
  prefs: []
  type: TYPE_NORMAL
- en: Both `add_book` and `remove_book` are mutating functions that potentially change
    the number of elements in the catalog. In order to modify the catalog, both methods
    require exclusive or write locks on the catalog. For this reason, we use `unique_lock`
    instances to acquire an exclusive lock on the `shared_mutex` (lines 43 and 59).
  prefs: []
  type: TYPE_NORMAL
- en: Upgradable locks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There is one glaring problem in the implementation of `add_book` and `remove_book`
    methods in listing 10.14b. Both methods modify the catalog conditionally, based
    on the outcome of a look-up that is run first. Yet an exclusive lock is acquired
    unconditionally at the start of both operations. One could conceivably call `remove_book`
    with a nonexistent title or `add_book` with an edition of a book that is already
    in the catalog, in a loop, and seriously hamper the concurrency of the system
    doing nothing.
  prefs: []
  type: TYPE_NORMAL
- en: If we acquired a shared lock to perform the look up, we would have to release
    it before acquiring an exclusive lock for modifying the catalog. In this case,
    the results of the look up would no longer be reliable, as some other thread could
    have modified the catalog between the time the shared lock is released and the
    exclusive lock acquired.
  prefs: []
  type: TYPE_NORMAL
- en: 'This problem can be addressed by using `boost::upgrade_lock` and a set of associated
    primitives. This is shown in the following rewrite of `add_book`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Instead of acquiring an exclusive lock to start with, we acquire an *upgrade
    lock* before performing the look up (line 2), and then *upgrade* it to a unique
    lock only if we need to modify the catalog (lines 6-7 and 11-12). To acquire an
    upgrade lock, we wrap the shared mutex in an `upgrade_lock<boost::shared_mutex>`
    instance (line 2). This blocks if there is an exclusive lock or another upgrade
    lock on the mutex in effect, but proceeds otherwise even if there be shared locks.
    Thus, at any point in time, there can be any number of shared locks and at most
    one upgrade lock on a mutex. Acquiring an upgrade lock thus does not impact read
    concurrency. Once the look up is performed, and it is determined that a write
    operation needs to be performed, the upgrade lock is promoted to a unique lock
    by wrapping it in an instance of `upgrade_to_unique_lock<boost::shared_mutex>`
    (lines 6-7 and 11-12). This blocks until there are no remaining shared locks,
    and then *atomically* releases the upgrade ownership and acquires an exclusive
    ownership on the `shared_mutex`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Acquiring an upgrade lock indicates intent to potentially upgrade it to an exclusive
    lock and perform writes or modifications.
  prefs: []
  type: TYPE_NORMAL
- en: Performance of shared_mutex
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`boost::shared_mutex` is slower than `boost::mutex` but acquiring additional
    read locks on an already read-locked mutex is much faster. It is ideally suited
    for frequent concurrent reads with infrequent need for exclusive write access.
    Any time you deal with frequent writes, just use `boost::mutex` to provide exclusive
    write access.'
  prefs: []
  type: TYPE_NORMAL
- en: Most solutions to the MRSW problem either prefer readers over writers or the
    other way round. In **read-preferring solutions**, when a shared lock is in effect,
    new reader threads can acquire a shared lock even with a writer waiting to acquire
    an exclusive lock. This leads to write-starvation as the writer only ever gets
    an exclusive lock at a point when no readers are around. In **write-preferring
    solutions**, if there is a writer thread waiting on an exclusive lock, then new
    readers are queued even if existing readers hold a shared lock. This impacts the
    concurrency of reads. Boost 1.57 (current release) provides a shared/exclusive
    lock implementation that is completely fair and does not have either a reader-
    or a writer-bias.
  prefs: []
  type: TYPE_NORMAL
- en: Standard Library primitives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The C++11 Standard Library introduces `std::mutex` and a whole host of RAII
    wrappers for locks, including `std::lock_guard`, `std::unique_lock`, and `std::lock`,
    available in the header `mutex`. C++11 Standard Library also introduces `std::condition_variable`
    available in the header `condition_variable`. The C++14 Standard Library introduces
    `std::shared_timed_mutex`, which corresponds to `boost::shared_mutex` and `std::shared_lock`,
    both available in the header `mutex`. They correspond to their Boost counterparts
    of the same names, and have very similar interfaces. There is no upgrade lock
    facility in the Standard Library as of C++14, nor any equivalent of the convenient
    `boost::thread_group`.
  prefs: []
  type: TYPE_NORMAL
- en: Boost Coroutine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Coroutines are functions that can *yield* or relinquish control to another
    coroutine, and then given control back, resuming from the point at which they
    earlier yielded. The state of automatic variables is maintained between a yield
    and the resumption. Coroutines can be used for complex control flow patterns with
    surprisingly simple and clean code. The Boost Coroutine library provides two types
    of coroutines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Asymmetric coroutines**: Asymmetric coroutines distinguish between a caller
    and a callee coroutine. With asymmetric coroutines, a callee can only yield back
    to the caller. They are often used for unidirectional data transfer from either
    the callee to caller, or the other way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Symmetric coroutines**: Such coroutines can *yield* to other coroutines,
    irrespective of who the caller was. They can be used to generate complex cooperative
    chains of coroutines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a coroutine yields control, it is said to be suspended—its registers are
    saved and it relinquishes control to another function. On resumption, the registers
    are restored and execution continues beyond the point of yield. The Boost Coroutine
    library utilizes the Boost Context library for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: A distinction is made between *stackful coroutines* versus *stackless coroutines*.
    A stackful coroutine can be suspended from within a function called by the coroutine,
    that is, from a nested stackframe. With stackless coroutines, only the top level
    routine may suspend itself. In this chapter, we only look at asymmetric stackful
    coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Asymmetric coroutines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core template used to define asymmetric coroutines is called `boost::coroutines::asymmetric_coroutine<>`.
    It takes a single type parameter that represents the type of value transferred
    from one coroutine to the other. It can be `void` if no value needs to be transferred.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coroutines that call other coroutines or yield to them must have a way to refer
    to other coroutines. The nested type `asymmetric_coroutine<T>::push_type` represents
    a coroutine that provides data of type `T`, and the nested type `asymmetric_coroutine<T>::pull_type`
    represents a coroutine that consumes the data of type `T`. Both the types are
    callable types, with an overloaded `operator()`. Using these types, we shall now
    write a program that uses coroutines to read data from a vector of elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 10.15: Using asymmetric coroutines**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: To start with, we define two alias templates called `pull_type` and `push_type`
    referring to `asymmetric_coroutine<T>::pull_type` and `asymmetric_coroutine<T>::push_type`
    for a type parameter T (lines 7-9 and 11-13).
  prefs: []
  type: TYPE_NORMAL
- en: The function `getNextElem` (line 16) is meant to be used as a coroutine that
    passes the next element from a vector to the caller each time it is called. The
    `main` function populates this vector (lines 26-27) and then calls `getNextElem`
    repeatedly to get each element. Thus data is transferred from `getNextElem` to
    `main`, `main` being the caller routine, and `getNextElem`, the callee routine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on whether the coroutine pushes data to the caller or pulls data
    from it, it should have one of the following two signatures:'
  prefs: []
  type: TYPE_NORMAL
- en: '`void (push_type&)`: Coroutine pushes data to caller'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`void(pull_type&)`: Coroutine pulls data from caller'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pull_type` or `push_type` reference passed to the coroutine refers to the
    calling context and represents the conduit through which it pushes data to, or
    pulls data from the caller.
  prefs: []
  type: TYPE_NORMAL
- en: 'The caller routine must wrap the function in `pull_type` or `push_type`, depending
    on whether it intends to pull data from it or push data to it. In our case, the
    `main` function must wrap `getNextElem` in an instance of `pull_type`. However,
    the signature of `getNextElem` is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Thus we must adapt it to a conforming signature using some mechanism such as
    lambda or `bind`. We use `boost::bind` to bind the second parameter of `getNextElem`
    to the vector (lines 29-30) and wrap the resulting unary function object in a
    `pull_type` instance called `greet_func`. Creating the instance of `pull_type`
    invokes the `getNextElem` coroutine for the first time.
  prefs: []
  type: TYPE_NORMAL
- en: We can use `greet_func` in a Boolean context to check whether a value is available
    from the callee, and we use this to spin in a loop (line 32). In each iteration
    of the loop, we call the `get` member function on the `pull_type` instance to
    obtain the next value furnished by `getNextElem` (line 33). We then invoke the
    overloaded `operator()` of `pull_type` to relinquish control to the `getNextElem`
    coroutine (line 34).
  prefs: []
  type: TYPE_NORMAL
- en: On the other side, the `getNextElem` coroutine does not use a conventional return
    value to send data back to the caller. It iterates through the vector and uses
    the overloaded `operator()` on the calling context to return each element (line
    20). If the caller had to push data to the callee instead, then the caller would
    have wrapped the callee in `push_type`, and the callee would be passed the caller's
    reference wrapped in `pull_type`. In the next chapter, we will see how Boost Asio
    uses coroutines to simplify asynchronous, event-driven logic.
  prefs: []
  type: TYPE_NORMAL
- en: Self-test questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For multiple choice questions, choose all options that apply:'
  prefs: []
  type: TYPE_NORMAL
- en: What happens if you do not call `join` or `detach` on a `boost::thread` object
    and a `std::thread` object?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. `join` is called on underlying thread of `boost::thread`.
  prefs: []
  type: TYPE_NORMAL
- en: b. `std::terminate` is called for `std::thread`, terminating the program.
  prefs: []
  type: TYPE_NORMAL
- en: c. `detach` is called on underlying thread of `boost::thread`.
  prefs: []
  type: TYPE_NORMAL
- en: d. `detach` is called on underlying thread of `std::thread`.
  prefs: []
  type: TYPE_NORMAL
- en: What happens if an exception is allowed to propagate past the initial function
    with which a `boost::thread` object is created?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. The program is terminated via `std::terminate`.
  prefs: []
  type: TYPE_NORMAL
- en: b. It is undefined behavior.
  prefs: []
  type: TYPE_NORMAL
- en: c. The call to `get` on the `future` object throws an exception in the calling
    thread.
  prefs: []
  type: TYPE_NORMAL
- en: d. The thread is terminated but the exception is not propagated.
  prefs: []
  type: TYPE_NORMAL
- en: Should you call `notify_one` or `notify_all` on a `condition_variable` object
    without holding the associated mutex?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. No, the call will block.
  prefs: []
  type: TYPE_NORMAL
- en: b. Yes, but it may result in priority inversion in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: c. No, some waiting threads may miss the signal.
  prefs: []
  type: TYPE_NORMAL
- en: d. Yes, it may even be faster.
  prefs: []
  type: TYPE_NORMAL
- en: What is the advantage of using `boost::unique_lock` over `boost::lock_guard`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. `boost::unique_lock` is more efficient and lightweight.
  prefs: []
  type: TYPE_NORMAL
- en: b. `boost::unique_lock` can or adopt an already acquired lock.
  prefs: []
  type: TYPE_NORMAL
- en: c. `boost::lock_guard` cannot be unlocked and relocked mid-scope.
  prefs: []
  type: TYPE_NORMAL
- en: d. `boost::unique_lock` can defer acquiring a lock.
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following are true of `boost::shared_mutex`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. `shared_mutex` is more lightweight and faster than `boost::mutex`.
  prefs: []
  type: TYPE_NORMAL
- en: b. Boost implementation of `shared_mutex` does not have reader- or writer-bias.
  prefs: []
  type: TYPE_NORMAL
- en: c. `shared_mutex` can be used as an upgradable lock.
  prefs: []
  type: TYPE_NORMAL
- en: d. `shared_mutex` is ideal for systems with high-write contention.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to write concurrent logic in terms of threads
    and tasks using the Boost Thread library and the C++11 Standard Library. We learned
    how to use the futures and promises paradigm to define ordering of operations
    across concurrent tasks, and some abstractions around futures and promises in
    the Standard Library. We also studied various lock-based thread synchronization
    primitives and applied them to some common multithreading problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multithreading is a difficult and complex topic, and this chapter merely introduces
    the portable APIs available in Boost to write concurrent programs. The Boost Thread
    library and the concurrent programming interfaces in the C++ Standard Library
    are an evolving set, and we did not cover several features: the C++ memory model
    and atomics, Boost Lockfree, thread cancellation, experimental continuations with
    `boost::future`s, and several more topics. Architectural concerns in designing
    concurrent systems and concurrent data structures are other relevant topics that
    are outside the scope of this book. Hopefully, the concepts and methods presented
    in this chapter will help you explore further in these directions.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*C++ Concurrency in Action*, *Anthony Williams*, *Manning Publications*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lockfree data structures: [http://www.boost.org/libs/lockfree](http://www.boost.org/libs/lockfree)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A proposal to add coroutines to the C++ standard library (Revision 1)*, *Oliver
    Kowalke* and *Nat Goodspeed*: [http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3985.pdf](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3985.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lock-Free Programming, Herb Sutter: [https://youtu.be/c1gO9aB9nbs](https://youtu.be/c1gO9aB9nbs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'atomic<> Weapons (video), Herb Sutter:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://channel9.msdn.com/Shows/Going+Deep/Cpp-and-Beyond-2012-Herb-Sutter-atomic-Weapons-1-of-2](https://channel9.msdn.com/Shows/Going+Deep/Cpp-and-Beyond-2012-Herb-Sutter-atomic-Weapons-1-of-2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://channel9.msdn.com/Shows/Going+Deep/Cpp-and-Beyond-2012-Herb-Sutter-atomic-Weapons-2-of-2](https://channel9.msdn.com/Shows/Going+Deep/Cpp-and-Beyond-2012-Herb-Sutter-atomic-Weapons-2-of-2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
