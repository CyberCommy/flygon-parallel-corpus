- en: Kernel Execution Model and Optimization Strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CUDA programming has a procedure for host operations. For example, we need to
    allocate the global memory, transfer data to the GPU, execute kernel functions,
    transfer data back to the host, and clean the global memory. That's because the
    GPU is an extra-processing unit in the system, so we need to care about its execution
    and data transfer. This is another aspect of GPU programming that's different
    compared to CPU programming.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover CUDA kernel execution models and CUDA streams,
    which control CUDA operations. Then, we will discuss optimization strategies at
    the system level. Then, we will cover CUDA events to measure GPU event time, and
    how to use CUDA events to measure kernel execution time. After that, we will cover
    various CUDA kernel executing models and discuss what those features bring to
    GPU operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel execution with a CUDA streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipelining GPU execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CUDA callback function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA streams with priority
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel execution time estimation using CUDA events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA dynamic parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grid-level cooperative groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA kernel calls with OpenMP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-Process Services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel execution overhead comparison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires us to use a CUDA version later than 9.x, and the GPU architecture
    should be Volta or Turing. If you use a GPU with Pascal architecture, skip the
    *Grid-level cooperative groups* section because this feature is introduced for
    Volta architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel execution with CUDA streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A stream is a sequence of commands that relate to the GPU in CUDA programming.
    In other words, all the kernel calls and data transfers are handled by the CUDA
    stream. By default, CUDA provides a default stream, and all the commands use the
    stream implicitly. Therefore, we do not have to handle this ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA supports additional streams, created explicitly. While the operations in
    a stream are sequential, CUDA can execute multiple operations concurrently by
    using the multiple streams. Let's learn how to handle streams, and what features
    they have.
  prefs: []
  type: TYPE_NORMAL
- en: The usage of CUDA streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code shows an example of how CUDA streams can be created, used,
    and terminated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we can handle a CUDA stream using `cudaStream_t`. And, we can
    create this using `cudaStreamCreate()` and terminate it using `cudaStreamDestroy()`.
    Note that we should provide a pointer to `cudaStreamCreate()`. The created stream
    is passed to the kernel's fourth argument.
  prefs: []
  type: TYPE_NORMAL
- en: However, we did not provide such a stream previously. That's because CUDA provides
    a default stream so that all the CUDA operations can operate. Now, let's write
    an application that uses the default stream and multiple streams. Then, we will
    see how our application can be changed.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s write an application that uses the default CUDA stream, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the code, we call the kernel function with the stream ID
    as `0`, because the identification value of the default stream is `0`. Compile
    the code and see the execution output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'How is the output? We can expect that the output will be the order of the loop
    index. The following timeline view shows this code''s operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/870c3559-e2cf-4a27-a481-53697923141d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is to be expected that having loop operations in the same stream shows the
    order of kernel execution. Then, what can be changed if we use multiple CUDA streams,
    and each loop step uses different ones? The following code shows an example of
    printing the loop index from the CUDA kernel function with the different streams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, we have five calls, the same as in the previous code, but here
    we will use five different streams. To do this, we built an array of `cudaStream_t`
    and created streams for each. What can you expect with this change? The printed
    output would be the same as the previous version. Run the following command to
    compile this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'However, that does not guarantee that they have the same operation. As we discussed
    at the beginning, this code shows the concurrency of the multiple streams, as
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c34f9def-ab64-4d9e-a159-f9ecb23c807d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see at the bottom of the screenshot, five individual streams execute
    the same kernel function concurrently and their operations are overlapped with
    each other. From this, we can discern two features of the streams, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel executions are asynchronous with the host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CUDA operations in different streams are independent of each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the concurrency of the stream, we can make extra optimization opportunities
    by overlapping independent operations.
  prefs: []
  type: TYPE_NORMAL
- en: Stream-level synchronization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CUDA streams provide stream-level synchronization with the `cudaStreamSynchronize()`
    function. Using this function forces the host to wait until the end of a certain
    stream's operation. This provides important optimization for the `cudaDeviceSynchronize()`
    function that we have used so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will discuss how to utilize this feature in the following sections, but
    let''s discuss its basic operations here. The previous example shows concurrent
    operations without synchronization within the loop. However, we can halt the host
    to execute the next kernel execution by using the `cudaStreamSynchronize()` function.
    The following code shows an example of using stream synchronization at the end
    of the kernel execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can easily predict that the kernel operation''s concurrency will vanish
    due to synchronization. To confirm this, let''s profile this and see how this
    impacts the kernel execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9a4b008-4225-4766-ac33-c26bec6a0fd6.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, all the kernel executions have no overlapping points, although
    they are executed with the different streams. Using this feature, we can let the
    host wait for the specific stream operation to start with the result.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the default stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To have multiple streams operating concurrently, we should use streams we created
    explicitly, because all stream operations are synchronous with the default stream.
    The following screenshot shows the default stream''s synchronous operation effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e31e5516-2fb9-434c-a0c7-9d729fbdcde0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can achieve this by modifying our multi-stream kernel call operation, like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command to compile the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: So, we can see that the last operation cannot be overlapped with the previous
    kernel executions, but that we have to wait until the fourth kernel execution
    has finished.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelining the GPU execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the major benefits of multiple streams is overlapping the data transfer
    with the kernel execution. By overlapping the kernel operation and data transfer,
    we can conceal the data transfer overhead and increase overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Concept of GPU pipelining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we execute the kernel function, we need to transfer data from the host
    to the GPU. Then, we transfer the result back from the GPU to the host. The following
    diagram shows an example of iterative operations that transfer data between the
    host and kernel executions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8db7a9f-522f-4e8f-aa07-19efc6233fc8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, the kernel execution is basically asynchronous in that the host and GPU
    can operate concurrently with each other. If the data transfer between the host
    and GPU has the same feature, we would be able to overlap their execution, as
    we could see in the previous section. The following diagram shows the operation
    when the data transfer can be executed like a normal kernel operation, and handled
    along with the stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b531d26-ec42-4576-9b9c-511bd175b612.png)'
  prefs: []
  type: TYPE_IMG
- en: In this diagram, we can see that the data transfer between the host and the
    device can be overlapped with the kernel execution. Then, the benefit of this
    overlapped operation is to reduce the application execution time. By comparing
    the length of the two pictures, you will be able to confirm which operation would
    have higher operation throughput.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding CUDA streams, all CUDA operations—data transfers and kernel executions—are
    sequential in the same stream. However, those can operate simultaneously along
    with the different streams. The following diagram shows overlapped data transfer
    with the kernel operation for multiple streams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c3141f46-c05d-4e22-85b4-bd4a2c193f9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To enable such a pipelining operation, CUDA has three prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: The host memory should be allocated as pinned memory—CUDA provides the `cudaMallocHost()` and `cudaFreeHost()` functions
    for this purpose.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transfer data between the host and GPUs without blocking the host—CUDA provides
    the `cudaMemcpyAsync()` function for this purpose.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manage each operation along with the different CUDA streams to have concurrent
    operations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's write a simple application that pipelines the workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Building a pipelining execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code shows a snippet of asynchronous data transfer and the synchronization
    of a CUDA stream at the end of the execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This code shows how to allocate pinned memory, and transfer data with the user-created
    stream. By merging this example and the multiple CUDA stream operations, we can
    have the pipelining CUDA operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s build an application that has the pipelining operation with the
    data transfer and the kernel execution. In this application, we will use a kernel
    function that adds two vectors, by slicing the number of streams, and outputs
    its result. However, the kernel implementation does not require any changes with
    this since we will do this at the host code level. But, we will iterate the addition
    operation 500 times to extend the kernel execution time. As a result, the implemented
    kernel code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To handle each stream''s operation, we will create a class that manages a CUDA
    stream and the CUDA operations. This class will allow us to manage the CUDA stream
    along with the index. The following code shows the basic architecture of the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s write some sequential GPU execution code that we have used in the
    previous section, but as a member function of the `Operator` class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This function''s operation is no different to the basic CUDA host programming
    pattern we have used previously, except we applied `cudaMemcpyAsync()` with the
    given `_stream`. Then, we write `main()` to work with multiple operator instances
    and page-locked memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will allocate host memories using `cudaMallocHost()` to have pinned
    memories, and initialize them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'And, we will have device memories with the same size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will create a list of CUDA operators using the class we used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We are ready to execute the pipelining operations. Before we start the execution,
    let''s place a stopwatch to see the overall execution time and see the overlapped
    data transfer benefit, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s execute each operator using a loop, and each operator will access the
    host and the device memory according to their order. We will, also, measure the
    execution time of the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will compare a sample''s result and print out the overall measured
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Terminate handles and memories, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To execute the code, let''s reuse the host initialization function and GPU
    kernel function from the previous recipes. We don''t have to modify these functions
    at this moment. Compile the code using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You must use your GPU''s compute capability version number for the `gencode`
    option. The output of the compilation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, GPU tasks are executed following the order of the kernel execution
    along with the stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s review how the application operates on the inside. By default,
    the sample code slices the host data into four and executes four CUDA streams
    concurrently. We can see each kernel''s outputs along with the streams'' execution.
    To see the overlapping operation, you need to profile the execution with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows four CUDA streams'' operations by overlapping
    data transfer with kernel execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1dc75db9-3e38-4f1f-8309-e4942488014e.png)'
  prefs: []
  type: TYPE_IMG
- en: Overlaps between the kernel executions and data transfers
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the GPU can be busy until the last kernel execution is finished,
    and we can conceal most of the data transfers. This not only enhances the GPU
    utilization, but also reduces total application execution time.
  prefs: []
  type: TYPE_NORMAL
- en: Between the kernel execution, we can find that none of them have no contention
    although they belong to different CUDA streams. That's because of the GPU scheduler
    being aware of the execution requests, and serving the first. However, when the
    current task is finished, the streaming multiprocessor can serve the next kernel
    in the other CUDA stream, since they have remained occupancies.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of all the multiple CUDA stream operations, we need to synchronize
    the host and GPU to confirm that all the CUDA operations on the GPU have finished.
    To do this, we used `cudaDeviceSynchronize()` right after the loop. This function
    can synchronize all the selected GPU operations at the calling point.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the synchronization task, we can replace the `cudaDeviceSynchronize()`
    function with the following code. To do this, we also have to change the private
    member `_stream` to be public:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This can be used when we need to provide specific operations from a single host
    thread along with the stream after each stream finishes. But, this is not a good
    operational design since the following operation cannot avoid syncing with the
    other streams.
  prefs: []
  type: TYPE_NORMAL
- en: 'What about using `cudaStreamSynchronize()` in the loop? In this case, we cannot
    perform the overlapping operation that we did before. The following screenshot
    shows the situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c3c1e57-e8fb-4393-9809-7e27ea4e2afc.png)'
  prefs: []
  type: TYPE_IMG
- en: This is because `cudaStreamSynchronize()` will synchronize every iteration and
    the application will serialize all the CUDA executions, accordingly. In this situation,
    the execution time was measured as 41.521 ms, which is about 40% slower than the
    overlapped execution time.
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA callback function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **CUDA callback function** is a callable host function to be executed by
    the GPU execution context. Using this, the programmer can specify the host-desired
    host operation following the GPU operations.
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA callback function has a special datatype named `CUDART_CB`, so it should
    be defined using this type. With this type, the programmers can specify which
    CUDA stream launches this function, pass the GPU error status, and provide user
    data.
  prefs: []
  type: TYPE_NORMAL
- en: To register the callback function, CUDA provides `cudaStreamAddCallback()`.
    This function accepts CUDA streams, the CUDA callback function, and its parameters,
    so that the specified CUDA callback function can be called from the specified
    CUDA stream and obtain user data. This function has four input parameters, but
    the last one is reserved. So, we don't use that parameter and it remains as `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's enhance our code to use the callback function and output an individual
    stream's performance. Duplicate the source code if you want to separate the previous
    work and this.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, place these function declarations into the `private` area of the `Operator`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Callback()` function will be called after each stream''s operation has
    finished, and the `print_time()` function will report the estimated performance
    using the host side timer, `_p_timer`. The functions'' implementations are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To have the right timer operations, we need a timer initializer at the `Operator` class''s
    constructor and a timer destroyer at the class''s terminator. Also, we have to
    start the timer at the beginning of the `Operator::async_operation()` function.
    Then, insert the following code block at the end of the function. This allows
    the CUDA stream to call the host-side function when it finishes the previous CUDA
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s compile and see the execution result. You must use your GPU''s
    compute capability version number for the `gencode` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the execution result of our update:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see the estimated execution time along with the CUDA stream. The
    callback function estimates its sequence''s execution time. Since there is overlapping
    with other streams and delays for later CUDA streams, we can see the prolonged
    execution time for the late CUDA streams'' execution time. We can confirm those
    elapsed times by matching with the profiled result, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9cf56a50-a62a-4ccc-839f-a86dbf6526c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Although their measured elapsed time is extended along with the stream execution,
    the delta between the streams is regular and we can see these operations from
    the profiled output.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we can conclude that we can write host code that can operate right
    after each individual CUDA stream's operation has finished. And, this is advanced
    one against to synchronize each stream from the main threads.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA streams with priority
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, all CUDA streams have equal priority so they can execute their operations
    in the right order. On top of that, CUDA streams also can have priorities and
    can be superseded by a higher prioritized stream. With this feature, we can have
    GPU operations that meet time-critical requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Priorities in CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use streams with priorities, we need to obtain the available priorities
    from the GPU first. We can obtain these using the `cudaDeviceGetStreamPriorityRange()`
    function. Its output is two numeric values, which are the lowest and highest priority
    values. Then, we can create a priority stream using the `cudaStreamCreaetWithPriority()`
    function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: There are two additional parameters we should provide. The first one determines
    the created streams' behavior with the default stream. We can make the new stream
    be synchronous with the default stream, like normal streams, using `cudaStreamDefault`.
    On the other hand, we can make it operate concurrently with the default stream
    using `cudaStreamNonBlocking`. Lastly, we can set the stream's priority within
    the priority range. In CUDA programming, the lowest value has the highest priority.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we can confirm whether the GPU supports this using the following code.
    But, we don''t have to worry too much about this because the priority stream has
    been available since CUDA compute capability 3.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: If the device properties value is `0`, we should stop the application since
    the GPU does not support the stream priorities.
  prefs: []
  type: TYPE_NORMAL
- en: Stream execution with priorities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will reuse the previous multi-stream application with the callback.
    In this code, we can see that the streams can operate in order, and we will see
    how this order can be changed with priorities. We will make a derived class from
    the `Operator` class, and it will handle the priority of the stream. So, we change
    the member variable stream''s protection level from the private member to the
    protected member. And, the constructor can create the stream optionally since
    that can be done by the derived class. The change is shown with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The derived class, `Operator_with_priority`, will have a function that creates
    a CUDA stream manually with the given priority. That class configuration is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As we handle each stream''s operation with the class, we will update the `ls_operator` creation
    code to use the `Operator_with_priority` class in `main()`, to use the class we
    wrote before, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'As we update the class, this class does not create streams before we request
    it to do so. As we discussed before, we need to obtain the available range of
    priority of the GPU using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s create each operation to have different prioritized streams. To
    ease this task, we will let the last operation have the highest stream, and see
    how preemption in CUDA streams works. This can be done with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we will execute each operation, as we did previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: To have the proper output, let's synchronize the host and GPU using the `cudaDeviceSynchronize()`
    function. And, finally, we can terminate the CUDA streams. The streams with priorities
    can be terminated with the `cudaStreamDestroy()` function, so we have nothing
    to do in this application as we already did what was needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s compile the code and see the effect. As always, you need to provide
    the right GPU compute capability version to the compiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'And, the following shows the output of the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'From the output, you can see that the operation order has been changed. Stream
    3 precedes stream 1 and stream 2\. The following screenshot shows the profile
    result of how it changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84f4080c-7d72-4b4c-ad7c-b0192fe69d97.png)'
  prefs: []
  type: TYPE_IMG
- en: In this screenshot, there was preemption with the second CUDA stream (Stream
    19 in this case) by the prioritized-last CUDA stream (Stream 21), so that Stream
    19 could finish its work after Stream 21 finished execution. Note that the order
    of data transfer does not change according to this prioritization.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel execution time estimation using CUDA events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous GPU operation time estimation has one limitation in that it cannot
    measure the kernel execution time. That's because we used timing APIs on the host
    side. So, we need to have synchronized with the host and GPU to measure the kernel
    execution time, and this is impractical considering the overhead and impact on
    the application's performance.
  prefs: []
  type: TYPE_NORMAL
- en: This can be resolved using CUDA events. The CUDA event records GPU-side events
    along with the CUDA stream. CUDA events can be events based on the GPU states
    and record the scheduled timing. Using this, we can trigger the following operations
    or estimate the kernel execution time. In this section, we will cover how we can
    measure the kernel execution time using CUDA events.
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA event is managed with the `cudaEvent_t` handle. We can create a CUDA
    event handle using `cudaEventCreate()` and terminate it with `cudaEventDestroy()`. To
    record event time, you can use `cudaEventRecord()`. Then, the CUDA event handle
    records the event time for the GPU. This function also accepts CUDA streams, so
    that we can enumerate the event time to the specific CUDA stream. After obtaining
    the start and end events of kernel execution, you can obtain the elapsed time
    using `cudaEventElapsedTime()` with millisecond units.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's cover how we can use CUDA events using those APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Using CUDA events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will reuse the previous multi-stream application from the
    second section. Then, we enumerate each GPU kernel''s execution time using CUDA
    events:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a simple vector addition kernel function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: This code has an iteration that extends the kernel execution time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will use the following snippet to measure the kernel execution time.
    To compare the result, we will use the host side''s timer and CUDA event:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in this code, we can record the CUDA event right after the kernel
    calls. However, the timer requires synchronization between the GPU and the host.
    For the synchronization, we use the `cudaEventSynchronize(stop)` function because
    we can also make the host thread synchronize with the event. Meanwhile, this code
    only covers handling the timing resources and the kernel execution. But, you also
    have to initialize the required memory to make it work.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the kernel execution, let''s write code that reports the execution time
    from each timing resource:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will finalize our application by terminating the timing resources,
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s compile and see the output using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we can measure kernel execution time using CUDA events. However,
    the measured times have gaps between the CUDA event and the timer. We can use
    NVIDIA Profiler to verify which provides more accurate information. When we use
    the `# nvprof ./cuda_event` command, the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/533f3b7e-846e-4f23-a97d-b277876a05c4.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, CUDA events provide accurate results compared to measuring from
    the host.
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of using CUDA events is that we can measure multiple kernel
    execution times simultaneously with multiple CUDA streams. Let's implement an
    example application and see its operation.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple stream estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `cudaEventRecord()` function is asynchronous to the host. In other words,
    there is no synchronization to measure the kernel execution time to the example
    code. To have synchronization with the event and the host, we need to use `cudaEventSynchronize()`.
    For example, kernel function prints can be placed ahead of asynchronous data transfers
    from the device to the host by the synchronization effect, when we place this
    function right after `cudaEventRecord(stop)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also useful to measure kernel execution time in multiple CUDA stream
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s apply this to the multiple CUDA streams overlapping recipe code in the `04_stream_priority` example
    code. Update the code with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will define the `print_time()` function we included at this time,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, insert the `cudaEventRecord()` function calls at the beginning and end
    of `Operator::async_operation()`, as in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: For this function, there is a challenge to place synchronization at the end
    of the function. Try this after finishing this section. This will impact the application's
    behavior. It is recommended to try to explain the output to yourself, and then
    confirm that using the profiler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s compile and see the execution time report, as follows; it shows
    similar performance to the previous executions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: In this output, we also can see each kernel's execution time thanks to the CUDA
    event. From this result, we could see that the kernel execution time is prolonged,
    as we saw in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to learn more about the features of CUDA events, check NVIDIA''s
    CUDA event documention: [https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will cover some other aspects of managing CUDA grids. The first item
    is dynamic parallelism, which enables kernel calls from the GPU kernel function.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA dynamic parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**CUDA dynamic parallelism** (**CDP**) is a device runtime feature that enables
    nested calls from device functions. These nested calls allow different parallelism
    for the child grid. This feature is useful when you need a different block size
    depending on the problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding dynamic parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like normal kernel calls from the host, the GPU kernel call can make a kernel
    call as well. The following sample code shows how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in these functions, we need to make sure which CUDA thread makes
    kernel calls to control the amount that the grid creates. To learn more about
    this, let's implement the first application using this.
  prefs: []
  type: TYPE_NORMAL
- en: Usage of dynamic parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our dynamic parallelism code will create a parent grid, and that parent will
    create a couple of child grids:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will write the `parent_kernel()` function and the `child_kernel()`
    function using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in this code, the parent kernel function creates child kernel
    grids as the number of blocks. And, the child grids increment the designated memory
    by `1` to mark their operation. After the kernel execution, the parent kernel
    waits until all the child grids finish their jobs using the `cudaDeviceSynchronize()` function.
    When we make synchronization, we should determine the range of the synchronization.
    If we need to synchronize at the block level, we should choose `__synchthread()`
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Write the `main()` function using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: As discussed earlier, we will create child grids along with the number of blocks.
    So, we will execute the parent kernel function with a grid size of `4`, whereas
    the block size is `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compile a CDP application, we should provide the `-rdc=true` option to the
    `nvcc` compiler. Hence, the command to compile the source is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s profile this application to understand its operation. The following screenshot
    shows how this nested call works:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/765aef65-f755-4eb1-9697-ff07d3efb6d7.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see in this screenshot, the parent kernel creates a child grid, and
    we can see their relationship with the right angle mark at the left panel. Then,
    the parent grid (parent_kernel) waits for its execution until the child to finish
    its job. CUDA does not support CDT profiling for SM70 (Volta architecture) at
    this time, so I have used Tesla P40 to obtain this output.
  prefs: []
  type: TYPE_NORMAL
- en: Recursion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the benefits of dynamic parallelism is that we can create a recursion.
    The following code shows an example of a recursive kernel function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there is not much difference from the previous dynamic parallelism
    kernel function. However, we should use this with caution considering the resource
    usage and limitations. In general, dynamic parallel kernels can conservatively
    reserve up to 150 MB of device memory to track pending grid launches and the parent
    grid status by synchronizing on a child grid launch. In addition, the synchronization
    must be carefully done across multiple levels, while the depth of nested kernel
    launches is limited to 24 levels. Finally, the runtime that controls nested kernel
    launches can affect overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to learn about the restrictions and limitations of dynamic parallelism,
    see the following programming guide: [https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#implementation-restrictions-and-limitations](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#implementation-restrictions-and-limitations).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover its application for quick sorting implementation in [Chapter
    7](71d77c43-0064-491e-9b43-307a05bd6555.xhtml), *Parallel Programming Patterns
    in CUDA*. To learn more about dynamic parallelism, see the following documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://devblogs.nvidia.com/cuda-dynamic-parallelism-api-principles/](https://devblogs.nvidia.com/cuda-dynamic-parallelism-api-principles/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://on-demand.gputechconf.com/gtc/2012/presentations/S0338-New-Features-in-the-CUDA-Programming-Model.pdf](http://on-demand.gputechconf.com/gtc/2012/presentations/S0338-New-Features-in-the-CUDA-Programming-Model.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grid-level cooperative groups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA
    Thread Programming,* CUDA provides cooperative groups. Cooperative groups can
    be categorized by their grouping targets: warp-level, block-level, and grid-level
    groups. This recipe covers grid-level cooperative groups, and looks at how cooperative
    groups handle the CUDA grid.'
  prefs: []
  type: TYPE_NORMAL
- en: The most prominent benefit of the cooperative group is the explicit synchronization
    of the target parallel object. Using the cooperative group, the programmer can
    design their application to synchronize CUDA parallel objects, thread blocks,
    or grids explicitly. Using the block-level cooperative group covered in [Chapter
    3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA Thread Programming*, we
    can write more readable code by specifying which CUDA threads or blocks need to
    synchronize.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding grid-level cooperative groups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since version 9.0, CUDA provides another level of cooperative groups, working
    with grids. To be specific, there are two grid-level cooperative groups: `grid_group`
    and `multi_grid_group`. Using these groups, the programmer can describe the grid''s
    operation to sync on a single GPU or multiple GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will explore the functionality of `grid_group`, which can
    synchronize grid with reduction problems, as mentioned in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml),
    *CUDA Thread Programming*, regarding the previous reduction design based on block-level
    reduction. Each thread block produces its own reduction results and stores them
    into the global memory. Then, another block-wise reduction kernel launches until
    we obtain a single reduced value. That''s because finishing kernel operation can
    guarantee that the next **reduction** kernel to read a reduced value from the
    multiple thread blocks. Its design is described by the diagram on the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e70bce0c-8537-4ad1-a5c6-82cc22c93cb0.png)'
  prefs: []
  type: TYPE_IMG
- en: On the other hand, grid-level synchronization enables another kernel design,
    which synchronizes block-wise **reduction** results internally, so that the host
    can have only a single kernel call to obtain the reduction **result**. In cooperative
    groups, `grid_group.sync()` provides such functionality, so we can write the reduction
    kernel without kernel-level iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the `grid_group.sync()` function, we need to call the kernel function
    using the `cudaLaunchCooperativeKernel()` function. Its interface design is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: So, its usage is the same as the `cudaLaunchKernel()` function, which launches
    a kernel function.
  prefs: []
  type: TYPE_NORMAL
- en: To make all the thread blocks in `grid_group` synchronize, the total number
    of active thread blocks in the grid should not exceed the number of maximum active
    blocks for the kernel function and the device. The maximum active block size on
    a GPU is a multiplication of the maximum amount of active blocks per SM and the
    number of streaming multiprocessors. The violation of this rule can result in
    deadlock or undefined behavior. We can obtain the maximum amount of active thread
    blocks of a kernel function per SM using the `cudaOccupancyMaxActiveBlocksPerMultiprocessor()`
    function, by passing the kernel function and block size information.
  prefs: []
  type: TYPE_NORMAL
- en: Usage of grid_group
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s apply `grid_group` to the parallel reduction problem and see how
    the GPU programming can be changed:'
  prefs: []
  type: TYPE_NORMAL
- en: We will reuse the host code from the previous parallel reduction code in `03_cuda_thread_programming/07_cooperative_groups`.
    In other words, we will change the GPU's operation with small changes in the host
    code. You also can use the code in the `07_grid_level_cg` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's write some block-level reduction code. When we have grid-level cooperative
    groups, all the thread blocks must be active. In other words, we cannot execute
    multiple thread blocks than the GPU-capable active blocks. So, this reduction
    will accumulate the input data first to cover all the data with a limited number
    of thread blocks. Then, it will conduct the parallel reduction at the block level
    as we covered in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA
    Thread Programming*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code shows its implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s write a kernel function that executes the block-wise reduction
    considering the number of active blocks and `grid_group`. In this function, we
    will call the block-level reduction code and synchronize them at the grid level.
    Then, we will perform parallel reduction from the outputs as we covered in [Chapter
    3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA Thread Programming*. The
    following code shows its implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will implement the host code that calls the kernel function with
    the available active thread block dimension. To do this, this function uses the
    `cudaoccupancyMaxActiveBlocksPerMultiprocessor()` function. Also, the grid-level
    cooperative group requires us to call the kernel function via the `cudaLaunchCooperativeKernel()`
    function. You can see the implementation here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Now, make sure that the host function can be called from the `reduction.cpp`
    file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, let''s compile the code and see its operation. The following shell command
    compiles the code and executes the application. The compute capability should
    be equal to or greater than `70`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The output performance is far behind what we saw in the final result of [Chapter
    3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA Thread Programming*. As
    the `block_reduction()` function uses high memory throughput at the beginning,
    it is highly memory bounded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbd18a08-d2a1-4bc1-a9b3-647c67a72240.png)'
  prefs: []
  type: TYPE_IMG
- en: The major impact factor is that we can only use the active thread blocks. So,
    we cannot hide the memory access time. Actually, the usage of `grid_group` has
    other purposes, such as graph search, genetic algorithms, and particle simulation,
    which requires us to keep the states active for long times for performance.
  prefs: []
  type: TYPE_NORMAL
- en: This grid-level synchronization can provide more benefits to performance and
    programmability. As this enables the kernel to synchronize itself, we can make
    the kernel iterate itself. So, it is useful to solve the graph search, genetic
    algorithms, and practical simulations. To learn more about cooperative groups
    in `grid_groups`, please refer to the documentation provided at [http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf](http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: CUDA kernel calls with OpenMP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To enlarge the concurrency of the application, we can make kernel calls from
    the host's parallel tasks. OpenMP, for instance, provides easy parallelism of
    the multi-core architecture. This recipe covers how CUDA can operate OpenMP.
  prefs: []
  type: TYPE_NORMAL
- en: OpenMP and CUDA calls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenMP uses a fork-and-join model of parallelism to target multi-core CPUs.
    The master thread initiates the parallel operations and creates worker threads.
    The host threads operate their own jobs in parallel and join after finishing their
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Using OpenMP, CUDA kernel calls can be executed in parallel with multiple threads.
    This helps the programmer to not have to maintain individual kernel calls, instead
    allowing them to have kernel executions depend on the host thread's index.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following OpenMP APIs in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '`omp_set_num_threads()` sets a number of worker threads that will work in parallel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`omp_get_thread_num()` returns an index of worker threads so that each thread
    can identify their task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#pragma omp parallel {}` specifies a parallel region that will be covered
    by the worker threads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's write some code in which OpenMP calls a CUDA kernel function.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA kernel calls with OpenMP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will implement a multi-stream vector add application that
    uses OpenMP. To do this, we will modify the previous version and see the difference:'
  prefs: []
  type: TYPE_NORMAL
- en: To test OpenMP with CUDA, we will modify the code from the `03_cuda_callback` directory.
    We will modify the body of the `main()` function, or you can use the provided
    sample code placed in the `08_openmp_cuda` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let''s include the OpenMP header file and modify the code. To use OpenMP
    in the code, we should use `#include <omp.h>`. And, we will update the code that
    iterates `for` each stream to use OpenMP:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the code with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Whenever you execute this application, you will see that each stream finishes
    their job out of order. Also, each stream shows a different time. That's because
    OpenMP can create multiple threads, and the operation is determined at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand its operation, let''s profile the application. The following
    screenshot shows the profiled timeline of the application. This can be different
    from yours due to the scheduling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50012166-e450-4d38-8ebf-a44a55ad9844.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in this screenshot, you will be able to see that the data transfer
    has reversed compared to Stream 17\. For this reason, we can see that the second
    stream could finish its job at last.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Process Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The GPU is capable of executing kernels from concurrent CPU processes. However,
    by default, they are only executed in a time-sliced manner even though each kernel
    doesn''t fully utilize GPU compute resources. To address this unnecessary serialization,
    the GPU provides **Multi-Process Service** (**MPS**) mode. This enables different
    processes to execute their kernels simultaneously on a GPU to fully utilize GPU
    resources. When it is enabled, the `nvidia-cuda-mps-control` daemon monitors the
    target GPU and manages process kernel operations using that GPU. This feature
    is only available on Linux. Here, we can see the MPS in which multiple processes
    share the same GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f97d170-6740-41d4-aa8b-7552745e9838.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, each process has a part that runs in parallel in the GPU (green
    bars), while some part runs on the CPU (blue bars). Ideally, you would need both
    the blue bars and green bars to get the best performance. This can be made possible
    by making use of the MPS feature, which is supported by all the latest GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that multiple MPI processes running on the same GPU are beneficial
    when one MPI process is unable to saturate the whole GPU and a significant part
    of the code is also running on the CPU. If one MPI process utilizes the whole
    GPU, even though the CPU part (blue bar) will reduce, the green bar time will
    not as the GPU is completely utilized by one MPI process. The other MPI processes
    will access the GPU one after another in a time-sliced manner based on the GPU
    architecture. This is similar to the launching-concurrent-kernels scenario. If
    one kernel utilizes the whole GPU, then the other kernel will either wait for
    the first kernel to finish or be time-sliced.
  prefs: []
  type: TYPE_NORMAL
- en: 'The good thing about this is that no changes need to be made to the application
    to make use of MPS. The MPS process runs as a daemon, as shown in the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: After running this command, all the processes submit their commands to the MPS
    daemon, which takes care of submitting the CUDA commands to GPU. For the GPU,
    there is only one process accessing the GPU (MPS Daemon) and hence multiple kernels
    can run concurrently from multiple processes. This can help overlap memory copies
    from one process with kernel executions from other MPI processes.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Message Passing Interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Message Passing Interface** (**MPI**) is a parallel computing interface which
    enables to trigger multiple processes across the computing units - CPU cores,
    GPU, and nodes. The typical dense multi-GPU system contains 4-16 GPUs, while the
    number of CPU cores ranges between 20-40 CPUs. In MPI-enabled code, some parts
    of the application run in parallel as different MPI processes on multiple cores.
    Each MPI process will call CUDA. It is very important to understand mapping an
    MPI process to the respective GPU. The easiest mapping is 1:1, that is, each MPI
    process gets exclusive access to the respective GPU. Also, we can ideally map
    multiple MPI processes to a single GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To have the multi-processes application scenario to a single GPU we will use
    MPI. To use MPI, you need to install OpenMPI for your system. Follow these steps
    to install OpenMPI for Linux. This operation has been tested on Ubuntu 18.04,
    so this can vary if you use another distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's implement an application that can work with MPI and CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an MPI-enabled application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make an application work with MPI, we need to put some code that can understand
    MPI commands in the application:'
  prefs: []
  type: TYPE_NORMAL
- en: We will reuse the OpenMP sample code, so copy the `openmp.cu` file in the `08_openmp_cuda`
    directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Insert the `mpi` header `include` statement at the beginning of the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Insert the following code right after the stopwatch has been created in the
    `main()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Slice the required memory size by the number of processes, after the code mentioned
    in step 3, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to make each thread to report their process which they belong. Let''s
    update the `printf()` function in the parallel execution code block, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: At the end of `main()`, place the `MPI_Finalize()` function to close the MPI
    instances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compile the code with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: You must use your GPU's compute capability version number for the `gencode`
    option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Test the compiled application using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, test MPI execution using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Enabling MPS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enabling MPS in GPUs requires some modification of the GPU operation mode. But,
    you need to have a GPU architecture later than the Kepler architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s follow the steps required to enable MPS as bellow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable MPS mode using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, you can use the `make enable_mps` command for this recipe sample code,
    which is pre-defined in `Makefile`. Then, we can see the updated compute mode
    from the `nivida-smi` output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/329cace9-b5f9-43e7-8716-c2aa40c0547f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, test MPI execution with MPS mode using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, each process' elapsed time has reduced compared to the previous
    executions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s recover the original mode. To disable MPS mode, use the following
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Or, you can use the `make disable_mps` command for this recipe sample code,
    which is pre-defined in `Makefile`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about MPS, please use the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://on-demand.gputechconf.com/gtc/2015/presentation/S5584-Priyanka-Sah.pdf](http://on-demand.gputechconf.com/gtc/2015/presentation/S5584-Priyanka-Sah.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling an MPI application and understanding MPS operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using MPI, the kernel from multiple processes can share GPU resources at the
    same time, which enhances overall GPU utilization. Without MPS, the GPU resources
    are shared inefficiently due to time-sliced sharing and the context-switching
    overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the timeline profile result of multiple processes
    without MPS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13ccf0f1-20b5-40be-8738-7b460f2f5cac.png)'
  prefs: []
  type: TYPE_IMG
- en: In this profile, we can see that two CUDA contexts share a GPU, and kernel execution
    times are prolonged due to the time-sharing between the contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, MPS mode manages the kernel execution request, so all the
    kernel executions are launched as though using a single process. The following
    screenshot shows kernel execution with MPS mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37c34d58-26dd-41ce-8289-65c68a535761.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, only one CUDA stream resides on a GPU and controls all the CUDA
    streams. Also, all the kernel execution times are stabilized and the total elapsed
    time is reduced using MPS. In conclusion, using MPS mode benefits overall performance
    for multiple GPU processes and shares GPU resources.
  prefs: []
  type: TYPE_NORMAL
- en: '`nvprof` supports dumping profiler information for multiple MPI processes in
    a different file. For example, for an Open MPI-based application, the following
    command will dump profiling information in multiple files, each with a unique
    name based on the MPI process rank:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, you can use the following command for the sample recipe code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Then, you will get two `nvvp` files for each process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will review these `nvvp` files using NVIDIA Visual Profiler with the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the File | Import menu to create a profiling session by importing the
    `nvvp` files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3f802cd1-25da-4e02-b172-9e41b730799f.png)'
  prefs: []
  type: TYPE_IMG
- en: In Windows or Linux, the shortcut key is *Ctrl* + *I*, and OSX uses *command*
    + *I*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then click the Next button after selecting Nvprof from the list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2f84e5d6-9933-4c8d-81d1-e6a6bd03545e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the Nvprof option, select Multiple processes and click Next >:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e70799cb-c6f8-4e70-abb6-f8704a433c83.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From Import Nvprof Data, click the Browse... button, and select `nvvp` files,
    which are generated by `nvprof`. To profile an application with multi-process,
    you need to import `nvvp` files as there are processes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1582fbfc-ddd9-42b4-882e-23772b33e767.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click Finish, then NVIDIA Visual Profiler shows profiled results in the timeline
    view, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/75dd51b7-edb6-4d14-80d3-a73b6725191d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Please note that only the synchronous MPI calls will be annotated by `nvprof`.
    In the case of an asynchronous MPI API being used, other MPI-specialized profiling
    tools need to be used. Some of the most famous tools include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TAU**: TAU is a performance profiling toolkit and is currently maintained
    by the University of Oregon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vampir**: This is a commercially available tool and provides good scalability
    to hundreds of MPI processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intel VTune Amplifier**: Another option when it comes to commercial tools
    is Intel VTune Amplifier. It is one of the best tools available and can be used
    for MPI application analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The latest CUDA Toolkits also allow the MPI API to be annotated. For this,
    the `--annotate-mpi` flag needs to be passed to `nvprof`, as shown in the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Kernel execution overhead comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For iterative parallel GPU tasks, we have three kinds of kernel execution methods:
    iterative kernel calls, having an inner loop, and having recursions using dynamic
    parallelism. The best operation is determined by the algorithm and the application.
    But, you also may consider kernel execution options among them. This recipe helps
    you to compare those kernel execution overheads and review their programmability.'
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, let's determine which operation we will test. This recipe will
    use a simple SAXPY operation. This helps us to focus and make iterative execution
    code. In addition, the operation control overhead will become heavier as the operation
    gets simpler. But, you can try with any other operation you want, of course.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing three types of kernel executions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following steps cover the performance comparison of three different iterative
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Create and navigate the `10_kernel_execution_overhead` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Write the `simple_saxpy_kernel()` function with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the `iterative_saxpy_kernel()` function with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the `recursive_saxpy_kernel()` function with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the host code that launches those CUDA kernel functions. At first, we
    will have an iterative function call of the `simple_saxpy_kernel()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Secondly, we will call the `iterative_saxpy_kernel()` kernel function, which
    has an iterative loop inside:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we will call the `recursive_saxpy_kernel()` kernel function, which
    calls itself in a recursive manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: The number of loops is smaller than or equal to 24, since the maximum recursion
    depth is 24\. Other than simple loop operations, you do not have to place a loop
    operation at the host since it is already defined in the kernel code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile the code using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: You must use your GPU's compute capability version number for the `gencode`
    option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Test the compiled application. This result was measured using Tesla P40 because
    CUDA 9.x does not support **CUDA Dynamic Parallelism** (**CDP**) profile for Volta
    GPUs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Comparison of three executions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the result, we can confirm that the inner loop is the fastest method for
    the iterative operation. The following screenshot shows a profiled result of this
    sample application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0040c495-f138-4acc-bf8a-c2e2b02ebbda.png)'
  prefs: []
  type: TYPE_IMG
- en: The iterative kernel call shows the kernel launch overhead for each kernel call.
    The GPU needs to fetch all the required data from the device memory, and needs
    to schedule the GPU resources, and so on. On the other hand, the inner loop kernel
    shows one packed operation because all the required resources are pre-located
    and there's no need to reschedule its execution. The recursive kernel operation
    shows the most prolonged execution time due to the dynamic parallelism limitations
    we discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: In general, using the approach with the least overhead is recommended. However,
    it is hard to say which kernel call design is superior to the others, since there's
    more to the algorithm and its problems than what we've covered here. For instance,
    CDP used to enhance parallelism in certain cases, such as for GPU trees and searches.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered several kernel execution mechanisms. We covered
    what CUDA streams are, and how to use them to execute multiple kernel functions
    concurrently. By utilizing the asynchronous operation between the host and the
    GPU, we have learned that we can hide the kernel execution time by making the
    pipelining architecture with data transfer and kernel executions. Also, we can
    make a CUDA stream call the host function using the callback function. We can
    create a prioritized stream, and confirm its prioritized execution, too. To measure
    the exact execution time of a kernel function, we have used CUDA events, and we
    also learned that CUDA events can be used to synchronize with the host. In the
    last section, we also discussed the performance of each kernel execution method.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also covered other kernel operation models: dynamic parallelism and grid-level
    cooperative groups. Dynamic parallelism enables kernel calls inside the kernel
    function so we can make recursive operations with that. The grid-level cooperative
    group enables versatile grid-level synchronization, and we discussed how this
    feature can be useful in a specific area: graph search, genetic algorithms, and
    particle simulations.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, we expanded our coverage to the host. CUDA kernels can be called from
    multiple threads or multiple processes. To execute multiple threads, we used OpenMP
    with CUDA and discussed its usefulness. We used MPI to simulate multiple process
    operations, and could see how MPS benefits overall application performance.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in this chapter, choosing the right kernel execution model is an important
    topic, as is thread programming. This can optimize application execution time.
    Now, we will expand our discussion to multi-GPU programming to solve big problems.
  prefs: []
  type: TYPE_NORMAL
