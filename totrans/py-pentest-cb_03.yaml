- en: Web Scraping with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading web pages with Python scripts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing the user agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a regular expression to get the information from the downloaded web pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requesting and downloading dynamic website pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic GET requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web scraping is the process of automating the extraction of data from the web
    into a format so that you can easily analyze or make use of it. The `urllib` Python
    module helps you to download data from web servers.
  prefs: []
  type: TYPE_NORMAL
- en: Download web pages with Python scripts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To download web pages from the web server, the `urllib` module, which is part
    of the standard Python library, can be usedÂ `urllib` includes functions for retrieving
    data from URLs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn the basics, we could use the Python interactive terminal. Type `python`
    in your Terminal window and press *Enter*. This will open up the Python (Python
    2.x) interactive terminal.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are some differences in commands for doing this in Python 2.x and Python
    3.x, mainly with the `print` statements. So please note the difference in the
    syntax. This will be helpful in our upcoming recipes.
  prefs: []
  type: TYPE_NORMAL
- en: With Python 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, import the required module, `urllib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With the `urlopen` method, you can download the web page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can read the file like a returned object with the `read` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Close the object when it''s done:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can print the HTML, which is in a string format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It is very easy to update the program to write the contents of the source string
    to a local file on your computer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: With Python 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Python 3 both `urllib` and `urllib2` are part of the `urllib` module, so
    there is some difference in using `urllib`. Also, the `urllib` package contains
    the following modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '`urllib.request`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`urllib.error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`urllib.parse`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`urllib.robotparser`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `urllib.request`module is used for opening and fetching URLs with Python
    3:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First import the `urllib.request` module from `urllib` packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the web page with the `urlopen` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the object with the `read` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Close the object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the source:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You can write the contents of the source string to a local file on your computer
    as follows. Make sure that the output file is in binary mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Python 2 modules `urllib` and `urllib2` help to do URL-request-related stuff,
    but both have different functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: '`urllib` provides the `urlencode` method, which is useful in generating `GET`
    requests. However, `urllib2` doesn''t support the `urlencode` method. Also, `urllib2`
    can accept the request object and modify the headers for a URL request, but `urllib`
    can only accept the URL and is not capable of modifying the headers in it.'
  prefs: []
  type: TYPE_NORMAL
- en: Changing the user agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many websites use a user agent string to identify the browser and serve it accordingly.
    As we are using `urllib` to access the website, it won't recognize this user agent
    and may behave in strange ways or fail. So, in this case, we could specify the
    user agent for our requests.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use custom user agent string in the request as following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the required modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then define the user agent we plan to specify for the request:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the headers for the request:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the request as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Request the web page with `urlopen`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Downloading files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can make use of the `requests` Python module to download files. The `requests`
    module is a **simple and easy-to-use** HTTP library in Python that has various
    applications. Also, it helps establish the seamless interaction with the web services.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First of all, you have to install the `requests` library. This can be done
    using `pip` by typing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s try downloading a simple image file with the `requests` module. Open
    Python 2:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, import the `requests` library first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an HTTP response object by passing a URL to the `get` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now send the HTTP request to the server and save it to a file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If it's a large file, the `response.``content` will be a large string and won't
    be able to save all the data in a single string. Here, we use the `iter_content`
    method to load the data in chunks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can create an HTTP response object as a `stream`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, send the request and save the file with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This will work in Python 3\. Also, make sure you install the required libraries
    in the Python 3 environment.
  prefs: []
  type: TYPE_NORMAL
- en: Using a regular expression to get the information from the downloaded web pages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **regular expression** (**re**) module helps to find specific patterns of
    text from the downloaded web page. Regular expressions can be used to parse data
    from the web pages.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we can try to download all images in a web page with the help
    of the regular expression module.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this, we can write a Python script that can download all JPG images in
    a web page:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a file named `download_image.py` in your working directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open this file in a text editor. You could use sublime text3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As usual, import the required modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Download the web page as we did in the previous recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, iterate each line in the downloaded web page, search for image URLs, and
    download them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The first *for loop* iterates through the lines in the downloaded web page.
    The second *for loop* searches each line for the image URLs with the regular expression
    pattern.
  prefs: []
  type: TYPE_NORMAL
- en: If the pattern is found, the filename of the image is extracted with the `urlsplit()`
    method in the `urlparse` module. Then, we download the image and save it to the
    local system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same script can be rewritten to Python 3 with minimal changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In Python 3, the request and `urlparse` modules are combined with `urllib` as
    `urllib.request` and `urllib.parse`. With regular expression patterns, we could
    parse a lot of useful information for a web page.
  prefs: []
  type: TYPE_NORMAL
- en: You could learn more about the regular expression module at [https://docs.python.org/3.7/library/re.html](https://docs.python.org/3.7/library/re.html).
  prefs: []
  type: TYPE_NORMAL
- en: Requesting and downloading dynamic website pages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In case of websites having forms or receiving user inputs, we have to submit
    a `GET` request or a `POST` request. Now let's try creating `GET` requests and
    post request with Python. The query string is the method for adding key-value
    pairs to a URL.
  prefs: []
  type: TYPE_NORMAL
- en: Escaping invalid characters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, what will happen if we remove the try catch block in
    the last step?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The script will fail after a few requests due to the error in the URL format.
    Some extra characters appeared in the URL and this failed the `urllib` request.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's impossible to remember which characters are invalid and manually escape
    them with percent signs, but the built-in Python module `urllib.parse` has the
    required methods to solve this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can try fixing this by escaping/URL encoding the request. Rewrite the
    script as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Dynamic GET requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we know that Python can programmatically download a website as long as we
    have the URL. If we have to download multiple pages that only differ in the query
    string, then we can write a script to do this without repeatedly rerunning the
    script, and instead download everything we need in one run.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out this URL-- [https://www.packtpub.com/all?search=&offset=12&rows=&sort=](https://www.packtpub.com/all?search=&offset=12&rows=&sort=).
    Here, the query string variable that defines the page number (*offset**)* is multiples
    of 12:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To download all the images in all of these pages, we can rewrite the previous
    recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the URL and query string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate the offset through multiples of 12:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
