- en: Building Your First Web Scraping Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading web pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing HTML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crawling the web
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subscribing to feeds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing web APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interacting with forms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Selenium for advanced interaction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing password-protected pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speeding up web scraping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The internet, and the **WWW** (**World Wide Web**), is probably the most prominent
    source of information today. Most of that information is retrievable through the
    HTTP protocol. **HTTP** was invented originally to share pages of hypertext (hence
    the name **HyperText Transfer Protocol**), which started the WWW.
  prefs: []
  type: TYPE_NORMAL
- en: This operation is very familiar, as it is what happens in any web browser. But
    we can also perform those operations programmatically to automatically retrieve
    and process information. Python has included in the standard library an HTTP client,
    but the fantastic `requests` module makes it very easy. In this chapter, we will
    see how.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading web pages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The basic ability to download a web page involves making an HTTP `GET` request
    against a URL. This is the basic operation of any web browser. Let''s quickly
    recap the different parts of this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the HTTP protocol.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the `GET` method, which is the most common HTTP method. We'll see more
    in the *Accessing web APIs* recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: URL describing the full address of the page, including the server and the path.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That request will be processed by the server and a response will be sent back.
    This response will contain a **status code**, typically 200 if everything went
    fine, and a body with the result, which will normally be text with an HTML page.
  prefs: []
  type: TYPE_NORMAL
- en: Most of this is handled automatically by the HTTP client used to perform the
    request. We'll see in this recipe how to make a simple request to obtain a web
    page.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP requests and responses can also contain headers. Headers contain extra
    information, such as the total size of the request, the format of the content,
    the date of the request, and what browser or server is used.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the fantastic `requests` module, getting web pages is super simple. Install
    the module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We'll download the page at [http://www.columbia.edu/~fdc/sample.html](http://www.columbia.edu/~fdc/sample.html) because
    it is a straightforward HTML page that is easy to read in text mode.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import the `requests` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Make a request to the URL, which will take a second or two:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the returned object status code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the content of the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the ongoing and returned headers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The operation of `requests` is very simple; perform the operation, `GET` in
    this case, over the URL. This returns a `result` object that can be analyzed.
    The main elements are the `status_code` and the body content, which can be presented
    as `text`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full request can be checked in the `request` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The full request's documentation can be found here: [http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/).
    Over the course of the chapter, we'll be showing more features.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All HTTP status codes can be checked on this web page: [https://httpstatuses.com/](https://httpstatuses.com/).
    They are also described in the `httplib` module with convenient constant names,
    such as `OK`, `NOT_FOUND`, or `FORBIDDEN`.
  prefs: []
  type: TYPE_NORMAL
- en: The most famous error status code is arguably 404, which happens when a URL
    is not found. Try it out by doing `requests.get('http://www.columbia.edu/invalid')`.
  prefs: []
  type: TYPE_NORMAL
- en: A request can use the **HTTPS** protocol (**secure HTTP**). It is equivalent,
    but ensures that the contents of the request and response are private. `requests`
    handles it transparently.
  prefs: []
  type: TYPE_NORMAL
- en: Any website that handles any private information will use HTTPS to ensure that
    the information has not leaked out. HTTP is vulnerable to someone eavesdropping.
    Use HTTPS where available.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Installing third-party packages* recipe in [Chapter 1](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml), *Let
    Us Begin Our Automation Journey*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Parsing HTML* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing HTML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Downloading raw text or a binary file is a good starting point, but the main
    language of the web is HTML.
  prefs: []
  type: TYPE_NORMAL
- en: HTML is a structured language, defining different parts of a document such as
    headers and paragraphs. HTML is also hierarchical, defining sub-elements. The
    ability to parse raw text into a structured document is basically to be able to
    extract information automatically from a web page. For example, some text can
    be relevant if enclosed in a particular `class div` or after a header `h3` tag.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll use the excellent Beautiful Soup module to parse the HTML text into
    a memory object that can be analyzed. We need to use the `beautifulsoup4` package to
    use the latest Python 3 version that is available. Add the package to your `requirements.txt`
    and install the dependencies in the virtual environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import `BeautifulSoup` and `requests`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the URL of the page to download and retrieve it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Parse the downloaded page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Obtain the title of the page. See that it is the same as what''s displayed
    in the browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Find all the `h3` elements in the page, to determine the existing sections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the text on the section links. Stop when you reach the next `<h3>`
    tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Notice that there are no HTML tags; it's all raw text.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step is to download the page. Then, the raw text can be parsed, as
    in step 3\. The resulting `page` object contains the parsed information.
  prefs: []
  type: TYPE_NORMAL
- en: The `html.parser` parser is the default one, but for specific operations it
    can have problems. For example, for big pages it can be slow, or has issue rendering
    highly dynamic web pages. You can use other parsers, such as,  `lxml`, which is
    much faster, or `html5lib`, which will be closer to how a browser operates, including
    dynamic changes produced by HTML5\. They are external modules that will need to
    be added to the `requirements.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: '`BeautifulSoup` allows us to search for HTML elements. It can search for the
    first one with `.find()` or return a list with `.find_all()`. In step 5, it searched
    for a specific tag `<a>` that had a particular attribute, `name=link`. After that,
    it kept iterating on `.next_elements` until it finds the next `h3` tag, which
    marks the end of the section.'
  prefs: []
  type: TYPE_NORMAL
- en: The text of each element is extracted and finally composed into a single text.
    Note the `or` that avoids storing `None`, returned when an element has no text.
  prefs: []
  type: TYPE_NORMAL
- en: HTML is highly versatile, and can have multiple structures. The case presented
    in this recipe is typical, but other options on dividing sections can be grouping
    related sections inside a big `<div>` tag or other elements, or even raw text.
    Some experimentation will be required until you find the specific process to extract
    the juicy bits on a web page. Don't be afraid to try!
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Regexes can be used as well as input in the `.find()` and `.find_all()` methods.
    For example, this search uses the `h2` and `h3` tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Another useful find parameter is including the CSS class with the `class_` parameter.
    This will be shown later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: The full Beautiful Soup documentation can be found here: [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Installing third-party packages* recipe in [Chapter 1](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml), *Let
    Us Begin Our Automation Journey*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Introducing regular expressions* recipe in [Chapter 1](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml), *Let
    Us Begin Our Automation Journey*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Downloading web pages* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crawling the web
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the nature of hyperlink pages, starting from a known place and following
    links to other pages is a very important tool in the arsenal when scraping the
    web.
  prefs: []
  type: TYPE_NORMAL
- en: To do so, we crawl a page looking for a small phrase, and will print any paragraph
    that contains it. We will search only in pages that belong to the same site. I.e. only
    URLs starting with www.somesite.com. We won't follow links to external sites.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe builds on the introduced concepts, so it will download and parse
    the pages to search for links and continue downloading.
  prefs: []
  type: TYPE_NORMAL
- en: When crawling the web, remember to set limits when downloading. It's very easy
    to crawl over too many pages. As anyone checking Wikipedia can confirm, the internet
    is potentially limitless.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use as an example a prepared example, available in the GitHub repo:
    [https://github.com/PacktPublishing/Python-Automation-Cookbook/tree/master/Chapter03/test_site](https://github.com/PacktPublishing/Python-Automation-Cookbook/tree/master/Chapter03/test_site).
    Download the whole site and run the included script.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This serves the site in the URL `http://localhost:8000`. You can check it on
    a browser. It's a simple blog with three entries. Most of it is uninteresting,
    but we added a couple of paragraphs that contain the keyword `python`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/28ceb1d7-d5a3-47b8-b776-e6a0d1bf8bcb.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The full script, `crawling_web_step1.py`, is available in GitHub at the following
    link: [https://github.com/PacktPublishing/Python-Automation-Cookbook/blob/master/Chapter03/crawling_web_step1.py](https://github.com/PacktPublishing/Python-Automation-Cookbook/blob/master/Chapter03/crawling_web_step1.py).
    The most relevant bits are displayed here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Search for references to `python`, to return a list with URLs that contain
    it and the paragraph. Notice there are a couple of errors because of broken links:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Another good search term is `crocodile`. Try it out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see each of the components of the script:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A loop that goes through all the found links, in the `main` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that there's a retrieval limit of 10 pages, and it's checking that any
    new link to add is not added already.
  prefs: []
  type: TYPE_NORMAL
- en: Note these two things are limits. We won't download the same link twice and
    we'll stop at some point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Downloading and parsing the link, in the `process_link` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It downloads the file, and checks that the status is correct to skip errors
    such as broken links. It also checks that the type (as described in `Content-Type`)
    is a HTML page to skip PDFs and other formats. And finally, it parses the raw
    HTML into a `BeautifulSoup` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also parses the source link using `urlparse`, so later, in step 4, it can
    skip all the references to external sources. `urlparse` divides a URL into its
    composing elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It finds the text to search, in the `search_text` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It searches the parsed object for the specified text. Note the search is done
    as a `regex` and only in the text. It prints the resulting matches, including
    `source_link`, referencing the URL where the match was found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The  **`get_links`** function retrieves all links on a page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It searches in the parsed page all `<a>` elements, and retrieves the `href`
    elements, but only elements that have such `href` elements and that are a fully
    qualified URL (starting with `http`). This removes links that are not a URL, such
    as a `'#'` link, or that are internal to the page.
  prefs: []
  type: TYPE_NORMAL
- en: An extra check is done to check they have the same source as the original link,
    then they are registered as valid links. The `netloc` attribute allows to detect
    that the link comes from the same URL domain than the parsed URL generated in
    step 2.
  prefs: []
  type: TYPE_NORMAL
- en: We won't follow links that point to a different address (for example, a [http://www.google.com](http://www.google.com) one).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the links are returned, where they'll be added to the loop described
    in step 1.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Further filters could be enforced, for example, discarding all links that end
    in `.pdf`, meaning they are PDF files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The use of `Content-Type` can also be determined to parse the returned object
    in different ways. A PDF result (`Content-Type: application/pdf`) won''t have
    a valid `response.text` object to be parsed, but it can be parsed in other ways.
    The same is valid for other types, such as a CSV file (`Content-Type: text/csv`)
    or a ZIP file that may need to be decompressed (`Content-Type: application/zip`).
    We''ll see how to deal with those later.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Downloading web pages* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Parsing HTML* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subscribing to feeds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RSS is probably the biggest *secret* of the internet. While its moment of glory
    seemed to be during the 2000s, and now it's not in the spotlight anymore, it allows
    easy subscription to websites. It is present in lots of places, and it's incredibly
    useful.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, RSS is a way of presenting a succession of ordered references (typically
    articles, but also other elements such as podcast episodes or YouTube publications)
    and a publishing time. This makes for a very natural way of knowing what articles
    are new since the last check, as well as presenting some structured data about
    them, such as the title and a summary.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will present the `feedparser` module, and determine how to
    obtain data from an RSS feed.
  prefs: []
  type: TYPE_NORMAL
- en: '**RSS** is not the only available feed format. There''s also a format called
    **Atom**, but both are very much equivalent. `feedparser` is also capable of parsing
    it, so both can be used indistinctly.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to add the `feedparser` dependency to our `requirements.txt` file and
    reinstall it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Feed URLs can be found on almost all pages that deal with publications, including
    blogs, news, podcasts, and so on. Sometimes they are very easy to find, but sometimes
    they are a little bit hidden. Search by `feed` or `RSS`.
  prefs: []
  type: TYPE_NORMAL
- en: Most newspapers and news agencies has their RSS feeds divided by themes. We'll
    use as example to parse **The New York Times** main page feed, [http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml](http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml).
    There are more feeds available in the main feed page: [https://archive.nytimes.com/www.nytimes.com/services/xml/rss/index.html](https://archive.nytimes.com/www.nytimes.com/services/xml/rss/index.html?mcubz=0).
  prefs: []
  type: TYPE_NORMAL
- en: Please note the feeds may be subjected to terms and conditions of use. In the
    New York Times case, they are described at the end of the main feed page.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that this feed changes quite often, meaning that the linked entries
    will change from the examples in this book.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import the `feedparser` module, as well as `datetime`, `delorean`, and `requests`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Parse the feed (it will be downloaded automatically) and check when it was
    last updated. Feed information, like the title of the feed, can be obtained in
    the `feed` attribute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the entries that are newer than six hours:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'There will be fewer entries than the total ones, because some of the returned
    entries will be older than six hours:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Retrieve information about the entries, such as the `title`. The full entry
    URL is available as `link`. Explore the available information in this particular
    feed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The parsed `feed` object contains the information of the entries, as well as
    general information about the feed itself, such as when it was updated. The `feed`
    information can be found in the `feed` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of the entries work as a dictionary, so the fields are easy to retrieve.
    They can also be accessed as attributes, but treating them as keys allows us to
    get all the available fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The basic strategy when dealing with feeds is to parse them and go through the
    entries, performing a quick check on whether they are interesting or not, for
    example, by checking the *description* or *summary*. If they are download the
    whole page using the `link` field. Then, to avoid rechecking entires, store the
    latest publication date and next time, only check newer entries.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The full `feedparser` documentation can be found here: [https://pythonhosted.org/feedparser/](https://pythonhosted.org/feedparser/).
  prefs: []
  type: TYPE_NORMAL
- en: The information available can differ from feed to feed. In the New York Times
    example, there's a `tag` field with tag information, but this is not standard.
    As a minimum, entries will have a title, a description, and a link.
  prefs: []
  type: TYPE_NORMAL
- en: RSS feeds are also a great way of curating your own selection of news sources.
    There are great feed readers for that.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Installing third-party packages* recipe in [Chapter 1](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml), *Let
    Us Begin Our Automation Journey*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Downloading web pages* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing web APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rich interfaces can be created through the web, allowing powerful interactions
    through HTTP. The most common interface is through RESTful APIs using JSON. These
    text-based interfaces are easy to understand and to program, and use common technologies
    that are **language agnostic**, meaning they can be accessed in any programming
    language that has an HTTP `client` module, including, of course, Python.
  prefs: []
  type: TYPE_NORMAL
- en: Formats other than JSON are used, such as XML, but JSON is a very simple and
    readable format that translates very well into Python dictionaries (and other
    language equivalents). JSON is, by far, the most common format in RESTful APIs
    at the moment. Learn more about JSON here: [https://www.json.org/](https://www.json.org/).
  prefs: []
  type: TYPE_NORMAL
- en: The strict definition of RESTful requires some characteristics, but a more informal
    definition could be accessing resources through URLs. This means a URL represents
    a particular resource, such as an article in a newspaper or a property on a real
    estate site. Resources can then be manipulated through HTTP methods (`GET` to
    view, `POST` to create, `PUT`/`PATCH` to edit, and `DELETE` to delete) to manipulate
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Proper RESTful interfaces need to have certain characteristics, and are a way
    of creating interfaces that is not strictly restricted to HTTP interfaces. You
    can read more about it here: [https://codewords.recurse.com/issues/five/what-restful-actually-means](https://codewords.recurse.com/issues/five/what-restful-actually-means).
  prefs: []
  type: TYPE_NORMAL
- en: Using `requests` is very easy with them, as it includes native JSON support.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate how to operate RESTful APIs, we''ll use the example site [https://jsonplaceholder.typicode.com/](https://jsonplaceholder.typicode.com/).
    It simulates a common case with posts, comments, and other common resources. We
    will use posts and comments. The URLs to use will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The site returns the adequate result to each of them. Pretty handy!
  prefs: []
  type: TYPE_NORMAL
- en: Because it is a test site, data won't be created, but the site will return all
    the correct responses.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import `requests`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the list of all posts and display the latest post:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new post. See the URL of the new created resource. The call also returns
    the resource:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the `POST` request to create the resource returns 201, which is
    the proper status for created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fetch an existing post with `GET`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `PATCH` to update its values. Check the returned resource:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Two kinds of resources are typically accessed. Single resources (`https://jsonplaceholder.typicode.com/posts/X`)
    and collections (`https://jsonplaceholder.typicode.com/posts`):'
  prefs: []
  type: TYPE_NORMAL
- en: Collections accept `GET` to retrieve them all and `POST` to create a new resource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single elements accept `GET` to get the element, `PUT` and `PATCH` to edit,
    and `DELETE` to remove them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the available HTTP methods can be called in `requests`. In the previous
    recipes, we used `.get()`, but `.post()`, `.patch()`, `.put()`, and `.delete()`
    are available.
  prefs: []
  type: TYPE_NORMAL
- en: The returned response object has a `.json()` method that decodes the result
    from JSON.
  prefs: []
  type: TYPE_NORMAL
- en: Equally, to send information, a `json` argument is available. This encodes a
    dictionary into JSON and sends it to the server. The data needs to follow the
    format of the resource or an error may be raised.
  prefs: []
  type: TYPE_NORMAL
- en: '`GET` and `DELETE` don''t require data, while `PATCH`, `PUT`, and `POST` do
    require data.'
  prefs: []
  type: TYPE_NORMAL
- en: The referred resource will be returned, and its URL is available in the header
    location. This is useful when creating a new resource, where its URL is not known
    beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between `PATCH` and `PUT` is that the latter replaces the whole
    resource, while the first does a partial update.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RESTful APIs are very powerful, but also have huge variability. Please check
    the documentation of the specific API to learn about its details.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Downloading web pages* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Installing third-party packages* recipe in [Chapter 1](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml),
    *Let Us Begin Our Automation Journey*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interacting with forms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common element present in web pages is forms. Forms are a way of sending values
    into a web page, for example, to create a new comment on a blog post, or to submit
    a purchase.
  prefs: []
  type: TYPE_NORMAL
- en: Browsers present the forms so you can input values and send them in a single
    action after pressing the submit or equivalent button. We'll see how to create
    this action programatically in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that sending data to a site is normally more sensible than receiving
    data from it. For example, sending automatic comments to a website is very much
    the definition of **spam**. This means that it can be more difficult to automate
    and include security measures. Double-check that what you're trying to achieve
    is a valid, ethical use case.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll work against the test server, [https://httpbin.org/forms/post](https://httpbin.org/forms/post),
    which allows us to send a test form and sends back the submitted information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example form to order a pizza:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3fe93fc3-a5a2-450e-ba88-003566953c79.png)'
  prefs: []
  type: TYPE_IMG
- en: You can fill the form manually and see it return the information in JSON format,
    including extra information such as the browser use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the frontend of the web form generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/32f55d90-7967-4733-9b79-a3a6eca11557.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following image is the back end of the web form generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1f516fd0-d5de-484c-8875-9369cbe29b1b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We need to analyze the HTML to see the accepted data for the form. Checking
    the source code, it shows this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a2f505fc-424b-47f5-a94f-93541d72e0b8.png)Source code'
  prefs: []
  type: TYPE_NORMAL
- en: Check the name of the inputs, `custname`, `custtel`, `custemail`, `size` (a
    radio option), `topping` (a multiselection checkbox), `delivery` (time), and `comments`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import `requests`, `BeautifulSoup`, and `re` modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Retrieve the form page, parse it, and print the input fields. Check that the
    posting URL is `/post` (not `/forms/post`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note `textarea` is a valid input, as well as defined in the HTML format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the data to be posted as a dictionary. Check the values are the same
    as defined in the form:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Post the values and check that the response is the same as returned in the
    browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`requests` directly accepts to send data in the proper way. By default, it
    sends the `POST` data in the `application/x-www-form-urlencoded` format.'
  prefs: []
  type: TYPE_NORMAL
- en: Compare that with the *Accessing web APIs* recipe, where the data is explicitly
    sent in JSON format using the argument `json`. This makes the `Content-Type` be
    `application/json` instead of `application/x-www-form-urlencoded`.
  prefs: []
  type: TYPE_NORMAL
- en: The key aspect here is to respect the format of the form and the possible values
    that can return an error if incorrect, typically a 400 error.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Other than following the format of forms and inputting valid values, the main
    problem when working with forms is the multiple ways of preventing spam and abusive
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: A very common limitation is to ensure that you downloaded the form before submitting
    it, to avoid submitting multiple forms or **Cross-Site Request Forgery** (**CSRF**).
  prefs: []
  type: TYPE_NORMAL
- en: CSRF, which means producing a malicious call from a page to another taking advantage
    that  your browser is authenticated, is a serious problem. For example, entering
    in a puppies site that take advantage of you being logged into your bank page
    to perform operations "on your behalf". Here is a good description of it: [https://stackoverflow.com/a/33829607](https://stackoverflow.com/a/33829607).
    New techniques in browsers help with these issues by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain the specific token, you need to first download the form, as shown
    in the recipe, obtain the value of the CSRF token, and resubmit it. Note that
    the token can have different names; this is just an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Downloading web pages* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Parsing HTML* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Selenium for advanced interaction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, nothing short of the real thing will work. Selenium is a project
    to achieve automation in web browsers. It's conceived as a way of automatic testing,
    but it also can be used to automate interactions with a site.
  prefs: []
  type: TYPE_NORMAL
- en: Selenium can control Safari, Chrome, Firefox, Internet Explorer, or Microsoft
    Edge, though it requires installing a specific driver for each case. We'll use
    Chrome.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to install the right driver for Chrome, called `chromedriver`. It is
    available here: [https://sites.google.com/a/chromium.org/chromedriver/](https://sites.google.com/a/chromium.org/chromedriver/).
    It is available for most platforms. It also requires that you have Chrome installed: [https://www.google.com/chrome/](https://www.google.com/chrome/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the `selenium` module to `requirements.txt` and install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import Selenium, start a browser, and load the form page. A page will open
    reflecting the operations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Note the banner with Chrome is being controlled by automated test software.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a value in the Customer name field. Remember that it is called `custname`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The form will update:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/13d8d74f-9f2e-4ef9-87e1-29deb1c70b95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Select the pizza size as `medium`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This will change the pizza size ratio box.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add `bacon` and `cheese`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the checkboxes will appear as marked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/789403c5-3c8f-4c14-b9c5-6318286ae9c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Submit the form. The page will submit and the result will be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The form will be submitted and the result from the server will be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b9063025-2521-4820-a446-90e9d89b4dbd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Close the browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Step 1 in the *How to do it…* section shows how to create a Selenium page and
    go to a particular URL.
  prefs: []
  type: TYPE_NORMAL
- en: Selenium works in a similar way to Beautiful Soup. Select the adequate element,
    and then manipulate it. The selectors in Selenium work in a similar way to those
    in Beautiful Soup, with the most common ones being `find_element_by_id`, `find_element_by_class_name`,
    `find_element_by_name`, `find_element_by_tag_name`, and `find_element_by_css_selector`.
    There are equivalent `find_elements_by_X`  that return a list instead of the first
    found element ( `find_elements_by_tag_name`, `find_elements_by_name`, and more).
    This is also useful when checking whether the element is there or not. If there's
    no elements, `find_element` will raise an error while `find_elements` will return
    an empty list.
  prefs: []
  type: TYPE_NORMAL
- en: The data on the elements can be obtained through `.get_attribute()` for HTML
    attributes (such as the values on the form elements) or `.text`.
  prefs: []
  type: TYPE_NORMAL
- en: The elements can be manipulated by simulating sending keystrokes to input text,
    with the method `.send_keys()`, clicked with `.click()` or submitted with `.submit()`
    if they accept that. `.submit()` will search on a form for the proper submission,
    and `.click()` will select/deselect in the same way that a click of the mouse
    will do.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, step 6 closes the browser.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here is the full Selenium documentation: [http://selenium-python.readthedocs.io/](http://selenium-python.readthedocs.io/).
  prefs: []
  type: TYPE_NORMAL
- en: For each of the elements, there's extra information that can be extracted, such
    as `.is_displayed()` or `.is_selected()`. Text can be searched using `.find_element_by_link_text()`
    and `.find_element_by_partial_link_text()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, opening a browser can be inconvenient. An alternative is to start
    the browser in headless mode and manipulate it from there, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The page won''t be displayed. But a screenshot can be saved anyway with the
    following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Parsing HTML* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Interact with forms* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing password-protected pages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes a web page is not open to the public, but protected in some way. The
    most basic aspect is to use basic HTTP authentication, which is integrated into
    virtually every web server, and it's a user/password schema.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can test this kind of authentication in [https://httpbin.org](https://httpbin.org).
  prefs: []
  type: TYPE_NORMAL
- en: It has a path, `/basic-auth/{user}/{password}`, which forces authentication,
    with the user and password stated. This is very handy for understanding how authentication
    works.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import `requests`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Make a `GET` request to the URL with the wrong credentials. Notice that we
    set the credentials on the URL to be `user` and `psswd`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the wrong credentials to return a 401 status code (Unauthorized):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The credentials can be also passed directly in the URL, separated by a colon
    and an `@` symbol before the server, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As HTTP basic authentication is supported everywhere, support from `requests`
    is very easy.
  prefs: []
  type: TYPE_NORMAL
- en: Steps 2 and 4 in the *How to do it…* section show how to provide the proper
    password. Step 3 shows what happens when the password is the wrong one.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to always use HTTPS to ensure that the sending of the password is kept
    secret. If you use HTTP, the password will be sent in the open over the web.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Adding the user and password to the URL works on the browser as well. Try to
    access the page directly to see a box displayed asking for the username and password:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b0d91d6e-fa37-4084-82f1-d825d15441bc.png)User credentials page'
  prefs: []
  type: TYPE_NORMAL
- en: When using the URL containing the user and password, `https://user:psswd@httpbin.org/basic-auth/user/psswd`,
    the dialog does not appear and it authenticates automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to access multiple pages, you can create a session in `requests`
    and set the authentication parameters to avoid having to input them everywhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Downloading web pages* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Accessing Web APIs* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speeding up web scraping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the time spent downloading information from web pages is usually spent
    waiting. A request goes from our computer to whatever server will process it,
    and until the response is composed and comes back to our computer, we cannot do
    much about it.
  prefs: []
  type: TYPE_NORMAL
- en: During the execution of the recipes in the book, you'll notice there's a wait
    involved in `requests` calls, normally of around one or two seconds. But computers
    can do other stuff while waiting, including making more requests at the same time.
    In this recipe, we will see how to download a list of pages in parallel and wait
    until they are all ready. We will use an intentionally slow server to show the
    point.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll get code to crawl and search for keywords, making use of the `futures`
    capabilities of Python 3 to download multiple pages at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: A `future` is an object that represents the promise of a value. This means that
    you immediately receive an object while the code is being executed in the background.
    Only, when specifically requesting for its `.result()` the code blocks until getting
    it.
  prefs: []
  type: TYPE_NORMAL
- en: To generate a `future`, you need a background engine, called **executor**. Once
    created, `submit` a function and parameters to it to retrieve a `future`.  The
    retrieval of the result can be delayed as long as necessary, allowing the generation
    of several `futures` in a row, and waiting until all are finished, executing them
    in parallel, instead of creating one, wait until it finishes, creating another,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to create an executor; in this recipe, we'll use `ThreadPoolExecutor`,
    which will use threads.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use as an example a prepared example, available in the GitHub repo: [https://github.com/PacktPublishing/Python-Automation-Cookbook/tree/master/Chapter03/test_site](https://github.com/PacktPublishing/Python-Automation-Cookbook/tree/master/Chapter03/test_site).
    Download the whole site and run the included script
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: This serves the site in the URL `http://localhost:8000`. You can check it on
    a browser. It's s simple blog with three entries. Most of it is uninteresting,
    but we added a couple of paragraphs that contain the keyword `python`. The parameter
    `-d 2` makes the server intentionally slow, simulating a bad connection.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Write the following script, `speed_up_step1.py`. The full code is available
    in GitHub under the  `Chapter03`: [https://github.com/PacktPublishing/Python-Automation-Cookbook/blob/master/Chapter03/speed_up_step1.py ](https://github.com/PacktPublishing/Python-Automation-Cookbook/blob/master/Chapter03/speed_up_step1.py)directory.
    Here are only the most relevant parts. It is based on `crawling_web_step1.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Notice the differences in the `main` function. Also, there's an extra parameter
    added (number of concurrent workers), and the function `process_link` now returns
    the source link.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the `crawling_web_step1.py` script to get a time baseline. Notice the output
    has been removed here for clarity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the new script with one worker, which is slower than the original one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Increase the number of workers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding more workers decreases the time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main engine to create the concurrent requests is the main function. Notice
    that the rest of the code is basically untouched (other than returning the source
    link in the `process_link` function).
  prefs: []
  type: TYPE_NORMAL
- en: This change is actually quite common when adapting for concurrency. Concurrent
    tasks need to return all the relevant data, as they cannot rely on an ordered
    context.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the relevant part of the code that handles the concurrent engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The `with` context creates a pool of workers, specifying its number. Inside,
    a list of futures containing all the URLs to retrieve is created. The `.as_completed()`
    function returns the futures that are finished, and then there's some work dealing
    with obtaining newly found links and checking whether they need to be added to
    be retrieved or not. This process is similar to the one presented in the *Crawling
    the web *recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The process starts again until enough links have been retrieved or there are
    no links to retrieve. Note that the links are retrieved in batches; the first
    time, the base link is processed and all links are retrieved. In the second iteration,
    all those links will be requested. Once they are all downloaded, a new batch will
    be processed.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with concurrent requests, keep in mind that they can change order
    between two executions. If a request takes a little more or a little less time,
    that can affect the ordering of the retrieved information. Because we stop after
    downloading 10 pages, that also means that the 10 pages could be different.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The full `futures` documentation in Python can be found here: [https://docs.python.org/3/library/concurrent.futures.html](https://docs.python.org/3/library/concurrent.futures.html).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in steps 4 and 5 in the *How to do it…* section, properly determining
    the number of workers can require some tests. Some numbers can make the process
    slower, due the increase in management. Do not be afraid to experiment!
  prefs: []
  type: TYPE_NORMAL
- en: In the Python world, there are other ways to make concurrent HTTP requests.
    There's a native request module that allows us to work with `futures`, called
    `requests-futures`. It can be found here: [https://github.com/ross/requests-futures](https://github.com/ross/requests-futures).
  prefs: []
  type: TYPE_NORMAL
- en: Another alternative is to use asynchronous programming. This way of working
    has recently gotten a lot of attention, as it can be very efficient in situations
    when dealing with many concurrent calls, but the resulting way of coding is different
    from the traditional way and requires some time to get used to. Python includes
    the `asyncio` module to work that way, and there's a good module called `aiohttp`
    to work with HTTP requests. You can find more information about `aiohttp` here: [https://aiohttp.readthedocs.io/en/stable/client_quickstart.html](https://aiohttp.readthedocs.io/en/stable/client_quickstart.html).
  prefs: []
  type: TYPE_NORMAL
- en: A good introduction to asynchronous programming can be found in this article: [https://djangostars.com/blog/asynchronous-programming-in-python-asyncio/](https://djangostars.com/blog/asynchronous-programming-in-python-asyncio/).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Crawling the web* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Downloading web pages* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
