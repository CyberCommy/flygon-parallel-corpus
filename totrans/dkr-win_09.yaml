- en: Orchestrating Distributed Solutions with Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can run Docker on a single PC, which is what I've done so far in this book,
    and it's how you would work with Docker in development and basic test environments.
    In more advanced test environments and in production, a single server isn't suitable.
    For high availability and to give you the flexibility to scale your solutions,
    you need multiple servers running as a cluster. Docker has cluster support built
    into the platform, and you can join several Docker hosts together using Docker
    Swarm mode.
  prefs: []
  type: TYPE_NORMAL
- en: All the concepts you've learned so far (images, containers, registries, networks,
    volumes, and services) still apply in swarm mode. Docker Swarm is an orchestration
    layer. It presents the same API as the standalone Docker Engine, with additional
    functions to manage aspects of distributed computing. When you run a service in
    swarm mode, Docker determines which hosts to run the containers on; it manages
    secure communication between containers on different hosts, and it monitors the
    hosts. If a server in a swarm goes down, Docker schedules the containers it was
    running to start on different hosts to maintain the service level of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm mode has been available in Docker since version 1.12, released in 2015,
    and provides production-hardened enterprise-grade service orchestration. All communication
    in a swarm is secured with mutual TLS, so network traffic between nodes is always
    encrypted. You can store application secrets securely in the swarm, and Docker
    presents them only to those containers that need access. Swarms are scaleable,
    so you can easily add nodes to increase capacity or remove nodes for maintenance.
    Docker can also run automated rolling service updates in swarm mode, so you can
    upgrade your application with zero downtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, I''ll set up a Docker Swarm and run NerdDinner across multiple
    nodes. I''ll start by creating individual services and then move on to deploying
    the whole stack from a Compose file. You''ll learn all about:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a swarm and managing nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and managing services in swarm mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing application configuration in Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying stacks to Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying updates with zero downtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need Docker running on Windows 10 update 18.09, or Windows Server 2019
    to follow along with the examples. The code for this chapter is available at [https://github.com/sixeyed/docker-on-windows/tree/second-edition/ch07](https://github.com/sixeyed/docker-on-windows/tree/second-edition/ch07)
  prefs: []
  type: TYPE_NORMAL
- en: Creating a swarm and managing nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Swarm mode uses a manager-worker architecture with high availability
    for managers and workers. Managers are administrator-facing, and you use the active
    manager to manage the cluster and the resources running on the cluster. Workers
    are user-facing, and they run the containers for your application services.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm managers can also run containers for your applications, which is unusual
    in manager-worker architectures. The overhead of managing a small swarm is relatively
    low, so if you have 10 nodes and 3 are managers, the managers can also run a share
    of the application workload (but in production you need to be aware of the risks
    of starving your managers of compute if you're running lots of application workloads
    on them).
  prefs: []
  type: TYPE_NORMAL
- en: You can have a mixture of Windows and Linux nodes in the same swarm, which is
    a great way to manage mixed workloads. It's recommended that you have all nodes
    running the same version of Docker, but it can be Docker CE or Docker Enterprise—Docker
    Swarm functionality is built into the core Docker Engine.
  prefs: []
  type: TYPE_NORMAL
- en: Many enterprises running Docker in production have a swarm with Linux nodes
    as the managers, and a mixture of Windows and Linux nodes as the workers. That
    means you can run Windows and Linux apps in containers in a single cluster, using
    the least-cost option for the node operating system.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Swarms can be practically any size. You can run a single-node swarm on your
    laptop to test the functionality, and you can scale up to thousands of nodes.
    You start by initializing the swarm with the `docker swarm init` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This creates the swarm with a single node—the Docker Engine where you run the
    command, and that node becomes the swarm manager. My machine has multiple IP addresses,
    so I've specified the `listen-addr` and `advertise-addr` options that tell Docker
    which network interface to use for swarm communication. It's a good practice to
    always specify the IP address and to use static addresses for the manager nodes.
  prefs: []
  type: TYPE_NORMAL
- en: You can keep your swarm secure using an internal private network for the swarm
    traffic, so that communication is not on the public network. You can even keep
    your managers off the public network completely. Only worker nodes with public-facing
    workloads need to be connected to the public network in addition to the internal
    network - and you can even avoid that if you're using a load-balancer as the public
    entrypoint to your infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Adding workers to the swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The output from `docker swarm init` tells you how to expand the swarm by joining
    other nodes. Nodes can belong to only one swarm, and to join, they need to use
    the joining token. The token prevents rogue nodes joining your swarm if the network
    is compromised, so you need to treat it as a secure secret. Nodes can join as
    workers or managers, and there are different tokens for each. You can view and
    rotate the tokens with the `docker swarm join-token` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'On a second machine running the same version of Docker, I can run the `swarm
    join` command to join the swarm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now my Docker host is running in swarm mode, there are more commands available
    to me when I''m connected to the manager node. The `docker node` commands manage
    the nodes in the swarm, so I can list all the nodes in the swarm and see their
    current status with `docker node ls`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `STATUS` value tells you whether the node is online in the swarm, and the
    `AVAILABILITY` value tells you whether the node is able to run containers. The
    `MANAGER STATUS` field has three options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Leader`: The active manager controlling the swarm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Reachable`: A backup manager; it can become the leader if the current leader
    goes down.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`No value`: A worker node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple managers support high availability. Docker Swarm uses the Raft protocol
    to elect a new leader if the current leader is lost, so with an odd number of
    managers, your swarm can survive hardware failure. For production, you should
    have three manager nodes, which is all you need, even for large swarms with hundreds
    of worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Worker nodes do not automatically get promoted to managers, so if all your managers
    are lost, then you cannot administer the swarm. In that situation, the containers
    on the worker nodes continue running, but there are no managers to monitor the
    worker nodes or the services you have running.
  prefs: []
  type: TYPE_NORMAL
- en: Promoting and removing swarm nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can make worker nodes into managers with `docker node promote` and make
    manager nodes into workers with `docker node demote`; these are commands you run
    on a manager node.
  prefs: []
  type: TYPE_NORMAL
- en: 'To leave a swarm, you need to run the `docker swarm leave` command on the node
    itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If you have a single-node swarm, you can exit swarm mode with the same command,
    but you need to use the `--force` flag, as this effectively switched you from
    swarm mode back into single Docker Engine mode.
  prefs: []
  type: TYPE_NORMAL
- en: The `docker swarm` and `docker node` commands manage the swarm. When you're
    running in swarm mode, you use swarm-specific commands to manage your container
    workload.
  prefs: []
  type: TYPE_NORMAL
- en: You will see references to *Docker Swarm* and *swarm mode*. Technically, they
    are different things. Docker Swarm was an early orchestrator that was later built
    into the Docker Engine as swarm mode. The *classic* Docker Swarm only ran on Linux,
    so when you're talking about swarm with Windows nodes, it's always swarm mode—but
    it's usually called Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Running Docker Swarm in the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker has a minimal set of infrastructure requirements, so you can easily spin
    up a Docker host or a clustered Docker Swarm in any cloud. All you need to run
    Windows containers at scale is the capacity to run Windows Server virtual machines
    and connect them on a network.
  prefs: []
  type: TYPE_NORMAL
- en: The cloud is a great place to run Docker, and Docker is a great way to move
    to the cloud. Docker gives you the power of a modern application platform, without
    the restrictions of a **Platform as a Service** (**PaaS**) product. PaaS options
    typically have proprietary deployment systems, proprietary integrations in your
    code, and the developer experience will not use the same runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Docker lets you package your applications and define your solution structure
    in a portable way that will run the same way on any machine and on any cloud.
    You can use basic **Infrastructure as a Service** (**IaaS**) services, which all
    cloud providers support, and have a consistent deployment, management, and runtime
    experience in every environment.
  prefs: []
  type: TYPE_NORMAL
- en: The major clouds also provide managed container services, but these have centralized
    on Kubernetes—AKS on Azure, EKS on Amazon Web Services, and GKE on Google Cloud.
    At the time of writing, they're all 100% Linux offerings. Windows support for
    Kubernetes is being actively worked on, and the cloud services will begin to offer
    Windows once it's supported, but Kubernetes is a far more complex orchestrator
    than swarm, and I won't cover it here.
  prefs: []
  type: TYPE_NORMAL
- en: One of the simplest ways to deploy Docker Swarm in the cloud is to use Terraform,
    which is a powerful **Infrastructure-as-Code** (**IaC**) technology that's typically
    much easier to use than the cloud providers' own templating language or scripting
    tools. With a few dozens lines of configuration, you can define VMs for the manager
    and worker nodes, along with the networking setup, load balancers, and any other
    services you need.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Certified Infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker uses Terraform to power **Docker Certified Infrastructure** (**DCI**),
    which is a single tool for deploying Docker Enterprise on the major cloud providers
    and the major on-premises virtualization tools. It uses the relevant services
    from each provider to set up enterprise-grade deployments of the Docker Enterprise
    platform, including Universal Control Plane and Docker Trusted Registry.
  prefs: []
  type: TYPE_NORMAL
- en: DCI is detailed in a series of Reference Architecture guides from Docker, available
    on the **Docker Success Center** ([https://success.docker.com](https://success.docker.com)).
    It's worth bookmarking that site—you'll also find great guides on modernizing
    traditional applications and best practice documentation for logging, monitoring,
    storage, and networking in containers.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and managing services in swarm mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you saw how to use Docker Compose to organize a distributed
    solution. In a Compose file, you define the parts of your application as services
    using networks to connect them together. The same Docker Compose file format and
    the same service concept is used in swarm mode. In swarm mode, the containers
    that make up a service are called **replicas**. You use the Docker command line
    to create services on the swarm, and the swarm manager creates replicas running
    as containers on the swarm nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ll deploy the NerdDinner stack by creating services. All the services will
    run in the same Docker network on my cluster. In swarm mode, Docker has a special
    type of network called **overlay network**. Overlay networks are virtual networks
    that span multiple physical hosts, so containers running on one swarm node can
    reach containers running on another node. Service discovery works in the same
    way: containers access one another by the service name, and Docker''s DNS server
    directs them to a container.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create an overlay network, you need to specify the driver to be used and
    give the network a name. The Docker CLI returns with the ID of the new network,
    as it does with other resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can list the networks, and you''ll see that the new network uses the overlay
    driver and is scoped to the swarm, which means any containers using this network
    can communicate with one another, whichever node they''re running on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The output here also shows the default `nat` network, which has a local scope,
    so containers can only reach one another on the same host. There's another network
    created in swarm mode called `ingress`, which is the default network for services
    created with published ports.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ll use the new network for the NerdDinner services, because that will segregate
    my app from others on the swarm that will use their own networks. I''ll use a
    Docker Compose file to deploy the whole solution later in this chapter, but I''ll
    start by manually creating services with the `docker service create` command,
    so you can see how services are different than containers. This is how to deploy
    the NATS message queue as a service in Docker Swarm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'There are no required options for `docker service create` other than the image
    name, but for a distributed application, you will want to specify the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`network`: The Docker network to connect to the service containers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: The service name used as the DNS entry for other components'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker supports different types of DNS resolution for containers. The default
    is Virtual IP `vip` mode, which you should use because it's the most performant.
    `vip` mode is only supported from Windows Server 2019, so for earlier versions
    you will see examples where the endpoint mode is set to `dnsrr`. That's DNS round-robin
    mode, which is less efficient and can cause issues when clients cache DNS responses,
    so avoid it, unless you have to work with containers on Windows Server 2016.
  prefs: []
  type: TYPE_NORMAL
- en: You run the `service create` command from a Docker CLI connected to the swarm
    manager. The manager looks at all the nodes in the swarm and determines which
    have the capacity to run a replica, and then it schedules the tasks to be created
    as containers on the node(s). The default replica level is *o**ne*, so this command
    just creates a single container—but it could be running on any node in the swarm.
  prefs: []
  type: TYPE_NORMAL
- en: '`docker service ps` shows the replicas that are running the service, including
    the name of the node hosting each container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the manager has scheduled a container to run on node `win2019-02`,
    which is the single worker node in my swarm. It looks as if I would get the same
    result if I ran a Docker container on that node directly, but running it as a
    Docker Swarm service gives me all the extra benefits of orchestration:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Application reliability**: If this container stops, the manager will schedule
    a replacement to start immediately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Infrastructure reliability**: If the worker node goes down, the manager will
    schedule a new container to run on a different node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discoverability**: This container is attached to an overlay network, so it
    can communicate with containers running on other nodes using the service name
    (Windows containers can even talk to Linux containers in the same swarm, and vice
    versa).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many more benefits to running services in Docker Swarm over containers
    on individual Docker servers, including security, scalability, and reliable application
    updates. I'll cover them all in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the source code repository, the `ch07-create-services` folder has a script
    that starts all the services for NerdDinner in the correct order. The options
    for each `service create` command are the equivalent of the service definition
    in the Compose file for [Chapter 6](83637474-1791-48b6-8ce1-5aa07f00b46c.xhtml),
    *Organizing Distributed Solutions with Docker Compose*. There are just a couple
    of differences in the frontend services and the Traefik reverse proxy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Traefik runs just fine in Docker Swarm—it connects to the Docker API to build
    its frontend-routing map, and proxies content from backend containers in exactly
    the same way as it does on a single server running Docker Engine. To register
    services with Traefik in swarm mode, you also need to tell Traefik which port
    the application in the container is using, because it can''t determine that itself.
    The REST API service definition adds the `traefik.port` label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Traefik itself is the most complex service to create, with a few extra options
    in swarm mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can only get information about swarm services from the Docker API running
    on a manager node—that's why you need to connect your Docker CLI to a manager
    to work with swarm resources. The `constraint` option for services ensures Docker
    will only schedule containers to run on nodes that meet the constraint. In this
    case, the service replicas will run only on manager nodes. This isn't the only
    option - you can run Traefik on a worker node if you have configured secure remote
    access to the Docker API.
  prefs: []
  type: TYPE_NORMAL
- en: To connect Traefik to the Docker API, I previously used a volume to mount the
    Windows named *pipe*, but that feature isn't supported yet in Docker Swarm. So,
    instead, I use a TCP connection to the API, specifying the DNS name of the manager
    `win2019-dev-02`. I've secured my Docker Engine with TLS (as I explained in [Chapter
    1](59b504fb-1012-4118-aa49-c5e0efce06d3.xhtml), *Getting Started with Docker on
    Windows*), so I also provide the client certificates to use the connection securely.
    The certificates are stored on my manager node in `C:\certs\client`, which I mount
    as a directory inside the container.
  prefs: []
  type: TYPE_NORMAL
- en: '*Named pipe support for service mounts* means you can use the approach of mounting
    the pipe, which is much easier, as you don''t need to specify the host name of
    the manager, or supply the TLS certificates. That feature is planned for Docker
    19.03, and will probably be available by the time you read this book. The great
    thing about Docker is that it''s built from open source components, so features
    such as this are all discussed in the open—[https://github.com/moby/moby/issues/34795](https://github.com/moby/moby/issues/34795)
    will tell you the background and the current status.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When I run the script on my swarm, I get a list of service IDs as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now I can see all the running services with `docker service ls`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of the services is listed as having a replica status of `1/1`, which means
    one replica is running out of a requested service level of one replica. That''s
    the number of containers used to run the service. Swarm mode supports two types
    of distributed service: replicated and global . The default is to have a distributed
    service with a single replica, which means one container on the swarm. The `service
    create` commands in my script don''t specify a replica count, so they all use
    the default of *one*.'
  prefs: []
  type: TYPE_NORMAL
- en: Running services across many containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Replicated services are how you scale in swarm mode, and you can update running
    services to add or remove containers. Unlike Docker Compose, you don''t need a
    Compose file that defines the desired state of each service; that detail is already
    stored in the swarm from the `docker service create` command. To add more message
    handlers, I use `docker service scale`, passing the name of one or more services
    and the desired service level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The message handler services were created with the default single replica,
    so this adds two more containers to share the work of the SQL Server-handler service.
    In a multi-node swarm, the manager can schedule the containers to run on any node
    with a capacity. I don''t need to know or care which server is actually running
    the containers, but I can drill down into the service list with `docker service
    ps` to see where the containers are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this case, I'm running a two-node swarm, and the replicas are split between
    the nodes `win2019-dev-02` and `win2019-02`. Swarm mode refers to service processes
    as replicas, but they're actually just containers. You can log on to the nodes
    of the swarm and administer service containers with the same `docker ps`, `docker
    logs` and `docker top` commands, as usual.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, you won''t do that. The nodes running replicas are just black boxes
    that are managed for you by the swarm; you work with your services through the
    manager node. Just as Docker Compose presents a consolidated view of logs for
    a service, you can get the same from the Docker CLI connected to a swarm manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Replicas are how the swarm provides fault tolerance to services. When you specify
    the replica level for a service with the `docker service create` , `docker service
    update`, or `docker service scale` command, the value is recorded in the swarm.
    The manager node monitors all the tasks for the service. If containers stop and
    the number of running services falls below the desired replica level, new tasks
    are scheduled to replace the stopped containers. Later in the chapter, I'll demonstrate
    that when I run the same solution on a multi-node swarm, I can then take a node
    out of the swarm, without causing any loss of service.
  prefs: []
  type: TYPE_NORMAL
- en: Global services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative to replicated services is **global services**. In some cases,
    you may want the same service running on every node of the swarm as a single container
    on each server. To do that, you can run a service in global mode—Docker schedules
    exactly one task on each node, and any new nodes that join will also have a task
    scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: Global services can be useful for high availability with components that are
    used by many services, but, again, you don't get a clustered application just
    by running many instances of it. The NATS message queue can run as a cluster across
    several servers, and it could be a good candidate to run as a global service.
    To run NATS as a cluster, though, each instance needs to know the address of other
    instances—which doesn't work well with dynamic virtual IP addresses allocated
    by the Docker Engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, I can run my Elasticsearch message handler as a global service, so
    every node will have an instance of the message handler running. You can''t change
    the mode of a running service, so, first, I need to remove the original service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, I can create a new global service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now I have one task running on each node in the swarm, and the total number
    of tasks will grow if nodes are added to the cluster, and shrink if the nodes
    are removed. This can be useful for services that you want to distribute for fault
    tolerance, and you want the total capacity of the service to be proportionate
    to the size of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Global services are also useful in monitoring and auditing functions. If you
    have a centralized monitoring system such as Splunk, or you're using Elasticsearch
    Beats for infrastructure data capture, you could run an agent on each node as
    a global service.
  prefs: []
  type: TYPE_NORMAL
- en: With global and replicated services, Docker Swarm provides the infrastructure
    to scale your application and maintain specified service levels. This works well
    for on-premises deployments if you have a fixed-size swarm but variable workloads.
    You can scale application components up and down to meet the demand, provided
    they don't all require peak processing at the same time. You have more flexibility
    in the cloud, where you can increase the total capacity of your cluster, just
    by adding new nodes to the swarm, allowing you to scale your application services
    more widely.
  prefs: []
  type: TYPE_NORMAL
- en: Running applications at scale across many instances typically adds complexity—you
    need to have a way of registering all the active instances, a way of sharing load
    between them, and a way of monitor all the instances, so that if any fail, they
    don't have any load sent to them. This is all built-in functionality in Docker
    Swarm, which transparently provides service discovery, load-balancing, fault-tolerance
    and the infrastructure for self-healing applications.
  prefs: []
  type: TYPE_NORMAL
- en: Load-balancing and scale in swarm mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker uses DNS for service discovery, so containers can find one another with
    standard networking. Applications use server names in their client connection
    configuration, and when the application makes a DNS query to find the target,
    Docker responds with the container's IP address. It's the same in Docker Swarm
    when your target **server** name could actually be the name of a Docker service
    running with dozens of replicas across the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways for Docker to respond to manage DNS for a service with multiple
    replicas. The default is to use **VIP**: a **virtual IP address**. Docker uses
    a single IP address for the service, and relies on the networking stack in the
    host operating system to route a request on the VIP to an actual container. VIP
    takes care of load-balancing and health. Requests are shared among all the healthy
    containers in the service. This feature has long been established in Linux, and
    is new to Windows with Server 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The alternative to VIP is **DNSRR**: **DNS round-robin**, which you specify
    in the `endpoint_mode` setting in the service configuration. DNSRR returns a list
    of the IP addresses of all the healthy containers in the service, and the order
    of the list is rotated to provide load-balancing. DNSRR was the only option for
    Windows containers before Server 2019, and you will see it in lots of examples,
    but VIP is a better option. Clients have a tendency to cache DNS query responses.
    With DNSRR, you can update a service and find clients have cached the IP address
    of an old container that has been replaced, so their connection fails. That doesn''t
    happen with VIP, where there''s a single IP address in the DNS response, and clients
    can safely cache it because it will always route to a healthy container.'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm takes care of load-balancing network traffic between service replicas,
    but it also load-balances external traffic coming into the swarm. In the new NerdDinner
    architecture, there is only one component that is publicly accessible—the **Traefik
    reverse proxy**. We know a port can only be used by a single process on a machine,
    so that should mean we can only scale the proxy service to a maximum of one container
    for each node in the cluster. But Docker Swarm lets us over provision or under
    provision the service, using the same port on the machine for zero or many replicas.
  prefs: []
  type: TYPE_NORMAL
- en: A swarm service attached to an overlay network behaves differently to standard
    containers when you publish ports. Every node in the swarm listens on the published
    port, and when traffic is received, it gets directed to a healthy container. That
    container could be running on the node that received the request, or on a different
    node.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, the client makes an HTTP GET request on standard port `80`
    for a service running in Docker Swarm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/f379e589-aa28-4882-9d1f-d8aa739e6e2f.png)'
  prefs: []
  type: TYPE_IMG
- en: The client request reaches a node that is not running any of the service replicas.
    The node has no containers listening on port `80`, so it cannot handle the request
    directly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The receiving node forwards on the request to another node in the swarm that
    does have a container listening on port `80`—this is all invisible to the original
    client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The new node forwards the request on to the running container, which processes
    it and sends the response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is called **ingress networking**, and it's an extremely powerful feature.
    It means you can run a low-scale service on a large cluster, or a high-scale service
    on a small cluster, and they will work in the same way. If the service is running
    with fewer replicas than there are nodes in the cluster, that isn't an issue,
    because Docker will transparently send the request to another node. If the service
    is running with more replicas than there are nodes, that isn't an issue, because
    every node can handle the request and Docker load-balances traffic between containers
    on the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Networking in Docker Swarm is a topic that's worth understanding in detail,
    because it will help you design and deliver scaleable and resilient systems. I
    have authored a Pluralsight course called **Managing Load Balancing and Scale
    in Docker Swarm Mode Clusters** that covers all the key topics for Linux and Windows
    containers.
  prefs: []
  type: TYPE_NORMAL
- en: Load-balancing and service discovery is all based on healthy containers, and
    it's a Docker Swarm feature that doesn't need any special setup from my side.
    A service running on an overlay network in swarm mode defaults to VIP service
    discovery and ingress networking for published ports. When I run NerdDinner in
    Docker Swarm, I don't need any changes to my deployment to gain high availability
    and scale in a production environment, and I can focus on my own application's
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Managing application configuration in Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I spent some time in [Chapter 5](e279ed60-09a9-4024-8e30-e5f08074c66a.xhtml), *Adopting
    Container-First Solution Design* building a flexible configuration system into
    my Docker images for the NerdDinner stack. The core principle of that was to bundle
    the default configuration for development into each image, but allow settings
    to be overridden when you run a container. That means we'll use the same Docker
    image in every environment, just swapping out the configuration settings to change
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: That works for a single Docker Engine where I can use environment variables
    to override individual settings and volume mounts to replace whole configuration
    files. You can do much more with configuration in Docker Swarm—using Docker config
    objects and Docker secrets to store data in the swarm that can be delivered to
    containers. This is a much neater way of dealing with configuration and sensitive
    data than using environment variables and files, but it means I still use the
    same Docker image in every environment.
  prefs: []
  type: TYPE_NORMAL
- en: Storing configuration in Docker config objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several new resources in swarm mode—as well as nodes and services,
    there are stacks, secrets, and configs. Config objects are just text files that
    are created in the swarm and have surfaced as files inside service containers.
    They're a great way of managing configuration settings, because they give you
    a single place to store the settings for all your applications.
  prefs: []
  type: TYPE_NORMAL
- en: You use config objects in two ways. You create and manage them with `docker
    config` commands, and you make them available to services in Docker service commands
    and Docker Compose files. This clean separation means your application definition
    is separate from your configuration—the definition is the same everywhere, and
    the configuration is loaded by Docker from the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Docker surfaces a config object as a text file inside a container at the path
    you specify, so you could have a secret called `my-app-config` in the swarm that
    appears as `C:\my-app\config\appSettings.config`. Docker doesn't care about the
    file contents, so it could be XML, JSON, key-value pairs, or anything else. It's
    up to your application to actually do something with the file, which could be
    using the complete file as config, or merging the file contents with some default
    configuration baked into the Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In my modernization of NerdDinner, I''ve moved to the .NET Core configuration
    framework for my application settings. I use the same `Config` class in all the
    .NET Framework and .NET Core apps that make up NerdDinner. The `Config` class
    adds custom file locations for configuration providers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The configuration providers are listed in reverse-order of precedence. First,
    they're loaded from the `config/appsettings.json` file that is part of the application
    image. Then, any environment variables are merged in—adding new keys, or replacing
    the values of existing keys. Next, if a file exists at the path `config/config.json`
    its contents get merged in—overriding any existing settings. And lastly if a file
    exists at `config/secrets.json` then its values are merged in.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern lets me use a hierarchy of configuration sources. The default values
    for the app are all present in the Docker image. At runtime, users can specify
    overrides with environment variables or environment variable files—which is easy
    for developers working on single Docker hosts. In a clustered environment, the
    deployment can use Docker config objects and secrets, which override the default
    values and any environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: As a simple example, I can change the logging level for the new REST API. In
    the `appsettings.json` file in the Docker image, the logging level is set to `Warning`.
    The app writes information-level logs every time there's a `GET` request, so if
    I change the logging level in config, I'll be able to see those log entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have the settings I want to use in a file called `nerd-dinner-api-config.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'First, I need to store that as a config object in the swarm, so containers
    don''t need access to the original file. I do that with `docker config create`,
    giving the object a name and the path to the source of the configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You only need access to the file when you create the config object. Now the
    data is stored in the swarm. Any node in the swarm can get the config data and
    supply it to containers, and anyone with access to the Docker Engine can see the
    config data without needing that source file. `docker config inspect` shows you
    the contents of the config object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: You can see the plain-text value of the config object by inspecting it. This
    is great for troubleshooting application issues, but bad for security—you should
    always use Docker secrets for sensitive configuration values, never config objects.
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker config objects in swarm services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You make config objects available to containers when you create the service,
    using the `--config` option. You should then be able to use them directly in your
    application, but there may be a catch. When config objects are presented as files
    to the container, they're secured so only administrative accounts can read them.
    If your application is running as a least-privileged user, it can see the config
    file, but it can't read it. This is a security feature, intended to keep your
    configuration files safe if someone gains access to the filesystem in the container.
  prefs: []
  type: TYPE_NORMAL
- en: This is different in Linux containers, where you can specify the ID of the user
    who has file ownership inside the container, so you can give least-privileged
    accounts access to the file. Windows containers don't support that feature, but
    Windows containers are evolving to be feature-complete with Linux containers,
    so this should come in a future release. At the time of writing, to use config
    objects, the application needs to be running as an administrator account, or as
    an account with local system access.
  prefs: []
  type: TYPE_NORMAL
- en: Running your application with elevated permissions is not a good idea from a
    security perspective, but it is less of a concern when you run in a container.
    I cover this in [Chapter 9](ea2edfd1-c625-4599-8ec2-d5ae811941ef.xhtml), *Understanding
    the Security Risks and Benefits of Docker*.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ve updated the Dockerfile for the REST API from [Chapter 5](e279ed60-09a9-4024-8e30-e5f08074c66a.xhtml),
    *Adopting Container-First Solution Design*, to use the built-in admin account
    in the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'All that''s changed is the `USER` instruction, which sets the user for the
    rest of the Dockerfile and for container startup. The code is exactly the same:
    I''m still using the builder image from [Chapter 5](e279ed60-09a9-4024-8e30-e5f08074c66a.xhtml),
    *Adopting Container-First Solution Design*. I''ve built this new image as `dockeronwindows/ch07-nerd-dinner-api:2e`,
    and I can upgrade my running API service and apply the new configuration with
    `docker service update`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Updating a service replaces the running replicas with the new configuration,
    in this case, using a new image and applying the config object. Now when I make
    a `GET` request to the REST API, it''s logging at an information level, and I
    can see a lot more detail in the service logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: You can use this approach for feature flags and behavior settings that change
    between environments. It's a really flexible approach to application configuration.
    Developers using a single Docker Engine can run the container with the default
    settings in the image, or override them with environment variables, or replace
    the whole config files by mounting a local volume. In test-and-production environments
    using Docker Swarm, admins can manage configuration centrally with config objects—still
    using the exact same Docker image in every environment.
  prefs: []
  type: TYPE_NORMAL
- en: Storing sensitive data in Docker secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Swarm mode is inherently secure. Communication among all the nodes is encrypted,
    and the swarm provides an encrypted data store that is distributed among the manager
    nodes. You can use this store for application **secrets**. Secrets work in exactly
    the same way as config objects—you create them in the swarm, and then you make
    them available to services. The difference is that the secret is encrypted in
    the swarm's data store, and it's encrypted in transit from the manager to the
    worker node. It's only decrypted inside the container running the replica, where
    it gets surfaced as a file in the same way as config objects.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets are created with a name and the contents of the secret, which can be
    read from a file or entered into the command-line. I'm going to move my sensitive
    data to secrets, starting with the SQL Server administrator account password.
    In the `ch07-app-config` folder, I have a folder called `secrets` that contains
    a secret files for the database password. I'll use that to securely store the
    password in the swarm, but I need to do some work to my database image before
    it can support secrets.
  prefs: []
  type: TYPE_NORMAL
- en: I packaged my latest SQL Server database schema in the Docker image `dockeronwindows/ch06-nerd-dinner-db`.
    That image uses environment variables to set the administrator password, which
    is fine for developers, but not good in a test environment where you want to restrict
    access. I have a new version for this chapter with an updated Dockerfile and startup
    script for the database, so I can read in the password from a secret file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `InitializeDatabase.ps1` script for `ch07-nerd-dinner-db`, I''ve added
    a new parameter called `sa_password_path` and some simple logic to read the password
    from the file, if one exists in that path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This is a completely different approach to the one taken in the REST API. Applications
    have their own expectations about configuration, and you'll need to integrate
    that with Docker's approach of surfacing config data in files. In most cases,
    you can do it all in the Dockerfile, so you shouldn't need to change code to read
    config from a file directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Dockerfile uses an environment variable with a default value for the path
    to the password file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This still supports different ways of running the database. Developers can run
    it without specifying any configuration settings, and it will use the default
    password built into the image—which is the same default built into the connection
    strings for the application images. In a clustered environment, admins can create
    the secret separately from deploying the app and secure access to the database
    container.
  prefs: []
  type: TYPE_NORMAL
- en: 'I need to create the secret and then update the database service to use the
    secret and the new image that applies the password from the secret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the database is using a strong password that is protected by Docker Swarm.
    Users with access to the Docker Engine can''t see the contents of the secret,
    as it''s only ever decrypted inside a container for a service that explicitly
    uses the secret. I can inspect the secret, but I only see metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Right now my application is broken, because I've updated the database password
    without updating the connection strings in the applications that use the database.
    This is the danger of managing distributed applications imperatively, by issuing
    commands to the Docker Swarm. Instead, you should manage your applications declaratively,
    using a Docker Compose file to define all the services and other resources and
    deploying them as a Docker stack.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying stacks to Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stacks in Docker Swarm address the limitations of using Docker Compose with
    a single host, or creating services manually on a Docker Swarm. You create a stack
    from a Compose file, and Docker stores all the metadata for the stack's services
    in the swarm. This means Docker is aware that the set of resources represents
    one application, and you can manage services from any Docker client without needing
    the Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: A *stack* is an abstraction over all the objects that make up your application.
    It contains services, volumes, and networks just like a standard Docker Compose
    file, but it also supports Docker Swarm objects—configs and secrets—and additional
    deployment settings for running applications at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Stacks can even abstract the orchestrator you're using. Docker Enterprise supports
    both Docker Swarm and Kubernetes on the same cluster, and you can deploy and manage
    applications as stacks to either orchestrator, using the simple Docker Compose
    format and the Docker CLI.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a stack using Docker Compose files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Docker Compose file schema has evolved from supporting client-side deployments
    on single Docker hosts to stack deployments across Docker Swarm. Different sets
    of attributes are relevant in different scenarios, and the tools enforce that.
    Docker Compose will ignore attributes that apply only to stack deployments, and
    Docker Swarm will ignore attributes that apply only to single-node deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'I can make use of multiple Compose files to exploit this, defining the basic
    setup of my application in one file, adding local settings in one override file
    and swarm settings in another override file. I''ve done that with the Compose
    files in the `ch07-docker-compose` folder. The core service definitions in `docker-compose.yml`
    are very simple now—they only include attributes that apply to every deployment
    mode. Even the reverse proxy definition for Traefik is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `docker-compose.local.yml` override file, I add the attributes that
    are relevant when I''m developing the application on my laptop and deploying with
    Docker Compose. For Traefik, I need to configure the command to run and the ports
    to publish and mount a volume for the Docker Engine named pipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `docker-compose.swarm.yml` override file, I have the attribute to apply
    when I''m running in a clustered Docker Swarm environment—which could be a two-node
    swarm in test and a 200-node swarm in production; the Compose file would be the
    same. I set up the Traefik command to connect to the swarm manager using TCP,
    and I''m using secrets to store the TLS certificates in the swarm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The only part of this application manifest that isn't portable is the DNS name
    of my swarm manager, `win2019-dev-02`. I explained in [Chapter 6](83637474-1791-48b6-8ce1-5aa07f00b46c.xhtml),
    *Organizing Distributed Solutions with Docker Compose,* that you can't mount the
    named pipe in swarm mode yet, but it's coming soon. When that feature arrives,
    I can use the named pipe for Traefik in swarm mode in the same way as on a single
    Docker Engine, and my Compose files will work on any Docker cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pattern is the same for the rest of the services: there''s a basic definition
    in docker: `compose.yml`, a set of overrides for developers in the local file,
    and an alternative set of overrides in the swarm file. The core Compose file can''t
    be used on its own, because it doesn''t have all the configuration specified,
    which is a different approach from [Chapter 6](83637474-1791-48b6-8ce1-5aa07f00b46c.xhtml), *Organizing
    Distributed Solutions with Docker Compose,* where my Docker Compose file was set
    up for development. You can use whichever approach works best for you, but the
    advantage of this way is that the setup for every environment is captured in its
    own override file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of service options that are worth looking at in more detail.
    The REST API is defined in the core Compose file with just the image and network
    settings. The local override adds the labels used to register the API with the
    proxy, and it also captures the dependency on the database service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Swarm mode does not support the `depends_on` attribute. When you deploy a stack,
    there is no guarantee which order the services will start in. If your application
    components are resilient and have `retry` logic for any dependencies, then the
    service startup order doesn't matter. If your components are not resilient and
    crash when they can't access dependencies, then Docker will restart failed containers,
    and the application should be ready after a few retries.
  prefs: []
  type: TYPE_NORMAL
- en: Resilience is often missing from legacy applications, which assume that their
    dependencies are always available and able to respond immediately. This is not
    the case if you move to cloud services, and this is also true of containers. Docker
    will keep replacing failed containers, but you can add resilience even to legacy
    apps by building startup checks and health checks into the Dockerfile.
  prefs: []
  type: TYPE_NORMAL
- en: 'The swarm definition adds the secret and config setup, and there is also a
    difference in how the container labels are applied for Traefik:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Configs and secrets only apply in swarm mode, but you can include them in any
    Compose file—Docker Compose will ignore them when you run on a single Docker Engine.
    The `deploy` section is also only for swarm mode, and this captures the infrastructure
    setup for the replicas. Here, I have a replica count of 2, meaning the swarm will
    run two containers for this service. I also have the labels for Traefik under
    the `deploy` section, which ensures the labels are applied to the containers,
    and not to the service itself.
  prefs: []
  type: TYPE_NORMAL
- en: Docker uses labels for annotating any type of object—volumes, nodes, services,
    secrets, containers, and any other Docker resource can have labels added or removed,
    and they are exposed as key-value pairs in the Docker Engine API. Traefik only
    looks for container labels, which are applied in swarm mode in the `deploy` section
    of the compose file. If you have labels directly under the service section, then
    they get added to the service and not to the containers. In this case, that would
    mean no labels on the containers, and so Traefik would not register any routes.
  prefs: []
  type: TYPE_NORMAL
- en: Defining swarm resources in Docker Compose files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the core `docker-compose.yml` file only contains a `services`
    section; there are no other resources specified. That's because the resources
    for my app are all different between single Docker Engines deployments and Docker
    Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The local override file uses the existing `nat` network, and it uses default
    specifications for the volumes used in SQL Server and Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The swarm override maps the same `nd-net` network that all the services are
    attached to as an external network called `nd-swarm`, which will need to exist
    before I can deploy this stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: There are no volumes defined in the swarm override. You can use volumes in swarm
    mode in the same way that you use them on a single Docker Engine, but you have
    the option to use different drivers and connect storage devices in the datacenter
    or cloud-storage services to your container volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Storage in Docker is a complete topic in itself. I cover it in detail in my
    Pluralsight course, **Handling Data and Stateful Applications in Docker**. In
    that course, I demonstrate how to run stateful apps and databases in containers
    on the desktop, and with high availability and scale in Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two other sections in the swarm override file, covering my app configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: If you're looking at that and thinking it's a lot of `configs` and `secrets`
    to manage, remember that this is configuration data your app needs anyway, whatever
    platform you run it on. The advantage of Docker is that all these settings are
    stored and managed centrally, and you have the option to stored and transport
    them securely, if they contain sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: 'All my config and secret objects are defined as external resources, so they
    need to exist in the swarm before I can deploy my app. There is a script in the
    `ch07-docker-stack` directory called `apply-configuration.ps1` that has all the
    `docker config create` and `docker secret create` commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The output is a list of the new object IDs. Now that the resources all exist,
    I can deploy my application as a stack.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a swarm stack from a Docker Compose file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I can deploy the application with Docker Compose on a development laptop by
    specifying multiple Compose files—the core file and the local override. In swarm
    mode, you use the standard `docker` command, rather than `docker-compose` to deploy
    a stack. The Docker CLI doesn''t support multiple files for stack deployment,
    but I can generate a single stack file by using Docker Compose to join the source
    files together. This command generates a single Compose file called `docker-stack.yml`
    from the two Compose files for the stack deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Docker Compose joins the input files and checks whether the output configuration
    is valid. I capture the output in a file called `docker-stack.yml`. This is an
    extra step that would easily fit into your deployment pipeline. Now I can deploy
    my stack on the swarm, using the stack file that contains the core service descriptions,
    plus the secrets and deployment configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'You deploy a stack from a Compose file with a single command, `docker stack
    deploy`. You need to pass the location of the Compose file and a name for the
    stack, and then Docker creates all the resources in the Compose file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a set of resources that are logically grouped together to form
    the stack. Unlike Docker Compose, which relies on naming conventions and labels
    to identify the grouping, the stack is a first-class citizen in Docker. I can
    list all stacks, which gives me the basic details—the stack name and the number
    of services—in the stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'There are 10 services in my stack, deployed from a single Docker Compose file
    that is 137 lines of YAML. That''s a tiny amount of configuration for such a complex
    system: two databases, a reverse proxy, multiple front-ends, a RESTful API, a
    message queue, and multiple message handlers. A system of that size would typically
    be described in a Word-deployment document running to hundreds of pages, and it
    would require a weekend of manual work to run all the steps. I deployed this with
    one command.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I can also drill down into the containers running the stack to see the status
    and the node they''re running on with `docker stack ps`, or get a higher-level
    view of the services in the stack with `docker stack services`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The output here shows that I have multiple replicas running the frontend containers
    and the message handlers. In total, there are 15 containers running on my two-node
    swarm, which is two VMs with a combined total of four CPU cores and 8 GB of RAM.
    At idle, the containers use very few compute resources, and I have plenty of capacity
    to run extra stacks on here. I could even deploy a copy of the same stack, using
    a different port for the proxy, and then I would have two completely separate
    test environments running on the same set of hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping services into stacks makes it much easier to manage your application,
    especially when you have multiple apps running with multiple services in each.
    The stack is an abstraction over a set of Docker resources, but you can still
    manage the individual resources directly. If I run `docker service rm`, it will
    remove a service, even if the service is part of a stack. When I run `docker stack
    deploy` again, Docker will see that a service is missing from the stack and will
    recreate it.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to updating your application with new image versions or changes
    to service attributes, you can take the imperative approach and modify the services
    directly, or you stay declarative by modifying the stack file and deploying it
    again. Docker doesn't force a process on you, but it's better to stay declarative
    and use compose files as the single source of truth.
  prefs: []
  type: TYPE_NORMAL
- en: I can scale up the message handlers in my solution either by adding `replicas
    :2` to the deploy section of the stack file and deploying it again or by running
    `docker service update --replicas=2 nerd-dinner_nerd-dinner-save-handler`. If
    I update the service and don't change the stack file as well, the next time I
    deploy the stack, my handler will go down to one replica. The stack file is viewed
    as the desired final state, and if the current state has deviated, it will be
    corrected when you deploy again.
  prefs: []
  type: TYPE_NORMAL
- en: Using the declarative approach means you always make these sorts of changes
    in the Docker Compose file(s), and update your app by deploying the stack again.
    The Compose files live in source control alongside your Dockerfiles and the application
    source code, so they can be versioned, compared, and labelled. That means when
    you pull the source code for any particular version of your app, you'll have everything
    you need to build and deploy it.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets and configurations are the exception, you would keep them in a more
    secure location than the central source repository, and only admin users would
    have access to the plain text. The Compose files just reference external secrets,
    so you get the benefit of a single source of truth for your app manifest inside
    source control, with sensitive data kept outside.
  prefs: []
  type: TYPE_NORMAL
- en: Running a single node or a two-node swarm is fine for development and test environments.
    I can run the full NerdDinner suite as a stack, verifying that the stack file
    is correctly defined, and I can scale up and down to check the behavior of the
    app. This doesn't give me high availability, because the swarm has a single manager
    node, so if the manager goes offline, then I can't administer the stack. In the
    datacenter you can run a swarm with many hundreds of nodes, and get full high
    availability with three managers.
  prefs: []
  type: TYPE_NORMAL
- en: You can build a swarm with greater elasticity for high availability and scale
    by running it in the cloud. All the major cloud operators support Docker in their
    IaaS services, so you can easily spin up Linux and Windows VMs with Docker pre-installed,
    and join them to a swarm with the simple commands you've seen in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm isn't just about running applications at scale across a cluster.
    Running across multiple nodes gives me high availability, so my application keeps
    running in the case of failure, and I can take advantage of that to support the
    application life cycle, with zero-downtime rolling updates and automated rollbacks.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying updates with zero downtime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Swarm has two features that enable updates of the whole stack without
    application downtime—rolling updates and node draining. Rolling updates replace
    application containers with new instances from a new image when you have a new
    version of a component to release. Updates are staged, so provided you have multiple
    replicas, there will always be tasks running to serve requests while other tasks
    are being upgraded.
  prefs: []
  type: TYPE_NORMAL
- en: Application updates will occur frequently, but less frequently you will also
    need to update the host, either to upgrade Docker or to apply Windows patches.
    Docker Swarm supports draining a node, which means all the containers running
    on the node are stopped and no more will be scheduled. If the replica level drops
    for any services when the node is drained, tasks are started on other nodes. When
    the node is drained, you can update the host and then join it back into the swarm.
  prefs: []
  type: TYPE_NORMAL
- en: I'll finish this chapter by covering both of these scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Updating application services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I have my stack running on Docker Swarm, and now I''m going to deploy an application
    update—a new home-page component with a restyled UI, which is a nice, easy change
    to validate. I''ve built that as `dockeronwindows/ch07-nerd-dinner-homepage:2e`.
    To make the update, I have a new Docker Compose override file, which just contains
    the new image name for the existing service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: In a normal release, you wouldn't use an override file to update one service.
    You would update the image tag in the core Docker Compose file and save the file
    in source control. I'm using an override file to make it easier to follow along
    the examples from this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two steps to this update. First, I need to generate a new application
    manifest by combining the Compose file and all the override files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now I can deploy the stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The command output shows all services are `Updating`, but Docker Swarm will
    only actually change services where the desired state in the Compose file is different
    than the running state. In this deployment, it will update the home-page service,
    using the new image name in the Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: The update doesn't have any restrictions on the image you're upgrading to. It
    doesn't need to be a new tag from the same repository name; it can be a completely
    different image. This is very flexible, but it means you need to be careful that
    you don't accidentally update your message handlers with a new version of the
    web application, or vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker updates one container at a time, and you can configure the delay interval
    between updates and the behavior to take if updates fail. While the update is
    in process, I can run `docker service ps` and see that the original containers
    are in the `Shutdown` state and the replacement containers are `Running` or `Starting`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The Dockerfile for the new NerdDinner home page application has a health check,
    and Docker waits until the health check on the new container passes before it
    moves on to replacing the next container. During the rolling update, some users
    will see the old home page, and some users will see the stylish new home page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/c490a14f-6719-4859-8377-c5232d8783cd.png)'
  prefs: []
  type: TYPE_IMG
- en: The communication between Traefik and the home page containers uses VIP networking,
    so it will only send traffic to hosts that have running containers—users will
    get a response from a container that has been updated and is running the `ch07`
    image, or one that is due to be updated and is running the `ch03` image. If this
    was a high-traffic application, I would need to ensure there's enough capacity
    in the service, so when one task is being updated, the remaining tasks can handle
    the load.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates give you zero downtime, but that doesn't necessarily mean your
    app will function correctly during the update. This process is only suitable for
    stateless applications—if tasks store any session state, then the user experience
    will be impacted. When the container holding state is replaced, the state will
    be lost. If you have stateful applications, you will need to plan a more careful
    upgrade process—or preferably modernize those components to store state in a shared
    component running in containers.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling back service updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you update a service in swarm mode, the swarm stores the configuration
    of the previous deployment. If you find a problem with the release, you can roll
    back to the previous state with a single command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The rollback is a specialized form of service update. Instead of passing an
    image name for tasks to update to, the `rollback` flag does a rolling update to
    the previous image used by the service. Again, the rollback happens one task at
    a time, so this is a zero-downtime process. You can use this command to rollback
    to, no matter how you applied the update—whether you used `docker stack deploy`
    or `docker service update`.
  prefs: []
  type: TYPE_NORMAL
- en: Rollbacks are one of the few scenarios where you might want to use imperative
    commands to manage your applications instead of declarative Docker Compose files.
    If you find a problem with a service update, it's great to be able to roll it
    back to the previous state with just a single command.
  prefs: []
  type: TYPE_NORMAL
- en: Service updates retain only one prior service configuration for rollbacks. If
    you update from version 1 to version 2 and then to version 3, the configuration
    of version 1 is lost. You can roll back from version 3 to version 2—but if you
    roll back again from version 2, it will be to the previous version, which will
    take you in a circle back to version 3.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring update behavior
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For large-scale deployments, you can change the default update behavior, either
    to complete the roll-out more quickly or to run a more conservative roll-out strategy.
    The default behavior updates one task at a time, with no delay between task updates,
    and if a task update fails, the roll-out is paused. The configuration can be overridden
    with three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`update-parallelism`: The number of tasks to update concurrently'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update-delay`: The period to wait between task updates; can be specified as
    hours, minutes, and seconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update-failure-action`: The action to take if a task update fails, either
    to continue or stop the roll out'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can specify the default parameters in the Dockerfile so they''re baked
    into the image, or the Compose file so they''re set at deployment time or with
    the service commands. For a production deployment of NerdDinner, I might have
    nine instances of the SQL message handler, with `update_config` in the Compose
    file set to update in batches of three, with a 10-second delay:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The update configuration for a service can also be changed with the `docker
    service update` command, so you can alter the update parameters and initiate a
    rolling upgrade with a single command.
  prefs: []
  type: TYPE_NORMAL
- en: Health checks are especially important in service updates. If a health check
    fails for a new task in a service update, that could mean there's a problem with
    the image. Completing the roll-out could result in 100% unhealthy tasks and a
    broken application. The default update configuration prevents this, so if an updated
    task does not enter the running state, the roll-out is paused. The update will
    not go ahead, but that's a better outcome than having an updated app that is broken.
  prefs: []
  type: TYPE_NORMAL
- en: Updating swarm nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Application updates are one part of the update routine, and host updates are
    the other. Your Windows Docker hosts should be running a minimal operating system,
    preferably Windows Server 2019 Core. This version has no UI, so there's a much
    smaller surface area for updates, but there will still be some Windows updates
    that require a reboot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rebooting the server is an invasive process—it stops the Docker Engine Windows
    Service, killing all running containers. Upgrading Docker is equally invasive
    for the same reason: It means a restart of the Docker Engine. In swarm mode, you
    can manage this by taking nodes out of service for the update period, without
    impacting service levels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ll show this with my swarm. If I need to work on `win2019-02`, I can gracefully
    reschedule the tasks it is running with `docker node update` to put it into drain
    mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting a node into drain mode means all containers are stopped, and as these
    are service task containers, they will be replaced with new containers on the
    other nodes. When the drain completes, I have no running tasks on `win-node02`:
    they have all been shut down. You can see that the tasks have been deliberately
    shut down, as `Shutdown` is listed as the desired state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'I can check the service list and see that every service is still at the required
    replica level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The swarm has created new containers to replace the replicas that were running
    on `win2019-02`. In fact, all my replicas are running on a single node now, but
    with ingress networking and VIP load-balancing, the application continues to work
    in the same way. The Docker Engine still runs in drain mode, so if any external
    traffic reaches the drained nodes, they still forward it to containers on active
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Nodes in the drain mode are considered to be unavailable, so if the swarm needs
    to schedule new tasks, none will be allocated to drained nodes. `win-node02` is
    effectively out of commission now, so I could log on and run a Windows update
    with the `sconfig` tool, or update the Docker Engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Updating the node may mean restarting the Docker Engine or rebooting the server.
    When that''s done, I can bring the server back online in the swarm with another
    `docker node update` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: This makes the node available again. When nodes join the swarm, Docker doesn't
    automatically rebalance running services, so the containers all stay on `win2019-dev02`
    , even though `win-node02` is available again and has more capacity.
  prefs: []
  type: TYPE_NORMAL
- en: In a high-throughput environment, where services are regularly started, stopped,
    and scaled, any nodes that join the swarm will soon be running their share of
    tasks. In a more static environment, you can manually rebalance services by running
    the Docker service `update --force`. This doesn't change the configuration of
    the service, but it replaces all replicas and will use all active nodes when it
    schedules the new containers to run.
  prefs: []
  type: TYPE_NORMAL
- en: This is a disruptive action, because it forces Docker to stop healthy containers.
    You need to be confident that you don't impact the availability of your application
    if you force a rebalance. Docker can't guarantee that without knowing the architecture
    of your app, which is why services aren't automatically rebalanced when nodes
    join the swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm mode gives you the power to update any component of your application and
    the nodes running the swarm, without any downtime. You may need to commission
    additional nodes in the swarm during updates to ensure you have enough capacity
    to cover nodes that are taken out of service, but these can be removed afterward.
    You don't need any additional tooling to get rolling updates, automated rollback,
    and routing to healthy containers—that's all built into Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Mixing hosts in hybrid swarms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's one more feature of swarm mode that makes it hugely powerful. Nodes
    in the swarm communicate using the Docker API, and the API is cross-platform—which
    means you can have a single swarm running a mixture of Windows and Linux servers.
    Docker also runs on different CPU architectures, so you can mix traditional 64-bit
    Intel servers with efficient new ARM boards.
  prefs: []
  type: TYPE_NORMAL
- en: Linux isn't the focus of this book, but I will cover hybrid swarms briefly because
    they open up a new range of possibilities. A hybrid swarm can have Linux and Windows
    nodes as managers and workers. You administer the nodes and the services they're
    running in the same way, using the exact same Docker CLI.
  prefs: []
  type: TYPE_NORMAL
- en: One use case for hybrid swarms is to run your manager nodes on Linux, to reduce
    licensing costs or running costs if you have your swarm in the cloud. A production
    swarm will need at least three manager nodes. Even if all your workloads are Windows-based,
    it may be more cost effective to run Linux nodes as managers - on ARM if that
    is an option - and save the Windows nodes for user workloads.
  prefs: []
  type: TYPE_NORMAL
- en: The other use case is for mixed workloads. My NerdDinner solution is using open
    source software that is available as Linux Docker images, but which I've had to
    package myself for Windows Server 2019 containers. I could migrate any cross-platform
    components to run in Linux containers on a hybrid swarm. That could be the .NET
    Core components from [Chapter 5](e279ed60-09a9-4024-8e30-e5f08074c66a.xhtml),
    *Adopting Container-First Solution Design*, as well as Traefik, the NATS message
    queue, Elasticsearch, Kibana, and even SQL Server. Linux images are typically
    much smaller and lighter than Windows images, so you should be able to run with
    greater density, packing more containers on to each host.
  prefs: []
  type: TYPE_NORMAL
- en: The great benefit of the hybrid swarm is that you manage all these components
    in the same way, from the same user interface. You can connect your local Docker
    CLI to the swarm manager and administer the Traefik proxy on Linux and the ASP.NET
    application on Windows with exactly the same commands.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was all about Docker Swarm mode, the native clustering option built
    right into Docker. You learned how to create a swarm, how to add and remove swarm
    nodes, and how to deploy services on the swarm connected with an overlay network.
    I showed that you have to create services for high availability and also discussed
    how to use configs and secrets to store sensitive application data securely in
    the swarm.
  prefs: []
  type: TYPE_NORMAL
- en: You can deploy your application as a stack on the swarm, using a Compose file,
    which makes it very easy to group and manage your application components. I demonstrated
    stack deployment on a single node swarm and on a multi-node swarm—and the process
    is the same for swarms with hundreds of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: High availability in Docker Swarm means you can perform application updates
    and rollbacks without downtime. You can even take nodes out of commission when
    you need to update Windows or Docker and still have your application running with
    the same service level on the remaining nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter I'll look more closely at the administration options for
    dockerized solutions. I'll start by looking at how to use your existing management
    tools with applications running in Docker. Then, I'll move on to managing swarms
    in production, with Docker Enterprise.
  prefs: []
  type: TYPE_NORMAL
