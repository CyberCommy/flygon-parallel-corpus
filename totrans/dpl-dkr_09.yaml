- en: Exploring the Largest-Scale Deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In earlier chapters, we covered many different aspects of deploying Docker containers,
    but if we are to turn our examples into a global service that would withstand
    the throughput of many millions of requests a second, a few things will still
    need to be addressed and this chapter was specifically written to go over the
    most important ones in some detail. Since implementations of topics covered here
    would involve enough material to be books on their own and infrastructure would
    differ wildly depending on a multitude of factors, the text here will be mostly
    on the theory side, but the previous understanding of services we gained in the
    text leading up to this chapter should be good enough to give you ideas on how
    you can proceed with the least amount of pain.
  prefs: []
  type: TYPE_NORMAL
- en: 'In its core, the topics we will cover revolve around choosing the right technologies
    and then following three basic ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: Automate everything!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Really, automate it all!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, automate even those one-off things you do every few weeks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It might be a joke, but hopefully by now it should be clear that one of the
    main points of all of this work (besides isolation) is to remove any human interaction
    from your system in regards to keeping your services running so that you and your
    team can focus on actually developing services and not wasting time on deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining quorums
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our previous examples, we mostly worked with a single-node manager but if
    you want resilience, you must ensure that there are minimal points of failure
    that will take your whole infrastructure down and a single orchestration management
    node is absolutely not enough for production services regardless of whether you
    use Swarm, Kubernetes, Marathon, or something else as your orchestration tooling.
    From the best practices perspective, you would want to have at least three or
    more management nodes in your cluster that are spread across three or more of
    your cloud's **Availability Zones** (**AZ**) or equivalent grouping to really
    ensure stability at scales since data center outages have been known to happen
    and have caused serious issues to companies that did not mitigate these types
    of circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: While in most orchestration platforms you can have any number of backing management
    nodes (or backing key-value stores in some cases), you will always have to balance
    resiliency vs speed due to the fact that with more nodes comes better capability
    to handle failures of larger parts of the system, but changes to this system (such
    as node additions and removals) must reach more points that will all have to agree,
    thus making it slower to process data. In most cases where this 3+ availability
    zone topology is required, we will need to go in details about quorums—the concept
    we lightly covered earlier, which is the backbone of all **high availability**
    (**HA**) systems.
  prefs: []
  type: TYPE_NORMAL
- en: Quorums in their basic sense are a grouping of the majority of management nodes,
    which together can decide whether updates to the cluster are going to be allowed
    or not. If the quorum is lost by the fact that half or more management nodes are
    unavailable, all changes to the cluster will be stopped to prevent your cluster
    infrastructure from having effectively split clusters. To properly divide your
    network topology for scale in this respect, you must make sure that you have a
    minimum of three nodes and/or availability zones as the quorum majority is lost
    with a single failure with less than that number. Taking this further, you will
    generally also want an odd number of nodes and availability zones since even numbers
    do not provide much additional protection for maintaining quorum, as we will see
    in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: To start off, let's say that you have five management nodes. To maintain a quorum
    of this number, you must have three or more nodes available, but if you have only
    two availability zones, the best split you can do is *3-2*, which will work fine
    if a connection is broken or the **AZ** with two management nodes goes down, but
    if the **AZ** with three nodes goes down, a quorum cannot be established since
    two is less than half of the total node count.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/feae7e99-8ae3-4908-be7d-713000fe7308.png) ![](assets/1f054a31-0e01-4b62-a99a-380baa499533.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us now see what kind of resilience we can get with three availability zones.
    The optimal layout of this grouping with five management nodes would be *2-2-1*
    and if you take a closer look at what happens when any one of the zones goes out,
    you will see that the quorum is always maintained since we will either have *3
    (2+1)* or *4 (2+2)* nodes still available from the rest of the cluster, ensuring
    that our services run without issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d0bb04d0-17b7-43d7-8740-b047fd872717.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Of course, it is also good to show what kind of effect even numbers have on
    the effectiveness since we mentioned that they may be a bit troublesome. With
    four AZs, the best split that we can make would be *2-1-1-1* across them and with
    those numbers we can only tolerate two zones being unavailable if they both contain
    only one node. With this setup, we have a 50/50 chance that two zones being unavailable
    will include the zone with two nodes within it, putting the number of total nodes
    unavailable to over 3, and thus the cluster will be completely offline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8afaa925-f216-4b68-b3e6-58dc2560d26e.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](assets/f48bc1dc-a1b2-4515-b900-c8bf945aafb3.png)'
  prefs: []
  type: TYPE_IMG
- en: This spread of management nodes across higher counts of AZs for clusters gets
    much more stable if you have more availability zones and managers, but for our
    simple example here, we can see this effect if we have five management nodes and
    five availability zones (*1-1-1-1-1* layout). With such a split, due to the quorum
    requiring at least three nodes, we will still be fully operational if any two
    of the five zones are unavailable, increasing your failure tolerance by 100 percent
    from the 3-AZ topology; but you can assume that communication between possibly
    wildly disparate geographical regions will add plenty of latency to any updates.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, with these examples, it should now be clear what kind of considerations
    and calculations you would use when trying to keep your cluster resilient and
    it is able to maintain quorum. While the tooling may differ depending on the orchestration
    tooling (that is `etcd` nodes versus Zookeeper nodes), the principles remain relatively
    the same in almost all of them, so this section should be relatively portable.
  prefs: []
  type: TYPE_NORMAL
- en: Node automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have worked on making **Amazon Machine Images** (**AMIs**) with Packer,
    we have seen what kind of thing we can do with pre-baked instance images, but
    their true power is only fully harnessed when the whole infrastructure is comprised
    of them. If your orchestration management nodes and worker nodes have their own
    system images, with a couple of startup scripts also baked-in though the init
    system (for example, `systemd` startup services), you can make instances launched
    with those images auto-join your cluster during boot in their predefined roles.
    Taking this further to a conceptual level, if we extract all stateful configuration
    into the image configurations and all dynamic configurations into a separate service
    accessible to all nodes such as EC2 `user-data` or HashiCorp Vault, your cluster
    will be almost fully self-configuring besides the initial deployment and image
    building.
  prefs: []
  type: TYPE_NORMAL
- en: 'By having this powerful auto-join capability, you are eliminating most of the
    manual work related to scaling your cluster up or down since there is no need
    for interacting with the VM instance other than starting it. A rather simple illustration
    of this architecture is depicted in the following figure, where orchestration
    and worker nodes have their own respective images and self-configure on startup
    using a shared configuration data provider within the **VPC** itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9fdaeb81-160f-41d5-bdd1-0e5b8aba0619.png)CAUTION! To prevent serious
    security breaches make sure to separate and isolate any sensitive information
    to be accessible only by the desired systems in this configuration service layout.
    As we mentioned in one of the early chapters, following security best practices
    by using need-to-know practices will ensure that a compromise of a single point
    (most likely a worker node) will not be able to spread easily to the rest of your
    cluster. As a simple example here, this would include making sure that management
    secrets are not readable by worker nodes or their network.'
  prefs: []
  type: TYPE_NORMAL
- en: Reactive auto-scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With automated self-configuration implemented, we can start looking even bigger
    by starting the instances automatically. If you remember auto-scaling groups from
    earlier chapters, even that can be automated in most cloud offerings. By using
    launch configurations and pre-configured images, like the ones we just talked
    about, adding or removing nodes with this setup would be as easy as dialing the
    desired nodes setting. The auto-scaling group would increase or decrease the worker
    instance count and because the images are self-configuring, that would be the
    full extent of input needed from you. With such a simple input, you can make scaling
    changes to your infrastructure extremely easy and done through many different
    ways.
  prefs: []
  type: TYPE_NORMAL
- en: Something to consider here as an even further step in automation is that with
    some cloud providers you can trigger these actions in your auto-scaling groups
    based on their metrics or even a `cron`-like schedule as well. In principle, if
    you have increased load on your cluster you could trigger a node count increase,
    and conversely, if the load on either the cluster or an individual node drops
    below a pre-defined value you can activate a service drain and shutdown of a fraction
    of your nodes to scale the system as needed. For periodic but predictable demand
    variations (see [https://en.wikipedia.org/wiki/Internet_Rush_Hour](https://en.wikipedia.org/wiki/Internet_Rush_Hour)
    for more info), the scheduled scaling changes we mentioned can make sure that
    you have enough resources to handle the expected demand.
  prefs: []
  type: TYPE_NORMAL
- en: Predictive auto-scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you manually dial up and down the node counts and auto-scale on schedule
    or metric triggers, you still will have some issues bringing up services you need
    at exactly the time you want them to run since services take a bit of time to
    get online, self-configure, and start getting propagated to various load balancers
    in your network. With that type of architecture, it is likely that your users
    will be the one discovering that you do not have enough capacity and your system
    then reacting to compensate. If you are really striving for all-out best user
    experience from your services sometimes you may also need to add one more layer
    to your auto-scaling triggers that can predict when your service will need more
    resources before they are even actually needed, aptly called **predictive scaling**.
  prefs: []
  type: TYPE_NORMAL
- en: In extremely broad terms, what you would do to add this predictive layer to
    your infrastructure is to funnel some fraction of your metrics collected over
    the last `x` amount of time to a **machine learning** (**ML**) tool such as TensorFlow
    ([https://www.tensorflow.org/](https://www.tensorflow.org/)) and generate a training
    set that would be able to make the tooling you are using able to predict with
    some certainty whether you will need more nodes or not. By using this method,
    your services can scale before they will even be needed to do so (!) and in a
    much smarter way than simple schedule-based approaches. Systems such as these
    are pretty difficult to integrate properly into your pipeline, but if you are
    working on global scales with crazy throughput and simple reactive auto-scaling
    comes up short, it is an avenue possibly worth exploring.
  prefs: []
  type: TYPE_NORMAL
- en: Training set in machine learning means just a set of training data (in our case
    it would be a chunk of our long-term metrics) that you can use to teach a neural
    network about how to correctly predict the demand that you will need.Like many
    of the topics in recent chapters, there are actual books written on this material
    (machine learning) that would eclipse the content of this one by volume many times
    over and would provide only marginal utility for you here. If you would like to
    learn more about machine learning in detail, this Wikipedia page has a good primer
    on it at [https://en.wikipedia.org/wiki/Machine_learning](https://en.wikipedia.org/wiki/Machine_learning)
    and you can give TensorFlow a whirl at [https://www.tensorflow.org/get_started/get_started](https://www.tensorflow.org/get_started/get_started).
  prefs: []
  type: TYPE_NORMAL
- en: In the end, if you manage to implement some or all of these techniques together,
    you will barely need any interventions with your clusters to handle scaling in
    either direction. As an added bonus to being able to sleep soundly, you will also
    save resources since you will be able to closely match your processing resources
    with the actual usage of your services making you, your budget, and your users
    all happy.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any service that you rely on in your service delivery should ideally have a
    way to notify you if something has gone wrong with it, and I do not mean user
    feedback here. Most service development nowadays is moving at incredible speeds
    and monitoring is one of those things like backups that most developers do not
    think about until something catastrophic happens, so it is something that we should
    cover a little bit. The big question that really should determine how you approach
    this topic is if your users can handle the downtimes that you will not see without
    monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Most tiny services might be OK with some outages, but for everything else, this
    would be at a bare minimum a couple of angry emails from users and at worst your
    company losing a huge percentage of your users, so monitoring at all scales is
    greatly encouraged.
  prefs: []
  type: TYPE_NORMAL
- en: While it is true that monitoring is maybe considered one of those boring pieces
    of your infrastructure to implement, having a way to gain insights into what your
    cloud is doing at all times is an absolutely essential part of managing the multitude
    of disparate systems and services. By adding monitoring to your **Key Performance
    Indicators** (**KPIs**) you can ensure that, as a whole, your system is performing
    as expected and by adding triggers to your critical monitoring targets you can
    be instantly alerted to any activity that can potentially impact your users. Having
    these type of insights into the infrastructure can both help reduce user turnover
    and drive better business decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we worked through our examples, you may have already come up with ideas
    of what you would monitor, but here are some common ones that consistently pop
    up as the most useful ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Node RAM utilization**: If you notice that your nodes aren''t using all the
    RAM allocated, you can move to smaller ones and vice versa. This generally gets
    less useful if you use memory-constrained Docker containers, but it is still a
    good metric to keep as you want to make sure you never hit a system-level max
    memory utilization on a node or your containers will run with much slower swap
    instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node CPU utilization**: You can see from this metric if your service density
    is too low or too high or if there are spikes in service demands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node unexpected terminations**: This one is good to track to ensure that
    your CI/CD pipeline is not creating bad images, that your configuration services
    are online, and a multitude of other issues that could take down your services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service unexpected terminations**: Finding out why a service is unexpectedly
    terminating is critical to ironing out bugs out of any system. Seeing an increase
    or a decrease in this value can be good indicators of codebase quality though
    they can also indicate a multitude of other problems, both internal and external
    to your infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Messaging queue sizes**: We covered this in a bit of detail before but ballooning
    queue sizes indicate that your infrastructure is unable to process data as quickly
    as it is generated, so this metric is always good to have.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Connection throughputs**: Knowing exactly how much data you are dealing with
    can be a good indicator of service load. Comparing this to other collected stats
    can also tell you if the problems you are seeing are internally or externally
    caused.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service latencies**: Just because there are no failures does not mean that
    the service is unusable. By tracking latencies you can see in detail what could
    use improving or what is not performing to your expectations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kernel panics**: Rare but extremely deadly, kernel panics can be a really
    disruptive force on your deployed services. Even though it is pretty tricky to
    monitor these, keeping track of kernel panics will alert you if there is an underlying
    kernel or hardware problem that you will need to start addressing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This obviously is not an exhaustive list, but it covers some of the more useful
    ones. As you develop your infrastructure, you will find that adding monitoring
    everywhere leads to better turnarounds on issues and discovery of scalability
    bugs with your services. So once you have monitoring added to your infrastructure,
    don't be afraid to plug it into as many pieces of your system that you can. At
    the end of the day, by gaining visibility and transparency of your whole infrastructure
    through monitoring, you can make wiser decisions and build better services, which
    is exactly what we want.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating next-gen technologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Something that I personally have been feeling has been left out of most documentation
    and learning material about containers (and most other tech topics) is proper
    evaluation and risk assessment of emerging technologies. While the risk of choosing
    a fundamentally flawed music player is trivial, choosing a fundamentally flawed
    cloud technology could tie you up in years of pain and development that you would
    otherwise not have needed. With the speed of tooling creation and development
    in the cloud space increasing at break-neck speed, good evaluation techniques
    are something that you might want to have in your toolbox of skills as they can
    save you effort, time, and money in the long run. Hunches are great but having
    a solid, repeatable, and deterministic way of evaluating technologies is a much
    more likely way to cause long-term success.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that while the advice given here has had a pretty good track record
    for me and other people I have talked to over my career, you can never fully predict
    the course that a disparate landscape of technologies will take, especially when
    most tech start-ups can close their doors at a moment's notice (i.e. ClusterHQ).
    So keep in mind that these are all just points of interest and not a magical list
    that will make the most common problems with choosing technologies disappear.
  prefs: []
  type: TYPE_NORMAL
- en: Technological needs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This should be a pretty obvious one, but it needs to be written down. If you
    have a need for a feature that is provided by a tool that you do not want to develop
    in-house, you will not have much of a choice but to go with it and hope for the
    best. Luckily, in most cloud technologies and the tooling modules that supports
    them, there are usually at least two competing options fighting for the same users
    so things are not as dire as they may seem today even though just a single year
    back almost everything in this space had a version number below `1.0`. As you
    evaluate how competing tools fit your needs, also keep in mind that not every
    tool is geared towards the same purpose even if they solve the same issues. If
    we take an example of current Kubernetes versus Marathon, even though they can
    both be used to solve the same service deployment problems, Kubernetes is mostly
    geared towards that single purpose but Marathon, for example, can also be used
    to do scheduling and cluster management as an additional functionality so we are
    in the proverbial sense really comparing apples and oranges.
  prefs: []
  type: TYPE_NORMAL
- en: In broad strokes, your service infrastructure needs will drive your tooling
    needs so you will not often end up dealing with your favorite programming language,
    having easy integration points, or working with a sane tooling codebase, but integrating
    a tool that will save you hundreds or thousands of man-hours is something not
    to be taken lightly. Sometimes it might be possible to completely skirt around
    a technological requirement by changing pieces of your system's architecture to
    avoid adding complexity to the system, but in my personal experience this was
    almost never easy to do so your mileage may vary.
  prefs: []
  type: TYPE_NORMAL
- en: Popularity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is probably the most controversial dimension to consider, but also one
    of the most important ones to pay attention to when dealing with new technologies.
    While it is absolutely true that popularity does not equate to technical merit,
    it can be assumed that:'
  prefs: []
  type: TYPE_NORMAL
- en: More people using a particular tool will be able to provide better integration
    help.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solutions to problems will be easier to find.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the codebase is open source, the project will be more likely to have fixes
    and features added to it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In another way of describing the problem, can you afford to risk weeks/months/years
    of integration work on a tool that is unproven or on track to be abandoned in
    the next couple of years? If you are a really big shop with massive budgets this
    might not be an issue but in most cases, you will not have the opportunity to
    play with integrating different competing technologies to see which one is the
    best. While there are times that there are perfectly valid cases where taking
    a calculated chance with a new tool is warranted and desired, in the majority
    of cases due to the sheer complexity and longevity of cloud systems the cost of
    failure is extremely high, so a pragmatic approach is generally recommended but
    your individual requirements may vary, so choose accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate this aspect of a project there is a variety of tooling that can
    be used, but the simplest and the easiest are the GitHub project forks/stars (for
    OSS projects), Google Trends ([https://trends.google.com](https://trends.google.com))
    projections, and general social media feedback from people that have used said
    technology. By looking at movements and shifts in these values, extrapolation
    of long-term viability can be made with a relatively good accuracy and combined
    together with comparisons against existing tooling can create a good picture of
    the general pulse of a project as well. Upwardly-mobile projects generally have
    been indicative of superior technological base but in some cases, this was spurred
    by rejection of existing tooling or a big marketing push, so don't always think
    the popular option is better when evaluating a tool.
  prefs: []
  type: TYPE_NORMAL
- en: '**![](assets/fac939d5-87a1-4de2-80f1-a8b5fbc11d9f.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, you can see a distinct increase over time in interest
    in Kubernetes that somewhat mirrors community adoption and acceptance of that
    orchestration tooling. If we were to implement this technology ourselves, we could
    be reasonably sure that for some period of time that we would be using a tool
    that will be easier to work with and get support for.
  prefs: []
  type: TYPE_NORMAL
- en: 'When comparing Kubernetes against Marathon and using the same technique, things
    get very messy as Marathon is also a very common long-distance running activity,
    so the results get muddled with unrelated Google queries. In the following screenshot,
    we overlaid the results versus a couple of other cloud-related keywords and you
    can see that there''s something wrong with our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f9e6c5e8-01cf-4e88-b1a1-4f87c0199812.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, taking a look at the top-right side of their GitHub pages and the
    forks/stars we can see how they compare (**3,483** stars and **810** forks versus **28,444**
    stars and **10,167** forks):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ad45d9ff-4388-4ece-a644-aff9600199c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Compare the preceding GitHub page with the following page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9b4d4d63-a545-4a67-875d-639c8fb138df.png)'
  prefs: []
  type: TYPE_IMG
- en: In this particular example, though, it is very hard to see long-term trends
    and we've mentioned that these two do not solve the same kind of problems, on
    top of which these two tools have vastly different setup complexity, so proper
    evaluation is really difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'Something that is really important that we should mention before moving on
    to the next dimension: a common and highly-recommended risk mitigation for immature
    tooling (this scenario is much more likely than you might think) is that your
    own developers can be used to fix bugs and add features to relevant upstream projects
    if they are capable and allowed to work on them. If a tool is such a good fit
    for your infrastructure and you can throw development resources behind it, it
    will not make much of a difference if it is popular or not as long as you can
    make it work for you in the way that you are satisfied with.'
  prefs: []
  type: TYPE_NORMAL
- en: As a reference data point, countless times during the development of cloud implementations,
    the teams that I worked on have found bugs and issues in upstream projects that
    we fixed rather quickly and in the process also helped all the other users of
    that software instead of potentially waiting days or weeks for the upstream developers
    to make time to fix them. I would highly encourage this type of approach to contributing
    back being applied to your workplace if possible since it helps the whole project's
    community and indirectly prevents loss of project momentum due to unfixed bugs.
  prefs: []
  type: TYPE_NORMAL
- en: A team's technical competency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: New tooling often has a great initial idea, but due to poor execution or architecture,
    it quickly turns into spaghetti code that is un-maintainable and prone to bugs.
    If design and implementation are kept to high standards, you can have a better
    assurance that you will not get unexpected breakages or at least that the bugs
    can be easier to find and fix. The competency of the core project developers plays
    a huge part in this aspect and since most of the newer tooling is open-source,
    taking a look at the codebase can often be very helpful in this respect.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is near impossible to put exact guidelines for evaluating projects that
    span all sorts of technologies and systems, but there are some red flags that
    should be treated as warning signs of potential troubles in the future for the
    tooling that is used in critical applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lack of tests**: Without tests, assurance that the code works is pretty much
    eliminated and you are hoping that the developer making changes was careful enough
    when implementing new features and that they did not break current functionality.
    I have only seen a handful of developers in my life that can be as mindful of
    all the edge cases as a test harness, but I would not hold my breath that the
    project you are looking into has one of them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clever code**: From time to time, a project will have one or more developers
    that are more concerned about showing their skills off than the maintainability
    of the project they are working on and they will almost always turn files they
    touch into code that only they can work on, causing future problems with adding
    features or fixing bugs. Almost always this type of change is one-directional
    and after a long enough period of time it usually ends up in the death of the
    project (more often than not in my experience).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A high count of critical bugs open for extended periods of time**: For any
    project, there will come a time where you will encounter a critical bug that must
    be fixed as soon as possible, and by seeing trends in how long fixes take, you
    can see whether the team is capable of quickly fixing an issue or whether it pays
    attention to the wider community. While more of a subjective metric, it becomes
    extremely important as the profile or security posture of your service increases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can also use any other metrics for evaluation such as: old and un-merged
    pull requests, arbitrarily closed bug reports, and many more as long as you get
    the right notion of the codebase''s quality. With that knowledge in hand, you
    can properly evaluate what the future might hold for your candidate tooling and
    how your infrastructure can evolve with it.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: And with that, we have reached the end of our book! In this chapter, we have
    covered various things that you will need to take your small service and make
    it global through aggressive automation, splitting things into multiple availability
    zones, and adding monitoring to your infrastructure. Since cloud technologies
    are also relatively young, we have more importantly included some tips on how
    to evaluate emerging tooling as objectively as you can to ensure that your projects
    have the greatest likelihood of success with the tooling ecosystem changes that
    will be common for the foreseeable future. By assuming that things will change
    in the future and having the tools to handle those changes, we can be ready to
    embrace anything that gets thrown at us.
  prefs: []
  type: TYPE_NORMAL
