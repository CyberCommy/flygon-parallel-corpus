- en: Chapter 6. Recursions and Reductions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In previous chapters, we''ve looked at several related kinds of processing
    designs; some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Mapping and filtering that create collections from collections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reductions that create a scalar value from a collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distinction is exemplified by functions such as `map()` and `filter()` that
    accomplish the first kind of collection processing. There are several specialized
    reduction functions, which include `min()`, `max()`, `len(),` and `sum()`. There's
    a general-purpose reduction function, also, `functools.reduce()`.
  prefs: []
  type: TYPE_NORMAL
- en: We'll also consider a `collections.Counter()` function as a kind of reduction
    operator. It doesn't produce a single scalar value per se, but it does create
    a new organization of the data that eliminates some of the original structure.
    At its heart, it's a kind of count-group-by operation that has more in common
    with a counting reduction than with a mapping.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll look at reduction functions in more detail. From a purely
    functional perspective, a reduction is defined recursively. For this reason, we'll
    look at recursion first before we look at reduction algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, a functional programming language compiler will optimize a recursive
    function to transform a call in the tail of the function to a loop. This will
    dramatically improve performance. From a Python perspective, pure recursion is
    limited, so we must do the tail-call optimization manually. The tail-call optimization
    technique available in Python is to use an explicit `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at a number of reduction algorithms including `sum()`, `count()`,
    `max()`, and `min()`. We'll also look at the `collections.Counter()` function
    and related `groupby()` reductions. We'll also look at how parsing (and lexical
    scanning) are proper reductions since they transform sequences of tokens (or sequences
    of characters) into higher-order collections with more complex properties.
  prefs: []
  type: TYPE_NORMAL
- en: Simple numerical recursions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can consider all numeric operations to be defined by recursions. For more
    depth, read about the **Peano axioms** that define the essential features of numbers.
    [http://en.wikipedia.org/wiki/Peano_axioms](http://en.wikipedia.org/wiki/Peano_axioms)
    is one place to start.
  prefs: []
  type: TYPE_NORMAL
- en: From these axioms, we can see that addition is defined recursively using more
    primitive notions of the next number, or successor of a number, *n*, ![Simple
    numerical recursions](graphics/B03652_06_01.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the presentation, we'll assume that we can define a predecessor
    function,![Simple numerical recursions](graphics/B03652_06_02.jpg), such that
    ![Simple numerical recursions](graphics/B03652_06_03.jpg), as long as ![Simple
    numerical recursions](graphics/B03652_06_04.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: 'Addition between two natural numbers could be defined recursively as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple numerical recursions](graphics/B03652_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we use more common ![Simple numerical recursions](graphics/B03652_06_06.jpg)
    and ![Simple numerical recursions](graphics/B03652_06_07.jpg) instead of ![Simple
    numerical recursions](graphics/B03652_06_01.jpg) and ![Simple numerical recursions](graphics/B03652_06_02.jpg),
    we can see that ![Simple numerical recursions](graphics/B03652_06_08.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'This translates neatly in Python, as shown in the following command snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We've simply rearranged common mathematical notation into Python. The `if` clauses
    are placed to the left instead of the right.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, we don't provide our own functions in Python to do simple addition.
    We rely on Python's underlying implementation to properly handle arithmetic of
    various kinds. Our point here is that fundamental scalar arithmetic can be defined
    recursively.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of these recursive definitions include at least two cases: the nonrecursive
    cases where the value of the function is defined directly and recursive cases
    where the value of the function is computed from a recursive evaluation of the
    function with different values.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to be sure the recursion will terminate, it's important to see how
    the recursive case computes values that approach the defined nonrecursive case.
    There are often constraints on the argument values that we've omitted from the
    functions here. The `add()` function in the preceding command snippet, for example,
    can include `assert a>= and b>=0` to establish the constraints on the input values.
  prefs: []
  type: TYPE_NORMAL
- en: Without these constraints. `a-1` can't be guaranteed to approach the nonrecursive
    case of `a == 0`.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, this is obvious. In a few rare cases, it might be difficult to
    prove. One example is the Syracuse function. This is one of the pathological cases
    where termination is unclear.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing tail-call optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the case of some functions, the recursive definition is the one often stated
    because it is succinct and expressive. One of the most common examples is the
    `factorial()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see how this is rewritten as a simple recursive function in Python from
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing tail-call optimization](graphics/B03652_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding formula can be executed in Python by using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This has the advantage of simplicity. The recursion limits in Python artificially
    constrain us; we can't do anything larger than about fact(997). The value of 1000!
    has 2,568 digits and generally exceeds our floating-point capacity; on some systems
    this is about ![Implementing tail-call optimization](graphics/B03652_06_10.jpg)
    Pragmatically, it's common to switch to a `log gamma` function, which works well
    with large floating-point values.
  prefs: []
  type: TYPE_NORMAL
- en: This function demonstrates a typical tail recursion. The last expression in
    the function is a call to the function with a new argument value. An optimizing
    compiler can replace the function call stack management with a loop that executes
    very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Since Python doesn't have an optimizing compiler, we're obliged to look at scalar
    recursions with an eye toward optimizing them. In this case, the function involves
    an incremental change from *n* to *n-1*. This means that we're generating a sequence
    of numbers and then doing a reduction to compute their product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stepping outside purely functional processing, we can define an imperative
    `facti()` calculation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This version of the factorial function will compute values beyond 1000! (2000!,
    for example, has 5733 digits). It isn't purely functional. We've optimized the
    tail recursion into a stateful loop depending on the `i` variable to maintain
    the state of the computation.
  prefs: []
  type: TYPE_NORMAL
- en: In general, we're obliged to do this in Python because Python can't automatically
    do the tail-call optimization. There are situations, however, where this kind
    of optimization isn't actually helpful. We'll look at a few situations.
  prefs: []
  type: TYPE_NORMAL
- en: Leaving recursion in place
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In some cases, the recursive definition is actually optimal. Some recursions
    involve a divide and conquer strategy that minimizes the work from ![Leaving recursion
    in place](graphics/B03652_06_11.jpg) to ![Leaving recursion in place](graphics/B03652_06_12.jpg).
    One example of this is the exponentiation by the squaring algorithm. We can state
    it formally like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Leaving recursion in place](graphics/B03652_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We''ve broken the process into three cases, easily written in Python as a recursion.
    Look at the following command snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This function has three cases. The base case, the `fastexp(a, 0)` method is
    defined as having a value of 1\. The other two cases take two different approaches.
    For odd numbers, the `fastexp()` method is defined recursively. The exponent,
    *n*, is reduced by 1\. A simple tail-recursion optimization would work for this
    case.
  prefs: []
  type: TYPE_NORMAL
- en: For even numbers, however, the `fastexp()` recursion uses `n/2`, chopping the
    problem into half of its original size. Since the problem size is reduced by a
    factor of 2, this case results in a significant speed-up of the processing.
  prefs: []
  type: TYPE_NORMAL
- en: We can't trivially reframe this kind of function into a tail-call optimization
    loop. Since it's already optimal, we don't really need to optimize this further.
    The recursion limit in Python would impose the constraint of ![Leaving recursion
    in place](graphics/B03652_06_14.jpg), a generous upper bound.
  prefs: []
  type: TYPE_NORMAL
- en: Handling difficult tail-call optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can look at the definition of **Fibonacci** numbers recursively. Following
    is one widely used definition for the *nth* Fibonacci number:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Handling difficult tail-call optimization](graphics/B03652_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A given Fibonacci number, ![Handling difficult tail-call optimization](graphics/B03652_06_16.jpg),
    is defined as the sum of the previous two numbers, ![Handling difficult tail-call
    optimization](graphics/B03652_06_17.jpg). This is an example of multiple recursion:
    it can''t be trivially optimized as a simple tail-recursion. However, if we don''t
    optimize it to a tail-recursion, we''ll find it to be too slow to be useful.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a naïve implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This suffers from the multiple recursion problem. When computing the `fib(n)`
    method, we must compute `fib(n-1)` and `fib(n-2)` methods. The computation of
    `fib(n-1)` method involves a duplicate calculation of `fib(n-2)` method. The two
    recursive uses of the Fibonacci function will duplicate the amount of computation
    being done.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the left-to-right Python evaluation rules, we can evaluate values
    up to about `fib(1000)`. However, we have to be patient. Very patient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is an alternative which restates the entire algorithm to use stateful
    variables instead of a simple recursion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our stateful version of this function counts up from 0, unlike the recursion,
    which counts down from the initial value of *n*. It saves the values of ![Handling
    difficult tail-call optimization](graphics/B03652_06_18.jpg) and ![Handling difficult
    tail-call optimization](graphics/B03652_06_19.jpg) that will be used to compute
    ![Handling difficult tail-call optimization](graphics/B03652_06_16.jpg). This
    version is considerably faster than the recursive version.
  prefs: []
  type: TYPE_NORMAL
- en: What's important here is that we couldn't trivially optimize the recursion with
    an obvious rewrite. In order to replace the recursion with an imperative version,
    we had to look closely at the algorithm to determine how many stateful intermediate
    variables were required.
  prefs: []
  type: TYPE_NORMAL
- en: Processing collections via recursion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When working with a collection, we can also define the processing recursively.
    We can, for example, define the `map()` function recursively. The formalism looks
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Processing collections via recursion](graphics/B03652_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We've defined the mapping of a function to an empty collection as an empty sequence.
    We've also specified that applying a function to a collection can be defined recursively
    with a three step expression. First, apply the function to all of the collection
    except the last element, creating a sequence object. Then apply the function to
    the last element. Finally, append the last calculation to the previously built
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a purely recursive function version of the older `map()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The value of the `mapr(f,[])` method is defined to be an empty `list` object.
    The value of the `mapr()` function with a non-empty list will apply the function
    to the last element in the `list` and append this to the list built recursively
    from the `mapr()` function applied to the head of the list.
  prefs: []
  type: TYPE_NORMAL
- en: We have to emphasize that this `mapr()` function actually creates a `list` object,
    similar to the older `map()` function in Python. The Python 3 `map()` function
    is an iterable, and isn't as good an example of tail-call optimization.
  prefs: []
  type: TYPE_NORMAL
- en: While this is an elegant formalism, it still lacks the tail-call optimization
    required. The tail-call optimization allows us to exceed the recursion depth of
    1000 and also performs much more quickly than this naïve recursion.
  prefs: []
  type: TYPE_NORMAL
- en: Tail-call optimization for collections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have two general ways to handle collections: we can use a higher-order function
    which returns a generator expression or we can create a function which uses a
    `for` loop to process each item in a collection. The two essential patterns are
    very similar.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a higher-order function that behaves like the built-in `map()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We've returned a generator expression which produces the required mapping. This
    uses an explicit `for` loop as a kind of tail-call optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a generator function with the same value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This uses a complete `for` statement for the required optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In both cases, the result is iterable. We must do something following this
    to materialize a sequence object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For performance and scalability, this kind of tail-call optimization is essentially
    required in Python programs. It makes the code less than purely functional. However,
    the benefit far outweighs the lack of purity. In order to reap the benefits of
    succinct and expression functional design, it is helpful to treat these less-than-pure
    functions as if they were proper recursions.
  prefs: []
  type: TYPE_NORMAL
- en: What this means, pragmatically, is that we must avoid cluttering up a collection
    processing function with additional stateful processing. The central tenets of
    functional programming are still valid even if some elements of our programs are
    less than purely functional.
  prefs: []
  type: TYPE_NORMAL
- en: Reductions and folding – from many to one
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can consider the `sum()` function to have the following kind of definition:'
  prefs: []
  type: TYPE_NORMAL
- en: We could say that the sum of a collection is 0 for an empty collection. For
    a non-empty collection the sum is the first element plus the sum of the remaining
    elements.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reductions and folding – from many to one](graphics/B03652_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can compute the product of a collection of numbers recursively
    using two cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reductions and folding – from many to one](graphics/B03652_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The base case defines the product of an empty sequence as 1\. The recursive
    case defines the product as the first item times the product of the remaining
    items.
  prefs: []
  type: TYPE_NORMAL
- en: We've effectively folded in `×` or `+` operators between each item of the sequence.
    Further, we've grouped the items so that processing will be done right-to-left.
    This could be called a fold-right way of reducing a collection to a single value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, the product function can be defined recursively as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is technically correct. It's a trivial rewrite from mathematical notation
    to Python. However, it is less than optimal because it tends to create a large
    number of intermediate `list` objects. It's also limited to only working with
    explicit collections; it can't work easily with `iterable` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can revise this slightly to work with an iterable, which avoids creating
    any intermediate `collection` objects. Following is a properly recursive product
    function which works with an iterable source of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can''t interrogate an iterable with the `len()` function to see how many
    elements it has. All we can do is attempt to extract the head of the `iterable`
    sequence. If there are no items in the sequence, then any attempt to get the head
    will raise the `StopIteration` exception. If there is an item, then we can multiply
    this item by the product of the remaining items in the sequence. For a demo, we
    must explicitly create an iterable from a materialized `sequence` object, using
    the `iter()` function. In other contexts, we might have an iterable result that
    we can use. Following is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This recursive definition does not rely on explicit state or other imperative
    features of Python. While it''s more purely functional, it is still limited to
    working with collections of under 1000 items. Pragmatically, we can use the following
    kind of imperative structure for reduction functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This lacks the recursion limits. It includes the required tail-call optimization.
    Further, this will work equally well with either a `sequence` object or an iterable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other functional languages, this is called a `foldl` operation: the operators
    are folded into the iterable collection of values from left-to-right. This is
    unlike the recursive formulations which are generally called `foldr` operations
    because the evaluations are done from right-to-left in the collection.'
  prefs: []
  type: TYPE_NORMAL
- en: For languages with optimizing compilers and lazy evaluation, the fold-left and
    fold-right distinction determines how intermediate results are created. This may
    have profound performance implications, but the distinction might not be obvious.
    A fold-left, for example, could immediately consume and process the first elements
    in a sequence. A fold-right, however, might consume the head of the sequence,
    but not do any processing until the entire sequence was consumed.
  prefs: []
  type: TYPE_NORMAL
- en: Group-by reductions – from many to fewer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very common operation is a reduction that groups values by some key or indicator.
    In **SQL**, this is often called the `SELECT GROUP BY` operation. The raw data
    is grouped by some columns value and reductions (sometimes aggregate functions)
    are applied to other columns. The SQL aggregate functions include `SUM`, `COUNT`,
    `MAX`, and `MIN`.
  prefs: []
  type: TYPE_NORMAL
- en: The statistical summary called the mode is a count that's grouped by some independent
    variable. Python offers us several ways to group data before computing a reduction
    of the grouped values. We'll start by looking at two ways to get simple counts
    of grouped data. Then we'll look at ways to compute different summaries of grouped
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use the trip data that we computed in [Chapter 4](ch04.html "Chapter 4. Working
    with Collections"), *Working with Collections*. This data started as a sequence
    of latitude-longitude waypoints. We restructured it to create legs represented
    by three tuples of start, end, and distance for the `leg`. The data looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'A common operation that can be approached either as a stateful map or as a
    materialized, sorted object is computing the mode of a set of data values. When
    we look at our trip data, the variables are all continuous. To compute a mode,
    we''ll need to quantize the distances covered. This is also called **binning**:
    we''ll group the data into different bins. Binning is common in data visualization
    applications, also. In this case, we''ll use 5 nautical miles as the size of each
    bin.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The quantized distances can be produced with a generator expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This will divide each distance by 5 – discarding any fractions – and then multiply
    by 5 to compute a number that represents the distance rounded down to the nearest
    5 nautical miles.
  prefs: []
  type: TYPE_NORMAL
- en: Building a mapping with Counter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A mapping like the `collections.Counter` method is a great optimization for
    doing reductions that create counts (or totals) grouped by some value in the collection.
    A more typical functional programming solution to grouping data is to sort the
    original collection, and then use a recursive loop to identify when each group
    begins. This involves materializing the raw data, performing a ![Building a mapping
    with Counter](graphics/B03652_06_27.jpg) sort, and then doing a reduction to get
    the sums or counts for each key.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use the following generator to create an simple sequence of distances
    transformed into bins:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We divided each distance by 5 using truncated integer division, and then multiplied
    by 5 to create a value that's rounded down to the nearest 5 miles.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following expression creates a `mapping` from distance to frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This is a stateful object, that was created by – technically – imperative object-oriented
    programming. Since it looks like a function, however, it seems a good fit for
    a design based on functional programming ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we print `Counter(quantized).most_common()` function, we''ll see the following
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The most common distance was about 30 nautical miles. The shortest recorded
    `leg` was four instances of 0\. The longest leg was 125 nautical miles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that your output may vary slightly from this. The results of the `most_common()`
    function are in order by frequency; equal-frequency bins may be in any order.
    These 5 lengths may not always be in the order shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Building a mapping by sorting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we want to implement this without using the `Counter` class, we can use
    a more functional approach of sorting and grouping. Following is a common algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The internal `group()` function steps through the sorted sequence of data items.
    If a given item has already been seen – it matches the value in `previous` – then
    the counter can be incremented. If a given item does not match the previous value
    and the previous value is `not-None`, then we''ve had a change in value; we can
    emit the previous value and the count, and begin a new accumulation of counts
    for the new value. The third condition only applies once: if the previous value
    has never been set, then this is the first value, and we should save it.'
  prefs: []
  type: TYPE_NORMAL
- en: The final line of the function creates a dictionary from the grouped items.
    This dictionary will be similar to a Counter dictionary. The primary difference
    is that a `Counter()` function will have a `most_common()` method function which
    a default dictionary lacks.
  prefs: []
  type: TYPE_NORMAL
- en: The `elif previous is None` method case is an irksome overhead. Getting rid
    of this `elif` clause (and seeing a slight performance improvement) isn't terribly
    difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'To remove the extra `elif` clause, we need to use a slightly more elaborate
    initialization in the internal `group()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This picks the first item out of the set of data to initialize the `previous`
    variable. The remaining items are then processed through the loop. This design
    shows a loose parallel with recursive designs where we initialize the recursion
    with the first item, and each recursive call provides either a next item or `None`
    to indicate that no items are left to process.
  prefs: []
  type: TYPE_NORMAL
- en: We can also do this with `itertools.groupby()`. We'll look at this function
    closely in [Chapter 8](ch08.html "Chapter 8. The Itertools Module"), *The Itertools
    Module*.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping or partitioning data by key values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are no limits to the kinds of reductions we might want to apply to grouped
    data. We might have data with a number of independent and dependent variables.
    We can consider partitioning the data by an independent variable and computing
    summaries like maximum, minimum, average, and standard deviation of the values
    in each partition.
  prefs: []
  type: TYPE_NORMAL
- en: The essential trick to doing more sophisticated reductions is to collect all
    of the data values into each group. The `Counter()` function merely collects counts
    of identical items. We want to create sequences of the original items based on
    a key value.
  prefs: []
  type: TYPE_NORMAL
- en: Looked at in a more general way, each 5-mile bin will contain the entire collection
    of legs of that distance, not merely a count of the legs. We can consider the
    partitioning as a recursion or as a stateful application of `defaultdict(list)`
    object. We'll look at the recursive definition of a `groupby()` function, since
    it's easy to design.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, the `groupby(C, key)` method for an empty collection, `C`, is the empty
    dictionary, `dict()`. Or, more usefully, the empty `defaultdict(list)` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a non-empty collection, we need to work with item `C[0]`, the head, and
    recursively process sequence `C[1:]`, the tail. We can use `head, *tail = C` command
    to do this parsing of the collection, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We need to do the `dict[key(head)].append(head)` method to include the head
    element in the resulting dictionary. And then we need to do the `groupby(tail,key)`
    method to process the remaining elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The interior function handles our essential recursive definition. An empty collection
    returns the provided dictionary. A non-empty collection is parsed into a head
    and tail. The head is used to update the dictionary. The tail is then used, recursively,
    to update the dictionary with all remaining elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can''t easily use Python''s default values to collapse this into a single
    function. We cannot use the following command snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If we try this, all uses of the `group_by()` function share one common `defaultdict(list)`
    object. Python builds default values just once. Mutable objects as default values
    rarely do what we want. Rather than try to include more sophisticated decision-making
    to handle an immutable default value (like `None`), we prefer to use a nested
    function definition. The `wrapper()` function properly initializes the arguments
    to the interior function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can group the data by distance as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We've defined simple, reusable `lambda` which puts our distances into 5 nm bins.
    We then grouped the data using the provided `lambda`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine the binned data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is what the output looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This can also be written as an iteration as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: When doing the tail-call optimization, the essential line of the code in the
    imperative version will match the recursive definition. We've highlighted that
    line to emphasize that the rewrite is intended to have the same outcome. The rest
    of the structure represents the tail-call optimization we've adopted as a common
    way to work around the Python limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Writing more general group-by reductions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have partitioned the raw data, we can compute various kinds of reductions
    on the data elements in each partition. We might, for example, want the northern-most
    point for the start of each leg in the distance bins.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll introduce some helper functions to decompose the tuple as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Each of these helper functions expects a `tuple` object to be provided using
    the `*` operator to map each element of the tuple to a separate parameter of the
    `lambda`. Once the tuple is expanded into the `s`, `e`, and `p` parameters, it's
    reasonably obvious to return the proper parameter by name. It's much more clear
    than trying to interpret the `tuple_arg[2]` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is how we use these helper functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Our initial point object is a nested three tuple with `(0)` - a starting position,
    `(1)` - the ending position, and `(2)` - the distance. We extracted various fields
    using our helper functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given these helpers, we can locate the northern-most starting position for
    the legs in each bin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The data that we grouped by distance included each leg of the given distance.
    We supplied all of the legs in each bin to the `max()` function. The `key` function
    we provided to the `max()` function extracted just the latitude of the starting
    point of the leg.
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us a short list of the northern-most legs of each distance as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Writing higher-order reductions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll look at an example of a higher-order reduction algorithm here. This will
    introduce a rather complex topic. The simplest kind of reduction develops a single
    value from a collection of values. Python has a number of built-in reductions,
    including `any()`, `all()`, `max()`, `min()`, `sum()`, and `len()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we noted in [Chapter 4](ch04.html "Chapter 4. Working with Collections"),
    *Working with Collections*, we can do a great deal of statistical calculation
    if we start with a few simple reductions such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This allows us to define mean, standard deviation, normalized values, correction,
    and even least-squares linear regression using a few simple functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last of our simple reductions, `s2()`, shows how we can apply existing
    reductions to create higher-order functions. We might change our approach to be
    more like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We've added a function that we'll use to transform the data. We'll compute the
    sum of the transformed values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can apply this function in three different ways to compute the three
    essential sums as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We've plugged in a small `lambda` to compute ![Writing higher-order reductions](graphics/B03652_06_23.jpg),
    which is the count, ![Writing higher-order reductions](graphics/B03652_06_24.jpg),
    the sum, and ![Writing higher-order reductions](graphics/B03652_06_25.jpg), the
    sum of the squares, which we can use to compute standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common extension to this includes a filter to reject raw data which is unknown
    or unsuitable in some way. We might use the following command to reject bad data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Execution of the following command snippet allows us to do things like reject
    `None` values in a simple way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This shows how we can provide two distinct `lambda` to our `sum_filter_f()`
    function. The `filter` argument is a `lambda` that rejects `None` values, we've
    called it `valid` to emphasize its meaning. The `function` argument is a `lambda`
    that implements a `count` or a `sum` method. We can easily add a `lambda` to compute
    a sum of squares.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that this function is similar to other examples in that
    it actually returns a function rather than a value. This is one of the defining
    characteristics of higher-order functions, and is pleasantly simple to implement
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Writing file parsers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can often consider a file parser to be a kind of reduction. Many languages
    have two levels of definition: the lower-level tokens in the language and the
    higher-level structures built from those tokens. When looking at an XML file,
    the tags, tag names, and attribute names form this lower-level syntax; the structures
    which are described by XML form a higher-level syntax.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The lower-level lexical scanning is a kind of reduction that takes individual
    characters and groups them into tokens. This fits well with Python''s generator
    function design pattern. We can often write functions that look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: For our purposes, we'll rely on lower-level file parsers to handle this for
    us. We'll use the CSV, JSON, and XML packages to manage these details. We'll write
    higher-level parsers based on these packages.
  prefs: []
  type: TYPE_NORMAL
- en: We'll still rely on a two-level design pattern. A lower-level parser will produce
    a useful canonical representation of the raw data. It will be an iterator over
    tuples of text. This is compatible with many kinds of data files. The higher-level
    parser will produce objects useful for our specific application. These might be
    tuples of numbers, or namedtuples, or perhaps some other class of immutable Python
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'We provided one example of a lower-level parser in [Chapter 4](ch04.html "Chapter 4. Working
    with Collections"), *Working with Collections*. The input was a KML file; KML
    is an XML representation of geographic information. The essential features of
    the parser look similar to the following command snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The bulk of the `row_iter_kml()` function is the XML parsing that allows us
    to use the `doc.findall()` function to iterate through the `<ns0:coordinates>`
    tags in the document. We've used a function named `comma_split()` to parse the
    text of this tag into a three tuple of values.
  prefs: []
  type: TYPE_NORMAL
- en: This is focused on working with the normalized XML structure. The document mostly
    fits the database designer's definitions of **First Normal Form**, that is, each
    attribute is atomic and only a single value. Each row in the XML data had the
    same columns with data of a consistent type. The data values weren't properly
    atomic; we had to split the points on a "," to separate longitude, latitude, and
    altitude into atomic string values.
  prefs: []
  type: TYPE_NORMAL
- en: A large volume of data – xml tags, attributes, and other punctuation – was reduced
    to a somewhat smaller volume including just floating-point latitude and longitude
    values. For this reason, we can think of parsers as a kind of reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll need a higher-level set of conversions to map the tuples of text into
    floating-point numbers. Also, we''d like to discard altitude, and reorder longitude
    and latitude. This will produce the application-specific tuple we need. We can
    use functions as follows for this conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The essential tool is the `float_lat_lon()` function. This is a higher-order
    function which returns a generator expression. The generator uses `map()` function
    to apply the `float()` function conversion to the results of `pick_lat_lon()`
    class. We've used the `*row` parameter to assign each member of the row `tuple`
    to a different parameter of the `pick_lat_lon()` function. This function then
    returns a tuple of the selected items in the required order.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this parser as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This will build a tuple-of-tuple representation of each waypoint along the path
    in the original KML file. It uses a low-level parser to extract rows of text data
    from the original representation. It uses a high-level parser to transform the
    text items into more useful tuples of floating-point values. In this case, we
    have not implemented any validation.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing CSV files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [Chapter 3](ch03.html "Chapter 3. Functions, Iterators, and Generators"),
    *Functions, Iterators and Generators*, we saw another example where we parsed
    a CSV file that was not in a normalized form: we had to discard header rows to
    make it useful. To do this, we used a simple function that extracted the header
    and returned an iterator over the remaining rows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The columns are separated by tab characters. Plus there are three rows of headers
    that we can discard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s another version of that CSV-based parser. We''ve broken it into three
    functions. The first, `row_iter()` function, returns the iterator over the rows
    in a tab-delimited file. The function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This is a simple wrapper around the CSV parsing process. When we look back at
    the previous parsers for XML and plain text, this was the kind of thing that was
    missing from those parsers. Producing an iterable over row tuples can be a common
    feature of parsers for normalized data.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a row of tuples, we can pass rows that contain usable data and
    reject rows that contain other metadata, like titles and column names. We'll introduce
    a helper function that we can use to do some of the parsing, plus a `filter()`
    function to validate a row of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This function handles the conversion of a single `string` to `float` values,
    converting bad data to `None` value. We can embed this function in a mapping so
    that we convert all columns of a row to a `float` or `None` value. The `lambda`
    looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is a row-level validator based on the use of the `all()` function
    to assure that all values are `float` (or none of the values are `None`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is a higher-order function which combines the row-level conversion
    and filtering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This function gives us a slightly more complete pattern for parsing an input
    file. The foundation is a lower-level function that iterates over tuples of text.
    We can then wrap this in functions to convert and validate the converted data.
    For the cases where files are either in first normal form (all rows are the same)
    or a simple validator can reject the other rows, this design works out nicely.
  prefs: []
  type: TYPE_NORMAL
- en: All parsing problems aren't quite this simple, however. Some files have important
    data in header or trailer rows that must be preserved, even though it doesn't
    match the format of the rest of the file. These non-normalized files will require
    a more sophisticated parser design.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing plain text files with headers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [Chapter 3](ch03.html "Chapter 3. Functions, Iterators, and Generators"),
    *Functions, Iterators, and Generators*, the `Crayola.GPL` file was presented without
    showing the parser. This file looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We can parse a text file using regular expressions. We need to use a filter
    to read (and parse) header rows. We also want to return an iterable sequence of
    data rows. This rather complex two-part parsing is based entirely on the two-part
    – head and tail – file structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a low-level parser that handles both head and tail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We've defined a regular expression that parses all four lines of the header,
    and assigned this to the `header_pat` variable. There are two internal functions
    for parsing different parts of the file. The `read_head()` function parses the
    header lines. It does this by reading four lines and merging them into a single
    long `string`. This is then parsed with the regular expression. The results include
    the two data items from the header plus an iterator ready to process additional
    lines.
  prefs: []
  type: TYPE_NORMAL
- en: The `read_tail()` function accepts the output from the `read_head()` function
    and parses the iterator over the remaining lines. The parsed information from
    the header rows forms a two tuple that is given to the `read_tail()` function
    along with the iterator over the remaining lines. The remaining lines are merely
    split on spaces, since that fits the description of the GPL file format.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more information, visit the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://code.google.com/p/grafx2/issues/detail?id=518](https://code.google.com/p/grafx2/issues/detail?id=518).'
  prefs: []
  type: TYPE_NORMAL
- en: Once we've transformed each line of the file into a canonical tuple-of-strings
    format, we can apply the higher level of parsing to this data. This involves conversion
    and (if necessary) validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a higher-level parser command snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'This function will work with the output of the lower-level `row_iter_gpl()`
    parser: it requires the headers and the iterator. This function will use the multiple
    assignment to separate the `color` numbers and the remaining words into four variables,
    `r`, `g`, `b,` and `name`. The use of the `*name` parameter assures that all remaining
    values will be assigned to names as a `tuple`. The `" ".join(name)` method then
    concatenates the words into a single space-separated string.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is how we can use this two-tier parser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We've applied the higher-level parser to the results of the lower-level parser.
    This will return the headers and a tuple built from the sequence of `Color` objects.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've looked at two significant functional programming topics.
    We've looked at recursions in some detail. Many functional programming language
    compilers will optimize a recursive function to transform a call in the tail of
    the function to a loop. In Python, we must do the tail-call optimization manually
    by using an explicit `for` loop instead of a purely function recursion.
  prefs: []
  type: TYPE_NORMAL
- en: We've also looked at reduction algorithms including `sum()`, `count()`, `max(),`
    and `min()` functions. We looked at the `collections.Counter()` function and related
    `groupby()` reductions.
  prefs: []
  type: TYPE_NORMAL
- en: We've also looked at how parsing (and lexical scanning) are similar to reductions
    since they transform sequences of tokens (or sequences of characters) into higher-order
    collections with more complex properties. We've examined a design pattern that
    decomposes parsing into a lower level that tries to produce tuples of raw strings
    and a higher level that creates more useful application objects.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll look at some techniques appropriate to working with
    namedtuples and other immutable data structures. We'll look at techniques that
    make stateful objects unnecessary. While stateful objects aren't purely functional,
    the idea of a class hierarchy can be used to package related method function definitions.
  prefs: []
  type: TYPE_NORMAL
