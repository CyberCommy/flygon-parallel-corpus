- en: Abstracting Data with DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing underlying RDDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance optimizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inferring the schema using reflection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying the schema programmatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a temporary table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SQL to interact with DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of DataFrame transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of DataFrame actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore the current fundamental data structure—DataFrames.
    DataFrames take advantage of the developments in the tungsten project and the
    Catalyst Optimizer. These two improvements bring the performance of PySpark on
    par with that of either Scala or Java.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project tungsten is a set of improvements to Spark Engine aimed at bringing
    its execution process closer to the *bare metal*. The main deliverables include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code generation at runtime**: This aims at leveraging the optimizations implemented
    in modern compilers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Taking advantage of the memory hierarchy**: The algorithms and data structures
    exploit memory hierarchy for fast execution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Direct-memory management**: Removes the overhead associated with Java garbage
    collection and JVM object creation and management'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low-level programming**: Speeds up memory access by loading immediate data
    to CPU registers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Virtual function dispatches elimination**: This eliminates the necessity
    of multiple CPU calls'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check this blog from Databricks for more information: [https://www.databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://www.databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html).
  prefs: []
  type: TYPE_NORMAL
- en: The Catalyst Optimizer sits at the core of Spark SQL and powers both the SQL
    queries executed against the data and DataFrames. The process starts with the
    query being issued to the engine. The logical plan of execution is first being
    optimized. Based on the optimized logical plan, multiple physical plans are derived
    and pushed through a cost optimizer. The selected, most cost-efficient plan is
    then translated (using code generation optimizations implemented as part of the
    tungsten project) into an optimized RDD-based execution code.
  prefs: []
  type: TYPE_NORMAL
- en: Creating DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Spark DataFrame is an immutable collection of data distributed within a cluster.
    The data inside a DataFrame is organized into named columns that can be compared
    to tables in a relational database.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to create Spark DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark 2.3 environment. If
    you do not have one, you might want to go back to [Chapter 1](part0026.html#OPEK0-dc04965c02e747b9b9a057725c821827), *Installing
    and Configuring Spark*, and follow the recipes you find there.
  prefs: []
  type: TYPE_NORMAL
- en: All the code that you will need for this chapter can be found in the GitHub
    repository we set up for the book: [http://bit.ly/2ArlBck](http://bit.ly/2ArlBck);
    go to `Chapter 3` and open the `3\. Abstracting data with DataFrames.ipynb` notebook.
  prefs: []
  type: TYPE_NORMAL
- en: There are no other requirements.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many ways to create a DataFrame, but the simplest way is to create
    an RDD and convert it into a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have read the previous chapter, you probably already know how to create
    RDDs. In this example, we simply call the `sc.parallelize(...)` method.
  prefs: []
  type: TYPE_NORMAL
- en: Our sample dataset contains just a handful of records of the relatively recent
    Apple computers. However, as with all RDDs, it is hard to figure out what each
    element of the tuple stands for since RDDs are schema-less structures.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, when using the `.createDataFrame(...)` method of `SparkSession`,
    we pass a list of column names as the second argument; the first argument is the
    RDD we wish to transform into a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we peek inside the `sample_data` RDD using `sample_data.take(1)`, we
    will retrieve the first record:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To compare the content of a DataFrame, we can run `sample_data_df.take(1)`
    to get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00035.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can now see, a DataFrame is a collection of `Row(...)` objects. A `Row(...)`
    object consists of data that is named, unlike an RDD.
  prefs: []
  type: TYPE_NORMAL
- en: If the preceding `Row(...)` object looks similar to a dictionary to you, you
    are not wrong. Any `Row(...)` object can be converted into a dictionary using
    the `.asDict(...)` method. For more information, check out [http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row).
  prefs: []
  type: TYPE_NORMAL
- en: 'If, however, we were to have a look at the data within the `sample_data_df`
    DataFrame, using the `.show(...)` method, we would see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since DataFrames have schema, let''s see the schema of our `sample_data_df`
    using the `.printSchema()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the columns in our DataFrame have the datatypes matching the
    datatypes of the original `sample_data` RDD.
  prefs: []
  type: TYPE_NORMAL
- en: Even though Python is not a strongly-typed language, DataFrames in PySpark are.
    Unlike RDDs, every element of a DataFrame column has a specified type (these are
    all listed in the `pyspark.sql.types` submodule) and all the data must conform
    to the specified schema.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you use the `.read` attribute of `SparkSession`, it returns a `DataFrameReader`
    object. `DataFrameReader` is an interface to read data into a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: From JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To read data from a JSON-formatted file, you can simply do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The only drawback (although a minor one) of reading the data from a JSON-formatted
    file is the fact that all the columns will be ordered alphabetically. See for
    yourself by running `sample_data_json_df.show()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The datatypes, however, remain unchanged: `sample_data_json_df.printSchema()`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From CSV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reading from a CSV file is equally simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The only additional parameters passed make sure that the method treats the first
    row as column names (the `header` parameter) and that it will attempt to assign
    the right datatype to each column based on the content (the `inferSchema` parameter
    assigns strings by default).
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to reading the data from a JSON-formatted file, reading from a CSV
    file preserves the order of columns.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check Spark's documentation for a full list of supported data formats: [http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing underlying RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Switching to using DataFrames does not mean we need to completely abandon RDDs.
    Under the hood, DataFrames still use RDDs, but of `Row(...)` objects, as explained
    earlier. In this recipe, we will learn how to interact with the underlying RDD
    of a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark 2.3 environment. Also,
    you should have already gone through the previous recipe as we will reuse the
    data we created there.
  prefs: []
  type: TYPE_NORMAL
- en: There are no other requirements.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will extract the size of the HDD and its type into separate
    columns, and will then calculate the minimum volume needed to put each computer
    in boxes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As pointed out earlier, each element of the RDD inside the DataFrame is a `Row(...)`
    object. You can check it by running these two statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'And:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The first one produces a single-item list where the element is `Row(...)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The other also produces a single-item list, but the item is a tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The `sample_data` RDD is the first RDD we created in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, let's now turn our attention to the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load the necessary modules: to work with the `Row(...)` objects,
    we need `pyspark.sql`, and we will use the `.round(...)` method later, so we need
    the `pyspark.sql.functions` submodule.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we extract `.rdd` from `sample_data_df`. Using the `.map(...)` transformation,
    we first add the `HDD_size` column to the schema.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are working with RDDs, we want to retain all the other columns. Thus,
    we first convert the row (which is a `Row(...)` object) into a dictionary using
    the `.asDict()` method, so then we can later unpack it using `**`.
  prefs: []
  type: TYPE_NORMAL
- en: In Python, the single `*` preceding a list of tuples, if passed as a parameter
    to a function, passes each element of a list as a separate argument to the function.
    The double `**` takes the first element and turns it into a keyword parameter,
    and uses the second element as the value to be passed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second argument follows a simple convention: we pass the name of the column
    we want to create (the `HDD_size`), and set it to the desired value. In our first
    example, we split the `.HDD` column and extract the first element since it is `HDD_size`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We repeat this step twice more: first, to create the `HDD_type` column, and
    second, to create the `Volume` column.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use the `.toDF(...)` method to convert our RDD back to a DataFrame.
    Note that you can still use the `.toDF(...)` method to convert a regular RDD (that
    is, where each element is not a `Row(...)` object) to a DataFrame, but you will
    you need to pass a list of column names to the `.toDF(...)` method or you end
    up with unnamed columns.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we `.select(...)` the columns so we can `.round(...)` the newly created
    `Volume` column. The `.alias(...)` method produces a different name for the resulting
    column.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting DataFrame looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00042.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Unsurprisingly, the desktop iMac would require the biggest box.
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Starting with Spark 2.0, the performance of PySpark using DataFrames was on
    apar with that of Scala or Java. However, there was one exception: using **User
    Defined Functions** (**UDFs**); if a user defined a pure Python method and registered
    it as a UDF, under the hood, PySpark would have to constantly switch runtimes
    (Python to JVM and back). This was the main reason for an enormous performance
    hit compared with Scala, which does not need to convert the JVM object to a Python
    object.'
  prefs: []
  type: TYPE_NORMAL
- en: Things have changed significantly in Spark 2.3\. First, Spark started using
    the new Apache project. Arrow creates a single memory space used by all environments,
    thus removing the need for constant copying and converting between objects.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Source: https://arrow.apache.org/img/shared.png
  prefs: []
  type: TYPE_NORMAL
- en: For an overview of Apache Arrow, go to [https://arrow.apache.org](https://arrow.apache.org).
  prefs: []
  type: TYPE_NORMAL
- en: Second, Arrow stores columnar objects in memory giving a big performance boost.
    Thus, in order to further leverage that, portions of the PySpark code have been
    refactored and that brought us vectorized UDFs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will learn how to use them and test the performance of both:
    the old, row-by-row UDFs, and the new vectorized ones.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark 2.3 environment.
  prefs: []
  type: TYPE_NORMAL
- en: There are no other requirements.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will use SciPy to return a value of a normal probability
    distribution function (PDF) for a set of 1,000,000 random numbers between 0 and
    1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, as always, we import all the modules we will need to run this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pyspark.sql.functions` gives us access to PySpark SQL functions. We will use
    it to create our DataFrame with random numbers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pandas` framework will give us access to the `.Series(...)` datatype so
    we can return a column from our UDF.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scipy.stats` give us access to statistical methods. We will use it to calculate
    the normal PDF for our random numbers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, our `big_df`. `SparkSession` has a convenience method, `.range(...)`,
    which allows us to create a range of numbers within specified bounds; in this
    example, we simply create a DataFrame with one million records.
  prefs: []
  type: TYPE_NORMAL
- en: In the next line, we add another column to our DataFrame using the `.withColumn(...)`
    method; the column's name is `val` and it will contain one million `.rand()` numbers.
  prefs: []
  type: TYPE_NORMAL
- en: The `.rand()` method returns pseudo-random numbers drawn from a uniform distribution
    that ranges between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we `.cache()` the DataFrame so it all remains fully in memory (for
    speeding up the process).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define the `pandas_cdf(...)` method. Note the `@f.pandas_udf` decorator
    preceding the method's declaration as this is key to registering a vectorized
    UDF in PySpark and has only became available in Spark 2.3.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we did not have to decorate our method; we could have instead registered
    our vectorized method as `f.pandas_udf(f=pandas_pdf, returnType='double', functionType=f.PandasUDFType.SCALAR)`.
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter to the decorator method is the return type of the UDF, in
    our case a `double`. This can be either a DDL-formatted type string or `pyspark.sql.types.DataType`.
    The second parameter is the function type; if we return a single column from our
    method (such as pandas' `.Series(...)` in our example), it will be `.PandasUDFType.SCALAR`
    (by default). If, on the other hand, we operate on multiple columns (such as pandas'
    `DataFrame(...)`), we would define `.PandasUDFType.GROUPED_MAP`.
  prefs: []
  type: TYPE_NORMAL
- en: Our `pandas_pdf(...)` method simply accepts a single column and returns a pandas'
    `.Series(...)` object with values of normal CDF-corresponding numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we simply use the new method to transform our data. Here''s what the
    top five records look like (yours most likely will look different since we are
    creating one million random numbers):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now compare the performance of the two approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `test_pandas_pdf()` method simply uses the `pandas_pdf(...)` method to retrieve
    the PDF from the normal distribution, performs the `.count(...)` operation, and
    prints out the results using the `.show(...)` method. The `test_pdf()` method
    does the same but uses the `pdf(...)` method instead, which is the row-by-row
    way of using the UDFs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `%timeit` decorator simply runs the `test_pandas_pdf()` or the `test_pdf()`
    methods seven times, multiplied by each execution. Here''s a sample output (abbreviated
    as it is, as you might have expected, highly repetitive) for running the `test_pandas_pdf()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The timings for the `test_pdf()` method are quoted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the vectorized UDFs provide ~100x performance improvements!
    Don't get too excited, as such speedups are only expected for more complex queries,
    such as the one we used previously.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more, check out this blog post from Databricks announcing the vectorized
    UDFs: [https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inferring the schema using reflection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DataFrames have schema, RDDs don't. That is, unless RDDs are composed of `Row(...)`
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to create DataFrames by inferring the schema
    using reflection.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark 2.3 environment.
  prefs: []
  type: TYPE_NORMAL
- en: There are no other requirements.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will first read our CSV sample data into an RDD and then
    create a DataFrame from it. Here''s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we load the SQL module of PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we read the `DataFrames_sample.csv` file using the `.textFile(...)` method
    of SparkContext.
  prefs: []
  type: TYPE_NORMAL
- en: Review the previous chapter if you do not yet know how to read data into RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting RDD looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the RDD still contains the row with column names. In order to
    get rid of it, we first extract it using the `.first()` method and then later
    using the `.filter(...)` transformation to remove any row that is equal to the
    header.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we split each row with a comma and create a `Row(...)` object for each
    observation. Note here that we convert all of the fields to the proper datatypes.
    For example, the `Id` column should be an integer, the `Model` name is a string,
    and `W` (width) is a float.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we simply call the `.createDataFrame(...)` method of SparkSession
    to convert our RDD of `Row(...)` objects into a DataFrame. Here''s the final result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check out Spark's documentation to learn more: [https://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection](https://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying the schema programmatically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we learned how to infer the schema of a DataFrame using
    reflection.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to specify the schema programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark 2.3 environment.
  prefs: []
  type: TYPE_NORMAL
- en: There are no other requirements.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will learn how to specify the schema programmatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we create a list of `.StructField(...)` objects. `.StructField(...)`
    is a programmatic way of adding a field to a schema in PySpark. The first parameter
    is the name of the column we want to add.
  prefs: []
  type: TYPE_NORMAL
- en: The second parameter is the datatype of the data we want to store in the column;
    some of the types  available include `.LongType()`, `.StringType()`, `.DoubleType()`,
    `.BooleanType()`, `.DateType()`, and `.BinaryType()`.
  prefs: []
  type: TYPE_NORMAL
- en: For a full list of available datatypes in PySpark, go to [http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types.](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types.)
  prefs: []
  type: TYPE_NORMAL
- en: The last parameter of `.StructField(...)` indicates whether the column can contain
    null values or not; if set to `True`, it means it can.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we read in the `DataFrames_sample.csv` file using the `.textFile(...)`
    method of SparkContext. We filter out the header, as we will specify the schema
    explicitly and we do not need the name columns that are stored in the first row.
    Next, we split each row with a comma and impose the right datatypes on each element
    so it conforms to the schema we just specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we call the `.createDataFrame(...)` method but this time, along with
    the RDD, we also pass `schema`. The resulting DataFrame looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check out Spark's documentation for more: [https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a temporary table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DataFrames can easily be manipulated with SQL queries in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to create a temporary view so you can access
    the data within DataFrame using SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark 2.3 environment. You
    should have gone through the previous recipe, as we will be using the `sample_data_schema`
    DataFrame we created there.
  prefs: []
  type: TYPE_NORMAL
- en: There are no other requirements.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We simply use the `.createTempView(...)` method of a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.createTempView(...)` method is the simplest way to create a temporary
    view that later can be used to query the data. The only required parameter is
    the name of the view.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how such a temporary view can now be used to extract data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We simply use the `.sql(...)` method of SparkSession, which allows us to write
    ANSI-SQL code to manipulate data within a DataFrame. In this example, we simply
    extract four columns. Here''s what we get back:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you have created a temporary view, you cannot create another view with
    the same name. However, Spark provides another method that allows us to either
    create or update a view: `.createOrReplaceTempView(...)`. As the name suggests,
    by calling this method, we either create a new view if one does not exist, or
    we replace an already existing one with the new one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, we can now use it to interact with the data using SQL queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what we get back:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Using SQL to interact with DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we learned how to create or replace temporary views.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to play with the data within a DataFrame using
    SQL queries.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark 2.3 environment. You
    should have gone through the *Specifying the schema programmatically* recipe,
    as we will be using the `sample_data_schema` DataFrame we created there.
  prefs: []
  type: TYPE_NORMAL
- en: There are no other requirements.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will extend our original data with the form factor for
    each model of Apple''s computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we create a simple DataFrame with two columns: `Model` and `FormFactor`.
    In this example, we use the `.toDF(...)` method of an RDD to quickly convert it
    into a DataFrame. The list that we pass is simply a list of column names and the
    schema will be inferred automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create the model's view and replace `sample_data_view`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to append `FormFactor` to our original data, we simply join the two
    views on the `Model` column. As the `.sql(...)` method accepts regular SQL expressions,
    we also use the `ORDER BY` clause so we can order by weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what we get back:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The SQL queries are not limited to extracting data only. We can also run some
    aggregations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this simple example, we will count how many different computers of different
    FormFactors we have. The `COUNT(*)` operator counts how many computers we have
    and works in conjunction with the `GROUP BY` clause that specifies the aggregation
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what we get from this query:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Overview of DataFrame transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like RDDs, DataFrames have both transformations and actions. As a reminder,
    transformations convert one DataFrame into another, while actions perform some
    computation on a DataFrame and normally return the result to the driver. Also,
    just like the RDDs, transformations in DataFrames are lazy.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will review the most common transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark 2.3 environment. You
    should have gone through the *Specifying schema programmatically* recipe, as we
    will be using the `sample_data_schema` DataFrame we created there.
  prefs: []
  type: TYPE_NORMAL
- en: There are no other requirements.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will list some of the most common transformations available
    for DataFrames. The purpose of this list is not to provide a comprehensive enumeration
    of all available transformations, but to give you some intuition behind the most
    common ones.
  prefs: []
  type: TYPE_NORMAL
- en: The .select(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.select(...)` transformation allows us to extract column or columns from
    a DataFrame. It works the same way as `SELECT` found in SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In SQL syntax, this would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The .filter(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.filter(...)` transformation, in contrast to `.select(...)`, selects only
    rows that pass the condition specified. It can be compared with the `WHERE` statement
    from SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In SQL syntax, the preceding would be equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The .groupBy(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.groupBy(...)` transformation performs data aggregation based on the value
    (or values) from a column (or multiple columns). In SQL syntax, this equates to
    `GROUP BY`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In SQL syntax, this would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The .orderBy(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.orderBy(...)` transformation sorts the results given the columns specified.
    An equivalent from the SQL world would also be `ORDER BY`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00057.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The SQL equivalent would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also change the order of sorting to descending by using the `.desc()`
    switch of a column (the `.col(...)` method). Look at the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Put in SQL syntax, the preceding expression would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The .withColumn(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.withColumn(...)` transformation applies a function to some other columns
    and/or literals (using the `.lit(...)` method) and stores it as a new function.
    In SQL, this could be any method that applies any transformation to any of the
    columns and uses `AS` to assign a new column name. This transformation extends
    the original DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You could achieve the same result with the `.select(...)` transformation. The
    following code will produce the same result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The SQL (T-SQL) equivalent would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The .join(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.join(...)` transformation allow us to join two DataFrames. The first parameter
    is the other DataFrame we want to join with, while the second parameter specifies
    the columns on which to join, and the final parameter specifies the nature of
    the join. Available types are `inner`, `cross`, `outer`, `full`, `full_outer`,
    `left`, `left_outer`, `right`, `right_outer`, `left_semi`, and `left_anti`. In
    SQL, the equivalent is the `JOIN` statement.
  prefs: []
  type: TYPE_NORMAL
- en: If you're not familiar with the `ANTI` and `SEMI` joins, check out this blog: [https://blog.jooq.org/2015/10/13/semi-join-and-anti-join-should-have-its-own-syntax-in-sql/](https://blog.jooq.org/2015/10/13/semi-join-and-anti-join-should-have-its-own-syntax-in-sql/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In SQL syntax, this would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'If we had a DataFrame that would not list every `Model` (note that the `MacBook`
    is missing), then the following code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate a table with some missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `RIGHT` join keeps only the records that are matched with the records in
    the right DataFrame. Thus, look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces a table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00062.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `SEMI` and `ANTI` joins are somewhat recent additions. The `SEMI` join
    keeps all the records from the left DataFrame that are matched with the records
    in the right DataFrame (as with the `RIGHT` join) but *only keeps the columns
    from the left DataFrame*; the `ANTI` join is the opposite of the `SEMI` join—it keeps
    only the records that are not found in the right DataFrame. So, the following
    example of a `SEMI` join is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Whereas the example of an `ANTI` join is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The .unionAll(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.unionAll(...)` transformation appends values from another DataFrame. An
    equivalent in SQL syntax is `UNION ALL`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In SQL syntax, the preceding would read as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The .distinct(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.distinct(...)` transformation returns a list of distinct values from a
    column. An equivalent in SQL would be `DISTINCT`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In SQL syntax, this would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The .repartition(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.repartition(...)` transformation shuffles the data around the cluster
    and combines it into a specified number of partitions. You can also specify the
    column or columns you want to use to perform the partitioning on. There is no
    direct equivalent in the SQL world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces (as expected) this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The .fillna(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.fillna(...)` transformation fills in the missing values in a DataFrame.
    You can either specify a single value and all the missing values will be filled
    in with it, or you can pass a dictionary where each key is the name of the column,
    and the values are to fill the missing values in the corresponding column. No
    direct equivalent exists in the SQL world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We could also specify the dictionary, as the `21.4` value does not really fit
    the `A` column. In the following code, we first calculate averages for each of
    the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The `.toPandas()` method is an action (that we will cover in the next recipe)
    and it returns a pandas DataFrame. The `.to_dict(...)` method of the pandas DataFrame
    converts it into a dictionary, where the `records` parameter produces a regular
    dictionary where each column is the key and each value is the record.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The .dropna(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.dropna(...)` transformation removes records that have missing values.
    You can specify the threshold that translates to a minimum number of missing observations
    in the record that qualifies it to be removed. As with `.fillna(...)`, there is
    no direct equivalent in the SQL world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Specifying `thresh=2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'It retains the first and the fourth records:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00070.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The .dropDuplicates(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.dropDuplicates(...)` transformation, as the name suggests, removes duplicated
    records. You can also specify a subset parameter as a list of column names; the
    method will remove duplicated records based on the values found in those columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: It produces the following result
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The .summary() and .describe() transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.summary()` and `.describe()` transformations produce similar descriptive
    statistics, with the `.summary()` transformation additionally producing quartiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00072.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The .freqItems(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.freqItems(...)` transformation returns a list of frequent items from a
    column. You can also specify a `minSupport` parameter that will throw away items
    that are below a certain threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to Spark's documentation for more transformations: [http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of DataFrame actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformations listed in the previous recipe transform one DataFrame into another.
    However, they only get executed once an action is called on a **DataFrame**.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will provide an overview of the most popular actions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark 2.3 environment. You
    should have gone through the previous recipe, *Specifying schema programmatically*,
    as we will be using the `sample_data_schema` DataFrame we created there.
  prefs: []
  type: TYPE_NORMAL
- en: There are no other requirements.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will list some of the most common actions available for
    DataFrames. The purpose of this list is not to provide a comprehensive enumeration
    of all available transformations, but to give you some intuition behind the most
    common ones.
  prefs: []
  type: TYPE_NORMAL
- en: The .show(...) action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.show(...)` action, by default, shows the top five rows in tabular form.
    You can specify how many records to retrieve by passing an integer as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The .collect() action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.collect()` action, as the name suggests, collects all the results from
    all the worker nodes, and returns them to the driver. Beware of using this method
    on a big dataset as your driver will most likely break if you try to return the
    whole DataFrame of billions of records; use this method only to return small,
    aggregated data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00075.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The .take(...) action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `.take(...)` action works in the same as in RDDs–it returns the specified
    number of records to the driver node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00076.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The .toPandas() action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.toPandas()` action, as the name suggests, converts the Spark DataFrame
    into a pandas DataFrame. The same warning needs to be issued here as with the
    `.collect()` action – the `.toPandas()` action collects all the records from all
    the workers, returns them to the driver, and then converts the results into a
    pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our sample data is tiny, we can do this without any problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what the results look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00077.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to Spark's documentation for more actions: [http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
