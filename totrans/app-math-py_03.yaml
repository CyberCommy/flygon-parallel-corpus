- en: Calculus and Differential Equations
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss various topics related to calculus. Calculus
    is the branch of mathematics that concerns the processes of differentiation and
    integration. Geometrically, the derivative of a function represents the gradient
    of the curve of the function, and the integral of a function represents the area
    below the curve of the function. Of course, these characterizations only hold
    in certain circumstances, but they provide a reasonable foundation for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by looking at calculus for a simple class of functions: the polynomials.
    In the first recipe, we create a class that represents a polynomial and define
    methods that differentiate and integrate the polynomial. Polynomials are convenient
    because the derivative or integral of a polynomial is again a polynomial. Then,
    we use the SymPy package to perform symbolic differentiation and integration on
    more general functions. After that, we see methods for solving equations using
    the SciPy package. Next, we turn our attention to numerical integration (quadrature)
    and solving differential equations. We use the SciPy package to solve ordinary
    differential equations and systems of ordinary differential equations, and then
    use a finite difference scheme to solve a simple partial differential equation.
    Finally, we use the fast Fourier transform to process a noisy signal and filter
    out the noise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with polynomials and calculus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentiating and integrating symbolically using SymPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving equations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating functions numerically using SciPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving simple differential equations numerically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving systems of differential equations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving partial differential equations numerically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using discrete Fourier transforms for signal processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to the scientific Python packages NumPy and SciPy, we also need
    the SymPy package. This can be installed using your favorite package manager,
    such as `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The code for this chapter can be found in the`Chapter 03` folder of the GitHub
    repository at [https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2003](https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2003).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action: [https://bit.ly/32HuH4X](https://bit.ly/32HuH4X).'
  prefs: []
  type: TYPE_NORMAL
- en: Working with polynomials and calculus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Polynomials are among the simplest functions in mathematics and are defined
    as a sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/21d2a94c-cfae-48b2-92cd-b345dd6fe425.png)'
  prefs: []
  type: TYPE_IMG
- en: '*x* represents a placeholder to be substituted, and *a[i]* is a number. Since
    polynomials are simple, they provide an excellent means for a brief introduction
    to calculus. Calculus concerns the *differentiation* and *integration* of functions.
    Integration is, roughly speaking, *anti-differentiation*, in the sense that first
    integrating and then differentiating yields the original function.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will define a simple class that represents a polynomial and
    write methods for this class to perform differentiation and integration.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Geometrically, the *derivative*, obtained by *differentiating*, of a function
    is its *gradient*, and the *integral*, obtained by *integrating*, of a function
    is the area that lies between the curve of the function and the *x* axis, accounting
    for whether the curve lies above or below the axis. In practice, differentiating
    and integrating are done symbolically, using a set of rules and standard results
    that are particularly simple for polynomials.
  prefs: []
  type: TYPE_NORMAL
- en: There are no additional packages required for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps describe how to create a class representing a polynomial
    and implement differentiation and integration methods for this class:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by defining a simple class to represent a polynomial:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have defined a basic class for a polynomial, we can move on to
    implement the differentiation and integration operations for this `Polynomial`
    class to illustrate how these operations change polynomials. We start with differentiation.
    We generate new coefficients by multiplying each element in the current list of
    coefficients without the first element. We use this new list of coefficients to
    create a new `Polynomial` instance that is returned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To implement the integration method, we need to create a new list of coefficients
    containing the new constant (converted to a float for consistency) given by the
    argument. We then add to this list of coefficients the old coefficients divided
    by their new position in the list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to make sure these methods work as expected, we should test these
    two methods with a simple case. We can check this using a very simple polynomial,
    such as *x² - 2x + 1*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Polynomials offer an easy introduction to the basic operations of calculus,
    but it isn't so easy to construct Python classes for other general classes of
    functions. That being said, polynomials are extremely useful because they are
    well understood and, perhaps more importantly, calculus for polynomials is very
    easy. For powers of a variable *x*, the rule for differentiation is to multiply
    by the power and reduce the power by 1, so that *x^n* becomes *nx^(n-1)*.
  prefs: []
  type: TYPE_NORMAL
- en: Integration is more complex, since the integral of a function is not unique.
    We can add any constant to an integral and obtain a second integral. For powers
    of a variable *x*, the rule for integration is to increase the power by 1 and
    divide by the new power, so that *x^n* becomes *x^(n+1)/*(*n+1*), so to integrate
    a polynomial, we increase each power of *x* by 1 and divide the corresponding
    coefficient by the new power.
  prefs: []
  type: TYPE_NORMAL
- en: The `Polynomial` class that we defined in the recipe is rather simplistic, but
    represents the core idea. A polynomial is uniquely determined by its coefficients,
    which we can store as a list of numerical values. Differentiation and integration
    are operations that we can perform on this list of coefficients. We include a
    simple `__repr__` method to help with the display of `Polynomial` objects, and
    a `__call__` method to facilitate evaluation at specific numerical values. This
    is mostly to demonstrate the way that a polynomial is evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomials are useful for solving certain problems that involve evaluating
    a computationally expensive function. For such problems, we can sometimes use
    some kind of polynomial interpolation, where we "fit" a polynomial to another
    function, and then use the properties of polynomials to help solve the original
    problem. Evaluating a polynomial is much "cheaper" than the original function,
    so this can lead to dramatic improvements in speed. This usually comes at the
    cost of some accuracy. For example, Simpson's rule for approximating the area
    under a curve approximates the curve by quadratic polynomials over intervals defined
    by three consecutive mesh points. The area below each quadratic polynomial can
    be calculated easily by integration.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Polynomials have many more important roles in computational programming than
    simply demonstrating the effect of differentiation and integration. For this reason,
    a much richer `Polynomial` class is provided in the NumPy package, `numpy.polynomial`.
    The NumPy `Polynomial` class, and the various derived subclasses, are useful in
    all kinds of numerical problems, and support arithmetic operations as well as
    other methods. In particular, there are methods for fitting polynomials to collections
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy also provides classes, derived from `Polynomial`, that represent various
    special kinds of polynomials. For example, the `Legendre` class represents a specific
    system of polynomials called the **Legendre** polynomials. The Legendre polynomials
    are defined for *x* satisfying *-1 ≤ x ≤ 1* and form an orthogonal system, which
    is important for applications such as numerical integration and the **finite element
    method** for solving partial differential equations. The Legendre polynomials
    are defined using a recursive relation. We define
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/52bf688b-c298-4854-8933-fe2424d18981.png)'
  prefs: []
  type: TYPE_IMG
- en: and for each *n ≥ 2*, we define the *n*th Legendre polynomial to satisfy the
    recurrence relation,
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/70211164-f54e-4915-8a81-c7e8dfb8c69b.png)'
  prefs: []
  type: TYPE_IMG
- en: There are several other so called *orthogonal (systems of) polynomials*, including
    *Laguerre**polynomials*, *Chebyshev polynomials*, and *Hermite polynomials*.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Calculus is certainly well documented in mathematical texts, and there are many
    textbooks that cover from the basic methods all the way to the deep theory. Orthogonal
    systems of polynomials are also well documented among numerical analysis texts.
  prefs: []
  type: TYPE_NORMAL
- en: Differentiating and integrating symbolically using SymPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At some point, you may have to differentiate a function that is not a simple
    polynomial, and you may need to do this in some kind of automated fashion, for
    example, if you are writing software for education. The Python scientific stack
    includes a package called SymPy, which allows us to create and manipulate symbolic
    mathematical expressions within Python. In particular, SymPy can perform differentiation
    and integration of symbolic functions, just like a mathematician.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will create a symbolic function, and then differentiate and
    integrate this function using the SymPy library.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike some of the other scientific Python packages, there does not seem to
    be a standard alias under which SymPy is imported in the literature. Instead the
    documentation uses a star import at several points, which is not in line with
    the PEP8 style guide. This is possibly to make the mathematical expressions more
    natural. We will simply import the module under its name `sympy`, to avoid any
    confusion with the `scipy` package''s standard abbreviation, `sp` (which is the
    natural choice for `sympy` too):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this recipe, we will define a symbolic expression that represents the function
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d45795ee-95b9-4f79-85b3-b118aafc7d48.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Differentiating and integrating symbolically (as you would do by hand) is very
    easy using the SymPy package. Follow these steps to see how it is done:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once SymPy is imported, we define the symbols that will appear in our expressions.
    This is a Python object that has no particular value, just like a mathematical
    variable, but can be used in formulas and expressions to represent many different
    values simultaneously. For this recipe, we need only define a symbol for *x*,
    since we will only require constant (literal) symbols and functions in addition
    to this. We use the `symbols` routine from `sympy` to define a new symbol. To
    keep the notation simple, we will name this new symbol `x`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The symbols defined using the `symbols` function support all of the arithmetic
    operations, so we can construct the expression directly using the symbol `x` we
    just defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use the symbolic calculus capabilities of SymPy to compute the derivative
    of `f`, that is, differentiate `f`. We do this using the `diff` routine in `sympy`,
    which differentiates a symbolic expression with respect to a specified symbol,
    and returns an expression for the derivative. This is often not expressed in its
    simplest form, so we use the `sympy.simplify` routine to simplify the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check whether the result of the symbolic differentiation using SymPy
    is correct, compared to the derivative computed by hand, defined as a SymPy expression,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'SymPy equality tests whether two expressions are equal, but not whether they
    are symbolically equivalent. Therefore, we must first simplify the difference
    of the two statements we wish to test and test for equality to `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can integrate the function `f`using SymPy by using the `integrate`function.
    It is a good idea to also provide the symbol with which the integration is to
    be performed by providing it as the second optional argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SymPy defines various classes to represent certain kinds of expressions. For
    example, symbols, represented by the `Symbol` class, are examples of *atomic expressions*.
    Expressions are built up in a similar way to how Python builds an abstract syntax
    tree from source code. These expression objects can then be manipulated using
    methods and the standard arithmetic operations.
  prefs: []
  type: TYPE_NORMAL
- en: SymPy also defines standard mathematical functions that can operate on the `Symbol`
    objects to create symbolic expressions. The most important feature is the ability
    to perform symbolic calculus – rather than the numerical calculus that we explore
    in the remainder of this chapter – and give exact (sometimes called *analytic*)
    solutions to calculus problems.
  prefs: []
  type: TYPE_NORMAL
- en: The `diff` routine from the SymPy package performs differentiation on these
    symbolic expressions. The result of this routine is usually not in its simplest
    form, which is why we used the `simplify` routine to simplify the derivative in
    the recipe. The `integrate` routine symbolically integrates a `scipy` expression
    with respect to a given symbol. (The `diff` routine also accepts a symbol argument
    that specifies the symbol for differentiating against.) This returns an expression
    whose derivative is the original expression. This routine does not add a constant
    of integration, which is good practice when doing integrals by hand.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SymPy can do much more than simple algebra and calculus. There are submodules
    for various areas of mathematics, such as number theory, geometry, and other discrete
    mathematics (such as combinatorics).
  prefs: []
  type: TYPE_NORMAL
- en: 'SymPy expressions (and functions) can be built into Python functions that can
    be applied to NumPy arrays. This is done using the `lambdify` routine from the
    `sympy.utilities` module. This converts a SymPy expression to a numerical expression
    that uses the NumPy equivalents of the SymPy standard functions to evaluate the
    expressions numerically. The result is similar to defining a Python Lambda, hence
    the name. For example, we could convert the function and derivative from this
    recipe into Python functions using this routine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `lambdify` routine takes two arguments. The first is the variables to be
    provided, `x` in the previous code block, and the second is the expression to
    be evaluated when this function is called. For example, we can evaluate the lambdified
    SymPy expressions defined previously as if they were ordinary Python functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can even evaluate these lambdified expressions on NumPy arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `lambdify` routine uses the Python `exec` routine to execute the code, so
    it should not be used with unsanitized input.
  prefs: []
  type: TYPE_NORMAL
- en: Solving equations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many mathematical problems eventually reduce to solving an equation of the form*f(x*)
    = 0, where *f* is a function of a single variable. Here, we try to find a value
    of *x* for which the equation holds. The values of *x* for which the equation
    holds are sometimes called *roots* of the equation. There are numerous algorithms
    for finding solutions to equations of this form. In this recipe, we will use the
    Newton-Raphson and secant methods to solve an equation of the form *f(x*) = 0.
  prefs: []
  type: TYPE_NORMAL
- en: The Newton-Raphson method (Newton's method) and the secant method are good,
    standard root finding algorithms that can be applied in almost any situation.
    These are*iterative methods* that start with an approximation of the root and
    iteratively improve this approximation until it lies within a given tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate these techniques, we will use the function from the *Symbolic
    calculus using SymPy* recipe defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c13a1277-2b71-46bc-9151-3a68073ce5b6.png)'
  prefs: []
  type: TYPE_IMG
- en: which is defined for all real values of *x* and has exactly two roots, one at*x*
    = 0 and one at*x*= 2.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The SciPy package contains routines for solving equations (among many other
    things). The root finding routines can be found in the `optimize`module from the
    `scipy` package.
  prefs: []
  type: TYPE_NORMAL
- en: If your equation is not in the form *f*(*x*) = *0*, then you will need to rearrange
    it so that this is the case. This is usually not too difficult, and simply requires
    moving any terms on the right-hand side over to the left-hand side. For example,
    if you wish to find the fixed points of a function, that is, when*g*(*x*)*= x*,
    then we would apply the method to the related function given by *f*(*x*) =*g*(*x*)*-
    x.*
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `optimize` package provides routines for numerical root finding. The following
    instructions describe how to use the `newton` routine from this module:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `optimize`module is not listed in the `scipy`namespace, so you must import
    it separately:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we must define this function and its derivative in Python:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The derivative of this function was computed in the previous recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'For both the Newton-Raphson and secant methods, we use the `newton` routine
    from `optimize`. Both the secant method and the Newton-Raphson method require
    the function and the first argument and the first approximation,`x0`, as the second
    argument. To use the Newton-Raphson method, we must provide thederivative of*f*,
    using the `fprime`keyword argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the secant method, only the function is needed, but we must provide
    the first two approximations for the root; the second is provided as the `x1`keyword
    argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Neither the Newton-Raphson nor the secant method are guaranteed to converge
    to a root. It is perfectly possible that the iterates of the method will simply
    cycle through a number of points (periodicity) or fluctuate wildly (chaos).
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Newton-Raphson method for a function*f*(*x*) with derivative*f'*(*x*) and
    initial approximation *x[0]*is defined iteratively using the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f3f99361-b4f0-4dde-8420-e7e76aa123a3.png)'
  prefs: []
  type: TYPE_IMG
- en: for each integer*i* ≥*0*. Geometrically, this formula arises by considering
    the direction in which the gradient is negative (so the function is decreasing)
    if*f(x[i])*>*0* or positive (so the function is increasing) if*f*(*x[i]*) <*o*.
  prefs: []
  type: TYPE_NORMAL
- en: The secant method is based on the Newton-Raphson method, but replaces the first
    derivative by the approximation
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f2412966-ce93-4807-8fb9-cadb324252e5.png)'
  prefs: []
  type: TYPE_IMG
- en: when*x[i]*-*x[i-1]* is sufficiently small, which occurs if the method is converging,
    then this is a good approximation. The price paid for not requiring the derivative
    of the function*f* is that we require an additional initial guess to start the
    method. The formula for the method is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/85704de4-933f-4fd1-86a4-792565350241.png)'
  prefs: []
  type: TYPE_IMG
- en: Generally speaking, if either method is given an initial guess (guesses for
    the secant method) that is sufficiently close to a root, then the method will
    converge to that root. The Newton-Raphson method can also fail if the derivative
    is zero at one of the iterations, in which case the formula is not well defined.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The methods mentioned in this recipe are general purpose methods, but there
    are others that may be faster or more accurate in some circumstances. Broadly
    speaking, root finding algorithms fall into two categories: algorithms that use
    information about the function''s gradient at each iterate (Newton-Raphson, secant,
    Halley) and algorithms that require bounds on the location of a root (bisection
    method, regula-falsi, Brent). The algorithms discussed so far are of the first
    kind, and while generally quite fast, they may fail to converge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second kind of algorithms are those for which a root is known to exist
    within a specified interval*a ≤**x* ≤*b*. We can check whether a root lies within
    such an interval by checking that*f*(*a*) and*f*(*b*) have different signs, that
    is, one of *f*(*a*) <*0*<*f*(*b*) or*f*(*b*) <*0*<*f*(*a*) is true. (Provided,
    of course, that the function is*continuous,* which tends to be the case in practice.)
    The most basic algorithm of this kind is the bisection algorithm, which repeatedly
    bisects the interval until a sufficiently good approximation to the root is found.
    The basic premise is to split the interval between*a*and *b*at the mid-point and
    select the interval in which the function changes sign. The algorithm repeats
    until the interval is very small. The following is a rudimentary implementation
    of this algorithm in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This method is guaranteed to converge, since at each step the distance*b-a*
    is halved. However, it is possible that the method will require more iterations
    than Newton-Raphson or the secant method. A version of the bisection method can
    also be found in`optimize`. This version is implemented in C and is considerably
    more efficient that the version presented here, but the bisection method is not
    the fastest method in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Brent''s method is an improvement on the bisection method, and is available
    in the`optimize` module as`brentq`. It uses a combination of bisection and interpolation
    to quickly find the root of an equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: It is important to note that the techniques that involve bracketing (bisection,
    regula-falsi, Brent) cannot be used to find the root functions of a complex variable,
    whereas those techniques that do not use bracketing (Newton, secant, Halley) can.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating functions numerically using SciPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Integration can be interpreted as the area that lies between a curve and the
    *x*axis, signed according to whether this area is above or below the axis. Some
    integrals cannot be computed directly, using symbolic means, and instead have
    to be approximated numerically. One classic example of this is the Gaussian error
    function, which was mentioned in the *Basic mathematical functions* section in
    [Chapter1](6bc3b1d7-d916-4560-b4a3-8f4001bee082.xhtml), *Basic Packages, Functions,
    and Concepts*. This is defined by the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/97d08d9b-2e67-4e07-88c0-f52be50f2ae1.png)'
  prefs: []
  type: TYPE_IMG
- en: and the integral that appears here cannot be evaluated symbolically.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to use the numerical integration routines in
    the SciPy package to compute the integral of a function.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We use the `scipy.integrate`module, which contains several routines for computing
    numerical integrals. We import this module as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps describe how to numerically integrate a function using
    SciPy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate the integral that appears in the definition of the error function
    at the value *x = 1*. For this, we need to define the integrand (the function
    that appears inside the integral) in Python:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: There are two main routines in `scipy.integrate`for performing numerical integration
    (quadrature) that can be used. The first is the `quad`function, which uses QUADPACK
    to perform the integration, and the second is `quadrature`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `quad`routine is a general-purpose integration tool. It expects three arguments,
    which are the function to be integrated (`erf_integrand`), the lower limit (`-1.0`),
    and the upper limit (`1.0`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The first returned value is the value of the integral and the second is an estimate
    for the error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Repeating the computation with the `quadrature`routine, we get the following.
    The arguments are the same as for the `quad` routine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The output is the same format as the code, with the value of the integral and
    then an estimate of the error. Notice that the error is larger for the `quadrature`routine.
    This is a result of the method terminating once the estimated error falls below
    a given tolerance, which can be modified when the routine is called.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most numerical integration techniques follow the same basic procedure. First,
    we choose points *x[i]*for *i = 1, 2,…, n* in the region of integration, and then
    use these values and the values *f*(*x[i]*) to approximate the integral. For example,
    with the trapezium rule, we approximate the integral by
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ae1f28f9-d69a-446b-a943-b34ce41e4bcb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *a < x[1]< x[2]< … < x[n-1]< b* and *h*is the (common) difference between
    adjacent *x[i]*values, including the end points *a*and *b*. This can be implemented
    in Python as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The algorithms used by `quad`and `quadrature`are far more sophisticated than
    this. Using this function to approximate the integral of `erf_integrand`using
    `trapezium`yields a result of 1.4936463036001209, which agrees with the approximations
    from the `quad` and `quadrature` routines to 5 decimal places.
  prefs: []
  type: TYPE_NORMAL
- en: The `quadrature`routine uses a fixed tolerance Gaussian quadrature, whereas
    the `quad`routine uses an adaptive algorithm implemented in the Fortran library
    QUADPACK routines. Timing both routines, we find that the `quad` routine is approximately
    5 times faster than the `quadrature` routine for the problem described in the
    recipe. The `quad` routine executes in approximately 27 µs, averaged over 1 million
    executions, while the `quadrature` routine executes in approximately 134 µs. (Your
    results may differ depending on your system.)
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The routines mentioned in this section require the integrand function to be
    known, which is not always the case. Instead, it might be the case that we know
    a number of pairs (*x*,y) with *y = f*(*x*), but we don't know the function *f*to
    evaluate at additional points. In this case, we can use one of the sampling quadrature
    techniques from `scipy.integrate`. If the number of known points is very large
    and all points are equally spaced, we can use Romberg integration for a good approximation
    of the integral. For this, we use the `romb`routine. Otherwise, we can use a variant
    of the trapezium rule (as above) using the `trapz`routine, or Simpson's rule using
    the `simps` routine.
  prefs: []
  type: TYPE_NORMAL
- en: Solving simple differential equations numerically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Differential equations arise in situations where a quantity evolves, usually
    over time, according to a given relationship. They are extremely common in engineering
    and physics, and appear quite naturally. One of the classic examples of a (very
    simple) differential equation is the law of cooling devised by Newton. The temperature
    of a body cools at a rate proportional to the current temperature. Mathematically,
    this means that we can write the derivative of the temperature *T* of the body
    at time *t > 0* using the differential equation
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/81295a45-5fdf-4837-86ac-96caf899fa75.png)'
  prefs: []
  type: TYPE_IMG
- en: where *k* is a positive constant that determines the rate of cooling. This differential
    equation can be solved *analytically* by first "separating the variables" and
    then integrating and rearranging. After performing this procedure, we obtain the
    general solution
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ddae7c6c-e2c7-43aa-b764-484f7b27b095.png)'
  prefs: []
  type: TYPE_IMG
- en: where *T[0]* is the initial temperature.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will solve a simple ordinary differential equation numerically
    using the `solve_ivp` routine from SciPy.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will demonstrate the technique for solving a differential equation numerically
    in Python using the cooling equation described previously since we can compute
    the true solution in this case. We take the initial temperature to be *T[0]= 50*
    and *k = 0.2*. Let's also find the solution for *t* values between 0 and 5.
  prefs: []
  type: TYPE_NORMAL
- en: A general (first order) differential equation has the form
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4df58dda-2ef5-4b6e-a9c6-b14742a31fcb.png)'
  prefs: []
  type: TYPE_IMG
- en: where *f* is some function of *t* (the independent variable) and *y* (the dependent
    variable). In this formula, *T* is the dependent variable and *f(t, T) = -kt*.
    The routines for solving differential equations in the SciPy package require the
    function *f* and an initial value *y[0]*[and the range of *t* values where we
    need to compute the solution. To get started, we need to define our function *f*
    in Python and create the variables *y[0]*[and *t* range ready to be supplied to
    the SciPy routine:]]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to define the initial condition from which the solution should
    be found. For technical reasons, the initial *y* values must be specified as a
    one-dimensional NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Since, in this case, we already know the true solution, we can also define
    this in Python ready to compare to the numerical solution that we will compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to solve a differential equation numerically and plot the
    solution along with the error:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the`solve_ivp` routine from the `integrate` module in SciPy to solve
    the differential equation numerically. We add a parameter for the maximum step
    size, with a value of `0.1`, so the solution is computed at a reasonable number
    of points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we extract the solution values from the `sol`object returned from the
    `solve_ivp`method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we plot the solution on a set of axes as follows. Since we are also going
    to plot the approximation error on the same figure, we create two subplots using
    the `subplots` routine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This plots the solution on a set of axes displayed in the left-hand side of
    *Figure 3.1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we need to compute the true solution at the points that we obtained
    from the `solve_ivp` routine, and then calculate the absolute value of the difference
    between the true and approximated solutions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, on the right-hand side of *Figure 3.1*, we plot the error in the approximation
    with a logarithmic scale on the *y* axis. We can then plot this on the right-hand
    side with a logarithmic scale *y* axis using the `semilogy` plot command as we
    saw in [Chapter 2](0f1d7ff9-fbe0-4b22-bee0-a5139e8d363d.xhtml), *Mathematical
    Plotting with Matplotlib*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The left-hand plot in *Figure 3.1* shows decreasing temperature over time,
    while the right-hand plot shows that the error increases as we move away from
    the known value given by the initial condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4c0a6446-02e4-49ec-9940-00d778538fbe.png)Figure 3.1: Plot of the
    numerical solution to the cooling equation obtained using the solve_ivp routine
    with default settings'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most methods for solving differential equations are "time-stepping" methods.
    The pairs (*t[i], y[i]*) are generated by taking small *t* steps and approximating
    the value of the function *y.* This is perhaps best illustrated by Euler's method,
    which is the most basic time-stepping method. Fixing a small step size *h > 0*,
    we form the approximation at the *i*th step using the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/62d74fc6-de22-40c3-a06b-eca3c00eaf85.png)'
  prefs: []
  type: TYPE_IMG
- en: 'starting from the known initial value *y[0].* We can easily write a Python
    routine that performs Euler''s method as follows (there are, of course, many different
    ways to implement Euler''s method; this is a very simple example):'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we set up the method by creating lists that will store the *t* values
    and *y* values that we will return:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Euler''s method continues until we hit the end of the *t* range. Here, we use
    a `while` loop to accomplish this. The body of the loop is very simple; we first
    increment a counter `i`, and then append the new *t* and *y* values to their respective
    lists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The method used by the `solve_ivp` routine, by default, is the Runge-Kutta-Fehlberg
    method (RK45), which has the ability to adapt the step size to ensure that the
    error in the approximation stays within a given tolerance. This routine expects
    three positional arguments: the function *f,* the *t* range on which the solution
    should be found, and the initial *y* value (*T[0]*in our example). Optional arguments
    can be provided to change the solver, the number of points to compute, and several
    other settings.'
  prefs: []
  type: TYPE_NORMAL
- en: The function passed to the `solve_ivp` routine must have two arguments as in
    the general differential equation described in *Getting ready* section. The function
    can have additional arguments, which can be provided using the `args` keyword
    for the `solve_ivp` routine, but these must be positioned after the two necessary
    arguments. Comparing the `euler` routine we defined earlier to the `solve_ivp`
    routine, both with a step size of 0.1, we find that the maximum true error between
    the `solve_ivp` solution is in the order of 10^(-6), whereas the `euler` solution
    only manages an error of 31\. The `euler` routine is working, but the step size
    is much too large to overcome the accumulating error.
  prefs: []
  type: TYPE_NORMAL
- en: The `solve_ivp` routine returns a solution object that stores information about
    the solution that has been computed. Most important here are the `t` and `y` attributes,
    which contain the *t* values on which the solution *y* is computed and the solution
    *y* itself. We used these values to plot the solution we computed. The *y*values
    are stored in a NumPy array of shape `(n, N)`, where `n`is the number of components
    of the equation (here, 1), and `N`is the number of points computed. The *y*values
    held in `sol`are stored in a two-dimensional array, which in this case has 1 row
    and many columns. We use the slice `y[0, :]`to extract this first row as a one-dimensional
    array that can be used to plot the solution in *step 4*.
  prefs: []
  type: TYPE_NORMAL
- en: We use a logarithmically scaled *y* axis to plot the error because what is interesting
    there is the order of magnitude. Plotting it on a non-scaled *y* axis would give
    a line that is very close to the *x* axis, which doesn't show the increase in
    the error as we move through the *t* values. The logarithmically scaled *y* axis
    shows this increase clearly.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `solve_ivp` routine is a convenient interface for a number of solvers for
    differential equations, the default being the Runge-Kutta-Fehlberg (RK45) method.
    The different solvers have different strengths, but the RK45 method is a good
    general-purpose solver.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more detailed instructions on how to add subplots to a figure in Matplotlib,
    see the*Adding subplots* recipe from [Chapter 2](0f1d7ff9-fbe0-4b22-bee0-a5139e8d363d.xhtml),
    *Mathematical Plotting with Matplotlib*.
  prefs: []
  type: TYPE_NORMAL
- en: Solving systems of differential equations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Differential equations sometimes occur in systems consisting of two or more
    interlinked differential equations. A classical example is a simple model of the
    populations of competing species. This is a simple model of competing species
    labeled *P* (the prey)*and *W* (the predators) given by the following equations:*
  prefs: []
  type: TYPE_NORMAL
- en: '*![](assets/e6e64a45-d031-4ff3-9d17-bac1ea19c5ec.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The first equation dictates the growth of the prey species *P*, which, without
    any predators, would be exponential growth. The second equation dictates the growth
    of the predator species *W*, which, without any prey, would be exponential decay.
    Of course, these two equations are *coupled*; each population change depends on
    both populations. The predators consume the prey at a rate proportional to the
    product of their two populations, and the predators grow at a rate proportional
    to the relative abundance of prey (again the product of the two populations).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will will analyze a simple system of differential equations
    and use the SciPy `integrate` module to obtain approximate solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tools for solving a system of differential equations using Python are the
    same as those for solving a single equation. We again use the `solve_ivp` routine
    from the `integrate` module in SciPy. However, this will only give us a predicted
    evolution over time with given starting populations. For this reason, we will
    also employ some plotting tools from Matplotlib to better understand the evolution.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps walk through how to analyze a simple system of differential
    equations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first task is to define a function that holds the system of equations.
    This function needs to take two arguments as for a single equation, except the
    dependent variable *y* (in the notation from the *Solving simple differential
    equations numerically* recipe) will now be an array with as many elements as there
    are equations. Here, there will be two elements. The function we need for the
    example system in this recipe is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have defined the system in Python, we can use the `quiver` routine from
    Matplotlib to produce a plot that will describe how the populations will evolve—given
    by the equations—at numerous starting populations. We first set up a grid of points
    on which we will plot this evolution. It is a good idea to choose a relatively
    small number of points for the `quiver` routine, otherwise it becomes difficult
    to see details in the plot. For this example, we plot the population values between
    0 and 100:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we compute the values of the system at each of these pairs. Notice that
    neither equation in the system is time-dependent (they are autonomous); the time
    variable *t* is unimportant in the calculation. We supply the value `0` for the
    *t* argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The variables `dp` and `dw` now hold the "direction" in which the population
    of *P* and *W* will evolve, respectively, if we started at each point in our grid.
    We can plot these directions together using the `quiver` routine from `matplotlib.pyplot`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting the result of these commands now gives us *Figure 3.2*, which gives
    a "global" picture of how solutions evolve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3e756b45-1557-4de1-99a4-980dfa5dbb69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: A quiver plot showing the population dynamics of two competing
    species modeled by a system of differential equations'
  prefs: []
  type: TYPE_NORMAL
- en: To understand a solution more specifically, we need some initial conditions
    so we can use the `solve_ivp` routine described in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have two equations, our initial conditions will have two values. (Recall
    in the *Solving simple differential equations numerically* recipe, we saw that
    the initial condition provided to `solve_ivp` needs to be a NumPy array.) Let''s
    consider the the initial values *P(0) = 85* and *W(0) = 40*. We define these in
    a NumPy array, being careful to place them in the correct order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use `solve_ivp` from the `scipy.integrate` module. We need to provide
    the `max_step` keyword argument to make sure that we have enough points in the
    solution to give a smooth solution curve:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot this solution on our existing figure to show how this specific
    solution relates to the direction plot we have already produced. We also plot
    the initial condition at the same time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this is shown in *Figure 3.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/23767cc7-85e5-4168-93d3-7ab550febf33.png)Figure 3.3: Solution trajectory
    plotted over a quiver plot showing the general behavior'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The method used for a system of ordinary differential equations is exactly the
    same as for a single ordinary differential equation. We start by writing the system
    of equations as a single vector differential equation,
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/949a688b-96e9-4194-80e5-6c1537087601.png)'
  prefs: []
  type: TYPE_IMG
- en: that can then be solved using a time-stepping method as though **y** were a
    simple scalar value.
  prefs: []
  type: TYPE_NORMAL
- en: The technique of plotting the directional arrows on a plane using the `quiver`
    routine is a quick and easy way of learning how a system might evolve from a given
    state. The derivative of a function represents the gradient of the curve (*x*,
    *u*(*x*)), and so a differential equation describes the gradient of the solution
    function at position *y* and time *t*. A system of equations describes the gradient
    of separate solution functions at a given position **y** and time *t.* Of course,
    the position is now a two-dimensional point, so when we plot the gradient at a
    point, we represent this as an arrow that starts at the point, in the direction
    of the gradient. The length of the arrow represents the size of the gradient;
    the longer the arrow, the "faster" the solution curve will move in that direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we plot the solution trajectory on top of this direction field, we can
    see that the curve (starting at the point) follows the direction indicated by
    the arrows. The behavior shown by the solution trajectory is a *limit cycle*,
    where the solution for each variable is periodic as the two species populations
    grow or decline. This description of the behavior is perhaps more clear if we
    plot each population against time, as seen in *Figure 3.4*. What is not immediately
    obvious from *Figure 3.3* is that the solution trajectory loops around several
    times, but this is clearly shown in *Figure 3.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c2de769a-db1c-4ac4-b755-a8f69457b59d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Plots of populations *P* and *W* against time. Both populations
    exhibit periodic behavior'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The technique of analyzing a system of ordinary differential equations by plotting
    the variables against one another, starting at various initial conditions, is
    called *phase space (plane) analysis.* In this recipe, we used the `quiver` plotting
    routine to quickly generate an approximation of the phase plane for the system
    of differential equations. By analyzing the phase plane of a system of differential
    equations, we can identify different local and global characteristics of the solution,
    such as limit cycles.
  prefs: []
  type: TYPE_NORMAL
- en: Solving partial differential equations numerically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Partial differential equations are differential equations that involve *partial
    derivatives* of functions in two or more variables, as opposed to *ordinary derivatives*
    in only a single variable. Partial differential equations is a vast topic, and
    could easily fill a series of books. A typical example of a partial differential
    equation is the (one-dimensional) *heat equation*
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6184866a-5a48-48db-9c1f-a991e60c83b6.png)'
  prefs: []
  type: TYPE_IMG
- en: where α is a positive constant and *f*(*t*, *x*) is a function. The solution
    to this partial differential equation is a function *u*(*t*, *x*), which represents
    the temperature of a rod, occupying the *x* range 0 ≤ *x ≤* *L*, at a given time
    *t* > 0\. To keep things simple, we will take *f*(*t*, *x*) = 0, which amounts
    to saying that no heating/cooling is applied to the system, α = 1, and *L = 2*.
    In practice, we can rescale the problem to fix the constant α, so this is not
    a restrictive problem. In this example, we will use the boundary conditions
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/245337ee-7e9f-441a-8425-7bbebaec6f5d.png)'
  prefs: []
  type: TYPE_IMG
- en: which are equivalent to saying that the ends of the rod are held at the constant
    temperature 0\. We will also use the initial temperature profile
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9723b438-e560-4a28-a6de-ca9930ec343a.png)'
  prefs: []
  type: TYPE_IMG
- en: This initial temperature profile describes a smooth curve between the values
    of 0 and 2, that peaks at a value of 3, which might be the result of heating the
    rod at the center to a temperature of 3.
  prefs: []
  type: TYPE_NORMAL
- en: We're going to use a method called *finite differences*, where we divide the
    rod into a number of equal segments and the time range into a number of discrete
    steps. We then compute approximations for the solution at each of the segments
    and each time step.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use finite differences to solve a simple partial differential
    equation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need the NumPy package and Matplotlib package, imported
    as `np` and `plt` as usual. We also need to import the `mplot3d` module from `mpl_toolkits`
    since we will be producing a 3D plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We will also need some modules from the SciPy package.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following steps, we work through solving the heat equation using finite
    differences:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first create variables that represent the physical constraints of the
    system: the extent of the bar and the value of α:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We first divide the *x*range into *N* equal*intervals—we take *N = 10*for this
    example—using *N+1*points. We can use the `linspace`routine from NumPy to generate
    these points. We also need the common length of each interval *h:**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*[PRE46]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to set up the steps in the time direction. We take a slightly
    different approach here; we set the time step size *k* and the number of steps
    (implicitly making the assumption that we start at time 0):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In order for the method to behave properly, we must have
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/bda8c9e8-45ba-4021-99ba-9cfaa5b43f1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'otherwise the system can become unstable. We store the left-hand side of this
    in a variable for use in *Step 4*, and use an assertion to check that this inequality
    holds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can construct a matrix that holds the coefficients from the finite difference
    scheme. To do this, we use the `diags`routine from the `scipy.sparse`module to
    create a sparse, tridiagonal matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a blank matrix that will hold the solution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to add the initial profile to the first row. The best way to do this
    is to create a function that holds the initial profile and store the result of
    evaluating this function on the `x` array in the matrix `u` that we just created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can simply loop through each step, computing the next row of the matrix
    `u` by multiplying `A` and the previous row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to visualize the solution we have just computed, we can plot the solution
    as a surface using Matplotlib:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this is the surface plot shown in *Figure 3.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7aca3573-b5b9-40e9-a959-4908ff776868.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Surface plot of the solution of the heat equation over the range
    0 ≤ *x* ≤ 2 computed using the finite difference method with 10 mesh points'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The finite difference method works by replacing each of the derivatives with
    a simple fraction that involves only the value of the function, which we can estimate.
    To implement this method, we first break down the spatial range and time range
    into a number of discrete intervals, separated by mesh points. This process is
    called *discretization*. Then we use the differential equation and the initial
    conditions and boundary conditions to form successive approximations, in a manner
    very similar to the time-stepping methods used by the `solve_ivp` routine in the
    *Solving differential equations numerically* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: In order to solve a partial differential equation such as the heat equation,
    we need at least three pieces of information. Usually, for the heat equation,
    this will come in the form of*boundary conditions* for the spatial dimension,
    which tell us what the behavior is at either end of the rod, and *initial conditions*for
    the time dimension, which is the initial temperature profile over the rod.
  prefs: []
  type: TYPE_NORMAL
- en: The finite difference scheme described previously is usually referred to as
    the **forward time central spatial** (**FTCS**) scheme, since we use the *forward
    finite difference* to estimate the time derivative and the *central finite difference*
    to estimate the (second order) spatial derivative. The formulas for these finite
    differences are given by
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1d60f63d-ee29-4640-ab39-515bf356bad2.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ea3d52cb-c07d-4a36-a313-713d29ff4f74.png)'
  prefs: []
  type: TYPE_IMG
- en: Substituting these approximations into the heat equation, and using the approximation
    *u*[*i*]^(*j*)for the value of *u*(*t[j]*, x[i]) after *j*time steps at the *i*spatial
    point, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c3c59b14-5e8f-4dcb-9122-65f09009fd65.png)'
  prefs: []
  type: TYPE_IMG
- en: which can be rearranged to obtain the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/58222441-072c-406c-a4c3-07fe3033a114.png)'
  prefs: []
  type: TYPE_IMG
- en: Roughly speaking, this equation says that the next temperature at a given point
    depends on the surrounding temperatures at the previous time. This also shows
    why the condition on the `r` value is necessary; if the condition does not hold,
    the middle term on the right-hand side will be negative.
  prefs: []
  type: TYPE_NORMAL
- en: We can write this system of equations in matrix form,
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/37370a20-7786-4985-9625-53e11fbdbe08.png)'
  prefs: []
  type: TYPE_IMG
- en: where **u***^j* is a vector containing the approximation *u[i]^j*and matrix
    *A*, which was defined in *step 4*. This matrix is tridiagonal, which means the
    non-zero entries appear on, or adjacent to, the leading diagonal. We use the `diag`
    routine from the SciPy `sparse` module, which is a utility for defining these
    kinds of matrices. This is very similar to the process described in the *Solving
    equations* recipe of this chapter. The first and last row of this matrix have
    zeros, except in the topleft and bottom right, respectively, that represent the
    (non-changing) boundary conditions. The other rows have coefficients that are
    given by the finite difference approximations for the derivatives on either side
    of differential equation. We first create the diagonal entries and the entries
    above and below the diagonal, and then we use the `diags` routine to create the
    sparse matrix. The matrix should have *N+1* rows and columns, to match the number
    of mesh points, and we set the data type as double-precision floats and CSR format.
  prefs: []
  type: TYPE_NORMAL
- en: The initial profile gives us the vector **u**⁰, and from this first point, we
    can compute each subsequent time step by simply performing a matrix multiplication,
    as we saw in *step 7*.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The method we describe here is rather crude since the approximation can become
    unstable, as we mentioned, if the relative sizes of time steps and spatial steps
    are not carefully controlled. This method is *explicit* since each time step is
    computed explicitly using only information from the previous time step. There
    are also *implicit* methods, which give a system of equations that can be solved
    to obtain the next time step. Different schemes have different characteristics
    in terms of the stability of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: When the function *f*(*t*, *x*) is not 0, we can easily accommodate this change
    by instead using the assignment
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/34ccf6b9-a549-469c-a194-2d67678204d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where the function is suitably vectorized to make this formula valid. In terms
    of the code used to solve the problem, we need only include the definition of
    the function and then change the loop of the solution as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Physically, this function represents an external heat source (or sink) at each
    point along the rod. This may change over time, which is why, in general, the
    function should have both *t* and *x* as arguments (though they need not both
    be used).
  prefs: []
  type: TYPE_NORMAL
- en: The boundary conditions we gave in this example represent the ends of the rod
    being kept at a constant temperature of 0\. These kinds of boundary conditions
    are sometimes called *Dirichlet* boundary conditions. There are also *Neumann*
    boundary conditions, where the derivative of the function *u* is given at the
    boundary. For example, we might have been given the boundary conditions
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/20efdf73-2579-492b-ad91-657fd131d376.png)'
  prefs: []
  type: TYPE_IMG
- en: which could be interpreted physically as the ends of the rod being insulated
    so that heat cannot escape through the end points. For such boundary conditions
    we need to modify the matrix *A* slightly, but otherwise the method remains the
    same. Indeed, inserting an imaginary *x* value to the left of the boundary and
    using the backward finite difference at the left-hand boundary (*x = 0*), we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3df7eb78-60b0-45a4-8ac8-a3d354f85c02.png)'
  prefs: []
  type: TYPE_IMG
- en: Using this in the second order finite difference approximation, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a8b3a28a-812c-4e04-bfc3-e40be3cb4481.png)'
  prefs: []
  type: TYPE_IMG
- en: which means that the first row of our matrix should contain *1-r*,*then *r*,
    followed by 0\. Using a similar computation for the right-hand limit gives a similar
    final row of the matrix:*
  prefs: []
  type: TYPE_NORMAL
- en: '*[PRE55]'
  prefs: []
  type: TYPE_NORMAL
- en: For more complex problems involving partial differential equations, it is probably
    more appropriate to use a *finite elements* solver. Finite element methods use
    a more sophisticated approach for computing solutions than partial differential
    equations, which are generally more flexible than the finite difference method
    we saw in this recipe. However, this comes at the cost of requiring more setup
    that relies on more advanced mathematical theory. On the other hand, there is
    a Python package for solving partial differential equations using finite element
    methods such as FEniCS ([fenicsproject.org](https://fenicsproject.org)). The advantage
    of using a package such as FEniCS is that they are usually tuned for performance,
    which is important when solving with complex problems to high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The FEniCS documentation gives a good introduction to the finite element method
    and a number of examples of using the package to solve various classic partial
    differential equations. A more comprehensive introduction to the method and the
    theory is given in the following book:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Johnson, C. (2009).Numerical solution of partial differential equations by
    the finite element method. Mineola, N.Y.: Dover Publications.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more details on how to produce three-dimensional surface plots using Matplotlib,
    see the *Surface and contour plots* recipe from [Chapter 2](0f1d7ff9-fbe0-4b22-bee0-a5139e8d363d.xhtml),
    *Mathematical Plotting with Matplotlib*.
  prefs: []
  type: TYPE_NORMAL
- en: Using discrete Fourier transforms for signal processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most useful tools coming from calculus is the *Fourier transfor**m*.
    Roughly speaking, the Fourier transform changes the representation, in a reversible
    way, of certain functions. This change of representation is particularly useful
    in dealing with signals represented as a function of time. In this instance, the
    Fourier transform takes the signal and represents it as a function of frequency;
    we might describe this as transforming from signal space to frequency space. This
    can be used to identify the frequencies present in a signal for identification
    and other processing. In practice, we will usually have a discrete sample of a
    signal, so we have to use the *discrete Fourier transform* to perform this kind
    of analysis. Fortunately, there is a computationally efficient algorithm, called
    the **fast Fourier transform***(**FFT**), for applying the discrete Fourier transform
    to a sample.*
  prefs: []
  type: TYPE_NORMAL
- en: '*We will follow a common process for filtering a noisy signal using FFT. The
    first step is to apply the FFT and use the data to compute the power spectral
    density of the signal. Then we identify the peaks and filter out the frequencies
    that do no contribute a sufficiently large amount to the signal. Then we apply
    the inverse FFT to obtain the filtered signal.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we use the FFT to analyze a sample of a signal and identify
    the frequencies present and clean the noise from the signal.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we will only need the NumPy and Matplotlib packages imported
    as `np` and `plt`, as usual.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these instructions to use the FFT to process a noisy signal:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a function that will generate our underlying signal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create our sample signal by adding some Gaussian noise to the underlying
    signal. We also create an array that holds the true signal at the sample *t* values
    for convenience later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `fft`module from NumPy to compute discrete Fourier transforms. We
    import this from NumPy before we start our analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'To see what the noisy signal looks like, we can plot the sample signal points
    with the true signal superimposed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot created here is shown in *Figure 3.6*. As we can see, the noisy signal
    does not bear much resemblance to the true signal (shown with the dashed line):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d0bf476c-f0fd-4b59-b167-0db423f6cc22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Noisy signal sample with true signal superimposed'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will use the discrete Fourier transform to extract the frequencies
    that are present in the sample signal. The `fft` routine in the `fft` module performs
    the FFT (discrete Fourier transform):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fft` module provides a routine for constructing the appropriate frequency
    values called `fftfreq`. For convenience, we also generate an array containing
    the integers at which the positive frequencies occur:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, compute the **power spectral density** (**PSD**) of the signal as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can plot the PSD of the signal for the positive frequencies and use
    this plot to identify the frequencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The result can be seen in *Figure 3.7*. We can see in this diagram that there
    are spikes at roughly 4 and 7, which are the frequencies of the signal that we
    defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dbf2eebf-e200-4bc8-8319-4a6d67561fc7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Power spectral density of a signal generated using the FFT'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can identify these two frequencies to try and reconstruct the true signal
    from the noisy sample. All of the minor peaks that appear are not larger than
    10,000, so we can use this as a cut-off value for the filter. Let''s now extract
    from the list of all positive frequency indices the (hopefully 2) indices that
    correspond to the peaks above 10,000 in the PSD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a new, clean spectrum that contains only the frequencies that
    we have extracted from the noisy signal. We do this by creating an array that
    contains only 0, and then copying the value of `spectrum` from those indices that
    correspond to the filtered frequencies and the negatives thereof:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we use the inverse FFT (using the `ifft` routine) to transform this clean
    spectrum back to the time domain of the original sample. We take the real part
    using the `real` routine from NumPy to eliminate the erroneous imaginary parts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plot this filtered signal over the true signal and compare the
    results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of *step 12* is shown in *Figure 3.8*. We can see that the filtered
    signal closely matches the true signal, except for some small discrepancies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c00691ae-14b1-4470-bda0-b8b048536c69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Plot comparing the filtered signal generated using FFTs and filtering
    to the true signal'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Fourier transform* of a function *f*(*t*) is given by the integral
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4eb1fffb-6aa9-4f17-875a-51c90d9c3040.png)'
  prefs: []
  type: TYPE_IMG
- en: and the discrete Fourier transform is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/87485bb8-515f-4654-9621-e8930510e1c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the *f[k]*values are the sample values as complex numbers. The discrete
    Fourier transform can be computed using the preceding formula, but in practice
    this is not efficient. Computing using this formula is *O*(*N*²). The FFT algorithm
    improves the complexity to *O*(*N* log *N*), which is significantly better. The
    book *Numerical Recipes* (full bibliographic details given in the *Further reading*
    section) gives a very good description of the FFT algorithm and the discrete Fourier
    transform.
  prefs: []
  type: TYPE_NORMAL
- en: We will apply the discrete Fourier transform to a sample generated from a known
    signal (with known frequency modes) so we can see the connection between the results
    we obtain and the original signal. To keep this signal simple, we created a signal
    that has only two frequency components with values 4 and 7\. From this signal,
    we generated the sample that we analyzed. Because of the way the FFT works, it
    is best if the sample has a size that is a power of 2; if this isn't the case,
    we can pad the sample with zero elements to make this the case. We add some Gaussian
    noise to the sample signal, which takes the form of a normally distributed random
    number.
  prefs: []
  type: TYPE_NORMAL
- en: The array returned by the `fft` routine contains *N+1*elements, where *N*is
    the sample size. The element that index 0 corresponds to is the 0 frequency, or
    DC shift. The next *N/2* elements are the values corresponding to the positive
    frequencies, and the final *N/2*elements are the values corresponding to the negative
    frequencies. The actual values of the frequencies are determined by the number
    of sampled points *N*and the sample spacing, which, in this example, is stored
    in `sample_d`.
  prefs: []
  type: TYPE_NORMAL
- en: The power spectral density at the frequency *ω* is given by the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6c55d8f3-b759-4931-a310-85478b2e65c6.png)'
  prefs: []
  type: TYPE_IMG
- en: where *H*(*ω*) represents the Fourier transformof the signal at frequency *ω*.
    The power spectral density measures the contribution of each frequency to the
    overall signal, which is why we see peaks at approximately 4 and 7\. Since Python
    indexing allows us to use negative indices for elements starting from the end
    of the sequence, we can use the positive index array to get both the positive
    and negative frequency elements from `spectrum`.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 9,* we extracted the indices of the two frequencies that peak above
    10,000 on the plot. The frequencies that correspond to these indices are 3.984375
    and 6.97265625, which are not exactly equal to 4 and 7, but are very close. The
    reason for this discrepancy is the fact that we have sampled a continuous signal
    using a finite number of points. (Using more points will, of course, yield better
    approximations.)
  prefs: []
  type: TYPE_NORMAL
- en: In *step 11*, we took the real part of the data returned from the inverse FFT.
    This is because, technically speaking, the FFT works with complex data. Since
    our data contained only real data, we expect that this new signal should also
    contain only real data. However, there will be some small errors made, meaning
    that the results are not totally real. We can remedy this by taking the real part
    of the inverse FFT. This is appropriate because we can see that the imaginary
    parts are very small.
  prefs: []
  type: TYPE_NORMAL
- en: We can see in *Figure 3.8* that the filtered signal very closely matches the
    true signal, but not exactly. This is because, as mentioned previously, we are
    approximating a continuous signal with a relatively small sample.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Signal processing in a production setting would probably make use of a specialized
    package, such as the `signal` module from `scipy`, or some lower-level code or
    hardware to perform filtering or cleaning of a signal. This recipe should be taken
    as more of a demonstration of the use of FFT as a tool for working with data sampled
    from some kind of underlying periodic structure (the signal). FFTs are useful
    for solving partial differential equations, such as the heat equation seen in
    the *Solving partial differential equations numerically* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More information about random numbers and the normal distribution (Gaussian)
    can be found in [Chapter 4](5da67d86-40e0-4cc5-9dd1-26b6d52369af.xhtml), *Working
    with Randomness and Probability*.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Calculus is a very important part of every undergraduate mathematics course.
    There are a number of excellent textbooks on calculus, including the classic textbook
    by Spivak and the more comprehensive course by Adams and Essex:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Spivak, M. (2006). Calculus. 3rd ed. Cambridge: Cambridge University Press*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adams, R. and Essex, C. (2018). Calculus: A Complete Course. 9th ed. Don Mills,
    Ont: Pearson.Guassian*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A good source for numerical differentiation and integration is the classic
    *Numerical Recipes* book, which gives a comprehensive description of how to solve
    many computational problems in C++, including a summary of the theory:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Press, W., Teukolsky, S., Vetterling, W. and Flannery, B. (2007). Numerical
    recipes:* *The Art of Scientific Computing. 3rd ed. Cambridge: Cambridge University
    Press*****'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
