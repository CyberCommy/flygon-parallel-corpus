- en: Machine Learning for Finance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is being rapidly adopted for a range of applications in the
    financial services industry. The adoption of machine learning in financial services
    has been driven by both supply factors, such as technological advances in data
    storage, algorithms, and computing infrastructure, and by demand factors, such
    as profitability needs, competition with other firms, and supervisory and regulatory
    requirements. Machine learning in finance includes algorithmic trading, portfolio
    management, insurance underwriting, and fraud detection, just to name a few subject
    areas.
  prefs: []
  type: TYPE_NORMAL
- en: There are several types of machine learning algorithms, but the two main ones
    that you will commonly come across in machine learning literature are supervised
    and unsupervised machine learning. Our discussion in this chapter focuses on supervised
    learning. Supervised machine learning involves supplying both the input and output
    data to help the machine predict new input data. Supervised machine learning can
    be regression-based or classification-based. Regression-based machine learning
    algorithms predict continuous values, while classification-based machine learning
    algorithms predict a class or label.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be introduced to machine learning, study its concepts
    and applications in finance, and look at some practical examples for applying
    machine learning to assist in trading decisions. We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Explore the uses of machine learning in finance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised and unsupervised machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification-based and regression-based machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using scikit-learn for implementing machine learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying single-asset regression-based machine learning in predicting prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding risk metrics in measuring regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying multi-asset regression-based machine learning in predicting returns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying classification-based machine learning in predicting trends
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding risk metrics in measuring classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before machine learning algorithms became mature, many software application
    decisions were rule-based, consisting of a bunch of `if` and `else` statements
    to generate the appropriate response in exchange to some input data. A commonly
    cited example is a spam filter function in email inboxes. A mailbox may contain
    blacklisted words defined by a mail server administrator or owner. Incoming emails
    have their contents scanned against blacklisted words, and should the blacklist
    condition hold true, the mail is marked as spammed and sent to the `Junk` folder.
    As the nature of unwanted emails continues to evolve to avoid detection, spam
    filter mechanisms must also continuously update themselves to keep up with doing
    a better job. However, with machine learning, spam filters can automatically learn
    from past email data and, given an incoming email, calculate the possibility of
    classifying whether the new email is spam or not.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithms behind facial recognition and image detection largely work in
    the same way. Digital images stored in bits and bytes are collected, analyzed,
    and classified according to expected responses provided by the owner. This process
    is known as **training**, using a **supervised learning** approach. The trained
    data may subsequently be used for predicting the next set of input data as some
    output response with a certain level of confidence. On the other hand, when the
    training data does not contain the expected response, the machine learning algorithm is
    expected to learn from the training data, and this process is called **unsupervised
    learning**.
  prefs: []
  type: TYPE_NORMAL
- en: Uses of machine learning in finance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is increasingly finding its uses in many areas of finance,
    such as data security, customer service, forecasting, and financial services.
    A number of use cases leverage big data and **artificial intelligence** (**AI**)
    as well; they are not exclusive to machine learning. In this section, we will
    examine some of the ways in which machine learning is transforming the financial
    sector.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic trading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning algorithms study the statistical properties of the prices of
    highly correlated assets, measure their predictive power on historical data during
    backtesting, and forecast prices to within certain accuracy. Machine learning
    trading algorithms may involve the analysis of the order book, market depth and
    volume, news releases, earnings calls, or financial statements, where the analysis
    translates into price movement possibilities and is taken into account for generating
    trading signals.
  prefs: []
  type: TYPE_NORMAL
- en: Portfolio management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of *robo advisors* has been gaining popularity in recent years to
    act as automated hedge fund managers. They aid with portfolio construction, optimization,
    allocation, and rebalancing, and even suggest to clients the instruments to invest
    in based on their risk tolerance and preferred choice of investment vehicle. These
    advisories serve as a platform for interacting with a digital financial planner,
    providing financial advice and portfolio management.
  prefs: []
  type: TYPE_NORMAL
- en: Supervisory and regulatory functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Financial institutions and regulators are adopting the use of AI and machine
    learning to analyze, identify, and flag suspicious transactions that warrant further
    investigation. Supervisors such as the **Securities and Exchange Commission**
    (**SEC**) take a data-driven approach and employ AI, machine learning, and natural
    language processing to identify behavior that warrants enforcement. Worldwide,
    central authorities are developing machine learning capabilities in regulatory
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Insurance and loan underwriting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Insurance companies actively use AI and machine learning to augment some insurance
    sector functions, improve pricing and marketing of insurance products, and to
    reduce claims processing times and operational costs. In loan underwriting, many
    data points of a single consumer, such as age, income, and credit score, are compared
    against a database of candidates in building credit risk profiles, determining
    credit scores, and calculating the possibility of loan defaults. Such data relies
    on transaction and payment history from financial institutions. However, lenders
    are increasingly turning to social media activities, mobile phone usage, and messaging
    activities to capture a more holistic view of creditworthiness, speed up lending
    decisions, limit incremental risk, and improve the rating accuracy of loans.
  prefs: []
  type: TYPE_NORMAL
- en: News sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Natural language processing, a subset of machine learning, may be used to analyze
    alternative data, financial statements, news announcements, and even Twitter feeds,
    in creating investment sentiment indicators used by hedge funds, high-frequency
    trading firms, social trading, and investment platforms for analyzing markets
    in real time. Politicians' speeches, or important new releases, such as those
    made by central banks, are also being analyzed in real time, where each and every
    word is being scrutinized and calculated to predict which asset prices could move
    and by how much. Machine learning will not only understand the movement of stock
    prices and trades, but also understand social media feeds, news trends, and other
    data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning beyond finance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is increasingly being employed in areas of facial recognition,
    voice recognition, biometrics, trade settlement, chatbots, sales recommendations,
    content creation, and more. As machine learning algorithms improve and their rate
    of adoption picks up, the list of use cases becomes even longer.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin our journey in machine learning by understanding some of the terminology
    you will come across in the machine learning literature.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised and unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many types of machine learning algorithms, but the two main ones that
    you will commonly come across are supervised and unsupervised machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Supervised learning predicts a certain output from given inputs. These pairings
    of input to output data are known as **training data**. The quality of the prediction
    entirely depends on the training data; incorrect training data reduces the effectiveness
    of the machine learning model. An example is a dataset of transactions with labels
    identifying which ones are fraudulent, and which are not. A model can then be
    built to predict whether a new transaction will be fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: Some common algorithms in supervised learning are logistic regression, the support
    vector machine, and random forests.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning builds a model based on given input data that does not
    contain labels, but instead is asked to detect patterns in the data. This may
    involve identifying clusters of observations with similar underlying characteristics.
    Unsupervised learning aims to make accurate predictions to new, never-before-seen
    data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, an unsupervised learning model may price illiquid securities by
    looking for a cluster of securities with similar characteristics. Common unsupervised
    learning algorithms include k-means clustering, principal component analysis,
    and autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: Classification and regression in supervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two major types of supervised machine algorithms, mainly classification
    and regression. Classification machine learning models attempt to predict and
    classify responses from a list of predefined possibilities. These predefined possibilities
    may be binary classification (such as a *Yes* or *No* response to a question:
    *Is this email spam?*) or multi-class classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Regression machine learning models attempt to predict continuous output values.
    For example, predicting housing prices or the temperature expects a continuous
    range of output values. Common forms of regressions are **ordinary least squares**
    (**OLS**) regression, LASSO regression, ridge regression, and elastic net regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting and underfitting models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Poor performance in machine learning models can be caused by overfitting or
    underfitting. An overfitted machine learning model is one that is trained too
    well with the training data such that it leads to negative performance on new
    data. This occurs when the training data is fitted to every minor variation, including
    noise and random fluctuations. Unsupervised learning algorithms are highly susceptible
    to overfitting, since the model learns from every piece of data, both good and
    bad.
  prefs: []
  type: TYPE_NORMAL
- en: An underfitted machine learning model gives poor accuracy of prediction. It
    may be caused by too little training data being available to build an accurate
    model, or that the data is not suitable for extracting its underlying trends.
    Underfitting models are easy to detect since they give consistently poor performance.
    To improve such models, provide more training data or use another machine learning
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A feature is an attribute of the data that defines its characteristic. By using
    domain knowledge of the data, features can be created to help machine learning
    algorithms increase their predictive performance. This can be as simple as grouping
    or bucketing related parts of the existing data to form defining features. Even
    removing unwanted features is also feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, suppose we have the following time series price data that looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **No.** | **Date and Time** | **Price** | **Price Action** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2019-01-02 09:00:01 | 55.00 | UP |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2019-01-02 10:03:42 | 45.00 | DOWN |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2019-01-02 10:31:23 | 48.00 | UP |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2019-01-02 11:14:02 | 33.00 | DOWN |'
  prefs: []
  type: TYPE_TB
- en: 'Grouping the time series into buckets by the hour of the day and taking the
    last price action in each bucket, we end up with a feature like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **No.** | **Hour of Day** | **Last Price Action** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 9 | UP |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 10 | UP |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 11 | DOWN |'
  prefs: []
  type: TYPE_TB
- en: 'The process of feature engineering involves these four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Brainstorming features to include in the training model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating those features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Checking how the features work with the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeating from step 1 until the features work perfectly
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are absolutely no hard and fast rules when it comes to what constitutes
    creating features. Feature engineering is considered more of an art than a science.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn for machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scikit-learn is a Python library designed for scientific computing and contains
    a number of state-of-the-art machine learning algorithms for classification, regression,
    clustering, dimensionality reduction, model selection, and preprocessing. Its
    name is derived from the SciPy Toolkit, which is an extension of the SciPy module.
    Comprehensive documentation on scikit-learn can be found at [https://scikit-learn.org](https://scikit-learn.org).
  prefs: []
  type: TYPE_NORMAL
- en: SciPy is a collection of Python modules for scientific computing, containing
    a number of core packages, such as NumPy, Matplotlib, IPython, and others.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using scikit-learn''s machine learning algorithms
    to predict securities movements. Scikit-learn require a working installation of
    NumPy and SciPy. Install scikit-learn with the `pip` package manager by using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Predicting prices with a single-asset regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pairs trading is a common statistical arbitrage trading strategy employed by
    traders using a pair of co-integrated and highly positively correlated assets,
    though negatively correlated pairs can also be considered.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will use machine learning to train regression-based models
    using the historical prices of a pair of securities that might be used in pairs
    trading. Given the current price of one security for a particular day, we predict
    the other security's price on a daily basis. The following examples uses the historical
    daily prices of **Goldman Sachs** (**GS**) and **J.P. Morgan** (**JPM**) traded
    on the **New York Stock Exchange** (**NYSE**). We will be predicting prices of
    JPM's stock price for the year 2018.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression by OLS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin our investigation of regression-based machine learning with a
    simple linear regression model. A straight line is in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/f7b4925f-5179-47a2-bf43-8b887bf19c6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This attempts to fit the data by OLS:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a* is the slope or coefficient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*c* is the value of the *y*-intercept'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x* is the input dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](Images/b25ba907-2ae5-48c0-a41b-538f32b462f6.png)is the predicted value
    from the straight line'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The coefficients and intercept are determined by minimizing the cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/2b7ae4b3-76ee-4adb-8a94-272eef594751.png)'
  prefs: []
  type: TYPE_IMG
- en: '*y* is the dataset of observed actual values used in performing a straight-line
    fit. In other words, we are performing a least sum of squared errors in finding
    the coefficients *a* and *c*, from which we can predict the current period.'
  prefs: []
  type: TYPE_NORMAL
- en: Before developing a model, let's download and prepare the required datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the independent and target variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s obtain the datasets of GS and JPM prices from Alpha Vantage with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `pandas` DataFrame objects `df_jpm` and `df_gs` contain the downloaded prices
    of JPM and GS respectively. We will be extracting the adjusted closing prices
    from the fifth column of each dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s prepare our independent variables with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The adjusted closing prices of GS are extracted to a new DataFrame object, `df_x`.
    Next, obtain our target variables with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The adjusted closing prices of JPM are extracted to the `jpm_prices` variable as
    a `pandas` Series object. Having prepared our datasets for use in modeling, let's
    proceed to develop the linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the linear regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will create a class for using a linear regression model to fit and predict
    values. This class also serves as a base class for implementing other models in
    this chapter. The following steps illustrates this process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Declare a class named `LinearRegressionModel` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the constructor of our new class, we declare a pandas DataFrame called `df_result`
    to store the actual and predicted values for plotting on a chart later on. The `get_model()` method
    returns an instance of the `LinearRegression` class in the `sklearn.linear_model`
    module for fitting and predicting the data. The `set_intercept` parameter is set
    to `True` as the data is not centered (around 0 on the *x*- and *y*-axes, that
    is).
  prefs: []
  type: TYPE_NORMAL
- en: More information about the `LinearRegression` of scikit-learn can be found at [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).
  prefs: []
  type: TYPE_NORMAL
- en: The `get_prices_since()` method slices a subset of the supplied dataset with
    the `iloc` command, from the given date index `date_since` and up to a number
    of earlier periods defined by the lookback value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a method named `learn()` into the `LinearRegressionModel` class, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `learn()` method serves as the entry point for running the model. It accepts
    the `df` and `ys` parameters as our independent and target variables, `start_date`
    and `end_date` as strings corresponding to the index of the dataset for the period
    we will be predicting, and the `lookback_period` parameter as the number of historical
    data points used for fitting the model in the current period.
  prefs: []
  type: TYPE_NORMAL
- en: The `for` loop simulates a backtest on a daily basis. The call to `get_prices_since()`
    fetches a subset of the dataset for fitting the model on the *x*- and *y*-axes
    with the `fit()` command. The `ravel()` command transforms the given `pandas`
    Series object into a flattened list of target values for fitting the model.
  prefs: []
  type: TYPE_NORMAL
- en: The `x_current` variable represents independent variable values for the specified
    date, fed into the `predict()` method. The predicted output is a `list` object,
    from which we extract the first value. Both the actual and predicted values are
    saved to the `df_result` DataFrame, indexed by the current date as a `pandas`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s instantiate this class and run our machine learning model by issuing
    the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the `learn()` command, we provided our prepared datasets, `df_x` and `jpm_prices`,
    and specified the prediction for the year of 2018\. For this example, we assumed
    there are 20 trading days in a month. Using a `lookback_period` value of `20`,
    we are using a past month's prices to fit our model for prediction daily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s retrieve the resulting `df_result` DataFrame from the model and plot
    both the actual and predicted values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `style` parameter, we specified that actual values are to be drawn as
    a solid line, and predicted values drawn as dotted lines. This gives us the following
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/13a145be-ebca-4b73-90da-a02f4b8fd62c.png)'
  prefs: []
  type: TYPE_IMG
- en: The chart shows our predicted results trailing closely behind the actual values
    up to a certain extent. How well does our model actually perform? In the next
    section, we will discuss several common risk metrics for measuring regression-based
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Risk metrics for measuring prediction performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `sklearn.metrics` module implements several regression metrics for measuring
    prediction performance. We will discuss the mean absolute error, the mean squared
    error, the explained variance score, and the R² score in subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error as a risk metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **mean absolute error** (**MAE**) is a risk metric that measures the average
    absolute prediction error and can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/9ba0fced-2749-4951-93ce-76bea007fb68.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y* and ![](Images/0b48f0bb-7913-4104-a460-d7f2da9a005c.png) are the actual
    and predicted lists of values, respectively, with the same length, *n*. ![](Images/9f3fb134-6d08-4e20-b86e-90a5de21fdf5.png) and *y[i]* are
    the predicted and actual values, respectively, at the index *i**.* Taking the
    absolute values of errors means that our output results in a positive decimal
    value. Low values of MAE are highly desired. A perfect score of 0 implies that
    our prediction powers are exactly aligned with actual values, since there are
    no differences between the two.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtain the MAE value of our predictions using the `mean_abolute_error` function
    of the `sklearn.metrics `module with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The MAE of our linear regression model is 2.458.
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error as a risk metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like the MAE, the **mean squared error** (**MSE**) is a risk metric that measures
    the average of the squares of the prediction errors and can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/e43040d0-6942-4890-be73-193e3ac3b317.png)'
  prefs: []
  type: TYPE_IMG
- en: Squaring the errors means that values of MSE are always positive, and low values
    of MSE are highly desired. A perfect MSE score of 0 implies that our prediction
    powers are exactly aligned with actual values, and that the squares of such differences
    are negligible. While the application of both the MSE and MAE helps determine
    the strength of our model's predictive powers, MSE triumphs over MAE by penalizing
    errors that are farther away from the mean. Squaring the errors places a heavier
    bias on the risk metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtain the MSE value of our predictions using the `mean_squared_error` function
    of the `sklearn.metrics` module with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The MSE of our linear regression model is 12.156.
  prefs: []
  type: TYPE_NORMAL
- en: Explained variance score as a risk metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The explained variance score explains the dispersion of errors of a given dataset,
    and the formula is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/24beb2c6-1765-419a-b919-cdf3608a9789.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *![](Images/186aff43-e401-4808-b8d8-8b6adc1bd9b0.png)* and *Var*(*y*) is
    the variance of prediction errors and actual values respectively. Scores close
    to 1.0 are highly desired, indicating better squares of standard deviations of
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtain the explained variance score of our predictions using the `explained_variance_score`
    function of the `sklearn.metrics` module with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The explained variance score of our linear regression model is 0.533.
  prefs: []
  type: TYPE_NORMAL
- en: R2 as a risk metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The R² score is also known as the **coefficient of determination**, and it
    measures how well future samples are likely to be predicted by the model. It is
    written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/9c360c02-0496-4e6f-a56b-5c31879971cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](Images/40eec259-3732-44f6-ba5e-f5edf10bd6a2.png) is the mean of actual
    values and can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/bdb47b9a-bf50-47c2-a325-2f1cd33d08b8.png)'
  prefs: []
  type: TYPE_IMG
- en: R² scores ranges from negative values to 1.0\. A perfect R² score of 1 implies
    that there is no error in the regression analysis, while a score of 0 indicates
    that the model always predicts the mean of target values. A negative R² score
    indicates that the prediction performs below average.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtain the R² score of our predictions using the `r2_score` function of the
    `sklearn.metrics` module with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The R² of our linear regression model is 0.4167\. This implies that 41.67% of
    the variability of the target variables have been accounted for.
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ridge regression, or L2 regularization, addresses some of the problems
    of OLS regression by penalizing the sum of squares of the model coefficients.
    The cost function for the ridge regression can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/7918e635-6bd5-4727-b68b-81d215aaa5f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the α parameter is expected to be a positive value that controls the amount
    of shrinkage. Larger values of alpha give greater shrinkage, making the coefficients
    more robust to collinearity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Ridge` class of the `sklearn.linear_model` module implements ridge regression.
    To implement this model, create a class named `RidgeRegressionModel` that extends
    the `LinearRegressionModel` class, and run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the new class, the `get_model()` method is overridden to return the ridge regression
    model of scikit-learn while reusing the other methods in the parent class. The
    `alpha` value is set to 0.5, and the rest of the model parameters are left as
    defaults. The `ridge_reg_model` variable represents an instance of our ridge regression
    model, and the `learn()` command is run with the usual parameters values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a function called `print_regression_metrics()` to print the various
    risk metrics covered earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass the `df_result` variable to this function and display the risk metrics
    to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Both mean error scores of the ridge regression model are lower than the linear
    regression model and are closer to zero. The explained variance score and the
    R² score are higher than the linear regression model and are closer to 1\. This
    indicates that our ridge regression model is doing a better job of prediction
    than the linear regression model. Besides having better performance, ridge regression
    computations are less costly than the original linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Other regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `sklearn.linear_model` module contains various regression models that we
    can consider implementing in our model. The remaining sections briefly describe
    them. A full list of linear models is available at [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model).
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to ridge regression, **Least Absolute Shrinkage and Selection Operator** (**LASSO**)
    regression is also another form of regularization that involves penalizing the
    sum of absolute values of regression coefficients. It uses the L1 regularization
    technique. The cost function for the LASSO regression can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fd0a41c4-b24d-43b6-bcfa-7e8970f31cae.png)'
  prefs: []
  type: TYPE_IMG
- en: Like ridge regression, the alpha parameter α controls the strength of the penalty.
    However, for geometric reasons, LASSO regression produces different results than
    ridge regression since it forces a majority of the coefficients to be set to zero.
    It is better suited for estimating sparse coefficients and models with fewer parameter
    values.
  prefs: []
  type: TYPE_NORMAL
- en: The `Lasso` class of `sklearn.linear_model` implements LASSO regression.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Elastic net is another regularized regression method that combines the L1 and
    L2 penalties of the LASSO and ridge regression methods. The cost function for
    elastic net can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/310632bc-d7e6-4ea2-b5a5-df69bc56c116.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The alpha values are explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/689be77f-a002-41e5-97fa-b52f59a26e5b.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](Images/4b0169a9-8d38-4ad6-8371-c4cb9a12fe28.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, `alpha` and `l1_ratio` are parameters of the `ElasticNet` function. When
    `alpha` is zero, the cost function is equivalent to an OLS. When `l1_ratio` is
    zero, the penalty is a ridge or L2 penalty. When `l1_ratio` is 1, the penalty
    is a LASSO or L1 penalty. When `l1_ratio` is between 0 and 1, the penalty is a
    combination of L1 and L2.
  prefs: []
  type: TYPE_NORMAL
- en: The `ElasticNet` class of `sklearn.linear_model` implements elastic net regression.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used a single-asset, trend-following momentum strategy by regression to predict
    the prices of JPM using GS, with the assumption that the pair is cointegrated
    and highly correlated. We can also consider cross-asset momentum to obtain better
    results from diversification. The next section explores multi-asset regression
    for predicting security returns.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting returns with a cross-asset momentum model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will create a cross-asset momentum model by having the prices
    of four diversified assets predict the returns of JPM on a daily basis for the
    year of 2018\. The prior 1-month, 3-month, 6-month, and 1-year of lagged returns
    of the S&P 500 stock index, 10-year treasury bond index, US dollar index, and
    gold prices will be used for fitting our model. This gives us a total of 16 features. Let's
    begin by preparing our datasets for developing our models.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the independent variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use Alpha Vantage again as our data provider. As this free service
    does not provide all of the dataset required for our investigation, we shall consider
    other close assets as a proxy. The ticker symbol for the S&P 500 stock index is
    SPX. We will use the SPDR Gold Trust (ticker symbol: GLD) to denote a share of
    the gold bullion as a proxy for gold prices. The Invesco DB US Dollar Index Bullish
    Fund (ticker symbol: UUP) will proxy the US dollar index. The iShares 7-10 Year
    Treasury Bond ETF (ticker symbol: IEF) will proxy the 10-year Treasury Bond Index. Run
    the following code to download our datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ts` variable is the `TimeSeries` object of Alpha Vantage created in the
    previous section. Combine the adjusted closing prices into a single `pandas` DataFrame
    named `df_assets` with the following codes and remove empty values with the `dropna()`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the lagged percentage returns of our `df_assets` dataset with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `pct_change()` command, the `periods` parameter specifies the number
    of periods to shift. We assumed 20 trading days in a month when calculating the
    lagged returns. Combine the four `pandas` DataFrame objects into a single DataFrame
    with the `join()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `info()` command to view its properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The output is truncated, but you can see 16 features as our independent variables
    spanning the years 2008 to 2019\. Let's continue to obtain the dataset for our
    target variables.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the target variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The closing prices of JPM having been downloaded to the `pandas` Series object
    `jpm_prices` earlier, simply calculate the actual percentage returns with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We obtain a `pandas` Series object as our target variable `y`.
  prefs: []
  type: TYPE_NORMAL
- en: A multi-asset linear regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we used a single asset with the prices of GS for fitting
    our linear regression model. This same model, `LinearRegressionModel`, accommodates
    multiple assets. Run the following commands to create an instance of this model
    and use our new datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the linear regression model instance, `multi_linear_model`, the `learn()`
    command is supplied with the `df_lagged` dataset with 16 features and `y` as the
    percentage changes of JPM. The `lookback_period` value is reduced in consideration
    of the limited lagged returns data available. Let''s plot the actual versus predicted
    percentage changes of JPM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This would give us the following graph in which the solid lines show the actual
    percentage returns of JPM, while the dotted lines show the predicted percentage
    returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/c417dd6d-9ece-4f2a-b9bd-964481cb61ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'How well did our model perform? Let''s run the same performance metrics in
    the `print_regression_metrics()` function defined in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The explained variance score and R² scores are in the negative range, suggesting
    that the model performs below average. Can we perform better? Let's explore more
    complex tree models used in regression.
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble of decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are widely used models for classification and regression tasks,
    much like a binary tree, where each node represents a question leading to a yes-no
    answer for traversing the respective left and right nodes. The goal is to get
    to the right answer by asking as few questions as possible.
  prefs: []
  type: TYPE_NORMAL
- en: A paper describing deep neural decision trees can be found at [https://arxiv.org/pdf/1806.06988.pdf](https://arxiv.org/pdf/1806.06988.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Traversing deep down decision trees can quickly lead to overfitting of the given
    data, rather than inferring the overall properties of the distributions from which they
    are drawn. To address this issue of overfitting, the data can be split into subsets
    and train on different trees, each on a subset. This way, we end up with an ensemble
    of different decision tree models. When random subsets of the samples are drawn
    with replacements for prediction, this method is called **bagging** or **bootstrap
    aggregation**. We may or may not get consistent results across these models, but
    the final model obtained by averaging the bootstrapped models yields better results
    than using a single decision tree. Using an ensemble of randomized decision trees
    is known as **random forests**.
  prefs: []
  type: TYPE_NORMAL
- en: Let's visit some decision tree models in scikit-learn that we may consider implementing
    in our multi-asset regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging regressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `BaggingRegressor` class of `sklearn.ensemble` implements the bagging regressor.
    We can see how a bagging regressor works for multi-asset predictions of the percentage
    returns of JPM. The following code illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We created a class named `BaggingRegressorModel` that extends `LinearRegressionModel`,
    and the `get_model()` method is overridden to return the bagging regressor. The
    `n_estimators` parameter specifies `20` base estimators or decision trees in the
    ensemble, with the `random_state` parameter as a seed of `0` used by the random
    number generator. The rest of the parameters are default values. We run this model
    with the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the same performance metrics and see how our model performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The MAE and MSE values indicate that an ensemble of decision trees produces
    fewer prediction errors than the simple linear regression model. Also, though
    the explained variance score and the R² scores are negative, it indicates a better
    variance of data towards the mean than is offered by the simple linear regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient tree boosting regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient tree boosting, or simply gradient boosting, is a technique of improving
    or boosting the performance of weak learners using a gradient descent procedure
    to minimize the loss function. Tree models, usually decision trees, are added
    one at a time and build the model in a stage-wise fashion, while leaving the existing
    trees in the model unchanged. Since gradient boosting is a greedy algorithm, it
    can overfit a training dataset quickly. However, it can benefit from regularization
    methods that penalize various parts of the algorithm and reduce overfitting to
    improve its performance.
  prefs: []
  type: TYPE_NORMAL
- en: The `sklearn.ensemble` module provides a gradient-boosting regressor called
    `GradientBoostingRegressor`.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forests consist of multiple decision trees each based on a random sub-sample
    of the training data and uses averaging to improve the predictive accuracy and
    to control overfitting. Selection by random inadvertently introduces some form
    of bias. However, due to averaging, it variance also decreases, helping to compensate
    for the increase in bias, and is considered to yield an overall better model.
  prefs: []
  type: TYPE_NORMAL
- en: The `sklearn.ensemble` module provides a random forest regressor called `RandomForestRegressor`.
  prefs: []
  type: TYPE_NORMAL
- en: More ensemble models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `sklearn.ensemble` module contains various other ensemble regressors, as
    well as classifier models. More information can be found at [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble).
  prefs: []
  type: TYPE_NORMAL
- en: Predicting trends with classification-based machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification-based machine learning is a supervised machine learning approach
    in which a model learns from given input data and classifies it according to new
    observations. Classification may be bi-class, such as identifying whether an option
    should be exercised or not, or multi-class, such as the direction of a price change,
    which can be either up, down, or unchanging.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look again at creating cross-asset momentum models
    by having the prices of four diversified assets predict the daily trend of JPM on
    a daily basis for the year of 2018\. The prior 1-month and 3-month lagged returns of
    the S&P 500 stock index, the 10-year treasury bond index, the US dollar index,
    and gold prices will be used to fit the model for prediction. Our target variables
    consist of Boolean indicators, where a `True` value indicates an increase or no-change
    from the previous trading day's closing price, and a `False` value indicates a
    decrease.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by preparing the dataset for our models.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the target variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already downloaded the JPM dataset to the `pandas` DataFrame, `df_jpm`,
    in a previous section, and the `y` variable contains the daily percentage change
    of JPM. Convert these values to labels with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `head()` command, we can see that the `y_direction` variable becomes
    a `pandas` Series object of Boolean values. A percentage change of zero or more
    classifies the value with a `True` label, and `False` otherwise. Let''s extract
    unique values with the `unique()` command as column names for use later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The column names are extracted to a variable called `flags`. With our target
    variables ready, let's continue to obtain our independent multi-asset variables.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the dataset of multiple assets as input variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be reusing the `pandas` DataFrame variables, `df_assets_1m` and `df_assets_3m`,
    from the previous section containing the lagged 1-month and 3-month percentage
    returns of the four assets and combine them into a single variable, `df_input`,
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `info()` command to view its properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The output is truncated, but you can see we have eight features as our independent
    variables spanning the years 2007 to 2019. With our input and target variables
    created, let's explore the various classifiers available in scikit-learn for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Despite its name, logistic regression is actually a linear model used for classification.
    It uses a logistic function, also known as a **sigmoid** function, to model the
    probabilities describing the possible outcomes of a single trial. A logistic function
    helps to map any real-valued number to a value between 0 and 1\. A standard logistic
    function is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/c47c9d05-fbda-41d4-85d1-563d19452828.png)'
  prefs: []
  type: TYPE_IMG
- en: '*e* is the base of the natural logarithm, and *x* is the *X*-value of the sigmoid''s
    midpoint. ![](Images/3da6b246-a810-490a-ae25-c0bb3259dafd.png) is the predicted
    real value between 0 and 1, to be converted to a binary equivalent of 0 or 1 either
    by rounding or a cut-off value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `LogisticRegression` class of the `sklean.linear_model` module implements
    logistic regression. Let''s implement this classifier model by writing a new class
    named `LogisticRegressionModel` that extends `LinearRegressionModel` with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The same underlying linear regression logic is used in our new classifier model.
    The `get_model()` method is overridden to return an instance of the `LogisticRegression`
    classifier model, using the LBFGS solver algorithm in the optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: A paper on the **limited-memory** **Broyden**–**Fletcher**–**Goldfarb**–**Shanno**
    (**LBFGS**) algorithm for machine learning can be read at [https://arxiv.org/pdf/1802.05374.pdf](https://arxiv.org/pdf/1802.05374.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an instance of this model and provide our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, the parameter values indicate that we are interested in performing predictions
    for the year of 2018, and we will be using a `lookback_period` value of `100`
    as the number of daily historical data points when fitting our model. Let''s inspect
    the results stored in `df_result` with the `head()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Date** | **Actual** | **Predicted** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **2018-01-02** | True | True |'
  prefs: []
  type: TYPE_TB
- en: '| **2018-01-03** | True | True |'
  prefs: []
  type: TYPE_TB
- en: '| **2018-01-04** | True | True |'
  prefs: []
  type: TYPE_TB
- en: '| **2018-01-05** | False | True |'
  prefs: []
  type: TYPE_TB
- en: '| **2018-01-08** | True | True |'
  prefs: []
  type: TYPE_TB
- en: Since our target variables are Boolean values, the model outputs predict Boolean
    values as well. But how well does our model perform? In the following sections,
    we will explore risk metrics for measuring our predictions. These metrics are
    different from those used for regression-based predictions in earlier sections.
    Classification-based machine learning takes another approach for measuring output
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: Risk metrics for measuring classification-based predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore common risk metrics for measuring classification-based
    machine learning predictions, namely the confusion matrix, accuracy score, precision
    score, recall score, and F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A confusion matrix, or error matrix, is a square matrix that helps to visualize
    and describe the performance of a classification model for which the true values
    are known. The `confusion_matrix` function of the `sklearn.metrics` module helps
    to calculate this matrix for us, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We obtain the actual and predicted values as separate lists. Since we have two
    types of class labels, we obtain a two-by-two matrix. The `heatmap` module of
    the `seaborn` library helps us understand this matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Seaborn is a data visualization library based on Matplotlib. It provides a
    high-level interface for drawing attractive and informative statistical graphics,
    and is a popular tool for data scientists. If you do not have Seaborn installed,
    simply run the command: `pip install seaborn`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following Python codes to generate the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/e8dcd93d-44a8-4d51-aaaf-28b98491978a.png)'
  prefs: []
  type: TYPE_IMG
- en: Don't let the confusion matrix confuse you. Let's break down the numbers in
    a logical manner and see how easily a confusion matrix works. Starting from the
    left column, we have a total of 126 samples classified as False, of which the
    classifier predicted correctly 60 times, and these are known as **true negatives**
    (**TNs**). However, the classifier predicted it wrongly 66 times, and these are
    known as **false negatives** (**FNs**). In the right column, we have a total of
    125 samples belonging to the True class. The classifier predicted wrongly 55 times,
    and these are known as **false positives** (**FPs**). The classifier did predict
    correctly 70 times though, and these are known as **true positives** (**TPs**).
    These computed rates are used in other risk metrics, as we shall discover in the
    following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An accuracy score is the ratio of correct predictions to the total number of
    observations. By default, it is expressed as a fractional value between 0 and
    1\. When the accuracy score is 1.0, it means that the entire set of predicted
    labels in the sample matches with the true set of labels. The accuracy score can
    be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/3a76457f-d1b9-4343-963b-e13f2919c63a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *I(x)* is the indicator function that returns 1 for a correct prediction,
    and 0 otherwise. The `accuracy_score` function of the `sklearn.metrics` module
    calculates this score for us with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy score suggests that our model is correct 52% of the time. Accuracy
    scores are great at measuring symmetrical datasets where values of false positives
    and false negatives are almost the same. To evaluate the performance of our model fully,
    we need to look at other risk metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Precision score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A precision score is the ratio of correctly predicted positive observations
    to the total number of predicted positive observations, and can be written as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/c729d639-a8bb-4c08-a8c6-6877e0fc1b97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This gives a precision score between 0 and 1, with 1 as the best value indicating
    that the model classifies correctly all the time. The `precision_score` function
    of the `sklearn.metrics` module calculates this score for us with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The precision score suggests that our model is able to predict a classification
    correctly 52% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Recall score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The recall score is the ratio of correctly predicted positive observations
    to all the observations in the actual class, and can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/5a9ad0ce-d532-48ba-9666-4151d3f5e5cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This gives a recall score of between 0 and 1, with 1 as the best value. The
    `recall_score` function of the `sklearn.metrics` module calculates this score
    for us with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The recall score suggests that our logistic regression model correctly identifies
    positive samples 56% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: F1 score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The F1 score, or F-measure, is the weighted average of the precision score
    and the recall score, and can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/e1661497-9810-45da-b138-0665a041a5c4.png)'
  prefs: []
  type: TYPE_IMG
- en: This gives an F1 score between 0 and 1\. When either the precision score or
    the recall score is 0, the F1 score will be 0\. However, when both the precision
    score and recall score are positive, the F1 score gives equal weights to both
    measures. Maximizing the F1 score creates a balanced classification model with
    optimal balance of recall and precision.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `f1_score` function of the `sklearn.metrics` module calculates this score
    for us with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The F1 score of our logistic regression model is 0.536.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **support vector classifier** (**SVC**) is a concept of a **support vector
    machine** (**SVM**) that uses support vectors for classifying datasets.
  prefs: []
  type: TYPE_NORMAL
- en: More information on SVMs can be found at [http://www.statsoft.com/textbook/support-vector-machines](http://www.statsoft.com/textbook/support-vector-machines).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `SVC` class of the `sklean.svm` module implements the SVM classifier. Write
    a class named `SVCModel` and extend `LogisticRegressionModel` with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are overriding the `get_model()` method to return the `SVC` class
    of scikit-learn. A high-penalty `C` value of `1000` is specified. The `gamma`
    parameter is the kernel coefficient with a default value of `auto`. The `learn()`
    command is executed with our usual model parameters. With that, let''s run the
    risk metrics on this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We obtain better scores than from the logistic regression classifier model.
    By default, the `C` value of the linear SVM is 1.0, which would in practice give
    us generally comparable performance with the logistic regression model. There
    is absolutely no rule of thumb for choosing a `C` value, as it depends entirely on
    the training dataset. A nonlinear SVM kernel can be considered by supplying a `kernel` parameter
    to the `SVC()` model. More information on SVM kernels is available at [https://scikit-learn.org/stable/modules/svm.html#svm-kernels](https://scikit-learn.org/stable/modules/svm.html#svm-kernels).
  prefs: []
  type: TYPE_NORMAL
- en: Other types of classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides logistic regression and SVC, scikit-learn contains many other types
    of classifiers for machine learning. The following sections discuss some classifiers
    that we can also consider implementing in our classification-based model.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent** (**SGD**) is a form of **gradient descent**
    that works by using an iterative process to estimate the gradient towards minimizing
    an objective loss function, such as a linear support vector machine or logistic
    regression. The stochastic term comes about as samples are chosen at random. When
    lesser iterations are used, bigger steps are taken to reach the solution, and
    the model is said to have a **high learning rate**. Likewise, with more iterations,
    smaller steps are taken, resulting in a model with a **small learning rate**.
    SGD is a popular choice of machine learning algorithm among practitioners as it
    has been effectively used in large-scale text classification and natural language
    processing models.'
  prefs: []
  type: TYPE_NORMAL
- en: The `SGDClassifier` class of the `sklearn.linear_model` module implements the
    SGD classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Linear discriminant analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Linear discriminant analysis** (**LDA**) is a classic classifier that uses
    a linear decision surface, where the mean and variance for every class of the
    data is estimated. It assumes that the data is Gaussian, and that each attribute
    has the same variance, and values of each variable are around the mean. LDA computes
    *discriminant scores* by using Bayes'' theorem for each observation to determine
    to which class it belongs.'
  prefs: []
  type: TYPE_NORMAL
- en: The `LinearDiscriminantAnalysis` class of the `sklearn.discriminant_analysis`
    module implements the LDA classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Quadratic discriminant analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Quadratic discriminant analysis** (**QDA**) is very similar to LDA, but uses
    a quadratic decision boundary and each class uses its own estimate of variance.
    Running the risk metrics shows that the QDA model does not necessarily give better
    performance than the LDA model. The type of decision boundary has to be taken
    into consideration for the model required. QDA is better suited for large datasets,
    as it tends to have a lower bias and higher variance. On the other hand, LDA is
    suitable for smaller datasets that have a lower bias and a higher variance.'
  prefs: []
  type: TYPE_NORMAL
- en: The `QuadraticDiscriminantAnalysis` class of the `sklearn.discriminant_analysis`
    module implements the QDA model.
  prefs: []
  type: TYPE_NORMAL
- en: KNN classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **k-nearest neighbors** (**k-NN**) classifier is a simple algorithm that
    conducts a simple majority vote of the nearest neighbors of each point, and that
    point is assigned to a class that has the most representatives within the nearest
    neighbors of the point. While there is not a need to train a model for generalization,
    the predicting phase is slower and costlier in terms of time and memory.
  prefs: []
  type: TYPE_NORMAL
- en: The `KNeighborsClassifier` class of the `sklearn.neighbors` module implements
    the KNN classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion on the use of machine learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may have observed that predicted values from our models are far off from
    actual values. This chapter aims to demonstrate the best of the machine learning
    features that scikit-learn offers, which may possibly be used to predict time
    series data. No studies to date have shown that machine learning algorithms can
    predict prices even close to 100% of the time. A lot more effort goes into building
    and running machine learning systems effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have been introduced to machine learning in the context
    of finance. We discussed how AI and machine learning is transforming the financial
    sector. Machine learning can be supervised or unsupervised, and supervised algorithms
    can be regression-based and classification-based. The scikit-learn Python library
    provides various machine learning algorithms and risk metrics.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the use of regression-based machine learning models such as OLS
    regression, ridge regression, LASSO regression, and elastic net regularization
    in predicting continuous values such as security prices. An ensemble of decision
    trees was also discussed, such as the bagging regressor, gradient tree boosting,
    and random forests. To measure the performance of regression models, we visited
    the MSE, MAE, explained variance score, and R² score.
  prefs: []
  type: TYPE_NORMAL
- en: Classification-based machine learning classifies input values as classes or
    labels. Such classes may be bi-class or multi-class. We discussed the use of logistic
    regression, SVC, LDA and QDA, and k-NN classifiers for predicting price trends.
    To measure the performance of classification models, we visited the confusion
    matrix, accuracy score, precision and recall scores, as well as the F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the use of deep learning in finance.
  prefs: []
  type: TYPE_NORMAL
