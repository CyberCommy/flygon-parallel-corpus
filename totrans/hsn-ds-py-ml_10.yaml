- en: Testing and Experimental Design
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll see the concept of A/B testing. We'll go through the
    t-test, the t-statistic, and the p-value, all useful tools for determining whether
    a result is actually real or a result of random variation. We'll dive into some
    real examples and get our hands dirty with some Python code and compute the t-statistics
    and p-values.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Following that, we'll look into how long you should run an experiment for before
    reaching a conclusion. Finally, we'll discuss the potential issues that can harm
    the results of your experiment and may cause you to reach the wrong conclusion.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing concepts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T-test and p-value
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring t-statistics and p-values using Python
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining how long to run an experiment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A/B test gotchas
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A/B testing concepts
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you work as a data scientist at a web company, you'll probably be asked to
    spend some time analyzing the results of A/B tests. These are basically controlled
    experiments on a website to measure the impact of a given change. So, let's talk
    about what A/B tests are and how they work.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: A/B tests
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you're going to be a data scientist at a big tech web company, this is something
    you're going to definitely be involved in, because people need to run experiments
    to try different things on a website and measure the results of it, and that's
    actually not as straightforward as most people think it is.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: What is an A/B test? Well, it's a controlled experiment that you usually run
    on a website, it can be applied to other contexts as well, but usually we're talking
    about a website, and we're going to test the performance of some change to that
    website, versus the way it was before.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: You basically have a *control* set of people that see the old website, and a
    *test* group of people that see the change to the website, and the idea is to
    measure the difference in behavior between these two groups and use that data
    to actually decide whether this change was beneficial or not.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: For example, I own a business that has a website, we license software to people,
    and right now I have a nice, friendly, orange button that people click on when
    they want to buy a license as shown on the left in the following figure. But what
    would happen if I changed the color of that button to blue, as shown on the right?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ec84f37-58d0-43d6-9ffc-29e792d95830.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: So in this example, if I want to find out whether blue would be better. How
    do I know?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: I mean, intuitively, maybe that might capture people's attention more, or intuitively,
    maybe people are more used to seeing orange buy buttons and are more likely to
    click on that, I could spin that either way, right? So, my own internal biases
    or preconceptions don't really matter. What matters is how people react to this
    change on my actual website, and that's what an A/B test does.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing will split people up into people who see the orange button, and
    people who see the blue button, and I can then measure the behavior between these
    two groups and how they might differ, and make my decision on what color my buttons
    should be based on that data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'You can test all sorts of things with an A/B test. These include:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '**Design changes**: These can be changes in the color of a button, the placement
    of a button, or the layout of the page.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**UI flow**: So, maybe you''re actually changing the way that your purchase
    pipeline works and how people check out on your website, and you can actually
    measure the effect of that.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithmic changes**: Let''s consider the example of doing movie recommendations
    that we discussed in [Chapter 6](15344fc7-40a2-4d4f-ba14-7f45b6481ef8.xhtml),
    *Recommender Systems*. Maybe I want to test one algorithm versus another. Instead
    of relying on error metrics and my ability to do a train test, what I really care
    about is driving purchases or rentals or whatever it is on this website.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The A/B test can let me directly measure the impact of this algorithm on the
    end result that I actually care about, and not just my ability to predict movies
    that other people have already seen.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And anything else you can dream up too, really, any change that impacts how
    users interact with your site is worth testing. Maybe it's even, making the website
    faster, or it could be anything.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pricing changes**: This one gets a little bit controversial. You know, in
    theory, you can experiment with different price points using an A/B test and see
    if it actually increases volume to offset for the price difference or whatever,
    but use that one with caution.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If customers catch wind that other people are getting better prices than they
    are for no good reason, they're not going to be very happy with you. Keep in mind,
    doing pricing experiments can have a negative backlash and you don't want to be
    in that situation.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring conversion for A/B testing
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing you need to figure out when you're designing an experiment on
    a website is what are you trying to optimize for? What is it that you really want
    to drive with this change? And this isn't always a very obvious thing. Maybe it's
    the amount that people spend, the amount of revenue. Well, we talked about the
    problems with variance in using amount spent, but if you have enough data, you
    can still, reach convergence on that metric a lot of times.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: However, maybe that's not what you actually want to optimize for. Maybe you're
    actually selling some items at a loss intentionally just to capture market share.
    There's more complexity that goes into your pricing strategy than just top-line
    revenue.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Maybe what you really want to measure is profit, and that can be a very tricky
    thing to measure, because a lot of things cut into how much money a given product
    might make and those things might not always be obvious. And again, if you have
    loss leaders, this experiment will discount the effect that those are supposed
    to have. Maybe you just care about driving ad clicks on your website, or order
    quantities to reduce variance, maybe people are okay with that.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: The bottom line is that you have to talk to the business owners of the area
    that's being tested and figure out what it is they're trying to optimize for.
    What are they being measured on? What is their success measured on? What are their
    key performance indicators or whatever the NBAs want to call it? And make sure
    that we're measuring the thing that it matters to them.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'You can measure more than one thing at once too, you don''t have to pick one,
    you can actually report on the effect of many different things:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Revenue
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profit
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clicks
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ad views
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If these things are all moving in the right direction together, that's a very
    strong sign that this change had a positive impact in more ways than one. So,
    why limit yourself to one metric? Just make sure you know which one matters the
    most in what's going to be your criteria for success of this experiment ahead
    of time.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: How to attribute conversions
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another thing to watch out for is attributing conversions to a change downstream.
    If the action you're trying to drive doesn't happen immediately upon the user
    experiencing the thing that you're testing, things get a little bit dodgy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Let's say I change the color of a button on page A, the user then goes to page
    B and does something else, and ultimately buys something from page C.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Well, who gets credit for that purchase? Is it page A, or page B, or something
    in-between? Do I discount the credit for that conversion depending on how many
    clicks that person took to get to the conversion action? Do I just discard any
    conversion action that doesn't happen immediately after seeing that change? These
    are complicated things and it's very easy to produce misleading results by fudging
    how you account for these different distances between the conversion and the change
    that you're measuring.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Variance is your enemy
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another thing that you need to really internalize is that variance is your enemy
    when you're running an A/B test.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: A very common mistake people make who don't know what they're doing with data
    science is that they will put up a test on a web page, blue button versus orange
    button, whatever it is, run it for a week, and take the mean amount spent from
    each of those groups. They then say "oh look! The people with the blue button
    on average spent a dollar more than the people with the orange button; blue is
    awesome, I love blue, I'm going to put blue all over the website now!"
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: But, in fact, all they might have been seeing was just a random variation in
    purchases. They didn't have a big enough sample because people don't tend to purchase
    a lot. You get a lot of views but you probably don't have a lot of purchases on
    your website in comparison, and it's probably a lot of variance in those purchase
    amounts because different products cost different amounts.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: So, you could very easily end up making the wrong decision that ends up costing
    your company money in the long run, instead of earning your company money if you
    don't understand the effect of variance on these results. We'll talk about some
    principal ways of measuring and accounting for that later in the chapter.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: You need to make sure that your business owners understand that this is an important
    effect that you need to quantify and understand before making business decisions
    following an A/B test or any experiment that you run on the web.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Now, sometimes you need to choose a conversion metric that has less variance.
    It could be that the numbers on your website just mean that you would have to
    run an experiment for years in order to get a significant result based on something
    like revenue or amount spent.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes if you're looking at more than one metric, such as order amount or
    order quantity, that has less variance associated with it, you might see a signal
    on order quantity before you see a signal on revenue, for example. At the end
    of the day, it ends up being a judgment call. If you see a significant lift in
    order quantities and maybe a not-so-significant lift in revenue, then you have
    to say "well, I think there might be something real and beneficial going on here."
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: However, the only thing that statistics and data size can tell you, are probabilities
    that an effect is real. It's up to you to decide whether or not it's real at the
    end of the day. So, let's talk about how to do this in more detail.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaway here is, just looking at the differences in means isn't enough.
    When you're trying to evaluate the results of an experiment, you need to take
    the variance into account as well.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: T-test and p-value
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How do you know if a change resulting from an A/B test is actually a real result
    of what you changed, or if it's just random variation? Well, there are a couple
    of statistical tools at our disposal called the t-test or t-statistic, and the
    p-value. Let's learn more about what those are and how they can help you determine
    whether an experiment is good or not.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The aim is to figure out if a result is real or not. Was this just a result
    of random variance that's inherent in the data itself, or are we seeing an actual,
    statistically significant change in behavior between our control group and our
    test group? T-tests and p-values are a way to compute that.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Remember, *statistically significant* doesn't really have a specific meaning.
    At the end of the day it has to be a judgment call. You have to pick a probability
    value that you're going to accept of a result being real or not. But there's always
    going to be a chance that it's still a result of random variation, and you have
    to make sure your stakeholders understand that.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: The t-statistic or t-test
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with the **t-statistic**, also known as a t-test. It is basically
    a measure of the difference in behavior between these two sets, between your control
    and treatment group, expressed in units of standard error. It is based on standard
    error, which accounts for the variance inherent in the data itself, so by normalizing
    everything by that standard error, we get some measure of the change in behavior
    between these two groups that takes that variance into account.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从**t-统计**开始，也被称为t-检验。它基本上是衡量这两组行为之间的差异的一种方式，即你的控制组和处理组之间的差异，以标准误差的单位表示。它基于标准误差，考虑了数据本身固有的方差，因此通过将一切都标准化为标准误差，我们得到了一些考虑到方差的这两组行为变化的度量。
- en: The way to interpret a t-statistic is that a high t-value means there's probably
    a real difference between these two sets, whereas a low t-value means not so much
    difference. You have to decide what's a threshold that you're willing to accept?
    The sign of the t-statistic will tell you if it's a positive or negative change.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 解释t-统计的方法是，高t值意味着这两组之间可能存在真正的差异，而低t值意味着差异不大。你必须决定你愿意接受的门槛是多少？t-统计的符号将告诉你这是一个正向还是负向的变化。
- en: If you're comparing your control to your treatment group and you end up with
    a negative t-statistic, that implies that this is a bad change. You ultimate want
    the absolute value of that t-statistic to be large. How large a value of t-statistic
    is considered large? Well, that's debatable. We'll look at some examples shortly.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将你的控制组与处理组进行比较，最终得到一个负的t-统计，这意味着这是一个不好的改变。你最终希望t-统计的绝对值很大。什么样的t-统计值被认为是大的？这是有争议的。我们很快会看一些例子。
- en: Now, this does assume that you have a normal distribution of behavior, and when
    we're talking about things like the amount people spend on a website, that's usually
    a decent assumption. There does tend to be a normal distribution of how much people
    spend.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这假设了你有一个正态分布的行为，当我们谈论人们在网站上的花费时，这通常是一个合理的假设。人们的花费往往有一个正态分布。
- en: However, there are more refined versions of t-statistics that you might want
    to look at for other specific situations. For example, there's something called
    **Fisher's exact test** for when you're talking about click through rates, the
    **E-test** when you're talking about transactions per user, like how many web
    pages do they see, and the **chi-squared** test, which is often relevant for when
    you're looking at order quantities. Sometimes you'll want to look at all of these
    statistics for a given experiment, and choose the one that actually fits what
    you're trying to do the best.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有更精细的 t-统计的版本，你可能想要针对其他特定情况进行研究。例如，当你谈论点击率时，有一种叫做**费舍尔精确检验**的东西，当你谈论每个用户的交易时，比如他们看了多少网页，有**E-检验**，还有**卡方检验**，通常与订单数量有关。有时你会想要查看给定实验的所有这些统计数据，并选择最适合你所尝试做的事情的那个。
- en: The p-value
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: p值
- en: Now, it's a lot easier to talk about p-values than t-statistics because you
    don't have to think about, how many standard deviations are we talking about?
    What does the actual value mean? The p-value is a little bit easier for people
    to understand, which makes it a better tool for you to communicate the results
    of an experiment to the stakeholders in your business.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，谈论p值比t-统计要容易得多，因为你不必考虑，我们谈论多少个标准偏差？实际值是什么意思？p值对人们来说更容易理解，这使得它成为一个更好的工具，用来向你业务中的利益相关者传达实验结果。
- en: The p-value is basically the probability that this experiment satisfies the
    null hypothesis, that is, the probability that there is no real difference between
    the control and the treatment's behavior. A low p-value means there's a low probability
    of it having no effect, kind of a double negative going on there, so it's a little
    bit counter intuitive, but at the end of the day you just have to understand that
    a low p-value means that there's a high probability that your change had a real
    effect.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: p值基本上是这个实验满足零假设的概率，也就是说，控制组和处理组的行为之间没有真正的差异的概率。低p值意味着它没有影响的概率很低，有点双重否定的意思，所以这有点反直觉，但最终你只需要明白，低p值意味着你的改变有真正的影响的概率很高。
- en: What you want to see are a high t-statistic and a low p-value, and that will
    imply a significant result. Now, before you start your experiment, you need to
    decide what your threshold for success is going to be, and that means deciding
    the threshold with the people in charge of the business.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要看到的是高 t-统计和低 p-值，这将意味着显著的结果。现在，在你开始实验之前，你需要决定你的成功门槛是多少，并且这意味着与业务负责人一起决定门槛。
- en: So, what p-value are you willing to accept as a measure of success? Is it 1
    percent? Is it 5 percent? And again, this is basically the likelihood that there
    is no real effect, that it's just a result of random variance. It is just a judgment
    call at the end of the day. A lot of times people use 1 percent, sometimes they
    use 5 percent if they're feeling a little bit riskier, but there's always going
    to be that chance that your result was just spurious, random data that came in.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你愿意接受什么样的p值作为成功的衡量标准？是1%？是5%？再次强调，这基本上是没有真正效应的可能性，只是随机方差的结果。这最终是一个判断。很多时候人们使用1%，有时如果他们感觉有点冒险，他们会使用5%，但总会有那种可能性，你的结果只是偶然的，是随机数据。
- en: However, you can choose the probability that you're willing to accept as being
    likely enough that this is a real effect, that's worth rolling out into production.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可以选择愿意接受的概率，认为这是一个真正的效应，值得投入生产。
- en: When your experiment is over, and we'll talk about when you declare an experiment
    to be over later, you want to measure your p-value. If it's less than the threshold
    you decided upon, then you can reject the null hypothesis and you can say "well,
    there's a high likelihood that this change produced a real positive or negative
    result."
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: If it is a positive result then you can roll that change out to the entire site
    and it is no longer an experiment, it is part of your website that will hopefully
    make you more and more money as time goes on, and if it's a negative result, you
    want to get rid of it before it costs you any more money.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Remember, there is a real cost to running an A/B test when your experiment has
    a negative result. So, you don't want to run it for too long because there's a
    chance you could be losing money.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: This is why you want to monitor the results of an experiment on a daily basis,
    so if there are early indications that the change is making a horrible impact
    to the website, maybe there's a bug in it or something that's horrible, you can
    pull the plug on it prematurely if necessary, and limit the damage.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Let's go to an actual example and see how you might measure t-statistics and
    p-values using Python.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Measuring t-statistics and p-values using Python
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's fabricate some experimental data and use the t-statistic and p-value to
    determine whether a given experimental result is a real effect or not. We're going
    to actually fabricate some fake experimental data and run t-statistics and p-values
    on them, and see how it works and how to compute it in Python.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Running A/B test on some experimental data
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s imagine that we''re running an A/B test on a website and we have randomly
    assigned our users into two groups, group `A` and group `B`. The `A` group is
    going to be our test subjects, our treatment group, and group `B` will be our
    control, basically the way the website used to be. We''ll set this up with the
    following code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this code example, our treatment group (`A`) is going to have a randomly
    distributed purchase behavior where they spend, on average, $25 per transaction,
    with a standard deviation of five and ten thousand samples, whereas the old website
    used to have a mean of $26 per transaction with the same standard deviation and
    sample size. We''re basically looking at an experiment that had a negative result.
    All you have to do to figure out the t-statistic and the p-value is use this handy
    `stats.ttest_ind` method from `scipy`. What you do is, you pass it in your treatment
    group and your control group, and out comes your t-statistic as shown in the output
    here:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe9d3310-3841-482e-b97a-098ff9084522.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: In this case, we have a t-statistic of -14\. The negative indicates that it
    is a negative change, this was a bad thing. And the p-value is very, very small.
    So, that implies that there is an extremely low probability that this change is
    just a result of random chance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Remember that in order to declare significance, we need to see a high t-value
    t-statistic, and a low p-value.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: That's exactly what we're seeing here, we're seeing -14, which is a very high
    absolute value of the t-statistic, negative indicating that it's a bad thing,
    and an extremely low P-value, telling us that there's virtually no chance that
    this is just a result of random variation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: If you saw these results in the real world, you would pull the plug on this
    experiment as soon as you could.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: When there's no real difference between the two groups
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just as a sanity check, let''s go ahead and change things so that there''s
    no real difference between these two groups. So, I''m going to change group `B`,
    the control group in this case, to be the same as the treatment, where the mean
    is 25, the standard deviation is unchanged, and the sample size is unchanged as
    shown here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If we go ahead and run this, you can see our t-test ends up being below one
    now:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81066228-b08e-4aa3-85af-015efe4ac2b3.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: Remember this is in terms of standard deviation. So this implies that there's
    probably not a real change there unless we have a much higher p-value as well,
    over 30 percent.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这是标准差的问题。因此，这意味着除非我们的p值更高，超过30%，否则那里可能没有真正的变化。
- en: Now, these are still relatively high numbers. You can see that random variation
    can be kind of an insidious thing. This is why you need to decide ahead of time
    what would be an acceptable limit for p-value.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些仍然是相对较高的数字。您可以看到随机变化可能是一种隐匿的东西。这就是为什么您需要提前决定p值的可接受限制。
- en: You know, you could look at this after the fact and say, "30 percent odds, you
    know, that's not so bad, we can live with that," but, no. I mean, in reality and
    practice you want to see p-values that are below 5 percent, ideally below 1 percent,
    and a value of 30 percent means it's actually not that strong of a result. So,
    don't justify it after the fact, go into your experiment in knowing what your
    threshold is.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您知道，您事后可能会看到这一点并说，“30%的几率，你知道，那还不错，我们可以接受”，但是，实际上，您希望看到的是低于5%的p值，理想情况下是低于1%，而30%的值意味着实际上并不是一个强有力的结果。因此，不要在事后为其辩护，进入实验时要知道您的阈值是多少。
- en: Does the sample size make a difference?
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样本量是否有影响？
- en: Let's do some changes in the sample size. We're creating these sets under the
    same conditions. Let's see if we actually get a difference in behavior by increasing
    the sample size.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对样本量进行一些更改。我们在相同条件下创建这些集合。让我们看看通过增加样本量是否实际上会在行为上产生差异。
- en: Sample size increased to six-digits
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样本量增加到六位数
- en: 'So, we''re going to go from `10000` to `100000` samples as shown here:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们将从`10000`增加到`100000`个样本，如下所示：
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can see in the following output that actually the p-value got a little bit
    lower and the t-test a little bit larger, but it's still not enough to declare
    a real difference. It's actually going in the direction you wouldn't expect it
    to go? Kind of interesting!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下输出中，您可以看到实际上p值略低，t检验略高，但仍不足以宣布真正的差异。它实际上是朝着你不希望的方向发展的？挺有趣的！
- en: '![](img/f580eba5-7fdb-41e9-a9fd-303797a27454.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f580eba5-7fdb-41e9-a9fd-303797a27454.jpg)'
- en: But these are still high values. Again, it's just the effect of random variance,
    and it can have more of an effect than you realize. Especially on a website when
    you're talking about order amounts.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些仍然是高值。再次强调，这只是随机变异的影响，它可能比您意识到的要大。特别是在网站上，当您谈论订单金额时。
- en: Sample size increased seven-digits
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样本量增加到七位数
- en: 'Let''s actually increase the sample size to `1000000,` as shown here:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将样本量实际增加到`1000000`，如下所示：
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here is the result:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '![](img/7b3983dd-a665-431d-adee-6de6bb8b15ee.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b3983dd-a665-431d-adee-6de6bb8b15ee.jpg)'
- en: What does that do? Well, now, we're back under 1 for the t-statistic, and our
    value's around 35 percent.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 那会有什么影响呢？现在，我们的t统计量又低于1，而我们的值约为35%。
- en: We're seeing these kind of fluctuations a little bit in either direction as
    we increase the sample size. This means that going from 10,000 samples to 100,000
    to 1,000,000 isn't going to change your result at the end of the day. And running
    experiments like this is a good way to get a good gut feel as to how long you
    might need to run an experiment for. How many samples does it actually take to
    get a significant result? And if you know something about the distribution of
    your data ahead of time, you can actually run these sorts of models.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 随着样本量的增加，我们会看到这种波动在某种程度上有所变化。这意味着从10000个样本增加到100000个样本再到1000000个样本，最终结果不会改变。进行这种实验是了解您可能需要运行实验的时间的一种好方法。需要多少样本才能得到显著结果？如果您事先了解数据的分布情况，您实际上可以运行这些模型。
- en: A/A testing
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: A/A测试
- en: 'If we were to compare the set to itself, this is called an A/A test as shown
    in the following code example:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将集合与自身进行比较，这被称为A/A测试，如下面的代码示例所示：
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can see in the following output, a t-statistic of `0` and a p-value of `1.0`
    because there is in fact no difference whatsoever between these sets.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下输出中看到，t统计量为`0`，p值为`1.0`，因为实际上这些集合之间根本没有任何差异。
- en: '![](img/8cd38bc5-b4f5-49db-8149-bad83224a650.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8cd38bc5-b4f5-49db-8149-bad83224a650.jpg)'
- en: Now, if you were to run that using real website data where you were looking
    at the same exact people and you saw a different value, that indicates there's
    a problem in the system itself that runs your testing. At the end of the day,
    like I said, it's all a judgment call.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您使用真实的网站数据进行运行，您观察到相同的人群并且看到不同的值，这表明您运行测试的系统本身存在问题。归根结底，就像我说的，这都是一种判断。
- en: Go ahead and play with this, see what the effect of different standard deviations
    has on the initial datasets, or differences in means, and different sample sizes.
    I just want you to dive in, play around with these different datasets and actually
    run them, and see what the effect is on the t-statistic and the p-value. And hopefully
    that will give you a more gut feel of how to interpret these results.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 继续尝试，看看不同标准差对初始数据集或均值差异以及不同样本量的影响。我只是希望您深入研究，尝试运行这些不同的数据集，看看它们对t统计量和p值的影响。希望这能让您更直观地理解如何解释这些结果。
- en: Again, the important thing to understand is that you're looking for a large
    t-statistic and a small p-value. P-value is probably going to be what you want
    to communicate to the business. And remember, lower is better for p-value, you
    want to see that in the single digits, ideally below 1 percent before you declare
    victory.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调的重要一点是，您要寻找一个较大的t统计量和一个较小的p值。P值可能是您想要向业务传达的内容。记住，p值越低越好，最好在单个数字以下，理想情况下在1%以下，然后再宣布胜利。
- en: We'll talk about A/B tests some more in the remainder of the chapter. SciPy
    makes it really easy to compute t-statistics and p-values for a given set of data,
    so you can very easily compare the behavior between your control and treatment
    groups, and measure what the probability is of that effect being real or just
    a result of random variation. Make sure you are focusing on those metrics and
    you are measuring the conversion metric that you care about when you're doing
    those comparisons.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的其余部分更多地讨论A/B测试。SciPy使得计算给定数据集的t统计量和p值变得非常容易，因此你可以非常容易地比较控制组和处理组之间的行为，并测量这种效果是真实的概率还是仅仅是随机变化的结果。确保你关注这些指标，并且在进行比较时测量你关心的转化指标。
- en: Determining how long to run an experiment for
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定运行实验的时间长短
- en: How long do you run an experiment for? How long does it take to actually get
    a result? At what point do you give up? Let's talk about that in more detail.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你运行实验多长时间？实际上要得到结果需要多长时间？在什么时候放弃？让我们更详细地讨论一下。
- en: If someone in your company has developed a new experiment, a new change that
    they want to test, then they have a vested interest in seeing that succeed. They
    put a lot of work and time into it, and they want it to be successful. Maybe you've
    gone weeks with the testing and you still haven't reached a significant outcome
    on this experiment, positive or negative. You know that they're going to want
    to keep running it pretty much indefinitely in the hope that it will eventually
    show a positive result. It's up to you to draw the line on how long you're willing
    to run this experiment for.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你公司的某人开发了一个新的实验，一个他们想要测试的新变化，那么他们对于看到它成功有着切身利益。他们投入了大量的工作和时间，他们希望它能够成功。也许你已经进行了几周的测试，但仍然没有在这个实验上取得显著的结果，无论是积极的还是消极的。你知道他们会希望继续无限期地运行它，希望最终能够显示出积极的结果。你需要决定你愿意运行这个实验多长时间。
- en: How do I know when I'm done running an A/B test? I mean, it's not always straightforward
    to predict how long it will take before you can achieve a significant result,
    but obviously if you have achieved a significant result, if your p-value has gone
    below 1 percent or 5 percent or whatever threshold you've chosen, and you're done.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我怎么知道何时结束A/B测试？我的意思是，预测在你能够取得显著结果之前需要多长时间并不总是直截了当的，但显然，如果你取得了显著结果，如果你的p值已经低于1%或5%或你选择的任何阈值，那么你就结束了。
- en: At that point you can pull the plug on the experiment and either roll out the
    change more widely or remove it because it was actually having a negative effect.
    You can always tell people to go back and try again, use what they learned from
    the experiment to maybe try it again with some changes and soften the blow a little
    bit.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在那一点上，你可以中止实验，要么更广泛地推出变化，要么移除它，因为它实际上产生了负面影响。你总是可以告诉人们重新尝试，利用他们从实验中学到的东西，也许做一些改变再试一次，减轻一点打击。
- en: The other thing that might happen is it's just not converging at all. If you're
    not seeing any trends over time in the p-value, it's probably a good sign that
    you're not going to see this converge anytime soon. It's just not going to have
    enough of an impact on behavior to even be measurable, no matter how long you
    run it.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可能发生的情况是根本没有收敛。如果你在p值上没有看到任何趋势，那可能是一个好迹象，表明你不会很快看到这种收敛。无论你运行多长时间，它都不会对行为产生足够的影响，甚至无法测量。
- en: In those situations, what you want to do every day is plot on a graph for a
    given experiment the p-value, the t-statistic, whatever you're using to measure
    the success of this experiment, and if you're seeing something that looks promising,
    you will see that p-value start to come down over time. So, the more data it gets,
    the more significant your results should be getting.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，你每天想做的是为给定实验绘制一个图表，显示p值、t统计量，或者你用来衡量这个实验成功的任何东西，如果你看到一些有希望的东西，你会发现p值随着时间的推移而下降。因此，它获得的数据越多，你的结果就应该变得更加显著。
- en: Now, if you instead see a flat line or a line that's all over the place, that
    kind of tells you that that p-value's not going anywhere, and it doesn't matter
    how long you run this experiment, it's just not going to happen. You need to agree
    up front that in the case where you're not seeing any trends in p-values, what's
    the longest you're willing to run this experiment for? Is it two weeks? Is it
    a month?
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你看到的是一条平直的线，或者一条到处都是的线，那就告诉你p值不会有任何变化，无论你运行这个实验多长时间，它都不会发生。你需要事先达成一致，即在你没有看到p值的任何趋势的情况下，你愿意运行这个实验多长时间？是两周？还是一个月？
- en: Another thing to keep in mind is that having more than one experiment running
    on the site at once can conflate your results.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件需要记住的事情是，同时在网站上运行多个实验可能会混淆你的结果。
- en: Time spent on experiments is a valuable commodity, you can't make more time
    in the world. You can only really run as many experiments as you have time to
    run them in a given year. So, if you spend too much time running one experiment
    that really has no chance of converging on a result, that's an opportunity you've
    missed to run another potentially more valuable experiment during that time that
    you are wasting on this other one.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 实验所花费的时间是一种宝贵的资源，你无法在世界上创造更多的时间。在一年内，你只能运行尽可能多的实验。因此，如果你花费太多时间运行一个几乎没有机会收敛到结果的实验，那么你就错过了在这段时间内运行另一个潜在更有价值的实验的机会。
- en: It's important to draw the line on experiment links, because time is a very
    precious commodity when you're running A/B tests on a website, at least as long
    as you have more ideas than you have time, which hopefully is the case. Make sure
    you go in with agreed upper bounds on how long you're going to spend testing a
    given experiment, and if you're not seeing trends in the p-value that look encouraging,
    it's time to pull the plug at that point.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验链接上划清界限是很重要的，因为当你在网站上进行A/B测试时，时间是非常宝贵的，至少在你有更多的想法而没有时间的情况下，这种情况希望是存在的。确保你在进行给定实验测试的时间上设定了上限，如果你没有看到p值中令人鼓舞的趋势，那么就是时候停止实验了。
- en: A/B test gotchas
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: A/B测试的陷阱
- en: An important point I want to make is that the results of an A/B test, even when
    you measure them in a principled manner using p-values, is not gospel. There are
    many effects that can actually skew the results of your experiment and cause you
    to make the wrong decision. Let's go through a few of these and let you know how
    to watch out for them. Let's talk about some gotchas with A/B tests.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我想要强调的一个重要观点是，即使你使用p值以一种合理的方式来衡量A/B测试的结果，这也不是绝对的。有很多因素实际上可能会扭曲你实验的结果，并导致你做出错误的决定。让我们来看看A/B测试中的一些陷阱，以及如何注意避免它们。让我们谈谈A/B测试的一些陷阱。
- en: It sounds really official to say there's a p-value of 1 percent, meaning there's
    only a 1 percent chance that a given experiment was due to spurious results or
    random variation, but it's still not the be-all and end-all of measuring success
    for an experiment. There are many things that can skew or conflate your results
    that you need to be aware of. So, even if you see a p-value that looks very encouraging,
    your experiment could still be lying to you, and you need to understand the things
    that can make that happen so you don't make the wrong decisions.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 说一个实验的p值为1%，听起来很正式，意味着某个实验结果是由偶然结果或随机变化引起的可能性只有1%，但这仍然不是衡量实验成功的全部和终极标准。有很多因素可能会扭曲或混淆你的结果，你需要意识到这一点。所以，即使你看到一个非常令人鼓舞的p值，你的实验仍然可能在欺骗你，你需要了解可能导致这种情况发生的因素，以免做出错误的决定。
- en: Remember, correlation does not imply causation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，相关性不意味着因果关系。
- en: Even with a well-designed experiment, all you can say is there is some probability
    that this effect was caused by this change you made.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 即使进行了精心设计的实验，你只能说这种效果有一定的概率是由你所做的改变引起的。
- en: At the end of the day, there's always going to be a chance that there was no
    real effect, or you might even be measuring the wrong effect. It could still be
    random chance, there could be something else going on, it's your duty to make
    sure the business owners understand that these experimental results need to be
    interpreted, they need to be one piece of their decision.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，总会有可能没有真正的效果，或者你甚至可能在测量错误的效果。这可能仍然是随机事件，可能还有其他事情发生，你有责任确保业主明白这些实验结果需要被解释，它们只是决策的一部分。
- en: They can't be the be-all and end-all that they base their decision on because
    there is room for error in the results and there are things that can skew those
    results. And if there's some larger business objective to this change, beyond
    just driving short-term revenue, that needs to be taken into account as well.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 它们不能成为他们决策的全部和终极标准，因为结果中存在误差，并且有可能扭曲这些结果。如果这种改变还有一些更大的商业目标，而不仅仅是驱动短期收入，那么这也需要考虑在内。
- en: Novelty effects
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新奇效应
- en: One problem is novelty effects. One major Achilles heel of an A/B test is the
    short time frame over which they tend to be run, and this causes a couple of problems.
    First of all, there might be longer-term effects to the change, and you're not
    going to measure those, but also, there is a certain effect to just something
    being different on the website.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一个问题是新奇效应。A/B测试的一个主要弱点是它们倾向于运行的短时间范围，这会导致一些问题。首先，改变可能会产生长期效果，而你无法测量到这些效果，但也有一定效果，因为网站上的某些东西变得与众不同。
- en: For instance, maybe your customers are used to seeing the orange buttons on
    the website all the time, and if a blue button comes up and it catches their attention
    just because it's different. However, as new customers come in who have never
    seen your website before, they don't notice that as being different, and over
    time even your old customers get used to the new blue button. It could very well
    be that if you were to make this same test a year later, there would be no difference.
    Or maybe they'd be the other way around.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，也许你的客户习惯于在网站上一直看到橙色按钮，如果出现蓝色按钮，它会因为与众不同而吸引他们的注意。然而，随着新客户的到来，他们从未见过你的网站，他们不会注意到这种不同，随着时间的推移，即使是你的老客户也会习惯新的蓝色按钮。很可能，如果你在一年后进行同样的测试，结果可能没有任何差异，或者可能会相反。
- en: I could very easily see a situation where you test orange button versus blue
    button, and in the first two weeks the blue button wins. People buy more because
    they are more attracted to it, because it's different. But a year goes by, I could
    probably run another web lab that puts that blue button against an orange button
    and the orange button would win, again, simply because the orange button is different,
    and it's new and catches people's attention just for that reason alone.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我很容易能想象到这样一种情况：你测试橙色按钮和蓝色按钮，前两周蓝色按钮获胜。人们购买更多，因为他们更喜欢它，因为它与众不同。但一年过去了，我可能可以再次进行实验，将蓝色按钮与橙色按钮对比，橙色按钮会再次获胜，仅仅因为橙色按钮与众不同，新颖，吸引人们的注意力。
- en: For that reason, if you do have a change that is somewhat controversial, it's
    a good idea to rerun that experiment later on and see if you can actually replicate
    its results. That's really the only way I know of to account for novelty effects;
    actually measure it again when it's no longer novel, when it's no longer just
    a change that might capture people's attention simply because it's different.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: And this, I really can't understate the importance of understanding this. This
    can really skew a lot of results, it biases you to attributing positive changes
    to things that don't really deserve it. Being different in and of itself is not
    a virtue; at least not in this context.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Seasonal effects
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you're running an experiment over Christmas, people don't tend to behave
    the same during Christmas as they do the rest of the year. They definitely spend
    their money differently during that season, they're spending more time with their
    families at home, and they might be a little bit, kind of checked out of work,
    so people have a different frame of mind.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: It might even be involved with the weather, during the summer people behave
    differently because it's hot out they're feeling kind of lazy, they're on vacation
    more often. Maybe if you happen to do your experiment during the time of a terrible
    storm in a highly populated area that could skew your results as well.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Again, just be cognizant of potential seasonal effects, holidays are a big one
    to be aware of, and always take your experience with a grain of salt if they're
    run during a period of time that's known to have seasonality.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: You can determine this quantitatively by actually looking at the metric you're
    trying to measure as a success metric, be it, whatever you're calling your conversion
    metric, and look at its behavior over the same time period last year. Are there
    seasonal fluctuations that you see every year? And if so, you want to try to avoid
    running your experiment during one of those peaks or valleys.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Selection bias
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another potential issue that can skew your results is selection bias. It's very
    important that customers are randomly assigned to either your control or your
    treatment groups, your A or B group.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: However, there are subtle ways in which that random assignment might not be
    random after all. For example, let's say that you're hashing your customer IDs
    to place them into one bucket or the other. Maybe there's some subtle bias between
    how that hash function affects people with lower customer IDs versus higher customer
    IDs. This might have the effect of putting all of your longtime, more loyal customers
    into the control group, and your newer customers who don't know you that well
    into your treatment group.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: What you end up measuring then is just a difference in behavior between old
    customers and new customers as a result. It's very important to audit your systems
    to make sure there is no selection bias in the actual assignment of people to
    the control or treatment group.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: You also need to make sure that assignment is sticky. If you're measuring the
    effect of a change over an entire session, you want to measure if they saw a change
    on page A but, over on page C they actually did a conversion, you have to make
    sure they're not switching groups in between those clicks. So, you need to make
    sure that within a given session, people remain in the same group, and how to
    define a session can become kind of nebulous as well.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Now, these are all issues that using an established off-the-shelf framework
    like Google Experiments or Optimizely or one of those guys can help with so that
    you're not reinventing the wheel on all these problems. If your company does have
    a homegrown, in-house solution because they're not comfortable with sharing that
    data with outside companies, then it's worth auditing whether there is selection
    bias or not.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Auditing selection bias issues
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way for auditing selection bias issues is running what's called an A/A test,
    like we saw earlier. So, if you actually run an experiment where there is no difference
    between the treatment and control, you shouldn't see a difference in the end result.
    There should not be any sort of change in behavior when you're comparing those
    two things.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: An A/A test can be a good way of testing your A/B framework itself and making
    sure there's no inherent bias or other problems, for example, session leakage
    and whatnot, that you need to address.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Data pollution
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another big problem is data pollution. We talked at length about the importance
    of cleaning your input data, and it's especially important in the context of an
    A/B test. What would happen if you have a robot, a malicious crawler that's crawling
    through your website all the time, doing an unnatural amount of transactions?
    What if that robot ends up getting either assigned to the treatment or the control?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: That one robot could skew the results of your experiment. It's very important
    to study the input going into your experiment and look for outliers, then analyze
    what those outliers are, and whether they should they be excluded. Are you actually
    letting some robots leak into your measurements and are they skewing the results
    of your experiment? This is a very, very common problem, and something you need
    to be cognizant of.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: There are malicious robots out there, there are people trying to hack into your
    website, there are benign scrapers just trying to crawl your website for search
    engines or whatnot. There are all sorts of weird behaviors going on with a website,
    and you need to filter out those and get at the people who are really your customers
    and not these automated scripts. That can actually be a very challenging problem.
    Yet another reason to use off-the-shelf frameworks like Google Analytics, if you
    can.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Attribution errors
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We talked briefly about attribution errors earlier. This is if you are actually
    using downstream behavior from a change, and that gets into a gray area.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: You need to understand how you're actually counting those conversions as a function
    of distance from the thing that you changed and agree with your business stakeholders
    upfront as to how you're going to measure those effects. You also need to be aware
    of if you're running multiple experiments at once; will they conflict with one
    another? Is there a page flow where someone might actually encounter two different
    experiments within the same session?
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: If so, that's going to be a problem and you have to apply your judgment as to
    whether these changes actually could interfere with each other in some meaningful
    way and affect the customers' behavior in some meaningful way. Again, you need
    to take these results with a grain of salt. There are a lot of things that can
    skew results and you need to be aware of them. Just be aware of them and make
    sure your business owners are also aware of the limitations of A/B tests and all
    will be okay.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Also, if you're not in a position where you can actually devote a very long
    amount of time to an experiment, you need to take those results with a grain of
    salt and ideally retest them later on during a different time period.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about what A/B tests are and what are the challenges
    surrounding them. We went into some examples of how you actually measure the effects
    of variance using the t-statistic and p-value metrics, and we got into coding
    and measuring t-tests using Python. We then went on to discuss the short-term
    nature of an A/B test and its limitations, such as novelty effects or seasonal
    effects.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: That also wraps up our time in this book. Congratulations for making it this
    far, that's a serious achievement and you should be proud of yourself. We've covered
    a lot of material here and I hope that you at least understand the concepts and
    have a little bit of hands-on experience with most of the techniques that are
    used in data science today. It's a very broad field, so we've touched on a little
    bit of everything there. So, you know, congratulations again.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: If you want to further your career in this field, what I'd really encourage
    you to do is talk to your boss. If you work at a company that has access to some
    interesting datasets of its own, see if you can play around with them. Obviously,
    you want to talk to your boss first before you use any data owned by your company,
    because there's probably going to be some privacy restrictions surrounding it.
    You want to make sure that you're not violating the privacy of your company's
    customers, and that might mean that you might only be able to use that data or
    look at it within a controlled environment at your workplace. So, be careful when
    you're doing that.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: If you can get permission to actually stay late at work a few days a week and,
    you know, mess around with some of these datasets and see what you can do with
    it, not only does show that you have the initiative to make yourself a better
    employee, you might actually discover something that might be valuable to your
    company, and that could just make you look even better, and actually lead to an
    internal transfer perhaps, into a field more directly related to where you want
    to take your career.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: So, if you want some career advice from me, a common question I get is, "hey,
    I'm an engineer, I want to get more into data science, how do I do that?" The
    best way to do it is just do it, you know, actually do some side projects and
    show that you can do it and demonstrate some meaningful results from it. Show
    that to your boss and see where it leads you. Good luck.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
