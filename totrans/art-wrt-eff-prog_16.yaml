- en: '*Assessments*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Chapter 1*](B16229_01_Epub_AM.xhtml#_idTextAnchor014):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many domains, the size of the problems grows as fast as or even faster than
    the available computational resources. As computing becomes more ubiquitous, heavy
    workloads may have to be executed on processors of limited power.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Single-core processing power largely stopped increasing about 15 years ago,
    and the advances in processor design and manufacturing largely translate into
    more processing cores and a large number of specialized computing units. Making
    the best use of these resources does not happen automatically and requires an
    understanding of how they work.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Efficiency refers to using more of the available computational resources more
    of the time and not doing any unnecessary work. Performance refers to meeting
    specific targets that depend on the problem the program is designed to solve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In different environments, the definition of performance may be completely
    different: the raw speed of the computation may be all that matters in a supercomputer,
    but it is not relevant in an interactive system as long as the system is faster
    than the person interacting with it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performance must be measured; the proof of success or the guidance to the causes
    of the failure is in the quantitative measurement results and their analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B16229_02_Epub_AM.xhtml#_idTextAnchor026):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance measurements are needed for two main reasons. First, they are used
    to define targets and describe the current status; without such measurements,
    we cannot say whether performance is poor or excellent; neither can we judge whether
    the performance targets are met. Second, measurements are used to study the effects
    of various factors on performance, evaluate the results of code changes and other
    optimizations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is no single way to measure performance for all situations because there
    are usually too many contributing factors and causes to analyze using a single
    approach and because of the sheer volume of data that is needed to characterize
    the performance fully.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Benchmarking done by manual instrumentation of the code has the advantage that
    it can collect any data you want, and it is easy to put the data in context: for
    each line of code, you know what function or step of the algorithm it belongs
    to. The main limitation is in the invasive nature of the method: you have to know
    what parts of the code to instrument and be able to do so; any areas of the code
    that are not covered by the data gathering instrumentation will not be measured.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Profiling is used to gather data on the distribution of the execution time or
    other metrics across the program. It can be done on the function or module level
    or at a lower level down to a single machine instruction. However, collecting
    the data at the lowest level of detail for the entire program at once is usually
    not practical, so the programs are usually profiled in stages, from coarse to
    fine granularity profiles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Small scale and micro-benchmarks are used to quickly iterate on code changes
    and evaluate their impact on performance. They can also be used to analyze the
    performance of small code fragments in detail. Care must be taken to ensure that
    the context of the execution in the micro-benchmark resembles that of the real
    program as closely as possible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B16229_03_Epub_AM.xhtml#_idTextAnchor047):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern CPUs have multiple computing units, many of which can operate at the
    same time. Using as much of the CPU computing power as possible at any time is
    the way to maximize a program's efficiency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any two computations that can be done at the same time take only as much time
    as the longer of the two computations; the other one is effectively *free*. In
    many programs, we can replace some computations that are to be done in the future
    with other computations that can be done now. Often the tradeoff is doing more
    computations now than would have been done later, but even that improves the overall
    performance as long as the extra computations take no additional time because
    they are done in parallel with some other work that has to be done anyway.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This situation is known as data dependency. The countermeasure is the pipelining,
    where part of the future computation that does not depend on any unknown data
    is executed in parallel with the code that precedes it in the program order.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conditional branches make the future computations indeterminate, which prevents
    the CPU from pipeline them. The CPU attempts to predict the code that will be
    executed so that it can maintain the pipeline. Whenever such a prediction fails,
    the pipeline must be flushed, and the results of all instructions that were predicted
    incorrectly are discarded.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Any code that may or may not be needed but is executed based on the CPU''s
    branch prediction is evaluated speculatively. In the speculative execution context,
    any action that cannot be undone must not be fully committed: the CPU cannot overwrite
    the memory, do any I/O operations, issue interrupts, or report any errors. The
    CPU has the necessary hardware to hold these actions *suspended* until the speculatively
    executed code is confirmed as real code, or not. In the latter case, all would-be
    results of the speculative execution are discarded with no observable effects.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A well-predicted branch typically has only a minor impact on performance. Therefore,
    the two main solutions to performance degradation caused by mispredicted branches
    are: rewrite the code such that the conditions become more predictable or change
    the computations to use conditionally accessed data instead of conditionally executed
    code. The latter is known as branchless computing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B16229_04_Epub_AM.xhtml#_idTextAnchor064):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern CPUs are significantly faster than even the best memories. The latency
    for accessing a random location in memory is several nanoseconds, enough time
    for the CPU to execute dozens of operations. Even in streaming access, the overall
    memory bandwidth is not enough to supply the CPU with the data at the same speed
    it can carry out the computations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The memory system includes a hierarchy of caches between the CPU and the main
    memory, so the first factor affecting the speed is the size of the data set: this
    ultimately determines whether the data fits into a cache or not. For a given size,
    the memory access pattern is critical: if the hardware can predict the next access,
    it can hide the latency by starting the transfer of data into the cache before
    this data is requested.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Often inefficient memory access is evident from a performance profile or timer
    output; this is particularly true for well-modularized code with good encapsulation
    of data. If the timing profile does not show the parts of the code that dominate
    the performance, the cache effectiveness profile may show which data is accessed
    inefficiently throughout the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any optimization that uses less memory is likely to improve memory performance
    since more of the data fits into the cache. However, sequential access to a large
    amount of data is likely to be faster than random access to a smaller amount of
    data, unless the smaller data fits into the L1 cache, or, at most, the L2 cache.
    Optimizations that directly target the memory performance usually take the form
    of data structure optimizations, aimed mostly at avoiding random access and indirect
    memory access. To go beyond these, we usually have to change the algorithms to
    change memory access patterns to more cache-friendly ones.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B16229_05_Epub_AM.xhtml#_idTextAnchor084):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The memory model describes the interaction of threads through shared memory;
    it is the set of restrictions and guarantees that are given when multiple threads
    access the same data in memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the one hand, if we did not need the shared data, all threads would run completely
    independently, and the program would scale perfectly as long as more processors
    are available. Also, writing such a program is no harder than writing a single-threaded
    program. On the other hand, all the bugs related to concurrency ultimately arise
    from invalid access to some shared data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The overall memory model is a superposition of the several memory models for
    different components of the system: first of all, the hardware has a memory model
    that applies to any program running on it. The OS and the runtime environment
    may provide additional restrictions and guarantees. Finally, the compiler implements
    the memory model of the language such as C++ and may impose additional restrictions
    if it offers a stricter memory model than the language requires.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Several factors limit the performance of concurrent programs. First is the availability
    of work to be done in parallel (this problem is to be solved by advances in concurrent
    algorithms and is outside of the scope of this book). Second is the availability
    of the hardware to actually do this work (we have seen the example of a program
    becoming memory-bound). Finally, any time the threads must access the same data
    (shared data) concurrently, this access must be synchronized, and the ability
    of the compiler and the hardware to optimize the execution across such synchronized
    accesses is severely limited.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B16229_06_Epub_AM.xhtml#_idTextAnchor103):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lock-based program, in general, cannot be guaranteed to do useful work toward
    the end goal at all times. In a lock-free program, at least one thread is guaranteed
    to make such progress, and in a wait-free program, all threads make progress toward
    the end goal all the time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"Wait-free" should be understood in the algorithmic sense: each thread completes
    one step of the algorithm and immediately moves on to the next one, and the computed
    results are never wasted or discarded due to the synchronization between threads.
    It does not mean that a particular step takes the same time when the computer
    runs many threads as it does on one thread; the contention for the hardware access
    is still there.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While the most commonly thought about drawback of locks is their relatively
    high cost, this is not the main reason to avoid their use: a good algorithm can
    often reduce the amount of data sharing enough that the cost of the lock itself
    is not a major issue. The more severe problem is the complexity of managing many
    locks in a program that needs fine-granularity data synchronization: locking large
    amounts of data with a single lock means that only one thread can operate on all
    the locked data, but using many locks for small chunks of data leads to deadlocks,
    or at least very complex lock management.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The difference is not in the implementation of the counter itself but in the
    data dependency: a counter has no dependencies and, therefore, does not need to
    provide any memory order guarantees. An index, on the other hand, should guarantee
    that the array or the container element indexed by a particular value is visible
    to a thread when the thread reads this index value.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The key feature of the publishing protocol is that it allows many consumer threads
    to access the same data without locking while guaranteeing that the data generated
    by the producer thread is visible to the consumers before they access this data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B16229_07_Epub_AM.xhtml#_idTextAnchor117):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Any data structure designed for thread safety must have a transactional interface:
    every operation must either not change the state of the data structure or transform
    it from one well-defined state to another well-defined state.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This comes to the general observation of the performance of concurrent code:
    the more shared variables there are, the slower the code is. A complex data structure
    usually needs more data shared between threads that access it concurrently. In
    addition, there are simple algorithms (some are wait-free) that allow limited
    thread-safe operations on the data structures.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With an efficient lock, a lock-guarded data structure is not necessarily slower.
    Often, it is faster. Again, it comes to how many variables are shared: a lock-free
    scheme that requires multiple atomic variables may be slower than a single lock.
    We also have to consider the locality of the access: if the data structure is
    accessed in one or two places (like a queue), the lock can be quite efficient.
    A data structure with many elements that can all be accessed simultaneously is
    likely to have very poor performance if the entire data structure must be locked
    every time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The main challenge is that adding memory to a data structure is usually a very
    disruptive operation that requires rearranging large parts of the internal data.
    It is difficult to do this while allowing other concurrent operations on the same
    data structure. For a lock-guarded data structure, this is of little concern (sometimes
    the lock is held for much longer than usual when one thread has to manage memory,
    but long delays can happen for other reasons as well, the program has to expect
    it). In lock-free data structures, it is very hard to manage memory if it affects
    the entire data structure. Nodal data structures do all their memory management
    on a single thread and use the publishing protocol to add new nodes to the structure,
    but sequential data structures may require data reallocation or at least complex
    internal memory management. In such cases, double-checked locking should be used
    to lock down the entire data structure while its memory is being reorganized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The A-B-A problem is common to all lock-free implementations of nodal data structures
    that use the position of data in memory to detect when a change was made. The
    problem happens when a new node is allocated in the memory of a previously deleted
    node. This creates the potential data race when another thread observes identical
    initial and final memory addresses, and the assumption is made that the data structure
    is unchanged. Multiple solutions exist, but all of them use various techniques
    to defer the deallocation of memory until the reallocation at the same address
    is no longer a problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B16229_08_Epub_AM.xhtml#_idTextAnchor138):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Without the standard giving some guarantees on the behavior of C++ programs
    in the presence of threads, it is not possible to write any portable concurrent
    C++ programs. Of course, in practice, we were using concurrency long before C++11,
    but this was made possible by the compiler writers who chose to follow an additional
    standard, such as POSIX. The downside of that situation was that these additional
    standards varied. There was no portable way to write, for example, concurrent
    programs for Linux and Windows without conditional compilation and OS-specific
    extensions for each platform. Similarly, atomic operations were implemented as
    CPU-specific extensions. Also, there were some subtle differences between various
    standards followed by different compilers, which occasionally resulted in very
    hard-to-find bugs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The use of parallel algorithms is very simple: any algorithm that has a parallel
    version can be invoked with an execution policy as the first argument. If this
    is the parallel execution policy, the algorithm will run on multiple threads.
    To achieve the best performance, on the other hand, it may be necessary to redesign
    parts of the program. In particular, parallel algorithms provide no benefit if
    the data sequence is too short (what constitutes short depends on the algorithm
    and the cost of operating on the data elements). It may be necessary, therefore,
    to redesign the program to operate on larger sequences at once.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Coroutines are functions that can suspend their own execution. After suspension,
    the control is returned to the caller (or to the resumer if this is not the first
    suspension). The coroutine can be resumed from any location in the code, from
    a different function or another coroutine, even from another thread.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B16229_09_Epub_AM.xhtml#_idTextAnchor149):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If it is necessary to make a copy of the object, then passing it by value accomplishes
    that. The programmer has to be careful to avoid making a second, unnecessary copy.
    Usually, this is done by moving from the function parameter; however, the programmer
    is responsible for not using the moved-from object as the compiler will not prevent
    it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the most common case, when the function operates on the object but does not
    affect its lifetime, the function should not get any access that allows it to
    affect the ownership. Even if the object ownership is managed by shared pointers,
    such functions should use references or raw pointers instead of creating unnecessary
    copies of shared pointers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return value optimization refers to the compiler optimization technique where
    a local variable is returned by value from a function. The optimization effectively
    removes the local variable and constructs the result directly in the memory allocated
    for it by the caller. This optimization is particularly useful in factory functions
    that must construct and return objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In memory-bound programs, the run time is limited by the speed of getting data
    to and from memory. Using less memory often leads directly to a faster running
    program. The second reason is more straightforward: memory allocations themselves
    take time. In concurrent programs, they also involve a lock, which serializes
    part of the execution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B16229_10_Epub_AM.xhtml#_idTextAnchor167):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most important constraint is that the result (or, more strictly, the observable
    behavior) of the program must not change. The bar here is high: the compiler is
    allowed to optimize only when it can be proven that the results are correct for
    all possible inputs. The second consideration is practicality: the compiler has
    to make tradeoffs between compilation time and efficiency of the optimized code.
    Even with the highest optimization enabled, it may be too expensive to prove that
    some code transformations do not break the program.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In addition to the obvious effect (elimination of the function call), inlining
    enables the compiler to analyze a larger fragment of code. Without inlining, the
    compiler generally has to assume that "anything is possible" inside a function
    body. With inlining, the compiler can see, for example, whether the call to the
    function produces any observable behavior, such as I/O. The inlining is beneficial
    only up to a point: when overdone, it increases the size of the machine code.
    Also, the compilers have difficulties analyzing very long code fragments (the
    longer the fragment, the more memory and time it takes for the optimizer to process
    it). Compilers have heuristics that determine whether a particular function is
    worth inlining.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the compiler does not make an optimization, it is often because this transformation
    is not guaranteed to be correct. The compiler does not have the same knowledge
    of how the program is going to be used that the programmer does; any combination
    of inputs is assumed to be valid. The other common reason is that the optimization
    is not expected to be universally effective. The compiler may be right on this
    count, but if the measurements show that the programmer is right, the optimization
    would have to be forced into the source code somehow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The main benefit of inlining is not that it eliminates the cost of the function
    call. Rather, it is that it allows the compiler to see what is going on inside
    the function. This enables continuous analysis of the code that immediately precedes
    and follows the function call. Some optimizations that were not possible when
    each section of the code was considered in isolation become possible when a larger
    code fragment is optimized as a single basic block.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B16229_11_Epub_AM.xhtml#_idTextAnchor176):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Undefined behavior is what happens when a program is executed out of contract:
    the specification says what the valid inputs are and what the results should be.
    If invalid input is detected, this is also a part of the contract. If the invalid
    input is not detected and the program proceeds on the (false) assumption that
    the input is valid, the results are undefined: the specification does not say
    what must happen.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In C++, there are two main reasons for allowing undefined behavior. First of
    all, there are operations that require hardware support or are executed differently
    on different hardware. It may be very difficult or even impossible to deliver
    a specific result on some hardware systems. The second reason is performance:
    it may be expensive to guarantee a specific outcome across all computing architectures.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No, an undefined result does not mean that the result must be wrong. The desired
    result is also permitted under undefined behavior, it's just not guaranteed. Further,
    undefined behavior taints the entire program. Compiling the same code in a file
    together with some other code may produce unexpected results. A new version of
    the compiler may be able to make better optimizations on the assumption that undefined
    behavior never happens. You should run the sanitizer and fix the errors it reports.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the same reason, the C++ standard does it: performance. If there is a special
    case that is hard to handle correctly without adding overhead to the "normal"
    case, you may choose not to handle the special case at all. While it is preferable
    to detect this situation at run time, such detection may also be expensive. In
    this case, the input validation should be optional. If the user supplies an invalid
    input but fails to run the detection tool, the program''s behavior is undefined
    since the algorithm itself assumes that the input is valid and that assumption
    has been violated.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B16229_12_Epub_AM.xhtml#_idTextAnchor184):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Design for performance boils down to creating a design that does not prevent
    high-performing algorithms and implementations by imposing constraints incompatible
    with such implementations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In general, the less the interface reveals the internal details of a component,
    the more freedom the implementer has. This should be balanced against the freedom
    of the client to use efficient algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Higher-level interfaces allow for better performance because they allow the
    implementer to temporarily violate the invariants specified by the interface contract.
    The initial and the final states of the component are visible to the caller and
    must maintain these invariants. However, if the implementer knows that the intermediate
    states are not exposed to the outside world, a more efficient temporary state
    can often be found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The short answer is, we can't. The objective is, then, to find a way to collect
    such measurements. This is done by measuring the performance of modeling benchmarks
    and prototypes and using the results to estimate performance limitations that
    result from different design decisions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
