- en: Asynchronous and Lock-Free Programming in C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at the threading library introduced by Modern
    C++ and various ways to create, manage, and synchronize threads. The way of writing
    code with threads is a rather low level and is prone to potential errors associated
    with concurrent code (deadlock, live-lock, and so on). Even though it is not noticed
    by many programmers, the Modern C++ language provides a standard memory model
    that helps to write concurrent code better. To be a concurrent programming language
    from the ground up, a language has to provide certain guarantees to the developer
    regarding memory access and the order in which things will be executed during
    runtime. If we are using constructs such as mutexes, condition variables, and
    futures to signal events, one doesn't need to be aware of the memory model. But
    awareness of the memory model and its guarantees will help us write faster concurrent
    code using lock-free programming techniques. Locks can be simulated using something
    called atomic operations, and we will look at this technique in depth.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in [Chapter 2](e1c95513-a3a7-40f2-ac25-9f95cbd9a2e6.xhtml),
    *A Tour of Modern C++ and its Key Idioms*, zero-cost abstraction remains one of
    the most fundamental principles of the C++ programming language. C++ is always
    a system programmer's language, and the standard committee managed to strike a
    good balance between higher-level abstraction mechanisms supported by the language
    and the ability to access lower-level resources to write system programs. C++
    exposes atomic types and a set of associated operations to have fine-grained control
    over the execution of programs. The standard committee has published detailed
    semantics of the memory model, and the language has a set of libraries that help
    programmers to exploit them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, we learned how to synchronize actions in separate
    threads using condition variables. This chapter discusses the facilities provided
    by the standard library to perform task-based parallelism using *futures*. In
    this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Task-based parallelism in C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The C++ memory model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Atomic types and atomic operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronizing operations and memory ordering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to write a lock-free data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task-based parallelism in C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A *task* is a computation that can be potentially executed concurrently with
    other computations. A thread is a system-level representation of a task. In the
    previous chapter, we learned how to execute a task concurrently with other tasks
    launched by constructing an `std::thread` object with the task as its argument
    to the constructor. A task can be any callable object such as a function, Lambda,
    or a functor. But this approach of executing a function concurrently using `std::thread`
    is called a *thread-based approach*. The preferred choice for concurrent execution
    is a *task-based approach*, and this will be discussed in this chapter. The advantage
    of a task-based approach over a thread-based approach is to operate at the (higher)
    conceptual level of tasks rather than directly at the lower level of threads and
    locks. Task-based parallelism is achieved by following standard library features:'
  prefs: []
  type: TYPE_NORMAL
- en: Future and promise for returning a value from a task associated with a separate
    thread
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`packaged_task` to help launch tasks and provide a mechanism for returning
    a result'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`async()` for launching a task similar to a function call'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future and promise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The C++ tasks often behave like a data channel of sorts. The sending end, often
    called promise, sends data to a receiving end, often called the **future**. The
    important notion about futures and promises is that they enable a transfer of
    values between two tasks without the explicit use of a lock. The transfer of values
    is handled by the system (runtime) itself. The basic concept behind **future** and
    **promise** is simple; when a task wants to pass a value into another, it puts
    the value into a **promise**.
  prefs: []
  type: TYPE_NORMAL
- en: 'A standard library makes sure that the future associated with this promise
    gets this value. The other task can read this value from this **future** (the
    following diagram has to be read from the right to the left):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fe6fd77-da39-41be-9e63-f72a5e88eb24.png)'
  prefs: []
  type: TYPE_IMG
- en: The future comes in handy if a calling thread needs to wait for a specific *one-off
    event*. The future representing this event makes itself available to the calling
    thread, and the calling thread can access the value once the future is ready (when
    a value is set to a corresponding promise). During its execution, a future may
    have data associated with it or not. Once the event occurs, data will be available
    in the future and it can't be reset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The template classes associated with task-based parallelism are declared inside
    the library header `<future>`. There are two sorts of futures available in the
    standard library: unique futures (`std::future<>`) and shared futures (`std::shared_future<>`).
    You can correlate these with the smart pointers `std::unique_ptr<>` and `std::shared_ptr<>`*,*
    respectively. The `std::future` instance refers to the one and only instance of
    the associated event. On the contrary, multiple instances of `std::shared_future`
    may point to the same event. In the case of `shared_future`, all the instances
    associated with a common event will become ready at the same time and they may
    access the data associated with the event. The template parameter is the associated
    data, and the `std::future<void>` and `std::shared_future<void>` template specifications
    should be used if there is no data associated with it. Even though data communication
    between threads is managed internally by futures, the future objects themselves
    don''t provide synchronized access. If multiple threads need to access a single
    `std::future` object, they must be protected with mutexes or other synchronization
    mechanisms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The classes `std::future` and `std::promise` work in pairs to separate task
    invocation and wait for results. For an `std::future<T>` object `f`, we can access
    the value `T` associated with it using the `std::future` class function `get()`.
    Similarly for an `std::promise<T>`, there are two put operation functions available
    with it (`set_value()` and `set_exception()`) to match the future''s `get()`.
    For a promise object, you can either give it a value by using `set_value()` or
    pass an exception to it using `set_exception()`. For example, the following pseudo
    code helps you see how the values are set in the promise (in `func1`) and how
    things are consumed in the function where invocation to `future<T>:: get()` is
    invoked (`func2`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding case, the *val* of type `T` is set to promise *pr* after processing
    and obtaining a result. If any exception happens during the execution, the exception
    is also set to promise. Now, let''s see how to access the value you set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, the value set in the corresponding promise is accessed using the future
    passed as an argument. The `get()` function associated with `std::future()` retrieves
    the value stored during the execution of the task. The call to `get()` must be
    prepared to catch the exception transmitted through the future and handle it.
    After explaining `std::packaged_task`, we will show a complete example where futures
    and promises work together in action.
  prefs: []
  type: TYPE_NORMAL
- en: std::packaged_task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s discuss how we get a return value associated with a future into
    your code that needs results. The `std::packaged_task` is a template class that
    is available in the standard library to achieve task-based parallelism with the
    help of futures and promises. By setting up futures and promises in threads, it
    simplifies setting up a task without any explicit locks for sharing the result.
    A `packaged_task` instance provides a wrapper over `std::thread` to put the return
    value or exception caught into a promise. The member function `get_future()` in
    `std::packaged_task` will give you the future instance associated with the corresponding
    promise. Let''s look at an example that uses a packaged task to find the sum of
    all elements in a vector (the working of promise is deep inside the implementation
    of `packaged_task`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `packaged_task` object takes the type of task as its template argument and
    the function pointer (`calc_sum`) as a constructor argument. The future instance
    is obtained through the call to the `get_future()` function of the task object.
    The explicit `std::move()` is used since the `packaged_task` instances cannot
    be copied. This is because it is a resource handle and is responsible for whatever
    resources its task may own. Then, a call to the `get()` function picks up the
    result from the task and prints it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how `packaged_task` can be used along with Lambdas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, instead of a function pointer, a Lambda is passed into the constructor
    of `packaged_task`. As you have already seen in previous chapters, for a small
    block of code to run concurrently, Lambdas come in handy. The primary notion behind
    futures is to be able to get results without having any concern for the mechanisms
    for managing communication. Also, these two operations are running in two different
    threads and thus are parallel.
  prefs: []
  type: TYPE_NORMAL
- en: std::async
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Modern C++ provides a mechanism to execute a task like a function that might
    or might not execute in parallel. Here, we are referring to `std::async`*,* which
    manages the threading detail internally. `std::async` takes a callable object
    as its argument and returns an `std::future` that will store the result or exception
    from the task that has been launched. Let''s rewrite our previous example to calculate
    the sum of all elements from a vector using `std::async`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Primarily, when using `std::async` for task-based parallelism, the launch of
    a task and fetching result from the task are following straightforward syntax and
    well-separated with task execution. In the preceding code, `std::async` is taking
    three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: The `async` flag determines the launch policy of the `async` task and `std::launch::async`,
    meaning that `async` executes the task on a new thread of execution. The `std::launch::deferred` flag
    doesn't spawn a new thread, but *lazy evaluation* is performed. If both the flags
    are set as in `std::launch::async` and `std::launch::deferred`, it is up to the
    implementation as to whether to perform an asynchronous execution or lazy evaluation.
    If you explicitly don't pass any launch policy into `std::async`, it is again
    up to the implementation to choose the method of execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second argument to the `std::async` is a callable object, and it can be
    a function pointer, function object, or a Lambda. In this example, the `calc_sum` function
    is the task that gets executed in a separate thread.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third argument is the input parameter to the task. Generally, that is a
    variadic argument and it can pass the number of parameters required for a task
    callable object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s see how `async` and Lambda go together for the same example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the callable object argument has a Lambda function inside it,
    which returns the result of `std::accumulate()`. As always, simple operations
    along with the Lambda beautify the code's overall appearance and improve readability.
  prefs: []
  type: TYPE_NORMAL
- en: Using `async`, you don't have to think about threads and locks. But just think
    in terms of tasks that do the computations asynchronously, and that you don't
    know how many threads will be used because that's up to the internal implementation
    to decide based on the system resources available at the time of calling. It checks
    for the idle cores (processors) that are available before deciding how many threads
    to use. This points to the obvious limitation with `async` in that it needs to
    be employed for tasks that share resources needing locks.
  prefs: []
  type: TYPE_NORMAL
- en: C++ memory model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The classic C++ was essentially a single threaded language. Even though people
    were writing multithread programs in C++, they were using respective platform
    threading facilities to write them. Modern C++ can be considered a concurrent
    programming language. The language standard provides a standard thread and task
    mechanism (as we have already seen) with the help of standard libraries. Since
    it is a part of the standard library, the language specification has defined how
    things should behave across the platform in a precise manner. Having a consistent
    platform-agnostic behavior for threads, tasks, and so on is a massive challenge
    that the standard committee handled really well. The committee designed and specified
    a standard memory model for achieving consistent behavior while the program is
    running. The memory model consists of two aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Structural** aspects, which relate to how data is laid out in memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concurrency** aspects, which deal with the concurrent access of memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a C++ program, all data is made up of *objects*. The language defines an
    object as a *region of storage*, which is defined with its type and lifetime.
    Objects can be an instance of a fundamental type such as an int or double, or
    instances of user-defined types. Some objects may have sub objects, but others
    don't. The key point is that every variable is an object, including the members'
    objects of other objects, and every object occupies at least some memory location.
    Now, let's take a look at what this has to do with concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Memory access and concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For multithread applications, everything hangs on those memory locations. If
    multiple threads access different memory locations, everything works fine. But
    if two threads access the same memory location, then you must be very careful.
    As you have seen in [Chapter 3](16bbadb9-c545-44b1-8edb-82ab82a83394.xhtml), *Language-Level
    Concurrency and Parallelism in C++*, multiple threads trying to read from the
    same memory location introduce no trouble, but as soon as any thread tries to
    modify data in a common memory location, chances for *race conditions* to occur
    come into the frame.
  prefs: []
  type: TYPE_NORMAL
- en: The problematic race conditions can only be avoided by enforced ordering between
    the access in multiple threads. As discussed in [Chapter 3](https://cdp.packtpub.com/c___reactive_programming/wp-admin/post.php?post=48&action=edit#post_40), *Language-Level
    Concurrency and Parallelism in C++*, lock-based memory access using mutexes is
    a popular option. The other way is to leverage the synchronization properties
    of *atomic operations* by enforcing ordering between the access in two threads.
    In later sections of this chapter, you will see the use of atomic operations to
    enforce ordering.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operation appears to the rest of the system and occurs at once without
    being interrupted (no task switch happens during atomic operation) in concurrent
    programming. Atomicity is a guarantee of isolation from interrupts, signals, concurrent
    processes, and threads. More can be read on this topic at the Wikipedia article
    at [https://en.wikipedia.org/wiki/Linearizability](https://en.wikipedia.org/wiki/Linearizability).
  prefs: []
  type: TYPE_NORMAL
- en: If there is no enforced ordering between multiple accesses to a single memory
    location from different threads, one or both accesses are not atomic. If there
    is a write involved, then it can cause a data race and could lead to an undefined
    behavior. The data race is a serious bug, and it must be avoided at all costs.
    The undefined behavior can be avoided by atomic operations, but it doesn't prevent
    the race situation. The atomic operation makes sure that thread switching never
    happens when the operation is going on. This is a guarantee against interleaved
    access to memory. The atomic operations guarantee the preclusion of the interleaved
    memory access (serial ordering), but cannot prevent race conditions (as there
    is potential to overwrite updates).
  prefs: []
  type: TYPE_NORMAL
- en: The modification contract
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While a program or process is in execution, all the threads in the system should
    agree on the modification order (for the memory). Every program is executed in
    an environment, which involves the instruction stream, memory, registers, heap,
    stack, caches, virtual memory, and so on. This modification order is a contract,
    between the programmer and system, that is defined by the memory model. The system
    consists of the compiler (and linker), which morphs the program into executable
    code, the processor, which executes the instruction set specified in the stream,
    the cache, and associated states of the program. The contract requires mandating
    the programmer to obey certain rules, which enables the system to generate a fully
    optimized program. This set of rules (or heuristics) that a programmer has to
    conform to while writing code to access memory is achieved with the help of atomic
    types and atomic operations that were introduced in the standard library.
  prefs: []
  type: TYPE_NORMAL
- en: 'These operations are not only atomic, but they create synchronization and order
    constraints on the program''s execution. Compared to higher-level lock-based synchronization
    primitives (mutexes and condition variables), discussed in [Chapter 3](https://cdp.packtpub.com/c___reactive_programming/wp-admin/post.php?post=48&action=edit#post_40), *Language-Level
    Concurrency and Parallelism in C++*, you can tailor synchronizations and order
    constraints to your needs. The important take away from the C++ memory model is
    this: even though the language has adopted a lot of modern programming idioms
    and language features, C++, as a system programmer''s language, has given more
    low-level control to your memory resources to optimize the code as you desire.'
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations and types in C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally, a non-atomic operation might be seen as half-done by other threads.
    As discussed in [Chapter 3](https://cdp.packtpub.com/c___reactive_programming/wp-admin/post.php?post=48&action=edit#post_40), *Language-Level
    Concurrency and Parallelism in C++*, in such cases, the invariance associated
    with the shared data structure will be broken. This happens when the modification
    to a shared data structure requires modification of more than one value. The best
    example of this is a partially removed node of a binary tree. If another thread
    tries to read from this data structure at the same time, the invariant will be
    broken and could result in undefined behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Using an *atomic operation*, you can't observe an operation that's half-done
    from any thread in the system, because atomic operations are indivisible. If any
    operation (such as read) associated with an object is atomic, then all of the
    modifications to the object are also atomic. C++ has provided atomic types so
    that you can use atomicity as you require.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All atomic types defined by the standard library can be found in the `<atomic>`
    header library. The system guarantees the atomicity to these types and all the
    related operations with these types. Some operations may not be atomic, but the
    system creates the illusion of atomicity in such cases. The standard atomic types
    use a member function, `is_lock_free()`, that allows the user to determine whether
    operations on a given type are done directly with atomic instructions (`is_lock_free()`
    returns `true`) or done using internal locks by the compiler and library (`is_lock_free()`
    returns `false`).
  prefs: []
  type: TYPE_NORMAL
- en: '`std::atomic_flag` is different among all atomic types. The operations on this
    type are required to be atomic as per the standard. Hence, this doesn''t provide
    the `is_lock_free()` member function. This is a very simple type with a minimal
    set of allowed operations such as `test_and_set()` (they can be either queried
    or set) or `clear()` (clears the value).'
  prefs: []
  type: TYPE_NORMAL
- en: The remaining atomic types follow a similar signature as per the specifications
    of the `std::atomic<>` class template. These types, compared to `std::atomic_flag`,
    are a bit more fully featured, but not all operations are always atomic. The atomicity
    of *operations* highly depends on the platform as well. On popular platforms,
    the atomic variants of built-in types are indeed lock-free, but this is not guaranteed
    everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of using `std::atomic<>` template classes, you can use the direct types
    supplied by the implementation, as given in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Atomic type** | **Corresponding specialization** |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_bool` | `std::atomic<bool>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_char` | `std::atomic<char>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_schar` | `std::atomic<signed char>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_uchar` | `std::atomic<unsigned char>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_int` | `std::atomic<int>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_uint` | `std::atomic<unsigned>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_short` | `std::atomic<short>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_ushort` | `std::atomic<unsigned short>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_long` | `std::atomic<long>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_ulong` | `std::atomic<unsigned long>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_llong` | `std::atomic<long long>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_ullong` | `std::atomic<unsigned long long>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_char16_t` | `std::atomic<char16_t>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_char32_t` | `std::atomic<char32_t>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_wchar_t` | `std::atomic<wchar_t>` |'
  prefs: []
  type: TYPE_TB
- en: 'Along with all of these basic atomic types, the C++ standard library has also
    provided a set of `typedefs` for atomic types compared to the `typedefs` available
    in the standard library such as `std::size_t`. There is a simple pattern to identify
    the corresponding atomic version of `typedefs`: for any standard `typedef T`,
    use the `atomic_ prefix`: `atomic_T`. The following table lists the standard atomic
    `typedefs` and their corresponding built-in `typedefs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Atomic** `typedef` | **Standard library** `typedef` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_size_t` | `size_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_intptr_t` | `intptr_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_uintptr_t` | `uintptr_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_ptrdiff_t` | `ptrdiff_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_intmax_t` | `intmax_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_uintmax_t` | `uintmax_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_int_least8_t` | `int_least8_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_uint_least8_t` | `uint_least8_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_int_least16_t` | `int_least16_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_uint_least16_t` | `uint_least16_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_int_least32_t` | `int_least32_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_uint_least32_t` | `uint_least32_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_int_least64_t` | `int_least64_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_uint_least64_t` | `uint_least64_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_int_fast8_t` | `int_fast8_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_uint_fast8_t` | `uint_fast8_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_int_fast16_t` | `int_fast16_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_uint_fast16_t` | `uint_fast16_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_int_fast32_t` | `int_fast32_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_uint_fast32_t` | `uint_fast32_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_int_fast64_t` | `int_fast64_t` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_uint_fast64_t` | `uint_fast64_t` |'
  prefs: []
  type: TYPE_TB
- en: 'The `std::atomic<>` class templates are not just a set of specializations;
    they have a primary template to expand and an atomic variant of the user-defined
    type. Being a generic template class, the operations supported are limited to
    `load()`, `store()`, `exchange()`, `compare_exchange_weak()`, and `compare_exchange_strong()`.
    Each of the operations on atomic types has an optional argument to specify the
    memory-ordering semantics that are required. The concepts of memory ordering will
    be covered in detail in a later section of this chapter. For now, just keep in
    mind that all atomic operations can be divided into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Store operations:** These operations can have `memory_order_relaxed`, `memory_order_release`,
    or `memory_order_seq_cst` ordering'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load operations:** These can have `memory_order_relaxed`, `memory_order_consume`,
    `memory_order_acquire`, or `memory_order_seq_cst` ordering'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Read-modify-write operations:** These operations can have `memory_order_relaxed`,
    `memory_order_consume`, `memory_order_acquire`, `memory_order_release`, `memory_order_acq_rel`,
    or `memory_order_seq_cst` ordering'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default memory ordering for all atomic operations is `memory_order_seq_cst`.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to conventional standard C++ types, standard atomic types are not *copiable*
    or *assignable*. This means that they have no copy constructors or copy assignment
    operators. Apart from direct member functions, they support from and implicit
    conversions to the corresponding built-in types. All operations on atomic types
    are defined as atomic, and assignment and copy-construction involve two objects.
    An operation involving two distinct objects cannot be atomic. In both operations,
    the value must read from one object and be written to the other. Therefore, these
    operations cannot be considered atomic.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at the operations that you can actually perform on each of the
    standard atomic types, beginning with `std::atomic_flag`.
  prefs: []
  type: TYPE_NORMAL
- en: std::atomic_flag
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`std::atomic_flag` represents a Boolean flag, and it is the simplest among
    all the atomic types in the standard library. This is the only type where all
    operations on it are required to be *lock-free* in every platform. This type is
    very basic, hence it is intended as a building block only.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `std::atomic_flag` object must always be initialized with `ATOMIC_FLAG_INIT`
    to set the state to *clear*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the only atomic type that requires such initialization, irrespective
    of the scope of its declaration. Once it is initialized, there are only three
    operations permissible with this type: destroy it, clear it, or set a query for
    the previous value. These correspond to the destructor, the `clear()` member function,
    and the `test_and_set()` member function, respectively. `clear()` is a *store*
    operation, whereas `test_and_set()` is a read-modify-write operation, as discussed
    in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, the `clear()` function call requests that the
    flag is cleared with default memory order, which is `std:: memory_order_seq_cst`,
    while the call to `test_and set()` uses the relaxed semantics (more on this in
    the *Relaxed ordering*), which are explicitly used for setting the flag and retrieving
    the old value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The primitive implementation of `std::atomic_flag` makes it ideal for the spin-lock
    mutex. Let''s see an example spin-lock:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, the instance variable `flg` (of the `std::atomic_flag`
    type) is cleared initially. In the lock method, it tries to set the flag by testing
    the `flg` to see whether the value is cleared.
  prefs: []
  type: TYPE_NORMAL
- en: If the value is cleared, the value will be set and we will exit  the loop. The
    value in the flag will only be reset when the flag is cleared by the `unlock()`
    method. In other words, this implementation achieves mutual exclusion with a busy
    wait in `lock()`.
  prefs: []
  type: TYPE_NORMAL
- en: Because of its limitation, `std::atomic_ flag` cannot be used as a Boolean atomic
    type, and it doesn't support any *non-modifying query* operations. So, let's look
    into `std::atomic<bool>` to compensate the requirement of atomic Boolean flags.
  prefs: []
  type: TYPE_NORMAL
- en: std::atomic<bool>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`std::atomic<bool>` is a full-featured atomic Boolean type compared to `std::atomic_flag`.
    But neither copy-construction nor assignment is possible with this type. The value
    of an `std::atomic<bool>` object can initially be either `true` or `false`. The
    objects of this type can be constructed or assigned values from a non-atomic `bool`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: One thing needs to be noted about the assignment operator of atomic types, which
    is that the operator returns the value of non-atomic types rather than the conventional
    scheme of returning references. If a reference is returned instead of a value,
    it would create a situation where the result of assignment gets the result of
    a modification by another thread, that is, if it depends on the result of the
    assignment operator. While returning the result of the assignment operator as
    a non-atomic value, this additional load can be avoided, and you can infer that
    the value obtained is the value that has actually been stored.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s move on to the operations supported by `std::atomic<bool>`. First
    and foremost, the `store()` member function, which is available in `std::atomic<bool>`,
    is used for write operations (either `true` or `false`), and it replaces the corresponding
    restrictive `clear()` function of `std::atomic_flag`. Also, the `store()` function
    is an atomic store operation. Similarly, the `test_and_set()` function has been
    effectively replaced with a more generic `exchange()` member function that allows
    you to replace the stored value with a chosen new one and retrieves the original
    value. This is an atomic *read-modify-write* operation. Then, `std::atomic<bool>`
    supports a simple non-modifying query of the value with an explicit call to `load()`,
    which is an atomic load operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Apart from `exchange()`, `std::atomic<bool>` introduces an operation to perform
    a read-modify-write operation, which executes the popular atomic **compare-and-swap**
    (**CAS**) instructions. This operation stores a new value if the current value
    is equal to an expected value. This is called a compare/exchange operation. There
    are two implementations of this operation that are available in standard library
    atomic types: `compare_exchange_weak()` and `compare_exchange_strong()`. This
    operation compares the value of the atomic variable with a supplied expected value
    and stores the supplied value if they are equal. If these values are not equal,
    the expected value is updated with the actual value of the atomic variable. The
    return type of the compare/exchange function is a *bool*, which is `true` if the
    store was performed; otherwise, it is `false`.'
  prefs: []
  type: TYPE_NORMAL
- en: For `compare_exchange_weak()`, the store might not be successful, even if the
    expected value and original value are equal. In such cases, the exchange of value
    will not happen and the function will return `false`. This most often happens
    on a platform that lacks single compare-and-swap instructions, which means that
    the processor cannot guarantee that the operation will be executed atomically.
    In such machines, the thread performing the operation might get switched out halfway
    through executing the sequence of instructions associated with the operation,
    and another thread will be scheduled in its place by the operating system with
    a given condition of more threads running than the number of available processors.
    This condition is called **spurious failure**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since `compare_exchange_weak()` can cause spurious failure, it should be used
    in a loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the loop continues to iterate as long as expected is
    `false`, and it denotes that spurious failure is happening to the `compare_exchange_weak()`
    call. On the contrary, `compare_exchange_strong()` is guaranteed to return `false`
    if the actual value isn't equal to the expected value. This can avoid the need
    for loops as in the previous situations where you want to know the status of variables
    with respect to running threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'The compare/exchange functions can take two memory-ordering parameters in order
    to allow the memory-ordering semantics to differ in success and failure cases.
    Those memory-ordering semantics are only valid for store operations and cannot
    be used for failure cases, since a store operation won''t occur:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If you won't specify any memory-ordering semantics, the default `memory_order_seq_cst`
    will be taken for both success and failure cases. If you don't specify any ordering
    for failure, then it's assumed to be the same as for success, except that the
    release part of the ordering is omitted. `memory_order_acq_rel` becomes `memory_order_acquire`
    and `memory_order_release` becomes `memory_order_relaxed`.
  prefs: []
  type: TYPE_NORMAL
- en: The specifications and consequences of memory ordering will be discussed in
    detail in the *Memory ordering* section of this chapter. Now, let's see the use
    of atomic integral types as a group.
  prefs: []
  type: TYPE_NORMAL
- en: Standard atomic integral types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to `std::atomic<bool>`, standard atomic integral types can be neither
    copy-constructible nor copy-assignable. However, they can be constructed and assigned
    from the corresponding non-atomic standard variant. Apart from the mandatory `is_lock_free()`
    member function, the standard atomic integral types, such as `std::atomic<int>`
    or `std::atomic<unsigned long long>`, also have `load()`, `store()`, `exchange()`,
    `compare_exchange_weak()`, and `compare_exchange_strong()` member functions, with
    similar semantics to those of `std::atomic<bool>`.
  prefs: []
  type: TYPE_NORMAL
- en: The integral variants of atomic types do support mathematical operations such
    as `fetch_add()`, `fetch_sub()`, `fetch_and()`, `fetch_or()` and `fetch_xor()`,
    compound-assignment operators (`+=`, `-=`, `&=`, `|=` and `^=`), and both post-
    and pre-increment and decrement operators with `++` and `--`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The named functions, such as `fetch_add()` and `fetch_sub()`, atomically perform
    their operations and return the old value, but the compound-assignment operators
    return the new value. Pre- and post-increment/decrement work as per usual C/C++
    conventions: the post-increment/decrement performs the operation, but returns
    the old value, and pre-increment/decrement operators perform the operation and
    return the new value. The following simple example can easily demonstrate the
    specifications of these operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for this code should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Except for `std::atomic_flag` and `std::atomic<bool>`, all of the other listed
    atomic types in the first table are atomic integral types. Now, let's look into
    the atomic pointer specialization, `std::atomic<T*>`.
  prefs: []
  type: TYPE_NORMAL
- en: std::atomic<T*> – pointer arithmetic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Along with the usual set of operations such as `load()`, `store()`, `exchange()`,
    `compare_exchange_weak()`, and `compare_exchange_strong()`, the atomic pointer
    type is loaded with the pointer arithmetic operations. The member functions `fetch_add()`
    and `fetch_sub()` provide operation support for the type to do atomic addition
    and subtraction on the stored address, and the operators `+=` and `-=`, and both
    pre- and post-increment/decrement, use the `++` and `--` operators.
  prefs: []
  type: TYPE_NORMAL
- en: The operators work in the same way as standard non-atomic pointer arithmetic
    works. If `obj` is an `std::atomic<some_class*>`, an object points to the first
    entry of an array of `some_class` objects. The `obj+=2` changes it to point to
    the third element in the array and returns a raw pointer to `some_class*` that
    points to the third element in the array. As discussed in the *Standard atomic
    integral types* section, the named functions such as `fetch_add()` and `fetch_sub`
    execute the operation on atomic types, but return the pointer to the first element
    in the array.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function forms of atomic operations also allow the memory-ordering semantics
    to be specified in an additional argument to the function call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Since both `fetch_add()` and `fetch_sub` are read-modify-write operations, they
    can use any memory ordering semantics in a standard atomic library. But, for the
    operator forms, memory ordering cannot be specified, so these operators will always
    have `memory_order_seq_cst` semantics.
  prefs: []
  type: TYPE_NORMAL
- en: std::atomic<> primary class template
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The primary class template in the standard library allows the user to create
    an atomic variant of a **user-defined type** (**UDT**). To use a user-defined
    type as an atomic type, you have to follow some criteria before implementing the
    class. For a user-defined class UDT, `std::atomic<UDT>` is possible if this type
    has a trivial copy-assignment operator. This means that the user-defined class
    should not contain any virtual functions or virtual base classes and must use
    the compiler-generated default copy-assignment operator. Also, every base class
    and non-static data member of the user-defined class must have a trivial copy-assignment
    operator. This allows the compiler to execute `memcpy()`or an equivalent operation
    for assignment operations, since there is no user-written code to execute.
  prefs: []
  type: TYPE_NORMAL
- en: Along with the requirements on assignment operators, the user-defined types
    must be *bitwise equality comparable*. This means that you must be able to compare
    the instances for equality using `memcmp()`. This guarantee is required to ensure
    that the compare/exchange operation will work.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an instance of the standard atomic type with the user-defined type `T`,
    that is, `std::atomic<T>`, the interface is limited to the operations available
    for `std::atomic<bool>`: `load()`, `store()`, `exchange()`, `compare_exchange_weak()`,
    `compare_exchange_strong()`, and the assignment from and conversion to an instance
    of type `T`.'
  prefs: []
  type: TYPE_NORMAL
- en: Memory ordering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already learned about the atomic types and atomic operators available
    in the standard library. While performing operations on atomic types, we need
    to specify memory ordering for certain operations. Now, we will talk about the
    significance and use cases for the different memory-ordering semantics. The key
    idea behind atomic operations is to provide synchronization in data access across
    multiple threads, and this is achieved by enforcing the order of execution. For
    example, if writing to the data happens before the read from the data, things
    will be fine. Otherwise, you are in trouble! There are six memory-ordering options
    available with the standard library that can be applied to operations on atomic
    types: `memory_order_relaxed`, `memory_order_consume`, `memory_order_acquire`,
    `memory_order_release`, `memory_order_acq_rel`, and `memory_order_seq_cst`. For
    all atomic operations on atomic types, `memory_order_seq_cst` is the memory order
    by default unless you specify something else.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These six options can be classified into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequentially consistent ordering**: `memory_order_seq_cst`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Acquire-release ordering**: `memory_order_consume`, `memory_order_release`,
    `memory_order_acquire`, and `memory_order_acq_rel`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relaxed ordering**: `memory_order_relaxed`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cost of execution varies with different CPUs for different memory-ordering
    models. The availability of distinct memory-ordering models allows an expert to
    take advantage of the increased performance of more fine-grained ordering relationships
    compared to blocking sequentially consistent ordering, but to choose the appropriate
    memory model as required, one should understand how these options affect the behavior
    of the program. Let's look into the sequentially consistent model first.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential consistency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of sequential consistency was defined by Leslie Lamport in 1979\.
    Sequential consistency provides two guarantees in the execution of a program.
    First and foremost, memory ordering the instructions of a program are executed
    in source code order, or an illusion of source code order will be guaranteed by
    the compiler. Then, there is a global order of all atomic operations in all threads.
  prefs: []
  type: TYPE_NORMAL
- en: For a programmer, the global ordering behavior of sequential consistency in
    which all operations in all threads take place in a global clock is an interesting
    high ground, but is also a disadvantage.
  prefs: []
  type: TYPE_NORMAL
- en: 'The interesting thing about sequential consistency is that the code works as
    per our intuition of multiple concurrent threads, but with the cost of a lot of
    background work being done by the system. The following program is a simple example
    to give us an edge into sequential consistency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding program synchronizes the threads `thread1` and `thread2` with
    the help of sequential consistency. Because of sequential consistency, the execution
    is totally *deterministic*, so the output of this program is always as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, `thread1` waits in the while loop until the atomic variable `ready` is
    `true`. As soon as *ready* becomes `true` in `thread2`, `thread1` continues its
    execution, hence the result always gets updated with strings in the same order.
    The usage of sequential consistency allows both threads to see the operations
    in other threads in the same order, hence both threads follow the same global
    time clock. The loop statement also helps to hold the time clock for the synchronization
    of both threads.
  prefs: []
  type: TYPE_NORMAL
- en: The details of *acquire-release semantics* will follow in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Acquire-release ordering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's dive deep into memory-ordering semantics provided by the C++ standard
    library. This is the area where the programmer's intuition about ordering in multithread
    code begins to fade, because there is no global synchronization between threads
    in acquire-release semantics of atomic operations. These semantics only allow
    synchronization between atomic operations on the same atomic variable. To elaborate,
    the load operation on an atomic variable performing in one thread can be synchronized
    with store operation happening on the same atomic variable in some other thread.
    A programmer must extract this feature that establishes a *happen-before* relationship
    between atomic variables to synchronize between threads. This makes working with
    an acquire-release model a bit difficult, but at the same time more thrilling.
    The acquire-release semantics shorten the journey towards lock-free programming,
    because you don't need to bother about synchronization of threads, but synchronization
    of the same atomic variables in different threads is the one we need to reason
    about.
  prefs: []
  type: TYPE_NORMAL
- en: As we explained previously, the key idea of acquire-release semantics is the
    synchronization between a release operation with an acquire operation on the same
    atomic variable and establishing an *ordering constant* in addition to this. Now,
    as the name implies, an acquire operation involves acquiring a lock, which includes
    the operations used to read an atomic variable, such as the `load()` and `test_and_set()`
    functions. Consequently, the releasing of a lock is a release operation, which
    consists of atomic operations such as `store()` and `clear()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, the lock of a *mutex* is an acquire operation, whereas the
    unlock is a release operation. Thus, in a *critical-section*, the operation on
    a variable cannot be taken outside in either direction. However, a variable can
    be moved inside a critical-section, because the variable moves from an unprotected
    area to a protected area. This is represented in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/688c368f-57ff-42a4-b55c-54a8feabee4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The critical-section contains one-way barriers: an acquire barrier and a release
    barrier. The same reasoning can be applied for starting a thread and placing a
    join-call on a thread, and the operations related to all other synchronization
    primitives available with the standard library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since synchronization takes place at atomic variable level rather than at thread
    level, let''s revisit the spin-lock that''s been implemented using `std::atomic_flag`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this code, the `lock()` function is an `acquire` operation. Instead of using
    the default sequentially consistent memory ordering that was used in the previous
    example, an explicit acquire memory ordering flag is used now. Also, the `unlock()`
    function, which is a release operation that was also using default memory order,
    has now been replaced with explicit release semantics. So, the heavyweight synchronization
    with sequential consistency of two threads is replaced by the lightweight and
    performant acquire-release semantics.
  prefs: []
  type: TYPE_NORMAL
- en: As the number of threads using the `spin_lock` increases more than two threads,
    the general acquire semantics using `std::memory_order_acquire` will not be sufficient,
    because the lock method becomes an acquire-release operation. Therefore, the memory
    model has to be changed to `std::memory_order_acq_rel`.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen that sequentially consistent ordering ensures synchronization
    between threads, while acquire-release ordering establishes ordering between read
    and write operations on the same atomic variable on multiple threads. Now, let's
    see the specifications of relaxed memory ordering.
  prefs: []
  type: TYPE_NORMAL
- en: Relaxed ordering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Operations on atomic types performed with relaxed memory ordering using the
    tag `std::memory_order_relaxed` are not synchronization operations. In contrast
    with other ordering options that are available in the standard library, they do
    not impose an order among concurrent memory access. The relaxed memory ordering
    semantics only guarantee that the operations on the same atomic type inside the
    same thread cannot be reordered, and this guarantee is called **modification order
    consistency**. In fact, relaxed ordering only guarantees atomicity and modification
    order consistency. Therefore, other threads can see these operations in different
    orders.
  prefs: []
  type: TYPE_NORMAL
- en: Relaxed memory ordering can be used effectively in places where synchronization
    or ordering is not required, and atomicity can be an added advantage for performance
    boosting. One typical example would be incrementing counters, such as reference
    counters of **std::shared_ptr**, where they only require atomicity. But decrementing
    the reference count needs acquire-release synchronization with the destructor
    of this template class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see a simple example to count the number of threads that were spawned
    with relaxed ordering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In this code, ten threads are spawned from the `main()` function with a thread
    function `func()`, where on each thread the atomic integer value is incremented
    by one using the atomic operation `fetch_add()`. In contrast to compound assignment
    operators and post- and pre-increment operators, available with `std::atomic<int>`,
    the `fetch_add()` function can accept the memory ordering argument and it is `std::memory_order_relaxed`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program prints the number of threads spawned in the program as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The output of the program remains the same for any other relevant memory-ordering
    tags, but the relaxed memory ordering ensures atomicity and thus performance.
  prefs: []
  type: TYPE_NORMAL
- en: Until now, we have discussed the levels of the different memory models, and
    their effect on atomic and non-atomic operations. Now, let's dive into an implementation
    of a lock-free data structure using atomic operations.
  prefs: []
  type: TYPE_NORMAL
- en: A lock-free data structure  queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we already know, the data in an actual system is often represented in the
    form of a data structure, and when it comes to concurrent operations on a data
    structure, performance is a big deal. In [Chapter 3](https://cdp.packtpub.com/c___reactive_programming/wp-admin/post.php?post=48&action=edit#post_40), *Language-Level
    Concurrency and Parallelism in C++*, we learned how to write a thread-safe stack.
    However, we used locks and condition variables to implement it. To explain how
    to write a lock-free data structure, let''s write a very basic queue system using
    a producer/consumer paradigm without using locks or condition variables. This
    will improve the performance of the code for sure. Rather than using a wrapper
    over a standard data type, we will roll it out from scratch. We have made an assumption
    that there is a single producer and a single consumer in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Lock_free_stack` class contains a structure to represent a queue node
    (named `Node`) with data members to represent the data of a node (`my_data`) and
    a pointer to the next node. Then, the class contains two instances of an atomic
    pointer to the user-defined structure `Node`, which is already defined inside
    the class. One instance stores the pointer to the head node of the queue, while
    the other points to the tail node. Finally, a `private pop_head_node()` function
    is used to retrieve the head node of the queue by calling an atomic *store* operation,
    but only if the queue contains at least one element. Here, the atomic operation
    follows the default sequentially consistent memory-ordering semantics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The head node is instantiated and the tail points to that memory when the queue
    object is constructed. The copy constructor and copy assignment operators are
    marked as deleted to prevent them from being used. Inside the destructor, all
    of the elements in the queue are deleted iteratively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet implements standard queue operations, which are Enqueue
    and Dequeue. Here, we have ensured that there is a *happens before* relationship
    between Enqueue and Dequeue using the swap and store atomic operations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed facilities provided by the standard library
    to write task-based parallelism. We saw how to use futures and promises with `std::packaged_task`
    and `std::async`. We discussed the new multi-threading-aware memory model that
    is available with the Modern C++ language. After that, we covered atomic types,
    and operations associated with them. The most important thing that we learned
    about are the various memory-ordering semantics of the language. In a nutshell,
    this particular chapter and the previous one will enable us to reason about the
    concurrency aspects of the reactive programming model.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will shift our attention from language and concurrency
    to the standard interface of the reactive programming model. We will be covering
    Observables!
  prefs: []
  type: TYPE_NORMAL
