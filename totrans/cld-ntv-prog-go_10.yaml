- en: Monitoring Your Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, you learned how to build a Microservice application
    with the Go programming language and how to (continuously) deploy it into various
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: However, our work is not yet complete. When you have an application running
    in a production environment, you will need to ensure that it stays up and running
    and behaves the way that you as a developer intended. This is what monitoring
    is for.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce you to **Prometheus**, an open source monitoring
    software that has quickly gained popularity for monitoring cloud-based distributed
    applications. It is often used together with **Grafana**, a frontend for visualizing
    metrics data collected by Prometheus. Both applications are licensed under the
    Apache license. You will learn how to set up Prometheus and Grafana and how to
    integrate them into your own applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing and using Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Grafana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exporting metrics to Prometheus from your own application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Prometheus and Grafana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before using Prometheus and Grafana in our own application, let's take a look
    at how Prometheus works in principle.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus's basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike other monitoring solutions, Prometheus works by pulling data (called
    **metrics** in Prometheus jargon) from clients at regular intervals. This process
    is called **scraping**. Clients monitored by Prometheus have to implement an HTTP
    endpoint that can be scraped by Prometheus in a regular interval (by default,
    1 minute). These metrics endpoints can then return application-specific metrics
    in a predefined format.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, an application could offer an HTTP endpoint at `/metrics` that
    responds to `GET` requests and returns the following body:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This document exposes two metrics—`memory_consumption_bytes` and `http_requests_count`.
    Each metric is associated with a value (for example, the current memory consumption
    of 6,168,432 bytes). Since Prometheus scrapes these metrics from your application
    at fixed intervals, it can use these point-in-time values to build a time series
    of this metric.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus metrics can also have labels. In the preceding example, you may note
    that the `http_request_count` metric actually has three different values for different
    combinations of the `path` and `method` labels. Later, you will be able to use
    these labels to query data from Prometheus using a custom query language, **PromQL**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics exported to Prometheus by applications can get quite complex. For example,
    using labels and different metrics names, a client could export a histogram where
    data is aggregated in different buckets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding metrics describe a histogram of your application's HTTP response
    times. In this case, 6,835 requests were processed with a response time of less
    than 0.1 seconds; 79,447 requests with a response time of less than 0.5 seconds
    (which includes the previous 6,835 requests); and so on. The last two metrics
    export the total amount of processed HTTP requests and the sum of time needed
    to process these requests. Both of these values together can be used to compute
    the average request duration.
  prefs: []
  type: TYPE_NORMAL
- en: Do not worry, you will not need to build these complex histogram metrics by
    yourself; that's what the Prometheus client library is for. However, first, let's
    get started by actually setting up a Prometheus instance.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an initial Prometheus configuration file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before using Prometheus and Grafana in our own application, we will need to
    set it up first. Luckily, you can find Docker images for both applications on
    the Docker Hub. Before starting our own Prometheus container, we just need to
    create a configuration file that we can then inject into the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by creating a new directory somewhere on your local machine and placing
    a new `prometheus.yml` file in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This configuration defines a global scraping interval of 15 seconds (the default
    is 1 minute) and already configures the first scraping target, which is Prometheus
    itself (yes, you have read correctly; Prometheus exports Prometheus metrics that
    you can then monitor with Prometheus).
  prefs: []
  type: TYPE_NORMAL
- en: Later, we will add more configuration items to the `scape_configs` property.
    For the time being, it will suffice.
  prefs: []
  type: TYPE_NORMAL
- en: Running Prometheus on Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After having created the configuration file, we can use a volume mount to inject
    this configuration file into the Docker containers we are about to start.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we will assume that you have the MyEvents application running
    in Docker containers on your local machine and that the containers are attached
    to a container network named `myevents` (whether you created the containers manually
    or via Docker Compose does not really matter).
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, starting both applications is easy enough. We''ll start by
    defining a separate container network for the monitoring components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a new volume in which the Prometheus server can store its data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can use both the newly created network and volume to create a Prometheus
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that in the preceding example, we are attaching the `prometheus` container
    to both the `myevents` and `monitoring` networks. This is because later, the Prometheus
    server will need to access the MyEvents service via the network to scrape metrics
    from them.
  prefs: []
  type: TYPE_NORMAL
- en: 'After starting the Prometheus container, you will be able to open the Prometheus
    web UI in your browser by navigating to [http://localhost:9090](http://localhost:9090/):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ad455d1-5604-4c92-9d13-6137d85c1a60.png)'
  prefs: []
  type: TYPE_IMG
- en: Prometheus web UI
  prefs: []
  type: TYPE_NORMAL
- en: 'In our configuration file, we have already configured our first scraping target—the
    Prometheus server itself. You will find an overview of all configured scraping
    targets by selecting the Status menu item and then the Targets item:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46a650c2-6df3-4475-bb76-3d038e7dd0fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Targets item in Prometheus web UI
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding screenshot, Prometheus reports the current state
    of the scrape target (UP, in this case) and when it was last scraped.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now use the Graph menu item to inspect the metrics that Prometheus
    has already collected about itself. There, enter `go_memstats_alloc_bytes` into
    the Expression input field and click on Execute. After that, switch to the Graph
    tab. Prometheus will now print its own memory usage over the past 1 hour. You
    can change the observation period using the controls above the graph. By default,
    Prometheus will keep its time series data for 2 weeks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c64c97ef-6549-4af0-807b-eea5ad923242.png)'
  prefs: []
  type: TYPE_IMG
- en: Prometheus web UI graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Prometheus also supports more complex expressions. For example, consider the `process_cpu_seconds_total` metric.
    When displaying it as a graph, you will note that it is monotonically increasing.
    This is because that specific metric describes the sum of all CPU seconds that
    the program used over its entire lifetime (which, by definition, must always be
    increasing). However, for monitoring purposes, it is often more interesting to
    know the current CPU usage of a process. For this, PromQL offers the `rate()`
    method that calculates the per-second average increase of a time series. Try this
    out using the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the graph view, you will now find the 1-minute average CPU usage per second
    (which is probably a more comprehensible metric than the total sum of all used
    CPU seconds ever):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae1cd1d5-19fd-4c3c-bf9c-5e27cd34d623.png)'
  prefs: []
  type: TYPE_IMG
- en: The Prometheus web UI is good for quick analyses and ad-hoc queries. However,
    Prometheus does not support saving queries for later use or presenting more than
    one graph on the same page. This is where Grafana comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Running Grafana on Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Running Grafana is equally as easy as running Prometheus. Start by setting
    up a volume for persistent storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, start the actual container and attach it to the `monitoring` network
    (not the `myevents` network; Grafana needs to communicate with the Prometheus
    server, but it will not have any need to communicate with your backend services
    directly):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: After this, you will be able to access Grafana in your browser on `http://localhost:3000`.
    The default credentials are the username `admin` and the password `admin`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e0ae6ea-2552-419b-99ea-0f89751f2a43.png)'
  prefs: []
  type: TYPE_IMG
- en: Gafana home page
  prefs: []
  type: TYPE_NORMAL
- en: On your first access, you will be prompted to configure a data source for your
    Grafana instance. Click on the Add data source button and configure access to
    your Prometheus server on the next page. There, select Prometheusas *Type*, enter
    `http://prometheus:9090` as URL, and select Proxy *as* Access mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'After adding your Data Source, continue by creating a dashboard (select the
    Button in the top-left corner, select Dashboards, and then New). Then, add a new
    Graph to the dashboard by clicking on the respective button. After adding the
    graph panel, edit the panel by clicking on the Panel Title and selecting Edit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92a7aafc-85bf-4350-9aeb-47c78c673dec.png)'
  prefs: []
  type: TYPE_IMG
- en: Panel
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in the Metrics tab, enter the CPU usage query from before into the Query
    input field. To further customize the panel, you might want to enter `{{ job }}`
    as a legend to make the graph legend more comprehensible and change the Y axis
    format (in the Axes tab, Left Y section, and Unit field) to Percent (0.0-1.0):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b35c2ab-936f-4eb0-b813-6503c27ca6d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Gafana new dashboard
  prefs: []
  type: TYPE_NORMAL
- en: Close the editing panel and save your dashboard by clicking on the Save button
    or pressing *Ctrl* + *S*. Your dashboard is now saved. You can view it again at
    a later point in time with updated metrics or share this dashboard with other
    users.
  prefs: []
  type: TYPE_NORMAL
- en: You can also experiment by adding more panels to your dashboard in which you
    visualize other metrics (by default, Prometheus already exports a boatload of
    metrics about itself that you can experiment with). For a detailed reference on
    the Prometheus Query Language, you can also take a look at the official documentation
    at the following URL: [https://prometheus.io/docs/querying/basics/](https://prometheus.io/docs/querying/basics/).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a working Prometheus and Grafana setup up and running, we can
    take a look at how to get metrics from your own application into Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As already shown, exporting metrics from your own application is easy, at least
    in principle. All your application needs to do is offer an HTTP endpoint that
    returns arbitrary metrics that can then be saved in Prometheus. In practice, this
    gets more difficult, especially when you care about the status of the Go runtime
    (for example, CPU and memory usage, Goroutine count, and so on). For this reason,
    it is usually a good idea to use the Prometheus client library for Go, which takes
    care of collecting all possible Go runtime metrics.
  prefs: []
  type: TYPE_NORMAL
- en: As a matter of fact, Prometheus is itself written in Go and also uses its own
    client library to export metrics about the Go runtime (for example, the `go_memstats_alloc_bytes`
    or `process_cpu_seconds_total` metrics that you have worked with before).
  prefs: []
  type: TYPE_NORMAL
- en: Using the Prometheus client in your Go application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can get the Prometheus client library using `go get`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In case your application uses a dependency management tool (such as Glide,
    which we introduced in the preceding chapter), you will also probably want to
    declare this new dependency in your `glide.yaml` file and add a stable release
    to your application''s `vendor/` directory. To do all this in one step, simply
    run `glide get` instead of `go get` within your application directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For security reasons, we will expose our metrics API on a different TCP port
    than the event service's and booking service's REST APIs. Otherwise, it would
    be too easy to accidentally expose the metrics API to the outside world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the event service. Setting up the metrics APIs does not require
    much code, so we will do this directly in the `main.go` file. Add the following
    code to the main function before the `rest.ServeAPI` method is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, compile your application and run it. Try opening the address at `http://localhost:9100/metrics`
    in your browser, and you should see a large number of metrics being returned by
    the new endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecb16ed5-5998-4cd1-a67a-0f501e2c0952.png)'
  prefs: []
  type: TYPE_IMG
- en: Page shown at localhost:9100/metrics
  prefs: []
  type: TYPE_NORMAL
- en: Now, make the same adjustment to the booking service. Also, remember to add
    an `EXPOSE 9100` statement to both service's Dockerfiles and to recreate any containers
    with an updated image and the `-p 9100:9100` flag (or `-p 9101:9100` to prevent
    port conflicts).
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Prometheus scrape targets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have two services up and running that expose Prometheus metrics,
    we can configure Prometheus to scrape these services. For this, we can modify
    the `prometheus.yml` file that you created earlier. Add the following sections
    to the `scrape_configs` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After adding the new scraping targets, restart the Prometheus container by
    running `docker container restart prometheus`. After that, the two new scraping
    targets should show up in the Prometheus web UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c508298-984f-4e2a-8280-ce15c5b80974.png)'
  prefs: []
  type: TYPE_IMG
- en: Prometheus web UI targets
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for the best part—remember the Grafana dashboard that you have created
    a few sections earlier? Now that you have added two new services to be scraped
    by Prometheus, take another look at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbf352c1-0262-4a68-a2ef-12daf019ee5e.png)'
  prefs: []
  type: TYPE_IMG
- en: Gafana
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, Grafana and Prometheus pick up metrics from the new services
    instantly. This is because the `process_cpu_seconds_total` and `go_memstats_alloc_bytes`
    metrics that we have worked with until now are actually exported by all three
    of our services since they're all using the Prometheus Go client library. However,
    Prometheus adds an additional job label to each metrics that is scraped; this
    allows Prometheus and Grafana to distinguish the same metrics coming from different
    scraping targets and present them accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting custom metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, you can also use the Prometheus client library to export your own
    metrics. These do not need to be technical metrics that reflect some aspect of
    the Go runtime (such as CPU usage and memory allocation), but it could also be
    business metrics. One possible example could be the amount of booked tickets with
    different labels per event.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, within the `todo.com/myevents/bookingservice/rest` package, you
    could add a new file—let''s call it `metrics.go`*—*that declares and registers
    a new Prometheus metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The Prometheus client library tracks all created metric objects in a package,
    a global registry that is automatically initialized. By calling the `prometheus.MustRegister`
    function, you can add new metrics to this registry. All registered metrics will
    automatically be exposed when a Prometheus server scrapes the `/metrics` endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `NewCounterVec` function used creates a collection of metrics that are
    all named `myevents_bookings_count` but are differentiated by two labels, `eventID`
    and `eventName` (in reality, these are functionally dependent and you wouldn''t
    really need both; but having the event name as a label comes in handy when visualizing
    this metric in Grafana). When scraped, these metrics might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The Prometheus client library knows different types of metrics. The Counter
    that we used in the preceding code is one of the simpler ones. In one of the previous
    sections, you saw how a complex histogram was represented as a number of different
    metrics. This is also possible with the Prometheus client library. Just to demonstrate,
    let''s add another metric—this time, a histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When being scraped, this histogram will be exported as seven individual metrics:
    you will get five histogram buckets (*Number of bookings with one seat or less* up
    to *Four seats or less* and *Infinitely many seats or less*), and one metric for
    the sum of all seats and sum of all observations, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Of course, we will need to tell the Prometheus library the values that should
    be exported for these metrics when scraped by the Prometheus server. Since both
    metrics (amount of bookings and amount of seats per booking) can only change when
    a new booking is made, we can add this code to the REST handler function that
    handles POST requests on the `/events/{id}/bookings` route.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `booking_create.go` file, add the following code somewhere after the
    original request has been processed (for example, after the `EventBooked` event
    is emitted on the event emitter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The first statement adds the amount of booked seats (`request.Seats`) to the
    counter metric. Since you defined one label named `event` in the `CounterVec`
    declaration, you will need to call the `WithLabelValues` method with the respective
    label values (if the metric declaration consisted of two labels, you would need
    to pass two parameters into `WithLabelValues`).
  prefs: []
  type: TYPE_NORMAL
- en: The second statement adds a new `observation` to the histogram. It will automatically
    find the correct bucket and increment it by one (for example if three seats are
    added with the same booking, the `myevents_seats_per_booking_bucket{le="3"}` metric
    will be increased by one).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, start your application and make sure that Prometheus is scraping it at
    regular intervals. Take the time and add a few example records to your application.
    Also, add a few event bookings at the booking service; ensure that you do not
    create them all at once. After that, you can use the `myevents_bookings_count`
    metric to create a new graph in your Grafana dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '>![](img/a6a1c84f-56d9-4b27-9cab-92b427f8b42a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Gafana graph
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Prometheus will create one time series per scraped instance. This
    means that when you have multiple instances of the booking service, you will get
    multiple time series, each with a different `job` label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'When displaying a business metric (for example, the number of tickets sold),
    you may not actually care at which instance each particular booking was placed
    and prefer an aggregated time series over all instances, instead. For this, you
    can use the PromQL function `sum()` when building your dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Running Prometheus on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have configured all scraping targets for Prometheus manually
    by adding them to the `prometheus.yml` configuration file. This works well for
    testing, but becomes tedious quickly in larger production setups (and completely
    pointless as soon as you introduce feature such as autoscaling).
  prefs: []
  type: TYPE_NORMAL
- en: When running your application within a Kubernetes cluster, Prometheus offers
    a turn-key solution for this—using the `prometheus.yml` configuration file, you
    can actually configure Prometheus to automatically load its scraping targets from
    the Kubernetes API. For example, if you have a Deployment defined for your booking
    service, Prometheus can automatically find all Pods that are managed by this Deployment
    and scrape them all. If the Deployment is scaled up, the additional instances
    will be automatically added to Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the following examples, we will assume that you have either a Minikube
    VM running on your local machine or a Kubernetes cluster somewhere in a cloud
    environment. We''ll start by deploying the Prometheus server first. To manage
    the Prometheus configuration file, we will be using a Kubernetes resource that
    we have not used before—a `ConfigMap`. A `ConfigMap` is basically just an arbitrary
    key-value map that you can save in Kubernetes. When creating a Pod (or Deployment
    or StatefulSet), you can mount these values into your container as files, which
    makes `ConfigMaps` ideal for managing configuration files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: You can create the `ConfigMap` just like any other resource by saving it to
    a `.yaml` file and then calling `kubectl apply -f` on that file. You can also
    use the same command to update the `ConfigMap` when you have modified the `.yaml`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the `ConfigMap` created, let''s deploy the actual Prometheus server. Since
    Prometheus is a stateful application, we will deploy it as a `StatefulSet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, create the associated `Service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now, you have a Prometheus server running inside your Kubernetes cluster; however,
    at the moment, that server only scrapes its own metrics endpoint, and not yet
    any of the other pods running in your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable the automatic scraping of Pods, add the following section to the
    `scrape_configs` section of your `prometheus.yml` file in your `ConfigMap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Yes, this is quite a lot of configuration, but do not panic. Most of these configurations
    is for mapping the properties of known Kubernetes pods (such as the Pod names
    and labels defined by users) to Prometheus labels that will be attached to all
    metrics that are scraped from these Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that after updating the `ConfigMap`, you may need to destroy your Prometheus
    Pod for the updated configuration to become active. Do not worry; even if you
    delete the Pod, the `StatefulSet` controller will create a new one almost immediately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This configuration also defines that Prometheus will scrape all Pods found
    in the cluster that have an annotation named `prometheus.io/scrape`. This annotation
    can be set when defining a Pod template, for example, in a Deployment. Also, you
    can now adjust your event service deployment as follows (remember to add the TCP
    port `9100` to the list of exposed ports):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: After updating the Deployment, Kubernetes should automatically start recreating
    the event service Pods. As soon as new Pods with the `prometheus.io/scrape` annotation
    are created, Prometheus will automatically pick them up and scrape them for metrics.
    If they are deleted again (for example, after updating or downscaling a Deployment),
    Prometheus will keep the metrics collected from these pods, but stop scraping
    them.
  prefs: []
  type: TYPE_NORMAL
- en: By having Prometheus pick up new scraping targets automatically based on annotations,
    managing the Prometheus server becomes very easy; after the initial setup, you
    probably will not need to edit the configuration file again.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use Prometheus and Grafana to set up a monitoring
    stack to monitor both your application's health on a technical level (by keeping
    an eye on system metrics, such as RAM and CPU usage) and custom, application-specific
    metrics, such as, in this case, the amount of booked tickets.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of this book, we have covered almost the entire lifecycle of
    a typical Go cloud application, starting at architecture and the actual programming,
    building container images, continuously deploying them in various cloud environments,
    and monitoring your applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will take the opportunity to look back in detail
    at what we have achieved so far and also point out where to go from here.
  prefs: []
  type: TYPE_NORMAL
