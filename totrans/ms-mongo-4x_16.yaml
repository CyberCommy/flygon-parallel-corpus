- en: Replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Replication has been one of the most useful features of MongoDB since the very
    early days. In general, replication refers to the process of synchronizing data
    across different servers. The benefits of replication include protection from
    data loss and high availability of data. Replication also provides disaster recovery,
    an avoidance of downtime for maintenance, scaling reads (since we can read from
    multiple servers), and scaling writes (only if we can write to multiple servers).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An architectural overview, elections, and the use cases for replication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a replica set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting to a replica set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replica set administration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best practices for deploying replica sets using cloud providers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replica set limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are different approaches to replication. The approach that MongoDB takes
    is logical replication with a master-slave, which we will explain in more detail
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Logical or physical replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With replication, we synchronize data across multiple servers, providing data
    availability and redundancy. Even if we lose a server due to a hardware or software
    failure, by using replication, we will have multiple copies that we can use to
    restore our data. Another advantage of replication is that we can use one of the
    servers as a dedicated reporting, or backup, server.
  prefs: []
  type: TYPE_NORMAL
- en: In logical replication, we have our master/primary server performing operations; the
    slave/secondary server tails a queue of operations from the master and applies
    the same operations in the same order. Using of MongoDB as an example, the **operations
    log** (**oplog**) keeps track of operations that have happened on the primary
    server and applies them in the exact same order on the secondary server.
  prefs: []
  type: TYPE_NORMAL
- en: Logical replication is useful for a wide array of applications, such as information
    sharing, data analysis, and **Online Analytical Processing** (**OLAP**) reporting.
  prefs: []
  type: TYPE_NORMAL
- en: In physical replication, data gets copied on the physical level, at a lower
    level than database operations. This means that we are not applying the operations,
    but copying the bytes that were affected by these operations. It also means that
    we can gain better efficiency, since we are using low-level structures to transfer
    data. We can also ensure that the state of the database is exactly the same, since
    they are identical, byte for byte.
  prefs: []
  type: TYPE_NORMAL
- en: What is typically missing from physical replication is knowledge about the database
    structure, which means that it is harder (if not impossible) to copy some collections
    from a database and ignore others.
  prefs: []
  type: TYPE_NORMAL
- en: Physical replication is typically suited for more rare circumstances, like disaster
    recovery, wherein a full and exact copy of everything (including data, indexes,
    the internal state of the database in a journal, and redoing/undoing logs) is
    of crucial importance to bringing the application back to the exact state it was
    in.
  prefs: []
  type: TYPE_NORMAL
- en: Different high availability types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In high availability, there are several configurations that we can use. Our
    primary server is called the **hot server**, as it can process each and every
    request coming in. Our secondary server can be in any of the following states:'
  prefs: []
  type: TYPE_NORMAL
- en: Cold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **secondary cold server** is a server that is there just in case the primary
    server goes offline, without any expectation of it holding the data and state
    that the primary server had.
  prefs: []
  type: TYPE_NORMAL
- en: A **secondary warm server** receives periodic updates of data from the primary
    server, but typically, it is not entirely up to date with the primary server.
    It can be used for some non-real-time analytics reporting to offload the main
    server, but typically, it will not be able to pick up the transactional load of
    the primary server if it goes down.
  prefs: []
  type: TYPE_NORMAL
- en: A **secondary hot server** always keeps an up-to-date copy of the data and state
    from the primary server. It usually waits in a hot standby state, ready to take
    over when the primary server goes down.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB has both the hot and warm server types of functionality, as we will
    explore in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Most database systems employ a similar notion of primary/secondary servers,
    so conceptually, everything from MongoDB gets applied there, too.
  prefs: []
  type: TYPE_NORMAL
- en: An architectural overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MongoDB''s replication is provided in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a5ebcaa-a723-4afe-9e23-054f1d663936.png)'
  prefs: []
  type: TYPE_IMG
- en: The primary server is the only one that can take writes at any time. The secondary
    servers are in a hot standby state, ready to take over if the primary server fails.
    Once the primary server fails, an election takes place regarding which secondary
    server will become primary.
  prefs: []
  type: TYPE_NORMAL
- en: We can also have **arbiter nodes**. Arbiter nodes do not hold any data, and
    their sole purpose is to participate in the election process.
  prefs: []
  type: TYPE_NORMAL
- en: We must always have an odd number of nodes (including arbiters). Three, five,
    and seven are all fine, so that in the event of the primary (or more servers)
    failing, we have a majority of votes in the election process.
  prefs: []
  type: TYPE_NORMAL
- en: When the other members of a replica set don't hear from the primary for more
    than 10 seconds (configurable), an eligible secondary will start the election
    process to vote for a new primary. The first secondary to hold the election and
    win the majority will become the new primary. All remaining servers will now replicate
    from the new primary server, keeping their roles as secondaries but syncing up
    from the new primary.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with MongoDB 3.6, client drivers can retry write operations a **single
    time **if they detect that the primary is down. A replica set can have up to 50
    members, but only up to seven of them can vote in the election process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The setup for our replica set after the new election will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66b01e77-7539-4d79-a218-813ea11b5d05.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will discuss how elections work.
  prefs: []
  type: TYPE_NORMAL
- en: How do elections work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the servers in a replica set maintain regular communication with every
    other member via a heartbeat. The heartbeat is a small packet that's regularly
    sent to verify that all members are operating normally.
  prefs: []
  type: TYPE_NORMAL
- en: Secondary members also communicate with the primary to get the latest updates
    from the oplog and apply them to their own data.
  prefs: []
  type: TYPE_NORMAL
- en: The information here refers to the latest replication election protocol, version
    1, which was introduced in MongoDB v3.2.
  prefs: []
  type: TYPE_NORMAL
- en: Schematically, we can see how this works.
  prefs: []
  type: TYPE_NORMAL
- en: When the primary member goes down, all of the secondaries will miss a heartbeat
    or more. They will be waiting up until the `settings.electionTimeoutMillis` time
    passes (the default is 10 seconds), and then the secondaries will start one or
    more rounds of elections to find the new primary.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a server to be elected as primary from the secondaries, it must have two
    properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Belong in a group of voters that have *50% + 1* of the votes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be the most up-to-date secondary in this group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a simple example of three servers with one vote each, once we lose the primary,
    the other two servers will each have one vote (so, in total, two-thirds), and
    as such, the one with the most up-to-date oplog will be elected as primary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, consider a more complex setup, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Seven servers (one primary, six secondaries)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One vote each
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We lose the primary server, and the six remaining servers have network connectivity
    issues, resulting in a network partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d794932d-b88f-4c1f-8bd7-be491ae4be45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These partitions can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Partition North: Three servers (one vote each)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Partition South: Three servers (one vote each)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neither partition has any knowledge of what happened to the rest of the servers.
    Now, when they hold elections, no partition can establish a majority, as they
    have three out of seven votes. No primary will get elected from either partition.
    This problem can be overcome by having, for example, one server with three votes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, our overall cluster setup looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Server #1**: one vote'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server #2**: one vote'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server #3**: one vote'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server #4**: one vote'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server #5**: one vote'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server #6**: one vote'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server #7**: three votes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After losing Server #1, our partitions now look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7841ea05-e5c9-411c-9a9d-b66cbaf99b5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Partition North is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Server #2**: One vote'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server #3**: One vote'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server #4**: One vote'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Partition South is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Server #5**: One vote'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server #6**: One vote'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server #7**: Three votes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Partition South has three servers, with a total of five out of nine votes.
    The secondary among servers #5, #6, and #7 that is most up to date (according
    to its oplog entries) will be elected as the primary.'
  prefs: []
  type: TYPE_NORMAL
- en: What is the use case for a replica set?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MongoDB offers most of the advantages of using a replica set, some of which
    are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Protection from data loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High availability of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disaster recovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoidance of downtime for maintenance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling reads, since we can read from multiple servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helping to design for geographically dispersed services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most notable item that's missing from the list is scaling writes. This is
    because, in MongoDB, we can only have one primary, and only this primary can take
    writes from our application server.
  prefs: []
  type: TYPE_NORMAL
- en: When we want to scale write performance, we typically design and implement sharding,
    which will be the topic of the next chapter. Two interesting properties of the
    way that MongoDB replication is implemented are geographically dispersed services
    and data privacy.
  prefs: []
  type: TYPE_NORMAL
- en: It is not uncommon for our application servers to be located in multiple data
    centers across the globe. Using replication, we can have a secondary server as
    close to the application server as possible. What this means is that our reads
    will be fast, as if they were local, and we will get a latency performance penalty
    just for our writes. This requires some planning at the application level, of
    course, so that we can maintain two different pools of connections to our database,
    which can be easily done by either using the official MongoDB drivers or using
    higher-level ODMs.
  prefs: []
  type: TYPE_NORMAL
- en: The second interesting property of MongoDB's replication design is implementing
    data privacy. When we have servers geographically dispersed across different data
    centers, we can enable replication per database. By keeping a database out of
    the replication process, we can make sure that our data stays confined in the
    data center that we need. We can also set up different replication schemas per
    database in the same MongoDB server so that we have multiple replication strategies
    according to our data privacy needs, excluding some servers from our replica sets
    if they are not allowed by our data privacy regulations.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a replica set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will go over the most common deployment procedures to set
    up a replica set. These involve either converting a standalone server into a replica
    set or setting up a replica set from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Converting a standalone server into a replica set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To convert a standalone server into a replica set, we first need to cleanly
    shut down the `mongo` server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we start the server with the `--replSet` configuration option via the
    command line (which we will do here), or by using a configuration file, as we
    will explain in the next section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we connect (via the mongo shell) to the new replica set enabled instance,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have the first server of our replica set. We can add the other servers
    (which must have also been started with `--replSet`) by using the mongo shell,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Double-check the replica set configuration by using `rs.conf()`. Verify the
    replica set status by using `rs.status()`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a replica set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Starting a MongoDB server as a part of a replica set is as easy as setting
    it in the configuration via the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This is fine for development purposes. For production environments, it''s recommended
    that we use a configuration file instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `<path-to-config>` can be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This configuration file has to be in a YAML format.
  prefs: []
  type: TYPE_NORMAL
- en: YAML does not support tabs. Convert tabs to spaces by using your editor of choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple configuration file sample is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Root-level options define the sections that leaf-level options apply to by nesting.
    Regarding replication, the mandatory options are `oplogSizeMB` (the oplog size
    for the member, in MB) and `replSetName` (the replica set name, such as `xmr_cluster`).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also set the following on the same level as `replSetName`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This is only available for the MMAPv1 storage engine, and it refers to the indexes
    on secondaries that will get loaded into memory before applying operations from
    the oplog.
  prefs: []
  type: TYPE_NORMAL
- en: 'It defaults to `all`, and the available options are `none` and `_id_only`,
    in order to load no indexes into memory and only load the default index that was created
    on `_id` fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is the configuration setting for enabling the read preference of `majority`
    for this member.
  prefs: []
  type: TYPE_NORMAL
- en: After we have started all of the replica set processes on different nodes, we
    log in to one of the nodes using `mongo` from the command line with the appropriate
    `host:port`. Then, we need to initiate the cluster from one member.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use configuration files, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, we can pass in the configurations as a document parameter, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can verify that the cluster was initiated by using `rs.conf()` in the shell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following that, we add each other member to our replica set by using the `host:port` that
    we defined in our networking setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The minimum number of servers that we must use for an HA replica set is `3`.
    We could replace one of the servers with an arbiter, but this is not recommended.
    Once we have added all of the servers and have waited a bit, we can check the
    status of our cluster by using `rs.status()`. By default, the oplog will be 5%
    of the free disk space. If we want to define it when we create our replica set,
    we can do so by passing the command-line parameter `--oplogSizeMB` or `replication.oplogSizeMB`
    in our configuration file. An oplog size cannot be more than 50 GB.
  prefs: []
  type: TYPE_NORMAL
- en: Read preference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, all writes and reads go/come from the primary server. Secondary
    servers replicate data, but are not used for querying.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, it may be beneficial to change this and start to take reads from
    secondaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The MongoDB official drivers support five levels of read preference:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Read Preference Mode** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `primary` | This is the default mode, where reads come from the `primary` server
    of the replica set. |'
  prefs: []
  type: TYPE_TB
- en: '| `primaryPreferred` | With this mode, applications will read from the `primary`,
    unless it is unavailable, in which case reads will come from `secondary` members.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `secondary` | Reads come exclusively from `secondary` servers. |'
  prefs: []
  type: TYPE_TB
- en: '| `secondaryPreferred` | With this mode, applications will read from `secondary`
    members, unless they are unavailable, in which case reads will come from the `primary` member.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nearest` | Applications will read from the member of the replica set that
    is `nearest` in terms of network latency, not taking into account the member''s
    type. |'
  prefs: []
  type: TYPE_TB
- en: Using any read preference other than `primary` can be beneficial for asynchronous
    operations that are not extremely time-sensitive. For example, reporting servers
    can take reads from secondaries instead of the primary, as we may be fine with
    a small delay in our aggregation data, with the benefit of incurring more read
    load on our primary server.
  prefs: []
  type: TYPE_NORMAL
- en: Geographically distributed applications will also benefit from reading from
    secondaries, as these will have significantly lower latency. Although it's probably
    counter-intuitive, just changing the read preference from `primary` to `secondary`
    will not significantly increase the total read capacity of our cluster. This is
    because all of the members of our cluster are taking the same write load from
    clients' writes, and replication for the primary and secondaries, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, however, reading from a secondary may return stale data, which
    has to be dealt with at the application level. Reading from different secondaries
    that may have variable replication lag (compared to our primary writes) may result
    in reading documents out of their insertion order (**non-monotonic reads**).
  prefs: []
  type: TYPE_NORMAL
- en: With all of the preceding caveats, it is still a good idea to test reading from
    secondaries if our application design supports it. An additional configuration
    option that can help us to avoid reading stale data is `maxStalenessSeconds`.
  prefs: []
  type: TYPE_NORMAL
- en: Based on a coarse estimation from each secondary as to how far behind the primary
    it is, we can set this to a value of 90 (seconds) or more to avoid reading stale
    data. Given that secondaries know how far behind they are from the primary (but
    don't accurately or aggressively estimate it), this should be treated as an approximation,
    rather than something we base our design o.
  prefs: []
  type: TYPE_NORMAL
- en: Write concern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By default, the write operations in MongoDB replica sets will be acknowledged
    once the write has been acknowledged by the primary server. If we want to change
    this behavior, we can do so in two different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can request a different write concern per operation, in cases where we want
    to make sure that a write has propagated to multiple members of our replica set
    before marking it as complete, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we are waiting for the write to be confirmed by two
    servers (the primary, plus any one of the secondaries). We are also setting a
    timeout of `5000` milliseconds to avoid our write from blocking in cases where
    the network is slow or we just don't have enough servers to acknowledge the request.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also change the default write concern across the entire replica set,
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, we set the write concern to `majority` with a timeout of `5` seconds.
    The write concern `majority` makes sure that our writes will propagate to at least
    *n/2+1* servers, where *n* is the number of our replica set members.
  prefs: []
  type: TYPE_NORMAL
- en: 'The write concern `majority` is useful if we have a read preference of `majority` as
    well, as it ensures that every write with `w: "majority"` will also be visible
    with the same read preference. If we set `w>1`, it''s useful to also set `wtimeout:
    <milliseconds>` with it. `wtimeout` will return from our write operation once
    the timeout has been reached, thus not blocking our client for an indefinite period
    of time. It''s recommended to set `j: true`, as well. `j: true` will wait for
    our write operation to be written to the journal before acknowledging it. `w>1`,
    along with `j: true`, will wait for the number of servers that we have specified
    to write to the journal before acknowledgement.'
  prefs: []
  type: TYPE_NORMAL
- en: Custom write concerns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also identify our replica set members with different tags (that is,
    `reporting`, East Coast Servers, and HQ servers) and specify a custom write concern
    per operation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the usual procedure for connecting to the primary via the mongo shell,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now set a custom write concern, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying this, we use the `reconfig` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now start by setting `writeConcern` in our writes, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This means that our write will only be acknowledged if the `UKWrites` write
    concern is satisfied, which, in turn, will be satisfied by at least two servers,
    with the tag `location_uk` verifying it. Since we only have two servers located
    in the UK, we can make sure that with this custom write concern, we have written
    our data to all of our UK-based servers.
  prefs: []
  type: TYPE_NORMAL
- en: Priority settings for replica set members
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MongoDB allows us to set different priority levels for each member. This allows
    for some interesting applications and topologies to be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 'To change the priority after we have set up our cluster, we have to connect
    to our primary using the mongo shell and get the configuration object (in this
    case, `cfg`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can change the `members` sub-document `priority` attribute to the
    value of our choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The default `priority` is `1` for every member. The priority can be set from
    `0` (never become a primary) to `1000`, in floating-point precision.
  prefs: []
  type: TYPE_NORMAL
- en: Higher priority members will be the first to call an election when the primary
    steps down, and are also the most likely to win the election.
  prefs: []
  type: TYPE_NORMAL
- en: Custom priorities should be configured with consideration of the different network
    partitions. Setting priorities the wrong way may lead to elections not being able
    to elect a primary, thus stopping all writes to our MongoDB replica set.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to prevent a secondary from becoming a primary, we can set its priority
    to `0`, as we will explain in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Zero priority replica set members
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some cases (for example, if we have multiple data centers), we will want
    some of the members to never be able to become a primary server.
  prefs: []
  type: TYPE_NORMAL
- en: In a scenario with multiple data center replications, we may have our primary
    data center with one primary and one secondary based in the UK, and a secondary
    server located in Russia. In this case, we don't want our Russia-based server
    to become primary, as it would incur latency on our application servers based
    in the UK. In this case, we will set up our Russia-based server with `priority`
    as `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Replica set members with `priority` as `0` also can''t trigger elections. In
    all other aspects, they are identical to every other member in the replica set.
    To change the `priority` of a replica set member, we must first get the current
    replica set configuration by connecting (via the mongo shell) to the primary server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This will provide the config document that contains the configuration for every
    member in our replica set. In the `members` sub-document, we can find the `priority`
    attribute, which we have to set to `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to reconfigure the replica set with the updated configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Make sure that you have the same version of MongoDB running in every node, otherwise
    there may be unexpected behavior. Avoid reconfiguring the replica set cluster
    during high volume periods. Reconfiguring a replica set may force an election
    for a new primary, which will close all active connections, and may lead to a
    downtime of 10-30 seconds. Try to identify the lowest traffic time window to run
    maintenance operations like reconfiguration, and always have a recovery plan in
    case something goes wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden replica set members
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hidden replica set members are used for special tasks. They are invisible to
    clients, will not show up in the `db.isMaster()` mongo shell command and similar
    administrative commands, and, for all purposes, will not be taken into account
    by clients (that is, read preference options).
  prefs: []
  type: TYPE_NORMAL
- en: They can vote for elections, but will never become a primary server. A hidden
    replica set member will only sync up to the primary server, and doesn't take reads
    from the clients. As such, it has the same write load as the primary server (for
    replication purposes), but no read load on its own.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the previously mentioned characteristics, reporting is the most common
    application of a hidden member. We can connect directly to this member and use
    it as the data source of truth for OLAP.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up a hidden replica set member, we follow a similar procedure to `priority`
    to `0`. After we have connected to our primary via the mongo shell, we get the
    configuration object, identify the member in the members sub-document that corresponds
    to the member we want to set as `hidden`, and subsequently set its `priority`
    to `0` and its `hidden` attribute to `true`. Finally, we have to apply the new
    configuration by calling `rs.reconfig(config_object)` with `config_object` that
    we used as a parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: A `hidden` replica set member can also be used for backup purposes. However,
    as you will see in the next section, we may want to use other options, either
    at the physical level or to replicate data at the logical level. In those cases,
    consider using a delayed replica set instead.
  prefs: []
  type: TYPE_NORMAL
- en: Delayed replica set members
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many cases, we will want to have a node that holds a copy of our data at
    an earlier point in time. This helps to recover from a big subset of human errors,
    like accidentally dropping a collection, or an upgrade going horrendously wrong.
  prefs: []
  type: TYPE_NORMAL
- en: A delayed replica set member has to be `priority = 0` and `hidden = true`. A
    delayed replica set member can vote for elections, but will never be visible to
    clients (`hidden = true`) and will never become a primary (`priority = 0`).
  prefs: []
  type: TYPE_NORMAL
- en: 'An example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This will set the `members[0]` to a delay of 2 hours. Two important factors
    for deciding the delta time period between the primary and delayed secondary server
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Enough oplog size in the primary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enough time for the maintenance to finish before the delayed member starts picking
    up data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table shows the delay of the replica set in hours:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Maintenance window, in hours** | **Delay** | **Oplog size on primary, in
    hours** |'
  prefs: []
  type: TYPE_TB
- en: '| *0.5* | *[0.5,5)* | *5* |'
  prefs: []
  type: TYPE_TB
- en: Production considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploy each `mongod` instance on a separate physical host. If you are using
    VMs, make sure that they map to different underlying physical hosts. Use the `bind_ip`
    option to make sure that your server maps to a specific network interface and
    port address.
  prefs: []
  type: TYPE_NORMAL
- en: Use firewalls to block access to any other port and/or only allow access between
    application servers and MongoDB servers. Even better, set up a VPN so that your
    servers communicate with each other in a secure, encrypted fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to a replica set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Connecting to a replica set is not fundamentally different from connecting
    to a single server. In this section, we will show some examples that use the official
    `mongo-ruby-driver`. We will use the following steps for the replica set, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to set our `host` and `options` objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we are getting ready to connect to `hostname:port`,
    in the database signals in `replica_set xmr_btc`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calling the initializer on `Mongo::Client` will now return a `client` object
    that contains a connection to our replica set and database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `client` object has the same options it has when connecting to a single
    server.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB uses auto-discovery after connecting to our `client_host` to identify
    the other members of our replica set, regardless of whether they are the primary
    or secondaries. The `client` object should be used as a singleton, created once
    and reused across our code base.
  prefs: []
  type: TYPE_NORMAL
- en: Having a singleton `client` object is a rule that can be overridden in some
    cases. We should create different `client` objects if we have different classes
    of connections to our replica set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'An example would be having a `client` object for most operations, and then
    another `client` object for operations that are fine with only reading from secondaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This Ruby MongoDB `client` command will return a copy of the `MongoDB:Client`
    object with a read preference secondary that can be used, for example, for reporting
    purposes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some of the most useful options that we can use in our `client_options` initialization
    object are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Option** | **Description** | **Type** | **Default** |'
  prefs: []
  type: TYPE_TB
- en: '| `replica_set` | As used in our example: the replica set name. | String |
    None |'
  prefs: []
  type: TYPE_TB
- en: '| `write` | The `write` concern options as a `hash` object; the available options
    are `w`, `wtimeout`, `j`, and `fsync`.That is, to specify writes to two servers,
    with journaling, flushing to disk (`fsync`) `true`, and a timeout of `1` second:`{
    write: { w: 2, j: true, wtimeout: 1000, fsync: true } }` | Hash | `{ w: 1 }` |'
  prefs: []
  type: TYPE_TB
- en: '| `read` | The read preference mode as a hash. Available options are `mode`
    and `tag_sets`.That is, to limit reads from secondary servers that have tag `UKWrites`:`{
    read:` ` { mode: :secondary,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`   tag_sets: [ "UKWrites" ]`'
  prefs: []
  type: TYPE_NORMAL
- en: '` }`'
  prefs: []
  type: TYPE_NORMAL
- en: '`}` | Hash | `{ mode: primary }` |'
  prefs: []
  type: TYPE_NORMAL
- en: '| `user` | The name of the user to authenticate with. | String | None |'
  prefs: []
  type: TYPE_TB
- en: '| `password` | The password of the user to authenticate with. | String | None
    |'
  prefs: []
  type: TYPE_TB
- en: '| `connect` | Using `:direct`, we can force treat a replica set member as a
    standalone server, bypassing auto-discovery.Other options include: `:direct`,
    `:replica_set`, and `:sharded`. | Symbol | None |'
  prefs: []
  type: TYPE_TB
- en: '| `heartbeat_frequency` | How often replica set members will communicate to
    check whether they are all alive. | Float | `10` |'
  prefs: []
  type: TYPE_TB
- en: '| `database` | Database connection. | String | `admin` |'
  prefs: []
  type: TYPE_TB
- en: Similar to connecting to a standalone server, there are also options for SSL
    and authentication that are used in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also configure the connection pool by setting the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The MongoDB driver will try to reuse existing connections, if available, or
    it will open a new connection. Once the pool limit has been reached, the driver
    will block, waiting for a connection to be released to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Replica set administration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The administration of a replica set can be significantly more complex than what
    is needed for single-server deployments. In this section, instead of trying to
    exhaustively cover all of the different cases, we will focus on some of the most
    common administrative tasks that we will have to perform, and how to do them.
  prefs: []
  type: TYPE_NORMAL
- en: How to perform maintenance on replica sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we have some maintenance tasks that we have to perform in every member in
    a replica set, we always start with the secondaries. We perform maintenance by
    performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we connect to one of the secondaries via the mongo shell. Then, we stop
    that secondary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, using the same user that was connected to the mongo shell in the previous
    step, we restart the mongo server as a standalone server in a different port:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to connect to this `mongod` server (which is using `dbpath`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we can safely perform all of the administrative tasks on our
    standalone server without affecting our replica set operations. When we are done,
    we shut down the standalone server in the same way that we did in the first step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can then restart our server in the replica set by using the command line
    or the configuration script that we normally use. The final step is to verify
    that everything works fine by connecting to the replica set server and getting
    its replica set `status`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The server should initially be in `state: RECOVERING`, and, once it has caught
    up with the secondary, it should be back in `state: SECONDARY`, like it was before
    starting the maintenance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will repeat the same process for every secondary server. In the end, we
    have to perform maintenance on the primary. The only difference in the process
    for the primary is that we will start by stepping down our primary server into
    a secondary server before every other step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: By using the above argument, we prevent our secondary from being elected as
    a master for 10 minutes. This should be enough time to shut down the server and
    continue with our maintenance, like we did with the secondaries.
  prefs: []
  type: TYPE_NORMAL
- en: Re-syncing a member of a replica set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Secondaries sync up with the primary by replaying the contents of the oplog.
    If our oplog is not large enough, or if we encounter network issues (partitioning,
    an underperforming network, or just an outage of the secondary server) for a period
    of time larger than the oplog, then MongoDB cannot use the oplog to catch up to
    the primary anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: The more straightforward option is to delete our `dbpath` directory and restart
    the `mongod` process. In this case, MongoDB will start an initial sync from scratch.
    This option has the downside of putting a strain on our replica set, and our network,
    as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The more complicated (from an operational standpoint) option is to copy data
    files from another well-behaving member of the replica set. This goes back to
    the contents of Chapter 8, *Monitoring, Backup, and Security*. The important thing
    to keep in mind is that a simple file copy will probably not suffice, as data
    files will have changed from the time that we started copying to the time that
    the copying ended.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, we need to be able to take a snapshot copy of the filesystem under our
    `data` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Another point of consideration is that by the time we start our secondary server
    with the newly copied files, our MongoDB secondary server will try to sync up
    to the primary using the oplog again. So, if our oplog has fallen so far behind
    the primary that it can't find the entry on our primary server, this method will
    fail, too.
  prefs: []
  type: TYPE_NORMAL
- en: Keep a sufficiently sized oplog. Don't let data grow out of hand in any replica
    set member. Design, test, and deploy sharding early on.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the oplog's size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hand in hand with the previous operational tip, we may need to rethink and
    resize our oplog as our data grows. Operations become more complicated and time-consuming
    as our data grows, and we need to adjust our oplog size to accommodate for it.
    The steps for changing the oplog''s size are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to restart our MongoDB secondary server as a standalone server,
    an operation that was described in the *How to perform maintenance on replica
    sets* section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then make a backup of our existing oplog:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We keep a copy of this data, just in case. We then connect to our standalone
    database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Up until now, we have connected to the `local` database and deleted the `temp`
    collection, just in case it had any leftover documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to get the last entry of our current oplog and save it in
    the `temp` collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This entry will be used when we restart our secondary server, in order to track
    where it has reached in the oplog replication:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we delete our existing oplog, and in the next step, we will create a new
    oplog of `4` GB in `size`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to copy the one entry from our `temp` collection back to our
    oplog:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we cleanly shut down our server from the `admin` database, using the
    `db.shutdownServer()` command, and we restart our secondary as a member of the
    replica set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We repeat this process for all secondary servers, and as a last step, we repeat
    the procedure for our primary member, which is done after we step the primary
    down by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Reconfiguring a replica set when we have lost the majority of our servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is only intended as an interim solution and a last resort when we are faced
    with downtime and disrupted cluster operations. When we lose the majority of our
    servers and we still have enough servers to start a replica set (maybe including
    some quickly spawned arbiters), we can force a reconfiguration with only the surviving
    members.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we get the replica set configuration document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `printjson(cfg)`, we identify the members that are still operational.
    Let''s say that these are `1`, `2`, and `3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'By using `force : true`, we are forcing this reconfiguration to happen. Of
    course, we need to have at least three surviving members in our replica set for
    this to work.'
  prefs: []
  type: TYPE_NORMAL
- en: It's important to remove the failing servers as soon as possible, by killing
    the processes and/or taking them out of the network, avoid unintended consequences;
    these servers may believe that they are still a part of a cluster that doesn't
    acknowledge them anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Chained replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Replication in MongoDB usually happens from the primary to the secondaries.
    In some cases, we may want to replicate from another secondary, instead of the
    primary. Chained replication helps to alleviate the primary from read load, but
    at the same time, it increases the average replication lag for the secondary that
    chooses to replicate from a secondary. This makes sense, as replication has to
    go from the primary to the secondary (1), and then from this server to another
    secondary (2).
  prefs: []
  type: TYPE_NORMAL
- en: 'Chained replication can be enabled (and disabled, respectively) with the following
    `cfg` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'In cases where `printjson(cfg)` doesn''t reveal a settings sub-document, we
    need to create an empty one first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: If there is already a `settings` document, the preceding command will result
    in deleting its settings, leading to potential data loss.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud options for a replica set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can set up and operate a replica set from our own servers, but we can reduce
    our operational overhead by using a **Database as a Service** (**DBaaS**) provider
    to do so. The two most widely used MongoDB cloud providers are mLab (formerly
    MongoLab) and MongoDB Atlas, the native offering from MongoDB, Inc.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will go over these options and how they fare in comparison
    to using our own hardware and data centers.
  prefs: []
  type: TYPE_NORMAL
- en: mLab
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: mLab is one of the most popular cloud DBaaS providers for MongoDB. It has been
    offered since 2011 and it is considered a stable and mature provider.
  prefs: []
  type: TYPE_NORMAL
- en: After signing up, we can easily deploy a replica set cluster in a set of cloud
    servers without any operational overhead. Configuration options include AWS, Microsoft
    Azure, or Google Cloud as the underlying server provider.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple sizing options for the latest MongoDB version. There was
    no support at the time of writing this book for the MMAPv1 storage engine. There
    are multiple regions for each provider (US, Europe, and Asia). Most notably, the
    regions that are missing are the AWS China, AWS US Gov, and AWS Germany regions.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB Atlas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MongoDB Atlas is a newer offering from MongoDB, Inc., and was launched in summer
    2016\. Similar to mLab, it offers the deployment of single-server, replica set,
    or sharded clusters, through a web interface.
  prefs: []
  type: TYPE_NORMAL
- en: It offers the latest MongoDB version. The only storage option is WiredTiger.
    There are multiple regions for each provider (US, Europe, and Asia).
  prefs: []
  type: TYPE_NORMAL
- en: Most notably, the regions that are missing are the AWS China and AWS US Gov
    regions.
  prefs: []
  type: TYPE_NORMAL
- en: In both (and most of the other) providers, we can't have a cross-region replica
    set. This is prohibitive if we want to deploy a truly global service, serving
    users from multiple data centers around the globe, with our MongoDB servers being
    as close to the application servers as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The running costs for cloud-hosted services can be significantly higher than
    setting them up in our own servers. What we gain in convenience and time to market
    may have to be paid in operational costs.
  prefs: []
  type: TYPE_NORMAL
- en: Replica set limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A replica set is great when we understand why we need it and what it cannot
    do. The different limitations for a replica set are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It will not scale horizontally; we need sharding for it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will introduce replication issues if our network is flaky
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will make debugging issues more complex if we use secondaries for reads,
    and these have fallen behind our primary server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the flip side, as we explained in previous sections in this chapter, a replica
    set can be a great choice for replication, data redundancy, conforming with data
    privacy, backups, and even recovery from errors caused by humans, or otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed replica sets and how to administer them. Starting
    with an architectural overview of replica sets and replica set internals involving
    elections, we dove into setting up and configuring a replica set.
  prefs: []
  type: TYPE_NORMAL
- en: You learned how to perform various administrative tasks with replica sets, and
    you learned about the main options for outsourcing operations to a cloud DBaaS
    provider. Finally, we identified some of the limitations that replica sets in
    MongoDB currently have.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will move on to one of the most interesting concepts
    in MongoDB (which helps it to achieve horizontal scaling): sharding.'
  prefs: []
  type: TYPE_NORMAL
