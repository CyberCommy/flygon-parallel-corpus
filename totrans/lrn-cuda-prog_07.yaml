- en: Parallel Programming Patterns in CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover parallel programming algorithms that will help
    you understand how to parallelize different algorithms and optimize CUDA. The
    techniques we will cover in this chapter can be applied to a variety of problems,
    for example, the parallel reduction problem we looked at in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA
    Thread Programming*, which can be used to design an efficient softmax layer in
    neural network operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prefix sum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pack and split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: N-body operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QuickSort in CUDA using dynamic parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radix sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Histogram calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete this chapter, it is recommended that you use an NVIDIA GPU card
    later than the Pascal architecture. In other words, your GPU's compute capability
    should be equal to or greater than 60\. If you are unsure of your GPU's architecture,
    please visit NVIDIA GPU's site ([https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus))
    and confirm your GPU's compute capability.
  prefs: []
  type: TYPE_NORMAL
- en: The same codes in this chapter have been developed and tested with CUDA version
    10.1\. In general, it is recommended to use the latest CUDA version if applicable.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we have used matrix multiplication code in many examples, we didn't
    investigate whether the operation was optimized. Now, let's review its operation
    and how we can find an opportunity for optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrix multiplication is a group of dot product operations from two matrices.
    We can simply parallelize the operations that are done by all the CUDA threads
    to generate a dot product of elements. However, this operation is inefficient
    in terms of memory usage because the data that''s loaded from memory isn''t reused. To
    confirm our analogy, let''s measure the performance limiter. The following chart shows
    the GPU utilization for a Tesla V100 card using Nsight Compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82f700d0-9e74-4188-b49c-dc58c1cd2baf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on our performance limiter analysis, this utilization ratio can be categorized
    as memory bounded. Therefore, we should review the memory utilization to mitigate
    utilization. The following screenshot shows the memory workload analysis section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b579069-63f5-4a3c-bb71-e5ce6de69cb5.png)'
  prefs: []
  type: TYPE_IMG
- en: From this analysis, we can see that the L2 cache hit rate is low and that the
    max bandwidth is low. We can presume that this is because the original matrix
    multiplication operation does not reuse loaded data, as we mentioned earlier.
    This can be resolved by using shared memory, that is, reusing the loaded data
    and mitigating global memory usage. Now, let's review matrix multiplication and
    how we can optimize this to use shared memory that has a small memory space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrix multiplication is a group of dot product operations with some small
    size matrices and a cumulation of output. The small matrices are called tiles
    and they map to the matrices along the output matrix. Each tile will compute its
    own output in parallel. This operation can be implemented in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine the tile size for two input and output matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Traverse the input tiles, along with their direction (matrix A goes to the right,
    and matrix B goes down).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute matrix multiplication within the tile.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue the second step until the tile reaches the end.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Flush the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following diagram shows the concept of tiled matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/320ce4a9-f221-4986-9909-716c76e4f6ad.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we compute a matrix multiplication, *C = AB*. We compute
    a smaller matrix multiplication as a tile, in green, from matrix A and matrix
    B. Then, we traverse the input tile position, respectively. The operation result
    is accumulated to the previous output to generate the matrix multiplication's
    output.
  prefs: []
  type: TYPE_NORMAL
- en: This operation provides an optimization opportunity because we can break down
    the large matrix operation with the small problems and place it in the small memory
    space. In CUDA programming, we place the small matrices in shared memory and mitigate
    global memory access. In our implementation, we will match the tile with the CUDA
    thread block. The tile's position will be determined by its block index, which
    is done with the `tid_*` variable.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of the tiling approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s implement an optimized matrix multiplication using the tiled approach. We
    will reuse the previous matrix multiplication sample code that we used in [Chapter
    3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA Thread Programming*. After
    optimization, we will look at how performance can be enhanced. Follow these steps
    to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a kernel function that will be our optimized version of matrix
    multiplication. We will name the kernel function `v2` in the `sgemm` operation.
    This kernel function will compute ![](img/7697ab42-f52b-40c7-971b-aaa358bf19e7.png),
    so we should provide the related parameters, respectively. We will also pass the
    matrix size information with `M`, `N`, and `K`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For this operation, we will use the block index and the thread index separately.
    As we discussed earlier, we need to use the block index separately to designate
    the tile position. We will use the thread index for the tile-level matrix multiplication.
    Therefore, we need to create the CUDA index parameter, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we will use shared memory as tiles and use a local register to
    save the output value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will write a loop that controls the tiles'' position. Here is the
    for loop code that controls a loop based on its block size. Be aware that the
    loop size is determined by `K` considering how many times blocks should be traversed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will write code that feeds data in the second loop. As we discussed
    earlier, each tile has its own moving direction, along with the matrices; tile
    `A` traverses the column of matrix `A` and tile `B` traverses the row of matrix
    `B`. We place them according to the diagram shown in the *Matrix multiplication
    optimization* section. After that, we should place `__syncthreads()` after copying
    data from global memory to shared memory to avoid un-updated data from the previous
    iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can write matrix multiplication code from the tiles. The local variable
    known as `element_c` will cumulate the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will write the result into global memory. The following operation should
    be placed after the second loop finishes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s review how this tiling approach benefits the matrix multiplication
    operation. By using shared memory in our tiled matrix multiplication, we can expect
    that we will reduce the global memory traffic by using the input data and thus
    enhancing performance. We can confirm this with the profile result easily:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we designed the kernel to reuse input data, the increased block size
    may help with performance. For instance, a 32 x 32 block size can be optimal considering
    the warp size and the number of shared memory banks to avoid bank conflicts. We
    can easily obtain its experiment result using the profile:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the increased tile size benefits the matrix multiplication operation's
    performance. Now, let's analyze its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Performance analysis of the tiling approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we looked at the tiling approach and how it can achieve good performance. Let's
    review what the tiling approach resolves and look at what steps we can take next.
    Covering this part is optional in general because NVIDIA provides the cuBLAS and
    CUTLASS libraries for the `GEMM` (short for **General Matrix Multiply**) operation
    to provide optimized performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following chart shows the updated GPU utilization report from Nsight Compute.
    The updated utilization output from the lower profile is a result of the upper
    profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c864bc30-a77c-4fcf-8261-00d776982d81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As both resources scored high utilization, we should review each one''s resource
    usage. First of all, let''s review the memory workload. The following screenshot
    shows the updated result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/113b5a48-87e6-47cf-b505-348afeaffec6.png)'
  prefs: []
  type: TYPE_IMG
- en: From this result, we can see that global memory access is optimized from maximized
    memory bandwidth and reduced memory throughput. Also, the L2 cache hit rate is
    enhanced. So, our tiling approach transforms matrix multiplication from global
    memory into the on-chip-level operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this does not mean that we achieved the most optimized performance.
    From the memory workload analysis, we can see that the memory pipes are too busy.
    This is due to our element-wise multiplication from shared memory. To resolve
    this issue, we need to remap the data in shared memory. We will not cover that
    in this book, but you can learn about it in this article: [https://github.com/NervanaSystems/maxas/wiki/SGEMM](https://github.com/NervanaSystems/maxas/wiki/SGEMM).'
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed earlier, the cuBLAS library shows much faster performance. We
    will cover its usage in the *cuBLAS* section in [Chapter 8](a4f84b40-7530-4ad1-83be-d4de09b071bf.xhtml),
    *Programming with Libraries and Other Languages*. However, understanding the tiling
    approach at this stage is useful so that we can understand how GPUs can begin
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The convolutional operation (or filtering) is another common operation in many
    applications, especially in image and signal processing, as well as deep learning.
    Although this operation is based on the product of sequential data from the input
    and filter, we have a different approach for matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution operation in CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The convolutional operation consists of source data and a filter. The filter
    is also known as a kernel. By applying the filter against the input data, we can
    obtain the modified result. A two-dimensional convolution is shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3992c61c-8920-487f-8b8a-f38031e6cd8e.png)'
  prefs: []
  type: TYPE_IMG
- en: We need to consider a couple of concepts when we implement convolution operation,
    that is, kernel and padding. The kernel is a set of coefficients that we want
    to apply to the source data. This is also known as a filter. The padding is extra
    virtual space around the source data so that we can apply kernel functions to
    the edge. When the padding size is 0, we don't allow the filter to move beyond
    the source space. However, in general, the padding size is half the size of the
    filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start easily, we can design the kernel function with the following in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Each CUDA thread generates one filtered output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each CUDA thread applies the filter's coefficients to the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The filter shape is a box filter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Following these conditions, we can have a simple convolutional operation filter
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This kernel function fetches input data and a filter for the very operation
    and does not reuse all the data. Considering the performance impact from memory
    inefficiency, we need to design our kernel code so that we can reuse the loaded
    data. Now, let's write the optimized version of convolution.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, the convolution filter is a read-only matrix and is used by all
    the CUDA threads. In this case, we can use CUDA's constant memory to utilize its
    cache operation with the broadcasting operation.
  prefs: []
  type: TYPE_NORMAL
- en: In convolution implementation design, we use the tiling approach, and each tile
    will generate the filtered output to the mapped position. Our tile design has
    extra space to consider the convolution filter size, which provides the required
    data for the convolution operation. This extra space is called **padding**. The
    following diagram shows an example of a thread block with a 6 x 6 dimension and
    a filter that's 3 x 3 size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we need to have an 8 x 8 sized tile on shared memory for each thread
    block, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/519d0f1f-3b3b-48dd-8606-676e66d417da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The pad area can be input data when the source''s addresses are invalid memory
    space, or they are filled with zero (zero-padding approach). By doing this, we
    can make the tile replace the input global memory with no additional effect on
    the boundary elements. To fill the tile, we iterate over the tile with the thread
    block size, and determine which value should be filled by checking the boundary
    condition of the input data. Our implementation sets the input data as a multiple
    of the tile size so that the boundary condition matches with the pad space of
    each thread block''s tile. A brief diagram of mapping the source data to the tile
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93e820c5-8149-4d4c-adab-98325e1e6512.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this design, the number of iterations we need to do to fill the tile is
    four. However, this should be changed depending on the filter size. This way,
    the number of iterations to fill the tile is determined by the number of the ceiling
    of tile size, divided by the thread block size. Its implementation is simple,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's implement the optimized convolution operation using shared memory
    as a box filter.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering coefficients optimization using constant memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Firstly, we will learn how to optimize filter coefficient data usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will make a modified version of `convolution_kernel()`. Let''s duplicate
    the kernel code and rename one of them as `convolution_kernel_v2()`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create a constant memory space to store the filter coefficients.
    The constant memory''s size is limited and we can''t make modifications to the
    kernel code. However, we can use this constant memory since our convolutional
    filter is suitable for this condition. We can use constant memory like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can place our convolutional filter coefficients in constant memory
    using the `cudaMemcpyToSymbol()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s switch the filter operation so that we can use constant memory. The
    whole kernel implementation is as follows. As you can see, only one variable''s
    usage has changed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can confirm the performance enhancement thanks to the filter data reusing `nvprof`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: From this result, we can see a reduced kernel execution time.
  prefs: []
  type: TYPE_NORMAL
- en: Tiling input data using shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will optimize input data usage using shared memory. To differentiate
    our next optimization step, let''s duplicate the previous convolution kernel function
    and name it `convolution_kernel_v3()`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to preprepare the shared memory space so that it can store the
    input data. To get the benefit of the filter operation from shared memory, we
    need to have extra input data. To create sufficient memory space, we need to modify
    the kernel call, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the kernel code, we can declare the shared memory space as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can copy the input data to shared memory, which will be calculated
    by the thread block. First, let''s declare some variables that help control the
    memory operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can copy the load input data to shared memory by following the tiling
    design we discussed previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the input memory has changed, our convolution code should be updated.
    We can write the convolution code as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can measure the performance gain using `nvprof`. From the result,
    we can confirm that we have accelerated about 35% faster than the original operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have covered how to utilize loaded data so that we can reuse it with
    other on-chip caches instead of global memory. We'll talk about this in more detail
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Getting more performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If the filter is the symmetric filter or separable filter, we can break down
    the box filter as two filters: a horizontal filter and a vertical filter. Using
    two directional filters, we can have more optimization in shared memory usage:
    memory space and memory utilization. If you want to learn more about this, have
    a look at a CUDA sample named `convolutionSeparable` in the `3_Imaging/convolutionSeparable`
    directory. Its detailed explanation is also included in the same directory as
    `doc/convolutionSeparable.pdf`.'
  prefs: []
  type: TYPE_NORMAL
- en: Prefix sum (scan)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Prefix sum (scan) is used to obtain a cumulative number array from the given
    input numbers array. For example, we can make a prefix-sum sequence as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input numbers ** | 1 | 2 | 3 | 4 | 5 | 6 | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **Prefix sums** | 1 | 3 | 6 | 10 | 15 | 21 | ... |'
  prefs: []
  type: TYPE_TB
- en: 'It differs from parallel reduction since reduction just generates the total
    operation output from the given input data. On the other hand, scan generates
    outputs from each operation. The easiest way to solve this problem is to iterate
    all the inputs to generate the output. However, it would take a long time and
    would be inefficient in GPUs. Hence, the mild approach can parallelize the prefix-sum
    operation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40a0239f-03fc-48b4-a4b9-bcfe8d18b761.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this approach, we can obtain the output using multiple CUDA cores. However,
    this method does not reduce the total number of iterations because the first input
    element should be added for all the outputs one by one. Also, we cannot predict
    the output result when the array is sufficiently large, so multiple thread blocks
    should be launched. This is because all the scheduled CUDA threads are not launched
    at the same time in the CUDA architecture and there would be conflicts in multiple
    CUDA threads. To avoid this, we need a double buffer approach for the array, which
    is another inefficiency. The following code shows its implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'There is another optimized approach named **Blelloch scan**. This method generates
    prefix-sum outputs by increasing and decreasing the strides exponentially. This
    method''s procedure is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/037f5546-f0b2-437e-a772-a3c9cdfcaf62.png)'
  prefs: []
  type: TYPE_IMG
- en: There are two steps based on the stride controls. While increasing the stride,
    it obtains the partial summations accordingly. Then, it obtains the partial summations
    while reducing the stride accordingly. Each step has a different operation pattern,
    but they can be figured out with the stride size. Now, let's cover the Blelloch
    scan's implementation and check out the updated performance.
  prefs: []
  type: TYPE_NORMAL
- en: Blelloch scan implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following steps will show you how to implement the optimized parallel scan
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a kernel function that can accept input and output memories,
    along with their size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will create a CUDA thread index and a global index to handle the input
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To speed up the iteration, we will use shared memory. This algorithm can generate
    outputs that are double the size of CUDA threads, so we will load extra block-sized
    input data into shared memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we start the iteration, we will declare the offset variable that counts
    the gap between the left-hand operand and the right-hand operand:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will add up the input data until the offset becomes larger than the
    input''s length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we will iterate again while we reduce the reduction size by two:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will store the output value in global memory using the kernel function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can call this scan kernel function as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: You can also write a naîve scan version with the same function interface. Now,
    let's review how fast our new version is, and if there are any other optimization
    opportunities we can take advantage of.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the profiled result of the naïve scan''s and Blelloch
    scan''s performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the Blolloch scan is about five times faster than the naive
    scan algorithm due to reduced overhead. We can also validate the operation result
    by comparing the output of the different implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Up until now, we have covered how to design and implement the optimized parallel
    prefix-sum operation on a single block size. To use the prefix-sum operation on
    the input data, which has more data than the block size, we need to build a block-level
    prefix-sum operation based on our block-level reduction code. We'll talk about
    this in more detail in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Building a global size scan
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our implemented prefix-sum operation works within a single thread block. Since
    the first step has two inputs and the maximum CUDA threads we can have in a thread
    block is 1,024, the maximum available size is 2,048\. Without considering other
    thread block operations, the thread block does up-sweeping and down-sweeping.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this operation can be enlarged if we perform a block-wise scan operation.
    To do this, you will need extra steps that collect the last prefix-sum result,
    scan them, and add each thread block''s result with each block''s block-level
    scanned value. This procedure can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9080ba5d-b61c-4763-bc01-989c933a5255.png)'
  prefs: []
  type: TYPE_IMG
- en: The pursuit of better performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our implementation code performs the optimal operation. However, we can make
    further optimizations by reducing the shared memory's bank conflicts. In our implementation,
    the CUDA threads access the same memory banks at certain points. NVIDIA's GPU
    Gem3 introduced prefix-sum (scan) in *Chapter 39, Parallel Prefix Sum (Scan) with
    CUDA* ([https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html](https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)),
    and points out this issue in *39.2.3 Avoiding Bank Conflicts*. You can adapt the
    solution to our implementation, but you should update `NUM_BANKS` to `32` and
    `LOG_NUM_BANKS` to `5` if you do. Nowadays, the CUDA architecture has 32 shared
    memory banks.
  prefs: []
  type: TYPE_NORMAL
- en: Other applications for the parallel prefix-sum operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dr. G.E. Blelloch published an article about his prefix-sum named *Prefix Sums
    and Their Application* ([https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf](https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf))
    in 1993\. You can learn more about the parallel prefix-sum algorithm and its applications
    by reading his article. The applications are compact, split, segmented scan, quick
    sort, radix sort, and merge sort.
  prefs: []
  type: TYPE_NORMAL
- en: Dr. Ahmed Sallm's video lecture, *Intro to Parallel Processing with CUDA - Lecture
    4 Part 2\3* ([https://youtu.be/y2HzWKTqo3E](https://youtu.be/y2HzWKTqo3E)), provides
    a good introduction to these. It provides conceptual introductions to how the
    prefix-sum algorithm can be used to clip graphics and build a sparse matrix. He
    also provides instructions regarding how to use the sort algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Compact and split
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Previously, we covered how to parallelize the sequential prefix sum algorithm
    and discussed how it can be used for other applications. Now, let''s cover some
    of those applications: compact and split. The compact operation is an algorithm
    that can consolidate values that fulfill the given condition from an array. On
    the other hand, the split operation is an algorithm that distributes the values
    to the designated place. In general, these algorithms work sequentially. However,
    we will see how the parallel prefix-sum operation can improve how it functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The compact operation is used to collect specific data that meets a certain
    condition into an array. For example, if we want to use the compact operation
    for the positive elements in an array, then the operation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec6509d7-6673-4967-898a-f387974abaca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In parallel programming, we have a different approach that can utilize multiple
    cores using the parallel prefix-sum operation. First, we mark the data to check
    whether it meets the condition or not (that is, predicate), and then we do the
    prefix-sum operation. The output of prefix-sum will be the index of the marked
    values, so we can obtain the gathered array by copying them. The following diagram
    shows an example of a compact operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16b52978-c007-437d-8b05-577c3b76efa9.png)'
  prefs: []
  type: TYPE_IMG
- en: Since all of these tasks can be done in parallel, we can obtain the gathered
    array in four steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, split means to distribute the data to a number of different
    places. In general, we distribute the data from where it was initially. The following
    diagram shows an example of its operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/517c99ed-34ed-471d-934e-7640d07b839f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This example shows that the gathered array elements are distributed where they
    were from. We can also do this in parallel using prefix-sum. Firstly, we refer
    to the predicate array and do the prefix-sum. Since the outputs are each element''s
    address, we can distribute them easily. The following diagram shows how this operation
    can be done:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7af231f8-fb0d-4d46-887a-99d82c9aa08a.png)`'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's implement this and discuss their performance limiters and their application.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing compact
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The compact operation is a sequence of predicate, scan, addressing, and gather.
    In this implementation, we will build an array of positive numbers from an array
    of randomly generated numbers. The initial version can only afford a single thread
    block operation since we will only use a single block-sized prefix-sum operation.
    However, we can learn how prefix-sum is useful for other applications and extend
    this operation to larger arrays with the extended prefix-sum operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement a compact operation, we will write several kernel functions that
    can do the required operation for each step and call those last:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write a kernel function that can make a predicate array by checking
    whether each element''s value is greater than zero or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we have to perform a prefix-sum operation for that predicate array. We
    will reuse the previous implementation here. After that, we can write a kernel
    function that can detect the address of the scanned array and gather the target
    elements as output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s call them all together to make a compact operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have an array of positive numbers that were gathered from a randomly
    generated array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: By using the parallel prefix-sum operation, we can implement the compact operation
    in parallel easily. Our implementation compacts the positive values from the given
    array, but we can switch this to the other condition and apply the compact operation
    without difficulty. Now, let's cover how to distribute these compact elements
    to the original array.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing split
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The split operation is a sequence of predicate, scan, address, and split. In
    this implementation, we will reuse the address array we created in the previous
    section. Therefore, we can skip the previous steps and just implement the split
    operation from the address array:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write the split kernel function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can call the kernel function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we''ll be using the output of the scan from the previous step, we will
    copy it to the input and clear the original array. In total, we can do a parallel
    compact and split using CUDA. Here is the output of our implementation. You can
    confirm that it operates as desired:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In our implementation, we generated a compact array and a split array for the
    positive elements. Thanks to the parallel prefix-sum, we can also do this in parallel.
    One of the major limitations of our version is that it only supports less than
    2,048 elements since our implementation is based on our previous parallel prefix-sum
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: N-body
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any N-body simulation is a simulation of the dynamical system that evolves under
    the influence of physical forces. Numerical approximation is done as the bodies
    continuously interact with each other. N-body simulation is done extensively in
    physics and astronomy, for example, so that scientists can understand the dynamics
    of particles in the Universe. N-body simulations are used in many other domains,
    including computational fluid dynamics in order to understand turbulent fluid
    flow simulation.
  prefs: []
  type: TYPE_NORMAL
- en: A relatively easy method for solving N-body simulation is to make use of a brute-force
    technique that has *O(N²)* complexity. This approach is embarrassingly parallel
    in nature. There are various optimizations at algorithmic scale that can reduce
    the compute complexity. Instead of applying all-pairs to the whole simulation,
    it can be used to determine forces in close-range interactions. Even in this case,
    creating a kernel for solving the forces on CUDA is very useful as it will also
    improve the performance of far-field components. Accelerating one component will
    offload work from the other components, so the entire application benefits from
    accelerating one kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an N-body simulation on GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The algorithm is basically an all-pairs algorithm calculating force, *f[ij]*, for
    an N![](img/5b868d65-55c7-40fd-a496-8c31b23a562d.png)N grid. The total force/acceleration, *F[i]*,
    on a body, *i*, is the result of a summation of all the entries in row *i*. From
    a parallelism point of view, this is an embarrassingly parallel task of *O(N²)*.
  prefs: []
  type: TYPE_NORMAL
- en: From a performance point of view, the application is memory bound and would
    be limited by memory bandwidth. The good part is that much of the data can be
    reused and stored in high bandwidth and low latency memory such as shared memory.
    Data reuse and storage in shared memory reduces the load on global memory and
    hence helps in reaching peak compute performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the strategy that we will be using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5eefd12-c31d-480d-a0b8-a473c3eed048.png)'
  prefs: []
  type: TYPE_IMG
- en: Instead of loading the memory again and again from global memory, we make use
    of tiling. We've already demonstrated the use of tiling for matrix multiplication
    and looked at its use in imaging applications in previous chapters. The preceding
    diagram shows that each row is evaluated in parallel. The tile size is defined
    by the maximum number of elements that can be stored in shared memory that don't
    affect the occupancy of the kernel. Each block loads the data into shared memory,
    followed by performing synchronization. Once the data has been loaded into shared
    memory, the force/acceleration calculation is done in every block. It is visible
    that even though a separate row is calculated in parallel, to achieve optimal
    data reuse, the interaction in each row is done sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of an N-body simulation implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s review the implementation of this in pseudocode format, followed by
    explaining its logic. In this example, we use gravitational potential to illustrate
    the basic form of computation in an all pairs N-body simulation. The implemented
    code can be found in `07_parallel_programming_pattern/05_n-body`. Follow these
    steps to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize n-space with random variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare and store the data in an intermediate shared memory space for efficient
    reuse. Synchronize it to guarantee that all the threads within the block see the
    updated values in shared memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the force by iterating every block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, compile the application with the `nvcc` compiler with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, implementing an N-body simulation is an embarrassingly parallel
    task and quite straightforward. While we have implemented the basic version of
    code here, there are various algorithmic variations that exist. You can make use
    of this version as a template that you can improve, based on changes that are
    made to the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Histogram calculation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an embarrassingly parallel job, ideally, you would assign computation to
    each thread working on independent data, resulting in no data races. By now, you
    will have realized that some patterns don't fit this category. One such pattern
    is when we're calculating a histogram. The histogram pattern displays the frequency
    of a data item, for example, the number of times we used the word CUDA in each
    ch
  prefs: []
  type: TYPE_NORMAL
- en: 'apter, the number of times each letter occurred in this chapter, and so on.
    A histogram takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/713c8091-8a43-4097-8ba0-192a4fe97b41.png)'
  prefs: []
  type: TYPE_IMG
- en: In this section, we will make use of atomic operations to serialize access to
    data in order to get the correct results.
  prefs: []
  type: TYPE_NORMAL
- en: Compile and execution steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The histogram provides important features about the datasets at hand, as well
    as useful insights about the same. For example, out of the whole image, there
    are only a few regions where regions of interest may lie. Creating a histogram
    is sometimes used to figure out where in the image the region of interest may
    be. In this example, we will be making use of calculating a histogram on an image
    where the whole image is divided into chunks. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare your GPU application. This code can be found at `07_parallel_programming_pattern/08_histogram`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compile your application with the `nvcc` compiler with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The `scrImagePgmPpmPackage.cpp` file provides the source code that we can use
    to read and write images with `.pgm` extensions. The histogram calculation code
    can be found in `image_histogram.cu`.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding a parallel histogram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Patterns such as the histogram demand atomic operation, which means updating
    a value at a specific address in a serialized fashion to remove contention from
    multiple threads, thereby updating the same address. This requires coordination
    among multiple threads. In this seven-step process, you might have observed that
    we made use of privatization. Privatization is a technique that makes use of low
    latency memory such as shared memory to reduce throughput and decrease latency,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02786dd6-3c15-4c9f-9d7a-770d31b640b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Basically, instead of making use of atomic operations on global memory, we make
    use of atomics on shared memory. The reason should be quite obvious to you by
    now. Atomic operations on global memory are more costly compared to doing the
    same on shared memory/an L1 cache. From the Maxwell architecture onward, atomic
    operations are hardware supported. The privatized shared memory implementation
    should ideally give you 2x performance from the Maxwell architecture onward. However,
    please note that atomic operations are limited to specific functions and data
    sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating a histogram with CUDA atomic functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Primarily, we are going to make use of the `atomicAdd()` operation on shared
    memory to calculate a histogram for each block in shared memory. Follow these
    steps to calculate the histogram in a kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Allocate shared memory per block equal to the size of the histogram per block.
    Since it is a char image, we expect the elements to be in the range of 0-255:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the shared memory array to `0` per block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Synchronize this to make sure all the threads within a block see initialized
    array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the data of the image from the global/texture memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Do an `atomicAdd()` operation on shared memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Synchronize across the block before writing to global memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the histogram per block to global memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have finished implementing the histogram calculation on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, histograms are easy to implement with shared atomic memory. This
    approach can attain high performance on Maxwell onward cards due to its native
    support for shared atomic memory in hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Quicksort in CUDA using dynamic parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the key algorithms that''s a fundamental building block for any application
    is sorting. There are many sorting algorithms available that have been studied
    extensively. The worst time complexity, best time complexity, input data characteristics
    (is the data almost sorted or random? Is it a key-value pair? Is it an integer
    or a float?), in-place or out of place memory requirements, and so on define which
    algorithm is suitable for which application. Some of the sorting algorithms fall
    into the category of divide and conquer algorithms. These algorithms are suitable
    for parallelism and suit architectures such as GPU where data to be sorted can
    be divided for sorting. One such algorithm is Quicksort. As we stated earlier,
    Quicksort falls into the category of divide and conquer. It is a three-step approach,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Pick an element from an array that needs to be sorted. This element acts as
    a pivot element.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second step is partitioning where all the elements go. All the elements
    that are less than the pivot are shifted to the left and all the elements greater
    than or equal to the pivot are shifted to the right of the pivot element. This
    step is also known as partitioning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recursively do steps 1 and 2 until all the sub-arrays have been sorted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Quicksort worst-case complexity is O(![](img/79e46e31-d8d8-4612-9c6d-9e08bf0e40a0.png)),
    which may not seem ideal compared to other sorting processes whose worst-case
    complexity is O(![](img/3859e9ea-10c4-44b8-9ddc-61e8a07878fe.png)), such as merge
    sort and heap sort). However, Quicksort is seen to be effective in practice. The
    choice of the pivot element can be chosen with consideration and sometimes randomly
    so that worst-case complexity hardly occurs. Also, Quicksort is seen to have less
    memory load and requirements compared to other sorting algorithms, such as merge
    sort, which requires extra storage. More practical implementations of Quicksort
    use a randomized version. The randomized version has the expected time complexity
    of O(![](img/52cec39e-957c-4bfc-be02-07aa00f96b82.png)). Worst-case complexity
    is also possible in the randomized version, but it doesn't occur for a particular
    pattern (such as a sorted array) and randomized Quicksort works well in practice.
  prefs: []
  type: TYPE_NORMAL
- en: While we can write a whole chapter on the characteristics of the sorting algorithm,
    we plan to cover only the features of CUDA that will help you to implement Quicksort
    efficiently on GPU. In this section, we will be making use of dynamic parallelism,
    which was introduced from CUDA 6.0 and GPUs with a 3.5 architecture onwards.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's review how dynamic parallelism contributes to the sorting algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Quicksort and CUDA dynamic parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Quicksort algorithm demands launching kernels recursively. So far, the algorithms
    we have seen call the kernel once via the CPU. After the kernel has finished executing,
    we return to the CPU thread and then relaunch it. Doing this results in giving
    back control to the CPU, and may also result in data transfer between CPU and
    GPU, which is a costly operation. It used to be very difficult to efficiently
    implement algorithms such as Quicksort on GPUs that demand features such as recursion.
    With the GPU architecture 3.5 and CUDA 5.0 onwards, a new feature was introduced
    called dynamic parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dynamic parallelism allows the threads within a kernel to launch new kernels
    from the GPU without returning control back to the CPU. The word dynamic comes
    from the fact that it is dynamically based on the runtime data. Multiple kernels
    can be launched by threads at once. The following diagram simplifies this explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e923707-6635-4f2b-8816-5c34e27ab0f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we were to translate this concept to how Quicksort is executed, it would
    look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82e06370-f447-437d-a8e3-15356096543e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Depth 0 is the call from the CPU. For each subarray, we launch two kernels:
    one for the left array and one for the right array. Recursion stops after the
    max depth of the kernel has been reached or the number of elements is less than
    32, which is the warp size. For the kernel''s launch to be in a non-zero stream
    and asynchronous so that the subarray kernel gets launched independently, we need
    to create a stream before every kernel launch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This is a really important step because, otherwise, the kernel's launch may
    get serialized. For more details on streams, please refer to the multi-GPU kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Quicksort with CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For our Quicksort implementation, we are going to make use of dynamic parallelism
    to launch the GPU kernel recursively. The major steps involved in implementing
    Quicksort are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The CPU launches the first kernel**: The kernel is launched with one block
    and one thread. The left element is the start of the array, while the right is
    the last element of the array (basically the whole array):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '**Limit check**: Check the two criteria before launching the kernel from inside
    a kernel. First, check if we have reached the max allowed limit of depth by the
    hardware. Second, we need to check whether the number of elements to be sorted
    in a sub-array is less than the warp size (32). If one of them is true, then we
    have to do a selection sort sequentially rather than launch a new kernel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '**Partitioning**: If the preceding conditions are met, then partition the array
    into two sub-arrays and launch two new kernels, one for the left array and another
    for the right array. If you look closely at the following code, you''ll see we
    are launching a kernel from inside the kernel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '**Executing the code**: The implemented code can be found at `07_parallel_programming_pattern/06_quicksort`. Compile
    your application with the `nvcc` compiler with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we have added two flags to the compilation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-- gpu-architecture=sm_70`: This flag tells the `nvcc` to compile and generate
    the binary/`ptx` for the Volta GPU. If you specifically do not add this flag,
    the compiler tries to compile the code compatible from `sm_20`, that is, Fermi
    generation cards, until the new architecture, which is `sm_70`, that is, Volta.
    The compilation will fail since dynamic parallelism is not supported by older
    generation cards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-rdc=true`: This is a key argument that enables dynamic parallelism on the
    GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic parallelism guidelines and constraints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though dynamic parallelism provides us with an opportunity to port algorithms
    such as Quicksort on GPU, there are some fundamental rules and guidelines that
    need to be followed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Programming model rules**: Basically, all the CUDA programming model rule
    apply:'
  prefs: []
  type: TYPE_NORMAL
- en: The kernel launches are asynchronous per thread.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronization is only allowed per block.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streams that are created are shared within a block.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Events can be used to create inter-stream dependencies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory consistency rules**:'
  prefs: []
  type: TYPE_NORMAL
- en: The child kernel sees the parent kernel's state at the time of the launch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parent kernel can see the changes that have been made by the child kernel,
    but only after the synchronization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local and shared memory is, as usual, private, and cannot be passed or accessed
    by the parent kernel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Guidelines**:'
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to understand that there is latency that gets added per
    kernel launch. The latency of launching a kernel from inside another kernel has
    gradually reduced over time with the new architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While launch throughput is an order of magnitude higher than from the host,
    limits can be placed on the maximum depth. The max depth that's allowed is 24
    for the latest generation cards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing `cudaDeviceSynchronize()` from inside the kernel is a very costly
    operation and should be avoided as much as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is additional memory preallocated on global memory so that we can store
    kernels before they are launched.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the kernel fails, the error is only visible from the host. Hence, you are
    advised to make use of the `-lineinfo` flag along with `cuda-memcheck` to locate
    the error's location.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radix sort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another very popular sorting algorithm is radix sort as it is really fast on
    sequential machines. The fundamental policy of radix sort is that each element
    is sorted digit by digit. Let''s look at a simple example to explain the steps
    involved in radix sort:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose the elements to be sorted are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Value | 7 | 14 | 4 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'The equivalent binary values of these numbers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Bits | 0111 | 1110 | 0100 | 0001 |'
  prefs: []
  type: TYPE_TB
- en: 'The first step is to sort based on bit 0\. Bit 0 for the numbers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0^(th) Bit | 1 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'To sort based on the *o^(th)* bit basically means that all the zeroes are on
    the left. All the ones are on the right while preserving the order of elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sorted value on 0^(th) bit | 14 | 4 | 7 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Sorted bits based on 0^(th) bit | 1110 | 0100 | 0111 | 0001 |'
  prefs: []
  type: TYPE_TB
- en: 'After the 0^(th) bit is done, we move on to the first bit. The result after
    sorting based on the first bit is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sorted value on the first bit | 4 | 14 | 7 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Sorted bits based on the first bit | 0100 | 1110 | 0111 | 0001 |'
  prefs: []
  type: TYPE_TB
- en: 'Then, we move on to the next higher bit until all the bits are over. The final
    result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sorted value on all bits | 1 | 4 | 7 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Sorted bits based on all bits | 0001 | 0100 | 0111 | 1110 |'
  prefs: []
  type: TYPE_TB
- en: As you can see, the upper limit that we set in this example was 4 bits. For
    larger numbers, such as integers, this will continue until 32 bits as integers
    are 32-bit.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood this algorithm, let's look at how this can be implemented
    in the GPU. Compared to the other sections in this chapter, we will take two approaches
    to showcase the CUDA ecosystem so that we can implement/use radix sort.
  prefs: []
  type: TYPE_NORMAL
- en: '**Option 1**: We are going to make use of a warp level to do radix sort on
    just 32 elements. The reason for this is that we want to make use of radix sort
    to introduce you to warp-level primitives.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Option 2**: We will be making use of the Thrust library, which is part of
    the CUDA Toolkit. It implements a generic radix sort. The best implementation
    is reuse. Since Thrust already provides one of the best implementations of radix
    sort, we will use that.'
  prefs: []
  type: TYPE_NORMAL
- en: Two approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To ease your understanding, let's begin with the example code. In this example,
    we will be making use of warp-level primitives and the Thrust library to implement/use
    the radix sort. The example code can be found at `07_parallel_programming_pattern/07_radixsort`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile your application with the `nvcc` compiler with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Warp-level primitive version:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Thrust library version:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: These two examples show the sorted output that's given by the GPU. Now, let's
    review how these operations are implemented in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Approach 1 – warp-level primitives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how CUDA warp-level primitives are used to implement our algorithm
    in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, load the data from global memory to shared memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The size of the memory is equal to the warp size, `*2`, so that it can implement
    the ping pong buffer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Loop through from the lower bit to the upper bit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the current bitmask:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the number of ones and zeroes (histogram):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Get the positions of the threads that have zero (0) in the current bit (Prefix
    Sum).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the positions of the threads that have one (1) in the current bit (Prefix
    Sum):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the data in the ping pong shared buffer memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Repeat steps 2-6 until the upper bit has been reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Store the final result in global memory from shared memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: It may not be clear to you where the histogram and prefix sum appeared all of
    a sudden. Let's talk about this implementation in detail so that we can understand how
    we use warp-level primitives to implement the same.
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of this section, we described how we sort using an example.
    What we did not cover, however, was how to find out the position of the element
    that needs to be swapped. Radix sort can be implemented using fundamental primitives
    such as histogram and prefix-sum and, hence, can easily be implemented in the
    GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s revisit the example we looked at and gather its details, included the
    steps of the histogram and prefix sum. The following table shows various calculations
    that were done at each bit iteratively:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Value | 7 | 14 | 4 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Binary | 0111 | 1110 | 0100 | 0001 |'
  prefs: []
  type: TYPE_TB
- en: '| Bit 0 | 1 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Histogram prefix sum | 2 | 0 | 2 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Offset | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| New index (prefix Sum and Offset) | 2 | 0 | 1 | 3 |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s explain each and every calculation shown in the preceding table, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we build a histogram for the number of elements with 0 in the 0th-bit
    position and the number of elements with 1 in the 0th-bit position:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Histogram: zero-bits (2 values), one-bits (2 values)*'
  prefs: []
  type: TYPE_NORMAL
- en: Then, we perform an exclusive prefix sum on these values. The prefix sum can
    be defined as the sum of all the previous values. In our case, we do this separately
    for both 0^(th)-bits and 1^(st)-bits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we move the elements based on the prefix sum values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The warp-level primitives we used to find the histogram and prefix sum were `__ballot_sync()` and `__popc()`,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The `__ballot_sync()` API evaluates the predicates for all the active threads
    of the warp and returns an integer whose Nth bit is set if, and only if, the predicate
    evaluates to non-zero for the Nth thread of the warp. `__popc()`, which counts
    the number of integers, was set to one.
  prefs: []
  type: TYPE_NORMAL
- en: In the CUDA programming model, we have seen that the minimum execution unit
    is a warp (32 threads). CUDA provides various warp-level primitives with fine-grained
    control that, in many applications, can result in better performance. We covered
    one such primitive, `__bllot__sync()`, in the previous section. Other important
    warp-level primitives include `shuffle` instructions, which are used for doing
    warp-level reduction in particular. `shuffle` instructions have already been covered
    in this book. If you have reached ninja programmer level proficiency in CUDA,
    then we recommend that you look at the CUDA API guide to understand more of these
    warp-level primitives.
  prefs: []
  type: TYPE_NORMAL
- en: This completes describing radix sort using warp-level primitives. Now, let's
    look at the Thrust-based library implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Approach 2 – Thrust-based radix sort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thrust-based radix sort is a generic implementation of radix sort and works
    pretty well for different types of data, such as integer, float, or key-value
    pairs. We would like to reemphasize the fact that sorting is a heavily studied
    algorithm and so has its parallel implementation. Therefore, we recommend that
    you reuse existing libraries before implementing one on your own.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps involved in making use of Thrust for radix sort are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant header files (Thrust is a header-only library, similar
    to STL):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare and initialize a device vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform sorting on the initialized device vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Using this library provides an easier and robust approach. Thrust provides different
    types of sorting methods, including radix sort for integers and floats. Alternatively,
    you can create a custom comparator to do customized sortings, such as sorting
    all the event numbers followed by odd numbers, sorting in descending order, and
    so on. You are advised to look at sample examples that have been provided by CUDA
    if you want to learn more about Thrust-based sorting examples.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have looked at both approaches to implementing radix sort on GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at the implementation of commonly used algorithms
    and patterns in CUDA. These algorithms and patterns are commonly available. We
    covered basic optimization techniques in matrix multiplication and convolution
    filtering. Then, we expanded our discussion on how to parallelize the problem
    by using prefix sum, N-body, histogram, and sorting. To do this, we have used
    dedicated GPU knowledge, libraries, and lower-level primitives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the algorithms we have covered are implemented in CUDA libraries. For
    example, matrix multiplication is in the cuBLAS library, while convolution is
    in the CUDNN library. In addition, we have covered two approaches in the radix
    sort implementation: using the Thrust library or warp-level primitives for histogram
    computation.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you've seen how these patterns can be implemented in commonly used
    libraries, the next logical step is to see how we can use these libraries. This
    is what we will be doing in the next chapter.
  prefs: []
  type: TYPE_NORMAL
