- en: Miscellaneous Topics
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at several topics that don't fit within the categories
    that we discussed in the previous chapters of this book. Most of these topics
    are concerned with different ways to facilitate computing and otherwise optimize
    the execution of our code. Others concern working with specific kinds of data
    or file formats.
  prefs: []
  type: TYPE_NORMAL
- en: In the first two recipes, we will cover packages that help keep track of units
    and uncertainties in calculations. These are very important for calculations that
    concern data that have a direct physical application. In the next recipe, we will
    look at loading and storing data from NetCDF files. NetCDF is a file format usually
    used for storing weather and climate data. (NetCDF stands for **network common
    data form**.) In the fourth recipe, we'll discuss working with geographical data,
    such as data that might be associated with weather or climate data. After that,
    we'll discuss how we can run Jupyter notebooks from the terminal without having
    to start up an interactive session. The next two recipes deal with validating
    data and working with data streamed from a Kafka server. Our final two recipes
    deal with two different ways we can accelerate our code using tools such as Cython
    and Dask.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Keeping track of units with Pint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accounting for uncertainties in calculations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading and storing data from NetCDF files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with geographical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing Jupyter notebooks as a script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with data streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accelerating code with Cython
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributing computation with Dask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires many different packages due to the nature of the recipes
    it contains. The list of packages we need is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Pint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: uncertainties
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NetCDF4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: xarray
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GeoPandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geoplot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papermill
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cerberus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cython
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All of these packages can be installed using your favorite package manager,
    such as `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To install the Dask package, we need to install the various extras associated
    with the package. We can do this using the following `pip` command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In addition to these Python packages, we will also need to install some supporting
    software. For the *Working with geographical data* recipe, the GeoPandas and Geoplot
    libraries have numerous lower-level dependencies that might need to be installed
    separately. Detailed instructions are given in the GeoPandas package documentation
    at [https://geopandas.org/install.html](https://geopandas.org/install.html).
  prefs: []
  type: TYPE_NORMAL
- en: For the *Working with streaming data* recipe, we will need to install the Kafka
    server. Detailed instructions on how to install and run a Kafka server can be
    found on the Apache Kafka documentation pages at [https://kafka.apache.org/quickstart](https://kafka.apache.org/quickstart).
  prefs: []
  type: TYPE_NORMAL
- en: For the *Accelerating code with Cython* recipe, we will need to have a C compiler
    installed. Instructions on how to obtain the **GNU C compiler** (**GCC**) are
    given in the Cython documentation at [https://cython.readthedocs.io/en/latest/src/quickstart/install.html](https://cython.readthedocs.io/en/latest/src/quickstart/install.html).
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter can be found in the `Chapter 10` folder of the GitHub
    repository at [https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2010](https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2010).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action: [https://bit.ly/2ZMjQVw](https://bit.ly/2ZMjQVw).'
  prefs: []
  type: TYPE_NORMAL
- en: Keeping track of units with Pint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Correctly keeping track of units in calculations can be very difficult, particularly
    if there are places where different units can be used. For example, it is very
    easy to forget to convert between different units – feet/inches into meters –
    or metric prefixes – converting 1 km into 1,000 m, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll learn how to use the Pint package to keep track of units
    of measurement in calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we need the Pint package, which can be imported as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps show you how to use the Pint package to keep track of units
    in calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create a `UnitRegistry` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a quantity with a unit, we multiply the number by the appropriate
    attribute of the registry object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can change the units of the quantity using one of the available conversion
    methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of these `print` statements is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We wrap a routine to make it expect an argument in seconds and output a result
    in meters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we call the `calc_depth` routine with a minute unit, it is automatically
    converted into seconds for the calculation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Pint package provides a wrapper class for numerical types that adds unit
    metadata to the type. This wrapper type implements all the standard arithmetic
    operations and keeps track of the units throughout these calculations. For example,
    when we divide a length unit by a time unit, we will get a speed unit. This means
    that you can use Pint to make sure the units are correct after a complex calculation.
  prefs: []
  type: TYPE_NORMAL
- en: The `UnitRegistry` object keeps track of all the units that are present in the
    session and handles things such as conversion between different unit types. It
    also maintains a reference system of measurements, which in this recipe is the
    standard international system with meters, kilograms, and seconds as base units,
    denoted `mks`.
  prefs: []
  type: TYPE_NORMAL
- en: The `wrap` functionality allows us to declare the input and output units of
    a routine, which allows Pint to do automatic unit conversions for the input function
    – in this recipe, we converted from minutes into seconds. Trying to call a wrapped
    function with a quantity that does not have an associated unit, or an incompatible
    unit, will raise an exception. This allows runtime validation of parameters and
    automatic conversion into the correct units for a routine.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Pint package comes with a large list of preprogrammed units of measurement
    that cover most globally used systems. Units can be defined at runtime or loaded
    from a file. This means that you can define custom units or systems of units that
    are specific to the application that you are working with.
  prefs: []
  type: TYPE_NORMAL
- en: Units can also be used within different contexts, which allows for easy conversion
    between different unit types that would ordinarily be unrelated. This can save
    a lot of time in situations where you need to move between units fluidly at multiple
    points in a calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Accounting for uncertainty in calculations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most measuring devices are not 100% accurate and instead are accurate up to
    a certain amount, usually somewhere between 0 and 10%. For instance, a thermometer
    might be accurate to 1%, while a pair of digital calipers might be accurate up
    to 0.1%. The true value in both of these cases is unlikely to be exactly the reported
    value, although it will be fairly close. Keeping track of the uncertainty in a
    value is difficult, especially when you have multiple different uncertainties
    combined in different ways. Rather than keeping track of this by hand, it is much
    better to use a consistent library to do this for you. This is what the `uncertainties`
    package does.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to quantify the uncertainty of variables and
    see how these uncertainties propagate through a calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need the `uncertainties` package, from which we will
    import the `ufloat` class and the `umath` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps show you how to quantify uncertainty on numerical values
    in calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create an uncertain float value of `3.0` plus or minus `0.4`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we perform a calculation involving this uncertain value to obtain a new
    uncertain value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a new uncertain float value and apply the `sqrt` routine from
    the `umath` module in the reverse of the previous calculation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `ufloat` class wraps around `float` objects and keeps track of the uncertainty
    throughout calculations. The library makes use of linear error propagation theory,
    which uses derivatives of non-linear functions, to estimate the propagated error
    during calculations. The library also correctly handles correlation so that subtracting
    a value from itself gives 0 with no error.
  prefs: []
  type: TYPE_NORMAL
- en: To keep track of uncertainties in standard mathematical functions, you need
    to use the versions that are provided in the `umath` module, rather than those
    defined in the Python Standard Library or in a third-party package such as NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `uncertainties` package provides support for NumPy, and the Pint package
    mentioned in the previous recipe can be combined with uncertainties to make sure
    that units and error margins are correctly attributed to the final value of a
    calculation. For example, we could compute the units in the calculation from *step
    2* of this recipe, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the `print` statement on the last line gives us `44+/-12 meter`,
    as we expect.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and storing data from NetCDF files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many scientific applications require that we start large quantities of multi-dimensional
    data in a robust format. NetCDF is one example of a format used for data that's
    developed by the weather and climate industry. Unfortunately, the complexity of
    the data means that we can't simply use the utilities from the Pandas package,
    for example, to load this data for analysis. We need the `netcdf4` package to
    be able to read and import the data into Python, but we also need to use `xarray`.
    Unlike the Pandas library, `xarray` can handle higher-dimensional data while still
    providing a Pandas-like interface.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to load data from and store data in NetCDF
    files.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need to import the NumPy package as `np`, the Pandas
    package as `pd`, the Matplotlib `pyplot` module as `plt`, and an instance of the
    default random number generator from NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to import the `xarray`package under the alias`xr`. You will also
    need to install the Dask package, as described in the *Technical requirements*
    section, and the NetCDF4 package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We don't need to import either of these packages directly.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to load and store sample data in a NetCDF file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create some random data. This data consists of a range of
    dates, a list of location codes, and randomly generated numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create an xarray `Dataset` object containing the data. The dates and
    locations are indexes, while the `steps` and `accumulated` variables are the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the `print` statement is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compute the mean over all the locations at each time index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we plot the mean accumulated values on a new set of axes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6481c4b7-78d8-474c-b1fe-e2e780d25381.png)Figure 10.1: Plot of accumulated
    means over time'
  prefs: []
  type: TYPE_NORMAL
- en: 'Save this dataset into a new NetCDF file using the `to_netcdf` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can load the newly created NetCDF file using the `load_dataset` routine
    from `xarray`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `xarray` package provides the `DataArray` and`DataSet` classes, which are
    (roughly speaking) multi-dimensional equivalents of the Pandas `Series` and `DataFrame`
    objects. We're using a dataset in this example because each index – a tuple of
    a date and location – has two pieces of data associated with it. Both of these
    objects expose a similar interface to their Pandas equivalents. For example, we
    can compute the mean along one of the axes using the `mean` method. The `DataArray`
    and `DataSet` objects also have a convenience method for converting into a Pandas
    `DataFrame` called `to_dataframe`. We used it in this recipe to convert to a `DataFrame`
    for plotting, which isn't really necessary because `xarray` has plotting features
    built into it.
  prefs: []
  type: TYPE_NORMAL
- en: The real focus of this recipe is on the `to_netcdf` method and the `load_dataset`
    routine. The former stores a `DataSet` in NetCDF format file. This requires the
    NetCDF4 package to be installed as it allows us to access the relevant C library
    for decoding NetCDF formatted files. The `load_dataset` routine is a general-purpose
    routine for loading data into a `DataSet` object from various file formats, including
    NetCDF (again, this requires the NetCDF4 package to be installed).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `xarray` package has support for a number of data formats in addition to
    NetCDF, such as OPeNDAP, Pickle, GRIB, and other formats that are supported by
    Pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Working with geographical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many applications involve working with geographical data. For example, when
    tracking global weather, we might want to plot the temperature as measured by
    various sensors around the world at their position on a map. For this, we can
    use the GeoPandas package and the Geoplot package, both of which allow us to manipulate,
    analyze, and visualize geographical data.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use the GeoPandas and Geoplot packages to load and visualize
    some sample geographical data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need the GeoPandas package, the Geoplot package, and
    the Matplotlib `pyplot` package imported as `plt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to create a simple plot of the capital cities plotted on
    a map of the world using sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to load the sample data from the GeoPandas package, which contains
    the world geometry information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to load the data containing the name and position of each of
    the capital cities of the world:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create a new figure and plot the outline of the world geometry
    using the `polyplot` routine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we use the `pointplot` routine to add the positions of the capital
    cities on top of the world map. We also set the axes limits to make the whole
    world visible:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot of the positions of the capital cities of the world looks
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c3ed4017-aab6-4ab8-9c90-1225fd886f1b.png)Figure 10.2: Plot of the
    world''s capital cities on a map'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The GeoPandas package is an extension of Pandas that works with geographical
    data, while the Geoplot package is an extension of Matplotlib that's used to plot
    geographical data. The GeoPandas package comes with a selection of sample datasets
    that we used in this recipe. `naturalearth_lowres` contains geometric figures
    that describe the boundaries of countries in the world. This data is not very
    high resolution, as signified by its name, which means that some of the finer
    details of geographical features might not be present on the map. (Some small
    islands are not shown at all.) `naturalearth_cities` contains the names and locations
    of the capital cities of the world. We're using the `datasets.get_path` routine
    to retrieve the path for these datasets in the package data directory. The `read_file`
    routine imports the data into the Python session.
  prefs: []
  type: TYPE_NORMAL
- en: The Geoplot package provides some additional plotting routines specifically
    for plotting geographical data. The `polyplot` routine plots polygonal data from
    a GeoPandas DataFrame, which might describe the geographical boundaries of a country.
    The `pointplot` routine plots discrete points on a set of axes from a GeoPandas
    DataFrame, which in this case describe the position of capital cities.
  prefs: []
  type: TYPE_NORMAL
- en: Executing a Jupyter notebook as a script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jupyter notebooks are a popular medium for writing Python code for scientific
    and data-based applications. A Jupyter notebook is really a sequence of blocks
    that are stored in a file in **JavaScript Object Notation** (**JSON**) with the
    `ipynb` extension. Each block can be one of several different types, such as code
    or markdown. These notebooks are typically accessed through a web application
    that interprets the blocks and executes the code in a background kernel that then
    returns the results to the web application. This is great if you are working on
    a personal PC, but what if you want to run the code contained within a notebook
    remotely on a server? In this case, it might not even be possible to access the
    web interface provided by the Jupyter notebook software. The papermill package
    allows us to parameterize and execute notebooks from the command line.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll learn how to execute a Jupyter notebook from the command
    line using papermill.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we will need to have the papermill package installed, and also
    have a sample Jupyter notebook in the current directory. We will use the `sample.ipynb`
    notebook file stored in the code repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to use the papermill command-line interface to execute a
    Jupyter notebook remotely:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we open the sample notebook, `sample.ipynb`, from the code repository
    for this chapter. The notebook contains three code cells that hold the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we open the folder containing the Jupyter notebook in the terminal and
    use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we open the output file, `output.ipynb`, which should now contain the
    notebook that''s been updated with the result of the executed code. The scatter
    plot that''s generated in the final block is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/05f0392f-a1b4-4448-b26a-2e3fb8d57bb7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Scatter plot of the random data that was generated inside a Jupyter
    notebook, executed remotely using papermill'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The papermill package provides a simple command-line interface that interprets
    and then executes a Jupyter notebook and then stores the results in a new notebook
    file. In this recipe, we gave the first argument – the input notebook file –`sample.ipynb`
    and the second argument – the output notebook file –`output.ipynb`. The tool then
    executes the code contained in the notebook and produces the output. The notebook's
    file format keeps track of the results of the last run, so these results are added
    to the output notebook and stored at the desired location. In this recipe, this
    is a simple local file, but papermill can also store to a cloud location such
    as **Amazon Web Services** (**AWS**) S3 storage or Azure data storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *step 2,* we added the `--kernel python3` option when using the papermill
    command-line interface. This option allows us to specify the kernel that is used
    to execute the Jupyter notebook. This might be necessary to prevent errors if
    papermill tries to execute the notebook with a different kernel than the one used
    to write the notebook. A list of available kernels can be found by using the following
    command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: If you get an error when executing a notebook, you could try changing to a different
    kernel.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Papermill also has a Python interface so that you can execute notebooks from
    within a Python application. This might be useful for building web applications
    that need to be able to perform long-running calculations on external hardware
    and where the results need to be stored in the cloud. It also has the ability
    to provide parameters to a notebook. To do this, we need to create a block in
    the notebook marked with the parameters tag with the default values. Updated parameters
    can then be provided through the command-line interface using the `-p` flag, followed
    by the name of the argument and the value.
  prefs: []
  type: TYPE_NORMAL
- en: Validating data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is often presented in a raw form and might contain anomalies or incorrect
    or malformed data, which will obviously present a problem for later processing
    and analysis. It is usually a good idea to build a validation step into a processing
    pipeline. Fortunately, the Cerberus package provides a lightweight and easy to
    use validation tool for Python.
  prefs: []
  type: TYPE_NORMAL
- en: For validation, we have to define a *schema*, which is a technical description
    of what the data should look like and the checks that should be performed on the
    data. For example, we can check the type and place bounds of the maximum and minimum
    values. Cerberus validators can also perform type conversions during the validation
    step, which allows us to plug data loaded directly from CSV files into the validator.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use Cerberus to validate data loaded from
    a CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we need to import the `csv` module from the Python Standard
    Library, as well as the Cerberus package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We will also need the `sample.csv` file from the code repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following steps, we will validate a set of data that''s been loaded
    from CSV using the Cerberus package:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to build a schema that describes the data we expect. To do this,
    we must define a simple schema for floating-point numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we build the schema for individual items. These will be the rows of our
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can define the schema for the whole document, which will contain a
    list of items:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a `Validator` object with the schema we just defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we load the data using a `DictReader` from the `csv` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use the `validate` method on the `Validator` to validate the document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we retrieve the errors from the validation process from the `Validator`
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can print any error messages that appeared:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the error messages is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The schema that we created is a technical description of all the criteria that
    we need to check against our data. This will usually be defined as a dictionary
    with the name of the item as the key and a dictionary of properties, such as the
    type or bounds on the value in a dictionary, as the value. For example, in *step
    1*, we defined a schema for floating-point numbers that limits the numbers so
    that they're between the values of -1 and 1\. Note that we include the `coerce`
    key, which specifies the type that the value should be converted into during the
    validation. This allows us to pass in data that's been loaded from a CSV document,
    which contains only strings, without having to worry about its type.
  prefs: []
  type: TYPE_NORMAL
- en: The `Validator` object takes care of parsing documents so that they're validated
    and checking the data they contain against all the criteria described by the schema.
    In this recipe, we provided the schema to the `Validator` object when it was created.
    However, we could also pass the schema into the `validate` method as a second
    argument. The errors are stored in a nested dictionary that mirrors the structure
    of the document.
  prefs: []
  type: TYPE_NORMAL
- en: Working with data streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some data is received in a constant stream from various sources. For example,
    we might have a situation where multiple temperature probes are reporting values
    at set intervals via a Kafka server. Kafka is a streaming data message broker
    that passes messages to different processing agents based on topics.
  prefs: []
  type: TYPE_NORMAL
- en: Processing streaming data is the perfect application for asynchronous Python.
    This allows us to process larger quantities of data concurrently, which could
    be very important in applications. Of course, we can't directly perform long-running
    analysis on this data in an asynchronous context, since this will interfere with
    the execution of the event loop.
  prefs: []
  type: TYPE_NORMAL
- en: For working with Kafka streams using Python's asynchronous programming features,
    we can use the Faust package. This package allows us to define asynchronous functions
    that will act as processing agents or services that can process or otherwise interact
    with a stream of data from a Kafka server.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use the Faust package to process a stream
    of data from a Kafka server.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike most of the recipes in this book, this recipe cannot be run in a Jupyter
    notebook since we will run the resulting app from the command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this recipe, we will need to import the Faust package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also need an instance of the default random number generator from the
    NumPy package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We will also need to run an instance of a Kafka service on our local machine
    so that our Faust application can interact with the message broker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have downloaded Kafka and decompressed the downloaded source, navigate
    to the folder that the Kafka application can be found in. Open this folder in
    the terminal. Start the ZooKeeper server using the following command for Linux
    or Mac:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'If you''re on Windows, use the following command instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in a new terminal, launch the Kafka server using the following command
    for Linux or Mac:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'If you''re on Windows, use the following command instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In each terminal, you should see some logging information that will indicate
    that the server is running.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to create a Faust app that will read (and write) data to
    a Kafka server and do some simple processing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create a Faust `App` instance that will act as the interface
    between Python and the Kafka server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create a record type that mimics the data we expect from the
    server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''ll add a topic to the Faust `App` object that sets the value type
    to the `Record` class that we just defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define an agent, which is an asynchronous function wrapped in the `agent`
    decorator on the `App` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define two source functions that will publish records to the Kafka
    server on the sample topic we set up. These are asynchronous functions wrapped
    in the `timer` decorator with an appropriate interval set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'At the bottom of the file, we start the application''s `main` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in a new terminal, we can use the following command to start a worker
    for the application (assuming our application is stored in `working-with-data-streams.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'At this stage, you should see some output that''s been generated by the agent
    printed into the terminal, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: This will be below some application information that's been generated by Faust.
  prefs: []
  type: TYPE_NORMAL
- en: Press *Ctrl* + *C* to close the worker and make sure to close both the Kafka
    server and the Zookeeper server in the same way.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a very basic example of a Faust application. Ordinarily, we wouldn't
    generate the records and send them through the Kafka server and process them within
    the same app. However, this is fine for the purposes of this demonstration. In
    a production environment, we'd probably connect to a remote Kafka server that
    is connected to multiple sources and publishing to multiple different topics simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: The Faust app controls the interaction between the Python code and the Kafka
    server. We use the `agent` decorator to add a function to process information
    published to a particular channel. This asynchronous function will be executed
    each time new data is pushed to the sample topic. In this recipe, the agent that
    we defined simply prints the information contained within the `Record` objects
    into the terminal.
  prefs: []
  type: TYPE_NORMAL
- en: The `timer` decorator defines a service that regularly performs some action
    at a specified interval. In our case, the timer sends a message to the Kafka server
    through the `App` object. These messages are then pushed to the agent for processing.
  prefs: []
  type: TYPE_NORMAL
- en: The Faust command-line interface is used to start a worker process running the
    application. These workers are what actually perform the processing in reaction
    to events on the Kafka server or locally in the process, such as the timer services
    defined in this recipe. Larger applications might use several worker processes
    in order to cope with vast quantities of data.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Faust documentation provides far more details about the capabilities of
    Faust, along with various alternatives to Faust: [https://faust.readthedocs.io/en/latest/](https://faust.readthedocs.io/en/latest/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'More information about Kafka can be found on the Apache Kafka website: [https://kafka.apache.org/](https://kafka.apache.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: Accelerating code with Cython
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python is often criticized for being a slow programming language – a statement
    that is endlessly debatable. Many of these criticisms can be addressed by using
    a high-performance compiled library with a Python interface – such as the scientific
    Python stack – to greatly improve performance. However, there are some situations
    where it is difficult to avoid the fact that Python is not a compiled language.
    One way to improve performance in these (fairly rare) situations is to write a
    C extension (or even rewrite the code entirely in C) to speed up the critical
    parts. This will certainly make the code run faster, but it might make it more
    difficult to maintain the package. Instead, we can use Cython, which is an extension
    of the Python language that is transpiled into C and compiled for great performance
    improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can consider some code that''s used to generate an image of
    the Mandelbrot set. For comparison, the pure Python code – which we assume is
    our starting point – is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason why this code is relatively slow in pure Python is fairly obvious:
    the nested loops. For demonstration purposes, let''s assume that we can''t vectorize
    this code using NumPy. A little preliminary testing shows that using these functions
    to generate the Mandelbrot set using 320 × 240 points and 255 steps takes approximately
    6.3 seconds. Your times may vary, depending on your system.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use Cython to greatly improve the performance of the
    preceding code in order to generate an image of the Mandelbrot set.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we will need the NumPy package and the Cython package to be
    installed. You will also need a C compiler such as GCC installed on your system.
    For example, on Windows, you can obtain a version of GCC by installing MinGW.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to use Cython to greatly improve the performance of the
    code for generating an image of the Mandelbrot set:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a new file called `cython_mandel.pyx` in the `mandelbrot` folder. In
    this file, we will add some simple imports and type definitions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a new version of the `in_mandel` routine using the Cython syntax.
    We add some declarations to the first few lines of this routine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The rest of the function is identical to the Python version of the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a new version of the `compute_mandel` function. We add two
    decorators to this function from the Cython package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the constants, just as we did in the original routine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `linspace` and `empty` routines from the NumPy package in exactly
    the same way as in the Python version. The only addition here is that we declare
    the `i` and `j` variables, which are of the `Int` type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The remainder of the definition is exactly the same as in the Python version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a new file called `setup.py` in the `mandelbrot` folder and
    add the following imports to the top of this file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we define an extension module with the source pointing to the original
    `python_mandel.py` file. Set the name of this module to `hybrid_mandel`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define a second extension module with the source set as the `cython_mandel.pyx`
    file that we just created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we add both these extension modules to a list and call the `setup` routine
    to register these modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Create a new empty file called `__init__.py` in the `mandelbrot` folder to make
    this into a package that can be imported in Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the terminal inside the `mandelbrot` folder and use the following command
    to build the Cython extension modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, start a new file called `run.py` and add the following import statements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the various `compute_mandel` routines from each of the modules we have
    defined: `python_mandel` for the original; `hybrid_mandel` for the Cythonized
    Python code; and `cython_mandel` for the compiled pure Cython code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a simple timer decorator that we will use to test the performance of
    the routines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply the `timer` decorator to each of the imported routines, and define some
    constants for testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Run each of the decorated routines with the constants we set previously. Record
    the output of the final call (the Cython version) in the `vals` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, plot the output of the Cython version to check that the routine computes
    the Mandelbrot set correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the `run.py` file will print the execution time of each of the routines
    to the terminal, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot of the Mandelbrot set can be seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c53ef857-15af-45ab-9dc1-6cd07339285a.png)Figure 10.4: Image of the
    Mandelbrot set computed using Cython code'
  prefs: []
  type: TYPE_NORMAL
- en: This is what we expect for the Mandelbrot set.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a lot happening in this recipe, so let's start by explaining the overall
    process. Cython takes code that is written in an extension of the Python language
    and compiles it into C code, which is then used to produce a C extension library
    that can be imported into a Python session. In fact, you can even use Cython to
    compile ordinary Python code directly to an extension, although the results are
    not as good as when using the modified language. The first few steps in this recipe
    define the new version of the Python code in the modified language (saved as a
    `.pyx` file), which includes type information in addition to the regular Python
    code. In order to build the C extension using Cython, we need to define a setup
    file, and then we create a file that we run to produce the results.
  prefs: []
  type: TYPE_NORMAL
- en: The final compiled version of the Cython code runs considerably faster than
    its Python equivalent. The Cython compiled Python code (hybrid, as we called it
    in this recipe) performs slightly better than the pure Python code. This is because
    the produced Cython code still has to work with Python objects with all of their
    caveats. By adding the typing information to the Python code, in the `.pyx` file,
    we start to see major improvements to performance. This is because the `in_mandel`
    function is now effectively defined as a C-level function that has no interaction
    with Python objects, and instead operates on primitive data types.
  prefs: []
  type: TYPE_NORMAL
- en: There are some small, but very important differences, between the Cython code
    and the Python equivalent. In *step 1*, you can see that we imported the NumPy
    package as usual but that we also used the `cimport` keyword to bring some C-level
    definitions into the scope. In *step 2*, we used the `cdef` keyword instead of
    the `def` keyword when we defined the `in_mandel` routine. This means that the
    `in_mandel` routine is defined as a C-level function that cannot be used from
    the Python level, which saves a significant amount of overhead when calling this
    function (which happens a lot).
  prefs: []
  type: TYPE_NORMAL
- en: The only other real differences regarding the definition of this function are
    the inclusion of some type declarations in the signature and in the first few
    lines of the function. The two decorators we applied here disable the checking
    of bounds when accessing elements from a list (array). The `boundscheck` decorator
    disables checking if the index is valid (between 0 and the size of the array),
    while the `wraparound` decorator disables the negative indexing. Both of these
    give a modest improvement to speed during execution, although they disable some
    of the safety features built into Python. In this recipe, it is OK to disable
    these checks because we are using a loop over the valid indices of the array.
  prefs: []
  type: TYPE_NORMAL
- en: The setup file is where we tell Python (and therefore Cython) how to build the
    C extension. The `cythonize` routine from Cython is the key here as it triggers
    the Cython build process. In *steps 9* and *10,* we defined extension modules
    using the `Extension` class from `setuptools` so that we could define some extra
    details for the build; specifically, we set an environment variable for the NumPy
    compilation and added the `include` files for the NumPy C headers. This is done
    via the `define_macros` keyword argument for the `Extension` class. The terminal
    command we used in *step 13* uses `setuptools` to build the Cython extensions,
    and the addition of the `--inplace` flat means that the compiled libraries will
    be added to the current directory, rather than being placed in a centralized location.
    This is good for development.
  prefs: []
  type: TYPE_NORMAL
- en: 'The run script is fairly simple: import the routines from each of the defined
    modules – two of these are actually C extension modules – and time their execution.
    We have to be a little creative with the import aliases and routine names to avoid
    collisions.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cython is a powerful tool for improving the performance of some aspects of your
    code. However, you must always be careful to spend your time wisely while optimizing
    code. Using a profile such as the cProfiler that is provided in the Python Standard
    Library can be used to find the places where performance bottlenecks occur in
    your code. In this recipe, it was fairly obvious where the performance bottleneck
    occurs. Cython is a good remedy to the problem in this case because it involves
    repetitive calls to a function inside a (double) `for` loop. However, it is not
    a universal fix for performance issues and, more often than not, the performance
    of code can be greatly improved by refactoring it so that it makes use of high-performance
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Cython is well integrated with Jupyter notebooks and can be used seamlessly
    in the code blocks of a notebook. Cython is also included in the Anaconda distribution
    of Python, so no additional setup is required for using Cython with Jupyter notebooks
    when it's been installed using the Anaconda distribution.
  prefs: []
  type: TYPE_NORMAL
- en: There are alternatives to Cython when it comes to producing compiled code from
    Python. For example, the NumBa package ([http://numba.pydata.org/](http://numba.pydata.org/))
    provides a **just in time** (**JIT**) compiler that optimizes Python code at runtime
    by simply placing a decorator on specific functions. NumBa is designed to work
    with NumPy and other scientific Python libraries and can also be used to leverage
    GPUs to accelerate code.
  prefs: []
  type: TYPE_NORMAL
- en: Distributing computing with Dask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dask is a library that's used for distributing computing across multiple threads,
    processes, or even computers in order to effectively perform computation at a
    huge scale. This can greatly improve performance and throughput, even if you are
    working on a single laptop computer. Dask provides replacements for most of the
    data structures from the Python scientific stack, such as NumPy arrays and Pandas
    DataFrames. These replacements have very similar interfaces, but under the hood,
    they are built for distributed computing so that they can be shared between multiple
    threads, processes, or computers. In many cases, switching to Dask is as simple
    as changing the `import` statement, and possibly adding a couple of extra method
    calls to start concurrent computations.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use Dask to do some simple computations
    on a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need to import the `dataframe` module from the Dask
    package. Following the convention set out in the Dask documentation, we will import
    this module under the alias `dd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: We will also need the `sample.csv` file from the code repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to use Dask to perform some computations on a DataFrame
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to load the data from `sample.csv` into a Dask `DataFrame`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we perform a standard calculation on the columns of the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Unlike with Pandas DataFrames, the result is not a new DataFrame. The `print`
    statement gives us the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'To actually get the result, we need to use the `compute` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is now shown as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'We compute the means of the final two columns in exactly the same way we would
    with a Pandas DataFrame, but we need to add a call to the `compute` method to
    execute the calculation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The result, as printed, is exactly as we expect it to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dask builds a *task graph* for the computation, which describes the relationships
    between the various operations and calculations that need to be performed on the
    collection of data. This breaks down the steps of the calculation so that calculations
    can be done in the right order across the different workers. This task graph is
    then passed into a scheduler that sends the actual tasks to the workers for execution.
    Dask comes with several different schedulers: synchronous, threaded, multiprocessing,
    and distributed. The type of scheduler can be chosen in the call to the `compute`
    method or set globally. Dask will choose a sensible default if one is not given.'
  prefs: []
  type: TYPE_NORMAL
- en: The synchronous, threaded, and multiprocessing schedulers work on a single machine,
    while the distributed scheduler is for working with a cluster. Dask allows you
    to change between schedulers in a relatively transparent way, although for small
    tasks, you might not get any performance benefits because of the overhead of setting
    up more complicated schedulers.
  prefs: []
  type: TYPE_NORMAL
- en: The `compute` method is the key to this recipe. The methods that would ordinarily
    perform the computation on Pandas DataFrames now just set up a computation that
    is to be executed through the Dask scheduler. The computation isn't started until
    the `compute` method is called. This is similar to the way that a `Future` is
    returned as a proxy for the result of an asynchronous function call, which isn't
    fulfilled until the computation is complete.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dask provides interfaces for NumPy arrays, as well as the DataFrames shown in
    this recipe. There is also a machine learning interface called `dask_ml` that
    exposes similar capabilities to the scikit-learn package. Some external packages,
    such as `xarray`, also have a Dask interface. Dask can also work with GPUs to
    further accelerate computations and load data from remote sources, which is useful
    if the computation is distributed across a cluster.
  prefs: []
  type: TYPE_NORMAL
