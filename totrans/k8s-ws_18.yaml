- en: 18\. Upgrading Your Cluster without Downtime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss how to upgrade your cluster without downtime.
    We will first understand the need to keep your Kubernetes cluster up to date.
    Then, we will understand basic application deployment strategies that can help
    zero-downtime upgrades of the Kubernetes cluster. We will then put these strategies
    into action by performing an upgrade on a Kubernetes cluster with no downtime
    for your application.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned how to set up a multi-node Kubernetes platform on AWS using kops
    in *Chapter 11*, *Build Your Own HA Cluster*. In this chapter, you will learn
    about upgrading the Kubernetes platform to a new version. We will walk you through
    hands-on examples of the steps that are required to upgrade the Kubernetes platform.
    These exercises will also equip you with the skills required to maintain a Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Different organizations set up and maintain their Kubernetes clusters in different
    ways. You saw in *Chapter 12*, *Your Application and HA*, that there are numerous
    ways to set up a cluster. We will present a simple technique to upgrade your cluster
    and, depending on the cluster you are dealing with, the exact techniques and steps
    that you will need to take for upgrading may be different, although the basic
    principles and precautions that we will mention here will be applicable regardless
    of how you go about upgrading your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The Need to Upgrade Your Kubernetes Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building up your business application and putting it out in the world is only
    half the game. Making your application usable by customers in a secure, scalable,
    and consistent way is the other half and the one that you have to keep working
    on. To be able to execute this other half well, you need a rock-solid platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'In today''s highly competitive environment, delivery of the latest features
    to customers in a timely manner is important to give your business an edge. This
    platform has to not only be dependable but also provide new and updated features
    to keep up with the demands of running modern applications. Kubernetes is a fast-moving
    platform and is well suited for such a dynamic environment. The pace of development
    and advancement of Kubernetes is evidenced by the number of commits in the official
    Kubernetes GitHub repository. Let''s take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.1: Daily commits to the Kubernetes project during the period August
    25–31, 2019'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.1: Daily commits to the Kubernetes project during the period August
    25–31, 2019'
  prefs: []
  type: TYPE_NORMAL
- en: The orange bar graph represents the commits per week and, as you can see, they
    are averaging over 100 per week. The green line graph underneath shows the commits
    for the week of August 25 through August 31\. That's more than 50 commits just
    on a Tuesday.
  prefs: []
  type: TYPE_NORMAL
- en: 'By now, it''s clear that Kubernetes is advancing at a fast pace, but you may
    still be unsure about whether you need to update the version of Kubernetes on
    your cluster. The following are some of the reasons why it is important to keep
    the platform up to date:'
  prefs: []
  type: TYPE_NORMAL
- en: '**New features**: The Kubernetes community is continuously adding new features
    to satisfy the needs of modern applications. Your software team may come up with
    a new software component that may be dependent on a newer Kubernetes feature.
    Thus, sticking to an older version of Kubernetes will hold back the development
    of *your* software.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security patches**: There are many moving parts in the Kubernetes platform.
    It has not only the Kubernetes binaries that need to be patched but also lots
    of Linux features, such as iptables and cgroups. If there are vulnerabilities
    in any of the components used by Kubernetes, you may need to patch the underlying
    component, such as the OS itself. Having a consistent way to upgrade is extremely
    important in keeping the Kubernetes ecosystem as secure as possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, there was a vulnerability in versions 1.0–1.12 of the Kubernetes
    API server that resulted in the API server possibly consuming lots of resources
    due to an invalid YAML or JSON payload. You can find more details about this vulnerability
    at this link: [https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-11253](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-11253)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Better handling of existing features**: The Kubernetes team not only adds
    new features but also keeps on improving existing features for stability and performance.
    These improvements may be useful for your existing applications or your automation
    scripts. So, keeping your platform updated is a good idea from this perspective,
    too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes Components – Refresher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By now, you are already aware of the basic components of the Kubernetes platform.
    Just as a refresher, let''s revisit the major components:'
  prefs: []
  type: TYPE_NORMAL
- en: The API server is responsible for exposing RESTful Kubernetes APIs and is stateless.
    All users on your cluster, Kubernetes master components, kubectl clients, worker
    nodes, and maybe even your application all need to interact with the API server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A key-value store (the etcd server) stores the objects and provides a persistent
    backend to the API server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scheduler and controller manager act to attain the state of the cluster
    and objects stored in etcd.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kubelet is a program that runs on every worker node and behaves like an agent
    to perform the work as directed by Kubernetes master components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we update the platform, as you will see in the later sections, we are going
    to utilize these components and upgrade them as separate modules.
  prefs: []
  type: TYPE_NORMAL
- en: A Word of Caution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes versions are marked as `A.B.C` and follow the semantic versioning
    concepts. `A` is the major version, `B` is the minor version, and `C` is the patch
    release. As per the Kubernetes documentation, "*in* [*highly available (HA) clusters*](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/)*,
    the newest and oldest kube-apiserver instances must be within one minor version.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the safest approach when planning your upgrade:'
  prefs: []
  type: TYPE_NORMAL
- en: Always upgrade to the latest patched release of your current minor version first.
    For example, if you are on `1.14.X`, first upgrade to the latest available version
    for the `1.14.X` release train. This will make sure that the platform has all
    the available fixes applied for the version of your cluster. The latest patch
    may have bug fixes, which might provide you with a smoother path toward the next
    minor version, which, in our example, would be `1.15.X`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrade to the next minor version. Avoid jumping over multiple minor versions,
    even if this is possible, as generally, API compatibility is within one minor
    release. During the upgrade, the Kubernetes platform will be running two different
    versions of an API because we upgrade one node at a time. For example, it is better
    to go from `1.14` to `1.15`, and not to `1.16`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another important thing to consider is to see whether the newer version needs
    some updated libraries from the underlying Linux OS. Although, in general, patch
    releases don't require any underlying component upgrades, keeping the underlying
    OS up to date should also be on top of your list to provide a safe and consistent
    environment for the Kubernetes platform.
  prefs: []
  type: TYPE_NORMAL
- en: The Upgrade Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will see the steps required to upgrade the Kubernetes platform.
    Note that upgrading the underlying OS is not covered here. To meet the requirement
    of zero-downtime upgrades, you must have an HA Kubernetes cluster with a minimum
    of three masters and etcd servers, which enables frictionless upgrades. The process
    will take one node out of the three and upgrade it. The upgraded component then
    will rejoin the cluster, and then we take the second node and apply the upgrade
    process to it. Since, at any given time, at least two of the servers are kept
    available, the cluster will remain available during the upgrade.
  prefs: []
  type: TYPE_NORMAL
- en: Some Considerations for kops
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have guided you through the creation of an HA Kubernetes cluster in *Chapter
    11*, *Build Your Own HA Cluster*. Hence, in this chapter, we will walk you through
    upgrading the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in that chapter, there are various ways of deploying and managing
    a Kubernetes cluster. We have opted for kops, which has built-in tools for upgrading
    Kubernetes components. We will be leveraging them in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The versioning of kops is set to be analogous to the minor version of Kubernetes
    it implements. For example, kops version `1.14.x` implements Kubernetes version
    `1.14.x`. For more details on this, please refer to this link: [https://kops.sigs.k8s.io/welcome/releases/](https://kops.sigs.k8s.io/welcome/releases/).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In the HA cluster we created in *Chapter 11*, *Build Your Own HA Cluster*, we
    deployed three master nodes, which host all the Kubernetes master plane components,
    including the etcd.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the Upgrade Process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The entire upgrade process can be diagrammatically summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.2: The recommended upgrade process'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.2: The recommended upgrade process'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a quick look at each step before we move on to the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Read the release notes**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These will indicate any special considerations that might be necessary during
    an upgrade. The release notes for each version are available on GitHub at this
    link: [https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG](https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Back up the etcd datastore**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you have learned earlier, etcd stores the entire state of the cluster. A
    backup of etcd would allow you to restore the state of your datastore, if needed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Back up the nodes as an optional failsafe**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This may come in handy if the upgrade process does not go as planned and you
    want to revert to a previous state. Cloud vendors (such as AWS, GCP, Azure, and
    others) enable you to take a snapshot of the hosts. If you are running in a private
    data center and using hypervisors for your machines, your hypervisor provider
    (for example, VMware) may provide tools to take snapshots of the nodes. Taking
    snapshots is beyond the scope of this book, but nonetheless, it is a useful step
    before you start upgrading your Kubernetes platform.
  prefs: []
  type: TYPE_NORMAL
- en: '**Upgrade the etcd if required**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The more recent versions of the tools used to deploy and manage a Kubernetes
    cluster (such as kops in our case) usually take care of this automatically. Even
    so, this is an important consideration, especially if you are not using any tools
    such as kops.
  prefs: []
  type: TYPE_NORMAL
- en: Check and verify whether the new version of Kubernetes needs a different version
    of the etcd store. This is not always necessary, but may be required depending
    on your version. For example, Kubernetes version `1.13` needs etcd v3, while prior
    versions work with etcd v2.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will know whether you need to upgrade etcd from reading the release notes
    (*step 1*). For example, when the earlier version of etcd was phased out in version
    1.13, it was explicitly mentioned in the release notes: [https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.13.md#urgent-upgrade-notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.13.md#urgent-upgrade-notes).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Upgrade the master components**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Log in to the bastion host and upgrade the version of kops based on the desired
    version of Kubernetes. This compatibility matrix should be a useful guide: [https://kops.sigs.k8s.io/welcome/releases/#compatibility-matrix](https://kops.sigs.k8s.io/welcome/releases/#compatibility-matrix).'
  prefs: []
  type: TYPE_NORMAL
- en: Run the upgrade on the first master node, verify that it is updated correctly,
    and then repeat the same steps for all other master nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Upgrade the worker node groups**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you have seen in *Chapter 11*, *Build Your Own HA Cluster*, kops allows you
    to manage the nodes using instance groups, which is tied to the autoscaling group,
    in the case of AWS. Run the upgrade on the first instance group of worker nodes.
    To verify that the nodes were successfully upgraded, you need to check that the
    nodes are upgraded to the desired version of Kubernetes and whether pods are scheduled
    on the upgraded nodes. Repeat the same steps for all other instance groups of
    worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Verify that the upgrade process succeeded**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check whether all the nodes are upgraded and all your applications are running
    as intended.
  prefs: []
  type: TYPE_NORMAL
- en: The Importance of Automation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you have seen from this overview, there are several steps required to upgrade
    the cluster. Given the number of releases and patches, you may need to do this
    often. Since the process is well documented, it is highly recommended that you
    consider using an automation tool, such as Ansible or Puppet, to automate this
    whole process. All the preceding steps can be fully automated, and you have a
    repeatable way to upgrade your cluster. Automation, however, will not be covered
    in this chapter as this is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Backing up the etcd Datastore
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: etcd stores the state of the entire cluster. So, taking a snapshot of etcd allows
    us to restore the entire cluster to the state when the snapshot was taken. This
    may come in handy if you want to revert the cluster to a previous state.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Before you begin with any exercises, make sure that the cluster is set up and
    available as per the instructions in *Chapter 11*, *Build Your Own HA Cluster*,
    and that you can access the nodes from your computer via SSH. It is also recommended
    that you take snapshots of the nodes before starting the upgrade process. This
    is especially beneficial because in this chapter, you will upgrade the cluster
    two times – once during the exercises and once during the activity.
  prefs: []
  type: TYPE_NORMAL
- en: Now, before we move on to the first exercise, we need to understand a bit more
    about etcd. The way that it works is that it runs as a pod on your cluster in
    the `kube-system` namespace (as you have seen in *Chapter 2*, *An Overview of
    Kubernetes*) and exposes an API, which is used to write data to it. Whenever the
    Kubernetes API server wants to persist any data to etcd, it will use etcd's API
    to access it.
  prefs: []
  type: TYPE_NORMAL
- en: 'For backing up etcd, we will also need to access its API and use a built-in
    function to save a snapshot. For that, we will use a command-line client called
    `etcdctl`, which is already present in the etcd pod. Detailed coverage of this
    tool and the etcd API is not necessary for our purposes and so we are not including
    it in this book. You can learn more about it at this link: [https://github.com/etcd-io/etcd/tree/master/etcdctl](https://github.com/etcd-io/etcd/tree/master/etcdctl).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how we can use etcdctl to back up etcd in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 18.01: Taking a Snapshot of the etcd Datastore'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will see how to take a snapshot of the etcd store. As
    mentioned in the previous section, a manual upgrade of etcd may not be required,
    depending on your upgrade path. However, backing up etcd is essential. For this,
    and all the following exercises and activities, use the same machine (your laptop
    or desktop) that you used to perform *Exercise 11.01*, *Setting Up Our Kubernetes
    Cluster*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have used kops to install the cluster. Kops uses two different etcd clusters
    – one for events generated by Kubernetes components, and the second one for everything
    else. You can see these pods by issuing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This should get the details of the etcd pods. You should see an output similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.3: Getting the list of etcd-manager pods'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.3: Getting the list of etcd-manager pods'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, kops'' `etcd-manager` function creates backups every 15 minutes.
    The location of the backups is the same S3 storage used by the kops tool. In *Exercise
    11.01*, you configured the S3 bucket to store kops'' state. Let''s query the bucket
    to see whether a backup is available there:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.4: Getting a list of available backups'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.4: Getting a list of available backups'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the backups are taken automatically every 15 minutes and timestamps
    of the backups are marked. We will use the `Key` of the latest backup, highlighted
    in the preceding screenshot, in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to get the backup from the S3 bucket. We can use AWS CLI commands
    to get the backup that we need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that this command contains the name of the bucket, the `Key` of the file
    from the previous step, and the filename that we want to use while saving the
    file. Use the `Key` that you get for your instance in the output of the previous
    step. You should see a response similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.5: Saving the etcd backup from our S3 bucket'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.5: Saving the etcd backup from our S3 bucket'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have used the `date` command to generate the filename. This is
    a very common technique used by system administrators to make sure that any files
    are not overwritten.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to recover your etcd instance using this backup, you can find the
    recovery instructions at this link: [https://kops.sigs.k8s.io/operations/etcd_backup_restore_encryption/](https://kops.sigs.k8s.io/operations/etcd_backup_restore_encryption/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that the backup file is created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.6: Confirming the saved etcd backup'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.6: Confirming the saved etcd backup'
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to see the snapshot that we created in the response.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you have seen how to generate a backup of the etcd datastore.
    This backup is the state of Kubernetes and could be useful not only if your upgrade
    is hit by any issues, but also to restore the cluster for any other reason, such
    as **Disaster Recovery** (**DR**) scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Draining a Node and Making It Non-Schedulable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we start to upgrade any nodes (master or worker), we need to make sure
    that no pods (including the pods for the master components) are running on this
    node. This is an important step to prepare any node to be upgraded. Furthermore,
    the node needs to be marked as unschedulable. An unschedulable node is a flag
    for the scheduler to not schedule any pods in this node.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the `drain` command to mark the node as un-schedulable and to evict
    all the pods. The `drain` command will not delete any DaemonSet pods unless we
    tell the flag to do so. One of the reasons for this behavior is that DaemonSet
    pods cannot be scheduled on any other nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `drain` command waits for the graceful termination of the pods
    and it is highly recommended to wait for all the pods to terminate gracefully
    in production environments. Let's see this in action in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 18.02: Draining All the Pods from the Nodes'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will remove all the pods running on a node. Once all the
    pods are removed, we will change the node back to schedulable so that it can accept
    new workloads. This is when the node has been upgraded and ready to take new pods:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get a list of all the nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.7: Getting a list of nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.7: Getting a list of nodes'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we have two worker nodes and three master nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new namespace called `upgrade-demo`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Run a bunch of pods to simulate a workload. Create a file named `multiple-pods.yaml`
    with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The deployment will create four replicas of the pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, use the config to create the deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see this response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that they are running on the worker pods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Your output should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.8: Verifying whether the pods are running on the worker nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.8: Verifying whether the pods are running on the worker nodes'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the pods are distributed among both worker nodes by the default scheduler
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `drain` command to evict all the pods from any of the nodes. This command
    will also mark the node as unschedulable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the name of your node that you obtain from the output of the previous step.
    Note that we have passed a flag to ignore the daemon sets. You should see the
    following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.9: Draining a node'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.9: Draining a node'
  prefs: []
  type: TYPE_NORMAL
- en: If we don't set the `--ignore-daemonsets` flag and there are some DaemonSet
    pods on the node, `drain` will not proceed without this flag. We recommend using
    this flag because your cluster may be running some essential pods as a DaemonSet
    –for example, a Fluentd pod that collects logs from all other pods on the node
    and sends them to the central logging server. You may want this log collection
    pod to be available until the very last minute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that all the pods are drained from this node. To do that, get a list
    of the pods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.10: Checking whether the pods have been moved away from the drained
    node'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.10: Checking whether the pods have been moved away from the drained
    node'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, you can see that all the pods are running on the
    other node. We only had two worker nodes in our cluster, and so all the pods were
    scheduled on the lone schedulable node. If we had several available worker nodes,
    the pods would have been distributed among them by the scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s describe our drained node and make a few important observations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the name of the node that you drained in *step 6*. This will give a pretty
    long output, but there are two sections worth observing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.11: Checking taints and the unschedulable status of our drained
    node'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.11: Checking taints and the unschedulable status of our drained node'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshot shows that our node is marked as unschedulable. Next,
    find the section like the following in your output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.12: Examining the non-terminated pods on the drained node'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.12: Examining the non-terminated pods on the drained node'
  prefs: []
  type: TYPE_NORMAL
- en: This shows that the only non-terminated pods running on our system have names
    starting with `kube-proxy` and `weave-net`. The first pod implements `kube-proxy`,
    which is the component that manages pod and service network rules on nodes. The
    second pod is `weave-net`, which implements virtual networking for our cluster
    (note that your networking provider depends on the type of network you have selected).
    Since we added a flag to exclude DaemonSets in *step 6*, these pods, which are
    managed by a DaemonSet, are still running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you drain the pod in *step 6*, you will be able to upgrade the node. Even
    though upgrading is not part of this exercise, we just want to make the node schedulable
    again. For that, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that the node is schedulable again. Check the `Taints` section in the
    following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.13: Checking the taints and unschedulable statuses of our uncordoned
    node'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.13: Checking the taints and unschedulable statuses of our uncordoned
    node'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows that the node is now schedulable, and the taint
    that we observed in *step 8* has been removed.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you have seen how to remove all the pods from the node and
    mark the node as unschedulable. This will make sure that no new pod will be scheduled
    in this node and we can work on upgrading this node. We also learned how to make
    the node schedulable again so that we can continue using it after completing the upgrade.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading Kubernetes Master Components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you are running Kubernetes in any capacity that is important for your
    organization, you will be running the platform in an HA configuration. To achieve
    that, the typical configuration is at least three replicas of master components,
    running on three different nodes. This allows you to upgrade single nodes from
    one minor version to the next, one by one, while still maintaining API compatibility
    when an upgraded node rejoins the cluster because Kubernetes provides compatibility
    across one minor version. This means the master components can be on different
    versions when you are upgrading each node at a time. The following table provides
    a logical flow of the versions. Let''s assume you are upgrading from version 1.14
    to 1.15:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.14: Upgrade plan for three master nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.14: Upgrade plan for three master nodes'
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, we will proceed with upgrading the Kubernetes master components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 18.03: Upgrading Kubernetes Master Components'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will upgrade all the master components on the Kubernetes
    master nodes. This exercise assumes that you are still logged in to the bastion
    host of your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we are demonstrating the process on a smaller number of nodes
    for the sake of simplicity, but the process of upgrading a large number of nodes
    would be the same. However, for a seamless upgrade, three master nodes are a minimum,
    and your applications should be HA and running on at least two worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the kops validator to validate the existing cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.15: Validating our kops cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.15: Validating our kops cluster'
  prefs: []
  type: TYPE_NORMAL
- en: This is a truncated version of the output. It shows the major infrastructure
    components of your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'List all the nodes in your cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.16: Getting a list of the nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.16: Getting a list of the nodes'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we have three master nodes and all of them are on version 1.15.7.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we are showcasing the upgrade from Kubernetes version 1.15.7
    to 1.15.10\. You can apply the same steps to upgrade to the version of Kubernetes
    supported by kops at the time when you perform this exercise. Just remember our
    earlier advice of upgrading to the latest patch version first (which is what we
    are doing here).
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `kops upgrade cluster` command to see what update is available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that this command will not directly run the update, but it will give you
    the latest update version possible. The `NAME` environment variable holds the
    name of your cluster. You should see an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.17: Checking the available cluster version'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.17: Checking the available cluster version'
  prefs: []
  type: TYPE_NORMAL
- en: You can see from the preceding screenshot that the `OLD` version is `1.15.7`,
    which is our current version, and an update is available to the `NEW` version
    of `1.15.10`, which is our target version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you verify the changes from the command in *step 4*, run the same command
    with a `--yes` flag. This will mark the desired state of the cluster in the kops
    state store:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.18: Upgrading the kops cluster configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.18: Upgrading the kops cluster configuration'
  prefs: []
  type: TYPE_NORMAL
- en: This output indicates that the desired version of the Kubernetes cluster is
    recorded in the updated kops configuration. In the next step, we will ask kops
    to update the cloud or cluster resources to match the new specifications – that
    is, Kubernetes version `1.15.10`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s run the following command so that kops updates the cluster to match
    the updated kops configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give a long output that will end in a similar way to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.19: Updating our cluster infrastructure as per'
  prefs: []
  type: TYPE_NORMAL
- en: the requirements of our cluster upgrade
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.19: Updating our cluster infrastructure as per the requirements of
    our cluster upgrade'
  prefs: []
  type: TYPE_NORMAL
- en: This has updated the cluster infrastructure to match the updated kops configuration.
    Next, we need to perform an upgrade of the Kubernetes master components running
    on this infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are running several instances of your master/worker nodes on different
    instance groups, then you can control which instance group is receiving the updates.
    For that, let''s get the name of our instance group first. Use the following command
    to get the names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.20: Getting a list of the instance groups'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.20: Getting a list of the instance groups'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this step, kops will update the Kubernetes cluster to match the kops specifications.
    Let''s upgrade the first master node to the new version using a rolling update:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that this command will only apply changes if you specify the `--yes` flag.
    This command may take time based on your node configuration. Be patient and watch
    the logs to see whether there are any errors. After some time, you should see
    a successful message similar to the one in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.21: Applying a rolling update to our first instance group'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.21: Applying a rolling update to our first instance group'
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that the node is upgraded to the target version, which is `1.15.10`,
    in our case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This should give a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.22: Checking whether the master components on the node have been
    upgraded'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.22: Checking whether the master components on the node have been
    upgraded'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the first master node is on the `1.15.10` version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that the pods are running on the newly upgraded node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the name of the node that you upgraded in the previous steps. This will
    give a long output. Look for the `Non-terminated Pod` section, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.23: Checking whether our upgraded node is running pods'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.23: Checking whether our upgraded node is running pods'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Repeat *steps 7* to *9* for all additional master nodes, using the appropriate
    names of the corresponding instance groups while updating and verifying.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that kops has successfully updated the master nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.24: Checking whether all the master nodes have been upgraded'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.24: Checking whether all the master nodes have been upgraded'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, this is a dry run, and the output shows which nodes require
    an update. Since all of them show `STATUS` as `Ready`, we know that they have
    been updated. By contrast, you can see that `nodes` (the worker nodes) return
    `NeedsUpdate`, since we have not updated them yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that all the master nodes have been upgraded to the desired version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.25: Checking the version of Kubernetes on all the master nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.25: Checking the version of Kubernetes on all the master nodes'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, all the master nodes are running version `1.15.10`, which is
    the desired version.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you have seen how to upgrade the master nodes of the Kubernetes
    cluster without any downtime for users. One node update at a time will make sure
    that enough master servers are available (a minimum of three are required for
    this to work) and the users and the cluster are not getting impacted during the
    update.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When you apply a rolling update to an instance group, kops will roll out the
    update through the nodes within the instance group by taking only one node offline
    at a time. On top of that, in this exercise, we applied a rolling update to only
    one instance group at a time. Eventually, what you should achieve is a situation
    where only one node from your cluster is taken offline at a time. Remember this
    if you choose to automate this process.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading Kubernetes Worker Nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although Kubernetes supports compatibility between master (API server) and worker
    nodes (kubelet) within one minor version, it is highly recommended that you upgrade
    the master and worker nodes in one go. Using kops, upgrading worker nodes is similar
    to upgrading master nodes. Due to the backward compatibility within one minor
    version, the worker nodes may still work if they are not version-matched by the
    master nodes, but it is strongly discouraged to run different versions of Kubernetes
    on worker and master nodes since this may create problems for the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the following considerations are of extreme importance if you want
    to keep your application online during the upgrade:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that your applications are configured to be highly available. This
    means that you should have at least two pods, each on different nodes, for each
    of your applications. If this is not the case, your applications may experience
    downtime once you evict the pods from the nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are running stateful components, make sure that the state of these components
    is backed up, or that your applications are designed to be able to withstand partial
    unavailability of the stateful components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, let's say that you are running a database with a single master
    node and multiple read replicas. Once the node that is running the master replica
    of your database evicts the database pod, if your applications are not correctly
    configured to handle this scenario, they will suffer a downtime. This has nothing
    to do with the upgrade of the Kubernetes cluster, but it is important to understand
    how your applications behave during an upgrade and to ensure that they are properly
    configured to be fault-tolerant.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood the requirements to ensure the uptime of your application,
    let's see how we can upgrade the worker nodes in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 18.04: Upgrading the Worker Nodes'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will upgrade all the worker nodes of the Kubernetes cluster.
    Worker nodes are the host of your applications:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the list of instance groups for your worker nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.26: Getting a list of the instance groups'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.26: Getting a list of the instance groups'
  prefs: []
  type: TYPE_NORMAL
- en: From this image, we can see that the name of the instance group for our worker
    nodes is `nodes`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that the nodes are ready:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.27: Checking node status'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.27: Checking node status'
  prefs: []
  type: TYPE_NORMAL
- en: If we had multiple instance groups, we would be upgrading each instance group
    one by one. However, our task here is simple since we have just one – that is, `nodes`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the kops rolling update for the `nodes` instance group **without** the
    `--yes` flag. This will provide you with a summary of what will be updated with
    the `kops rolling-update` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have changed the verbosity value in the preceding command to get
    more detailed logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s break down this command:'
  prefs: []
  type: TYPE_NORMAL
- en: – The `node-interval` flag sets the minimum delay between different node restarts.
  prefs: []
  type: TYPE_NORMAL
- en: – The `instance-group` flag states which instance group the rolling update should
    be applied to.
  prefs: []
  type: TYPE_NORMAL
- en: – The `post-drain-delay` flag sets the delay after draining the node before
    it can be restarted. Remember from earlier in this chapter that the drain operation
    will wait for the graceful termination of pods. This delay will be applied after
    that.
  prefs: []
  type: TYPE_NORMAL
- en: The `node-interval` and `post-drain-delay` flags provide an option to control
    the rate of change in the cluster. The value of these options partially depends
    on the type of application you are running. For example, if you are running a
    log agent DaemonSet on the nodes, you may want to give enough time for the pod
    to flush the content to a central logging server.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We did not use these delays when we performed a rolling update in the previous
    case since in that case, the instance groups each had just one node in them. Here,
    we have three nodes in this instance group.
  prefs: []
  type: TYPE_NORMAL
- en: – The `logtosterr` flag outputs all the logs to the **stderr** stream so that
    we can see them in our terminal output.
  prefs: []
  type: TYPE_NORMAL
- en: – The `v` flag sets the verbosity of the logs that we will see.
  prefs: []
  type: TYPE_NORMAL
- en: 'This command will show the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.28: Performing a dry run of the rolling update'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.28: Performing a dry run of the rolling update'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, run the upgrade. Use the same command as the previous step with the addition
    of the `--yes` flag. This tells kops to perform the upgrade:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Kops will drain a node, wait for the post drain delay time, and then upgrade
    and restart the node. This will be repeated for each node, one by one. You will
    see a long log in the terminal, and this process may take up to half an hour to
    complete. In your terminal, you should start seeing the logs, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.29: Starting the rolling update process'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.29: Starting the rolling update process'
  prefs: []
  type: TYPE_NORMAL
- en: 'After a while, you will see that the cluster upgrade is finished with a success
    message, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.30: Rolling update completion message'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.30: Rolling update completion message'
  prefs: []
  type: TYPE_NORMAL
- en: Keen readers will notice, in *Figure 18.29*, that in the author's logs, the
    cluster upgrade started at around 3:05 and finished, as can be seen in *Figure
    18.29*, at around 3:25\. The total time is around 20 minutes for three nodes.
    We had set a delay of 3 minutes for each node after stopping it and 3 minutes
    for each node after draining all the pods. So, the waiting time for each node
    adds up to 6 minutes. With three nodes in the instance group, the total wait time
    is 6 × 3 = 18 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that the worker nodes are updated to the target version – that is, `1.15.10`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.31: Checking the version of Kubernetes on worker nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_31.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.31: Checking the version of Kubernetes on worker nodes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that the pods are in a running state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see all pods with `STATUS` set to `Running`, as in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.32: Checking the status of our pods'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_18_32.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.32: Checking the status of our pods'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, you have seen how easy it is to upgrade the worker nodes
    through kops. However, we do not recommend upgrading all worker nodes in one go
    for production clusters and strongly recommend creating instance groups for worker
    nodes. The following are some strategies that can be used for production-grade clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: Don't keep all of your worker nodes in a single instance group. Create multiple
    instance groups for different sets of worker nodes. By default, kops creates only
    one instance group, but you can change this behavior to create many instance groups
    for worker nodes. We recommend having different worker instance groups for infrastructure
    components (such as monitoring and logging), ingress, critical applications, non-critical
    applications, and static applications. This will help you apply the upgrade to
    less critical parts of your cluster first. This strategy would help limit any
    issues in the upgrade process, keeping them to a minimum while isolating the affected
    nodes from the rest of the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are running the cluster in the cloud, you can provision new nodes on
    demand. Thus, it may be a good idea to create a sister instance group for upgrades.
    This new instance group should be running the upgraded version of Kubernetes.
    Now, cordon and drain all the pods from the old instance group. The Kubernetes
    scheduler will see that the new nodes are available and will automatically move
    all your pods to the new nodes. Once this is complete, you can just delete the
    old instance group and your upgrade is complete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This strategy needs a bit of planning, especially if you are running stateful
    applications on the cluster. This strategy also assumes that you are able to provision
    new nodes on demand, since creating a sister instance group may require temporary
    additional hardware, which may be a challenge for an on-premises data center.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that these are advanced strategies and are beyond the scope of this book.
    However, you can find more information about it at [https://kops.sigs.k8s.io/tutorial/working-with-instancegroups/](https://kops.sigs.k8s.io/tutorial/working-with-instancegroups/).
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have seen all the steps required to upgrade your cluster, you can
    bring it all together in the following activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 18.01: Upgrading the Kubernetes Platform from Version 1.15.7 to 1.15.10'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, you will upgrade the Kubernetes platform from version `1.15.7`
    to version `1.15.10`. Here, we will bring together everything that we have learned
    in this chapter. These guidelines should help you to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we are showcasing the upgrade from Kubernetes version `1.15.7`
    to `1.15.10`. You can apply the same steps to upgrade to the version of Kubernetes
    supported by kops at the time when you perform this activity.
  prefs: []
  type: TYPE_NORMAL
- en: Using *Exercise 11.01*, *Setting Up Our Kubernetes Cluster*, set up a fresh
    cluster running Kubernetes version `1.15.7`. If you are using the cloud to spin
    up machines, you can take a snapshot of the machines (your cloud vendor may charge
    you for this) before the upgrade to quickly rerun the upgrade again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upgrade kops to the version you want to upgrade on the master or bastion node.
    For this activity, we need to have version `1.15`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upgrade one of the master nodes to Kubernetes version `1.15.10`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that the master node is back in service and in the `Ready` state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, upgrade all the other master nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verify that all the master nodes are upgraded to the desired version, as in
    the following screenshot:![Figure 18.33: Upgraded version of Kubernetes on master
    nodes'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B14870_18_33.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.33: Upgraded version of Kubernetes on master nodes'
  prefs: []
  type: TYPE_NORMAL
- en: Now, upgrade the worker nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verify that the pods are running successfully on the newly upgraded nodes.
    Finally, you should be able to verify that your pods are running on the new node,
    as follows:![Figure 18.34: Pods running on upgraded worker nodes'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B14870_18_34.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.34: Pods running on upgraded worker nodes'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to this activity can be found at the following address: [https://packt.live/304PEoD](https://packt.live/304PEoD).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned that keeping your Kubernetes platform up to
    date is very important when it comes to providing a secure and reliable foundation
    for running your applications. In this fast-moving digital world, many businesses
    rely on critical applications and keeping them available, even though upgrading
    the underlying platform is important.
  prefs: []
  type: TYPE_NORMAL
- en: You have seen that a no-downtime upgrade of the platform is possible if you
    have set up the cluster in a high availability configuration to start with. However,
    the platform does not guarantee the availability of your applications unless you
    have designed and deployed your application in a fault-tolerant manner. One factor
    is to make sure that you have multiple instances of your application running and
    that the application is designed to handle the termination of these instances
    gracefully.
  prefs: []
  type: TYPE_NORMAL
- en: With that taken into account, we have seen the important considerations for
    upgrading your cluster in a way that the platform itself does not cause downtime
    for your application. We looked at the upgrade process for the master nodes as
    well as worker nodes separately. The key takeaway from this chapter is the principles
    underlined at various instances that you can apply for different kinds of Kubernetes
    clusters managed by different tools.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned at the beginning of the chapter, keeping your platform up to date
    is important to keep up with the latest developments in DevOps and enable your
    application development team to continue delivering new features to your end customers.
    With the skills acquired from this chapter, you should be able to handle the upgrade
    of your platform without causing disruption to your customers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how to extend your Kubernetes platform
    with custom resources. Custom resources allow you to offer a Kubernetes native
    API experience for your own projects.
  prefs: []
  type: TYPE_NORMAL
