- en: Implementing a Dialog-Based Search Engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have come so far in this book! We have learned the fundamentals of C++ application
    development and discussed architecting and designing world-ready applications.
    We also dove into data structures and algorithms, which are at the heart of efficient
    programming. It's now time to leverage all of these skills to design complex software,
    such as a search engine.
  prefs: []
  type: TYPE_NORMAL
- en: With the popularity of the internet, search engines have become the most in-demand
    products out there. Most users start their journey through the web from a search
    engine. Various web search services, such as Google, Baidu, Yandex, and so on,
    receive huge amounts of traffic, serving trillions of requests daily. Search engines
    process each request in less than a second. Although they maintain thousands of
    servers to tackle the load, at the heart of their efficient processing are data
    structures and algorithms, data architecture strategies, and caching.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of designing an efficient searching system doesn't just appear in
    web search engines. Local databases, **Customer Relationship Management** (**CRM**)
    systems, accounting software, and others need robust searching functionality.
    In this chapter, we will discover the fundamentals of search engines and discuss
    the algorithms and data structures used to build fast search engines. You will
    learn how web search engines generally work and meet the new data structures used
    in projects requiring high processing capabilities. You will also build the confidence
    to go out there and build your own search engine that will compete with existing
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the structure of a search engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding and designing an inverted index used to map keywords to documents
    in the search engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing and building a recommendation engine for users of a search platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using knowledge graphs to design a dialog-based search engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A `g++` compiler with a `-std=c++2a` option is used to compile the examples
    throughout this chapter. You can find the source files used in this chapter at [https://github.com/PacktPublishing/Expert-CPP](https://github.com/PacktPublishing/Expert-CPP).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the structure of a search engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine the billions of web pages in the world. Typing a word or phrase into
    a search engine interface returns us a long list of results in less than a second.
    The speed at which a search engine processes so many web pages is miraculous.
    How does it find the correct document so quickly? To answer this question, we
    will do the wisest thing a programmer can do design an engine of our own.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the basic idea behind a search engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e863777e-e6a1-428a-9543-34793e6ebfc4.png)'
  prefs: []
  type: TYPE_IMG
- en: The **User** types in words using the search engine's **user interface**. The
    **Search engine** scans all the documents, filters them, sorts them by relevance,
    and responds to the user as fast as it can. Our main interest lies in the web
    search engine's implementation. Looking for something will require searching for
    it among billions of documents.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try to devise an approach to find the phrase *Hello, world!* from among
    billions of documents (we will refer to web pages as documents for the sake of
    brevity). Scanning each document for the phrase would take a huge amount of time.
    If we consider each document to have at least 500 words, searching for a specific
    word or a combination of words would take a lot of time. It would be more practical
    to scan all the documents beforehand. This scanning process includes building
    an index of each occurrence of words in the documents and storing the information
    in a database, which is also known as **indexing documents**. When the user types
    in a phrase, the search engine will look up the words in its database and respond
    with links to documents that satisfy the query.
  prefs: []
  type: TYPE_NORMAL
- en: Before searching the documents, it wouldn't hurt for the engine to validate
    the user's input. It's not uncommon for users to have typos in phrases. Aside
    from typos, the user experience would be a lot better if the engine autocompleted
    words and phrases. For example, while the user is typing *hello*, the engine might
    suggest searching for the phrase *Hello, world!*. Some search engines keep track
    of users, storing information about their recent searches, details of the device
    they're using to make the request, and so on. For example, a user searching *how
    to restart the computer* will get an even better result if the search engine knows
    the operating system of the user. If it's a Linux distribution, the search engine
    will sort the search results so that documents describing the restarting of a
    Linux-based computer appear first.
  prefs: []
  type: TYPE_NORMAL
- en: We should also be careful of new documents appearing on the web regularly. A
    background job might analyze the web continuously to find new content. We call
    this job a **crawler**, as it crawls the web and indexes documents. The crawler
    downloads documents in order to parse their contents and build an index. Already-indexed
    documents might get updated, or worse, removed. So, another background job should
    take care of updating existing documents regularly. You might encounter the term
    **spider** for tasks that crawl the web to parse documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following updated diagram illustrates the structure of a search engine
    in a bit more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80b2e7a9-acbb-4672-a83c-d3bce75bde7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Searching has a wide range of applications. Imagine the simplest form of searching—finding
    a word in an array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Although the previous example applies to the simplest search engine, the real
    deal is designing a search engine that scales. You don't want to serve user requests
    by searching through an array of strings. Instead, you should strive to implement
    a scalable search engine that is able to search through millions of documents.
    This requires a lot of thinking and designing because everything matters, from
    the right choice of data structure to efficient algorithms for data processing.
    Let's now discuss the components of a search engine in more detail. We will incorporate
    all of the skills learned from previous chapters to design a good search engine.
  prefs: []
  type: TYPE_NORMAL
- en: Providing a convenient user interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s crucial to invest time and resources in building a fine-grained user
    interface that will provide an astonishing user experience. The key here is simplicity.
    The simpler the interface, the better its usage. We will use the market-dominant
    Google as an example. It has a simple input field at the center of the page. The
    user types their request in the field and the engine suggests a number of phrases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51e853fb-bc2e-4002-b368-fbd3e49d7057.png)'
  prefs: []
  type: TYPE_IMG
- en: We don't think of users as lazy people, but providing a list of suggestions
    is helpful because sometimes users don't know the exact term they are looking
    for. Let's concentrate on the structure and implementation of the suggestions
    list. After all, we are interested in solving problems rather than designing nice
    user interfaces. We will not discuss user interface design in this chapter; it
    would be better to concentrate on the backend of the search engine. However, there
    is just one thing that we should consider before moving on. The search engine
    that we are implementing here is dialog-based. The user queries the engine and
    can choose from several answers to narrow down the list of results. For example,
    suppose the user queries *a computer* and the search engine asks *a desktop or
    a laptop?*. That cuts down the search results drastically and provides a better
    result for the user. We will use a decision tree to achieve this. But, before
    that, let's understand the complexity of search engines.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, there is the problem of **input tokenization**. This relates to
    both document parsing and search-phrase analysis. You might build a great query
    parser that breaks just because the user made a mistake in their query. Let's
    take a look at a couple of approaches to dealing with vague queries.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with typos in queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s not uncommon for users to make typos while typing. While it might seem
    like a simple thing, it can be a real problem for search engine designers. Searching
    through millions of documents might give unexpectedly wrong results if the user
    typed helo worl instead of hello world. You may be familiar with autosuggestions
    provided by a search engine. For example, here''s how the Google Search interface
    looks when we type with mistakes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3885d527-3de0-4b25-946d-c20714d9eb89.png)'
  prefs: []
  type: TYPE_IMG
- en: Pay attention to the two lines at the bottom of the screenshot. One of them
    says Showing results for hello world, which suggests that the search engine has
    assumed that the user typed the query with typos and has taken the initiative
    of showing results for the correct query. However, there is still a possibility
    that the user did want to search for the exact words they typed. So, the user
    experience provides the next line as Search instead for helo worl.
  prefs: []
  type: TYPE_NORMAL
- en: So, when building search engines, we have several problems to solve, starting
    with user requests. First of all, we need to provide a convenient interface for
    users to type in their text. The interface should also interact with them to provide
    better results. This includes providing suggestions based on partially typed words,
    as discussed earlier. Making the search engine interact with the user is another
    improvement in the user interface that we are going to discuss in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next comes a check for typos or incomplete words, which is not an easy task.
    Keeping a list of all the words in a dictionary and comparing the words typed
    by the user might take a while. To solve this problem, using specific data structures
    and algorithms is a must. For example, finding the **Levenshtein distance** between
    words might be helpful when checking for typos in user queries. The Levenshtein distance
    is the number of characters that should be added, removed, or substituted in a
    word for it to be equal to another one. For example, the Levenshtein distance
    between the words *world* and *worl* is 1 because removing the letter *d* from
    *world* or adding *d* to *worl* makes these words equal. The distance between
    the words *coding* and *sitting* is 4 since the following four edits change one
    word into the other:'
  prefs: []
  type: TYPE_NORMAL
- en: coding -> cod**t**ing (insertion of **t** in the middle)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: co**d**ting -> co**t**ting (substitution of **t** for **d**)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: c**o**tting -> c**i**tting  (substitution of **i** for **o**)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**c**itting -> **s**itting (substitution of **s** for **c**)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, imagine how long processing would take if we were to compare each user
    input with tens of thousands of words to find the closest ones. Another approach
    would be to use a big **trie** (data structure) to discover possible typos beforehand.
    A trie is an ordered search tree where keys are strings. Take a look at the following
    diagram representing a trie:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/296999a5-2bd6-4b40-bc87-9fdf30532cc2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each path represents a valid word. For example, the a node points to the n and
    r nodes. Pay attention to the # after n. It tells us that the path up to this
    node represents a word, an. However, it continues to point to d, which is then
    followed by another #, meaning that the path up to this node represents another
    word, and. The same logic applies to the rest of the trie. For example, imagine
    the portion of the trie for the word *world*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d174ec96-3f73-441a-8f25-6ebd22d5038a.png)'
  prefs: []
  type: TYPE_IMG
- en: When the engine encounters *worl*, it goes through the preceding trie. w is
    fine, as is o, and everything else is fine up until the second-to-last character
    in the word, l. In the preceding diagram, there is no terminal node after l, only
    d. That means we are sure that there is no such word as *worl;* so it might be
    *world*. To provide good suggestions and check for typos, we should have a complete
    dictionary of words in the user's language. It gets even harder when you plan
    to support multiple languages. However, while collecting and storing the dictionary
    is arguably an easy task, the harder task is collecting all the documents on the
    web and storing them accordingly to perform a fast search. The tool, program,
    or module of the search engine that collects and parses websites to build the
    search engine database (as discussed previously) is called a crawler. Before looking
    in more depth into the way we will store these website pages, let's take a quick
    look at the functionality of a crawler.
  prefs: []
  type: TYPE_NORMAL
- en: Crawling websites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Searching through millions of documents each time the user types a query is
    not realistic. Imagine a search engine that parses websites to search for a user
    query right after the user hits the search button on the UI of a system. That
    would take forever to complete. Each request to the website from the search engine
    takes some time. Even if it is less than a millisecond (0.001 seconds), it will
    take a long time to analyze and parse all of the websites while the user waits
    for their query to complete. To make things clearer, let's suppose accessing and
    searching one website takes about 0.5 milliseconds (even then, that's unreasonably
    fast). That means searching through 1 million websites will take around 8 minutes.
    Now imagine you open a Google search and make a query—would you wait for 8 minutes?
  prefs: []
  type: TYPE_NORMAL
- en: The correct approach is to store all the information in the database efficiently
    accessible by the search engine. The crawler downloads website pages and stores
    them as temporary documents until parsing and indexing take place. A complex crawler
    might also parse documents to keep them in a format more convenient for the indexer.
    The important point here is that downloading a web page is not an action that
    happens just once. The contents of a web page might get updated. Also, new pages
    might appear during this time. So, the search engine has to keep its database
    up to date. To achieve this, it schedules the crawler to download pages regularly.
    A smart crawler might compare the differences in the content before passing it
    on to the indexer.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the crawler works as a multithreaded application. Developers should
    take care to make crawling as fast as possible because keeping billions of documents
    up to date is not an easy task. As we have already mentioned, the search engine
    doesn't search through documents directly. It performs searches in the so-called
    index file. Although crawling is an interesting coding task, we will mostly concentrate
    on indexing in this chapter. The next section introduces indexing in the search
    engine.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The key functionality of search engines is indexing. The following diagram
    shows how documents downloaded by the crawler are processed to build the index
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17ba8dc9-5ed9-428a-8b09-0db88bbcb4d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The index is shown as an **inverted index** in the preceding diagram. As you
    can see, the user queries are directed to the inverted index. Although we use
    the terms **index** and **inverted index** interchangeably in this chapter, **inverted
    index** is a more accurate name for it. First, let''s see what the index for the
    search engine is. The whole reason for indexing documents is to provide a fast
    searching functionality. The idea is simple: each time the crawler downloads documents,
    the search engine processes its contents to divide it into words that refer to
    that document. This process is called **tokenization**. Let''s say we have a document
    downloaded from Wikipedia containing the following text (for brevity, we will
    only a part of a paragraph as an example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The search engine divides the preceding document into separate words, as follows
    (only the first few words are shown here for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After dividing the document into words, the engine assigns an **identifier**
    (**ID**) to each word in the document. Assuming the ID of the preceding document
    is 1, the following table shows that words refer to (occur in) the document with
    ID 1 :'
  prefs: []
  type: TYPE_NORMAL
- en: '| In | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1979 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Bjarne | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Stroustrup | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| a | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Danish | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| computer | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| scientist | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| ... |  |'
  prefs: []
  type: TYPE_TB
- en: 'There might be several documents that contain the same words, so the preceding
    table might actually look more like the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '| In | 1, 4, 14, 22 |'
  prefs: []
  type: TYPE_TB
- en: '| 1979 | 1, 99, 455 |'
  prefs: []
  type: TYPE_TB
- en: '| Bjarne | 1, 202, 1314 |'
  prefs: []
  type: TYPE_TB
- en: '| Stroustrup | 1, 1314 |'
  prefs: []
  type: TYPE_TB
- en: '| a | 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ... |'
  prefs: []
  type: TYPE_TB
- en: '| Danish | 1, 99, 102, 103 |'
  prefs: []
  type: TYPE_TB
- en: '| computer | 1, 4, 5, 6, 24, 38, ... |'
  prefs: []
  type: TYPE_TB
- en: '| scientist | 1, 38, 101, 3958, ... |'
  prefs: []
  type: TYPE_TB
- en: 'The following table represents the inverted index. It maps words with the IDs
    of the documents downloaded by the crawler. It now becomes much faster to find
    documents that contain words typed by the user as a query. Now, when the user
    queries the engine by typing *computer*, the result is generated based on the
    ID retrieved from the index, that is, 1, 4, 5, 6, 24, 38, ... in the preceding
    example. Indexing also helps to find results for more complex queries. For example, *computer
    scientist* matches the following documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '| computer | **1**, 4, 5, 6, 24, **38**, ... |'
  prefs: []
  type: TYPE_TB
- en: '| scientist | **1**, **38**, 101, 3958, ... |'
  prefs: []
  type: TYPE_TB
- en: To respond to the user with the documents that contain both terms, we should
    find the intersection of the referenced documents (see the bold numbers in the
    preceding table), for example, 1 and 38.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the user query is also tokenized before matching it with the index.
    Tokenization usually involves word normalization. If it is not normalized, a *Computer
    Scientist* query wouldn't give any results (note the capital letters in the query).
    Let's learn a bit more about this.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might remember the concept of tokenization from [Chapter 1](2297d785-7242-4149-8b31-f9af1fcdd833.xhtml),
    *Building C++ Applications, *where we discussed how a compiler parses the source
    files by tokenizing them into smaller, indivisible units called tokens. A search
    engine parses and tokenizes documents in a similar way.
  prefs: []
  type: TYPE_NORMAL
- en: 'We won''t go into too much detail about this but you should consider that the
    document is processed in a way that means tokens (the indivisible terms bearing
    meaning in terms of search engine context) are normalized. For example, all of
    the words we are looking at are lowercase. So, the index table should look like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| in | 1, 4, 14, 22 |'
  prefs: []
  type: TYPE_TB
- en: '| 1979 | 1, 99, 455 |'
  prefs: []
  type: TYPE_TB
- en: '| bjarne | 1, 202, 1314 |'
  prefs: []
  type: TYPE_TB
- en: '| stroustrup | 1, 1314 |'
  prefs: []
  type: TYPE_TB
- en: '| a | 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ... |'
  prefs: []
  type: TYPE_TB
- en: '| danish | 1, 99, 102, 103 |'
  prefs: []
  type: TYPE_TB
- en: '| computer | 1, 4, 5, 6, 24, 38, ... |'
  prefs: []
  type: TYPE_TB
- en: '| scientist | 1, 38, 101, 3958, ... |'
  prefs: []
  type: TYPE_TB
- en: As C++ programmers, you might feel uncomfortable with seeing bjarne or stroustrup
    in lowercase. However, as we are matching the user input with the inverted index
    keys, we should consider that the user input might not have the form that we expect
    it to have. So, we need to apply the same rules to the user input so that it matches
    that of the inverted index.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, pay attention to a. Without exaggeration, that''s a word that appears
    in every document. Other examples of this are the words *the*, *an*, *in*, and
    so on. We refer to them as **stop words**; they are filtered out before the actual
    processing. Usually, search engines ignore them, so the inverted index updates
    to the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 1979 | 1, 99, 455 |'
  prefs: []
  type: TYPE_TB
- en: '| bjarne | 1, 202, 1314 |'
  prefs: []
  type: TYPE_TB
- en: '| stroustrup | 1, 1314 |'
  prefs: []
  type: TYPE_TB
- en: '| danish | 1, 99, 102, 103 |'
  prefs: []
  type: TYPE_TB
- en: '| computer | 1, 4, 5, 6, 24, 38, ... |'
  prefs: []
  type: TYPE_TB
- en: '| scientist | 1, 38, 101, 3958, ... |'
  prefs: []
  type: TYPE_TB
- en: You should note that normalization is not just making words lowercase. It also
    involves changing words to their normal forms.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing a word to its root form (or to its word stem) is also called **stemming**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following sentence from the document we used as an example
    at the beginning of the section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'creating, originated, and Stroustrup''s are normalized, so the inverted index
    will have the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '| motivation | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **create** | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| new | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| language | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **originate** | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **stroustrup** | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| experience | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| programming | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| phd | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| thesis | 1 |'
  prefs: []
  type: TYPE_TB
- en: Also, note that we have ignored the stop words and didn't include *the* in the
    preceding table.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization is the first step in index creation. Besides that, we are free
    to process the input in any way that makes searching better, as shown in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Relevance is one of the most important features of search engines. Responding
    with documents that match the user input is not enough. We should rank them in
    a way that means the most relevant documents appear first.
  prefs: []
  type: TYPE_NORMAL
- en: 'One strategy is recording the number of occurrences of each word in a document.
    For example, a document describing a computer might contain several occurrences
    of the word *computer*, and if the user searches for *a computer*, the results
    will display the document that contains the most occurrences of *computer* first.
    Here''s an example index table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| computer | 1{18}, 4{13}, 899{3} |'
  prefs: []
  type: TYPE_TB
- en: '| map | 4{9}, 1342{4}, 1343{2} |'
  prefs: []
  type: TYPE_TB
- en: '| world | 12{1} |'
  prefs: []
  type: TYPE_TB
- en: The values in curly braces define the number of occurrences of each word in
    the document.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous factors that we can consider when presenting search results
    to a user. Some search engines store user-related information in order to respond
    with personalized results. Even the program that the user uses to access the search
    engine (usually a web browser) might change the results of search platforms. For
    example, a user searching for *reinstalling the operating system* on a Linux OS
    gets results that contain *reinstalling Ubuntu* at the top of the list because
    the browser provided the search engine with the OS type and version information.
    However, taking into account privacy concerns, there are search engines that completely
    eliminate the use of personalized user data.
  prefs: []
  type: TYPE_NORMAL
- en: Another property of a document is the date it was updated. Fresh content always
    has a higher priority. So, when returning a list of documents to the user, we
    might also reorder them in the order that their content was updated. Concern about
    the relevant ranking of documents brings us to the next section, where we will
    discuss recommendation engines.
  prefs: []
  type: TYPE_NORMAL
- en: Building a recommendation engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We introduced **Artificial Intelligence** (**AI**) along with **machine learning**
    (**ML**) in the previous chapter. A recommendation engine could be treated as
    an AI-driven solution or a simple collection of conditional statements. Building
    a system that takes in user data and returns the options that best satisfy that
    input is a complex task. Incorporating ML into such a task might sound quite reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: However, you should take into account the fact that a recommendation engine
    might comprise a list of rules by which data is processed before being output
    to the end user. Recommendation engines can run in both expected and unexpected
    places. For example, when browsing products on Amazon, a recommendation engine
    suggests products to us based on the product that we are currently viewing. Movie
    databases suggest new movies based on the movies we have previously watched or
    rated. It might seem unexpected to many but a recommendation engine also runs
    behind search engines.
  prefs: []
  type: TYPE_NORMAL
- en: You may be familiar with the way some e-commerce platforms suggest products.
    Most of the time, the suggestions pane is titled something similar to *Customers
    who bought this, also bought...*. Recall cluster analysis, which we introduced
    in the previous chapter. Now, if we try to understand how these suggestions work
    under the hood, we will likely discover some clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a simpler look at and try to devise some recommendation mechanisms.
    Let''s take, for example, a bookstore website. John buys a book titled *Mastering
    Qt5*, so let''s put that information in the table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  Mastering Qt5 |'
  prefs: []
  type: TYPE_TB
- en: '|  John |  yes |'
  prefs: []
  type: TYPE_TB
- en: 'Next, John decides to buy a C++ book, *Mastering C++ Programming*. Leia buys
    a book called *Design Patterns***.** Karl buys three books, called *Learning Python*,
    *Mastering Machine Learning*, and *Machine Learning with Python*. The table is
    updated and now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  Mastering Qt5 |  Mastering C++ Programming |  Design Patterns | Learning
    Python | Mastering Machine Learning | Machine Learning with Python |'
  prefs: []
  type: TYPE_TB
- en: '|  John |  yes |  yes |  no | no | no | no |'
  prefs: []
  type: TYPE_TB
- en: '|  Leia |  no |  no |  yes | no | no | no |'
  prefs: []
  type: TYPE_TB
- en: '|  Karl |  no |  no |  no |  yes |  yes |  yes |'
  prefs: []
  type: TYPE_TB
- en: So, now let's imagine Harut visits the website and buys two of the books listed
    earlier, *Learning Python* and *Machine Learning with Python*. Would it be reasonable
    to recommend the book *Mastering Qt5* to him? We don't think so. But we are aware
    of the books that he bought and we are also aware that one of the other users,
    Karl, bought three books, two of which are the same as the books that Harut bought.
    So, it might be reasonable to recommend *Mastering Machine Learning* to Harut
    by telling him that customers who bought those other two books also bought this
    one. That's a simple example of how a recommendation engine works from a high-level
    perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Using a knowledge graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s get back to our search engine. A user is searching for an eminent
    computer scientist— say, Donald Knuth. They type the name in the search field
    and get results from all over the web that are sorted to provide the best results.
    Let''s, once again, take a look at Google Search. To make the most of the user
    interface, Google shows us some brief information about the search topic. In this
    case, it shows several pictures of the great scientist and some information about
    him to the right side of the web page with the results. Here''s what that section
    looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/703e4005-d07b-4044-8fdd-c249be7de3e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This way, the search engine tries to cover the user''s basic needs to allow
    them to find the information faster without even having to visit any websites.
    What interests us most, in this case, is the suggestion box placed under the preceding
    information box. It is titled People also search for and here''s how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97e5ef3e-1898-47e4-8368-92a7fcba26c5.png)'
  prefs: []
  type: TYPE_IMG
- en: These are recommendations based on the activity of users who searched for, let's
    say, Alan Turing, right after they searched for Donald Knuth. This prompted the
    recommendation engine to come up with the suggestion that if someone new is searching
    for Donald Knuth, they might also be interested in Alan Turing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can organize a similar suggestion mechanism through something that Google
    calls a **knowledge graph**. This is a graph consisting of nodes, each of which
    represents some topic, person, movie, or anything else that is searchable. A graph
    data structure is a collection of nodes and edges connecting these nodes, as in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45cb1241-4a3d-44db-9fde-8bb9a30d3c84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In a knowledge graph, each node represents a single entity. By entity, we mean
    a city, a person, a pet, a book, or almost anything else that you can imagine.
    Now, edges in that graph represent the connections between entities. Each node
    can be connected to another by more than one node. For example, take a look at
    these two nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca796132-cf5f-4ed1-b8f0-f703724ac5f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These two nodes only contain text. We might guess that Donald Knuth is a name
    and The Art of Computer Programming is some sort of art. The essence of building
    a knowledge graph is that we can relate each node to another node that represents
    its type. The following diagram expands on the previous graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e6932f4-bec8-455f-8dd5-f15a92aa7946.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the two new nodes that we have added. One of them represents
    a **person**, while the other one a **book**. And what is even more exciting is
    that we connected the Donald Knuth node with an edge to the **person** node and
    labeled it as an is a relationship. In the same way, we have connected the **The
    Art of Computer Programming** node to the book node and so we can say that The
    Art of Computer Programmingis a book. Let''s now connect Donald Knuth to the book
    he wrote:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e68dfd54-f84d-4ee7-89ef-5c187ee45498.png)'
  prefs: []
  type: TYPE_IMG
- en: So, now we have a complete relationship because we know that Donald Knuth is
    a person who is the author of The Art of Computer Programming, which in turn represents
    a book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add a couple more nodes that represent people. The following graph shows
    how we''ve added the Alan Turing and Peter Weyland nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d433010e-be15-41f9-9669-d36aefa0590e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, both Alan Turing and Peter Weyland are people. Now, if this is a part of
    the search engine knowledge base, then it gives us a good insight into the search
    intent of the user. When we hit the result for Donald Knuth, we know that it is
    about a person. If necessary, we can recommend that the user takes a look at the
    other people that we have accumulated knowledge of in our knowledge graph. Would
    it be reasonable to recommend that the user searching for Donald Knuth also takes
    a look at the Alan Turing and Peter  Weyland pages? Well, here comes the tricky
    part: although both are people, they are not strongly connected. So, we need something
    extra to define the relevancy of the connection between two different people.
    Take a look at the following additions to the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d8c7081-485a-487e-9692-9f9e43c47eb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is now clear that Donald Knuth and Alan Turing share the same activity,
    presented as the **Computer Science** node, which represents a **field of study**,
    while Peter Weyland turns out to be a **fictional character.** So, the only thing
    that makes Peter Weyland and Donald Knuth related is that they are both people.
    Take a look at the numbers that we put on the edges leading from the person node
    to the Computer Science node. Let''s say we rate the relationship from **0** to
    **100**, with the latter meaning the relationship is the strongest. So, we put
    99 for both Alan Turing and Donald Knuth. We should have omitted the edge from
    Peter Weyland to Computer Science instead of putting **0**, but we have done this
    on purpose to show the contrast. Those numbers are weights. We add weights to
    the edges to emphasize the connectivity factor; that is, Alan Turing and Donald
    Knuth share the same thing and are strongly related to each other. If we add **Steve
    Jobs** as a new person in the knowledge graph, the graph will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39e8ff27-3843-4762-b607-d5af7f171c25.png)'
  prefs: []
  type: TYPE_IMG
- en: Take a look at the weights for the edges. Steve Jobs is somehow related to Computer
    Science, but he is mostly related to the **businessman** and **influencer** nodes.
    In the same way, we can now see that Peter Weyland shares more with Steve Jobs
    than with Donald Knuth. Now, it's more informative for a recommendation engine
    to suggest that the user searching for Donald Knuth should also take a look at
    Alan Turing because they are both people and connected to Computer Science with
    equal or near-to-equal weights. That was a great example of incorporating such
    a graph in a search engine. The next thing that we are going to do is introduce
    you to using a similar knowledge graph to build an even smarter framework to provide
    relevant search results. We call this dialog-based searching.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a dialog-based search engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, let''s tackle designing the part of our search engine that will give
    us our fine-grained user interface. As we mentioned at the beginning of the chapter,
    a dialog-based search engine involves building a user interface that asks the
    user questions related to their query. This approach is most applicable in situations
    where we have ambiguous results. For example, a user searching for Donald might
    have in mind one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Donald Knuth*, the great computer scientist'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Donald Duck*, the cartoon character'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Donald Dunn*, the real name of Jared Dunn, the fictional character'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Donald Trump*, the businessman and 45^(th) US president'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding list is just a small example of potential results for the Donald search
    term. Now, what do search engines lacking a dialog-based approach do? They provide
    a list of relevant results for the best match for the user input. For example,
    at the time of writing this book, searching for Donald resulted in a list of websites
    all related to Donald Trump, even though I had Donald Knuth in mind. Here, we
    can see the thin line between the best match and the best match for the user.
  prefs: []
  type: TYPE_NORMAL
- en: Search engines collect a lot of data to use for personalized search results.
    If the user works in the field of website development, most of their search requests
    will somehow be related to that particular field. This is quite helpful in providing
    a user with better search results. For example, a user that has a big search history,
    consisting mostly of requests related to website development, will get better,
    more focused results when searching for zepelin. The ideal search engine will
    provide websites linking to the Zeplin application for building a web UI, while
    for other users, the engine will provide results with information on the rock
    band named Led Zeppelin.
  prefs: []
  type: TYPE_NORMAL
- en: 'Designing a dialog-based search engine is the next step in providing the user
    with a better interface. Now, it''s simple enough to build if we already have
    a strong knowledge base. We are going to use the concept of the knowledge graph
    described in the previous section. Let''s suppose when the user types a search
    word we fetch all the matched topics from the knowledge graph and have a list
    of potential hits for the user, as in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fcc30b2-7cd7-4d82-bd2b-7d3d5b733845.png)'
  prefs: []
  type: TYPE_IMG
- en: So, it's now much easier for the user to choose a topic and save time on recalling
    the full name. The information from the knowledge graph can be (and for some search
    engines it is) incorporated in automatic suggestions when the user is typing the
    query. Further, we are going to tackle the major components of the search engine.
    Obviously, this chapter cannot cover every aspect of the implementation, but the
    fundamental components we will discuss are enough for you to jump into the design
    and implementation of your own search engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'We won''t bother with the UI part of the search engine. What concerns us most
    is the backend. When talking about the backend of an application, we usually mean
    the part that is invisible to the user. To be more specific, let''s take a look
    at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ef212ea-f319-4913-a62e-db2027a3059c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, most of the engine lies at the backend. While the UI might
    feel simple, it is an important part of the search system overall. That''s where
    the user starts their journey and the more the UI is designed to offer the best
    possible experience, the less discomfort the user experiences when searching for
    something. We will concentrate on the backend; the following are several major
    modules that we are going to discuss:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The query parser**: Analyzes the user query, normalizes words, and gathers
    information for each term in the query to later pass on to the query processor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The query processor**: Retrieves data associated with the query using the
    index and supplementary databases and constructs the response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The dialog generator**: Provides more options for the user to choose from
    while searching for something. The dialog generator is a supplementary module.
    The user making requests can omit the dialog or use it to further narrow down
    the search results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have skipped some of the components that are common in search engines (such
    as the crawler) and instead we will concentrate on those components that are strongly
    related to the dialog-based search engine. Let's now start with the query parser.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the query parser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The query parser does what its name suggests: *parses* the query. As a base
    task for the query parser, we should divide the words by space. For example, the
    user query *zeplin best album* is divided into the following terms: `zeplin`,
    `best`, and `album`. The following class represents the basic query parser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the preceding `parse()` function. It''s the only public function
    in the class. We will add more private functions that are called from the `parse()`
    function to completely parse the query and get results as a `Query` object. `Query` represents
    a simple struct containing information on the query, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`raw_query` is the textual representation of the query that the user typed,
    while `normalized_query` is the same query after normalization. For example, if
    the user types *good books, a programmer should read*., `raw_query` is that exact
    text and `normalized_query` is *good books programmer should read*. In the following
    snippets, we don''t use `normalized_query`, but you will need it later when you
    complete the implementation. We also store tokens in the `Token` vector, where `Token`
    is a struct, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `related` property represents a list of words that are **semantically related**
    to the token. We call two words **semantically related** if they express similar
    meanings conceptually. For example, the words *best* and *good*, or *album* and
    *collection*, can be considered semantically related. You may have guessed the
    purpose of the weight in the hash table value. We use it to store the `Weight`
    of similarity.
  prefs: []
  type: TYPE_NORMAL
- en: The range for the **weight** is something that should be configured during the
    exploitation of the search engine. Let's suppose we chose the range to be between
    0 to 99\. The weight of the similarity of the words *best* and *good* could be
    expressed as a number near to 90, while the weight of the similarity of the words
    *album* and *collection* could deviate from 40 to 70\. Choosing these numbers
    is tricky and they should be tuned in the course of the development and exploitation
    of the engine.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `dialog_id` of the `Query` struct represents the ID of the generated
    dialog if the user chose a path suggested by the generator. We will come to this
    soon. Let's now move on to finalizing the `parse()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following additions to the `QueryParser` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Although the two private functions (`tokenize` and `retrieve_word_relations`)
    are not implemented in the preceding snippet, the basic idea is that they normalize
    and collect information on the search query. Take a look at the preceding code
    before we continue on to implementing the query processor.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the query processor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The query processor carries out the main job of the search engine; that is,
    it retrieves the results from the search index and responds with a relevant list
    of documents according to the search query. In this section, we will also cover
    dialog generation.
  prefs: []
  type: TYPE_NORMAL
- en: As you have seen in the previous section, the query parser constructs a `Query`
    object that contains tokens and `dialog_id`. We will use both here in the query
    processor.
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended to have a separate component for the dialog generator due
    to scalability concerns. For educational purposes, we will keep the implementation
    succinct, but you are free to redesign the dialog-based search engine and complete
    the implementation along with the crawler and other supplementary modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tokens in the `Query` object are used to make a request to the search index
    in order to retrieve the set of documents associated with each word. Here''s how
    the corresponding `QueryProcessor` class looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Consider the preceding code snippet as an introduction to the implementation.
    We want to express the fundamental idea of the `QueryProcessor` class. It has
    the `process_query()` function that retrieves documents from the index based on
    the tokens inside the query argument. The crucial role here is played by the search
    index. The way we define its construction and the way it stores documents is essential
    in terms of making fast queries. In the meantime, the dialog ID, provided as an
    additional argument, allows the `process_query()` function to request the knowledge
    base (or the knowledge graph) to retrieve more relevant tokens related to the
    query.
  prefs: []
  type: TYPE_NORMAL
- en: It's also essential to consider that `QueryProcessor` is also responsible for
    producing dialogs (that is, defining a set of paths to offer the user possible
    scenarios for the query). The produced dialogs are sent to the user and, when
    the user makes another query, the used dialog is associated with that query by
    the dialog ID that we have seen already.
  prefs: []
  type: TYPE_NORMAL
- en: Although the preceding implementation is mostly introductory (because the real
    size of the code is too big to fit into the chapter), it's a great fundament for
    you to move further in designing and implementing the engine.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a search engine from scratch is a task for seasoned programmers. We
    have touched on many topics in this book and combined most of them in this chapter
    by designing a search engine.
  prefs: []
  type: TYPE_NORMAL
- en: We have learned that web search engines are complex systems consisting of several
    components, such as the crawler, the indexer, and user interface. The crawler
    is responsible for regularly checking the web to download web pages for the search
    engine to index. Indexing results in the production of a big data structure called
    an inverted index. An inverted index, or just an index, is a data structure that
    maps words with the documents in which they have occurred.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we defined what a recommendation engine is and tried to design a simple
    one for our search engine. The recommendation engine was connected to the dialog-based
    features of the search engine discussed in this chapter. A dialog-based search
    engine aims to provide targeted questions to the user to find out more about what
    the user actually intended to search for.
  prefs: []
  type: TYPE_NORMAL
- en: We reached the end of the book by discussing various subjects in computer science
    from a C++ perspective. We started with the details of C++ programs and then briefly
    went through efficient problem-solving using data structures and algorithms. Knowing
    a programming language is not enough to succeed in programming. You need to tackle
    coding problems requiring intensive skills in data structures, algorithms, multithreading,
    and so on. Also, tackling different programming paradigms may greatly enhance
    your view of computer science and allow you to take a fresh look at problem-solving.
    In this book, we have touched on several programming paradigms, such as functional
    programming.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, as you know by now, software development is not limited to coding only.
    Architecting and designing a project is one of the crucial steps toward successful
    application development. Chapters [10](069ab9af-21a4-4b8c-bc3f-f7bc0d9e4712.xhtml),
    *Designing World-Ready Applications, *to [16](0a4355bb-9bc6-4451-8661-fbf29155cfa2.xhtml),
    *Implementing a Dialog-Based Search*, are mostly related to the approaches to
    and strategies of designing real-world applications. Let this book be your introductory
    guide to the world of programming from a C++ developer's perspective. Develop
    your skills by developing more complex applications and share your knowledge with
    colleagues and those who are just starting their careers. One of the best ways
    to learn something new is by teaching it.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the role of the crawler in a search engine?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we call the search index an inverted index?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the main rules for tokenizing words before indexing them?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the role of recommendation engines?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a knowledge graph?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information, refer to the following book:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Introduction to Information Retrieval*, *Christopher Manning, et al.*, [https://www.amazon.com/Introduction-Information-Retrieval-Christopher-Manning/dp/0521865719/](https://www.amazon.com/Introduction-Information-Retrieval-Christopher-Manning/dp/0521865719/)'
  prefs: []
  type: TYPE_NORMAL
