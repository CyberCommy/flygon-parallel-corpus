- en: Network and Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've learned how to deploy containers with different resources in Kubernetes
    in [Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting
    Started with Kubernetes*, and know how to use volume to persist the data, dynamic
    provisioning, and different storage classes. Next, we'll learn how Kubernetes
    routes the traffic to make all of this possible. Networking always plays an important
    role in the software world. We'll describe the networking from containers on a
    single host, multiple hosts and finally to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Docker networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are plenty of choices you can use to implement networking in Kubernetes.
    Kubernetes itself doesn''t care how you implement it, but you must meet its three
    fundamental requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: All containers should be accessible to each other without NAT, regardless of
    which nodes they are on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All nodes should communicate with all containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The IP container should see itself the same way as the others see it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before getting into anything further, we'll first review how does the default
    container networking works. That's the pillar of the network to make all of this
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: Docker networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's review how Docker networking works before getting into Kubernetes networking.
    In [Chapter 2](part0047.html#1CQAE0-6c8359cae3d4492eb9973d94ec3e4f1e), *DevOps
    with Container*, we learned three modes of container networking, bridge, none,
    and host.
  prefs: []
  type: TYPE_NORMAL
- en: Bridge is the default networking model. Docker creates and attaches virtual
    Ethernet device (also known as veth) and assigns network namespace to each container.
  prefs: []
  type: TYPE_NORMAL
- en: The **network namespace** is a feature in Linux, which is logically another
    copy of a network stack. It has its own routing tables, arp tables, and network
    devices. It's a fundamental concept of container networking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Veth always comes in a pair, one is in network namespace and the other is in
    the bridge. When the traffic comes into the host network, it will be routed into
    the bridge. The packet will be dispatched to its veth, and will go into the namespace
    inside the container, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00088.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a closer look. In the following example, we''ll use a minikube
    node as the docker host. Firstly, we''ll have to use `minikube ssh` to ssh into
    the node because we''re not using Kubernetes yet. After we get into the minikube
    node, let''s launch a container to interact with us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the implementation of outbound traffic within a container. `docker
    exec <container_name or container_id>` can run a command in a running container.
    Lets use `ip link list` to list down all the interfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that we have three interfaces inside the `busybox` container. One
    is with ID `53` with the name `eth0@if54`. The number after `if` is the other
    interface ID in the pair. In this case, the pair ID is `54`. If we run the same
    command on the host, we could see the veth in the host is pointing to the `eth0`
    inside the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a veth on the host named `vethfeec36a@if53`**.** It pairs with `eth0@if54`
    in the container network namespace. The veth 54 is attached to the `docker0` bridge,
    and eventually accesses the internet via eth0\. If we take a look at the iptables
    rules, we can find a masquerading rule (also known as SNAT) on the host that Docker
    creates for outbound traffic, which will make internet access available for containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, for the inbound traffic, Docker creates a custom filter
    chain on prerouting and creates forwarding rules in the `DOCKER` filter chain
    dynamically. If we expose a container port `8080` and map it to a host port `8000`,
    we can see we''re listening to port `8000` on any IP address (`0.0.0.0/0`), which
    will then be routed to container port `8080`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now we know how packet goes in/out of containers. Let's have a look at how containers
    in a pod communicates with each other.
  prefs: []
  type: TYPE_NORMAL
- en: Container-to-container communications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pods in Kubernetes have their own real IP addresses. Containers within a pod
    share network namespace, so they see each other as *localhost*. This is implemented
    by the **network container** by default, which acts as a bridge to dispatch the
    traffic for every container in a pod. Let''s see how this works in the following
    example. Let''s use the first example from [Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Getting Started with Kubernetes*, which includes two containers, `nginx` and
    `centos` inside one pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will describe the pod and see its container ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, `web` is with container ID `d9bd923572ab` and `centos` is
    with container ID `f4c019d289d4`. If we go into the node `minikube/192.168.99.100`
    using `docker ps`, we can check how many containers Kubernetes actually launches
    since we''re in minikube, which launches lots of other cluster containers. Check
    out the latest launch time by `CREATED` column, where we will find that there
    are three containers that have just been launched:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'There is an additional container `4ddd3221cc47` that was launched. Before digging
    into which container it is, let''s check the network mode of our `web` container.
    We will find that the containers in our example pod are running in containers
    with mapped container mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`4ddd3221cc47` container is the so-called network container in this case, which
    holds network namespace to let `web` and `centos` containers join. Containers
    in the same network namespace share the same IP address and same network configuration.
    This is the default implementation in Kubernetes to achieve container-to-container
    communications, which is mapped to the first requirement.'
  prefs: []
  type: TYPE_NORMAL
- en: Pod-to-pod communications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pod IP addresses are accessible from other pods no matter which nodes they're
    on. This fits the second requirement. We'll describe the pods' communication within
    the same node and across nodes in the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: Pod communication within the same node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pod-to-pod communication within the same node goes through the bridge by default.
    Let''s say we have two pods, which have their own network namespaces. When pod1
    wants to talk to pod2, the packet passes through pod1''s namespace to the corresponding
    veth pair **vethXXXX** and eventually goes to the bridge. The bridge then broadcasts
    the destination IP to help the packet find its way, **vethYYYY** responses. The
    packet then arrives at pod2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00089.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: However, Kubernetes is all about clusters. How does traffic get routed when
    the pods are in different nodes?
  prefs: []
  type: TYPE_NORMAL
- en: Pod communication across nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'According to the second requirement, all nodes must communicate with all containers.
    Kubernetes delegates the implementation to the **container network interface**
    (**CNI**). Users could choose different implementations, by L2, L3, or overlay.
    Overlay networking is one of the common solutions, known as **packet encapsulation**.
    It wraps a message before leaving the source, gets delivered, and unwraps the
    message at the destination. This leads to a situation where overlay increases
    the network latency and complexity. As long as all the containers can access each
    other across nodes, you''re free to use any technology, such as L2 adjacency or
    L3 gateway. For more information about CNI, refer to its spec ([https://github.com/containernetworking/cni/blob/master/SPEC.md](https://github.com/containernetworking/cni/blob/master/SPEC.md)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00090.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Let's say we have a packet from pod1 to pod4\. The packet leaves from container
    interface and reaches to the veth pair, then passes through the bridge and node's
    network interface. Network implementation comes into play in step 4\. As long
    as the packet could be routed to the target node, you are free to use any options.
    In the following example, we'll launch minikube with the `--network-plugin=cni`
    option. With CNI enabled, the parameters will be passed through kubelet in the
    node. Kubelet has a default network plugin, but you could probe any supported
    plugin when it starts up. Before starting minikube, you could use `minikube stop`
    first if it's been started or `minikube delete` to delete the whole cluster thoroughly
    before doing anything further. Although minikube is a single node environment,
    which might not completely represent the production scenario we'll encounter,
    this just gives you a basic idea of how all of this works. We will learn the deployment
    of networking options in the real world in [Chapter 9](part0226.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on AWS* and [Chapter 10](part0247.html#7BHQU0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on GCP*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When we specify the `network-plugin` option, it will use the directory specified
    in `--network-plugin-dir` for plugins on startup. In the CNI plugin, the default
    plugin directory is `/opt/cni/net.d`. After the cluster comes up, let''s log in
    to the node and see the setting inside via `minikube ssh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We will find that there is one new bridge in the node, and if we create the
    example pod again by `5-1-1_pod.yml`, we will find that the IP address of the
    pod becomes `10.1.0.x`, which is attaching to `mybridge` instead of `docker0`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Why is that? That''s because we specify that we''ll use CNI as the network
    plugin, and `docker0` will not be used (also known as **container network model**
    or **libnetwork**). CNI creates a virtual interface, attaches it to the underlay
    network, and sets the IP address and routes and maps it to the pods'' namespace
    eventually. Let''s take a look at the configuration located at `/etc/cni/net.d/`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we use the bridge CNI plugin to reuse the L2 bridge for pod
    containers. If the packet is from `10.1.0.0/16`, and its destination is to anywhere,
    it'll go through this gateway. Just like the diagram we saw earlier, we could
    have another node with CNI enabled with `10.1.2.0/16` subnet, so that ARP packets
    could go out to the physical interface on the node that the target pod is located
    at. It then achieves pod-to-pod communication across nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check the rules in iptables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: All the related rules have been switched to `10.1.0.0/16` CIDR.
  prefs: []
  type: TYPE_NORMAL
- en: Pod-to-service communications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is dynamic. Pods are created and deleted all the time. The Kubernetes
    service is an abstraction to define a set of pods by label selectors. We normally
    use the service to access pods instead of specifying a pod explicitly. When we
    create a service, an `endpoint` object will be created, which describes a set
    of pod IPs that the label selector in that service has selected.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, `endpoint` object will not be created with service creation.
    For example, services without selectors will not create a corresponding `endpoint`
    object. For more information, refer to the service without selectors section in
    [Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting Started
    with Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: Then, how does traffic get from pod to the pod behind service? By default, Kubernetes
    uses iptables to perform the magic by `kube-proxy`. This is explained in the following
    figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00091.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s reuse the `3-2-3_rc1.yaml` and `3-2-3_nodeport.yaml` examples from [Chapter
    3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting Started with
    Kubernetes*, to observe the default behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Let's observe iptable rules and see how this works. As shown next, our service
    IP is `10.0.0.167`, two pods IP addresses underneath are `10.1.0.4` and `10.1.0.5`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s get into minikube node by `minikube ssh` and check its iptable rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The key point here is that the service exposes the cluster IP to outside traffic
    from the target `KUBE-SVC-37ROJ3MK6RKFMQ2B`, which links to two custom chains
    `KUBE-SEP-SVVBOHTYP7PAP3J5` and `KUBE-SEP-AYS7I6ZPYFC6YNNF` with statistic mode
    random probability 0.5\. This means, iptables will generate a random number and
    tune it based on the probability distribution 0.5 to the destination. These two
    custom chains have the `DNAT` target set to the corresponding pod IP. The `DNAT`
    target is responsible for changing the packets' destination IP address. By default,
    conntrack is enabled to track the destination and source of connection when the
    traffic comes in. All of this results in a routing behavior. When the traffic
    comes to service, iptables will randomly pick one of the pods to route, and modify
    the destination IP from service IP to real pod IP, and un-DNAT to go all the way
    back.
  prefs: []
  type: TYPE_NORMAL
- en: External-to-service communications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ability to serve external traffic to Kubernetes is critical. Kubernetes
    provides two API objects to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Service**: External network LoadBalancer or NodePort (L4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ingress:** HTTP(S) LoadBalancer (L7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For ingress, we''ll learn more in the next section. We''ll focus on L4 first.
    Based on what we''ve learned about pod-to-pod communication across nodes, how
    the packet goes in and out between service and pod. The following figure shows
    how it works. Let''s say we have two services, one service A has three pods (pod
    a, pod b, and pod c) and another service B gets only one pod (pod d). When the
    traffic comes in from LoadBalancer, the packet will be dispatched to one of the
    nodes. Most of the cloud LoadBalancer itself is not aware of pods or containers.
    It only knows about the node. If the node passes the health check, then it will
    be the candidate for the destination. Assume that we want to access service B,
    it currently only has one pod running on one node. However, LoadBalancer sends
    the packet to another node that doesn''t have any of our desired pods running.
    The traffic route will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00092.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The packet routing journey will be:'
  prefs: []
  type: TYPE_NORMAL
- en: LoadBalancer will choose one of the nodes to forward the packet. In GCE, it
    selects the instance based on a hash of the source IP and port, destination IP
    and port, and protocol. In AWS, it's based on a round-robin algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, the routing destination will be changed to pod d (DNAT) and forward it
    to the other node similar to pod-to-pod communication across nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, comes service-to-pod communication. The packet arrives at pod d with the
    response accordingly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pod-to-service communication is manipulated by iptables as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The packet will be forwarded to the original node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The source and destination will be un-DNAT to LoadBalancer and client, and sent
    all the way back.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Kubernetes 1.7, there is a new attribute in service called **externalTrafficPolicy**.
    You can set its value to local, then after the traffic goes into a node, Kubernetes
    will route the pods on that node, if any.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pods and services in Kubernetes have their own IP; however, it is normally not
    the interface you'd provide to the external internet. Though there is service
    with node IP configured, the port in the node IP can't be duplicated among the
    services. It is cumbersome to decide which port to manage with which service.
    Furthermore, the node comes and goes, it wouldn't be clever to provide a static
    node IP to external service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ingress defines a set of rules that allows the inbound connection to access
    Kubernetes cluster services. It brings the traffic into the cluster at L7, allocates
    and forwards a port on each VM to the service port. This is shown in the following
    figure. We define a set of rules and post them as source type ingress to the API
    server. When the traffic comes in, the ingress controller will then fulfill and
    route the ingress by the ingress rules. As shown in the following figure, ingress
    is used to route external traffic to the kubernetes endpoints by different URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00093.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will go through an example and see how this works. In this example,
    we''ll create two services named `nginx` and `echoserver` with ingress path `/welcome`
    and `/echoserver` configured. We can run this in minikube. The old version of
    minikube doesn''t enable ingress by default; we''ll have to enable it first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Enabling ingress in minikube will create an nginx ingress controller and a
    `ConfigMap` to store nginx configuration (refer to [https://github.com/kubernetes/ingress/blob/master/controllers/nginx/README.md](https://github.com/kubernetes/ingress/blob/master/controllers/nginx/README.md))),
    and a RC and service as default HTTP backend for handling unmapped requests. We
    could observe them by adding `--namespace=kube-system` in the `kubectl` command.
    Next, let''s create our backend resources. Here is our nginx `Deployment` and
    `Service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll then create another service with RS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll create the ingress resource. There is an annotation named `ingress.kubernetes.io/rewrite-target`.
    This is required if the service requests are coming from the root URL. Without
    a rewrite annotation, we''ll get 404 as response. Refer to [https://github.com/kubernetes/ingress/blob/master/controllers/nginx/configuration.md#annotations](https://github.com/kubernetes/ingress/blob/master/controllers/nginx/configuration.md#annotations)
    for more supported annotation in nginx ingress controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In some cloud providers, service LoadBalancer controller is supported. It could
    be integrated with ingress via the `status.loadBalancer.ingress` syntax in the
    configuration file. For more information, refer to [https://github.com/kubernetes/contrib/tree/master/service-loadbalancer](https://github.com/kubernetes/contrib/tree/master/service-loadbalancer).
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our host is set to `devops.k8s`, it will only return if we access it
    from that hostname. You could either configure the DNS record in the DNS server,
    or modify the hosts file in local. For simplicity, we''ll just add a line with
    the `ip hostname` format in the host file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we should be able to access our service by the URL directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The pod ingress controller dispatches the traffic based on the URL path. The
    routing path is similar to external-to-service communication. The packet hops
    between nodes and pods. Kubernetes is pluggable. Lots of third-party implementation
    is going on. We only scratch the surface here while iptables is just a default
    and common implementation. Networking evolves a lot in every single release. At
    the time of this writing, Kubernetes had just released version 1.7.
  prefs: []
  type: TYPE_NORMAL
- en: Network policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Network policy works as a software firewall to the pods. By default, every pod
    could communicate with each other without any boundaries. Network policy is one
    of the isolations you could apply to the pods. It defines who can access which
    pods in which port by namespace selector and pod selector. Network policy in a
    namespace is additive, and once a pod has policy on, it denies any other ingress
    (also known as default deny all).
  prefs: []
  type: TYPE_NORMAL
- en: Currently, there are multiple network providers that support network policy,
    such as Calico ([https://www.projectcalico.org/calico-network-policy-comes-to-kubernetes/](https://www.projectcalico.org/calico-network-policy-comes-to-kubernetes/)),
    Romana ([https://github.com/romana/romana](https://github.com/romana/romana))),
    Weave Net ([https://www.weave.works/docs/net/latest/kube-addon/#npc)](https://www.weave.works/docs/net/latest/kube-addon/#npc)),
    Contiv ([http://contiv.github.io/documents/networking/policies.html)](http://contiv.github.io/documents/networking/policies.html))
    and Trireme ([https://github.com/aporeto-inc/trireme-kubernetes](https://github.com/aporeto-inc/trireme-kubernetes)).
    Users are free to choose any options. For simplicity, we're going to use Calico
    with minikube. To do that, we'll have to launch minikube with the `--network-plugin=cni`
    option. Network policy is still pretty new in Kubernetes at this point. We're
    running Kubernetes version v.1.7.0 with v.1.0.7 minikube ISO to deploy Calico
    by self-hosted solution ([http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/hosted/](http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/hosted/)).
    First, we'll have to download a `calico.yaml` ([https://github.com/projectcalico/calico/blob/master/v2.4/getting-started/kubernetes/installation/hosted/calico.yaml](https://github.com/projectcalico/calico/blob/master/v2.4/getting-started/kubernetes/installation/hosted/calico.yaml)))
    file to create Calico nodes and policy controller. `etcd_endpoints` needs to be
    configured. To find out the IP of etcd, we need to access localkube resources.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The default port of etcd is `2379`. In this case, we modify `etcd_endpoint`
    in `calico.yaml` from `http://127.0.0.1:2379` to `http://10.0.2.15:2379`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s reuse `5-2-1_nginx.yaml` as the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We will find that our nginx service has IP `10.0.0.42`. Let''s launch a simple
    bash and use `wget` to see if we can access our nginx:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `--spider` parameter is used to check whether the URL exists. In this case,
    busybox can access nginx successfully. Next, let''s apply a `NetworkPolicy` to
    our nginx pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see some important syntax here. The `podSelector` is used to select
    pods, which should match the labels of the target pod. Another one is `ingress[].from[].podSelector`,
    which is used to define who can access these pods. In this case, all the pods
    with `project=chapter5` labels are eligible to access the pods with `server=nginx`
    labels. If we go back to our busybox pod, we''re unable to contact nginx anymore
    because right now, the nginx pod has NetworkPolicy on. By default, it is deny
    all, so busybox won''t be able to talk to nginx:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We could use `kubectl edit deployment busybox` to add the label `project=chaper5`
    into busybox pods.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the labels and selectors section in [Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Getting Started with Kubernetes* if you forget how to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we can contact nginx pod again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'With the help of the preceding example, we have an idea how to apply network
    policy. We could also apply some default polices to deny all or allow all by tweaking
    the selector to select nobody or everybody. For example, deny all behavior could
    be achieved as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This way, all pods that don't match labels will deny all other traffic. Alternatively,
    we could create a `NetworkPolicy` whose ingress is listed from everywhere. Then
    the pods running in this namespace could be accessed by anyone else.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how containers communicate with each other
    as it is essential, and we introduced how pod-to-pod communication works. Service
    is an abstraction to route the traffic to any of the pods underneath, if label
    selectors match. We learned how service works with pod by iptables magic. We got
    to know how packet routes from external to a pod and the DNAT, un-DAT tricks.
    We also learned new API objects such as *ingress*, which allow us to use the URL
    path to route to different services in the backend. In the end, another object
    `NetworkPolicy` was introduced. It provides a second layer of security, acting
    as a software firewall rule. With network policy, we can make certain pods communicate
    only with certain pods. For example, only data retrieval service can talk to the
    database container. All of these things make Kubernetes more flexible, secure,
    and powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Until now, we've learned the basic concepts of Kubernetes. Next, we'll get a
    clearer understanding of what is happening inside your cluster by monitoring cluster
    metrics and analyzing applications and system logs for Kubernetes. Monitoring
    and logging tools are essential for every DevOps, which also play an extremely
    important role in dynamic clusters such as Kubernetes. So we'll get an insight
    into the activities of the cluster, such as scheduling, deployment, scaling, and
    service discovery. The next chapter will help you better understand the act of
    operating Kubernetes in the real world.
  prefs: []
  type: TYPE_NORMAL
