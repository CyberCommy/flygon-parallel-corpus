- en: Chapter 1. Getting Started with Apache Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will set up Spark and configure it. This chapter is divided
    into the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark from binaries
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the Spark source code with Maven
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launching Spark on Amazon EC2
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Spark on a cluster in standalone mode
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Spark on a cluster with Mesos
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Spark on a cluster with YARN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Tachyon as an off-heap storage layer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a general-purpose cluster computing system to process big data
    workloads. What sets Spark apart from its predecessors, such as MapReduce, is
    its speed, ease-of-use, and sophisticated analytics.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark was originally developed at AMPLab, UC Berkeley, in 2009\. It was
    made open source in 2010 under the BSD license and switched to the Apache 2.0
    license in 2013\. Toward the later part of 2013, the creators of Spark founded
    Databricks to focus on Spark's development and future releases.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Talking about speed, Spark can achieve sub-second latency on big data workloads.
    To achieve such low latency, Spark makes use of the memory for storage. In MapReduce,
    memory is primarily used for actual computation. Spark uses memory both to compute
    and store objects.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Spark also provides a unified runtime connecting to various big data storage
    sources, such as HDFS, Cassandra, HBase, and S3\. It also provides a rich set
    of higher-level libraries for different big data compute tasks, such as machine
    learning, SQL processing, graph processing, and real-time streaming. These libraries
    make development faster and can be combined in an arbitrary fashion.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Though Spark is written in Scala, and this book only focuses on recipes in Scala,
    Spark also supports Java and Python.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Spark is an open source community project, and everyone uses the pure open source
    Apache distributions for deployments, unlike Hadoop, which has multiple distributions
    available with vendor enhancements.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the Spark ecosystem:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction](img/3056_01_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: The Spark runtime runs on top of a variety of cluster managers, including YARN
    (Hadoop's compute framework), Mesos, and Spark's own cluster manager called **standalone
    mode**. Tachyon is a memory-centric distributed file system that enables reliable
    file sharing at memory speed across cluster frameworks. In short, it is an off-heap
    storage layer in memory, which helps share data across jobs and users. Mesos is
    a cluster manager, which is evolving into a data center operating system. YARN
    is Hadoop's compute framework that has a robust resource management feature that
    Spark can seamlessly use.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark from binaries
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark can be either built from the source code or precompiled binaries can be
    downloaded from [http://spark.apache.org](http://spark.apache.org). For a standard
    use case, binaries are good enough, and this recipe will focus on installing Spark
    using binaries.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All the recipes in this book are developed using Ubuntu Linux but should work
    fine on any POSIX environment. Spark expects Java to be installed and the `JAVA_HOME`
    environment variable to be set.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'In Linux/Unix systems, there are certain standards for the location of files
    and directories, which we are going to follow in this book. The following is a
    quick cheat sheet:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '| Directory | Description |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| `/bin` | Essential command binaries |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| `/etc` | Host-specific system configuration |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| `/opt` | Add-on application software packages |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| `/var` | Variable data |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| `/tmp` | Temporary files |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| `/home` | User home directories |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: How to do it...
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the time of writing this, Spark's current version is 1.4\. Please check the
    latest version from Spark's download page at [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
    Binaries are developed with a most recent and stable version of Hadoop. To use
    a specific version of Hadoop, the recommended approach is to build from sources,
    which will be covered in the next recipe.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the installation steps:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the terminal and download binaries using the following command:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Unpack binaries:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Rename the folder containing binaries by stripping the version information:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Move the configuration folder to the `/etc` folder so that it can be made a
    symbolic link later:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create your company-specific installation directory under `/opt`. As the recipes
    in this book are tested on `infoobjects` sandbox, we are going to use `infoobjects`
    as directory name. Create the `/opt/infoobjects` directory:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Move the `spark` directory to `/opt/infoobjects` as it''s an add-on software
    package:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Change the ownership of the `spark` home directory to `root`:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Change permissions of the `spark` home directory, `0755 = user:read-write-execute
    group:read-execute world:read-execute`:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Move to the `spark` home directory:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create the symbolic link:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Append to `PATH` in `.bashrc`:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Open a new terminal.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the `log` directory in `/var`:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Make `hduser` the owner of the Spark `log` directory.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create the Spark `tmp` directory:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Configure Spark with the help of the following command lines:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Building the Spark source code with Maven
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Installing Spark using binaries works fine in most cases. For advanced cases,
    such as the following (but not limited to), compiling from the source code is
    a better option:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Compiling for a specific Hadoop version
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding the Hive integration
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding the YARN integration
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the prerequisites for this recipe to work:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Java 1.6 or a later version
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maven 3.x
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the steps to build the Spark source code with Maven:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Increase `MaxPermSize` for heap:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Open a new terminal window and download the Spark source code from GitHub:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Unpack the archive:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Move to the `spark` directory:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Compile the sources with these flags: Yarn enabled, Hadoop version 2.4, Hive
    enabled, and skipping tests for faster compilation:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Move the `conf` folder to the `etc` folder so that it can be made a symbolic
    link:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Move the `spark` directory to `/opt` as it''s an add-on software package:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Change the ownership of the `spark` home directory to `root`:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Change the permissions of the `spark` home directory `0755 = user:rwx group:r-x
    world:r-x`:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Move to the `spark` home directory:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Create a symbolic link:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Put the Spark executable in the path by editing `.bashrc`:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create the `log` directory in `/var`:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Make `hduser` the owner of the Spark `log` directory:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Create the Spark `tmp` directory:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Configure Spark with the help of the following command lines:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Launching Spark on Amazon EC2
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon Elastic Compute Cloud** (**Amazon EC2**) is a web service that provides
    resizable compute instances in the cloud. Amazon EC2 provides the following features:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: On-demand delivery of IT resources via the Internet
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The provision of as many instances as you like
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Payment for the hours you use instances like your utility bill
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No setup cost, no installation, and no overhead at all
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you no longer need instances, you either shut down or terminate and walk
    away
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The availability of these instances on all familiar operating systems
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EC2 provides different types of instances to meet all compute needs, such as
    general-purpose instances, micro instances, memory-optimized instances, storage-optimized
    instances, and others. They have a free tier of micro-instances to try.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `spark-ec2` script comes bundled with Spark and makes it easy to launch,
    manage, and shut down clusters on Amazon EC2.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you start, you need to do the following things:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the Amazon AWS account ([http://aws.amazon.com](http://aws.amazon.com)).
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Security Credentials** under your account name in the top-right corner.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Access Keys** and **Create New Access Key**:![Getting ready](img/3056_01_02.jpg)
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note down the access key ID and secret access key.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now go to **Services** | **EC2**.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Key Pairs** in left-hand menu under NETWORK & SECURITY.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Create Key Pair** and enter `kp-spark` as key-pair name:![Getting
    ready](img/3056_01_15.jpg)
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the private key file and copy it in the `/home/hduser/keypairs folder`.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set permissions on key file to `600`.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set environment variables to reflect access key ID and secret access key (please
    replace sample values with your own values):'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: How to do it...
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark comes bundled with scripts to launch the Spark cluster on Amazon EC2\.
    Let''s launch the cluster using the following command:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Launch the cluster with the example value:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`<key-pair>`: This is the name of EC2 key-pair created in AWS'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<key-file>`: This is the private key file you downloaded'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<num-slaves>`: This is the number of slave nodes to launch'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<cluster-name>`: This is the name of the cluster'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sometimes, the default availability zones are not available; in that case,
    retry sending the request by specifying the specific availability zone you are
    requesting:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If your application needs to retain data after the instance shuts down, attach
    EBS volume to it (for example, a 10 GB space):'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'If you use Amazon spot instances, here''s the way to do it:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Spot instances allow you to name your own price for Amazon EC2 computing capacity.
    You simply bid on spare Amazon EC2 instances and run them whenever your bid exceeds
    the current spot price, which varies in real-time based on supply and demand (source:
    [amazon.com](http://amazon.com)).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: After everything is launched, check the status of the cluster by going to the
    web UI URL that will be printed at the end.![How to do it...](img/3056_01_03.jpg)
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the status of the cluster:![How to do it...](img/3056_01_04.jpg)
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, to access the Spark cluster on EC2, let''s connect to the master node
    using **secure shell protocol** (**SSH**):'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You should get something like the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3056_01_05.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: 'Check directories in the master node and see what they do:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| Directory | Description |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| `ephemeral-hdfs` | This is the Hadoop instance for which data is ephemeral
    and gets deleted when you stop or restart the machine. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| `persistent-hdfs` | Each node has a very small amount of persistent storage
    (approximately 3 GB). If you use this instance, data will be retained in that
    space. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| `hadoop-native` | These are native libraries to support Hadoop, such as snappy
    compression libraries. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| `Scala` | This is Scala installation. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| `shark` | This is Shark installation (Shark is no longer supported and is
    replaced by Spark SQL). |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| `spark` | This is Spark installation |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| `spark-ec2` | These are files to support this cluster deployment. |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| `tachyon` | This is Tachyon installation |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: 'Check the HDFS version in an ephemeral instance:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Check the HDFS version in persistent instance with the following command:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Change the configuration level in logs:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The default log level information is too verbose, so let's change it to Error:![How
    to do it...](img/3056_01_06.jpg)
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the `log4.properties` file by renaming the template:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Open `log4j.properties` in vi or your favorite editor:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Change second line from `| log4j.rootCategory=INFO, console` to `| log4j.rootCategory=ERROR,
    console`.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Copy the configuration to all slave nodes after the change:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You should get something like this:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3056_01_07.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: 'Destroy the Spark cluster:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: See also
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[http://aws.amazon.com/ec2](http://aws.amazon.com/ec2)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying on a cluster in standalone mode
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compute resources in a distributed environment need to be managed so that resource
    utilization is efficient and every job gets a fair chance to run. Spark comes
    along with its own cluster manager conveniently called **standalone mode**. Spark
    also supports working with YARN and Mesos cluster managers.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: The cluster manager that should be chosen is mostly driven by both legacy concerns
    and whether other frameworks, such as MapReduce, are sharing the same compute
    resource pool. If your cluster has legacy MapReduce jobs running, and all of them
    cannot be converted to Spark jobs, it is a good idea to use YARN as the cluster
    manager. Mesos is emerging as a data center operating system to conveniently manage
    jobs across frameworks, and is very compatible with Spark.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: If the Spark framework is the only framework in your cluster, then standalone
    mode is good enough. As Spark evolves as technology, you will see more and more
    use cases of Spark being used as the standalone framework serving all big data
    compute needs. For example, some jobs may be using Apache Mahout at present because
    MLlib does not have a specific machine-learning library, which the job needs.
    As soon as MLlib gets this library, this particular job can be moved to Spark.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s consider a cluster of six nodes as an example setup: one master and
    five slaves (replace them with actual node names in your cluster):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: How to do it...
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since Spark''s standalone mode is the default, all you need to do is to have
    Spark binaries installed on both master and slave machines. Put `/opt/infoobjects/spark/sbin`
    in path on every node:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Start the standalone master server (SSH to master first):'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Master, by default, starts on port 7077, which slaves use to connect to it.
    It also has a web UI at port 8088.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'Please SSH to master node and start slaves:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '| Argument (for fine-grained configuration, the following parameters work with
    both master and slaves) | Meaning |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| `-i <ipaddress>,-ip <ipaddress>` | IP address/DNS service listens on |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| `-p <port>, --port <port>` | Port service listens on |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| `--webui-port <port>` | Port for web UI (by default, 8080 for master and
    8081 for worker) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| `-c <cores>,--cores <cores>` | Total CPU cores Spark applications that can
    be used on a machine (worker only) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| `-m <memory>,--memory <memory>` | Total RAM Spark applications that can be
    used on a machine (worker only) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| `-d <dir>,--work-dir <dir>` | The directory to use for scratch space and
    job output logs |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: Rather than manually starting master and slave daemons on each node, it can
    also be accomplished using cluster launch scripts.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, create the `conf/slaves` file on a master node and add one line per
    slave hostname (using an example of five slaves nodes, replace with the DNS of
    slave nodes in your cluster):'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Once the slave machine is set up, you can call the following scripts to start/stop
    cluster:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '| Script name | Purpose |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| `start-master.sh` | Starts a master instance on the host machine |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| `start-slaves.sh` | Starts a slave instance on each node in the slaves file
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| `start-all.sh` | Starts both master and slaves |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| `stop-master.sh` | Stops the master instance on the host machine |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| `stop-slaves.sh` | Stops the slave instance on all nodes in the slaves file
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| `stop-all.sh` | Stops both master and slaves |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: 'Connect an application to the cluster through the Scala code:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Connect to the cluster through Spark shell:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: How it works...
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In standalone mode, Spark follows the master slave architecture, very much like
    Hadoop, MapReduce, and YARN. The compute master daemon is called **Spark master**
    and runs on one master node. Spark master can be made highly available using ZooKeeper.
    You can also add more standby masters on the fly, if needed.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'The compute slave daemon is called **worker** and is on each slave node. The
    worker daemon does the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Reports the availability of compute resources on a slave node, such as the number
    of cores, memory, and others, to Spark master
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spawns the executor when asked to do so by Spark master
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restarts the executor if it dies
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is, at most, one executor per application per slave machine.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Both Spark master and worker are very lightweight. Typically, memory allocation
    between 500 MB to 1 GB is sufficient. This value can be set in `conf/spark-env.sh`
    by setting the `SPARK_DAEMON_MEMORY` parameter. For example, the following configuration
    will set the memory to 1 gigabits for both master and worker daemon. Make sure
    you have `sudo` as the super user before running it:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'By default, each slave node has one worker instance running on it. Sometimes,
    you may have a few machines that are more powerful than others. In that case,
    you can spawn more than one worker on that machine by the following configuration
    (only on those machines):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Spark worker, by default, uses all cores on the slave machine for its executors.
    If you would like to limit the number of cores the worker can use, you can set
    it to that number (for example, 12) by the following configuration:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Spark worker, by default, uses all the available RAM (1 GB for executors).
    Note that you cannot allocate how much memory each specific executor will use
    (you can control this from the driver configuration). To assign another value
    for the total memory (for example, 24 GB) to be used by all executors combined,
    execute the following setting:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'There are some settings you can do at the driver level:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify the maximum number of CPU cores to be used by a given application
    across the cluster, you can set the `spark.cores.max` configuration in Spark submit
    or Spark shell as follows:'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'To specify the amount of memory each executor should be allocated (the minimum
    recommendation is 8 GB), you can set the `spark.executor.memory` configuration
    in Spark submit or Spark shell as follows:'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The following diagram depicts the high-level architecture of a Spark cluster:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3056_01_08.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: See also
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/spark-standalone.html](http://spark.apache.org/docs/latest/spark-standalone.html)
    to find more configuration options'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying on a cluster with Mesos
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mesos is slowly emerging as a data center operating system to manage all compute
    resources across a data center. Mesos runs on any computer running the Linux operating
    system. Mesos is built using the same principles as Linux kernel. Let's see how
    we can install Mesos.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Mesosphere provides a binary distribution of Mesos. The most recent package
    for the Mesos distribution can be installed from the Mesosphere repositories by
    performing the following steps:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute Mesos on Ubuntu OS with the trusty version:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Update the repositories:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Install Mesos:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: To connect Spark to Mesos to integrate Spark with Mesos, make Spark binaries
    available to Mesos and configure the Spark driver to connect to Mesos.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use Spark binaries from the first recipe and upload to HDFS:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The master URL for single master Mesos is `mesos://host:5050`, and for the ZooKeeper
    managed Mesos cluster, it is `mesos://zk://host:2181`.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set the following variables in `spark-env.sh`:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Run from the Scala program:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Run from the Spark shell:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Note
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Mesos has two run modes:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine-grained**: In fine-grained (default) mode, every Spark task runs as
    a separate Mesos task'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '**Coarse-grained**: This mode will launch only one long-running Spark task
    on each Mesos machine'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'To run in the coarse-grained mode, set the `spark.mesos.coarse` property:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Deploying on a cluster with YARN
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Yet another resource negotiator** (**YARN**) is Hadoop''s compute framework
    that runs on top of HDFS, which is Hadoop''s storage layer.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: YARN follows the master slave architecture. The master daemon is called `ResourceManager`
    and the slave daemon is called `NodeManager`. Besides this application, life cycle
    management is done by `ApplicationMaster`, which can be spawned on any slave node
    and is alive for the lifetime of an application.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: When Spark is run on YARN, `ResourceManager` performs the role of Spark master
    and `NodeManagers` work as executor nodes.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: While running Spark with YARN, each Spark executor is run as YARN container.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running Spark on YARN requires a binary distribution of Spark that has YARN
    support. In both Spark installation recipes, we have taken care of it.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run Spark on YARN, the first step is to set the configuration:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'You can see this in the following screenshot:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3056_01_09.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
- en: 'The following command launches YARN Spark in the `yarn-client` mode:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Here''s an example:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The following command launches Spark shell in the `yarn-client` mode:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The command to launch in the `yarn-cluster` mode is as follows:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Here''s an example:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: How it works…
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark applications on YARN run in two modes:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '`yarn-client`: Spark Driver runs in the client process outside of YARN cluster,
    and `ApplicationMaster` is only used to negotiate resources from ResourceManager'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`yarn-cluster`: Spark Driver runs in `ApplicationMaster` spawned by `NodeManager`
    on a slave node'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `yarn-cluster` mode is recommended for production deployments, while the
    y`arn-client` mode is good for development and debugging when you would like to
    see immediate output. There is no need to specify Spark master in either mode
    as it's picked from the Hadoop configuration, and the master parameter is either
    `yarn-client` or `yarn-cluster`.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows how Spark is run with YARN in the client mode:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_01_10.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
- en: 'The following figure shows how Spark is run with YARN in the cluster mode:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_01_11.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
- en: 'In the YARN mode, the following configuration parameters can be set:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '`--num-executors`: Configure how many executors will be allocated'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--executor-memory`: RAM per executor'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--executor-cores`: CPU cores per executor'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Tachyon as an off-heap storage layer
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark RDDs are a great way to store datasets in memory while ending up with
    multiple copies of the same data in different applications. Tachyon solves some
    of the challenges with Spark RDD management. A few of them are:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: RDD only exists for the duration of the Spark application
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same process performs the compute and RDD in-memory storage; so, if a process
    crashes, in-memory storage also goes away
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Different jobs cannot share an RDD even if they are for the same underlying
    data, for example, an HDFS block that leads to:'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slow writes to disk
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplication of data in memory, higher memory footprint
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the output of one application needs to be shared with the other application,
    it's slow due to the replication in the disk
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tachyon provides an off-heap memory layer to solve these problems. This layer,
    being off-heap, is immune to process crashes and is also not subject to garbage
    collection. This also lets RDDs be shared across applications and outlive a specific
    job or session; in essence, one single copy of data resides in memory, as shown
    in the following figure:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![Using Tachyon as an off-heap storage layer](img/3056_01_12.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s download and compile Tachyon (Tachyon, by default, comes configured
    for Hadoop 1.0.4, so it needs to be compiled from sources for the right Hadoop
    version). Replace the version with the current version. The current version at
    the time of writing this book is 0.6.4:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Unarchive the source code:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Remove the version from the `tachyon` source folder name for convenience:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Change the directory to the `tachyon` folder:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Comment the following line:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Uncomment the following line:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Change the following properties:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Replace `${tachyon.home}` with `/var/log/tachyon`.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new `core-site.xml` file in the `conf` directory:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Add `<tachyon home>/bin` to the path:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Restart the shell and format Tachyon:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Tachyon''s web interface is `http://hostname:19999`:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3056_01_13.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
- en: 'Run the sample program to see whether Tachyon is running fine:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '![How to do it...](img/3056_01_14.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
- en: 'You can stop Tachyon any time by running the following command:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Run Spark on Tachyon:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: See also
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[http://www.cs.berkeley.edu/~haoyuan/papers/2013_ladis_tachyon.pdf](http://www.cs.berkeley.edu/~haoyuan/papers/2013_ladis_tachyon.pdf)
    to learn about the origins of Tachyon'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击链接[http://www.cs.berkeley.edu/~haoyuan/papers/2013_ladis_tachyon.pdf](http://www.cs.berkeley.edu/~haoyuan/papers/2013_ladis_tachyon.pdf)了解Tachyon的起源
- en: '[http://www.tachyonnexus.com](http://www.tachyonnexus.com)'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击链接[http://www.tachyonnexus.com](http://www.tachyonnexus.com)
