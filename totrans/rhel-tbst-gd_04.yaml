- en: Chapter 4. Troubleshooting Performance Issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](part0022_split_000.html#KVCC1-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 3. Troubleshooting a Web Application"), *Troubleshooting a Web Application*
    we walked through troubleshooting a web application problem by using the troubleshooting
    methodology covered in [Chapter 1](part0014_split_000.html#DB7S1-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 1. Troubleshooting Best Practices"), *Troubleshooting Best Practices*.
    We also used several of the fundamental troubleshooting commands and resources
    found in [Chapter 2](part0019_split_000.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 2. Troubleshooting Commands and Sources of Useful Information"), *Troubleshooting
    Commands and Sources of Useful Information*.
  prefs: []
  type: TYPE_NORMAL
- en: Performance issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, we will continue the scenario that we covered in [Chapter
    3](part0022_split_000.html#KVCC1-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 3. Troubleshooting
    a Web Application"), *Troubleshooting a Web Application*, where we are a new systems
    administrator at a new company. As we arrive to start our day, a fellow systems
    administrator asks us to look into a server being "slow."
  prefs: []
  type: TYPE_NORMAL
- en: When asked for details, the only information our colleague could provide was
    the hostname and the IP of the server deemed "slow." Our peer mentioned that a
    user reported it and that the user did not provide many details.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, unlike in the scenario discussed in [Chapter 3](part0022_split_000.html#KVCC1-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 3. Troubleshooting a Web Application"), *Troubleshooting a Web Application*
    we don't have much information to begin with. It also seems that we are not able
    to ask the user troubleshooting questions. It is not uncommon as a systems administrator
    to be required to troubleshoot an issue with very little information. In fact,
    this type of scenario is quite common.
  prefs: []
  type: TYPE_NORMAL
- en: It's slow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '"It''s slow" is problematic to troubleshoot. The biggest problem with a complaint
    about a server or service being slow is that "slow" is relative to the user experiencing
    the issue.'
  prefs: []
  type: TYPE_NORMAL
- en: An important distinction to understand when dealing with any complaint about
    performance is the benchmark that the environment has been designed for. In some
    environments, a system running at 30% CPU utilization could be a business-as-usual
    activity, whereas the other environments may keep their systems running at 10%
    CPU utilization and a spike of 30% utilization would signal an issue.
  prefs: []
  type: TYPE_NORMAL
- en: While troubleshooting and investigating performance issues, it is important
    to look back at the historical performance metrics of the system to ensure that
    you have context around the measurements being collected. This will assist in
    determining whether the current system utilization is expected or abnormal.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In general, performance issues can be categorized into five areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bottleneck in any one area can often affect other areas as well; therefore,
    it is a good idea to understand each of these topics. By understanding how each
    of these resources is accessed and interacts, you will be able to find the root
    cause of issues that consume multiple resources.
  prefs: []
  type: TYPE_NORMAL
- en: Since the issue being reported did not include any details of the performance
    issue, we will explore and learn about each of these areas. Once complete, we
    will look at the data collected and look at historical statistics to determine
    whether the performance is as expected or whether the system performance really
    is degraded.
  prefs: []
  type: TYPE_NORMAL
- en: Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While creating a list of performance categories, I ordered them by areas that
    I see most often. Every environment is different, but in my experience, the application
    can often be a primary source of performance issues.
  prefs: []
  type: TYPE_NORMAL
- en: While this chapter is designed to cover performance issues, [Chapter 9](part0061_split_000.html#1Q5IA1-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 9. Using System Tools to Troubleshoot Applications"), *Using System Tools
    to Troubleshoot Applications* is dedicated to using system tools for troubleshooting
    application issues, including performance issues. For this chapter, we will assume
    that our issue is not application related and focus specifically on system performance.
  prefs: []
  type: TYPE_NORMAL
- en: CPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CPU is a very common performance bottleneck. Sometimes, issues are strictly
    CPU-based, and at other times, there are instances where an increase in CPU usage
    is a symptom of another issue.
  prefs: []
  type: TYPE_NORMAL
- en: The most common command to investigate CPU utilization is the top command. The
    primary role of this command is to identify the CPU utilization of the processes.
    In [Chapter 2](part0019_split_000.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 2. Troubleshooting Commands and Sources of Useful Information"), *Troubleshooting
    Commands and Sources of Useful Information* we discussed utilizing the `ps` command
    for this type of activity. In this section, we are going to investigate our slowness
    complaint by using both top and ps to investigate our CPU utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Top – a single command to look at everything
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **top** command is one of the first commands that both systems administrators
    and users run to look at the overall system performance. The reason for this is
    that top shows not only a breakdown of Load Average, CPU, and memory, but it also
    shows a sorted list of processes utilizing these resources.
  prefs: []
  type: TYPE_NORMAL
- en: The best part of `top` is the fact that when run without any flags, these details
    are updated every 3 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: The following is an example of the `top` output when run without any flags.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: There is quite a bit of information displayed with just the default output of
    `top`. For this section, we will focus solely on the CPU utilization information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first section of the `top` command''s output, there is a single line
    that shows a breakdown of the current CPU utilization. Each item in this list
    represents a different way in which the CPU is being used. To understand the output
    better, let''s take a look at what each of these values mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '**us – User**: This number is the percentage of CPU being consumed by the processes
    in the user mode. In this mode, applications are not able to access the underlying
    hardware and are required to use system APIs (a.k.a system calls) to perform privileged
    executions. When executing these system calls, the execution will be part of the
    system CPU utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sy – System**: This number is the percentage of CPU being consumed by kernel
    mode execution. In this mode, systems can directly access the underlying hardware;
    this mode is generally reserved for trusted OS processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ni – Nice user processes**: This number is the percentage of CPU time being
    consumed by user processes that have had a nice value set. The `us%` value is
    specifically for processes that have not had their niceness values modified from
    the original value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**id – Idle**: This number is the percentage of CPU time spent being idle.
    Essentially, it is the amount of CPU time spent not being utilized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**wa – Wait**: This number is the percentage of CPU time spent waiting. This
    is typically high when many processes are waiting for I/O devices. I/O wait states
    do not just refer to hard disks, but rather to all I/O devices including hard
    disks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hi – Hardware interrupts**: This number is the percentage of CPU time being
    consumed by hardware interrupts. Hardware interrupts are signals from system hardware,
    such as hard drives or network devices, that are sent to the CPU. These interrupts
    signal that there are events that require CPU time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**si – Software interrupts**: This number is the percentage of CPU time being
    consumed by software interrupts. Software interrupts are similar to hardware interrupts;
    however, they are triggered by a signal sent by the running processes to the kernel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**st – Stolen**: This number specifically applies to Linux systems running
    as a virtual machine. This number is the percentage of CPU time stolen from this
    machine by the host. This number is usually present when the host machine itself
    is running into CPU contention. This can also happen in some cloud environments
    as a method of enforcing resource limitations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Earlier I mentioned that the output from `top` is refreshed every 3 seconds
    by default. The CPU percentage line is also refreshed every 3 seconds; `top` will
    display the percentage of CPU time in each state since the last refresh interval.
  prefs: []
  type: TYPE_NORMAL
- en: What does this output tell us about our issue?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If we review the previous `top` command's output, we can determine quite a bit
    about this system.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding output, we can see that `37.3%` of the CPU time was being
    consumed by processes in the user mode. Another `0.7%` of the CPU time was used
    by processes in the kernel execution mode; this is based on the `us` and `sy`
    values. The `id` value tells us that the rest of the CPU was not utilized, meaning
    that overall, there is ample CPU available on this server.
  prefs: []
  type: TYPE_NORMAL
- en: Another fact that the `top` command shows is the lack of CPU time being spent
    waiting for I/O. We can see this from the `wa` value being `0.0`. This is important
    as it tells us the performance issue that was reported is not likely due to high
    I/O. Later in this chapter, as we start exploring disk performance, we will explore
    I/O wait in depth.
  prefs: []
  type: TYPE_NORMAL
- en: Individual processes from top
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The CPU line in `top` commands output is a summary for the server as a whole,
    but top also includes CPU utilization for individual processes. To get a clearer
    focus, we can execute top again, but this time, let's focus on the `top` running
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This time when executing the `top` command, the `–n` (number) flag was used.
    This flag tells `top` to only refresh for a specified number of times, in this
    case 1 time. This trick can be helpful when trying to capture the output of `top`.
  prefs: []
  type: TYPE_NORMAL
- en: If we review the output of the above `top` command, we can see something quite
    interesting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: By default, the `top` command orders the processes by the percentage of CPU
    utilized by the processes. This means that the first process in the list is the
    process consuming the most amount of CPU in that interval.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the top process running under the process id of `3001`, we find
    that it is using `98.4%` of the CPU time. However, according to the top commands
    system-wide CPU statistics, `65.1%` of the CPU time is spent in an idle state.
    This type of scenario is actually a common source of confusion for many systems
    administrators.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How can a single process be utilizing almost 100% of the CPU time, while the
    system itself is showing 65% of the CPU time is spent idle? The answer turns out
    to be simple; when `top` displays the CPU utilization in its header, the scale
    is based on the system as a whole. With individual processes, however, the CPU
    utilization scale is for one CPU. This means that our process 3001 is actually
    utilizing almost one full CPU and that our system most likely has multiple CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: It is quite common to see processes that are able to utilize multiple CPUs show
    a percentage higher than 100%. For example, a process that is fully utilizing
    three CPUs would show 300%. This can also cause quite a bit of confusion for users
    unfamiliar with the difference of `top` commands server total and per-process
    output.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the number of CPUs available
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previously, we determined that this system must have multiple CPUs available.
    What we did not determine is how many. The easiest method to determine the number
    of CPUs available is to simply read the `/proc/cpuinfo` file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `/proc/cpuinfo` file contains quite a bit of useful information about the
    CPUs available to our system. It shows the type of CPU down to the model, what
    flags are available, the speed of the CPU, and, most importantly, the number of
    CPUs available.
  prefs: []
  type: TYPE_NORMAL
- en: Every CPU available to the system will be listed in the `cpuinfo` file. This
    means that you can simply count the number of processors available in the `cpuinfo`
    file to determine the number of CPUs available to a server.
  prefs: []
  type: TYPE_NORMAL
- en: From the above example, we can determine that this server has 2 CPUs available.
  prefs: []
  type: TYPE_NORMAL
- en: Threads and Cores
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An interesting caveat with using `cpuinfo` to determine whether the number of
    CPUs available is that the details are a bit misleading when working with CPUs
    that have multiple cores and are hyper-threaded. The `cpuinfo` file reports both
    a core and a thread on the CPU as a processor that it can utilize. This means
    that even though you may have one physical chip installed on your system, if that
    chip was a four-core hyper-threaded CPU, the `cpuinfo` file would display eight
    processors.
  prefs: []
  type: TYPE_NORMAL
- en: lscpu – Another way to look at CPU info
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While `/proc/cpuinfo` is a method that many admins and users use to determine
    CPU information; on RHEL-based distributions, there is another command that will
    display this info as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: A difference between the contents of `/proc/cpuinfo` and the `lscpu` command
    is that `lscpu` makes it very easy to identify the number of cores, sockets, and
    threads. It can often be a bit difficult to identify this same information from
    the `/proc/cpuinfo` file.
  prefs: []
  type: TYPE_NORMAL
- en: ps – Drill down deeper on individual processes with ps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the `top` command can be used to look at individual processes, I personally
    feel that the `ps` command is better-suited for investigating running processes.
    In [Chapter 2](part0019_split_000.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 2. Troubleshooting Commands and Sources of Useful Information"), *Troubleshooting
    Commands and Sources of Useful Information* we covered the `ps` command and how
    it can be used to look at many different aspects of a running process.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use the `ps` command to take a deeper look at process
    `3001` that we identified with the `top` command as the process utilizing the
    most CPU time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In [Chapter 2](part0019_split_000.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 2. Troubleshooting Commands and Sources of Useful Information"), *Troubleshooting
    Commands and Sources of Useful Information* we discussed using the `ps` command
    to display running processes. In the preceding example, we specified two flags
    that were shown in [Chapter 2](part0019_split_000.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 2. Troubleshooting Commands and Sources of Useful Information"), *Troubleshooting
    Commands and Sources of Useful Information*, `-l` (long listing) and `–f` (full
    format). In this chapter, we discussed how these flags provide additional details
    for the processes displayed.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand the above process, let's break down the additional details
    that these two flags provide.
  prefs: []
  type: TYPE_NORMAL
- en: 'Current State: `S` (interruptible sleep)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'User: `vagrant`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Process ID: `3001`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parent Process ID: `3000`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Priority Value: `80`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Niceness Level: `0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Command being executed: `lookbusy –cpu-mode-curve –cpu-curve-peak 14h –c 20-80`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Earlier with the `top` command, this process was utilizing almost a full CPU,
    which means that this process is a suspect for the reported slowness. By looking
    at the above details, we can determine a few things about this process.
  prefs: []
  type: TYPE_NORMAL
- en: First, it is a sub process of process `3000`; something we determined by the
    parent process ID. The second being that, when we ran the `ps` command, it was
    waiting for a task to finish; we can determine this by the interruptible sleep
    state that the process is currently in.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to these two items, we can tell that the process does not have
    a high scheduling priority. We can determine this by looking at the priority value,
    which in this case is 80\. The scheduling priority system works as follows: the
    higher the number, the lower the priority that the process has with the system
    scheduler.'
  prefs: []
  type: TYPE_NORMAL
- en: We can also see that the niceness level is set to `0`, the default. This means
    that a user has not adjusted the niceness level to a higher (or lower) priority.
  prefs: []
  type: TYPE_NORMAL
- en: These are all important data points to collect about the process, but by themselves,
    they do not answer whether or not this process is the cause of the reported slowness.
  prefs: []
  type: TYPE_NORMAL
- en: Using ps to determine process CPU utilization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since we know that process `3001` is a child of process `3000`, we should not
    only look at the same information for process `3000` but also use `ps` to identify
    how much CPU process `3000` is utilizing. We can do this all in one command by
    using the `-o` (options) flag with `ps`. This flag allows you to specify your
    own output format; it also allows you to see fields that are not always visible
    via common `ps` flags.
  prefs: []
  type: TYPE_NORMAL
- en: In the following command, the `–o` flag is used to format the `ps` command's
    output with the key fields from our previous run and include the `%cpu` field.
    This additional field will show the CPU utilization of the process. The command
    will also use the `–p` flag to specify both process `3000` and process `3001`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: While the above command is quite long, it shows just how useful the `–o` flag
    can be. Given the right options, it is possible to find out a great deal of information
    about processes with just the `ps` command.
  prefs: []
  type: TYPE_NORMAL
- en: From the above command's output, we can see that process `3000` is yet another
    instance of the `lookbusy` command. We can also see that process `3000` is a child
    process of process `2980`. Before going much further, we should try to identify
    all of the processes associated with process `3001`.
  prefs: []
  type: TYPE_NORMAL
- en: We can do this by using the `ps` command with the `--forest` flag, which tells
    `ps` to print the parent and child processes in a tree format. When provided the
    `–e` (everything) flag, the `ps` command will print all processes in this tree
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By default, the `ps` command will only print processes related to the user who
    executed the command. The `–e` flag changes this behavior to print all possible
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: The below output is truncated to specifically identify the `lookbusy` process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: From the `ps` output above, we can see that the `lookbusy` process with the
    ID `3000` has spawned two processes, namely `3001` and `3002`. We can also see
    that the vagrant user, who is currently logged in via SSH started the `lookbusy`
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: Since we also used the `–o` flag with `ps` to show CPU utilization, we can see
    that process `3002` is utilizing `14.6%` of a single CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is important to note that the `ps` command also displays the percentage of
    CPU time for a single processor, meaning that a process that utilizes more than
    one processor could have a value higher than 100%.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have gone through the commands to identify the system's CPU utilization,
    let's put it together to summarize what has been found.
  prefs: []
  type: TYPE_NORMAL
- en: A quick look with top
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our first step to identifying issues related to CPU performance is to execute
    the `top` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'From the output of `top`, we can identify the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the system is around 60%–70% idle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are two processes running the `lookbusy` command/program, one of which
    appears to be using 70% of a single CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the CPU utilization on this individual process and the system CPU utilization,
    the server in question most likely has multiple CPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can confirm the presence of multiple CPUs with the `lscpu` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes 3001 and 3002 are the top two processes utilizing CPU on this system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CPU wait state percentage is 0, which means that the issue is not likely
    to be disk I/O related
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Digging deeper with ps
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since we identified processes `3001` and `3002` as suspicious from the `top`
    command's output, we can investigate these processes further with the `ps` command.
    To keep our investigation quick, we will use the `ps` command with the `–o` and
    `--forest` flags to identify the maximum possible information with one command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'From this output, we can determine the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Processes 3001 and 3002 are child processes of process 3000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process 3000 was started by the `vagrant` user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `lookbusy` command seems to be a command that utilizes a significant amount
    of CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The method used to launch `lookbusy` is not indicative of a system process but
    rather a user running an ad-hoc command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the above information, there is a possibility that the `lookbusy` process
    launched by the `vagrant` user is the source of the performance issue. This is
    a reasonable hypothesis of the root cause if this system normally operates with
    a lower CPU utilization. However, considering that we are not very familiar with
    this system, it is also possible that the `lookbusy` process using almost a full
    CPU is normal.
  prefs: []
  type: TYPE_NORMAL
- en: Considering that we are not familiar with the system's normal running conditions,
    we should continue to investigate the other possible sources for performance issues
    before reaching a conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After application and CPU utilization, memory utilization is a very common source
    of performance degradation. In the CPU section, we utilized `top` quite extensively,
    and while `top` can also be used to identify system and process memory utilization,
    in this section, we will be using other commands.
  prefs: []
  type: TYPE_NORMAL
- en: free – Looking at free and used memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed in [Chapter 2](part0019_split_000.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 2. Troubleshooting Commands and Sources of Useful Information"), *Troubleshooting
    Commands and Sources of Useful Information* the `free` command simply prints the
    current memory availability and usage for the system.
  prefs: []
  type: TYPE_NORMAL
- en: When executed with no flags, the `free` command will output its values in kilobytes.
    To have the output in megabytes, we can simply execute the `free` command with
    the -m (megabytes) flag.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `free` command shows quite a bit of information about this system and how
    much memory is being used. In order to gain a better understanding of this command,
    let's break down the output a bit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since there are multiple lines in the output, we will start with the first
    line after the output header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The first value in this line is the total amount of **physical memory** available
    to the system. In our case, this is 490 MB. The second value is the amount of
    **memory used** by the system. The third value is the amount of memory on the
    system that is **unused**; note that I used the term "unused" rather than "available."
    The fourth value is the amount of memory used for **shared memory**; unless your
    system uses shared memory often, this is typically a low number.
  prefs: []
  type: TYPE_NORMAL
- en: The fifth value is the amount of memory used for **buffers**. Linux will often
    try to speed up disk access by putting frequently used disk information into physical
    memory. The buffer memory is typically file system metadata. The **Cached memory**,
    which happens to be the sixth value, is the contents of frequently accessed files.
  prefs: []
  type: TYPE_NORMAL
- en: Linux memory buffers and caches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Linux will typically try to use "unused" memory for buffers and caches. This
    means that in order to gain efficiencies, the Linux kernel will store frequently
    accessed file data and filesystem metadata in the unused memory. This allows the
    system to utilize memory that otherwise would not have been used to enhance disk
    access which is often slower than system memory.
  prefs: []
  type: TYPE_NORMAL
- en: This is why the third value "unused" memory is typically a lower number than
    expected.
  prefs: []
  type: TYPE_NORMAL
- en: When a system is running low on unused memory, however, the Linux kernel will
    release the buffers and cached memory, as it needs. This means that even though
    technically, the memory used for buffers and caches is used, it is technically
    available to the system when required.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the second line in the output of free.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The second line has two values, the first being a part of the **Used** column
    and the second being a part of the **Free** or "unused" column. These values are
    Used or Free memory values after taking into consideration the availability of
    buffers and cached memory.
  prefs: []
  type: TYPE_NORMAL
- en: To explain in simpler terms, the Used value on the second line is the result
    of the used memory value from the first line being subtracted by the buffers and
    cached values. For our example, it is 92 MB (used) minus 17 MB (cached).
  prefs: []
  type: TYPE_NORMAL
- en: The free value in the second line is the result of the Free value on the first
    line with the buffers and cached memory added. Using the values from our example,
    this would be 397 MB (free) plus 17 MB (cached).
  prefs: []
  type: TYPE_NORMAL
- en: Swapped memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The third line in the output of the `free` command is for swap memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In this line, there are three columns: available, used, and free. The swap
    memory values are fairly self-explanatory. The available swap value is the amount
    of swap memory available to the system, the used value is the amount of swap currently
    allocated, and the free value is essentially the amount of swap available minus
    the amount of swap allocated.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many environments where a significant amount of swap being allocated
    is frowned upon as this is generally an indicator that the system has run out
    of memory and used the swap space to compensate.
  prefs: []
  type: TYPE_NORMAL
- en: What free tells us about our system
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If we look again at the output of free, we can determine quite a few things
    about this server.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can identify that only a small amount of memory (79 MB) is actually in use.
    This means that overall, the system should have plenty of memory available for
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: There is an additional interesting fact, however, on the third line, it shows
    that **56** MB of memory has been written to swap. Although there is currently
    plenty of memory available on the system, 56 MB has been written to swap. This
    means that at some point in the past, this system might have been low on memory,
    low enough that the system had to swap memory pages from the physical memory to
    the swap memory.
  prefs: []
  type: TYPE_NORMAL
- en: Checking for oomkill
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a Linux system runs out of physical memory, it first attempts to reuse
    the memory allocated to buffers and caches. If there is no additional memory that
    can be reclaimed from these sources, then the kernel will take older memory pages
    from the physical memory and write them to the swap memory. Once both the physical
    and the swap memory have been allocated, the kernel will launch the **out of memory
    killer** (**oomkill**) process. The `oomkill` process is designed to find processes
    that utilize large amounts of memory and kill (stop) them.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the `oomkill` process is unwanted in most environments. When invoked,
    the `oomkill` process can kill many different types of processes. Whether processes
    are part of the system or at the user level, `oomkill` has the ability to kill
    them.
  prefs: []
  type: TYPE_NORMAL
- en: With a performance issue that may have affected memory utilization, it is always
    a good idea to check whether the `oomkill` process was invoked recently or not.
    The easiest way to determine whether `oomkill` was run recently is to simply view
    the console of the system as the initiation of this process is logged directly
    to the system console. In the cloud and virtual environments, however, the console
    may not be available.
  prefs: []
  type: TYPE_NORMAL
- en: Another good way to determine whether `oomkill` was invoked recently is to search
    the `/var/log/messages` log file. We can do this by executing the `grep` command
    and searching for the string `Out of memory`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'For our example system, there have been no `oomkill` invocations recently.
    If our system had invoked the `oomkill` process, we could expect a message similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In [Chapter 11](part0074_split_000.html#26I9K2-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 11. Recovering from Common Failures"), *Recovering from Common Failures*
    we will once again investigate memory issues and take a deeper look into `oomkill`
    and how it works. For this chapter, we can conclude that the system has not completely
    exhausted its available memory.
  prefs: []
  type: TYPE_NORMAL
- en: ps - Checking individual processes memory utilization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, the memory usage on this system seems pretty small, but we know from
    the CPU validation steps that the processes running `lookbusy` are suspicious
    and possibly cause our performance issues. Since we suspect that the `lookbusy`
    processes are a problem, we should also look at how much memory these processes
    are using. To do this, we can once again use the `ps` command with the `-o` flag.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This time, however, we ran our `ps` command a little differently and thus, received
    different results. This time when executing the `ps` command, we used the `–e`
    (everything) flag to show all processes. The results were then piped to `grep`
    in order to narrow filter them to only the processes that match the pattern `lookbusy`.
  prefs: []
  type: TYPE_NORMAL
- en: This is a very common way of using the `ps` command; in fact, it is even more
    common than specifying process ID(s) on the command line. In addition to using
    `grep`, this `ps` command example introduces a few new formatting options.
  prefs: []
  type: TYPE_NORMAL
- en: '**%mem**: This is the percentage of system memory that the process is using.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rss**: This is the amount of the resident site size of the process, which
    essentially means the amount of memory used by the process that is not swappable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vsize**: This is the amount of virtual memory size; it contains the amount
    of memory that the process is fully using irrespectively of whether this memory
    is a part of the physical memory or of the swap memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**comm**: This option is similar to cmd with the exception that it does not
    display the command-line arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `ps` example shows interesting information, particularly the following
    lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: It seems that several additional `lookbusy` processes have been started and
    these processes are utilizing 40% and 34% of the system memory (by using the `%mem`
    column). From the rss column, we can see that these two processes are using about
    374 MB of the total 490 MB of the physical memory.
  prefs: []
  type: TYPE_NORMAL
- en: It also seems that these processes started utilizing a large amount of memory
    after we started our investigation. Originally, our free output stated that only
    70 MB of memory was in use; however, these processes seem to be utilizing much
    more. We can confirm this by running free again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Our system is in fact utilizing almost all of its memory now; in fact, we are
    also using 310 MB of swap space.
  prefs: []
  type: TYPE_NORMAL
- en: vmstat – Monitoring memory allocation and swapping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since this system seems to have fluctuating memory utilization, there is one
    very useful command that shows memory allocation and de-allocation along with
    the number of pages swapped in and out at regular intervals. This command is called
    `vmstat`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the above example, the `vmstat` command was executed with the `-n` (one-header)
    flag followed by the delay in seconds (10) and the number of reports to generate
    (5). These options tell `vmstat` to only output one header line for this execution
    rather than a new header line for each report, run the report every 10 seconds,
    and limit the number of reports to 5\. If the limitation on the number of reports
    is omitted than `vmstat` will simply run continuously until stopped with *CTRL*+*C*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of `vmstat` can be a bit overwhelming at first, but if we break
    down the output, it will be easier to understand. The output of `vmstat` has six
    output categories, namely Procs, Memory, Swap, IO, System, and CPU. In this section,
    we will focus on two of these categories: Memory and Swap.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`swpd`: Amount of memory written to swap'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`free`: Amount of unused memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`buff`: Amount of memory used as buffers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache`: Amount of memory used as cache'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inact`: Amount of inactive memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`active`: amount of active memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Swap**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`si`: Amount of memory swapped in from disk'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`so`: Amount of memory swapped to disk'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a definition of these values, let's see what the output of
    `vmstat` tells us about this system's memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If we compare the first and the second line from `vmstat's` output, we can see
    a rather large disparity. In particular, we can see that in the first interval,
    the cache memory was `7676`, whereas in the second interval, this value was 2096\.
    We can also see that the `si` or swapped-in value in the first line is 8 but 1887
    in the second line.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this disparity is that the first report of `vmstat` is always
    a summary of statistics since the last reboot, whereas the second report is a
    summary of statistics since the previous report. Each subsequent report will be
    a summary of the previous one, meaning that the third report will summarize statistics
    since the second report. This behavior of `vmstat` can often cause confusion for
    new systems administrators and users; therefore, it is often considered an advanced
    troubleshooting tool.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the method of how `vmstat` generates the first report, the common
    practice is to discard it and start from the second report. We will follow this
    philosophy and specifically look at the second and the third reports.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the second and the third reports, we can see some interesting data.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that sticks out is the fact that from the first report's generation
    time to the second report's generation time, there were 1,887 pages swapped in
    and 130 pages swapped out. The second report also shows that only 35 MB of the
    memory is free with 0 MB of the memory in buffer and 2 MB of the memory in cache.
    Based on how Linux utilizes memory, this means that there is effectively only
    37 MB of available memory on this system.
  prefs: []
  type: TYPE_NORMAL
- en: This low amount of available memory explains why our system has swapped in a
    large number of pages. We can see from the third line that the trend is continuing,
    we continue to swap in quite a few pages and our available memory has reduced
    to roughly 35 MB.
  prefs: []
  type: TYPE_NORMAL
- en: From this example of `vmstat`, we can see that our system is now running out
    of physical memory. Because of this, our system is taking pages of memory from
    the physical RAM and writing it to our swap device.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have explored the tools required for troubleshooting memory utilization,
    let's put all of them together to troubleshoot the issue of slow system performance.
  prefs: []
  type: TYPE_NORMAL
- en: Taking a look at the system's memory utilization with free
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first command to give us a snapshot of the systems memory utilization is
    the `free` command. This command will give us an idea of where to look further
    for any memory utilization issues.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: From the output of `free`, we can see that there is currently 215 MB of memory
    available. We can see this via the `free` column on the second line. We can also
    see that overall, this system has 183 MB of memory that has been swapped to our
    swap devices.
  prefs: []
  type: TYPE_NORMAL
- en: Watch what is happening with vmstat
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since the system has swapped (or rather paged) at some point, we can use the
    `vmstat` command to see whether the system is swapping right now.
  prefs: []
  type: TYPE_NORMAL
- en: When executing `vmstat` this time around, we will leave off the number of reports
    value, which will cause `vmstat` to continuously report memory statistics, similar
    to the top command's output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This `vmstat` output is different from our earlier execution. From this output,
    we can see that while there is quite a bit of memory swapped, the system is not
    currently swapping. We can determine this by the 0 values in both the `si` (swap
    in) and so (swap out) columns.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the memory utilization seems steady during this `vmstat` run. The `free`
    memory value is fairly consistent between each `vmstat` report, as well as the
    cache and buffer memory statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the processes that utilize the most memory with ps
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our system has 490 MB of physical memory, and both `free` and `vmstat` show
    that roughly, 215 MB of memory available. This means that more than half of our
    system memory is currently utilized; with this level of use, it is a good idea
    to find out which processes are utilizing our system's memory. If nothing else,
    this data will be useful to show what the system's current state is.
  prefs: []
  type: TYPE_NORMAL
- en: To identify the process using the highest amount of memory, we can use the `ps`
    command along with sort and tail.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The above example uses pipes to redirect the output of `ps` to the sort command.
    The sort command is performing a numeric (`-n`) sort of the first column (`-k
    1`). This will have the effect of sorting the output, putting the process with
    the highest `rss` size at the bottom. After the `sort` command, the output is
    also piped to the `tail` command, which when specified with the `-n` (number)
    flag followed by a number will limit the output to only include the specified
    number of results.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the concept of chaining commands together with pipes is new, I highly suggest
    practicing this as it is extremely useful for day-to-day `sysadmin` tasks as well
    as during troubleshooting. We will discuss this concept and provide examples several
    times throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: From the output of `ps`, we can see that process 5383 is using roughly 200 MB
    of memory. We can also see that the process is another `lookbusy` process, which
    was again spawned by the vagrant user.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the output of free, `vmstat`, and `ps`, we can determine the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The system currently has roughly 200 MB of available memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the system is not currently swapping, it has in the past, and given what
    we saw earlier from `vmstat`, we know that it was swapping recently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We found that process `5383` is utilizing roughly 200 MB of memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also can see that process `5383` was started by the `vagrant` user and is
    running the `lookbusy` process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `free` command, we can see that this system has 490 MB of physical
    memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the above information, it seems that the `lookbusy` process executed by
    the `vagrant` user is not only a suspicious user of the CPU but also a suspicious
    user of the memory.
  prefs: []
  type: TYPE_NORMAL
- en: Disk
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Disk utilization is another common performance bottleneck. In general, performance
    issues are rarely due to the amount of disk space. While I have seen performance
    issues due to the large number of files or files of a large size, in general,
    disk performance is limited by how much is being written to and read from a disk.
    So, while it is important to know if a file system is full while troubleshooting
    performance issues, file system usage alone does not always indicate whether or
    not there is an issue.
  prefs: []
  type: TYPE_NORMAL
- en: iostat – CPU and device input/output statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `iostat` command is an essential command for troubleshooting disk performance
    issues and is similar to vmstat in terms of both the usage and the information
    that it provides. Like `vmstat`, `iostat` when executed is followed by two numbers,
    the first being the delay in report generation and the second being the number
    of reports to generate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the above example, the `–x` (extended statistics) flag was provided to print
    extended statistics. The extended statistics are extremely useful and provide
    additional information that can be essential for identifying performance bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: CPU details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `iostat` command will display CPU statistics along with I/O statistics.
    This is yet another command that can be utilized to troubleshoot CPU utilization.
    This is particularly useful when the CPU utilization indicates high I/O wait time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The above is the same information displayed from the `top` command; it is not
    uncommon with Linux to find multiple commands that output similar information.
    Since these details have been covered in the CPU troubleshooting section, we will
    focus on the I/O statistics portion of the `iostat` command.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing I/O statistics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To start reviewing the I/O statistics, let's start with the first two reports.
    I am including the CPU utilization below to help indicate where each report starts
    as it is the first item in each statistics report.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: By comparing the first two reports, we find that there is a large disparity
    between them. In the first report, the `%util` value for the `sda` device is `0.56`,
    and it is `65.91` in the second report.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this difference is that as in the case of `vmstat`, the statistics
    from the first execution of `iostat` are based on the last time the server rebooted.
    The second report is based on the time since the first report. This means that
    the output of the second report is based on the 10 s between the first and the
    second report generation. This is the same behavior seen in `vmstat` and is a
    common behavior for other tools that gather performance statistics.
  prefs: []
  type: TYPE_NORMAL
- en: As with `vmstat`, we will discard the first report and only look at the second
    report.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: From the above, we can identify several things about this system. The first
    and most important is the `%iowait` value in the CPU line.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Earlier when executing the top command, the percentage of time spent waiting
    for I/O was quite minimal; however, when running `iostat`, we can see that the
    CPUs are actually spending a lot of time waiting for I/O. While I/O wait does
    not necessarily mean waiting for the disk, the rest of this output seems to suggest
    that there is quite a bit of disk activity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The extended statistics output has many columns, to make this output a little
    easier to understand, let's break down what these columns tell us.
  prefs: []
  type: TYPE_NORMAL
- en: '**rrqm/s**: Number of read requests per second that are merged and queued'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**wrqm/s**: Number of write requests per second that are merged and queued'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**r/s**: Number of read requests per second completed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**w/s**: Number of write requests per second completed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rkB/s**: Number of reads in kilobytes per second'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**wkB/s**: Number of writes in kilobytes per second'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**avgr-sz**: Average size (in sectors) of requests made to the device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**avgqu-sz**: Average queue length of requests made to the device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**await**: Average time in milliseconds that requests wait for to be served'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**r_await**: Average time in milliseconds that read requests wait for to be
    serviced'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**w_await**: Average time in milliseconds that write requests wait for to be
    serviced'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**svctm**: This field is invalid and is slated to be removed; it should not
    be trusted or used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**%util**: Percentage of CPU time spent while I/O requests are being serviced
    by this device. A device can only be at most 100% utilized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For our example, we will focus solely on the `r/s`, `w/s`, `await`, and `%util`
    values, since these values will tell us quite a bit about this system's disk utilization
    while keeping our example simple.
  prefs: []
  type: TYPE_NORMAL
- en: After reviewing the `iostat` output, we can see that both the `sda` and `dm-1`
    devices have the highest `%util` value, meaning that they are the closest to being
    at capacity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: From this report, we can see that the `sda` device had completed an average
    of 764 reads (`r/s`) and 808 writes (`w/s`) per second. We can also identify that
    these requests are taking an average of 39 ms (await) to complete. While these
    numbers are interesting, they do not necessarily mean that the system is in an
    abnormal state. Since we are unfamiliar with this system, we do not necessarily
    know whether the level of reads and writes are unexpected for this system. The
    information is however important to collect, as these statistics are important
    pieces of data for the data collection stage of the troubleshooting process.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting statistic we can see from `iostat` is that the `%util` values
    for both `sda` and `dm-1` devices are about 66%. This means that during the 10
    s between the first report generation and the second, 66% of the CPU time spent
    was spent waiting for either the `sda` or the `dm-1` device.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying devices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Having 66% utilization for a disk device is generally considered high, while
    this is quite useful information, it does not tell us who or what is utilizing
    the disk. To answer these questions, we will need to figure out what exactly `sda`
    and `dm-1` are being used for.
  prefs: []
  type: TYPE_NORMAL
- en: Since devices from `iostat` commands output are generally disk devices, the
    first step to identifying these devices is to run the `mount` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The `mount` command, when run without any options, will display all of the current
    mounted file systems. The first column in the output of `mount` is the device
    that has been mounted. In the output above, we can see that the `sda` device is
    in fact a disk device and that it has a partition called `sda1` that is mounted
    as `/boot`.
  prefs: []
  type: TYPE_NORMAL
- en: What we don't see however is the `dm-1` device. Since this device is not listed
    in the output of the `mount` command another way, we may identify the `dm-1` device
    by looking within the `/dev` folder.
  prefs: []
  type: TYPE_NORMAL
- en: All devices on a system are presented as a file within the `/dev` folder structure.
    The `dm-1` device is no different.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: While we have been able to find the location of the `dm-1` device, we have yet
    to identify its use. One thing that does stick out about this device, however,
    is the name `dm-1`. When devices start with `dm`, this is an indication that the
    device is a logical device created by the device mapper.
  prefs: []
  type: TYPE_NORMAL
- en: Device mapper is a Linux kernel framework that allows the system to create virtual
    disk devices that "map" back to physical devices. This functionality is used for
    many features including software raid, disk encryption, and logical volumes.
  prefs: []
  type: TYPE_NORMAL
- en: A common practice within the device mapper framework is to create symlinks for
    these features that link back to a single logical device. Since we can see with
    the `ls` command that `dm-1` is a block device via the "b" value in the first
    column's output (`brw-rw----.`), we know that `dm-1` is not a symlink. We can
    use this information along with the find command to identify any symlinks that
    link back to the `dm-1` block device.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In the earlier chapters, we used the find command to identify configuration
    and log files. In the above example, we use the `-L` (follow links) flag, followed
    by the `/dev` path and the `--samefile` flag to tell find to search the `/dev`
    folder structure, searching any symlinked folders to identify any file that is
    the "same file" as `/dev/dm-1`.
  prefs: []
  type: TYPE_NORMAL
- en: The `--samefile` flag identifies files that have the same `inode` number. When
    the `-L` flag is included in the command, the output includes symlinks, and it
    seems that this example has returned several results. The symlink file that sticks
    out the most is `/dev/mapper/root`; the reason that this file sticks out is that
    it was also present in the output of the mount command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: It seems that `/dev/mapper/root` appears to be a logical volume. A logical volume
    within Linux is essentially storage virtualization. This functionality allows
    you to create pseudo devices (as part of the device mapper), which is mapped to
    one or more physical devices.
  prefs: []
  type: TYPE_NORMAL
- en: For example, it is possible to take four different hard disks and combine these
    disks into one logical volume. The logical volume can then be used as the disk
    for a single file system. It is even possible to add another hard disk at a later
    time by using logical volumes.
  prefs: []
  type: TYPE_NORMAL
- en: To confirm that the `/dev/mapper/root` device is in fact a logical volume, we
    can execute the `lvdisplay` command, which is used to display the logical volumes
    on the system.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: From the output of `lvdisplay`, we can see an interesting path called `/dev/rhel/root`,
    which also exists with the output of our `find` command. Let's take a look at
    this device with the `ls` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that `/dev/rhel/root` is a symlink to `/dev/dm-1`; this confirms
    that `/dev/rhel/root` is the same as `/dev/dm-1` and that these are in fact logical
    volume devices, which means that these are not really the physical device.
  prefs: []
  type: TYPE_NORMAL
- en: To display the physical device behind these logical volumes, we can use the
    `pvdisplay` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We can see from the output of `pvdisplay` that the `dm-1` device actually maps
    to `sda2`, which explains why the disk utilizations for `dm-1` and `sda` were
    extremely close, as any activity on `dm-1` is actually being performed on `sda`.
  prefs: []
  type: TYPE_NORMAL
- en: Who is writing to these devices?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have found where I/O is being utilized, we need to find out who
    is utilizing this I/O. The easiest method to find out which processes are writing
    to disk the most is to use the `iotop` command. This tool is a relatively new
    command and is now included by default with Red Hat Enterprise Linux 7\. However,
    this command has not always been available in previous RHEL versions.
  prefs: []
  type: TYPE_NORMAL
- en: Before the adoption of `iotop`, the method for finding the top processes that
    are using I/O involved using the `ps` command and looking through the `/proc`
    filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: ps – Using ps to identify processes utilizing I/O
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While collecting data related to the CPU, we covered the state field in the
    output of the `ps` command. What we didn''t cover is the various states that a
    process can be in. The following list contains the seven possible states that
    the `ps` command will show:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Uninterruptible sleep** (`D`): Processes generally in a sleep state when
    waiting for I/O'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Running or Runnable** (`R`): Processes on the run queue'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interruptible sleep** (`S`): Processes waiting for an event to complete but
    not blocking CPU or I/O'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stopped** (`T`): Processes that are stopped by a job control system such
    as the jobs command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Paging** (`P`): Processes that are current paging; however, this is less
    relevant on newer kernels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dead** (`X`): Processes that are dead, this should never be seen, as dead
    processes should not show up when running `ps`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Defunct** (`Z`): Zombie processes that are terminated but left in an undead
    state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When investigating I/O utilization, it is important to identify with a state
    listed as `D` **Uninterruptible Sleep**. As these processes are generally waiting
    for I/O, they are the most likely processes to be over utilizing disk I/O.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we will use the `ps` command with the `–e` (everything), `-l` (long
    format), and `-f` (full format) flags. We will also use pipes again to redirect
    the output to the `grep` command and filter the output to only show processes
    with a `D` state.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: With the above output, we see that there are two processes currently in an uninterruptible
    sleep state. One process is `kworker`, which is a kernel system process, and the
    other is `bonnie++`, a process launched by the root user. As the `kworker` process
    is a generic kernel process, we will focus on the `bonnie++` process first.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand this process, we will run the `ps` command again but this
    time with the `--forest` option.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: By reviewing the above output, we can see that the `bonnie++` process is actually
    a child process of process `16052`, which is another child process of `11243`,
    which is the bash shell for the `vagrant` user.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding `ps` command has shown us that the `bonnie++` process with the
    process id of `16053` is waiting on I/O tasks. However, this does not tell us
    how much I/O this process is using; to determine this, we can read a special file
    in the `/proc` file system called `io`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Every running process has a subfolder in `/proc` with the same name as the process
    `id`; for our example, this is `/proc/16053`. This folder is maintained by the
    kernel for each running process, and within these folders exist many files that
    contain information about running processes.
  prefs: []
  type: TYPE_NORMAL
- en: These files are so useful that they are actually the source of the `ps` command's
    information. One of these useful files is named `io`; the `io` file contains statistics
    about the number of reads and writes that the process has performed.
  prefs: []
  type: TYPE_NORMAL
- en: From the output of the cat command, we can see that this process has read and
    written approximately 1 GB of data. While this seems like a lot, it could be over
    a long period of time. To get a picture of how much this process is writing to
    disk, we can read this file again to capture the differences.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: It seems, however, that when we executed the cat command a second time, we received
    an error that the `io` file is no longer present. If we run the `ps` command again
    and use `grep` to search the output for the bonnie++ process, we can see that
    a `bonnie++` process is running; however, it is a new process with a new process
    `ID`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: As it seems that the child `bonnie++` processes are short-lived processes, following
    the I/O statistics by reading the `io` file may be quite difficult.
  prefs: []
  type: TYPE_NORMAL
- en: iotop – A top top-like command for disk i/o
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since these processes are starting and stopping so frequently, we can use the
    `iotop` command to identify which processes are utilizing I/O the most.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding output from `iotop`, we can see some interesting I/O statistics.
    With `iotop`, we can see not only system-wide statistics such as **Total Disk
    Reads** per second and **Total Disk Writes** per second but also quite a few statistics
    for single processes.
  prefs: []
  type: TYPE_NORMAL
- en: From the per-process perspective, we can see that the `bonnie++` process is
    reading from disk at a rate of 101.96 MBps and is writing to disk at a rate of
    26.96 MBps.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The `iotop` command is very similar to the top command in that it will refresh
    the reported results every few seconds. This has the effect of showing the I/O
    statistics "live."
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Commands such as `top` and `iotop` are very difficult to show in a book format.
    I highly suggest executing these commands on a system that has them available
    to get a feel of how they work.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have covered some of the tools for troubleshooting disk performance
    and utilization, let's put it all together while troubleshooting our reported
    slowness.
  prefs: []
  type: TYPE_NORMAL
- en: Using iostat to determine whether there is a I/O bandwidth problem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first command that we will run is `iostat`, as this will first validate
    for us whether there is in fact an issue or not.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'From the output of `iostat`, we can determine the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The CPU of this system is currently spending quite a bit of time waiting for
    I/O, 30%–40%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It appears that the `dm-1` and `sda` devices are the most-utilized devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From `iostat`, it appears that these devices are at 68% utilization, a number
    that seems is quite high
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the basis of these data points, we can identify that there is a potential
    I/O utilization issue, unless 68% utilization is expected.
  prefs: []
  type: TYPE_NORMAL
- en: Using iotop to determine which processes are consuming disk bandwidth
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that we have determined that a sizeable amount of CPU time is being spent
    waiting for I/O, we should now focus on what processes are utilizing disks the
    most. To do this, we will use the `iotop` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: From the `iotop` command, we can see that process `20262`, which is running
    the `bonnie++` command, has a high utilization along with large disk read and
    write values.
  prefs: []
  type: TYPE_NORMAL
- en: 'From `iotop`, we can determine the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The system's total disk reads per second is 100.64 MBps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system's total disk writes per second is 23.91 MBps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process `20262` running the `bonnie++` command is reading 100.35 MBps and writing
    23.91 MBps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing the totals, we find that process `20262` is the majority contributor
    of disk reads and writes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the above, it seems that we will need to identify more information about
    process `20262`.
  prefs: []
  type: TYPE_NORMAL
- en: Using ps to understand more about processes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that we have identified a process that is using a significant amount of
    I/O, we can investigate the details of this process with the `ps` command. We
    will once again use the `ps` command with the `--forest` flag to show the parent
    and child process relationship.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `ps` command, we can determine the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `bonnie++` process `20262` identified with `iotop` is absent; however, other
    `bonnie++` processes are present
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `vagrant` user has started the parent `bonnie++` processes by using the
    `sudo` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `vagrant` user is the same user as the user in the earlier observations
    discussed in the CPU and memory sections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the above details, it seems that the vagrant user is a likely suspect
    for our performance issues.
  prefs: []
  type: TYPE_NORMAL
- en: Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final common resource for performance issues is the network. There are many
    tools to troubleshoot networking issues; however, very few of these commands are
    geared solely towards network performance. Most of these tools are designed for
    in-depth network troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: Since [Chapter 5](part0032_split_000.html#UGI01-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 5. Network Troubleshooting"), *Network Troubleshooting* is dedicated
    to troubleshooting network issues, this section will focus specifically on performance.
  prefs: []
  type: TYPE_NORMAL
- en: ifstat – Review interface statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it comes to a network, there are about four metrics that can be used to
    measure throughput.
  prefs: []
  type: TYPE_NORMAL
- en: '**Received Packets**: Number of packets received by the interface'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sent Packets**: Number of packets sent out by the interface'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Received Data**: Amount of data received by the interface'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sent Data**: Amount of data sent by the interface'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many commands that can provide these metrics, everything from `ifconfig`
    or `ip` to `netstat`. A very useful utility that specifically outputs these metrics
    is the `ifstat` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Much like `vmstat` or `iostat`, the first report generated by `ifstat` is based
    on statistics since the server last rebooted. What this means is that the above
    report indicates that the `enp0s3` interface has received 70,579 packets since
    the last reboot.
  prefs: []
  type: TYPE_NORMAL
- en: When executing `ifstat` a second time, the results will show a very large disparity
    from the first report. The reason for this is that the second report is based
    on the time since the first report.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: In the example above, we can see that our system received 23 packets (RX Pkts)
    and transmitted 18 packets (`TX Pkts`) over the `enp0s3` interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the `ifstat` command, we can determine the following about our system:'
  prefs: []
  type: TYPE_NORMAL
- en: The network utilization at the moment is fairly small and not likely to cause
    an impact on this system as a whole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The processes from the `vagrant` user shown earlier are not likely utilizing
    a significant amount of network resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the statistics seen with `ifstat`, there is minimal network traffic
    on this system, and is not likely causing the perceived slowness.
  prefs: []
  type: TYPE_NORMAL
- en: Quick review of what we have identified
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before going too far ahead, let''s review what we have learned from the performance
    statistics that we have gathered thus far:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `vagrant` user has been launching processes that run the `bonnie++` and
    `lookbusy` applications.
  prefs: []
  type: TYPE_NORMAL
- en: The `lookbusy` application seems to either use up to 20%–30% of the overall
    system CPU.
  prefs: []
  type: TYPE_NORMAL
- en: This server in question has two CPUs and `lookbusy` seems to utilize about 60%
    of one CPU consistently.
  prefs: []
  type: TYPE_NORMAL
- en: The `lookbusy` application also seems to use around 200 MB of memory consistently;
    however, during troubleshooting, we did see these processes using almost all of
    the system's memory causing the system to swap.
  prefs: []
  type: TYPE_NORMAL
- en: While the `vagrant` user was launching the `bonnie++` process the system was
    experiencing a high I/O wait time.
  prefs: []
  type: TYPE_NORMAL
- en: When running, the `bonnie++` processes were utilizing approximately 60%–70%
    of the disk throughput.
  prefs: []
  type: TYPE_NORMAL
- en: The activity being performed by the `vagrant` user seems to have little to no
    effect on network utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing historical metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at all of the facts that we learned about this system so far, it seems
    that our next best course of action would be to recommend contacting the `vagrant`
    user to identify whether the `lookbusy` and `bonnie++` applications should be
    running with such high resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: While the previous observations show a high resource utilization, this level
    of utilization may be expected for this environment. Before we start contacting
    users, we should first review the historical performance metrics of this server.
    In most environments, there is some sort of server performance monitoring software
    such as Munin, Cacti, or one of the many cloud SaaS providers in place that collects
    and stores system statistics.
  prefs: []
  type: TYPE_NORMAL
- en: If your environment utilizes one of these services, you can use the collected
    performance data to compare previous performance statistics with the information
    that we just gathered. If for instance over the past 30 days, the CPU performance
    was never higher than 10%, it stands to reason that the `lookbusy` processes may
    not have been running at that time.
  prefs: []
  type: TYPE_NORMAL
- en: Even if your environment does not utilize one of these tools, you still may
    be able to perform the historical comparisons. To do so, we will use a tool that
    is installed by default on most Red Hat Enterprise Linux systems; this tool is
    called `sar`.
  prefs: []
  type: TYPE_NORMAL
- en: sar – System activity report
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 2](part0019_split_000.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 2. Troubleshooting Commands and Sources of Useful Information"), *Troubleshooting
    Commands and Sources of Useful Information* we briefly discussed the use of the
    `sar` command to review historical performance statistics.
  prefs: []
  type: TYPE_NORMAL
- en: When the `sysstat` package that deploys the `sar` utility is installed, it will
    deploy the `/etc/cron.d/sysstat` file. Within this file are two `cron` jobs that
    run `sysstat` commands with the sole purpose of collecting system performance
    statistics and generating reports of the collected information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: When these commands are executed, the information collected is then stored in
    the `/var/log/sa/` folder.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The data files that the `sysstat` package generates use a filename that follows
    the "`sa<two digit day>`" format. For example, in the above output, we can see
    that the "`sa24`" file was generated on January 24th. We can also see that this
    system has files from January 23rd to February 9th.
  prefs: []
  type: TYPE_NORMAL
- en: The `sar` command is a command that allows us to read these captured performance
    metrics. This section will show you how to use the `sar` command to review the
    same statistics that we reviewed earlier with commands such as `iostat`, `top`,
    and `vmstat`. This time, however, the `sar` command will provide both recent and
    historical information.
  prefs: []
  type: TYPE_NORMAL
- en: CPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To look at CPU statistics with the `sar` command, we can simply use the `–u`
    (CPU Utilization) flag.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: If we look at the header information from above, we can see that the `sar` command
    with the `-u` flag matches the `iostat` and top CPU details.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'From the `sar -u` output, we can identify an interesting trend: from 00:00
    to 05:30, there was a constant CPU I/O wait time of 30%–40%. However, as of 05:40,
    the I/O wait decreased, but the user-level CPU utilization increased to 65%–70%
    utilization.'
  prefs: []
  type: TYPE_NORMAL
- en: While these two measurements don't specifically point to any one process, they
    do show that the I/O wait time has decreased recently while the user CPU time
    has increased.
  prefs: []
  type: TYPE_NORMAL
- en: To get a better picture of historical statistics, we will need to look at the
    previous day's CPU utilization. Luckily, we can do just that with the `–f` (filename)
    flag. The `–f` flag will allow us to specify a historical file for the `sar` command.
    This will allow us to selectively view statistics from the previous day.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: In the report from February 7th, we can see a drastic difference in CPU utilization
    than what was identified during our previous troubleshooting. One item that stands
    out is that in the report from the 7th, no CPU time was spent in the I/O wait
    state.
  prefs: []
  type: TYPE_NORMAL
- en: However, we do see that the user CPU time fluctuated from 20% to 65% depending
    on the time of day. This may indicate that a higher user CPU time utilization
    is expected.
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To display memory statistics, we can execute the `sar` command with the `–r`
    (memory) flag.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Again, if we look at the header from the memory report of `sar`, we can see
    some familiar values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: From this report, we can see from the **kbmemused** column that as of 05:40,
    the system suddenly freed up 150 MB of physical memory. It appears from the `kbcached`
    column that this 150 MB of memory was allocated to the disk cache. This is based
    on the fact that at 05:40, the cached memory went from 196 MB to 22 MB.
  prefs: []
  type: TYPE_NORMAL
- en: What is interesting is that this aligns with the CPU utilization change that
    also occurred at 05:40\. If we wished to review historical memory utilization,
    we could also use the `-f` (filename) flag with the `-r` (memory) flag. However,
    since we can see a rather obvious trend at 05:40, we will focus on this time for
    now.
  prefs: []
  type: TYPE_NORMAL
- en: Disk
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To show disk statistics for today, we can use the `–d` (block device) flag.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: By default, the `sar` command will print the device name as "`dev<major>-<minor>`,"
    which can be a bit confusing. If the `-p` (persistent names) flag is added, the
    device names will use persistent names, which match the devices from the mount
    command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Even with the names in an unrecognizable format, we can see that `dev253-1`
    seems to have had quite a bit of activity up to 05:40, where the disk `tps` (transactions
    per seconds) decreases from 1170 to 0.11\. This large drop in disk I/O utilization
    seems to indicate that a rather large change occurred at `05:40` today.
  prefs: []
  type: TYPE_NORMAL
- en: Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To show network statistics, we will need to execute the `sar` command with the
    `–n DEV` flag.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: In the network statistics report, we see no change throughout the day. This
    suggests that, overall, there has never been any network performance bottlenecks
    associated with this server.
  prefs: []
  type: TYPE_NORMAL
- en: Review what we learned by comparing historical statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After looking through historical statistics with `sar` and recent statistics
    using commands such as `ps`, `iostat`, `vmstat`, and `top`, we can come to the
    following conclusions regarding our "slow performance."
  prefs: []
  type: TYPE_NORMAL
- en: Since we were asked by one of our peers to investigate the issue, our conclusions
    will be formatted in the form of an e-mail reply to this peer.
  prefs: []
  type: TYPE_NORMAL
- en: '*Hi Bob!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*I looked into that one server where the user said the server was "slow." It
    seems that the user called vagrant has been running multiple instances of two
    main programs. The first being the lookbusy application, which seems to use roughly
    20%–40% CPU at all times. However, in at least one instance, the lookbusy application
    also used a great deal of memory, exhausting the system of physical memory and
    forcing the system to swap heavily. However, this process did not last very long.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The second program was the bonnie++ application, which seems to utilize a
    lot of disk I/O resources. While the vagrant user was running the bonnie++ application,
    it utilized approximately 60% of the dm-1 and sda disk bandwidths, causing a high
    I/O wait of around 30%. Typically, this system has an I/O wait of 0% (confirmed
    via sar).*'
  prefs: []
  type: TYPE_NORMAL
- en: '*It seems that the vagrant user may be running applications that are using
    resources beyond the expected levels, causing performance degradation for the
    other users.*'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started to use some advanced Linux commands that we explored
    in [Chapter 2](part0019_split_000.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 2. Troubleshooting Commands and Sources of Useful Information"), *Troubleshooting
    Commands and Sources of Useful Information* such as `iostat` and `vmstat`. We
    also became very familiar with a fundamental utility within Linux, the `ps` command,
    while troubleshooting a vague performance issue.
  prefs: []
  type: TYPE_NORMAL
- en: While in [Chapter 3](part0022_split_000.html#KVCC1-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 3. Troubleshooting a Web Application"), *Troubleshooting a Web Application*
    we were able to follow the full troubleshooting process from Data Collection to
    Trial and Error, in this chapter, our actions were primarily focused on the Data
    Collection and Establishing a Hypothesis stages. It is quite common to find yourself
    only troubleshooting an issue and not performing corrective actions. There are
    many issues that should be resolved by a user of the system and not the systems
    administrator, but it is still the administrator's role to identify the source
    of the issue.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](part0032_split_000.html#UGI01-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 5. Network Troubleshooting"), *Network Troubleshooting* we will be troubleshooting
    some very interesting network issues. Networking is critical to any system; issues
    can sometimes be simple, and at other times, they are very complex. In the next
    chapter, we will explore networking and how to troubleshoot network issues by
    using tools such as `netstat` and `tcpdump`.
  prefs: []
  type: TYPE_NORMAL
