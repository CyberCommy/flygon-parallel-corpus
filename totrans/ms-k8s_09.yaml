- en: Rolling Updates, Scalability, and Quotas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore the automated pod scalability that Kubernetes
    provides, how it affects rolling updates, and how it interacts with quotas. We
    will touch on the important topic of provisioning and how to choose and manage
    the size of the cluster. Finally, we will go over how the Kubernetes team tests
    the limits of Kubernetes with a 5,000-node cluster. Here are the main points we
    will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal pod autoscaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing rolling updates with autoscaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling scarce resources with quotas and limits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pushing the envelope with Kubernetes performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have the ability to plan a large-scale
    cluster, provision it economically, and make informed decisions about the various
    trade-offs between performance, cost, and availability. You will also understand
    how to set up horizontal pod auto-scaling and use resource quotas intelligently
    to let Kubernetes automatically handle intermittent fluctuations in volume.
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal pod autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes can watch over your pods and scale them when the CPU utilization
    or some other metric crosses a threshold. The autoscaling resource specifies the
    details (percentage of CPU, how often to check) and the corresponding autoscale
    controller adjusts the number of replicas, if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the different players and their relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/d2b4b9da-15eb-42e9-b9da-71eb36db89b1.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the horizontal pod autoscaler doesn't create or destroy pods
    directly. It relies instead on the replication controller or deployment resources.
    This is very smart because you don't need to deal with situations where autoscaling
    conflicts with the replication controller or deployments trying to scale the number
    of pods, unaware of the autoscaler's efforts.
  prefs: []
  type: TYPE_NORMAL
- en: The autoscaler automatically does what we had to do ourselves before. Without
    the autoscaler, if we had a replication controller with replicas set to `3`, but
    we determined that based on average CPU utilization we actually needed `4`, then
    we would update the replication controller from `3` to `4` and keep monitoring
    the CPU utilization manually in all pods. The autoscaler will do it for us.
  prefs: []
  type: TYPE_NORMAL
- en: Declaring horizontal pod autoscaler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To declare a horizontal pod autoscaler, we need a replication controller, or
    a deployment, and an autoscaling resource. Here is a simple replication controller
    configured to maintain three `nginx` pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `autoscaling` resource references the NGINX replication controller in `scaleTargetRef`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`minReplicas` and `maxReplicas` specify the range of scaling. This is needed
    to avoid runaway situations that could occur because of some problem. Imagine
    that, due to some bug, every pod immediately uses 100% CPU regardless of the actual
    load. Without the `maxReplicas` limit, Kubernetes will keep creating more and
    more pods until all cluster resources are exhausted. If we are running in a cloud
    environment with autoscaling of VMs then we will incur a significant cost. The
    other side of this problem is that, if there are no `minReplicas` and there is
    a lull in activity, then all pods could be terminated, and, when new requests
    come in, all the pods will have to be created and scheduled again. If there are
    patterns of on and off activity, then this cycle can repeat multiple times. Keeping
    the minimum of replicas running can smooth this phenomenon. In the preceding example,
    `minReplicas` is set to `2` and `maxReplicas` is set to `4`. Kubernetes will ensure
    that there are always between `2` to `4` NGINX instances running.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **target CPU** utilization percentage is a mouthful. Let''s abbreviate
    it to **TCUP**. You specify a single number like 80%. This could lead to constant
    thrashing if the average load hovers around the TCUP. Kuberentes will alternate
    frequently between adding more replicas and removing replicas. This is often not
    a desired behavior. To address this concern, you can specify a delay for either
    scaling up or scaling down. There are two flags to the `kube-controller-manager`
    to support this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--horizontal-pod-autoscaler-downscale-delay`: The value for this option is
    a duration that specifies how long the autoscaler has to wait before another downscale
    operation can be performed after the current one has completed. The default value
    is 5 minutes (5m0s).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--horizontal-pod-autoscaler-upscale-delay`: The value for this option is a
    duration that specifies how long the autoscaler has to wait before another upscale
    operation can be performed after the current one has completed. The default value
    is 3 minutes (3m0s).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CPU utilization is an important metric to gauge whether pods that are bombarded
    with too many requests should be scaled up, or whether they are mostly idle and
    can be scaled down. But the CPU is not the only and sometimes not even the best
    metric to keep track of. Memory may be the limiting factor, and there are more
    specialized metrics, such as the depth of a pod's internal on-disk queue, the
    average latency on a request, or the average number of service timeouts.
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal pod custom metrics were added as an alpha extension in version
    1.2\. In version 1.6 they were upgraded to beta status. You can now autoscale
    your pods based on multiple custom metrics. The autoscaler will evaluate all the
    metrics and will autoscale based on the largest number of replicas required, so
    the requirements of all the metrics are respected.
  prefs: []
  type: TYPE_NORMAL
- en: Using custom metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the horizontal pod autoscaler with custom metrics requires some configuration
    when launching your cluster. First, you need to enable the API aggregation layer.
    Then you need to register your resource metrics API and your custom metrics API.
    Heapster provides an implementation of the resource metrics API you can use. Just
    start Heapster with the `--api-server` flag set to `true`. You need to run a separate
    server that exposes the custom metrics API. A good starting point is this: [https://github.com/kubernetes-incubator/custom-metrics-apiserver](https://github.com/kubernetes-incubator/custom-metrics-apiserver).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to start the `kube-controller-manager` with the following
    flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `--master` flag will override `--kubeconfig` if both are specified. These
    flags specify the location of the API aggregation layer, allowing the controller
    manager to communicate to the API server.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Kubernetes 1.7, the standard aggregation layer that Kubernetes provides
    runs in-process with the `kube-apiserver`, so the target IP address can be found
    with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Autoscaling with kubectl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`kubectl` can create an autoscale resource using the standard `create` command
    and accepting a configuration file. But `kubectl` also has a special command,
    `autoscale`, that lets you easily set an autoscaler in one command without a special
    configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s start a replication controller that makes sure there are three
    replicas of a simple pod that just runs an infinite `bash-loop`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a replication controller:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the resulting replication controller:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that the desired and current count are both three, meaning three
    pods are running. Let''s make sure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create an autoscaler. To make it interesting, we''ll set the minimum
    number of replicas to `4` and the maximum number to `6`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the resulting horizontal pod autoscaler (you can use `hpa`). It shows
    the referenced replication controller, the target and current CPU percentage,
    and the min/max pods. The name matches the referenced replication controller:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Originally, the replication controller was set to have three replicas, but
    the autoscaler has a minimum of four pods. What''s the effect on the replication
    controller? That''s right. Now the desired number of replicas is four. If the
    average CPU utilization goes above 50%, then it may climb to five, or even six:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Just to make sure everything works, here is another look at the pods. Note
    the new pod (17 minutes old) that was created because of the autoscaling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'When we delete the horizontal pod autoscaler, the replication controller retains
    the last desired number of replicas (four in this case). Nobody remembers that
    the replication controller was created with three replicas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the replication controller wasn''t reset and still maintains
    four pods even when the autoscaler is gone:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Let's try something else. What happens if we create a new horizontal pod autoscaler
    with a range of `2` to `6` and the same CPU target of `50`%?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, the replication controller still maintains its four replicas, which is
    within the range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: However, the actual CPU utilization is zero, or close to zero. The replica count
    should have been scaled down to two replicas, but because the horizontal pod autoscaler
    didn't receive CPU metrics from Heapster it doesn't know it needs to scale down
    the number of replicas in the replication controller.
  prefs: []
  type: TYPE_NORMAL
- en: Performing rolling updates with autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rolling updates are the cornerstone of managing large clusters. Kubernetes support
    rolling updates at the replication controller level and by using deployments.
    Rolling updates using replication controllers are incompatible with the horizontal
    pod autoscaler. The reason is that, during the rolling deployment, a new replication
    controller is created and the horizontal pod autoscaler remains bound to the old
    replication controller. Unfortunately, the intuitive `kubectl rolling-update`
    command triggers a replication controller rolling update.
  prefs: []
  type: TYPE_NORMAL
- en: Since rolling updates are such an important capability, I recommend that you
    always bind horizontal pod autoscalers to a deployment object instead of a replication
    controller or a replica set. When the horizontal pod autoscaler is bound to a
    deployment, it can set the replicas in the deployment spec and let the deployment
    take care of the necessary underlying rolling update and replication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a deployment configuration file we''ve used for deploying the `hue-reminders`
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To support it with autoscaling and ensure we always have between `10` to `15`
    instances running, we can create an `autoscaler` configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `kind` of the `scaleTargetRef` field is now `Deployment` instead of `ReplicationController`.
    This is important because we may have a replication controller with the same name.
    To disambiguate and ensure that the horizontal pod autoscaler is bound to the
    correct object, the `kind` and the `name` must match.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can use the `kubectl autoscale` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Handling scarce resources with limits and quotas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the horizontal pod autoscaler creating pods on the fly, we need to think
    about managing our resources. Scheduling can easily get out of control, and inefficient
    use of resources is a real concern. There are several factors that can interact
    with each other in subtle ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Overall cluster capacity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource granularity per node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Division of workloads per namespace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DaemonSets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StatefulSets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affinity, anti-affinity, taints, and tolerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, let's understand the core issue. The Kubernetes scheduler has to take
    into account all these factors when it schedules pods. If there are conflicts
    or a lot of overlapping requirements, then Kubernetes may have a problem finding
    room to schedule new pods. For example, a very extreme yet simple scenario is
    that a daemon set runs on every node a pod that requires 50% of the available
    memory. Now, Kubernetes can't schedule any pod that needs more than 50% memory
    because the daemon set pod gets priority. Even if you provision new nodes, the
    daemon set will immediately commandeer half of the memory.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful sets are similar to daemon sets in that they require new nodes to expand.
    The trigger to adding new members to the stateful set is growth in data, but the
    impact is taking resources from the pool available for Kubernetes to schedule
    other members. In a multi-tenant situation, the noisy neighbor problem can rear
    its head in a provisioning or resource allocation context. You may plan exact
    rations meticulously in your namespace between different pods and their resource
    requirements, but you share the actual nodes with your neighbors from other namespaces
    that you may not even have visibility into.
  prefs: []
  type: TYPE_NORMAL
- en: Most of these problems can be mitigated by judiciously using namespace resource
    quotas and careful management of the cluster capacity across multiple resource
    types, such as CPU, memory, and storage.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling resource quotas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most Kubernetes distributions support resource quota out of the box. The API
    servers' `--admission-control` flag must have `ResourceQuota` as one of its arguments.
    You will also have to create a `ResourceQuota` object to enforce it. Note that
    there may be at most one `ResourceQuota` object per namespace to prevent potential
    conflicts. This is enforced by Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Resource quota types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are different types of quota we can manage and control. The categories
    are compute, storage, and objects.
  prefs: []
  type: TYPE_NORMAL
- en: Compute resource quota
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Compute resources are CPU and memory. For each one, you can specify a limit
    or request a certain amount. Here is the list of compute related fields. Note
    that `requests.cpu` can be specified as just `cpu`, and `requests.memory` can
    be specified as just memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`limits.cpu`: Across all pods in a non-terminal state, the sum of CPU limits
    cannot exceed this value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limits.memory`: Across all pods in a non-terminal state, the sum of memory
    limits cannot exceed this value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests.cpu`: Across all pods in a non-terminal state, the sum of CPU requests
    cannot exceed this value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests.memory`: Across all pods in a non-terminal state, the sum of memory
    requests cannot exceed this value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage resource quota
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The storage resource quota type is a little more complicated. There are two
    entities you can restrict per namespace: the amount of storage and the number
    of persistent volume claims. However, in addition to just globally setting the
    quota on total storage or total number of persistent volume claims, you can also
    do that per `storage` class. The notation for `storage` class resource quota is
    a little verbose, but it gets the job done:'
  prefs: []
  type: TYPE_NORMAL
- en: '`requests.storage`: Across all persistent volume claims, the sum of storage
    requests cannot exceed this value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`persistentvolumeclaims`: The total number of persistent volume claims that
    can exist in the namespace'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<storage-class>.storageclass.storage.k8s.io/requests.storage`: Across all
    persistent volume claims associated with the `storage-class-name`, the sum of
    storage requests cannot exceed this value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<storage-class>.storageclass.storage.k8s.io/persistentvolumeclaims`: Across
    all persistent volume claims associated with the `storage-class-name`, this is
    the total number of persistent volume claims that can exist in the namespace'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes 1.8 added alpha support for ephemeral storage quotas too:'
  prefs: []
  type: TYPE_NORMAL
- en: '`requests.ephemeral-storage`: Across all pods in the namespace, the sum of
    local ephemeral storage requests cannot exceed this value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limits.ephemeral-storage`: Across all pods in the namespace, the sum of local
    ephemeral storage limits cannot exceed this value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object count quota
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes has another category of resource quotas, which is API objects. My
    guess is that the goal is to protect the Kubernetes API server from having to
    manage too many objects. Remember that Kubernetes does a lot of work under the
    hood. It often has to query multiple objects to authenticate, authorize, and ensure
    that an operation doesn't violate any of the many policies that may be in place.
    A simple example is pod scheduling based on replication controllers. Imagine that
    you have 1 billion replication controller objects. Maybe you just have three pods
    and most of the replication controllers have zero replicas. Still, Kubernetes
    will spend all its time just verifying that indeed all those billion replication
    controllers have no replicas of their pod template and that they don't need to
    kill any pods. This is an extreme example, but the concept applies. Too many API
    objects means a lot of work for Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: The overage of objects that can be restricted is a little spotty. For example,
    you can limit the number of replication controllers, but not replica sets, which
    are almost an improved version of replication controller that can do exactly the
    same damage if too many of them are around.
  prefs: []
  type: TYPE_NORMAL
- en: The most glaring omission is namespaces. There is no limit to the number of
    namespaces. Since all limits are per namespace, you can easily overwhelm Kubernetes
    by creating too many namespaces, as each namespace has only a small number of
    API objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are all the supported objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ConfigMaps`: The total number of configuration maps that can exist in the
    namespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PersistentVolumeClaims`: The total number of persistent volume claims that
    can exist in the namespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pods`: The total number of pods in a non-terminal state that can exist in
    the namespace. A pod is in a terminal state if `status.phase` in (`Failed`, `Succeeded`)
    is `true`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReplicationControllers`: The total number of replication controllers that
    can exist in the namespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ResourceQuotas`: The total number of resource quotas that can exist in the
    namespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Services`: The total number of services that can exist in the namespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Services.LoadBalancers`: The total number of load balancer services that can
    exist in the namespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Services.NodePorts`: The total number of node port services that can exist
    in the namespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Secrets`: The total number of secrets that can exist in the namespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quota scopes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some resources, such as pods, may be in different states, and it is useful
    to have different quotas for these different states. For example, if there are
    many pods that are terminating (this happens a lot during rolling updates) then
    it is OK to create more pods even if the total number exceeds the quota. This
    can be achieved by only applying a `pod` object `count quota` to `non-terminating`
    pods. Here are the existing scopes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Terminating`: Match pods where `spec.activeDeadlineSeconds >= 0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NotTerminating`: Match pods where `spec.activeDeadlineSeconds` is nil'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BestEffort`: Match pods that have best effort quality of service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NotBestEffort`: Match pods that do not have best effort quality of service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While the `BestEffort` scope applies only to pods, the `Terminating`, `NotTerminating`,
    and `NotBestEffort` scopes apply to CPU and memory, too. This is interesting because
    a resource quota limit can prevent a pod from terminating. Here are the supported
    objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cpu`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limits.cpu`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limits.memory`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pods`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests.cpu`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests.memory`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requests and limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The meaning of requests and limits in the context of resource quotas is that
    it requires the containers to explicitly specify the target attribute. This way,
    Kubernetes can manage the total quota because it knows exactly what range of resources
    is allocated to each container.
  prefs: []
  type: TYPE_NORMAL
- en: Working with quotas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create a `namespace` first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Using namespace-specific context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When working with namespaces other than default, I prefer to use a `context`,
    so I don''t have to keep typing `--namespace=ns` for every command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Creating quotas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a `compute quota` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s add a `count quota` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can observe all the quotas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can even get all the information using `describe`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This view gives us an instant understanding of the global resource usage of
    important resources across the cluster without diving into too many separate objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add an NGINX server to our namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Uh-oh. No resources found. But there was no error when the `deployment` was
    created. Let''s check out the `deployment` resource:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'There it is, in the `conditions` section. The `ReplicaFailure` status is `True`
    and the reason is `FailedCreate`. You can see that the deployment created a new
    replica set called `nginx-8586cf59`, but it couldn''t create the pod it was supposed
    to create. We still don''t know why. Let''s check out the replica set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The output is very wide, so it overlaps several lines, but the message is crystal
    clear. Since there is a compute quota in the namespace, every container must specify
    its CPU, memory requests, and limit. The quota controller must account for every
    container compute resources usage to ensure the total namespace quota is respected.
  prefs: []
  type: TYPE_NORMAL
- en: OK. We understand the problem, but how to resolve it? One way is to create a
    dedicated `deployment` object for each pod type we want to use and carefully set
    the CPU and memory requests and limit. But what if we're not sure? What if there
    are many pod types and we don't want to manage a bunch of `deployment` configuration
    files?
  prefs: []
  type: TYPE_NORMAL
- en: 'Another solution is to specify the limit on the command line when we run the
    `deployment`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'That works, but creating deployments on the fly with lots of arguments is a
    very fragile way to manage your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Using limit ranges for default compute quotas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A better way is to specify default compute limits. Enter limit ranges. Here
    is a configuration file that sets some defaults for containers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the current default `limits`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s run NGINX again without specifying any CPU or memory requests and
    limits. But first, let''s delete the current NGINX deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see if the pod was created. Yes, it was:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Choosing and managing the cluster capacity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With Kubernetes' horizontal pod autoscaling, daemon sets, stateful sets, and
    quotas, we can scale and control our pods, storage, and other objects. However,
    in the end, we're limited by the physical (virtual) resources available to our
    Kubernetes cluster. If all your nodes are running at 100% capacity, you need to
    add more nodes to your cluster. There is no way around it. Kubernetes will just
    fail to scale. On the other hand, if you have very dynamic workloads then Kubernetes
    can scale down your pods, but if you don't scale down your nodes correspondingly,
    you will still pay for the excess capacity. In the cloud, you can stop and start
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing your node types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest solution is to choose a single node type with a known quantity
    of CPU, memory, and local storage. But that is typically not the most efficient
    and cost-effective solution. It makes capacity planning simple because the only
    question is how many nodes are needed. Whenever you add a node, you add a known
    quantity of CPU and memory to your cluster, but most Kubernetes clusters and components
    within the cluster handle different workloads. We may have a stream processing
    pipeline where many pods receive some data and process it in one place. This workload
    is CPU-heavy and may or may not need a lot of memory. Other components, such as
    a distributed memory cache, need a lot of memory, but very little CPU. Other components,
    such as a Cassandra cluster, need multiple SSD disks attached to each node.
  prefs: []
  type: TYPE_NORMAL
- en: For each type of node, you should consider proper labeling and making sure that
    Kubernetes schedules the pods that are designed to run on that node type.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing your storage solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Storage is a huge factor in scaling a cluster. There are three categories of
    scalable storage solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Roll your own
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use your cloud platform storage solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use an out-of-cluster solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you use roll your own, you install some type of storage solution in your
    Kubernetes cluster. The benefits are flexibility and full control, but you have
    to manage and scale it yourself.
  prefs: []
  type: TYPE_NORMAL
- en: When you use your cloud platform storage solution, you get a lot out of the
    box, but you lose control, you typically pay more, and, depending on the service,
    you may be locked in to that provider.
  prefs: []
  type: TYPE_NORMAL
- en: When you use an out-of-cluster solution, the performance and cost of data transfer
    may be much greater. You typically use this option if you need to integrate with
    an existing system.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, large clusters may have multiple data stores from all categories.
    This is one of the most critical decisions you have to make, and your storage
    needs may change and evolve over time.
  prefs: []
  type: TYPE_NORMAL
- en: Trading off cost and response time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If money is not an issue you can just over-provision your cluster. Every node
    will have the best hardware configuration available, you'll have way more nodes
    than are needed to process your workloads, and you'll have copious amounts of
    available storage. Guess what? Money is always an issue!
  prefs: []
  type: TYPE_NORMAL
- en: You may get by with over-provisioning when you're just starting and your cluster
    doesn't handle a lot of traffic. You may just run five nodes, even if two nodes
    are enough most of the time. Multiply everything by 1,000 and someone will come
    asking questions if you have thousands of idle machines and petabytes of empty
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: OK. So, you measure and optimize carefully and you get 99.99999% utilization
    of every resource. Congratulations, you just created a system that can't handle
    an iota of extra load or the failure of a single node without dropping requests
    on the floor or delaying responses.
  prefs: []
  type: TYPE_NORMAL
- en: You need to find the middle ground. Understand the typical fluctuations of your
    workloads and consider the cost/benefit ratio of having excess capacity versus
    having reduced response time or processing ability.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, if you have strict availability and reliability requirements, you
    can build redundancy into the system and then you over-provision by design. For
    example, you want to be able to hot swap a failed component with no downtime and
    no noticeable effects. Maybe you can't lose even a single transaction. In this
    case, you'll have a live backup for all critical components, and that extra capacity
    can be used to mitigate temporary fluctuations without any special actions.
  prefs: []
  type: TYPE_NORMAL
- en: Using effectively multiple node configurations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Effective capacity planning requires you to understand the usage patterns of
    your system and the load each component can handle. That may include a lot of
    data streams generated inside the system. When you have a solid understanding
    of the typical workloads, you can look at workflows and which components handle
    which parts of the load. Then you can compute the number of pods and their resource
    requirements. In my experience, there are some relatively fixed workloads, some
    workloads that vary predictably (such as office hours versus non-office hours),
    and then you have your completely crazy workloads that behave erratically. You
    have to plan according to each workload, and you can design several families of
    node configurations that can be used to schedule pods that match a particular
    workload.
  prefs: []
  type: TYPE_NORMAL
- en: Benefiting from elastic cloud resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most cloud providers let you scale instances automatically, which is a perfect
    complement to Kubernetes' horizontal pod autoscaling. If you use cloud storage,
    it also grows magically without you having to do anything. However, there are
    some gotchas that you need to be aware of.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling instances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the big cloud providers have instance autoscaling in place. There are some
    differences, but scaling up and down based on CPU utilization is always available,
    and sometimes custom metrics are available too. Sometimes, load balancing is offered
    as well. As you can see, there is some overlap with Kubernetes here. If your cloud
    provider doesn't have adequate autoscaling with proper control, it is relatively
    easy to roll your own, so that you monitor your cluster resource usage and invoke
    cloud APIs to add or remove instances. You can extract the metrics from Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a diagram that shows how two new instances are added based on a CPU
    load monitor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/997c6242-6b95-4225-b1b3-992e7f12863d.png)'
  prefs: []
  type: TYPE_IMG
- en: Mind your cloud quotas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with cloud providers, some of the most annoying things are quotas.
    I've worked with four different cloud providers (AWS, GCP, Azure, and Alibaba
    Cloud) and I was always bitten by quotas at some point. The quotas exist to let
    the cloud providers do their own capacity planning (and also to protect you from
    inadvertently starting 1 million instances that you won't be able to pay for),
    but from your point of view it is yet one more thing that can trip you up. Imagine
    that you set up a beautiful autoscaling system that works like magic, and suddenly
    the system doesn't scale when you hit 100 nodes. You quickly discover that you
    are limited to 100 nodes and you open a support request to increase the quota.
    However, a human must approve quota requests, and that can take a day or two.
    In the meantime, your system is unable to handle the load.
  prefs: []
  type: TYPE_NORMAL
- en: Manage regions carefully
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud platforms are organized in regions and availability zones. Some services
    and machine configurations are available only in some regions. Cloud quotas are
    also managed at the regional level. The performance and cost of data transfers
    within regions is much lower (often free) than across regions. When planning your
    cluster, you should consider your geo-distribution strategy carefully. If you
    need to run your cluster across multiple regions, you may have some tough decisions
    to make regarding redundancy, availability, performance, and cost.
  prefs: []
  type: TYPE_NORMAL
- en: Considering Hyper.sh (and AWS Fargate)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Hyper.sh` is a container-aware hosting service. You just start containers.
    The service takes care of allocating the hardware. Containers start within seconds.
    You never need to wait minutes for a new VM. Hypernetes is Kubernetes on Hyper.sh,
    and it completely eliminates the need to scale the nodes because there are no
    nodes as far as you''re concerned. There are only containers (or pods).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see on the right how **Hyper Containers**
    run directly on a multi-tenant bare-metal container cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/061b3cae-185a-4856-8612-506f76e3365c.png)'
  prefs: []
  type: TYPE_IMG
- en: AWS recently released Fargate, which similarly abstracts away the underlying
    instances and just let you schedule containers in the cloud. In combination with
    EKS, it may become the most popular way to deploy Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Pushing the envelope with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will see how the Kubernetes team pushes Kubernetes to its
    limit. The numbers are quite telling, but some of the tools and techniques, such
    as Kubemark, are ingenious, and you may even use them to test your clusters. In
    the wild, there are some Kubernetes clusters with 3,000 nodes. At CERN, the OpenStack
    team achieved 2 million requests per second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://superuser.openstack.org/articles/scaling-magnum-and-kubernetes-2-million-requests-per-second/](http://superuser.openstack.org/articles/scaling-magnum-and-kubernetes-2-million-requests-per-second/).'
  prefs: []
  type: TYPE_NORMAL
- en: Mirantis conducted a performance and scaling test in their scaling lab where
    they deployed 5,000 Kubernetes nodes (in VMs) on 500 physical servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more detail on Mirantis, please refer to: [http://bit.ly/2oijqQY](http://bit.ly/2oijqQY).'
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI scaled their machine learning Kubernetes cluster to 2,500 nodes and
    learned some valuable lessons, such as minding the query load of logging agents
    and storing events in a separate `etcd` cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://blog.openai.com/scaling-kubernetes-to-2500-nodes/](https://blog.openai.com/scaling-kubernetes-to-2500-nodes/)'
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this section, you'll appreciate the effort and creativity that
    goes into improving Kubernetes on a large scale, you will know how far you can
    push a single Kubernetes cluster and what performance to expect, and you'll get
    an inside look at some of the tools and techniques that can help you evaluate
    the performance of your own Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the performance and scalability of Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubernetes team focused heavily on performance and scalability in Kubernetes
    1.6\. When Kubernetes 1.2 was released, it supported clusters of up to 1,000 nodes
    within the Kubernetes service-level objectives. Kubernetes 1.3 doubled the number
    to 2,000 nodes, and Kubernetes 1.6 brought it to a staggering 5,000 nodes per
    cluster. We will get into the numbers later, but first let's look under the hood
    and see how Kubernetes achieved these impressive improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Caching reads in the API server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes keeps the state of the system in etcd, which is very reliable, though
    not superfast (although etcd3 delivered a massive improvement specifically in
    order to enable larger Kubernetes clusters). The various Kubernetes components
    operate on snapshots of that state and don't rely on real-time updates. That fact
    allows the trading of some latency for throughput. All the snapshots used to be
    updated by etcd watches. Now, the API server has an in-memory read cache that
    is used for updating state snapshots. The in-memory read cache is updated by etcd
    watches. These schemes significantly reduce the load on etcd and increase the
    overall throughput of the API server.
  prefs: []
  type: TYPE_NORMAL
- en: The pod life cycle event generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Increasing the number of nodes in a cluster is key for horizontal scalability,
    but pod density is crucial too. Pod density is the number of pods that the Kubelet
    can manage efficiently on one node. If pod density is low, then you can't run
    too many pods on one node. That means that you might not benefit from more powerful
    nodes (more CPU and memory per node) because the Kubelet will not be able to manage
    more pods. The other alternative is to force the developers to compromise their
    design and create coarse-grained pods that do more work per pod. Ideally, Kubernetes
    should not force your hand when it comes to pod granularity. The Kubernetes team
    understands this very well and has invested a lot of work in improving pod density.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes 1.1, the official (tested and advertised) number was 30 pods per
    node. I actually ran 40 pods per node on Kubernetes 1.1, but I paid for it with
    an excessive kubelet overhead that stole CPU from the worker pods. In Kubernetes
    1.2, the number jumped to 100 pods per node.
  prefs: []
  type: TYPE_NORMAL
- en: The kubelet used to poll the container runtime constantly for each pod in its
    own go routine. That put a lot of pressure on the container runtime so that, during
    performance peaks, there were reliability issues, particularly with CPU utilization.
    The solution was the **Pod Lifecycle Event Generator** (**PLEG**). The way the
    PLEG works is that it lists the state of all the pods and containers and compares
    it to the previous state. This is done once for all the pods and containers. Then,
    by comparing the state to the previous state, the PLEG knows which pods need to
    sync again and invokes only those pods. That change resulted in a significant
    four times lower CPU usage by the Kubelet and the container runtime. It also reduced
    the polling period, which improves responsiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the **CPU utilization for 120 pods** on Kubernetes
    1.1 versus Kubernetes 1.2\. You can see the 4x factor very clearly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/233f183e-7dd1-4856-9050-2862a9e5e591.png)'
  prefs: []
  type: TYPE_IMG
- en: Serializing API objects with protocol buffers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The API server has a REST API. REST APIs typically use JSON as their serialization
    format, and the Kubernetes API server was no different. However, JSON serialization
    implies marshaling and unmarshaling JSON to native data structures. This is an
    expensive operation. In a large-scale Kubernetes cluster, a lot of components
    need to query or update the API server frequently. The cost of all that JSON parsing
    and composition adds up quickly. In Kubernetes 1.3, the Kubernetes team added
    an efficient protocol buffers serialization format. The JSON format is still there,
    but all internal communication between Kubernetes components uses the protocol
    buffers serialization format.
  prefs: []
  type: TYPE_NORMAL
- en: etcd3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes switched from etcd2 to etcd3 in Kubernetes 1.6\. This was a big
    deal. Scaling Kubernetes to 5,000 nodes wasn''t possible due to limitations of
    etcd2, especially related to the watch implementation. The scalability needs of
    Kubernetes drove many of the improvements of etcd3, as CoreOS used Kubernetes
    as a measuring stick. Some of the big ticket items are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: GRPC instead of REST-etcd2 has a REST API, etcd3 has a gRPC API (and a REST
    API via gRPC gateway). The http/2 protocol at the base of gRPC can use a single
    TCP connection for multiple streams of requests and responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leases instead of TTLs-etcd2 uses **time to live** (**TTL**) per key as the
    mechanism to expire keys, and etcd3 uses leases with TTLs, where multiple keys
    can share the same key. This reduces significantly keep alive traffic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The watch implementation of etcd3 takes advantage of GRPC bi-directional streams
    and maintains a single TCP connection to send multiple events, which reduced the
    memory footprint by at least an order of magnitude.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With etcd3, Kubernetes started storing all the state as protobug, which eliminated
    a lot of wasteful JSON serialization overhead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other optimizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Kubernetes team made many other optimizations:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the scheduler (which resulted in 5-10x higher scheduling throughput)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Switching all controllers to a new recommended design using shared informers,
    which reduced resource consumption of controller-manager-for reference see this
    document at [https://github.com/kubernetes/community/blob/master/contributors/devel/controllers.md](https://github.com/kubernetes/community/blob/master/contributors/devel/controllers.md)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing individual operations in the API server (conversions, deep-copies,
    patch)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing memory allocation in the API server (which significantly impacts the
    latency of API calls)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the performance and scalability of Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to improve performance and scalability, you need a sound idea of what
    you want to improve and how you're going to measure the improvements. You must
    also make sure that you don't violate basic properties and guarantees in the quest
    for improved performance and scalability. What I love about performance improvements
    is that they often buy you scalability improvements for free. For example, if
    a pod needs 50% of the CPU of a node to do its job and you improve performance
    so that the pod can do the same work using 33% of the CPU, then you can suddenly
    run three pods instead of two on that node, and you've improved the scalability
    of your cluster by 50% overall (or reduced your cost by 33%).
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes SLOs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes has **Service Level Objectives** (**SLOs**). These guarantees must
    be respected when trying to improve performance and scalability. Kubernetes has
    a one-second response time for API calls. That's 1,000 milliseconds. It actually
    achieves an order of magnitude faster response time most of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring API responsiveness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The API has many different endpoints. There is no simple API responsiveness
    number. Each call has to be measured separately. In addition, due to the complexity
    and the distributed nature of the system, not to mention networking issues, there
    can be a lot of volatility to the results. A solid methodology is to break the
    API measurements into separate endpoints, then run a lot of tests over time and
    look at percentiles (which is standard practice).
  prefs: []
  type: TYPE_NORMAL
- en: It's also important to use enough hardware to manage a large number of objects.
    The Kubernetes team used a 32-core VM with 120 GB for the master in this test.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram describes the 50th, 90th, and 99th percentile of various
    important API call latencies for Kubernetes 1.3\. You can see that the 90th percentile
    is very low, below 20 milliseconds. Even the 99th percentile is less than 125
    milliseconds for the `DELETE` pods operation, and less than 100 milliseconds for
    all other operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ebd83246-25cd-4544-9354-0137881e81e3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another category of API calls is LIST operations. Those calls are more expansive
    because they need to collect a lot of information in a large cluster, compose
    the response, and send a potential large response. This is where performance improvements
    such as the in-memory read-cache and the protocol buffers serialization really
    shine. The response time is understandably greater than the single API calls,
    but it is still way below the SLO of one second (1,000 milliseconds):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/8dbffc43-0ad7-4818-9d70-18f9b77b6f24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is excellent, but check out the API call latencies with Kubernetes 1.6
    on a 5,000 nodes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/a2a907bc-9ab2-4c00-9ecf-f6d7a1135793.png)'
  prefs: []
  type: TYPE_IMG
- en: Measuring end-to-end pod startup time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important performance characteristics of a large dynamic cluster
    is end-to-end pod startup time. Kubernetes creates, destroys, and shuffles pods
    around all the time. You could say that the primary function of Kubernetes is
    to schedule pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see that pod startup time is less volatile
    than API calls. This makes sense since there is a lot of work that needs to be
    done, such as launching a new instance of a runtime that doesn''t depend on cluster
    size. With Kubernetes 1.2 on a 1,000-node cluster, the 99th percentile end-to-end
    time to launch a pod was less than 3 seconds. With Kubernetes 1.3, the 99th percentile
    end-to-end time to launch a pod was a little over 2.5 seconds. It''s remarkable
    that the time is very close, but a little better with Kubernetes 1.3 on a 2,000-node
    cluster versus a 1,000-node cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/13d475e6-8cf8-4e11-ac2f-e81dd4a16300.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Kubernetes 1.6 takes it to the next level and does even better on a larger
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ef3e1fed-94c5-41e5-8e4a-fbe72aa0a21f.png)'
  prefs: []
  type: TYPE_IMG
- en: Testing Kubernetes at scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clusters with thousands of nodes are expensive. Even a project such as Kubernetes
    that enjoys the support of Google and other industry giants still needs to come
    up with reasonable ways to test without breaking the bank.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes team runs a full-fledged test on a real cluster at least once
    per release to collect real-world performance and scalability data. However, there
    is also a need for a lightweight and cheaper way to experiment with potential
    improvements and to detect regressions. Enter the Kubemark.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Kubemark tool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubemark is a Kubernetes cluster that runs mock nodes called hollow nodes
    used for running lightweight benchmarks against large-scale (hollow) clusters.
    Some of the Kubernetes components that are available on a real node such as the
    kubelet is replaced with a hollow kubelet. The hollow kubelet fakes a lot of the
    functionality of a real kubelet. A hollow kubelet doesn't actually start any containers,
    and it doesn't mount any volumes. But from the Kubernetes cluster point of view
    -the state stored in etcd- all those objects exist and you can query the API server.
    The hollow kubelet is actually the real kubelet with an injected mock Docker client
    that doesn't do anything.
  prefs: []
  type: TYPE_NORMAL
- en: Another important hollow component is the `hollow-proxy`, which mocks the Kubeproxy
    component. It again uses the real Kubeproxy code with a mock proxier interface
    that does nothing and avoids touching iptables.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Kubemark cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Kubemark cluster uses the power of Kubernetes. To set up a Kubemark cluster,
    perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a regular Kubernetes cluster where we can run `N hollow-nodes`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a dedicated VM to start all master components for the Kubemark cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Schedule `N hollow-node` pods on the base Kubernetes cluster. Those hollow-nodes
    are configured to talk to the Kubemark API server running on the dedicated VM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create add-on pods by scheduling them on the base cluster and configuring them
    to talk to the Kubemark API server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A full-fledged guide on GCP is available at [http://bit.ly/2nPMkwc](http://bit.ly/2nPMkwc).
  prefs: []
  type: TYPE_NORMAL
- en: Comparing a Kubemark cluster to a real-world cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The performance of Kubemark clusters is pretty similar to the performance of
    real clusters. For the pod startup end-to-end latency, the difference is negligible.
    For the API-responsiveness, the differences are higher, though generally less
    than a factor of two. However, trends are exactly the same: an improvement/regression
    in a real cluster is visible as a similar percentage drop/increase in metrics
    in Kubemark.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've covered many topics relating to scaling Kubernetes clusters.
    We discussed how the horizontal pod autoscaler can automatically manage the number
    of running pods-based CPU utilization or other metrics, how to perform rolling
    updates correctly and safely in the context of auto-scaling, and how to handle
    scarce resources via resource quotas. Then we moved on to overall capacity planning
    and management of the cluster's physical or virtual resources. Finally, we delved
    into a real-world example of scaling a single Kubernetes cluster to handle 5,000
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you have a good understanding of all the factors that come into
    play when a Kubernetes cluster is facing dynamic and growing workloads. You have
    multiple tools to choose from for planning and designing your own scaling strategy.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive into advanced Kubernetes networking. Kubernetes
    has a networking model based on the **Common Networking Interface** (**CNI**)
    and supports multiple providers.
  prefs: []
  type: TYPE_NORMAL
