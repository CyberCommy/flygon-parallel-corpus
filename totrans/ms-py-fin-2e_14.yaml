- en: Deep Learning for Finance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning represents the very cutting edge of **Artificial Intelligence**
    (**AI**). Unlike machine learning, deep learning takes a different approach in
    making predictions by using a neural network. An artificial neural network is
    modeled on the human nervous system, consisting of an input layer and an output
    layer, with one or more hidden layers in between. Each layer consists of artificial
    neurons working in parallel and passing outputs to the next layer as inputs. The
    word *deep* in deep learning comes from the notion that as data passes through
    more hidden layers in an artificial neural network, more complex features can
    be extracted.
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow** is an open source, powerful machine learning and deep learning
    framework developed by Google. In this chapter, we will take a hands-on approach
    to learning TensorFlow by building a deep learning model with four hidden layers
    to predict the prices of a security. Deep learning models are trained by passing
    the entire dataset forward and backward through the network, with each iteration
    known as an **epoch**. Because the input data can be too big to be fed, training
    can be done in batches, and this process is known as **mini-batch training**.'
  prefs: []
  type: TYPE_NORMAL
- en: Another popular deep learning library is Keras, which utilizes TensorFlow as
    the backend. We will also take a hands-on approach to learning Keras and see how
    easy it is to build a deep learning model to predict credit card payment defaults.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neurons, activation functions, loss functions, and optimizers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of neural network architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build security price prediction deep learning model using TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras, a user-friendly deep learning framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build credit card payment default prediction deep learning model using
    Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to display recorded events in a Keras history
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief introduction to deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The theory behind deep learning began as early as the 1940s. However, its popularity
    has soared in recent years thanks in part to improvements in computing hardware
    technology, smarter algorithms, and the adoption of deep learning frameworks.
    There is much to cover beyond this book. This section serves as a quick guide
    to gain a working knowledge for following the examples that we will cover in later
    parts of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: What is deep learning ?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 10](28d7845a-ecc1-46b1-99a0-0e6dea8fd2be.xhtml), *Machine Learning
    for Finance*, we learned how machine learning is useful for making predictions.
    Supervised learning uses error-minimization techniques to fit a model with training
    data, and can be regression based or classification based.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning takes a different approach in making predictions by using a neural
    network. Modeled on the human brain and the nervous system, an artificial neural
    network consists of a hierarchy of layers, with each layer made up of many simple
    units known as neurons, working in parallel and transforming the input data into
    abstract representations as the output data, which are fed to the next layer as
    input. The following diagram illustrates an artificial neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/58845918-179c-4b97-adf0-fa1c1a52d550.png)'
  prefs: []
  type: TYPE_IMG
- en: Artificial neural networks consist of three types of layers. The first layer
    that accepts input is known as the **input layer**. The last layer where output
    is collected is known as the **output layer**. The layers between the input and
    output layers are known as **hidden layers**, since they are hidden from the interface
    of the network. There can be many combinations of hidden layers performing different
    activation functions. Naturally, more complex computations lead to a rise in demand
    for more powerful machines, such as the GPUs required to compute them.
  prefs: []
  type: TYPE_NORMAL
- en: The artificial neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An artificial neuron receives one or more input and are multiplied by values
    known as **weights**, summed up and passed to an activation function. The final
    values computed by the activation function makes up the neuron''s output. A bias
    value may be included in the summation term to help fit the data. The following
    diagram illustrates an artificial neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/4cac940f-299f-438f-b10b-7931a0bd1ecf.png)'
  prefs: []
  type: TYPE_IMG
- en: The summation term can be written as a linear equation such that *Z=x[1]w[1]+x[2]w[2]+...+b.*
    The neuron uses a nonlinear activation function *f* to transform the input to
    become the output ![](Images/36c3c2e2-b1af-4e63-996e-a9bee2122a52.png), and can
    be written as ![](Images/07dfbda6-8a1f-4a27-9880-d57a3a8ffd35.png).
  prefs: []
  type: TYPE_NORMAL
- en: Activation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An activation function is part of an artificial neuron that transforms the
    sum of weighted inputs into another value for the next layer. Usually, the range
    of this output value is -1 or 0 to 1\. An artificial neuron is said to be activated
    when it passes a non-zero value to another neuron. There are several types of
    activation functions, mainly:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hard tanh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rectified linear unit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaky ReLU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softplus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, a **rectified linear unit** (**ReLU**) function is written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/82e59b35-40fa-4400-9942-6b51fed40fa5.png)'
  prefs: []
  type: TYPE_IMG
- en: The ReLU activates a node with the same input value only when the input is above
    zero. Researchers prefer to use ReLU as it trains better than sigmoid activation
    functions. We will be using ReLU in later parts of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In another example, the leaky ReLU is written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/6e095e7b-4ee0-485e-8f49-d5425cd952b1.png)'
  prefs: []
  type: TYPE_IMG
- en: The leaky ReLU addresses the issue of a dead ReLU when ![](Images/c809e2c3-a19f-4989-b587-6cdb3167d31c.png)
    by having a small negative slope around 0.01 when *x* is zero and below.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The loss function computes the error between the predicted value of a model
    and the actual value. The smaller the error value, the better the model is in
    prediction. Some loss functions used in regression-based models are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean squared error** (**MSE**) loss'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean absolute error** (**MAE**) loss'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huber loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantile loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some loss functions used in classification-based models are:'
  prefs: []
  type: TYPE_NORMAL
- en: Focal loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinge loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exponential loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Optimizers help to tweak the model weights optimally in minimizing the loss
    function. There are several types of optimizers that you may come across in deep
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AdaGrad** (**adaptive gradient**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adam** (**adaptive moment estimation**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LBFGS** (**limited-memory Broyden-Fletcher-Goldfarb-Shannon**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rprop** (**resilient backpropagation**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RMSprop** (**root mean square propagation**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SGD** (**stochastic gradient descent**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam is a popular choice of optimizer, and is seen as a combination of RMSprop
    and SGD with momentum. It is an adaptive learning rate optimization algorithm,
    computing individual learning rates for different parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Network architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The network architecture of a neural network defines its behavior. There are
    many forms of network architecture available; some are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Perceptron** (**P**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feed forward** (**FF**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep feed forward** (**DFF**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Radial basis function network** (**RBF**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent neural network** (**RNN**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long/short-term memory** (**LSTM**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoencoder** (**AE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hopfield network** (**HN**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boltzmann machine** (**BM**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative adversarial network** (**GAN**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most well-known and easy-to-understand neural network is the feed forward
    multilayer neural network. It can represent any function using an input layer,
    one or more hidden layers, and a single output layer. A list of neural networks
    can be found at [http://www.asimovinstitute.org/neural-network-zoo/](http://www.asimovinstitute.org/neural-network-zoo/).
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow and other deep learning frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is a free and open source library from Google, available in Python,
    C++, Java, Rust, and Go. It contains various neural networks for training deep
    learning models. TensorFlow can be applied to various scenarios, such as image
    classification, malware detection, and speech recognition. The official page for
    TensorFlow is [https://www.tensorflow.org](https://www.tensorflow.org).
  prefs: []
  type: TYPE_NORMAL
- en: Other popular deep learning frameworks used in the industry are Theano, PyTorch,
    CNTK (Microsoft Cognitive Toolkit), Apache MXNet, and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: What is a tensor ?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Tensor* in TensorFlow indicates that the frameworks define and run computations
    involving tensors. A tensor is nothing more than a type of *n*-dimensional vector
    with certain transformative properties. A non-dimensional tensor is a scalar or
    number. A one-dimensional tensor is a vector. A two-dimensional tensor is a matrix.
    Tensors offer more natural representations of data, for example in images in the
    field of computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: The basic properties of vector spaces and the elementary mathematical properties
    of tensors make them particularly useful in physics and engineering.
  prefs: []
  type: TYPE_NORMAL
- en: A deep learning price prediction model with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to use TensorFlow as a deep learning framework
    in building a price prediction model. Five years of pricing data, from 2013 to
    2017, will be used for training our deep learning model. We will attempt to predict
    the prices of Apple (AAPL) in the following year of 2018.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering our model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The daily adjusted closing prices of our data make up the target variables.
    The independent variables defining the features of our model are made up of these
    technical indicators:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Relative strength index** (**RSI**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Williams %R** (**WR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Awesome oscillator** (**AO**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volume-weighted average price** (**VWAP**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average daily trading volume** (**ADTV**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5-day **moving average** (**MA**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 15-day moving average
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 30-day moving average
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This gives us eight features for our model.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should have NumPy, pandas, Jupyter, and scikit-learn libraries installed,
    as mentioned in previous chapters. The following sections highlight additional
    important requirements for building our deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Intrinio as our data provider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Intrinio ([https://intrinio.com/](https://intrinio.com/)) is a premium API-based
    financial data provider. We will be using the US Fundamentals and Stock Prices
    subscription, which gives us access to US historical stock prices and well-calculated
    technical indicator values. After registering for an account, your API keys can
    be found in your account settings, which we will use later.
  prefs: []
  type: TYPE_NORMAL
- en: Compatible Python environment for TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of writing, the latest stable version of TensorFlow is r1.13\. This
    version is compatible with Python 2.7, 3.4, 3.5, and 3.6\. As the preceding chapters
    in this book use Python 3.7, we need to set up a separate Python 3.6 environment
    for running the examples in this chapter. The virtualenv tool ([https://virtualenv.pypa.io/](https://virtualenv.pypa.io/))
    is recommended to isolate Python environments.
  prefs: []
  type: TYPE_NORMAL
- en: The requests library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `requests` Python library is required to help us make HTTP calls to Intrinio
    APIs. The official web page for `requests` is [http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/).
    Install `requests` by running this command in your terminal: `pip install requests`.'
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a number of variants of TensorFlow available for installation. You
    may choose between CPU-only or GPU support versions, alpha versions, and nightly
    versions. More installation instructions are available at [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip).
    At a minimum, the following terminal command installs the latest CPU-only stable
    version of TensorFlow: `pip install tensorflow`.'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section describes the steps for downloading our required prices and technical
    indicator values from Intrinio. Comprehensive documentation on the API calls can
    be found at [https://docs.intrinio.com/documentation/api_v2](https://docs.intrinio.com/documentation/api_v2).
    If you decide to use another data provider, go ahead and skip this section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Write a `query_intrinio()` function that will make an API call to Intrinio,
    with the following codes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This function accepts the `path` and `kwargs` parameters. The `path` parameter
    refers to the specific Intrinio API context path. The `kwargs` keyword argument is
    a dictionary that gets passed along to a HTTP GET request call as request parameters.
    The API key is inserted into this dictionary on every API call to identify the
    user account. Any API responses are expected to be in JSON format with a HTTP
    status code of 200; otherwise, an exception will be thrown.
  prefs: []
  type: TYPE_NORMAL
- en: 'Write a `get_technicals()` function to download technical indicator values
    from Intrinio, with the following codes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `ticker` and `indicator` parameters make up the API context path for downloading
    the specific indicator of a security. The response is expected to be in JSON format,
    with a key named `technicals` containing the list of technical indicator values.
    The `json_normalize()` function of pandas helps to convert these values into a
    flat table DataFrame object. Extra formatting is required to set date and time
    values as the index under the `date` name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the values for the request parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We shall be querying data for the security `AAPL`, from 2013 to 2018, inclusive.
    The big `page_size` value gives us sufficient space to request six years of data
    in a single query.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands at one-minute intervals each to download the technical
    indicator data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Beware of paging limits when performing Intrinio API queries! API requests with
    a `page_size` greater than 100 are subjected to a per-minute request limit. If
    a call fails with status code 429, try again in one minute. Information on Intrinio's
    limits can be found at [https://docs.intrinio.com/documentation/api_v2/limits](https://docs.intrinio.com/documentation/api_v2/limits).
  prefs: []
  type: TYPE_NORMAL
- en: This gives us eight variables, each containing the DataFrame object of the respective
    technical indicator values. The MA data columns are renamed to avoid a naming
    conflict when joining the data later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Write a `get_prices()` function to download the historical prices of a security:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `tag` parameter specifies the data tag of the security to download. The
    JSON response is expected to contain a key named `historical_data` containing
    the list of values. The column containing the prices in the DataFrame object is
    renamed from `value` to its data tag.
  prefs: []
  type: TYPE_NORMAL
- en: The Intrinio data tags are used to download specific values from the system.
    The list of data tags available with explanations can be found at [https://data.intrinio.com/data-tags/all](https://data.intrinio.com/data-tags/all).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `get_prices()` function, download the adjusted closing prices of
    AAPL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As the features are used to predict the next day''s closing prices, we need
    to shift the prices backwards by one day to align this mapping. Create the target
    variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, combine all the DataFrame objects together with the `join()` command
    and drop the empty values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Our dataset is now ready, contained in the `df` DataFrame. We can proceed to
    split the data for training.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling and splitting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are interested in using the earliest five years of pricing data for training
    our model, and the most recent year of 2018 for testing our predictions. Run the
    following codes to split our `df` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `df_train` and `df_test` variables contain our training and testing data
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: An important step in data preprocessing is to normalize the dataset. This will
    transform input feature values to a mean of zero and a variance of one. Normalization
    helps to avoid biases during training due to the different scales of input features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `MinMaxScaler` function of the `sklearn` module helps to transform each
    feature into a range between -1 and 0, with the following codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `fit_transform()` function computes the parameters for scaling and transforms
    the data, while the `transform()` function only transforms the data by reusing
    the computed parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, split the scaled training dataset into independent and target variables.
    The target values are on the last column, with the remaining columns as the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Do the same on our testing data for features only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: With our training and testing dataset prepared, let's begin to build an artificial
    neural network with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Building an artificial neural network with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks you through the process of setting up an artificial neural
    network for deep learning with four hidden layers. There are two phases involved;
    first in assembling the graph, and next in training the model.
  prefs: []
  type: TYPE_NORMAL
- en: Phase 1 – assembling the graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following steps describe the process of setting up a TensorFlow graph:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create placeholders for inputs and labels with the following codes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: A TensorFlow operation starts with placeholders. Here, we defined two placeholders
    `x` and `y` for containing the network inputs and outputs, respectively. The `shape`
    parameter defines the shape of the tensor to be fed, with `None` meaning that
    the number of observations is unknown at this point. The second dimension of `x`
    is the number of features that we have, reflected in the `num_features` variable.
    Later, as we shall see, placeholder values are fed using the `feed_dict` command.
  prefs: []
  type: TYPE_NORMAL
- en: Create weight and bias initializers for hidden layers. Our model will consist
    of four hidden layers. The first layer contains 512 neurons, about three times
    the size of the input. The second, third, and fourth layers contain 256, 128,
    and 64 neurons, respectively. The reduction of the number of neurons in subsequent
    layers compresses the information in the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initializers are used to initialize the network variables before training.
    It is important to use proper initialization at the start of the optimization
    problem to produce good solutions to the underlying problem. The use of a variance
    scaling initializer and a zeros initializer is demonstrated with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Besides placeholders, variables within TensorFlow are updated during graph execution.
    Here, the variables are the weights and bias that will change during training.
    The  `variance_scaling_initializer()` command returns an initializer that generates
    tensors for our weights without scaling variance. The `FAN_AVG` mode indicates
    to the initializer to use the average number of input and output connections,
    with the `uniform` parameter as `True` to use uniform random initialization and
    a scale factor of 1\. This is akin to training DFF neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: In **multilayer perceptrons** (**MLP**) such as our model, the first dimension
    of the weights layer is the same as the second dimension of the previous weights
    layer. The bias dimensions correspond to the number of neurons in the current
    layer. The neuron of the last layer is expected to have only one output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now is the time to combine our placeholder inputs with weights and bias for
    the four hidden layers using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `tf.matmul` command multiplies the input and weight matrices, adding the
    bias values using the `tf.add` command. Each hidden layer of the neural network
    is transformed by an activation function. In this model, we are using ReLU as
    the activation function for all layers using the `tf.nn.relu` command. The output
    of each hidden layer is fed to the input of the next hidden layer. The last layer,
    which is the output layer with a single vector output, must be transposed with
    the `tf.transpose` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the loss function of the network to measure the error between predicted
    and actual values during training. For regression-based models such as ours, the
    MSE is commonly used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `tf.squared_difference` command is defined to return the squared errors
    between the predicted and actual values, and the `tf.reduce_mean` command is the
    loss function for minimizing the mean during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the optimizer with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In minimizing the loss function, an optimizer helps to compute the network weight
    and bias during training. Here, we are using the Adam algorithm with default values.
    With this important step completed, we may now embark on phase two in training
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: Phase 2 – training our model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following steps describe the process of training our model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a TensorFlow `Session` object to encapsulate the environment in which
    a neural network model operates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are specifying a session for use in an interactive context, in this
    case a Jupyter notebook. A regular `tf.Session` is non-interactive and requires
    an explicit `Session` object to be passed using the `with` keyword when running
    operations. `InteractiveSession` removes this need and is more convenient as it
    reuses the `session` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow requires that all global variables are to be initialized before
    training. Do this using the `session.run` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following codes to train our model using mini-batch training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: An epoch is a single iteration of the entire dataset being passed forward and
    backward through the network. Usually several epochs are performed on different
    permutations of the training data for the network to learn its behavior. There
    is no fixed number of epochs for a good model, as it depends on how diverse the
    data is. Because the dataset can be too big to be fed into the model in one epoch,
    mini-batch training divides the dataset into parts and feeds it into the `session.run`
    command for learning. The first parameter specifies the optimization algorithm
    instance. The `feed_dict` parameter is given a dictionary containing our `x` and
    `y` placeholders mapped to batches of our independent and target values respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'After our model is fully trained, use it for prediction with our testing data
    containing the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `session.run` command is called with the first parameter as the output layer
    transformation function. The `feed_dict` parameter is fed with our testing data.
    The first item in the output list is read as the final output predicted values.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the predicted values are also normalized, we need to scale them back to
    the original values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Create a copy of our initial training data with the `copy()` command onto the
    new `predicted_scaled_data `variable. The last column will be replaced with our
    predicted values. Next, the `inverse_transform()` command scales our data back
    to the original size, giving us the predicted values for comparison with actual
    observed values.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting predicted and actual values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s plot the predicted and actual values onto a graph to visualize the performance
    of our deep learning model. Run the following codes to extract our values of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The rescaled `predicted_values` dataset is a NumPy `ndarray` object with predicted
    values on the last column. These values and the actual adjusted closing prices
    of 2018 are extracted to the `predictions` and `actual` variables respectively.
    Since the format of the original dataset is in descending order of time, we reverse
    them in ascending order for plotting on a graph. Run the following codes to generate
    a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output is produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/a894ecce-9919-4d3e-aff9-3655883f5ff5.png)'
  prefs: []
  type: TYPE_IMG
- en: The solid line shows the actual adjusted closing prices, while the dotted lines
    show the predicted prices. Notice how our predictions follow a general trend with
    actual prices even though the model did not have any knowledge of the actual prices
    in 2018\. Still, there is plenty of room for improvements in our deep learning
    prediction model, such as in the design of the neuron network architecture, hidden
    layers, activation functions, and initialization schemes.
  prefs: []
  type: TYPE_NORMAL
- en: Credit card payment default prediction with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another popular deep learning Python library is Keras. In this section, we will
    use Keras to build a credit card payment default prediction model, and see how
    easy it is to construct an artificial neural network with five hidden layers,
    apply activation functions, and train this model as compared to TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras is an open source deep learning library in Python, designed to be high
    level, user friendly, modular, and extensible. Keras was conceived to be an interface
    rather than a standalone machine learning framework, running on top of TensorFlow,
    CNTK, and Theano. Its huge community base with over 200,000 users makes it one
    of the most popular deep learning libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation page for Keras is at [https://keras.io](https://keras.io).
    The easiest way to install Keras is running this command in your terminal: `pip
    install keras`. By default, Keras will use TensorFlow as its tensor manipulation
    library, though it is also possible to configure another backend implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the Default of Credit Card Clients dataset downloaded from the
    UCI Machine Learning Repository ([https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)
    ). Source: Yeh, I. C., and Lien, C. H.(2009).* The comparisons of data mining
    techniques for the predictive accuracy of probability of default of credit card
    clients. Expert Systems with Applications, 36(2), 2473-2480.*'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset contains customer default payments in Taiwan. Refer to the section
    Attribute Information on the web page for the naming conventions used for the
    columns in the dataset. As the original dataset is in Microsoft Excel Spreadsheet
    XLS format, additional data processing is required. Open the file and remove the
    first row and first column containing supplementary attribute information, and
    save it as a CSV file. A copy of this file is found in `files\chapter11\default_cc_clients.csv`
    of the source code repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read this dataset as a `pandas` DataFrame object to a new variable named `df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect this DataFrame with the `info()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The output is truncated, but the summary shows that we have 30,000 rows of credit
    default data available with 23 features. The target variable is the last column
    named `default payment next month`. A value of 1 indicates a default has occurred,
    and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Should you get a chance to open the CSV file, you will notice that all values
    in the dataset are in numeric format, and values such as gender, education, and
    marital status are already converted to the integer equivalent, saving the need
    for additional data preprocessing steps. Should you have datasets containing string
    or Boolean values, remember to perform label encoding and convert them to dummy
    or indicator values.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting and scaling the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before feeding the dataset into our model, we have to prepare it in a proper
    format. The following steps guide you through the process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Split the dataset into independent and target variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Our target values in the last column of the dataset are assigned to the `target` variable,
    while remaining values are feature values and are assigned to the `features` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Split the dataset into training data and testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `train_test_split()` command of `sklearn` helps to split arrays or matrices
    into random train and test subsets. Every non-keyword argument supplied provides
    a pair of train-test splits of input. Here, we will obtain two such pairs for
    input and output data. The `test_size` parameter indicates we will be including
    20 percent of the input in the test split. The `random_state` parameter sets the
    random number generator to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert the split data into NumPy array objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, standardize the dataset by scaling the features with `MinMaxScaler()`
    of the `sklearn` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As in the previous section, the `fit_transform()` and `transform()` commands
    are applied. However, this time the default scaling range is 0 to 1\. With our
    dataset prepared, we can start to design a neural network using Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a deep neural network with five hidden layers using Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keras uses the concept of layers when working with models. There are two ways
    to do so. The simplest way is by using a sequential model for a linear stack of
    layers. The other is the functional API for building complex models such as multi-output
    models, directed acyclic graphs, or models with shared layers. This means that the
    tensor output from a layer can be used to define a model, or a model itself can
    become a layer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the Keras library and create a `Sequential` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The `add()` method simply adds layers to our model. The first and last layers
    are input and output layers respectively. Each `Dense()` command creates a regular
    layer of densely connected neurons. In between them, a dropout layer is used to
    randomly set input units to zero, helping to prevent overfitting. Here, we specified
    the dropout rate as 20%, though 20% to 50% is usually used.
  prefs: []
  type: TYPE_NORMAL
- en: The first `Dense()` command parameter with a value of 80 refers to the dimensionality
    of the output space. The optional `input_dim` parameter refers to the number of
    features for the input layer only. The ReLU activation function is specified for
    all except the output layer. Right before the output layer, a batch normalization
    layer transforms the activation mean to zero and standard deviation close to one.
    Together with a sigmoid activation function at the final output layer, output
    values can be rounded off to the nearest 0 or 1, satisfying our binary classification
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `summary()` command prints a summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We can see the output shape and weights in each layer. The number of parameters
    for a dense layer is calculated as the total number of weights matrix plus the
    number of elements in the bias matrix. For example, the first hidden layer, `dense_17`,
    will have 23×80+80=1920 parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The list of activations available in Keras can be found at [https://keras.io/activations/](https://keras.io/activations/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Configure this model for training with the `compile()` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `optimizer` parameter specifies the optimizer for training the model. Keras
    provides some optimizers, but we can choose a custom optimizer instance instead,
    using the Adam optimizer in TensorFlow from the previous section. The binary cross-entropy
    calculation is chosen as the loss function as it is suitable for our binary classification
    problem. The `metrics` parameter specifies a list of metrics to be produced during
    training and testing. Here, the accuracy will be produced for retrieval after
    fitting the model.
  prefs: []
  type: TYPE_NORMAL
- en: A list of optimizers available in Keras can be found at [https://keras.io/optimizers/](https://keras.io/optimizers/).
    A list of loss functions available in Keras can be found at [https://keras.io/losses/](https://keras.io/losses/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now is the time to train our model using the `fit()` command with 100 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output is truncated as the model produces detailed training updates
    for every epoch. A `History()` object is created and fed into the model's callback
    for recording events during training. The `fit()` command allows the number of
    epochs and batch size to be specified. The `validation_split` parameter is set
    such that 20% of the training data will be set aside as validation data, evaluating
    the loss and model metrics at the end of each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of training the data all at once, you can also train your data in batches.
    Call the `fit()` command with an `epochs` and `batch_size` parameter, like this:
    `model.fit(x_train, y_train, epochs=5, batch_size=32)`. You can also train batches
    manually using the `train_on_batch()` command, like this: `model.train_on_batch(x_batch,
    y_batch)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the performance of our model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using our test data, we can compute the loss and accuracy of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Our model has 82% prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Running risk metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 10](28d7845a-ecc1-46b1-99a0-0e6dea8fd2be.xhtml), *Machine Learning
    for Finance*, we discussed the confusion matrix, accuracy score, precision score,
    recall score, and F1 score in measuring classification-based predictions. We can
    reuse those metrics on our model as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the model output is in the normalized decimal format between 0 and 1,
    we round it up to the nearest 0 or 1 integer to obtain the predicted binary classification
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `ravel()` command presents the result as a single list stored in the `pred_values` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute and display the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/3df17841-5419-4db4-beda-87adfab81b81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Print the accuracy, precision score, recall score, and F1 score using the `sklearn`
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The low recall score and the slightly below-average F1 score hint that our model
    is not sufficiently competitive. Perhaps we can visit historical metrics in the
    next section to find out more.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying recorded events in Keras history
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s review the `callback_history` variable, which is the `History` object
    populated during the `fit()` command. The `History.history` attribute is a dictionary
    containing four keys, storing the accuracy and loss values during training and
    validation. These are represented as a list of values saved after every epoch.
    Extract this information into separate variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the training and validation loss with the following codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following graph of losses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/09db1e3c-fd8d-4c82-99b5-29c17b5bba05.png)'
  prefs: []
  type: TYPE_IMG
- en: The solid line shows the path of the training loss decreasing as the number
    of epochs increases, meaning that our model is learning the training data better
    over time. The dashed line shows the validation loss increasing as the number
    of epochs increases, meaning that our model is not generalizing well enough on
    the validation set. These trends suggest that our model is prone to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the training and validation accuracy with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ec25ea42-9a97-419f-8b9e-2418e1b09b75.png)'
  prefs: []
  type: TYPE_IMG
- en: The solid line shows the path of the training accuracy increasing as the number
    of epochs increases, while the dashed line shows the validation accuracy decreasing.
    These two graphs strongly suggest that our model is overfitting the training data.
    Looks like more work needs to be done! To prevent overfitting, you can use more
    training data, reduce the capacity of the network, add weight regularization,
    and/or use a dropout layer. In reality, deep learning modeling requires understanding
    the underlying problem, finding a suitable neural network architecture, and investigating
    the effects of activation functions at each layer in order to produce good results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have been introduced to deep learning and the use of neural
    networks. An artificial neutral network consists of an input layer and an output
    layer, with one or more hidden layers in between. Each layer consists of artificial
    neurons, and each artificial neuron receives weighted inputs that are  summed
    together with a bias. An activation function transforms these inputs into an output,
    and feeds it as input to another neuron.
  prefs: []
  type: TYPE_NORMAL
- en: Using the TensorFlow Python library, we built a deep learning model with four
    hidden layers to predict the prices of a security. The dataset is preprocessed
    by scaling and split into training and testing data. Designing an artificial neuron
    network involves two phases. The first phase is to assemble the graph, and the
    second phase is to train the model. A TensorFlow session object provides an execution
    environment, where training is done over several epochs, and each epoch uses mini-batch
    training. As the model output includes normalized values, we scale the data back
    to its original representation to return predicted prices.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular deep learning library is Keras, utilizing TensorFlow as the
    backend. We built another deep learning model to predict credit card payment defaults
    with five hidden layers. Keras uses the concept of layers when working with models,
    and we saw how easy it was to add layers, configure the model, train it, and evaluate
    its performance. The `History` object of Keras records the loss and accuracy of
    training and validation data for successive epochs.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, a good deep learning model requires effort and understanding the
    underlying problem in order to produce good results.
  prefs: []
  type: TYPE_NORMAL
