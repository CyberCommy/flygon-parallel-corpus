- en: Working with Stateful Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, everything was fun and games. We built services, deployed them to Kubernetes,
    and ran commands and queries against these services. We enabled Kubernetes to
    have those services up and running by scheduling pods on deployment or if anything
    went wrong. This works great for stateless services that can just run anywhere.
    In the real world, distributed systems manage important data. If a database stores
    its data on the host filesystem and that host goes down, you (or Kubernetes) can't
    just start a fresh instance of the database on a new node because the data will
    be lost.
  prefs: []
  type: TYPE_NORMAL
- en: In general, you keep your data from getting lost by redundancy; you keep multiple
    copies, store backups, utilize append-only logs, and more. Kubernetes assists
    by providing a whole storage model with concepts and related resources, such as
    volumes, volume claims, and StatefulSets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will dive deeper into the Kubernetes storage model. We
    will also extend the Delinkcious news service to store its data in Redis instead
    of in memory. We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Abstracting storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data outside your Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data inside your Kubernetes cluster with StatefulSets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving high performance with local storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using relational databases in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using non-relational data stores in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will examine a number of Kubernetes manifests, work with
    different storage options, and extend Delinkcious to support a new data store.
    There is no need to install anything new.
  prefs: []
  type: TYPE_NORMAL
- en: The code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code is split between two Git repositories, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code samples at [https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter08)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the updated Delinkcious application at [https://github.com/the-gigi/delinkcious/releases/tag/v0.6](https://github.com/the-gigi/delinkcious/releases/tag/v0.6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abstracting storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At its core, Kubernetes is an orchestration engine used for managing containerized
    workloads. Note that, here, the keyword is *containerized*. Kubernetes doesn't
    care what the workloads are as long as they are packaged in containers; it knows
    how to handle them. Initially, Kubernetes only supported Docker images, and then,
    later, it added support for other runtimes. Then, Kubernetes 1.5 introduced the
    **Container Runtime Interface** (**CRI**), and gradually pushed the explicit support
    for other runtimes out of tree. Here, Kubernetes no longer cared about which container
    runtime was actually deployed on the nodes and just needed to work with the CRI.
  prefs: []
  type: TYPE_NORMAL
- en: A similar story unfolded with networking, where the **Container Networking Interface**
    (**CNI**) was defined early. The life of Kubernetes was simple. It was left to
    different networking solutions to provide their CNI plugins. Storage, however,
    was different (until it wasn't). In the following subsections, we'll go over the
    Kubernetes storage model, understand the differences between in-tree and out-of-tree
    storage plugins, and, finally, learn about the **Container Storage Interface**
    (**CSI**), which provides a neat solution for storage in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes storage model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Kubernetes storage model consists of several concepts: storage classes,
    volumes, persistent volumes, and persistent volume claims. Let''s examine how
    these concepts interact to allow containerized workloads access to storage during
    execution.'
  prefs: []
  type: TYPE_NORMAL
- en: Storage classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The storage class is a way of describing the available types of storage that
    can be provisioned. Often, there is a default storage class that is used when
    provisioning a volume without specifying a particular storage class. Here is the
    standard storage class in Minikube, which stores data on the host (that is, the
    hosting node):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Different storage classes have different parameters tied to the actual backing
    storage. Volume provisioners know how to use the parameters of their storage classes.
    The storage class metadata includes the provisioner, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Volumes, persistent volumes, and provisioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A volume in Kubernetes has an explicit lifetime that coincides with its pod.
    When the pod goes away, so does the storage. There are many types of volumes that
    are very useful. We've already seen a few examples, such as ConfigMap and secret
    volumes. But there are other volume types that are used for reading and writing.
  prefs: []
  type: TYPE_NORMAL
- en: You can take a look at the full list of volume types here: [https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes](https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes).
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes also supports the concept of persistent volumes. These volumes must
    be provisioned by system administrators, and they are not managed by Kubernetes
    itself. When you want to store data persistently, then you use persistent volumes.
    Administrators can statically provision persistent volumes ahead of time. The
    process involves administrators provisioning external storage and creating a `PersistentVolume`
    Kubernetes object that users can consume.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic provisioning is the process of creating volumes on the fly. Users request
    storage and this is created dynamically. Dynamic provisioning depends on storage
    classes. Users can specify a particular storage class, otherwise, the default
    storage class (if it exists) will be used. All Kubernetes cloud providers support
    dynamic provisioning. Minikube supports it too (the backing store is the localhost
    filesystem).
  prefs: []
  type: TYPE_NORMAL
- en: Persistent volume claims
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, the cluster administrator either provisions some persistent volumes or,
    alternatively, the cluster supports dynamic provisioning. We can now claim some
    storage for our workload by creating a persistent volume claim. But, first, it's
    important to understand the difference between ephemeral and persistent storage.
    We'll create an ephemeral file in a pod, restart the pod, and check that the file
    vanished. Then, we'll do the same thing again, but, this time, write the file
    to the persistent storage and check that the file still exists once the pod is
    restarted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start, let me share some convenient shell functions and aliases that
    I created in order to quickly launch an interactive session in specific pods.
    A Kubernetes deployment generates random pod names. For example, for the `trouble`
    deployment, the current pod name is `trouble-6785b4949b-84x22`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This is not a very memorable name, and it also changes whenever the pod is
    restarted (automatically by the deployment). Unfortunately, the `kubectl exec`
    command requires an exact pod name to run commands. I created a little shell function
    called `get_pod_name_by_label()`, which returns a pod name based on a label. Since
    labels from the pod template don''t change, this is a good way to discover pod
    names. However, there may be multiple pods from the same deployment with the same
    labels. We just need any kind of pod, so we can simply pick the first. Here is
    the function, and I aliased it to `kpn` so that it''s easier to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, the `trouble` deployment pods can have a label called `run=trouble`.
    Here is how to find the actual pod name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this function, I created an alias called `trouble`, which launches an
    interactive bash session in the `trouble` pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can connect to the `trouble` pod and start working in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This was a long digression, but it''s a very useful technique. Now, let''s
    get back to our plan and create an ephemeral file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s kill the pod. The `trouble` deployment will schedule a new `trouble`
    pod, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'When we access the new pod, we discover that `life.txt` vanished as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s understandable because it was stored in the filesystem of the container. The
    next step is to have the `trouble` pod claim some persistent storage. Here is
    a persistent volume claim that provisions one gibibyte dynamically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the YAML manifest for the entire `trouble` deployment that consumes
    this claim as a volume and mounts it to the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `keep-me` volume is based on the `some-storage` persistent volume claim:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The volume is mounted to the `/data` directory inside the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s write something to `/data`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The final state is to delete the pod and, when a new pod is created, verify
    whether the `infinity.txt` file is still in `/data`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Yay, it works! A new pod was created and the persistent storage with the `infinity.txt`
    file was mounted to the new container.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent volumes can also be used to share information directly between multiple
    instances of the same image because the same persistence storage will be mounted
    to all containers using the same persistent storage claim.
  prefs: []
  type: TYPE_NORMAL
- en: In-tree and out-of-tree storage plugins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two types of storage plugins: in-tree and out-of-tree. In-tree means
    that these storage plugins are part of Kubernetes itself. In the volume clause,
    you refer to them by name. For example, here, a **Google Compute Engine** (**GCE**)
    persistent disk is configured by name. Kubernetes explicitly knows that such a
    volume has fields such as `pdName` and `fsType`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Take a look at the complete list of in-tree storage plugins at: [https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes).
  prefs: []
  type: TYPE_NORMAL
- en: There are several other specialized volume types, such as `emptyDir`, `local`,
    `downwardAPI`, and `hostPath`, that you can read more about. The concept of in-tree
    plugins is somewhat cumbersome. It bloats Kubernetes and requires changing Kubernetes
    itself whenever a provider wants to improve their storage plugin or introduce
    a new one.
  prefs: []
  type: TYPE_NORMAL
- en: This is where out-of-tree plugins come into the picture. The idea is that Kubernetes
    defines a standard storage interface and a standard way of providing plugins to
    implement the interface in a running cluster. Then, it's the job of the cluster
    administrator to make sure that the proper out-of-tree plugins are available.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of out-of-tree plugins that Kubernetes supports: FlexVolume
    and CSI. FlexVolume is old and deprecated. I will not go into detail about FlexVolume,
    except to recommend that you don''t use it.'
  prefs: []
  type: TYPE_NORMAL
- en: For more detail, you can refer to the following link: [https://kubernetes.io/docs/concepts/storage/volumes/#flexVolume](https://kubernetes.io/docs/concepts/storage/volumes/#flexVolume)
  prefs: []
  type: TYPE_NORMAL
- en: The big star of storage is the CSI. Let's drill down and understand how CSI
    works and what a huge improvement it is.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CSI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CSI was designed to address all the issues with in-tree plugins and the cumbersome
    aspects of FlexVolume plugins. What makes CSI so enticing to storage providers
    is that it is not a Kubernetes-only standard, but an industry-wide standard. It
    allows storage providers to write a single driver for their storage solution and
    become immediately compatible with a broad range of container orchestration platforms
    such as Docker, Cloud Foundry, Mesos, and, of course, Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the official specification at [https://github.com/container-storage-interface/spec](https://github.com/container-storage-interface/spec).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kubernetes team provides three components that are sidecar containers and
    provide generic CSI support for any CSI storage provider. These components are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Driver registrar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: External provisioner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: External attacher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Their job is to interface with the kubelet as well as the API server. The storage
    provider will typically package these sidecar containers along with their storage
    driver implementation in a single pod that can be deployed as a Kubernetes DaemonSet
    on all nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a diagram that demonstrates the interaction between all the pieces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/84bc28f4-4c46-4a6d-b11f-6deb77c2c413.png)'
  prefs: []
  type: TYPE_IMG
- en: It is pretty complicated, but this complication is necessary to separate concerns,
    allow the Kubernetes team to do a lot of the heavy lifting, and leave storage
    providers to focus on their storage solution. As far as users and developers are
    concerned, this is all completely transparent. They continue to interact with
    storage through the same Kubernetes storage abstractions of storage classes, volumes,
    and persistent volume claims.
  prefs: []
  type: TYPE_NORMAL
- en: Standardizing on CSI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CSI is superior to in-tree plugins (and FlexVolume plugins). However, the current
    situation of a hybrid, where you can use either in-tree plugins (or FlexVolume
    plugins) or CSI plugins is suboptimal. The Kubernetes team has a detailed plan
    to migrate in-tree plugins to CSI.
  prefs: []
  type: TYPE_NORMAL
- en: You can find out more about this detailed plan at [https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/csi-migration.md](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/csi-migration.md).
  prefs: []
  type: TYPE_NORMAL
- en: Storing data outside your Kubernetes cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is not a closed system. Workloads running inside a Kubernetes cluster
    can access storage running outside the cluster. This is most appropriate when
    you migrate an existing application that is already in storage, and configured
    and operated outside of Kubernetes. In this case, it is a wise move to do it gradually.
    First, move the workloads to run as containers managed by Kubernetes. These containers
    will be configured with endpoints to data stores that live outside the cluster.
    Later, you can consider whether it is worth the effort to bring this external
    storage into the fold.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some other use cases where it makes sense to use out-of-cluster storage,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Your storage cluster uses some exotic hardware, or the networking doesn't have
    a mature in-tree or CSI plugin (hopefully, as CSI becomes the gold standard, this
    will become rare).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You run Kubernetes through a cloud provider and it's going to be too expensive,
    too risky, and/or too slow to migrate all the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other applications in your organization use the same storage cluster and it
    is often impractical and non-economical to migrate all the applications and systems
    in your organization to Kubernetes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to regulatory requirements, you must retain control of your data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several downsides to managing storage outside of Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: Security (you need to provide network access from your workloads to a separate
    storage cluster).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You must implement the scaling, availability, monitoring, and configuration
    of your storage cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When things change on the storage cluster side, you often need to make corresponding
    configuration changes on the Kubernetes side.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might suffer performance or latency overhead due to extra network hops and/or
    authentication, authorization, or encryption.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data inside your cluster with StatefulSets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's best to store data within your Kubernetes cluster. This provides a uniform
    one-stop shop to manage your workloads and all the resources they depend on (excluding
    third-party external services). Additionally, you get to integrate your storage
    with your streamlined monitoring, which is very important. We will discuss monitoring
    in depth in a future chapter. However, running out of disk space is the bane of
    many system administrators. But there is a problem if you store data on a node
    and your data store pods get rescheduled to a different node, and the data it
    expects to be available is not there. The Kubernetes designers realized that the
    ephemeral pod philosophy doesn't work for storage. You could try to manage it
    yourself using pod-node affinity and other mechanisms that Kubernetes provides,
    but it's much better to use StatefulSet, which is a specific solution for managing
    storage-aware services in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding a StatefulSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At its core, a StatefulSet is a controller that manages a set of pods with
    some extra properties, such as ordering and uniqueness. The StatefulSet allows
    its set of pods to be deployed and scaled, while preserving their special properties.
    StatefulSets reached **g****eneral availability** (**GA**) status in Kubernetes
    1.9\. You can think of a StatefulSet as a souped-up deployment. Let''s take a
    look at a sample StatefulSet for the user service, which uses a relational PostgresDB
    as its data store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: There is a lot going on here, but it's all a composition of familiar concepts.
    Let's break it down into its components.
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSet components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The StatefulSet is comprised of three main parts, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**StatefulSet metadata and definition**: The StatefulSet metadata and definition are
    pretty similar to a deployment. You have the standard API version, kind, and metadata
    name; then, `spec`, which includes a selector for the pods (which must match the
    pod template selectors that will come next), the number of replicas (just one,
    in this case), and the major difference compared with a deployment, that is, `serviceName`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'A StatefulSet *must* have a headless service associated with the StatefulSet
    to manage the network identity of the pods. The service name is `user-db` in this
    case; here it is for completeness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**A pod template**: The next part is a standard pod template. The PGDATA environment
    variable (`/data/user-db`), which tells postgres where to read and write its data,
    must be the same as the mount path of the `user-db` volume (`/data/user-db`) or
    a subdirectory. This is where we wire up the data store with the underlying storage:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Volume claim templates**: The last part is the volume claim templates. Note
    that this is plural; some data stores may require multiple types of volumes (for
    example, for logging or caching) that require their own persistent claims. In
    this case, one persistent claim is enough:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now is a good time to dive deeper and gain an understanding of the special properties
    of StatefulSets and why they are important.
  prefs: []
  type: TYPE_NORMAL
- en: Pod identity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'StatefulSet pods have a stable identity that includes the following triplet:
    a stable network identity, an ordinal index, and stable storage. These always
    go together; the name of each pod is `<statefulset name>-<ordinal>`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The headless service associated with the StatefulSet provides the stable network
    identity. The service DNS name will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Each pod, *X*, will have a stable DNS name as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, the first pod of the `user-db` StatefulSet will be called the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, StatefulSet pods automatically get assigned a label, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Orderliness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each pod in a StatefulSet gets an ordinal index. But, what is this for? Well,
    some data stores rely on the orderly sequence of initialization. The StatefulSet
    ensures that when the StatefulSet pods are initialized, scaled up, or scaled down,
    it is always done in order.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes 1.7, the orderliness restriction was relaxed. For data stores
    that don't require orderliness, it makes sense to allow for parallel operations
    on multiple pods in the StatefulSet. This can be specified in the `podPolicy`
    field. The values allowed are `OrderedReady` for the default orderly behavior,
    or *parallel* for the relaxed parallel mode, where pods can be launched or terminated
    while other pods are still launching or terminating.
  prefs: []
  type: TYPE_NORMAL
- en: When should you use a StatefulSet?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should use a StatefulSet when you manage your data store yourself in the
    cloud and require good control over the storage your data store uses. The primary
    use case is for distributed data stores, but a StatefulSet is useful even if your
    data store has just one instance or pod. The stable pod identity with the stable
    attached storage is well worth it, although orderliness, of course, is not required.
    If your data store is backed up by a shared storage layer such as NFS, then a
    StatefulSet might not be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, this may be common sense, but if you don't manage the data store
    yourself, then you don't need to worry about the storage layer and you don't need
    to define your own StatefulSets. For example, if you run your system on AWS and
    use S3, RDS, DynamoDB, and Redshift, then you don't really need a StatefulSet.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing deployment and StatefulSets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deployments are designed to manage any sets of pods. They can also be used
    to manage the pods of a distributed data store. StatefulSets were specifically
    designed to support the needs of distributed data stores. However, the special
    properties of ordering and uniqueness are not always necessary. Let''s compare
    deployments to StatefulSets and see for ourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployments don't have associated storage, whereas StatefulSets do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployments have no associated service, whereas StatefulSets do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment pods have no DNS name, whereas StatefulSet pods do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployments launch and terminate pods in any order, whereas StatefulSets follow
    a prescribed order (by default).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I recommend that you stick to deployments unless your distributed data store
    requires the special properties of StatefulSets. If you just need a stable identity,
    and not an ordered launch and shutdown, then use `podPolicy=Parallel`.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing a large StatefulSet example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cassandra ([https://cassandra.apache.org/](https://cassandra.apache.org/)) is
    an interesting distributed data store that I have a lot of experience with. It
    is very powerful, but it requires a lot of knowledge to operate properly and develop
    against. It is also a great use case for StatefulSets. Let's quickly review Cassandra
    and learn how to deploy it in Kubernetes. Note that we will not use Cassandra
    in Delinkcious.
  prefs: []
  type: TYPE_NORMAL
- en: A quick introduction to Cassandra
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cassandra is an Apache open source project. It's a columnar data store and is
    very well suited for managing time series data. I've used it to collect and manage
    data from a network of thousands of air quality sensors for more than three years.
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra has an interesting modeling approach, but, here, we care about storage.
    Cassandra is highly available, linearly scalable, and very reliable (no SPOF)
    via redundancy. Cassandra nodes share responsibility for the data (which is partitioned
    through the **distributed hash table**, or **DHT**). Multiple copies of the data
    are spread across multiple nodes (this is typically three or five).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this way, if a Cassandra node goes down, then there are two other nodes
    that have the same data and can respond to queries. All nodes are the same; there
    are no masters and no slaves. The nodes constantly chat with each other through
    a gossip protocol and, when new nodes join the cluster, Cassandra redistributes
    the data among all the nodes. Here is a diagram that shows how data is distributed
    across the Cassandra cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/82ce0658-00d3-4bb0-a37f-bcf38ec26f6a.png)'
  prefs: []
  type: TYPE_IMG
- en: You can think of the nodes as a ring and the DHT algorithm hashes each wide
    row (the unit of work) and assigns it to the *N* nodes (depending on the replication
    factor of the cluster). With that kind of precise placement of individual rows
    in specific nodes, you can see how the stable identity and, potentially, the ordering
    properties of a StatefulSet can come in handy.
  prefs: []
  type: TYPE_NORMAL
- en: Let's explore what it takes to deploy a Cassandra cluster as a StatefulSet in
    Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Cassandra on Kubernetes using StatefulSets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here is a truncated version that includes the parts we should focus on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part includes `apiVersion`, `kind`, `metadata`, and `spec`, as we''ve
    seen before. The name is `cassandra`, and the label is `app: cassandra`. In `spec`,
    the `serviceName` name is also `cassandra`, and there are three replicas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The pod template has a matching label of `app: cassandra`. The container is
    named `cassandra`, too, and uses a Google sample image with the always pull policy.
    Here, `terminationGraceInSeconds` is set to 1,800 seconds (that is, 30 minutes).
    That''s the time that the StatefulSet will allow the pod to try and recover if
    it becomes unresponsive. Cassandra has a lot of redundancy built in, so it''s
    okay to let a node attempt recovery for 30 minutes. I removed a lot of ports,
    environment variables, and readiness checks (the ellipses). The volume mount is
    called `cassandra-data`, and its path is `/cassandra_data`. That''s where Cassandra
    stores its data files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the volume claim template defines the persistent storage that matches
    the volume mounted in the container with the name `cassandra-data`. The storage
    class, `fast`, is not shown here, but it is typically local storage on the same
    node that runs the Cassandra pod. The storage size is one gibibyte:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This should all look very familiar to you at this point. However, there's more
    successful Cassandra deployment to discover. If you recall, Cassandra has no master;
    Cassandra nodes talk to each other constantly using the gossip protocol.
  prefs: []
  type: TYPE_NORMAL
- en: But how do Cassandra nodes find each other? Enter the seed provider; whenever
    a new node is added to the cluster, it is configured with the IP addresses of
    some seed nodes (in this case, `10.0.0.1`, `10.0.0.2`, and `10.0.0.3`). It starts
    exchanging messages with these seed nodes, which inform the new node of other
    Cassandra nodes in the cluster, as well as notifying all the other existing nodes
    that a new node has joined the cluster. In this way, each node in the cluster
    can very quickly know about every other node in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a section from a typical Kubernetes config file (`cassandra.yaml`)
    that defines the seed provider. In this case, it''s just a simple list of IP addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The seed provider can be a custom class, too. This is a very nice extensible
    design. In Kubernetes, it is necessary because the original seed nodes may be
    moved around and get new IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: To address this, there is a custom `KubernetesSeedProvider` class that talks
    to the Kubernetes API server and can always return the IP addresses of the seed
    nodes at the time of the query. Cassandra is implemented in Java, and so is the
    custom seed provider that implements the `SeedProvider` Java interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re not going to dissect this code in detail. The main thing to note is
    that it interfaces with a native Go library called `cassandra-seed.so`, and then
    it uses it to get the Kubernetes endpoints of the Cassandra service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The complete source code can be found at [https://github.com/kubernetes/examples/blob/master/cassandra/java/src/main/java/io/k8s/cassandra/KubernetesSeedProvider.java](https://github.com/kubernetes/examples/blob/master/cassandra/java/src/main/java/io/k8s/cassandra/KubernetesSeedProvider.java).
  prefs: []
  type: TYPE_NORMAL
- en: That's the magic that connects Cassandra to Kubernetes and allows them to work
    together. Now that we've seen how a complicated distributed data store can be
    deployed in Cassandra, let's take a look at local storage, which graduated to
    GA in Kubernetes 1.14.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving high performance with local storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now discuss the affinity between compute and storage. There is an interesting
    relationship between speed, capacity, persistence, and cost. When your data lives
    near your processor, you can start working on it immediately, as opposed to fetching
    it over the network. That's the promise of local storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two primary ways to store your data locally: in memory and on local
    drives. However, there are nuances; memory is the fastest, SSD drives are about
    4 times slower than memory, and spinning disks are roughly 20 times slower than
    SSD drives ([https://gist.github.com/jboner/2841832](https://gist.github.com/jboner/2841832)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider both of these following options:'
  prefs: []
  type: TYPE_NORMAL
- en: Storing your data in memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing your data on a local SSD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing your data in memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The highest performance, as far as read and write latency and throughput is
    concerned, is when you keep your data in memory. There are different memory types
    and caches, but the bottom line is that memory is super fast. However, memory
    has significant downsides too, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A node has much more limited memory compared to disks (that is, it requires
    more machines to store the same amount of data).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory is very expensive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory is ephemeral.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are some use cases where you require your entire dataset in memory. In
    these cases, either the dataset is very small, or you can split it across multiple
    machines. If the data is important and can''t be easily generated, then you can
    address the ephemeral nature of memory in the following two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Keep a persistent copy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redundancy (that is, keep data in memory across multiple machines and potentially
    geodistributed).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing your data on a local SSD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A local SSD is not as fast as memory, but it is very fast. Of course, you can
    always combine in-memory caching too (any respectable data store will use memory
    caching to its advantage). Using an SSD is appropriate when you require fast performance,
    but your working set doesn't fit in memory or, alternatively, you don't want to
    pay the premium of large memory when you can get by with a much cheaper, yet still
    very fast, SSD. For example, Cassandra recommends using local SSD storage as the
    backing store for its data.
  prefs: []
  type: TYPE_NORMAL
- en: Using relational databases in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've used a relational database in all our services, but, as we will
    soon discover, we didn't have real persistence. First, we'll look at where the
    data is stored, and then we'll explore how durable it is. Finally, we'll migrate
    one of the databases to use a StatefulSet for proper persistence and durability.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding where the data is stored
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For PostgreSQL, there is a `data` directory; this directory can be set using
    the `PGDATA` environment variable. By default, it is set to `/var/lib/postgresql/data`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at what this directory contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: However, the `data` directory can be ephemeral or persistent depending on how
    it was mounted to the container.
  prefs: []
  type: TYPE_NORMAL
- en: Using a deployment and service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With a service fronting your database pods, you can easily access the data.
    When a database pod is killed, it will be restarted by the deployment. However,
    since the pod can be scheduled on a different node, it is up to you to make sure
    that it has access to the storage where the actual data is. Otherwise, it will
    just start empty and you'll lose all the data. This is a development-only setup,
    and how most Delinkcious services keep their data – by running a PostgresDB container
    that is only as persistent as its pod. It turns out that the data is stored in
    the Docker container itself running inside the pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Minikube, I can inspect the Docker container directly by first SSH-ing into
    the node, finding the ID of the postgres container, and then inspecting it (that
    is, only if the relevant information is displayed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This means that, if the container goes away (for example, if we upgrade to a
    new version) and certainly if the node goes away, then all our data disappears.
  prefs: []
  type: TYPE_NORMAL
- en: Using a StatefulSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With a StatefulSet, the situation is different. The data directory is mounted
    to the container, but the storage itself is managed externally. As long as the
    external storage is reliable and redundant, our data is safe, regardless of what
    happens to specific containers, pods, and nodes. We've previously mentioned how
    to define a StatefulSet for the user database using a headless service. However,
    consuming the storage of the StatefulSet can be a little challenging. The headless
    service attached to a StatefulSet has no cluster IP. So, how would the user service
    connect to its database? Well, we will have to help it.
  prefs: []
  type: TYPE_NORMAL
- en: Helping the user service locate StatefulSet pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The headless `user-db` service has no cluster IP, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'However, it does have endpoints, which are the IP addresses in the cluster
    of all the pods that back the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This is a good option; endpoints are not exposed through environment variables,
    such as a service with a cluster IP (`<service name>_SERVICE_HOST and <service
    name>_SERVICE_PORT`). So, for a service to find the endpoints of a headless service,
    they'll have to query the Kubernetes API directly. While that's possible, it adds
    unnecessary coupling between the service and Kubernetes. We won't be able to run
    the service outside of Kubernetes for testing because it relies on the Kubernetes
    API. However, we can trick the user service and populate `USER_DB_SERVICE_HOST`
    and `USER_DB_SERVICE_PORT` using a config map.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is that StatefulSet pods have a stable DNS name. For the user database,
    there is one pod whose DNS name is `user-db-0.user-db.default.svc.cluster.local`.
    Inside the troubleshooter container shell, we can verify that the DNS name indeed
    resolves to the user database endpoint, `172.17.0.25`, by running the `dig` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can take this stable DNS name and assign it to `USER_DB_SERVICE_HOST`
    in a config map for the `user-manager` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this config map is applied, the user service will be able to locate the
    user database pod of the StatefulSet through the environment variables. Here is
    the code that uses these environment variables from `pkg/db_util/db_util.go`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The user service calls it in its `Run()` function to initialize its database
    store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's take a look at how to address the problem of managing schema changes.
  prefs: []
  type: TYPE_NORMAL
- en: Managing schema changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most challenging topics when working with relational databases is
    managing the SQL schema. When the schema changes, the change may be backward compatible
    (by adding a column) or non-backward compatible (by splitting a table into two
    separate tables). When the schema changes, we need to migrate our database, but
    also migrate the code that is affected by the schema change.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you can afford a short downtime, then the process can be very simple, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Shut down all the impacted services and perform DB migration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy a new code that knows how to work with the new schema.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Everything just works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, if you need to keep the system running, you'll have to go through a
    more complicated process by breaking the schema change into multiple backward-compatible
    changes, including corresponding code changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, when splitting a table into two tables, the following process
    can be performed:'
  prefs: []
  type: TYPE_NORMAL
- en: Keep the original table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the two new tables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy code that writes both to the old table and the new tables and can read
    from all of the tables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Migrate all the data from the old table to the new tables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy a code change that reads only from the new tables (which have all the
    data now).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Delete the old table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Relational databases are very useful; however, sometimes, the correct solution
    is a non-relational data store.
  prefs: []
  type: TYPE_NORMAL
- en: Using non-relational data stores in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes and StatefulSets are not limited or even geared toward relational
    data stores. Non-relational (also known as NoSQL) data stores are very useful
    for many use cases. One of the most versatile and popular in-memory data stores
    is Redis. Let's get to know Redis and examine how to migrate the Delinkcious news
    service to use Redis instead of storing events in ephemeral memory.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to Redis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Redis is often described as a data structure server. Since it keeps the entire
    data store in memory, it can perform many advanced operations on the data efficiently.
    The price you pay, of course, is that you have to keep *all* of the data in memory.
    This is possible only for small datasets and, even then, it''s expensive. If you
    don''t access most of your data, keeping it in memory is a huge waste. Redis can
    be used as a fast, distributed cache for hot data; so, even if you can''t use
    it as a distributed cache for your entire dataset in memory, you can still use
    Redis for the hot data (which is frequently used). Redis also supports clusters
    where the data is shared across multiple nodes, so it''s able to handle very large
    datasets too. Redis has an impressive list of features, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It provides multiple data structures such as lists, hashes, sets, sorted sets,
    bitmaps, streams, and geospatial indexes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides atomic operations on many data structures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports transactions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports auto-eviction with TTL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports LRU eviction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It enables pub/sub.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows optional persistence to the disk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows optional appending of operations to the journal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides Lua scripting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's take a look at how Delinkcious uses Redis.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting events in the news service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The news service provisions a Redis instance as a StatefulSet, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'It is supported by a headless service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the same trick of injecting the DNS name of the Redis pod through
    environment variables using a config map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'With the provisioning out of the way, let''s take a look at how the code is
    accessing Redis. In the `Run()` function of the news service, if the environment
    variables for Redis are not empty, then it will create a new Redis store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The `NewRedisNewStore()` function is defined in `pkg/new_manager/redis_news_store`.
    It creates a new Redis client (from the `go-redis` library). It also calls the
    client''s `Ping()` method to ensure that Redis is up and running and is reachable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`RedisNewsStore` stores the events in a Redis list, which is serialized to
    TOML. This is all implemented in `AddEvent()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`RedisNewsStore` implements the `GetNews()` method to fetch events in order.
    First, it calculates the start and end indexes to query the event list based on
    the starting index and the maximum page size. Then, it gets the results, which
    are serialized to TOML, unmarshals them into the `om.Event` struct, and appends
    them to the result list of events. Finally, it computes the next index to fetch
    (`-1` if there are no more events):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: At this point, you should have a good grasp of a non-relational data store,
    including when to use them and how to integrate Redis as a data store for your
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we dealt with the very important topic of storage and real-world
    data persistence. We learned about the Kubernetes storage model, the common storage
    interface, and StatefulSets. Then, we discussed how to manage relational and non-relational
    data in Kubernetes and migrated several Delinkcious services to use proper persistent
    storage through StatefulSets, including how to provide data store endpoints for
    StatefulSet pods. Finally, we implemented a non-ephemeral data store for the news
    service using Redis. At this point, you should have a clear idea of how Kubernetes
    manages storage and is able to choose the proper data stores for your system,
    as well as integrate them into your Kubernetes cluster and with your services.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the exciting domain of serverless computing.
    We'll consider when the serverless model is useful, discuss current solutions
    for Kubernetes, and extend Delinkcious with some serverless tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following references for more information:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CSI**: [https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b](https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**StatefulSet**: [https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cassandra**: [https://cassandra.apache.org/](https://cassandra.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Redis**: [http://redis.io/](http://redis.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latency numbers every programmer should know**: [https://gist.github.com/jboner/2841832](https://gist.github.com/jboner/2841832)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
