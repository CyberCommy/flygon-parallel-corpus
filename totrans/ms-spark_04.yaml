- en: Chapter 4. Apache Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, I would like to examine Apache Spark SQL, the use of Apache
    Hive with Spark, and DataFrames. DataFrames have been introduced in Spark 1.3,
    and are columnar data storage structures, roughly equivalent to relational database
    tables. The chapters in this book have not been developed in sequence, so the
    earlier chapters might use older versions of Spark than the later ones. I also
    want to examine user-defined functions for Spark SQL. A good place to find information
    about the Spark class API is: `spark.apache.org/docs/<version>/api/scala/index.html`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I prefer to use Scala, but the API information is also available in Java and
    Python formats. The `<version>` value refers to the release of Spark that you
    will be using—1.3.1\. This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: SQL context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing and saving data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User-defined functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Hive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before moving straight into SQL and DataFrames, I will give an overview of the
    SQL context.
  prefs: []
  type: TYPE_NORMAL
- en: The SQL context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The SQL context is the starting point for working with columnar data in Apache
    Spark. It is created from the Spark context, and provides the means for loading
    and saving data files of different types, using DataFrames, and manipulating columnar
    data with SQL, among other things. It can be used for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Executing SQL via the SQL method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registering user-defined functions via the UDF method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data source access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DDL operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I am sure that there are other areas, but you get the idea. The examples in
    this chapter are written in Scala, just because I prefer the language, but you
    can develop in Python and Java as well. As shown previously, the SQL context is
    created from the Spark context. Importing the SQL context implicitly allows you
    to implicitly convert RDDs into DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For instance, using the previous `implicits` call, allows you to import a CSV
    file and split it by separator characters. It can then convert the RDD that contains
    the data into a data frame using the `toDF` method.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to define a Hive context for the access and manipulation
    of Apache Hive database table data (Hive is the Apache data warehouse that is
    part of the Hadoop eco-system, and it uses HDFS for storage). The Hive context
    allows a superset of SQL functionality when compared to the Spark context. The
    use of Hive with Spark will be covered in a later section in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I will examine some of the supported file formats available for importing
    and saving data.
  prefs: []
  type: TYPE_NORMAL
- en: Importing and saving data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I wanted to add this section about importing and saving data here, even though
    it is not purely about Spark SQL, so I could introduce concepts such as **Parquet**
    and **JSON** file formats. This section also allows me to cover how to access
    and save data in loose text; as well as the CSV, Parquet and JSON formats, conveniently,
    in one place.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the Text files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the Spark context, it is possible to load a text file into an RDD using
    the `textFile` method. Also, the `wholeTextFile` method can read the contents
    of a directory into an RDD. The following examples show how a file, based on the
    local file system (`file://`), or HDFS (`hdfs://`) can be read into a Spark RDD.
    These examples show that the data will be partitioned into six parts for increased
    performance. The first two examples are the same, as they both manipulate a file
    on the Linux file system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Processing the JSON files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'JSON is a data interchange format, developed from Javascript. **JSON** actually
    stands for **JavaScript** **Object** **Notation**. It is a text-based format,
    and can be expressed, for instance, as XML. The following example uses the SQL
    context method called `jsonFile` to load the HDFS-based JSON data file named `device.json`.
    The resulting data is created as a data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Data can be saved in JSON format using the data frame `toJSON` method, as shown
    by the following example. First, the Apache Spark and Spark SQL classes are imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the object class called `sql1` is defined as is a main method with parameters.
    A configuration object is defined that is used to create a spark context. The
    master Spark URL is left as the default value, so Spark expects local mode, the
    local host, and the `7077` port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'An SQL context is created from the Spark context, and a raw text file is loaded
    in CSV format called `adult.test.data_1x`, using the `textFile` method. A schema
    string is then created, which contains the data column names and the schema created
    from it by splitting the string by its spacing, and using the `StructType` and
    `StructField` methods to define each schema column as a string value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Each data row is then created from the raw CSV data by splitting it with the
    help of a comma as a line divider, and then the elements are added to a `Row()`
    structure. A data frame is created from the schema, and the row data which is
    then converted into JSON format using the `toJSON` method. Finally, the data is
    saved to HDFS using the `saveAsTextFile` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: So the resulting data can be seen on HDFS, the Hadoop file system `ls` command
    below shows that the data resides in the `target` directory as a success file
    and two part files.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the Hadoop file system''s `cat` command, it is possible to display the
    contents of the JSON data. I will just show a sample to save space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Processing the Parquet data is very similar, as I will show next.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the Parquet files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Parquet is another columnar-based data format used by many tools in the
    Hadoop tool set for file I/O, such as Hive, Pig, and Impala. It increases performance
    by using efficient compression and encoding routines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Parquet processing example is very similar to the JSON Scala code. The
    DataFrame is created, and then saved in a Parquet format using the save method
    with a type of Parquet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in an HDFS-based directory, which contains three Parquet-based
    files: a common Metadata file, a Metadata file, and a temporary file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing the contents of the metadata file, using the Hadoop file system''s
    `cat` command, gives an idea of the data format. However the Parquet header is
    binary, and so, it does not display with `more` and `cat`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For more information about possible Spark and SQL context methods, check the
    contents of the classes called `org.apache.spark.SparkContext`, and `org.apache.spark.sql.SQLContext`,
    using the Apache Spark API path here for the specific `<version>` of Spark that
    you are interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, I will examine Apache Spark DataFrames, introduced in Spark
    1.3.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I have already mentioned that a DataFrame is based on a columnar format. Temporary
    tables can be created from it, but I will expand on this in the next section.
    There are many methods available to the data frame that allow data manipulation,
    and processing. I have based the Scala code used here, on the code in the last
    section, so I will just show you the working lines and the output. It is possible
    to display a data frame schema as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It is possible to use the `select` method to filter columns from the data.
    I have limited the output here, in terms of rows, but you get the idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'It is possible to filter the data returned from the DataFrame using the `filter`
    method. Here, I have added the occupation column to the output, and filtered on
    the worker age:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'There is also a `group by` method for determining volume counts within a data
    set. As this is an income-based dataset, I think that volumes within the wage
    brackets would be interesting. I have also used a bigger dataset to give more
    meaningful results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This is interesting, but what if I want to compare `income` brackets with `occupation`,
    and sort the results for a better understanding? The following example shows how
    this can be done, and gives the example data volumes. It shows that there is a
    high volume of managerial roles compared to other occupations. This example also
    sorts the output by the occupation column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: So, SQL-like actions can be carried out against DataFrames, including `select`,
    `filter`, sort `group by`, and `print`. The next section shows how tables can
    be created from the DataFrames, and how the SQL-based actions are carried out
    against them.
  prefs: []
  type: TYPE_NORMAL
- en: Using SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After using the previous Scala example to create a data frame, from a CSV based-data
    input file on HDFS, I can now define a temporary table, based on the data frame,
    and run SQL against it. The following example shows the temporary table called
    `adult` being defined, and a row count being created using `COUNT(*)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives a row count of over 32,000 rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to limit the volume of the data selected from the table
    using the `LIMIT` SQL option, which is shown in the following example. The first
    10 rows have been selected from the data, this is useful if I just want to check
    data types and quality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'A sample of the data looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'When the schema for this data was created in the Scala-based data frame example
    in the last section, all the columns were created as strings. However, if I want
    to filter the data in SQL using `WHERE` clauses, it would be useful to have proper
    data types. For instance, if an age column stores integer values, it should be
    stored as an integer so that I can execute numeric comparisons against it. I have
    changed my Scala code to include all the possible types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'I have also now defined my schema using different types, to better match the
    data, and I have defined the row data in terms of the actual data types, converting
    raw data string values into integer values, where necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The SQL can now use numeric filters in the `WHERE` clause correctly. If the
    `age` column were a string, this would not work. You can now see that the data
    has been filtered to give age values below 60 years:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives a row count of around 30,000 rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'It is possible to use Boolean logic in the `WHERE`-based filter clauses. The
    following example specifies an age range for the data. Note that I have used variables
    to describe the `select` and `filter` components of the SQL statement. This allows
    me to break down the statement into different parts as they become larger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Giving a data count of around 23,000 rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'I can create compound filter clauses using the Boolean terms, such as `AND`,
    `OR`, as well as parentheses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives me a row count of 17,000 rows, and represents a count of two age
    ranges in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to use subqueries in Apache Spark SQL. You can see in the
    following example that I have created a subquery called `t1` by selecting three
    columns; `age`, `education`, and `occupation` from the table `adult`. I have then
    used the table called `t1` to create a row count. I have also added a filter clause
    acting on the age column from the table `t1`. Notice also that I have added `group
    by` and `order by` clauses, even though they are empty currently, to my SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to examine the table joins, I have created a version of the adult
    CSV data file called `adult.train.data2`, which only differs from the original
    by the fact that it has an added first column called `idx`, which is a unique
    index. The Hadoop file system''s `cat` command here shows a sample of the data.
    The output from the file has been limited using the Linux `head` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The schema has now been redefined to have an integer-based first column called
    `idx` for an index, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'And the raw row RDD in the Scala example now processes the new initial column,
    and converts the string value into an integer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We have looked at subqueries. Now, I would like to consider table joins. The
    next example will use the index that was just created. It uses it to join two
    derived tables. The example is somewhat contrived, given that it joins two data
    sets from the same underlying table, but you get the idea. Two derived tables
    are created as subqueries, and are joined at a common index column.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SQL for a table join now looks like this. Two derived tables have been
    created from the temporary table `adult` called `t1` and `t2` as subqueries. The
    new row index column called `idx` has been used to join the data in tables `t1`
    and `t2`. The major `SELECT` statement outputs all seven columns from the compound
    data set. I have added a `LIMIT` clause to minimize the data output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that in the major `SELECT` statement, I have to define where the index
    column comes from, so I use `t1.idx`. All the other columns are unique to the
    `t1` and `t2` datasets, so I don''t need to use an alias to refer to them (that
    is, `t1.age`). So, the data that is output now looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This gives some idea of the SQL-based functionality within Apache Spark, but
    what if I find that the method that I need is not available? Perhaps, I need a
    new function. This is where the **user-defined functions** (**UDFs**) are useful.
    I will cover them in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: User-defined functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to create some user-defined functions in Scala, I need to examine my
    data in the previous adult dataset. I plan to create a UDF that will enumerate
    the education column, so that I can convert the column into an integer value.
    This will be useful if I need to use the data for machine learning, and so create
    a LabelPoint structure. The vector used, which represents each record, will need
    to be numeric. I will first determine what kind of unique education values exist,
    then I will create a function to enumerate them, and finally use it in SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have created some Scala code to display a sorted list of the education values.
    The `DISTINCT` keyword ensures that there is only one instance of each value.
    I have selected the data as a subtable, using an alias called `edu_dist` for the
    data column to ensure that the `ORDER BY` clause works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The data looks like the following. I have removed some values to save space,
    but you get the idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'I have defined a method in Scala to accept the string-based education value,
    and return an enumerated integer value that represents it. If no value is recognized,
    then a special value called `9999` is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'I can now register this function using the SQL context in Scala, so that it
    can be used in an SQL statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The SQL, and the Scala code to enumerate the data then look like this. The
    newly registered function called `enumEdu` is used in the `SELECT` statement.
    It takes the education type as a parameter, and returns the integer enumeration.
    The column that this value forms is aliased to the name `idx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting data output, as a list of education values and their enumerations,
    looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Another example function called `ageBracket` takes the adult integer age value,
    and returns an enumerated age bracket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, the function is registered using the SQL context so that it can be used
    in an SQL statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the Scala-based SQL uses it to select the age, age bracket, and education
    value from the adult dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting data then looks like this, given that I have used the `LIMIT`
    clause to limit the output to 10 rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to define functions for use in SQL, inline, during the
    UDF registration using the SQL context. The following example defines a function
    called `dblAge`, which just multiplies the adult''s age by two. The registration
    looks like this. It takes integer parameters (`age`), and returns twice its value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'And the SQL that uses it, now selects the `age`, and the double of the `age`
    value called `dblAge(age)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The two columns of the output data, which now contain the age and its doubled
    value, now look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: So far, DataFrames, SQL, and user-defined functions have been examined, but
    what if, as in my case, you are using a Hadoop stack cluster, and have Apache
    Hive available? The adult table that I have defined so far is a temporary table,
    but if I access Hive using Apache Spark SQL, I can access the static database
    tables. The next section will examine the steps needed to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Using Hive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have a business intelligence-type workload with low latency requirements
    and multiple users, then you might consider using Impala for your database access.
    Apache Spark on Hive is for batch processing and ETL chains. This section will
    be used to show how to connect Spark to Hive, and how to use this configuration.
    First, I will develop an application that uses a local Hive Metastore, and show
    that it does not store and persist table data in Hive itself. I will then set
    up Apache Spark to connect to the Hive Metastore server, and store tables and
    data within Hive. I will start with the local Metastore server.
  prefs: []
  type: TYPE_NORMAL
- en: Local Hive Metastore server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following example Scala code shows how to create a Hive context, and create
    a Hive-based table using Apache Spark. First, the Spark configuration, context,
    SQL, and Hive classes are imported. Then, an object class called `hive_ex1`, and
    the main method are defined. The application name is defined, and a Spark configuration
    object is created. The Spark context is then created from the configuration object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, I create a new Hive context from the Spark context, and import the Hive
    implicits, and the Hive context SQL. The `implicits` allow for implicit conversions,
    and the SQL include allows me to run Hive context-based SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The next statement creates an empty table called `adult2` in Hive. You will
    recognize the schema from the adult data that has already been used in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, a row count is taken from the table called `adult2` via a `COUNT(*)`,
    and the output value is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: As expected, there are no rows in the table.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to create Hive-based external tables in Apache Spark Hive.
    The following HDFS file listing shows that the CSV file called `adult.train.data2`
    exists in the HDFS directory called `/data/spark/hive`, and it contains data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, I adjust my Scala-based Hive SQL to create an external table called `adult3`
    (if it does not exist), which has the same structure as the previous table. The
    row format in this table-create statement specifies a comma as a row column delimiter,
    as would be expected for CSV data. The location option in this statement specifies
    the `/data/spark/hive` directory on HDFS for data. So, there can be multiple files
    on HDFS, in this location, to populate this table. Each file would need to have
    the same data structure matching this table structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'A row count is then taken against the `adult3` table, and the count is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the table now contains around 32,000 rows. Since this is an
    external table, the HDFS-based data has not been moved, and the row calculation
    has been derived from the underlying CSV-based data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'It occurs to me that I want to start stripping dimension data out of the raw
    CSV-based data in the external `adult3` table. After all, Hive is a data warehouse,
    so a part of a general ETL chain using the raw CSV-based data would strip dimensions
    and objects from the data, and create new tables. If I consider the education
    dimension, and try to determine what unique values exist, then for instance, the
    SQL would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'And the ordered data matches the values that were derived earlier in this chapter
    using Spark SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'This is useful, but what if I want to create dimension values, and then assign
    integer index values to each of the previous education dimension values. For instance,
    `10th` would be `0`, and `11th` would be `1`. I have set up a dimension CSV file
    for the education dimension on HDFS, as shown here. The contents just contain
    the list of unique values, and an index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, I can run some Hive QL in my Apache application to create an education
    dimension table. First, I drop the education table if it already exists, then
    I create the table by parsing the HDFS CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: I can then select the contents of the new education table to ensure that it
    looks correct.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the expected list of indexes and the education dimension values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: So, I have the beginnings of an ETL pipeline. The raw CSV data is being used
    as external tables, and the dimension tables are being created, which could then
    be used to convert the dimensions in the raw data to numeric indexes. I have now
    successfully created a Spark application, which uses a Hive context to connect
    to a Hive Metastore server, which allows me to create and populate tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have the Hadoop stack Cloudera CDH 5.3 installed on my Linux servers. I am
    using it for HDFS access while writing this book, and I also have Hive and Hue
    installed and running (CDH install information can be found at the Cloudera website
    at [http://cloudera.com/content/cloudera/en/documentation.html](http://cloudera.com/content/cloudera/en/documentation.html)).
    When I check HDFS for the `adult3` table, which should have been created under
    `/user/hive/warehouse`, I see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The Hive-based table does not exist in the expected place for Hive. I can confirm
    this by checking the Hue Metastore manager to see what tables exist in the default
    database. The following figure shows that my default database is currently empty.
    I have added red lines to show that I am currently looking at the default database,
    and that there is no data. Clearly, when I run an Apache Spark-based application,
    with a Hive context, I am connecting to a Hive Metastore server. I know this because
    the log indicates that this is the case and also, my tables created in this way
    persist when Apache Spark is restarted.
  prefs: []
  type: TYPE_NORMAL
- en: '![Local Hive Metastore server](img/B01989_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Hive context within the application that was just run has used a local Hive
    Metastore server, and has stored data to a local location; actually in this case
    under `/tmp` on HDFS. I now want to use the Hive-based Metastore server, so that
    I can create tables and data in Hive directly. The next section will show how
    this can be done.
  prefs: []
  type: TYPE_NORMAL
- en: A Hive-based Metastore server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I already mentioned that I am using Cloudera's CDH 5.3 Hadoop stack. I have
    Hive, HDFS, Hue, and Zookeeper running. I am using Apache Spark 1.3.1 installed
    under `/usr/local/spark`, in order to create and run applications (I know that
    CDH 5.3 is released with Spark 1.2, but I wanted to use DataFrames in this instance,
    which were available in Spark 1.3.x.).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that I need to do to configure Apache Spark to connect to Hive,
    is to drop the Hive configuration file called `hive-site.xml` into the Spark configuration
    directory on all servers where Spark is installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, given that I have installed Apache Hive via the CDH Manager to be able
    to use PostgreSQL, I need to install a PostgreSQL connector JAR for Spark, else
    it won''t know how to connect to Hive, and errors like this will occur:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'I have stripped that error message down to just the pertinent parts, otherwise
    it would have been many pages long. I have determined the version of PostgreSQL
    that I have installed, as follows. It appears to be of version 9.0, determined
    from the Cloudera parcel-based jar file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, I have used the [https://jdbc.postgresql.org/](https://jdbc.postgresql.org/)
    website to download the necessary PostgreSQL connector library. I have determined
    my Java version to be 1.7, as shown here, which affects which version of library
    to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The site says that if you are using Java 1.7 or 1.8, then you should use the
    JDBC41 version of the library. So, I have sourced the `postgresql-9.4-1201.jdbc41.jar`
    file. The next step is to copy this file to the Apache Spark install `lib` directory,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the PostgreSQL library must be added to the Spark `CLASSPATH`, by adding
    an entry to the file called `compute-classpath.sh`, in the Spark `bin` directory,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'In my case, I encountered an error regarding Hive versions between CDH 5.3
    Hive and Apache Spark as shown here. I thought that the versions were so close
    that I should be able to ignore this error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'I decided, in this case, to switch off schema verification in my Spark version
    of the `hive-site.xml` file. This had to be done in all the Spark-based instances
    of this file, and then Spark restarted. The change is shown here; the value is
    set to `false`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when I run the same set of application-based SQL as the last section,
    I can create objects in the Apache Hive default database. First, I will create
    the empty table called `adult2` using the Spark-based Hive context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, when I run the application and check the Hue Metastore browser,
    the table `adult2` now exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A Hive-based Metastore server](img/B01989_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'I have shown the table entry previously, and it''s structure is obtained by
    selecting the table entry called `adult2`, in the Hue default database browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A Hive-based Metastore server](img/B01989_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now the external table `adult3` Spark based Hive QL can be executed and data
    access confirmed from Hue. In the last section, the necessary Hive QL was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can now see, the Hive-based table called `adult3` has been created in
    the default database by Spark. The following figure is again generated from the
    Hue Metastore browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A Hive-based Metastore server](img/B01989_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following Hive QL has been executed from the Hue Hive query editor. It
    shows that the `adult3` table is accessible from Hive. I have limited the rows
    to make the image presentable. I am not worried about the data, only the fact
    that I can access it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A Hive-based Metastore server](img/B01989_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The last thing that I will mention in this section which will be useful when
    using Hive QL from Spark against Hive, will be user-defined functions or UDF''s.
    As an example, I will consider the `row_sequence` function, which is used in the
    following Scala-based code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Either existing, or your own, JAR-based libraries can be made available to your
    Spark Hive session via the `ADD JAR` command. Then, the functionality within that
    library can be registered as a temporary function with `CREATE TEMPORARY FUNCTION`
    using the package-based class name. Then, the new function name can be incorporated
    in Hive QL statements.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has managed to connect an Apache Spark-based application to Hive,
    and run Hive QL against Hive, so that table and data changes persist in Hive.
    But why is this important? Well, Spark is an in-memory parallel processing system.
    It is an order faster than Hadoop-based Map Reduce in processing speed. Apache
    Spark can now be used as a processing engine, whereas the Hive data warehouse
    can be used for storage. Fast in-memory Spark-based processing speed coupled with
    big data scale structured data warehouse storage available in Hive.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter started by explaining the Spark SQL context, and file I/O methods.
    It then showed that Spark and HDFS-based data could be manipulated, as both DataFrames
    with SQL-like methods and with Spark SQL by registering temporary tables. Next,
    user-defined functions were introduced to show that the functionality of Spark
    SQL could be extended by creating new functions to suit your needs, registering
    them as UDF's, and then calling them in SQL to process data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the Hive context was introduced for use in Apache Spark. Remember that
    the Hive context in Spark offers a super set of the functionality of the SQL context.
    I understand that over time, the SQL context is going to be extended to match
    the Hive Context functionality. Hive QL data processing in Spark using a Hive
    context was shown using both, a local Hive, and a Hive-based Metastore server.
    I believe that the latter configuration is better, as the tables are created,
    and data changes persist in your Hive instance.
  prefs: []
  type: TYPE_NORMAL
- en: In my case, I used Cloudera CDH 5.3, which used Hive 0.13, PostgreSQL, ZooKeeper,
    and Hue. I also used Apache Spark version 1.3.1\. The configuration setup that
    I have shown you is purely for this configuration. If you wanted to use MySQL,
    for instance, you would need to research the necessary changes. A good place to
    start would be the `<[user@spark.apache.org](mailto:user@spark.apache.org)>` mailing
    list.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I would say that Apache Spark Hive context configuration, with Hive-based
    storage, is very useful. It allows you to use Hive as a big data scale data warehouse,
    with Apache Spark for fast in-memory processing. It offers you the ability to
    manipulate your data with not only the Spark-based modules (MLlib, SQL, GraphX,
    and Stream), but also other Hadoop-based tools, making it easier to create ETL
    chains.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will examine the Spark graph processing module, GraphX, it
    will also investigate the Neo4J graph database, and the MazeRunner application.
  prefs: []
  type: TYPE_NORMAL
