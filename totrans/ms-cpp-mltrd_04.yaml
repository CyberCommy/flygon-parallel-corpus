- en: Thread Synchronization and Communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While, generally, threads are used to work on a task more or less independently
    from other threads, there are many occasions where one would want to pass data
    between threads, or even control other threads, such as from a central task scheduler
    thread. This chapter looks at how such tasks are accomplished with the C++11 threading
    API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Topics covered in this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Using mutexes, locks, and similar synchronization structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using condition variables and signals to control threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Safely passing and sharing data between threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Safety first
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The central problem with concurrency is that of ensuring safe access to shared
    resources even when communicating between threads. There is also the issue of
    threads being able to communicate and synchronize themselves.
  prefs: []
  type: TYPE_NORMAL
- en: What makes multithreaded programming such a challenge is to be able to keep
    track of each interaction between threads, and to ensure that each and every form
    of access is secured while not falling into the trap of deadlocks and data races.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at a fairly complex example involving a task scheduler.
    This is a form of high-concurrency, high-throughput situation where many different
    requirements come together with many potential traps, as we will see in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: The scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A good example of multithreading with a significant amount of synchronization
    and communication between threads is the scheduling of tasks. Here, the goal is
    to accept incoming tasks and assign them to work threads as quickly as possible.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, a number of different approaches are possible. Often one has
    worker threads running in an active loop, constantly polling a central queue for
    new tasks. Disadvantages of this approach include wasting of processor cycles
    on the said polling, and the congestion which forms at the synchronization mechanism
    used, generally a mutex. Furthermore, this active polling approach scales very
    poorly when the number of worker threads increase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, each worker thread would wait idly until it is needed again. To accomplish
    this, we have to approach the problem from the other side: not from the perspective
    of the worker threads, but from that of the queue. Much like the scheduler of
    an operating system, it is the scheduler which is aware of both the tasks which
    require processing as well as the available worker threads.'
  prefs: []
  type: TYPE_NORMAL
- en: In this approach, a central scheduler instance would accept new tasks and actively
    assign them to worker threads. The said scheduler instance may also manage these
    worker threads, such as their number and priority, depending on the number of
    incoming tasks and the type of task or other properties.
  prefs: []
  type: TYPE_NORMAL
- en: High-level view
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At its core, our scheduler or dispatcher is quite simple, functioning like
    a queue with all of the scheduling logic built into it, as seen in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00016.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As one can see from the preceding high-level view, there really isn't much to
    it. However, as we'll see in a moment, the actual implementation does have a number
    of complications.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As is usual, we start off with the `main` function, contained in `main.cpp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The custom headers we include are those for our dispatcher implementation, as
    well as the `request` class that we'll use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Globally, we define an atomic variable to be used with the signal handler,
    as well as a mutex which will synchronize the output (on the standard output)
    from our logging method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Our signal handler function (for `SIGINT` signals) simply sets the global atomic
    variable that we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In our logging function, we use the global mutex to ensure that writing to
    the standard output is synchronized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `main` function, we install the signal handler for `SIGINT` to allow
    us to interrupt the execution of the application. We also call the static `init()`
    function on the `Dispatcher` class to initialize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, we set up the loop in which we will create new requests. In each cycle,
    we create a new `Request` instance, and use its `setValue()` function to set an
    integer value (current cycle number). We also set our logging function on the
    request instance before adding this new request to `Dispatcher` using its static
    `addRequest()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This loop will continue until the maximum number of cycles have been reached,
    or `SIGINT` has been signaled using *Ctrl*+*C* or similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we wait for 5 seconds using the thread's `sleep_for()` function, and
    the `chrono::seconds()` function from the `chrono` STL header.
  prefs: []
  type: TYPE_NORMAL
- en: We also call the `stop()` function on `Dispatcher` before returning.
  prefs: []
  type: TYPE_NORMAL
- en: Request class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A request for `Dispatcher` always derives from the pure virtual `AbstractRequest`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This `AbstractRequest` class defines an API with three functions, which a deriving
    class always has to implement. Out of these, the `process()` and `finish()` functions
    are the most generic and likely to be used in any practical implementation. The
    `setValue()` function is specific to this demonstration implementation, and would
    likely be adapted or extended to fit a real-life scenario.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using an abstract class as the basis for a request is that
    it allows the `Dispatcher` class to handle many different types of requests as
    long as they all adhere to this same basic API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this abstract interface, we implement a basic `Request` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In its header file, we first define the function pointer's format. After this,
    we implement the request API, and add the `setOutput()` function to the base API,
    which accepts a function pointer for logging. Both setter functions merely assign
    the provided parameter to their respective private class members.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the class function implementations are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Both of these implementations are very basic; they merely use the function pointer
    to output a string indicating the status of the worker thread.
  prefs: []
  type: TYPE_NORMAL
- en: In a practical implementation, one would add the business logic to the `process()`
    function with the `finish()` function containing any functionality to finish up
    a request such as writing a map into a string.
  prefs: []
  type: TYPE_NORMAL
- en: Worker class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next is the `Worker` class. This contains the logic which will be called by
    `Dispatcher` in order to process a request.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Whereas the adding of a request to `Dispatcher` does not require any special
    logic, the `Worker` class does require the use of condition variables to synchronize
    itself with the dispatcher. For the C++11 threads API, this requires a condition
    variable, a mutex, and a unique lock.
  prefs: []
  type: TYPE_NORMAL
- en: The unique lock encapsulates the mutex, and will ultimately be used with the
    condition variable as we will see in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond this, we define methods to start and stop the worker, to set a new request
    for processing, and to obtain access to its internal condition variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on, the rest of the implementation is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Beyond the `getter` function for the condition variable, we define the `run()`
    function, which `dispatcher` will run for each worker thread upon starting it.
  prefs: []
  type: TYPE_NORMAL
- en: Its main loop merely checks that the `stop()` function hasn't been called yet,
    which would have set the running Boolean value to `false`, and ended the work
    thread. This is used by `Dispatcher` when shutting down, allowing it to terminate
    the worker threads. Since Boolean values are generally atomic, setting and checking
    can be done simultaneously without risk or requiring a mutex.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, the check of the `ready` variable is to ensure that a request is
    actually waiting when the thread is first run. On the first run of the worker
    thread, no request will be waiting, and thus, attempting to process one would
    result in a crash. Upon `Dispatcher` setting a new request, this Boolean variable
    will be set to `true`.
  prefs: []
  type: TYPE_NORMAL
- en: If a request is waiting, the `ready` variable will be set to `false` again,
    after which the request instance will have its `process()` and `finish()` functions
    called. This will run the business logic of the request on the worker thread's
    thread, and finalize it.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the worker thread adds itself to the dispatcher using its static `addWorker()`
    function. This function will return `false` if no new request is available, and
    cause the worker thread to wait until a new request has become available. Otherwise,
    the worker thread will continue with the processing of the new request that `Dispatcher`
    will have set on it.
  prefs: []
  type: TYPE_NORMAL
- en: If asked to wait, we enter a new loop. This loop will ensure that when the condition
    variable is woken up, it is because we got signaled by `Dispatcher` (`ready` variable
    set to `true`), and not because of a spurious wake-up.
  prefs: []
  type: TYPE_NORMAL
- en: Last of all, we enter the actual `wait()` function of the condition variable
    using the unique lock instance we created before along with a timeout. If a timeout
    occurs, we can either terminate the thread, or keep waiting. Here, we choose to
    do nothing and just re-enter the waiting loop.
  prefs: []
  type: TYPE_NORMAL
- en: Dispatcher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the last item, we have the `Dispatcher` class itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Most of this will look familiar. As you will have surmised by now, this is a
    fully static class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on, its implementation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After setting up the static class members, the `init()` function is defined.
    It starts the specified number of worker threads keeping a reference to each worker
    and thread instance in their respective vector data structures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the `stop()` function, each worker instance has its `stop()` function called.
    This will cause each worker thread to terminate, as we saw earlier in the `Worker`
    class description.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we wait for each thread to join (that is, finish) prior to returning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `addRequest()` function is where things get interesting. In this function,
    a new request is added. What happens next depends on whether a worker thread is
    waiting for a new request or not. If no worker thread is waiting (worker queue
    is empty), the request is added to the request queue.
  prefs: []
  type: TYPE_NORMAL
- en: The use of mutexes ensures that the access to these queues occurs safely, as
    the worker threads will simultaneously try to access both queues as well.
  prefs: []
  type: TYPE_NORMAL
- en: An important `gotcha` to note here is the possibility of a deadlock. That is,
    a situation where two threads will hold the lock on a resource, with the second
    thread waiting for the first one to release its lock before releasing its own.
    Every situation where more than one mutex is used in a single scope holds this
    potential.
  prefs: []
  type: TYPE_NORMAL
- en: In this function, the potential for a deadlock lies in releasing of the lock
    on the workers mutex, and when the lock on the requests mutex is obtained. In
    the case that this function holds the workers mutex and tries to obtain the requests
    lock (when no worker thread is available), there is a chance that another thread
    holds the requests mutex (looking for new requests to handle) while simultaneously
    trying to obtain the workers mutex (finding no requests and adding itself to the
    workers queue).
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution here is simple: release a mutex before obtaining the next one.
    In the situation where one feels that more than one mutex lock has to be held,
    it is paramount to examine and test one''s code for potential deadlocks. In this
    particular situation, the workers mutex lock is explicitly released when it is
    no longer needed, or before the requests mutex lock is obtained, thus preventing
    a deadlock.'
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect of this particular section of code is the way it signals
    a worker thread. As one can see in the first section of the if/else block, when
    the workers queue is not empty, a worker is fetched from the queue, has the request
    set on it, and then has its condition variable referenced and signaled, or notified.
  prefs: []
  type: TYPE_NORMAL
- en: Internally, the condition variable uses the mutex we handed it before in the
    `Worker` class definition to guarantee only atomic access to it. When the `notify_one()`
    function (generally called `signal()` in other APIs) is called on the condition
    variable, it will notify the first thread in the queue of threads waiting for
    the condition variable to return and continue.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `Worker` class `run()` function, we would be waiting for this notification
    event. Upon receiving it, the worker thread would continue and process the new
    request. The thread reference will then be removed from the queue until it adds
    itself again once it is done processing the request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: With this last function, a worker thread will add itself to the queue once it
    is done processing a request. It is similar to the earlier function in that the
    incoming worker is first actively matched with any request which may be waiting
    in the request queue. If none are available, the worker is added to the worker
    queue.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note here that we return a Boolean value which indicates
    whether the calling thread should wait for a new request, or whether it already
    has received a new request while trying to add itself to the queue.
  prefs: []
  type: TYPE_NORMAL
- en: While this code is less complex than that of the previous function, it still
    holds the same potential deadlock issue due to the handling of two mutexes within
    the same scope. Here, too, we first release the mutex we hold before obtaining
    the next one.
  prefs: []
  type: TYPE_NORMAL
- en: Makefile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The makefile for this `Dispatcher` example is very basic again--it gathers
    all C++ source files in the current folder, and compiles them into a binary using
    `g++`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After compiling the application, running it produces the following output for
    the 50 total requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we can already clearly see that even with each request taking
    almost no time to process, the requests are clearly being executed in parallel.
    The first request (request 0) only starts being processed after the sixteenth
    request, while the second request already finishes after the ninth request, long
    before this.
  prefs: []
  type: TYPE_NORMAL
- en: The factors which determine which thread, and thus, which request is processed
    first depends on the OS scheduler and hardware-based scheduling as described in
    [chapter 2](part0026.html#OPEK0-1ab5991b318547348fc444437bdacb24), *Multithreading
    Implementation on the Processor and OS*. This clearly shows just how few assumptions
    can be made about how a multithreaded application will be executed even on a single
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the fourth and fifth requests also finish in a rather
    delayed fashion.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: At this point, the first request finally finishes. This may indicate that the
    initialization time for the first request will always be delayed as compared to
    the successive requests. Running the application multiple times can confirm this.
    It's important that if the order of processing is relevant, this randomness does
    not negatively affect one's application.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Request 19 also became fairly delayed, showing once again just how unpredictable
    a multithreaded application can be. If we were processing a large dataset in parallel
    here, with chunks of data in each request, we might have to pause at some points
    to account for these delays, as otherwise, our output cache might grow too large.
  prefs: []
  type: TYPE_NORMAL
- en: As doing so would negatively affect an application's performance, one might
    have to look at low-level optimizations, as well as the scheduling of threads
    on specific processor cores in order to prevent this from happening.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: All 10 worker threads which were launched in the beginning terminate here as
    we call the `stop()` function of the `Dispatcher`.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the example given in this chapter, we saw how to share information between
    threads in addition to synchronizing threads--this in the form of the requests
    we passed from the main thread into the dispatcher from which each request gets
    passed on to a different thread.
  prefs: []
  type: TYPE_NORMAL
- en: The essential idea behind the sharing of data between threads is that the data
    to be shared exists somewhere in a way which is accessible to two threads or more.
    After this, we have to ensure that only one thread can modify the data, and that
    the data does not get modified while it's being read. Generally, we would use
    mutexes or similar to ensure this.
  prefs: []
  type: TYPE_NORMAL
- en: Using r/w-locks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read-write locks are a possible optimization here, because they allow multiple
    threads to read simultaneously from a single data source. If one has an application
    in which multiple worker threads read the same information repeatedly, it would
    be more efficient to use read-write locks than basic mutexes, because the attempts
    to read the data will not block the other threads.
  prefs: []
  type: TYPE_NORMAL
- en: A read-write lock can thus be used as a more advanced version of a mutex, namely,
    as one which adapts its behavior to the type of access. Internally, it builds
    on mutexes (or semaphores) and condition variables.
  prefs: []
  type: TYPE_NORMAL
- en: Using shared pointers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First available via the Boost library and introduced natively with C++11, shared
    pointers are an abstraction of memory management using reference counting for
    heap-allocated instances. They are partially thread-safe in that creating multiple
    shared pointer instances can be created, but the referenced object itself is not
    thread-safe.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the application, this may suffice, however. To make them properly
    thread-safe, one can use atomics. We will look at this in more detail in [Chapter
    8](part0169.html#515F20-1ab5991b318547348fc444437bdacb24), *Atomic Operations
    - Working with the Hardware*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to pass data between threads in a safe manner
    as part of a fairly complex scheduler implementation. We also looked at the resulting
    asynchronous processing of the said scheduler, and considered some potential alternatives
    and optimizations for passing data between threads.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should be able to safely pass data between threads, as well
    as synchronize access to other shared resources.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at native C++ threading and the primitives
    API.
  prefs: []
  type: TYPE_NORMAL
