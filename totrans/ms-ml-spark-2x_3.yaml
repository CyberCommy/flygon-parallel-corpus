- en: Ensemble Methods for Multi-Class Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our modern world is already interconnected with many devices for collecting
    data about human behavior - for example, our cell phones are small spies in our
    pockets tracking number of steps, route, or our eating habits. Even the watches
    that we wear now can track everything from the number of steps we take to our
    heart rate at any given moment in time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In all these situations, the gadgets try to guess what the user is doing based
    on collected data to provide reports of the user''s activities through the day.
    From a machine learning perspective, the task can be viewed as a classification
    problem: detecting patterns in collected data and assigning the right activity
    category to them (that is, swimming, running, sleeping). But importantly, it is
    still supervised problem - that means to train a model, we need to provide observations
    annotated by actual categories.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are going to focus on ensemble methods for modeling problems
    of multi-class classification - sometimes referred to as multinomial classification
    - using a sensor dataset provided by the UCI dataset library.
  prefs: []
  type: TYPE_NORMAL
- en: Note that multi-class classification should not be confused with multi-label
    classification whereby multiple labels can be predicted for a given example. For
    example, a blog post can be tagged with multiple labels as one blog can encompass
    any number of topics; however, in multi-class classification, we are *forced* to
    choose one out of *N* possible topics where *N >* 2 possible labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reader is going to learn in this chapter about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing data for multi-class classification, including handling missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-class classification using the Spark RF algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the quality of Spark classification models using different measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building H2O tree-based classification models and exploring their quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to use **Physical Activity Monitoring Data Set**
    (**PAMAP2**) published in the Machine Learning Repository by the University of
    Irvine: [https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring](https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full dataset contains **52** input features and **3,850,505** events describing
    18 different physical activities (for example, walking, cycling, running, `watching
    TV`). The data was recorded by a heart rate monitor and three inertial measurement
    units located on the wrist, chest, and dominant side''s ankle. Each event is annotated
    by an activity label describing the ground truth and also a timestamp. The dataset
    contains missing values indicated by the value `NaN`. Furthermore, some columns
    produced by sensors are marked as invalid ("orientation" - see dataset description):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Properties of dataset as published in the Machine Learning Repository
    of the University of Irvine.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset represents the perfect example for activity recognition: we would
    like to train a robust model which would be able to predict a performed activity
    based on incoming data from physical sensors.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the dataset is spread over multiple files, each file representing
    measurements of a single subject, which is another real-life aspect of data produced
    by multiple data sources so we will need to utilize Spark's ability to read from
    a directory and merge the files to make training/test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lines show a sample of the data. There are a couple of important
    observations that are worth noting:'
  prefs: []
  type: TYPE_NORMAL
- en: Individual values are separated by an empty space character
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first value in each row represents a timestamp, while the second value holds
    the `activityId`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `activityId` is represented by a numeric value; hence, we need a translation
    table to transform an ID to a corresponding activity label which the dataset gives
    and we show as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 1 lying | 2 sitting |'
  prefs: []
  type: TYPE_TB
- en: '| 3 standing | 4 walking |'
  prefs: []
  type: TYPE_TB
- en: '| 5 running | 6 cycling |'
  prefs: []
  type: TYPE_TB
- en: '| 7 Nordic walking | 9 watching TV |'
  prefs: []
  type: TYPE_TB
- en: '| 10 computer work | 11 car driving |'
  prefs: []
  type: TYPE_TB
- en: '| 12 ascending stairs | 13 descending stairs |'
  prefs: []
  type: TYPE_TB
- en: '| 16 vacuum cleaning | 17 ironing |'
  prefs: []
  type: TYPE_TB
- en: '| 18 folding laundry | 19 house cleaning |'
  prefs: []
  type: TYPE_TB
- en: '| 20 playing soccer | 24 rope jumping |'
  prefs: []
  type: TYPE_TB
- en: '| 0 other (transient activities) |  |'
  prefs: []
  type: TYPE_TB
- en: The example lines represent one "other activity" and then two measurements representing
    "car driving".
  prefs: []
  type: TYPE_NORMAL
- en: 'The third column contains heart rate measurements, while the rest of the columns
    represent data from three different inertia measurements units: columns 4-20 are
    from the hand sensor, 21-37 contain data from chest sensor and finally the columns
    38-54 hold ankle sensor measurements. Each sensor measures 17 different values
    including temperature, 3-D acceleration, gyroscope and magnetometer data, and
    orientation. However, the orientation columns are marked as invalid in this dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: The input data pack contains two different folders - protocol, and optional
    measurements which contains data from a few subjects who performed some additional
    activities. In this chapter, we are going to use only data from optional folder.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling goal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we would like to build a model based on information about physical
    activities to classify unseen data and annotate it with the corresponding physical
    activity.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the sensor data, there are numerous way to explore and build models. In
    this chapter, we mainly focus on classification; however, there are several aspects
    which would need deeper exploration, especially the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Training data represents a time-ordered flow of events, but we are not going
    to reflect the time information but look at the data as one complete piece of
    information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same for test data -a single activity event is a part of an event stream
    captured during performing an activity and it can be easier to categorize it with
    knowledge of actual context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nevertheless, for now, we ignore the time dimension and apply classification
    to explore possible patterns in the sensor data which would characterize performed
    activities.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build an initial model, our workflow includes several steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data load and preprocessing, often referenced as **extract-transform-load**
    (**ETL**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handle missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unify data into a form expected by an algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting Spark shell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step is to prepare the Spark environment to perform analysis. As
    in the previous chapter, we are going to start Spark shell; however, in this case,
    the command line is slightly more complicated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we require more memory since we are going to load larger data.
    We also need to increase the size of PermGen - a part of JVM memory which stores
    information about loaded classes. This is only necessary if you are using Java
    7.
  prefs: []
  type: TYPE_NORMAL
- en: The memory settings for Spark jobs are an important part of job launching. In
    the simple `local[*]`-based scenario as we are using, there is no difference between
    the Spark driver and executor. However, for a larger job deployed on a standalone
    or YARN Spark cluster, the configuration of driver memory and executor memory
    needs to reflect the size of the data and performed transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, as we discussed in the previous chapter, you can mitigate memory pressure
    by using a clever caching strategy and the right cache destination (for example,
    disk, off-heap memory).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step involves data load. In the case of multiple files, the SparkContext's
    method `wholeTextFiles` provides the functionality we need. It reads each file
    as a single record and returns it as a key-value pair, where the key contains
    the location of the file and the value holds the file content. We can reference
    input files directly via the wildcard pattern `data/subject*`. This is not only
    useful during loading files from a local filesystem but especially important for
    loading files from HDFS as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the names are not part of the input data, we define a variable that is
    going to hold the column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We simply defined the first three column names, and then column names for each
    of three position sensors. Furthermore, we also prepared a list of column indexes
    which are useless for modeling, including timestamp and orientation data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to process the content of the referenced files and create
    an `RDD` which we use as input for data exploration and modeling. Since we are
    expecting to iterate over the data several times and perform different transformations,
    we are going to cache the data in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, for each key-value pair we extract its content and split it based
    on line boundaries. Then we transform each line based on the file delimiter, which
    is a space between features. Since the files contains only numeric values and
    the string value `NaN` as a marker for missing values, we can simply transform
    all values into Java's `Double`, leaving `Double.NaN` as a representation for
    a missing value.
  prefs: []
  type: TYPE_NORMAL
- en: We can see our input file has `977,972` rows. During loading, we also skipped
    the timestamp column and columns which were marked as invalid in the dataset description
    (see the `ignoredColumns` array).
  prefs: []
  type: TYPE_NORMAL
- en: The RDD's interface follows the design principle of functional programming,
    the same principle which is adopted by the Scala programming language. This shared
    concept brings a uniform API for manipulating data structures; on the other hand,
    it is always good to know when an operation is invoked on a local object (array,
    list, sequence) and when it causes a distribution operation (`RDD`).
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep our view of the dataset consistent, we also need to filter column names
    based on the list of ignored columns which was prepared in previous steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It is always good to get rid of data which is useless for modeling. The motivation
    is to mitigate memory pressure during computation and modeling. For example, good
    targets for data removal are columns which contain random IDs, timestamps, constant
    columns, or columns which are already represented in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: From an intuitive point of view also, modelling ID terms, for example, doesn't
    make a lot of sense given the nature of the field. Feature selection is a hugely
    important topic and one that we will spend a great deal of time on later in the
    book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at the distribution of the individual activities in our dataset.
    We are going to use the same trick as in the previous chapter; however, we also
    would like to see actual names of activities instead of pure number-based representation.
    Hence, at first we define mapping describing a relation between an activity number
    and its name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we compute the number of individual activities in the data with the help
    of the Spark method `reduceByKey`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The command computes the count of individual activities, translates the activity
    number to its label, and sorts the result in descending order based on counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Or visualized based on activity frequencies as shown in *Figure 2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00062.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Frequencies of different activities in input data.'
  prefs: []
  type: TYPE_NORMAL
- en: It is always good to think about the order of the individual transformations
    which are applied on the data. In the preceding example, we applied the `sortBy`
    transformation after collecting all data locally with help of the Spark `collect`
    action. In this context, it makes perfect sense since we know that the result
    of the `collect` action is reasonably small (we have only 22 activity labels)
    and `sortBy` is applied on the local collection. On the other hand, putting `sortBy`
    before the `collect` action would force invocation of Spark RDD's transformation
    and scheduling sort as Spark distributed task.
  prefs: []
  type: TYPE_NORMAL
- en: Missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data description mentions that sensors used for activity tracking were not
    fully reliable and results contain missing data. We need to explore them in more
    detail to see how this fact can influence our modeling strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first question is how many missing values are in our dataset. We know from
    the data description that all missing values are marked by the string `NaN` (that
    is, not a number), which is now represented as `Double.NaN` in the `RDD` `rawData`.
    In the next code snippet, we compute the number of missing values per row and
    the total number of missing values in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Right, now we have overall knowledge about the amount of missing values in our
    data. But we do not know how the missing values are distributed. Are they spread
    uniformly over the whole dataset? Or are there rows/columns which contain more
    missing values? In the following text, we will try to find answers to these questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common mistake is to compare a numeric value and `Double.NaN` with comparison
    operators. For example, the `if (v == Double.NaN) { ... }` is wrong, since the
    Java specification says:'
  prefs: []
  type: TYPE_NORMAL
- en: '"`NaN` is unordered: (1) The numerical comparison operators `<`,  `<=`, `>`,
    and `>=` return `false` if either or both operands are `NaN`, (2) The equality
    operator `==` returns `false` if either operand is `NaN`."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, `Double.NaN == Double.NaN` returns always `false`. The right way to
    compare numeric values with `Double.NaN` is to use the method `isNaN`: `if (v.isNaN)
    { ... }` (or use the corresponding static method `java.lang.Double.isNaN`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, considering rows we have already computed numbers of missing values
    per row in the previous step. Sorting them and taking the unique values give us
    an understanding of how rows are affected by missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can see that the majority of rows contain a single missing value. However,
    there are lot of rows containing `13` or `14` missing values, or even `40` rows
    containing `27` *NaNs* and 107 rows which contain more than 30 missing values
    (`104` rows with `40` missing values, and `3` rows with `39` missing values).
    Considering that the dataset contains 41 columns, it means there are 107 rows
    which are useless (majority of values are missing), leaving 3,386 rows with at
    least two missing values which need attention, and `885,494` rows with a single
    missing value. We can now look at these rows in more detail. We select all rows
    which contain more missing values than a given threshold, for example, `26`. We
    also collect the index of the rows (it is a zero-based index!):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now we know exactly which rows are not useful. We have already observed that
    there are 107 bad rows which do not contain any useful information. Furthermore,
    we can see that lines which have `27` missing values have them in the places representing
    hand and ankle IMU sensors.
  prefs: []
  type: TYPE_NORMAL
- en: And finally, most of the lines have assigned `activityId` 10, 19, or 20, which
    represents `computer work`, `house cleaning`, and `playing soccer` activities,
    which are classes with top frequencies in dataset. That can lead us to theory
    that the "bad" lines were produced by explicitly rejecting a measurement device
    by subjects. Furthermore, we can also see the index of each wrong row and verify
    them in the input dataset. For now, we are going to leave the bad rows and focus
    on columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can ask the same question about columns - are there any columns which contain
    a higher amount of missing values? Can we remove such columns? We can start by
    collecting the number of missing values per column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The result shows that the second column (do not forget that we have already
    removed invalid columns during data load), which represents subjects' heart rate,
    contains lot of missing values. More than 90% of values are marked by `NaN`, which
    was probably caused by a measurement process of the experiment (subjects probably
    do not wear the heart rate monitor during usual daily activities but only when
    practicing sport).
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the columns contain sporadic missing values.
  prefs: []
  type: TYPE_NORMAL
- en: Another important observation is that the first column containing `activityId`
    does not include any missing values - that is good news and means that all observations
    were properly annotated and we do not need to drop any of them (for example, without
    a training target, we cannot train a model).
  prefs: []
  type: TYPE_NORMAL
- en: The RDD's `reduce` method represents action. That means it forces evaluation
    of the `RDD` and the result of the reduce is a single value and not `RDD`. Do
    not confuse it with `reduceByKey` which is an `RDD` operation and returns a new
    `RDD` of key-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to decide what to do with missing data. There are many strategies
    to adopt; however we need to preserve the meaning of our data.
  prefs: []
  type: TYPE_NORMAL
- en: We can simply drop all rows or columns which contain missing data - a very common
    approach as a matter of fact! It makes good sense for rows which are polluted
    by too many missing values but this is not a good global strategy in this case
    since we observed that missing values are spread over almost all columns and rows.
    Hence, we need a better strategy for handling missing values.
  prefs: []
  type: TYPE_NORMAL
- en: A summary of missing values sources and imputation methods is available, for
    example, in the book *Data Analysis Using Regression and Mutlilevel/Hierarchical
    Models* by A. Gelman and J. Hill ([http://www.stat.columbia.edu/~gelman/arm/missing.pdf](http://www.stat.columbia.edu/~gelman/arm/missing.pdf))
    or in the presentation [https://www.amstat.org/sections/srms/webinarfiles/ModernMethodWebinarMay2012.pdf](https://www.amstat.org/sections/srms/webinarfiles/ModernMethodWebinarMay2012.pdf)
    or [https://www.utexas.edu/cola/prc/_files/cs/Missing-Data.pdf](https://www.utexas.edu/cola/prc/_files/cs/Missing-Data.pdf)[.](http://www.stat.columbia.edu/~gelman/arm/missing.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering the heart rate column first, we cannot drop it since there is an
    obvious link between higher heart rate and practiced activity. However, we can
    still fill missing values with a reasonable constant. In the context of the heart
    rate, replacing missing values with the mean value of column values - a technique
    sometimes referred to as *mean computation of missing values* - can make good
    sense. We can compute it with the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that `mean heart rate` is quite a high value, which reflects the
    fact that heart rate measurements are mainly associated with sport activities
    (a reader can verify that). But, for example, considering the activity `watching
    TV`, the value over 90 is slightly higher than the expected value, since the average
    resting rate is between 60 and 100 (based on Wikipedia).
  prefs: []
  type: TYPE_NORMAL
- en: So for this case, we can replace missing heart rate values with mean resting
    rate (80) or we can take the computed mean value of heart rate. Later, we will
    impute the computed mean value and compare or combine the results (this is called,
    multiple imputation method). Or we can append a column which marks a line with
    missing value (see, for example, [https://www.utexas.edu/cola/prc/_files/cs/Missing-Data.pdf](https://www.utexas.edu/cola/prc/_files/cs/Missing-Data.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to replace missing values in the rest of the columns. We should
    perform the same analysis that we did for the heart rate column and see if there
    is a pattern in missing data or if they are just missing at random. For example,
    we can explore a dependency between missing value and our prediction target (in
    this case, `activityId`). Hence, we collect a number of missing values per column
    again; however, now we also remember `activityId` with each missing value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding code is slightly more complicated and deserves explanation. The
    call `(1)` transforms each value in a row into a sequence of `(K, V)` pairs where
    `K` represents the `activityId` stored in the row, and `V` is `1` if a corresponding
    column contains a missing value, else it is `0`. Then the reduce method `(2)`
    recursively transforms row values represented by sequences into the final results,
    where each column has associated a distribution represented by a sequence of `(K,V)` pairs
    where `K` is `activityId` and `V` represents the number of missing values in rows
    with the `activityId`. The method is straightforward but overcomplicated using
    a non-trivial function `inc` `(3)`. Furthermore, this naive solution is highly
    memory-inefficient, since for each column we duplicate information about `activityId`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, we can reiterate the naive solution by slightly changing the result
    representation by not computing distribution per column, but by counting all columns,
    missing value count per `activityId`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the result is an array of key-value pairs, where key is activity
    name, and value contains representing distribution of missing value in individual
    columns. Simply by running both samples, we can observe that the first one takes
    much more time than the second one. Also, the first one has higher memory demands
    and is much more complicated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can visualize the result as a heatmap where the *x *axis corresponds
    to columns and the *y *axis represents activities as shown in *Figure 3*. Such
    graphical representation gives us a comprehensible overview of how missing values
    are correlated with response column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Heatmap showing number of missing values in each column grouped by
    activity.'
  prefs: []
  type: TYPE_NORMAL
- en: The generated heatmap nicely shows the correlation of missing values. We can
    see that missing values are connected to sensors. If a sensor is not available
    or malfunctioning, then all measured values are not available. For example, this
    is visible for ankle sensor and `playing soccer`, other activities. On the other
    hand, the activity `watching TV` does not indicate any missing value pattern connected
    to a sensor.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, there is no other directly visible connection between missing data
    and activity. Hence, for now, we can decide to fill missing values with `0.0`
    to express that a missing sensor provides default values. However, our goal is
    to be flexible to experiment with different imputation strategies later (for example,
    imputing mean value of observation with the same `activityId`).
  prefs: []
  type: TYPE_NORMAL
- en: Summary of missing value analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now summarize all facts which we learned about missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: There are 107 rows which are useless and need to be filtered out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are 44 rows with `26` or `27` missing values. These rows seem useless,
    so we are going to filter them out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The heart rate column contains the majority of missing values. Since we expect
    that the column contains important information which can help to distinguish between
    different sport activities, we are not going to ignore the column. However, we
    are going to impute the missing value based on different strategies:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean resting heart rate based on medical research
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean heart rate` computed from available data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a pattern in the missing values in the rest of the columns - missing
    values are strictly linked to a sensor. We replace all these missing values with
    the value `0.0`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data unification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This exploratory analysis gives us an overview about shape of the data and
    the actions which we need to perform to deal with missing values. However, we
    still need to transform the data into a form expected by Spark algorithms. That
    includes:'
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling categorical values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The missing value handling step is easy, since we already performed missing
    value exploration and summarized the required transformations in the previous
    section. The following steps are going to implement them.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define a list of imputed values - for each column, we assign a single
    `Double` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'And a function which allow us to inject the values into our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The defined function accepts a Spark `RDD` where each row is represented as
    an array of `Double` numbers, and a parameter which contains values to replace
    the missing value for each column.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, we define a row filter - a method which removes all rows
    which contain more missing values than a given threshold. In this case, we can
    easily reuse the already computed value `nanCountPerRow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Please notice that we parameterize defined transformations. It is good practice
    to keep code flexible enough to permit further experimentation with parameters.
    On the other hand, it is good to avoid building a complex framework. The rule
    of thumb is to parameterize functionality which we would like to use in different
    contexts or we need to have a freedom in configuring code constants.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark algorithms can handle different forms of categorical features, but they
    need to be transformed into a form expected by an algorithm. For example, decision
    trees can handle categorical features as they are; on the other hand, linear regression
    or neural networks need to expand categorical values into binary columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, the good news is that all input features in our dataset are
    continuous. However, the target feature - `activityId` - represents multi-class
    features. The Spark MLlib classification guide ([https://spark.apache.org/docs/latest/mllib-linear-methods.html#classification](https://spark.apache.org/docs/latest/mllib-linear-methods.html#classification))
    says:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The training data set is represented by an RDD of LabeledPoint in MLlib, where
    labels are class indices starting from zero."'
  prefs: []
  type: TYPE_NORMAL
- en: 'But our dataset contains different numbers of activityIds - see the computed
    variable `activityIdCounts`. Hence, we need to transform them into a form expected
    by MLlib by defining a map from `activityId` to `activityIdx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Final transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we can compose all the defined functionality together and prepare
    the data for model building. First, the `rawData` `RDD` is filtered and all bad
    rows are removed with the help of `filterBadRows`, then the result is processed
    by the `imputeNaN` method which injects given values at the location of missing
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end, verify that we invoked the right transformations by at least computing
    the number of rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that we filtered out 151 rows ,which corresponds to our preceding
    observations.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data is the key point of data science. It involves also understanding
    missing data. Never skip this stage since it can lead to biased models giving
    too good results. And, as we continuously point out, not understanding your data
    will lead you to ask poor questions which ultimately results in lackluster answers.
  prefs: []
  type: TYPE_NORMAL
- en: Modelling data with Random Forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random Forest is an algorithm which can be used for different problems - binomial
    as we showed in the previous chapter, regression, or multiclass classification.
    The beauty of Random Forest is that it combines multiple weak learners represented
    by decision trees into one ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, to reduce variance of individual decision trees, the algorithms
    use the concept of bagging (Bootstrap aggregation). Each decision tree is trained
    on a subset of data generated by random selection with replacement.
  prefs: []
  type: TYPE_NORMAL
- en: Do not confuse bagging with boosting. Boosting incrementally builds an ensemble
    by training each new model to emphasize observations that previous model misclassified.
    Typically, after a weak model is added into the ensemble, the data is reweighted,
    observations that are misclassified gain weight, and vice versa. Furthermore,
    bagging can be invoked in parallel while boosting is a sequential process. Nevertheless,
    the goal of boosting is the same as of bagging - combine predictions of several
    weak models in order to improve generalization and robustness over a single model.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a boosting method is a **Gradient Boosting Machine** (**GBM**)
    which uses the boosting method to combine weak models (decision trees) into an
    ensemble; however, it generalizes the approach by allowing the use of an arbitrary
    loss function: instead of trying to correct the previous weak model misclassified
    observations, the GBM allows you to minimize a specified loss function (for example,
    mean squared error for regression).'
  prefs: []
  type: TYPE_NORMAL
- en: There are different variations of GBM - for example, stochastic GBM which combines
    boosting with bagging. The regular GBM and also stochastic GBM are available in
    H2O's machine learning toolbox. Furthermore, it is important to mention that GBM
    (as well as RandomForest) is an algorithm which builds pretty good models without
    extensive tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'More information about GBM is available in original paper of J.H. Friedman:
    *Greedy Function Approximation: A Gradient Boosting Machine* [http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf](http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, RandomForest employs so-called "feature bagging" - while building
    a decision tree, it selects a random subset of feature to make a split decision.
    The motivation is to build a weak learner and enhance generalization - for example,
    if one of the features is a strong predictor for given target variable, it would
    be selected by the majority of trees, resulting in highly similar trees. However,
    by random selection of features, the algorithm can avoid the strong predictor
    and build trees which find a finer-grained structure of data.
  prefs: []
  type: TYPE_NORMAL
- en: RandomForest also helps easily select the most predictive feature since it allows
    for computation of variable importance in different ways. For example, computing
    an overall feature impurity gain over all trees gives a good estimate of how the
    strong feature is.
  prefs: []
  type: TYPE_NORMAL
- en: From an implementation point of view, RandomForest can be easily parallelized
    since the *built trees* step is independent. On the other hand, distributing RandomForest
    computation is slightly harder problem, since each tree needs to explore almost
    the full set of data.
  prefs: []
  type: TYPE_NORMAL
- en: The disadvantage of RandomForest is complicated interpretability. The resulting
    ensemble is hard to explore and explain interactions between individual trees.
    However, it is still one of the best models to use if we need to obtain a good
    model without advanced parameters tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good source of information about RandomForest is the original paper of Leo
    Breiman and Adele Cutler available, for example, here: [https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm).'
  prefs: []
  type: TYPE_NORMAL
- en: Building a classification model using Spark RandomForest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we explored data and unified it into a form without
    missing values. We still need to transform the data into a form expected by Spark
    MLlib. As explained in the previous chapter, it involves the creation of `RDD`
    of `LabeledPoints`. Each `LabeledPoint` is defined by a label and a vector defining
    input features. The label serves as a training target for model builders and it
    references the index of categorical variables (see prepared transformation `activityId2Idx`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to prepare data for training and model validation. We simply
    split the data into two parts: 80% for training and the remaining 20% for validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And after this step we are ready to invoke the modeling part of the workflow.
    The strategy for building a Spark RandomForest model is the same as GBM we showed
    in the previous chapter by calling the static method `trainClassifier` on object
    `RandomForest`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the parameters are split into two sets:'
  prefs: []
  type: TYPE_NORMAL
- en: Strategy which defines common parameters for building a decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RandomForest specific parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The strategy parameter list overlaps with the parameter list of decision tree
    algorithms discussed in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input`: References training data represented by `RDD` of `LabeledPoints`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numClasses`: Number of output classes. In this case we model only classes
    which are included in the input data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`categoricalFeaturesInfo`: Map of categorical features and their arity. We
    don''t have categorical features in input data, that is why we pass an empty map.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`impurity`: Impurity measure used for tree node splitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subsamplingRate`: A fraction of training data used for building a single decision
    tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxDepth`: Maximum depth of a single tree. Deep trees have tendency to encode
    input data and overfit. On the other hand, overfitting in RandomForest is balanced
    by assembling multiple trees together. Furthermore, larger trees means longer
    training time and higher memory footprint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxBins`: Continuous features are transformed into ordered discretized features
    with at most `maxBins` possible values. The discretization is done before each
    node split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The RandomForest - specific parameters are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`numTrees`: Number of trees in the resulting forest. Increasing the number
    of trees decreases model variance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`featureSubsetStrategy`: Specifies a method which produces a number of how
    many features are selected for training a single tree. For example: "sqrt" is
    normally used for classification, while "onethird" for regression problems. See
    value of `RandomForest.supportedFeatureSubsetStrategies` for available values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed`: Seed for random generator initialization, since RandomForest depends
    on random selection of features and rows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The parameters `numTrees` and `maxDepth` are often referenced as stopping criteria.
    Spark also provides additional parameters to stop tree growing and produce fine-grained
    trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '`minInstancesPerNode`: A node is not split anymore, if it would provide left
    or right nodes which would contain smaller number of observations than the value
    specified by this parameter. Default value is 1, but typically for regression
    problems or large trees, the value should be higher.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minInfoGain`: Minimum information gain a split must get. Default value is
    0.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, Spark RandomForest accepts parameters which influence the performance
    of execution (see Spark documentation).
  prefs: []
  type: TYPE_NORMAL
- en: RandomForest is by definition an algorithm which depends on randomization. However,
    having non-deterministic runs is not the right behavior if you are trying to reproduce
    results or test corner cases. In this case, the seed parameter provides a way
    of fixing execution and providing deterministic results.
  prefs: []
  type: TYPE_NORMAL
- en: This is a common practice for non-deterministic algorithms; however, it is not
    enough if the algorithm is parallelized and its result depends on thread scheduling.
    In this case, ad-hoc methods need to be adopted (for example, limit parallelization
    by having only one computation thread, limit parallelization by limiting number
    of input partitions, or switching task scheduler to provide a fix schedule).
  prefs: []
  type: TYPE_NORMAL
- en: Classification model evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, when we have a model, we need to evaluate the quality of the model to decide
    whether the model is good enough for our needs. Keep in mind, that all quality
    metrics connected to a model need to be considered in your specific context and
    evaluated with respect to your target objective (such as sales increase, fraud
    detection, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: Spark model metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At first, use the embedded model metrics which the Spark API provides. We are
    going to use the same approach that we used in the previous chapter. We start
    by defining a method to extract model metrics for a given model and dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can directly compute Spark `MulticlassMetrics`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'And look at first interesting classification model metrics called `Confusion
    matrix`. It is represented by the type `org.apache.spark.mllib.linalg.Matrix`
    allowing you to perform algebraic operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00070.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, Spark prints predicted classes in columns. The predicted classes
    are stored in the field `labels` of the object `rfModelMetrics`. However, the
    field contains only translated indexes (see the created variable `activityId2Idx`).
    Nevertheless, we can easily create a function to transform the label index to
    an actual label string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For example, we can see that other activity was mispredicted many times with
    other activities - it was predicted correctly for `36455` cases; however, for
    `1261` cases the model predicted the `other` activity, but actual activity was
    `house cleaning`. On the other hand, the model predicted the activity `folding
    laundry` instead of the `other` activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can directly see that we can directly compute overall prediction accuracy
    based on correctly predicted activities located on the diagonal of the `Confusion
    matrix`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00072.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, the overall accuracy can be misleading in cases of classes are not
    evenly distributed (for example, most of the instances are represented by a single
    class). In such cases, overall accuracy can be confusing, since the model just
    predicting a dominant class would provide a high accuracy. Hence, we can look
    at our predictions in more detail and explore accuracy per individual class. However,
    first we look at the distribution of actual labels and predicted labels to see
    `(1)` if there is a dominant class and `(2)` if model preserves input distribution
    of classes and is not skewed towards predicting a single class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can easily see that there is no dominant class; however, the classes are
    not uniformly distributed. It is also worth noticing that the model preserves
    distribution of actual classes and there is no trend to prefer a single class.
    This just confirms our observation based on the `Confusion matrix`.
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, we can look at individual classes and compute precision (aka positive
    predictive value), recall (or so-called sensitivity) and `F-1` score. To remind
    definitions from the previous chapter: precision is a fraction of the correct
    predictions for a given class (that is, TP/TP+TF), while recall is defined as
    a fraction of all class instances that were correctly predicted (that is, TP/TP+FN).
    And finally, the `F-1` score combines both of them since it is computed as the
    weighted harmonic mean of precision and recall. We can easily compute them with
    the help of functions we already defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In our case, we deal with a quite good model since most of values are close
    to value 1.0\. It means that the model performs well for each input category -
    generating a low number of false positives (precision) and also false negatives
    (recall).
  prefs: []
  type: TYPE_NORMAL
- en: The nice feature of the Spark API is that it already provides methods to compute
    all three metrics we computed manually. We can easily call methods `precision`,
    `recall`, `fMeasure` with the index of label to get the same values. However,
    in the Spark case, the `Confusion matrix` is collected for each call and hence
    increases overall computation time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we use the already computed `Confusion matrix` and get the same
    results directly. Readers can verify that the following code gives us the same
    numbers as stored in `rfPerClassSummary`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'By having statistics per class, we can compute macro-average metrics simply
    by computing the mean value for each of the computed metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00075.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The `Macro` statistics give us an overall characteristic for all feature statistics.
    We can see expected values close to 1.0 since our model performs quite well on
    the testing data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, the Spark ModelMetrics API provides also weighted precision, recall
    and `F-1` scores, which are mainly useful if we deal with unbalanced classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00076.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And at the end, we are going to look at one more way of computing model metrics
    which is useful also in the cases when the classes are not well distributed. The
    method is called one-versus-all and it provides performance of the classifier
    with respect to one class at a time. That means we will compute a `Confusion matrix`
    for each output class - we can consider this approach as treating the classifier
    as a binary classifier predicting a class as positive case and any of other classes
    as negative case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us performance of each class with respect to other classes represented
    by a simple binary `Confusion matrix`. We can sum up all matrices and get a `Confusion
    matrix` to compute average accuracy and micro-averaged metrics per class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00077.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Having an overall `Confusion matrix`, we can compute average accuracy per class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The matrix gives us also `Micro-averaged metrics` (recall, precision, `F-1`).
    However, it is worth mentioning that our `rfOneVsAllCM` matrix is symmetric. This
    means that `Recall`, `Precision` and `F-1` have the same value (since FP and FN
    are the same):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00079.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An overview of the Spark ModelMetrics API is provided by the Spark documentation
    [https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html](https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, an understanding of model metrics and especially of a role of `Confusion
    matrix` in multiclass classification is crucial but not connected only to the
    Spark API. A great source of information is the Python scikit documentation ([http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html))
    or various R packages (for example, [http://blog.revolutionanalytics.com/2016/03/com_class_eval_metrics_r.html](http://blog.revolutionanalytics.com/2016/03/com_class_eval_metrics_r.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Building a classification model using H2O RandomForest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: H2O provides multiple algorithms for building classification models. In this
    chapter, we will focus on tree ensembles again, but we are going to demonstrate
    their usage in the context of our sensor data problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already prepared data which we can use directly to build the H2O RandomForest
    model. To transfer it them into H2O format we need to create `H2OContext` and
    then call the corresponding transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We created two tables referenced by the names `trainHF` and `testHF`. The code
    also updated names of columns by calling the method `setNames` since input `RDD`
    does not carry information about columns. The important step is call of the `update`
    method to save changes into H2O's distributed memory store. This is an important
    pattern exposed by the H2O API - all changes made on an object are done locally;
    to make them visible to other computation nodes, it is necessary to save them
    into the memory store (so-called **distributed key-value store** (**DKV**) )
  prefs: []
  type: TYPE_NORMAL
- en: Having data stored as H2O tables, we can open the H2O Flow user interface by
    calling `h2oContext.openFlow` and graphically explore the data. For example, the
    distribution of the `activityId` column as a numeric feature is shown in *Figure
    4:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00080.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The view of numeric column activityId which needs transformation
    to categorical type.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can directly compare the results and verify that we observe right distribution
    by a piece of Spark code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00081.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next step is to prepare the input data to run H2O algorithms. First we
    need to verify that column types are in the form expected by the algorithm. The
    H2O Flow UI provides a list of columns with basic attributes (*Figure 5*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00082.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Columns of imported training dataset shown in Flow UI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the `activityId` column is numeric; however, to perform classification,
    H2O requires columns to be categorical. So we need to transform the column by
    clicking on Convert to enum in UI or programmatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Again, we need to update the modified frame in the memory store by calling the
    `update` method. Furthermore, we are transforming a vector to another vector type
    and we do not need the original vector anymore, hence we can call the `remove`
    method on the result of the `replace` call.
  prefs: []
  type: TYPE_NORMAL
- en: 'After transformation, the `activityId` column is categorical; however, the
    vector domain contains values "0", "1", ..."6" - they are stored in the field
    `trainHF.vec("activityId").domain`. Nevertheless, we can update the vector with
    actual category names. We have already prepared index to name transformation called
    `idx2Activity` - hence we prepare a new domain and update the `activityId` vector
    domain for training and test tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we need to update the modified vector in the memory store as well
    - instead of calling the `update` method, the code makes explicit call of the
    method `water.DKV.put` which directly saves the object into the memory store.
  prefs: []
  type: TYPE_NORMAL
- en: In the UI, we can again explore the `activityId` column of test dataset and
    compare it with the results computed - *Figure 6:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00083.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The column activityId values distribution in test dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have prepared the data to perform model building. The configuration
    of H2O RandomForest for a classification problem follows the same pattern we introduced
    in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'There are several important differences which distinguish the H2O algorithm
    from Spark. The first important difference is that we can directly specify a validation
    dataset as an input parameter (the `_valid` field). This is not necessary since
    we can perform validation after the model is built; however, when the validation
    dataset is specified, we can track the quality of the model in real-time during
    building and stop model building if we consider the model is good enough (see
    *Figure 7* - the "Cancel Job" action stops training but the model is still available
    for further actions). Furthermore, later we can continue model building and append
    more trees if it is demanded. The parameter `_score_each_iteration` controls how
    often scoring should be performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00084.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Model training can be tracked in Flow UI and also stopped by pressing
    "Cancel Job" button.'
  prefs: []
  type: TYPE_NORMAL
- en: Another difference is represented by the parameters `_nbins`, `_nbins_top_level`,
    and `_nbins_cats`. The Spark RandomForest implementation accepts the parameter
    `maxBins` which controls discretization of continuous features. In the H2O case,
    it corresponds to the parameter `_nbins`. However, the H2O machine learning platform
    allows finer-grained tuning of discretization. Since top-level splits are the
    most important and can suffer from loss of information due to discretization,
    H2O permits temporary increase in the number of discrete categories for top-level
    splits via the parameter `_nbins_top_level`. Furthermore, high-value categorical
    features (> 1,024 levels) often degrades performance of computation by forcing
    an algorithm to consider all possible splits into two distinct subsets. Since
    there are 2^N subsets for `N` categorical levels, finding split points for these
    features can be expensive. For such cases, H2O brings the parameter `_nbins_cats`,
    which controls the number of categorical levels - if a feature contains more categorical
    levels than the value stored in the parameter, then the values are re-binned to
    fit into `_nbins_cats` bins.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last important difference is that we specified an additional stopping criterion
    together with traditional depth and number of trees in ensemble. The criterion
    limits improvement of computed misclassification on validation data - in this
    case, we specified that model building should stop if two consecutive scoring
    measurements on validation data (the field `_stopping_rounds`) do not improve
    by 0.001 (the value of the field `_stopping_tolerance`). This is a perfect criterion
    if we know the expected quality of the model and would like to limit model training
    time. In our case, we can explore the number of trees in the resulting ensemble:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00085.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Even we demanded 50 trees, the resulting model has only `14` trees since model
    training was stopped since the misclassification rate did not improve with respect
    to the given threshold.
  prefs: []
  type: TYPE_NORMAL
- en: H2O API exposes multiple stopping criteria which can be used by any of the algorithms
    - a user can use AUC value for binomial problems or MSE for regression problems.
    This is one of the most powerful feature which allow you to decrease computation
    time if huge space of hyper-parameters is explored
  prefs: []
  type: TYPE_NORMAL
- en: 'The quality of model can be explored in two ways: (1) directly by using the
    Scala API and accessing the model field `_output` which carries all output metrics,
    or (2) using the graphical interface to explore metrics in a more user-friendly
    way. For example, `Confusion matrix` on a specified validation set can be displayed
    as part of the model view directly in the Flow UI. Refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Confusion matrix for initial RandomForest model composed of 14 trees.'
  prefs: []
  type: TYPE_NORMAL
- en: It directly gives us error rate (0.22%) and misclassification per class and
    we can compare results directly with computed accuracy using Spark model. Furthermore,
    the `Confusion matrix` can be used to compute additional metrics which we explored.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, compute recall, precision, and `F-1` metrics per class. We can
    simply transform H2O''s `Confusion matrix` to Spark `Confusion matrix` and reuse
    all defined methods. But we have to be careful not to confuse actual and predicted
    values in the resulting `Confusion matrix` (the Spark matrix has predicted values
    in columns while the H2O matrix has them in rows):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that computed metrics for the specified validation dataset are
    stored in model output field `_output._validation_metrics`. It contains `Confusion
    matrix` but also additional information about model performance tracked during
    training. Then we simply transformed the H2O representation into Spark matrix.
    Then we can easily compute macro-performance per class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00087.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You can see that results are slightly better than the Spark results computed
    before, even though H2O used less trees. The explanation needs to explore H2O
    implementation of the RandomForest algorithm - H2O is using an algorithm based
    on generating a regression decision tree per output class - an approach which
    is often referenced as a "one-versus-all" scheme. This algorithm allows more fine-grained
    optimization with respect to individual classes. Hence in this case 14 RandomForest
    trees are internally represented by 14*7 = 98 internal decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: The reader can find more explanation about the benefits of a "one-versus-all"
    scheme for multiclass classification problems in the paper *In Defense of One-Vs-All
    Classification* from Ryan Rifkin and Aldebaro Klautau. The authors show that the
    schema is as accurate as any other approaches; on the other hand, the algorithm
    forces the generation of more decision trees which can negatively influence computation
    time and memory consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can explore more properties about the trained model. One of the important
    RandomForest metrics is variable importance. It is stored under the model''s field
    `_output._varimp`. The object contains raw values which can be scaled by calling
    the `scaled_values` method or obtain relative importance by calling the `summary`
    method. Nevertheless, they can be explored visually in the Flow UI as shown in
    *Figure 9*. The graph shows that the most significant features are measured temperatures
    from all three sensors followed by various movement data. And surprisingly to
    our expectation, the heart rate is not included among the top-level features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00088.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Variable importance for model "drfModel". The most important features
    include measured temperature.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are not satisfied with the quality of the model, it can be extended by
    more trees. We can reuse defined parameters and modify them in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up the desired numbers of trees in the resulting ensemble (for example,
    20).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disable early stopping criterion to avoid stopping model training before achieving
    the demanded number of trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure a so called *model checkpoint* to point to the previously trained
    model. The model checkpoint is unique feature of the H2O machine learning platform
    available for all published models. It is useful in situations when you need to
    improve a given model by performing more training iterations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After that, we can simply launch model building again. In this case, the H2O
    platform simply continues model training, reconstructs model state, and builds
    and appends new trees into a new model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00089.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, only `6` trees were built - to see that, the user can explore
    the model training output in the console and find a line which ends model training
    output and reporting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00090.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The 6^(th) tree was generated in 2 seconds and it was the last tree appended
    into the existing ensemble creating a new model. We can again explore `Confusion
    matrix` of newly built model and see improvement in overall error rate from 0.23
    to 0.2% (see *Figure 9*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00091.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Confusion matrix for RandomForest model with 20 trees.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced several important concepts including data cleanup and
    handling missing and categorical values, using Spark and H2O to train multi-classification
    models, and various evaluation metrics for classification models. Furthermore,
    the chapter brings the notion of model ensembles demonstrated on RandomForest
    as the ensemble of decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: The reader should see the importance of data preparation, which plays a key
    role during every model training and evaluation process. Training and using a
    model without understanding the modeling context can lead to misleading decisions.
    Moreover, every model needs evaluation with respect to the modeling goal (for
    example, minimization of false positives). Hence understanding trade-offs of different
    model metrics of classification models is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we did not cover all possible modelling tricks for classification
    models, but there are a few of them still opened for curious readers:'
  prefs: []
  type: TYPE_NORMAL
- en: We used a simple strategy to impute missing values in the heart rate column,
    but there are other possible solutions - for example, mean value imputation, or
    combining imputation with additional binary column which marks rows with the missing
    value. Both strategies can improve the accuracy of the model and we will use them
    later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the Occam's razor principle suggests that it is good idea to prefer
    a simpler model than a complex model providing the same accuracy. Hence, a good
    idea is to define a hyper-space of parameters and use an exploration strategy
    to find the simplest model (for example, fewer trees, less depth) which provides
    the same (or better) accuracy as the models trained in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this chapter, it is important to mention that the tree ensemble
    presented in this chapter is a primitive instance powerful concept of ensembles
    and super-learners which we are going to introduce later in this book.
  prefs: []
  type: TYPE_NORMAL
