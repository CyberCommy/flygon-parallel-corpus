- en: Unit Testing and Functional Testing
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing has become a primary part of good software development practice.
    It is a method by which individual units of source code are tested to ensure they
    function properly. Each unit is theoretically the smallest testable part of an
    application.
  prefs: []
  type: TYPE_NORMAL
- en: In unit testing, each unit is tested separately, isolating the unit under test
    as much as possible from other parts of the application. If a test fails, you
    would want it to be due to a bug in your code rather than a bug in the package
    that your code happens to use. A common technique is to use mock objects or mock
    data to isolate individual parts of the application from one another.
  prefs: []
  type: TYPE_NORMAL
- en: Functional testing, on the other hand, doesn't try to test individual components.
    Instead, it tests the whole system. Generally speaking, unit testing is performed
    by the development team, while functional testing is performed by a **Quality
    Assurance** (**QA**) or **Quality Engineering** (**QE**) team. Both testing models
    are needed to fully certify an application. An analogy might be that unit testing
    is similar to ensuring that each word in a sentence is correctly spelled, while
    functional testing ensures that the paragraph containing that sentence has a good
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a book requires not just ensuring the words are correctly spelled, but
    ensuring that the words string together as useful grammatically correct sentences
    and chapters that convey the intended meaning. Similarly, a successful software
    application requires much more than ensuring each "unit" correctly behaves. Does
    the system as a whole perform the intended actions?
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Assertions as the basis of software tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Mocha unit testing framework and the Chai assertions library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using tests to find bugs and fix the bug
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Docker to manage test infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing a REST backend service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UI functional testing in a real web browser using Puppeteer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving UI testability with element ID attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will know how to use Mocha, as well as how to
    write test cases for both directly invoked code under test and for testing code
    accessed via REST services. You will have also learned how to use Docker Compose
    to manage test infrastructure, both on your laptop and on the AWS EC2 Swarm infrastructure
    from [Chapter 12](8551a26c-6834-4df6-b392-60a15c20f6ff.xhtml), *Deploying Docker
    Swarm to AWS EC2 with Terraform*.
  prefs: []
  type: TYPE_NORMAL
- en: That's a lot of territory to cover, so let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Assert – the basis of testing methodologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Node.js has a useful built-in testing tool known as the `assert` module. Its
    functionality is similar to assert libraries in other languages. Namely, it's
    a collection of functions for testing conditions, and if the conditions indicate
    an error, the `assert` function throws an exception. It's not a complete test
    framework by any stretch of the imagination, but it can still be used for some
    amount of testing.
  prefs: []
  type: TYPE_NORMAL
- en: At its simplest, a test suite is a series of `assert` calls to validate the
    behavior of the thing being tested. For example, a test suite could instantiate the
    user authentication service, then make an API call and use `assert` methods to
    validate the result, then make another API call to validate its results, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following code snippet, which you can save in a file named `deleteFile.mjs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: This is what's called a negative test scenario, in that it's testing whether
    requesting to delete a nonexistent file throws the correct error. The `deleteFile`
    function throws an error containing the text that *does not exist* if the file
    to be deleted does not exist.  This test ensures the correct error is thrown and
    would fail if the wrong error is thrown, or if no error is thrown.
  prefs: []
  type: TYPE_NORMAL
- en: If you are looking for a quick way to test, the `assert` module can be useful
    when used this way. Each test case would call a function, then use one or more
    `assert` statements to test the results. In this case, the `assert` statements
    first ensure that `err` has some kind of value, then ensures that value is an
    `Error` instance, and finally ensures that the `message` attribute has the expected
    text. If it runs and no messages are printed, then the test passes. But what happens
    if the `deleteFile` callback is never called? Will this test case catch that error?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: This, of course, sets up a `package.json` file and installs the required packages.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond Mocha and Chai, we've installed two additional tools. The first, `cross-env`,
    is one we've used before and it enables cross-platform support for setting environment
    variables on the command line. The second, `npm-run-all`, simplifies using `package.json`
    to drive build or test procedures.
  prefs: []
  type: TYPE_NORMAL
- en: For the documentation of `cross-env`, go to [https://www.npmjs.com/package/cross-env](https://www.npmjs.com/package/cross-env).
  prefs: []
  type: TYPE_NORMAL
- en: For the documentation of `npm-run-all`, go to [https://www.npmjs.com/package/npm-run-all](https://www.npmjs.com/package/npm-run-all).
  prefs: []
  type: TYPE_NORMAL
- en: With the tools set up, we can move on to creating tests.
  prefs: []
  type: TYPE_NORMAL
- en: Notes model test suite
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because we have several Notes models, the test suite should run against any
    model. We can write tests using the NotesStore API, and an environment variable
    should be used to declare the model to test. Therefore, the test script will load
    `notes-store.mjs` and call functions on the object it supplies. Other environment
    variables will be used for other configuration settings.
  prefs: []
  type: TYPE_NORMAL
- en: Because we've written the Notes application using ES6 modules, we have a small
    item to consider. Older Mocha releases only supported running tests in CommonJS
    modules, so this would require us to jump through a couple of hoops to test Notes
    modules.  But the current release of Mocha does support them, meaning we can freely
    use ES6 modules.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start by writing a single test case and go through the steps of running
    that test and getting the results. After that, we'll write several more test cases,
    and even find a couple of bugs. These bugs will give us a chance to debug the
    application and fix any problems. We'll close out this section by discussing how
    to run tests that require us to set up background services, such as a database
    server.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the initial Notes model test case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the `test` directory, create a file named `test-model.mjs` containing the
    following. This will be the outer shell of the test suite:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: What we've done here is create a `test-all` script that will run the test suite
    against the individual NotesStore implementations. We can run this script to run
    every test combination, or we can run a specific script to test just the one combination.
    For example, `test-notes-sequelize-sqlite` will run tests against `SequelizeNotesStore` using
    the SQLite3 database.
  prefs: []
  type: TYPE_NORMAL
- en: 'It uses `npm-run-all` to support running the tests in series. Normally, in
    a `package.json` script, we would write this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: This, as the test script name suggests, uses SQLite3 as the underlying database,
    storing it in the named file.
  prefs: []
  type: TYPE_NORMAL
- en: We are missing two combinations, `test-notes-sequelize-mysql` for `SequelizeNotesStore`
    using MySQL and `test-notes-mongodb`, which tests against `MongoDBNotesStore`.
    We'll implement these combinations later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having automated the run of all test combinations, we can try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have a `describe` function that defines a test suite containing another
    `describe` function. That's the structure of a nested test suite.
  prefs: []
  type: TYPE_NORMAL
- en: We do not have test cases in the `it` function defined at the moment, but we
    do have the `before` and `after` functions.  These two functions do what they
    sound like; namely, the `before` function runs before all the test cases, while
    the `after` function runs after all the test cases have finished. The `before`
    function is meant to set up conditions that will be tested, while the `after`
    function is meant for teardown.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the `before` function adds entries to `NotesStore`, while the
    `after` function removes all entries. The idea is to have a clean slate after
    each nested test suite is executed.
  prefs: []
  type: TYPE_NORMAL
- en: The `before` and `after` functions are what Mocha calls a hook. The other hooks
    are `beforeEach` and `afterEach`. The difference is that the `Each` hooks are
    triggered before or after each test case's execution.
  prefs: []
  type: TYPE_NORMAL
- en: These two hooks also serve as test cases since the `create` and `destroy` methods
    could fail, in which case the hook will fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Between the `before` and `after` hook functions, add the following test cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: Compare the outputs with the descriptive strings in the `describe` and `it`
    functions. You'll see that the structure of this output matches the structure
    of the test suites and test cases. In other words, we should structure them so
    that they have well-structured test output.
  prefs: []
  type: TYPE_NORMAL
- en: As they say, testing is never completed, only exhausted. So, let's see how far
    we can go before exhausting ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: More tests for the Notes model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'That wasn''t enough to test much, so let''s go ahead and add some more tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: This is what a failed test looks like. Instead of the checkmark, there is a
    number, and the number corresponds to a report below it. In the failure report,
    the `deepEqual` function gave us clear information about how the object fields
    differed. In this case, it is the test we forced to fail because we wanted to
    see how the `deepEqual` function works.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that for the negative tests – where the test passes if an error is thrown
    – we run it in a `try/catch` block. The `throw new Error` line in each case should
    not execute because the preceding code should throw an error. Therefore, we can
    check if the message in that thrown error is the message that arrives, and fail
    the test if that's the case.
  prefs: []
  type: TYPE_NORMAL
- en: Diagnosing test failures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can add more tests because, obviously, these tests are not sufficient to
    be able to ship Notes to the public. After doing so, and then running the tests
    against the different test combinations, we will find this result for the SQLite3
    combination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: If this receives an empty result, an error is thrown. While the database doesn't
    see empty results set as an error, Notes does. Furthermore, Notes already knows
    how to deal with a thrown error in this case. Make this change and that particular
    test case will pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a second similar error in the `destroy` logic. In SQL, it obviously
    is not an SQL error if this SQL (from `models/notes-sqlite3.mjs`) does not delete
    anything:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we read the note and, as a byproduct, we verify the note exists.
    If the note doesn't exist, `read` will throw an error, and the `DELETE` operation
    will not even run.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run `test-notes-sequelize-sqlite`, there is also a similar failure
    in its `destroy` method. In `models/notes-sequelize.mjs`, make the following change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: As with the other NotesStore implementations, this reads the Note before trying
    to destroy it. If the `read` operation fails, then the test case sees the expected
    error.
  prefs: []
  type: TYPE_NORMAL
- en: These are the bugs we referred to in [Chapter 7](ae8529e5-3a08-45cc-89e9-82895eb45641.xhtml), *Data
    Storage and Retrieval*. We simply forgot to check for these conditions in this
    particular model. Thankfully, our diligent testing caught the problem. At least,
    that's the story to tell the managers rather than telling them that we forgot
    to check for something we already knew could happen.
  prefs: []
  type: TYPE_NORMAL
- en: Testing against databases that require server setup – MySQL and MongoDB
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: That was good, but we obviously won't run Notes in production with a database
    such as SQLite3 or Level. We can run Notes against the SQL databases supported
    by Sequelize (such as MySQL) and against MongoDB. Clearly, we've been remiss in
    not testing those two combinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our test results matrix reads as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`notes-fs`: PASS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`notes-memory`: PASS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`notes-level`: 1 failure, now fixed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`notes-sqlite3`: 2 failures, now fixed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`notes-sequelize`: With SQLite3: 1 failure, now fixed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`notes-sequelize`: With MySQL: untested'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`notes-mongodb`: Untested'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two untested NotesStore implementations both require that we set up a database
    server. We avoided testing these combinations, but our manager won't accept that
    excuse because the CEO needs to know we've completed the test cycles. Notes must
    be tested with a configuration similar to the production environments'.
  prefs: []
  type: TYPE_NORMAL
- en: In production, we'll be using a regular database server, with MySQL or MongoDB
    being the primary choices. Therefore, we need a way to incur a low overhead to
    run tests against those databases. Testing against the production configuration
    must be so easy that we should feel no resistance in doing so, to ensure that
    tests are run often enough to make the desired impact.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we made a lot of progress and have a decent start on a test
    suite for the NotesStore database modules. We learned how to set up test suites
    and test cases in Mocha, as well as how to get useful test reporting. We learned
    how to use `package.json` to drive test suite execution. We also learned about
    negative test scenarios and how to diagnose errors that come up.
  prefs: []
  type: TYPE_NORMAL
- en: But we need to work on this issue of testing against a database server. Fortunately,
    we've already worked with a piece of technology that supports easily creating
    and destroying the deployment infrastructure. Hello, Docker!
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll learn how to repurpose the Docker Compose deployment
    as a test infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker Swarm to manage test infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One advantage Docker gives is the ability to install the production environment
    on our laptop. In [Chapter 12](8551a26c-6834-4df6-b392-60a15c20f6ff.xhtml), *Deploying
    Docker Swarm to AWS EC2 Using Terraform*, we converted a Docker setup that ran
    on our laptop so that it could be deployed on real cloud hosting infrastructure.
    That relied on converting a Docker Compose file into a Docker Stack file, along
    with customization for the environment we built on AWS EC2 instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we''ll repurpose the Stack file as test infrastructure deployed
    to a Docker Swarm. One approach is to simply run the same deployment, to AWS EC2,
    and substitute new values for the `var.project_name` and `var.vpc_name` variables.
    In other words, the EC2 infrastructure could be deployed this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: As before, this will print a message about the join token. If desired, if you
    have multiple computers in your office, it might be interesting for you to experiment
    with setting up a local Swarm. But for this exercise, that's not important. This
    is because we can do everything required with a single-node Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'This isn''t a one-way street, meaning that when you''re done with this exercise,
    it is easy to turn off swarm mode. Simply shut down anything deployed to your
    local Swarm and run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: This deletes the placement constraints we declared for use on AWS EC2 and sets
    it to one replica for each service. For a single-node cluster, we don't worry
    about placement, of course, and there is no need for more than one instance of
    any service.
  prefs: []
  type: TYPE_NORMAL
- en: For the database services, remove the `volumes` tag. Using this tag is required
    when it's necessary to persist in the database data directory. For test infrastructure,
    the data directory is unimportant and can be thrown away at will. Likewise, remove
    the top-level `volumes` tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the `svc-notes` and `svc-userauth` services, make these changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: We run `swarm init` to turn on swarm mode on our laptop, then add the two `TWITTER`
    secrets to the swarm. Since it is a single-node swarm, we don't need to run a
    `docker swarm join` command to add new nodes to the swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in the `compose-stack-test-local` directory, we can run these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: Because this is a single-host swarm, we don't need to use SSH to access the
    swarm nodes, nor do we need to set up remote access using `docker context`. Instead,
    we run the Docker commands, and they act on the Docker instance on the localhost.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `docker ps` command will tell us the precise container name for each service.
    With that knowledge, we can run the following to gain access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: 'The tests should execute as they did on our laptop, but they''re running inside
    the container instead. However, the MySQL test won''t have run because the `package.json`
    scripts are not set up to run that one automatically. Therefore, we can add this
    to `package.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: The tests should execute correctly against MySQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'To automate this, we can create a file named `run.sh` containing the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: This runs the preceding script, which will run each test combination individually
    and also make sure the `DEBUG` variable is not set. This variable is set in the
    Dockerfile and causes debugging information to be printed among the test results
    output. Inside the script, the `--workdir` option sets the current directory of
    the command's execution in the `test` directory to simplify running the test scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this script won't execute as-is on Windows. To convert this for use
    on PowerShell, save the text starting at the second line into `run.ps1`, and then change
    `SVC_NOTES` references into `%SVC_NOTES%` references.
  prefs: []
  type: TYPE_NORMAL
- en: We have succeeded in semi-automating test execution for most of our test matrix.
    However, there is a glaring hole in the test matrix, namely the lack of testing
    on MongoDB. Plugging that hole will let us see how we can set up MongoDB under
    Docker.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB setup under Docker and testing Notes against MongoDB
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Chapter 7](ae8529e5-3a08-45cc-89e9-82895eb45641.xhtml),* Data Storage and
    Retrieval*, we developed MongoDB support for Notes. Since then, we've focused
    on `Sequelize`. To make up for that slight, let's make sure we at least test our
    MongoDB support. Testing on MongoDB simply requires defining a container for the
    MongoDB database and a little bit of configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Visit [https://hub.docker.com/_/mongo/](https://hub.docker.com/_/mongo/) for
    the official MongoDB container. You'll be able to retrofit this in order to deploy
    the Notes application running on MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code to `compose-stack-test-local/docker-compose.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: We simply added the MongoDB container to `frontnet`, making the database available
    at the URL shown here. Hence, it's simple to now run the test suite using the
    Notes MongoDB model.
  prefs: []
  type: TYPE_NORMAL
- en: The `--no-timeouts` option was necessary to avoid a spurious error while testing
    the suite against MongoDB. This option instructs Mocha to not check whether a
    test case execution takes too long.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final requirement is to add the following line to `run.sh` (or `run.ps1`
    for Windows):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem is that the initializer for the MongoClient object has changed
    slightly. Therefore, we must modify `notes/models/notes-mongodb.mjs` with this
    new `connectDB` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: This sets up Mocha and the SuperTest client. The `URL_USERS_TEST` environment
    variable specifies the base URL of the server to run the test against. You'll
    almost certainly be using `http://localhost:5858`, given the configuration we've
    used earlier, but it can be any URL pointing to any host. SuperTest initializes
    itself a little differently to SuperAgent.
  prefs: []
  type: TYPE_NORMAL
- en: The `SuperTest` module supplies a function, and we call that function with the `URL_USERS_TEST` variable.
    That gives us an object, which we call `request`, that is used for interacting
    with the service under test.
  prefs: []
  type: TYPE_NORMAL
- en: We've also set up a pair of variables to store the authentication user ID and
    key. These are the same values that are in the user authentication server. We
    simply need to supply them when making API calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, there''s the outer shell of the Mocha test suite. So, let''s start
    filling in the `before` and `after` test cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can test some API methods, such as the `/list` operation.
  prefs: []
  type: TYPE_NORMAL
- en: We have already guaranteed that there is an account in the `before` method,
    so `/list` should give us an array with one entry.
  prefs: []
  type: TYPE_NORMAL
- en: This follows the general pattern for using Mocha to test a REST API method.
    First, we use SuperTest's `request` object to call the API method and `await`
    its result. Once we have the result, we use `assert` methods to validate it is
    what's expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following test cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we should check the `/destroy` operation. This operation is already
    checked the `after` method, where we `destroy` a known user account. We also need
    to perform the negative test and verify its behavior against an account we know
    does not exist.
  prefs: []
  type: TYPE_NORMAL
- en: The desired behavior is that either an error is thrown or the result shows an
    HTTP `status` indicating an error. In fact, the current authentication server
    code gives a 500 status code, along with some other information.
  prefs: []
  type: TYPE_NORMAL
- en: This gives us enough tests to move forward and automate the test run.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `compose-stack-test-local/docker-compose.yml`, we need to inject the `test.js` script
    into the `svc-userauth-test` container. We''ll add that here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: In the dependencies, we list Mocha, Chai, SuperTest, and cross-env. Then, in
    the `test` script, we run Mocha along with the required environment variable. 
    This should run the tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could use this test suite from our laptop. Because the test directory is
    injected into the container the tests, we can also run them inside the container.
    To do so, add the following code to `run.sh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: Because `URL_USERS_TEST` can take any URL, we could run the test suite against
    any instance of the user authentication service. For example, we could test an
    instance deployed on AWS EC2 from our laptop using a suitable value for `URL_USERS_TEST`.
  prefs: []
  type: TYPE_NORMAL
- en: We're making good progress. We now have test suites for both the Notes and User
    Authentication services. We have learned how to test a REST service using the
    REST API. This is different than directly calling internal functions because it
    is an end-to-end test of the complete system, in the role of a consumer of the
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Our next task is to automate test results reporting.
  prefs: []
  type: TYPE_NORMAL
- en: Automating test results reporting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's cool we have automated test execution, and Mocha makes the test results
    look nice with all those checkmarks. But what if management wants a graph of test
    failure trends over time? There could be any number of reasons to report test
    results as data rather than as a user-friendly printout on the console.
  prefs: []
  type: TYPE_NORMAL
- en: For example, tests are often not run on a developer laptop or by a quality team
    tester, but by automated background systems. The CI/CD model is widely used, in
    which tests are run by the CI/CD system on every commit to the shared code repository.
    When fully implemented, if the tests all pass on a particular commit, then the
    system is automatically deployed to a server, possibly the production servers.
    In such a circumstance, the user-friendly test result report is not useful, and
    instead, it must be delivered as data that can be displayed on a CI/CD results
    dashboard website.
  prefs: []
  type: TYPE_NORMAL
- en: Mocha uses what's called a **Reporter** to report test results. A Mocha Reporter
    is a module that prints data in whatever format it supports. More information
    on this can be found on the Mocha website: [https://mochajs.org/#reporters](https://mochajs.org/#reporters).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will find the current list of available `reporters` like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `npm run script-name` command, we can inject command-line arguments,
    as we''ve done here. The `--` token tells npm to append the remainder of its command
    line to the command that is executed. The effect is as if we had run this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: This installs not just Puppeteer, but Mocha, Chai, and Supertest. We'll also
    be using the `package.json` file to record scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'During installation, you''ll see that Puppeteer causes Chromium to be downloaded,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on what you need to do, `docker-compose build` might also be required.
    In any case, this brings up the test infrastructure and lets you see the running
    system.
  prefs: []
  type: TYPE_NORMAL
- en: We can use a browser to visit `http://localhost:3000` and so on. Because this
    system won't contain any users, our test script will have to add a test user so
    that the test can log in and add notes.
  prefs: []
  type: TYPE_NORMAL
- en: Another item of significance is that tests will be running in an anonymous Chromium
    instance. Even if we use Chrome as our normal desktop browser, this Chromium instance
    will have no connection to our normal desktop setup. That's a good thing from
    a testability standpoint since it means your test results will not be affected
    by your personal web browser configuration. On the other hand, it means Twitter
    login testing is not possible, because that Chromium instance does not have a
    Twitter login session.
  prefs: []
  type: TYPE_NORMAL
- en: With those things in mind, let's write an initial test suite. We'll start with
    a simple initial test case to prove we can run Puppeteer inside Mocha. Then, we'll
    test the login and logout functionality, the ability to add notes, and a couple
    of negative test scenarios. We'll close this section with a discussion on improving
    testability in HTML applications. Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an initial Puppeteer test for the Notes application stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our first test goal is to set up the outline of a test suite. We will need
    to do the following, in order:'
  prefs: []
  type: TYPE_NORMAL
- en: Add a test user to the user authentication service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch the browser.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visit the home page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify the home page came up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Close the browser.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Delete the test user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will establish that we have the ability to interact with the launched infrastructure,
    start the browser, and see the Notes application. We will continue with the policy
    and clean up after the test to ensure a clean environment for subsequent test
    runs and will add, then remove, a test user.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `notesui` directory, create a file named `uitest.mjs` containing the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: This adds a user to the authentication service. Refer back and you'll see this
    is similar to the test case in the REST test suite. If you want a verification
    phase, there is another test case that calls the `/find/testme` endpoint to verify
    the result. Since we've already verified the authentication system, we do not
    need to reverify it here. We just need to ensure we have a known test user we
    can use for scenarios where the browser must be logged in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep this at the very end of `uitest.mjs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: Remember that within `describe`, the tests are the `it` blocks. The `before`
    block is executed before all the `it` blocks, and the `after` block is executed
    afterward.
  prefs: []
  type: TYPE_NORMAL
- en: In the `before` function, we set up Puppeteer by launching a Puppeteer instance
    and starting a new Page object. Because `puppeteer.launch` has the `headless`
    option set to `false`, we'll see a browser window on the screen. This will be
    useful so we can see what's happening. The `sloMo` option also helps us see what's
    happening by slowing down the browser interaction. In the `after` function, we
    call the `close` method on those objects in order to close out the browser. The
    `puppeteer.launch` method takes an `options` object, with a long list of attributes
    that are worth learning about.
  prefs: []
  type: TYPE_NORMAL
- en: The `browser` object represents the entire browser instance that the test is
    being run on. In contrast, the `page` object represents what is essentially the
    currently open tab in the browser. Most Puppeteer functions execute asynchronously.
    Therefore, we can use `async` functions and the `await` keywords.
  prefs: []
  type: TYPE_NORMAL
- en: The `timeout` setting is required because it sometimes takes a longish time
    for the browser instance to launch. We're being generous with the timeout to minimize
    the risk of spurious test failures.
  prefs: []
  type: TYPE_NORMAL
- en: For the `it` clause, we do a tiny amount of browser interaction. Being a wrapper
    around a browser tab, the `page` object has methods related to managing an open
    tab. For example, the `goto` method tells the browser tab to navigate to the given
    URL. In this case, the URL is the Notes home page, which is passed in as an environment
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: The `waitForSelector` method is part of a group of methods that wait for certain
    conditions. These include `waitForFileChooser`, `waitForFunction`, `waitForNavigation`, `waitForRequest`,
    `waitForResponse`, and `waitForXPath`. These, and the `waitFor` method, all cause
    Puppeteer to asynchronously wait for a condition to happen in the browser. The
    purpose of these methods is to give the browser time to respond to some input,
    such as clicking on a button. In this case, it waits until the web page loading
    process has an element visible at the given CSS selector. That selector refers
    to the Login button, which will be in the header.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, this test visits the Notes home page and then waits until the
    Login button appears. We could call that a simple smoke test that's quickly executed
    and determines that the basic functionality is there.
  prefs: []
  type: TYPE_NORMAL
- en: Executing the initial Puppeteer test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have the beginning of a Puppeteer-driven test suite for the Notes application.
    We have already launched the test infrastructure using `docker-compose`. To run
    the test script, add the following to the scripts section of the `package.json` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: We have successfully created the structure that we can run these tests in. We
    have set up Puppeteer and the related packages and created one useful test. The
    primary win is to have a structure to build further tests on top of.
  prefs: []
  type: TYPE_NORMAL
- en: Our next step is to add more tests.
  prefs: []
  type: TYPE_NORMAL
- en: Testing login/logout functionality in Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we created the outline within which to test the Notes
    user interface. We didn't do much testing regarding the application, but we proved
    that we can test Notes using Puppeteer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we''ll add an actual test. Namely, we''ll test the login and
    logout functionality. The steps for this are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in using the test user identity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that the browser was logged in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log out.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that the browser is logged out.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In `uitest.js`, insert the following test code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: With that, our new tests are passing. Notice that the time required to execute
    some of the tests is rather long. Even longer times were observed while debugging
    the test, which is why we set long timeouts.
  prefs: []
  type: TYPE_NORMAL
- en: That's good, but of course, there is more to test, such as the ability to add
    a Note.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the ability to add Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have a test case to verify login/logout functionality. The point of this
    application is adding notes, so we need to test this feature. As a side effect,
    we will learn how to verify page content with Puppeteer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test this feature, we will need to follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in and verify we are logged in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the Add Note button to get to the form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the information for a Note.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that we are showing the Note and that the content is correct.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the Delete button and confirm deleting the Note.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that we end up on the home page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log out.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You might be wondering "*Isn't it duplicative to log in again*?" The previous
    tests focused on login/logout. Surely that could have ended with the browser in
    the logged-in state? With the browser still logged in, this test would not need
    to log in again. While that is true, it would leave the login/logout scenario
    incompletely tested. It would be cleaner for each scenario to be standalone in
    terms of whether or not the user is logged in. To avoid duplication, let's refactor
    the test slightly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `outermost` describe block, add the following two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: All we've done is move the code that had been here into their own functions.
    This means we can reuse those functions in other tests, thus avoiding duplicative
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code for the Note creation test suite to `uitest.mjs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: All we did was add the ID attribute. This is an example of improving testability,
    which we'll discuss later.
  prefs: []
  type: TYPE_NORMAL
- en: A technique we're using is to call `page.$` to query whether the given element
    is on the page. This method inspects the page, returning an array containing any
    matching elements. We are simply testing if the return value is non-null because
    `page.$` returns `null` if there are no matching elements. This makes for an easy
    way to test if an element is present.
  prefs: []
  type: TYPE_NORMAL
- en: We end this by logging out by clicking on the **Logout** button.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having created these test cases, we can run the test suite again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: There are several methods supported by the `uuid` package, and the `v4` method
    is what generates random strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, add the following scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]js\1'
  prefs: []
  type: TYPE_NORMAL
- en: This computes the bad URL by taking the URL for the home page (`NOTES_HOME_URL`)
    and setting the *pathname* portion of the URL to `/bad-unknown-url`. Since there
    is no route in Notes for this path, we're certain to get an error. If we wanted
    more certainty, it seems we could use the `uuidv4()` function to make the URL
    random.
  prefs: []
  type: TYPE_NORMAL
- en: Calling `page.goto()` simply gets the browser to go to the requested URL. For
    the subsequent page, we wait until a page with a `header` element shows up. Because
    this page doesn't have much on it, the header element is the best choice for determining
    when we have the subsequent page.
  prefs: []
  type: TYPE_NORMAL
- en: To check the 404 status code, we call `response.status()`, which is the status
    code that's received in the HTTP response. Then, we call `page.$eval` to get a
    couple of items from the page and make sure they contain the text that's expected.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we did not find any code problems, but we did find another user
    experience problem. The error page is downright ugly and user-unfriendly. We know
    the user experience team will scream about this, so add it to your backlog to
    do something to improve this page.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we wrapped up test development by creating a couple of negative
    tests. While this didn't result in finding code bugs, we found a pair of user
    experience problems. We know this will result in an unpleasant discussion with
    the user experience team, so we've proactively added a task to the backlog to
    fix those pages. But we also learned about being on the lookout for any kind of
    problem that crops up along the way. It's well-known that the lowest cost of fixing
    a problem is the issues found by the development or testing team. The cost of
    fixing problems goes up tremendously when it is the user community reporting the
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Before we wrap up this chapter, we need to talk a little more in-depth about
    testability.
  prefs: []
  type: TYPE_NORMAL
- en: Improving testability in the Notes UI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the Notes application displays well in the browser, how do we write test
    software to distinguish one page from another? As we saw in this section, the
    UI test often performed an action that caused a page refresh and had to wait for
    the next page to appear. This means our test must be able to inspect the page
    and work out whether the browser is displaying the correct page. An incorrect
    page is itself a bug in the application. Once the test determines it is the correct
    page, it can then validate the data on the page.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom line is a requirement stating that each HTML element must be easily
    addressable using a CSS selector.
  prefs: []
  type: TYPE_NORMAL
- en: While in most cases it is easy to code a CSS selector for every element, in
    a few cases, this is difficult. The **Software Quality Engineering** (**SQE**)
    manager has requested our assistance. At stake is the testing budget, which will
    be stretched further the more the SQE team can automate their tests.
  prefs: []
  type: TYPE_NORMAL
- en: All that's necessary is to add a few `id` or `class` attributes to HTML elements
    to improve testability. With a few identifiers and a commitment to maintaining
    those identifiers, the SQE team can write repeatable test scripts to validate
    the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already seen one example of this: the Delete button in `views/noteview.hbs`.
    It proved impossible to write a CSS selector for that button, so we added an ID
    attribute that let us write the test.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, *testability* is about adding things to an API or user interface
    for the benefit of software quality testers. For an HTML user interface, that
    means making sure test scripts can locate any element in the HTML DOM. And as
    we've seen, the `id` and `class` attributes go a long way to satisfying that need.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about user interface testing as a form of functional
    testing. We used Puppeteer, a framework for driving a headless Chromium browser
    instance, as the vehicle for testing the Notes user interface. We learned how
    to automate user interface actions and how to verify that the web pages that showed
    up matched with their correct behavior. That included test scenarios covering
    login, logout, adding notes, and logging in with a bad user ID. While this didn't
    discover any outright failures, watching the user interaction told us of some
    usability problems with Notes.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we are ready to close out this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We covered a lot of territory in this chapter and looked at three distinct
    areas of testing: unit testing, REST API testing, and UI functional tests. Ensuring
    that an application is well tested is an important step on the road to software
    success. A team that does not follow good testing practices is often bogged down
    with fixing regression after regression.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we talked about the potential simplicity of simply using the assert module
    for testing. While the test frameworks, such as Mocha, provide great features,
    we can go a long way with a simple script.
  prefs: []
  type: TYPE_NORMAL
- en: There is a place for test frameworks, such as Mocha, if only to regularize our
    test cases and to produce test results reports. We used Mocha and Chai for this,
    and these tools were quite successful. We even found a couple of bugs with a small
    test suite.
  prefs: []
  type: TYPE_NORMAL
- en: When starting down the unit testing road, one design consideration is mocking
    out dependencies. But it's not always a good use of our time to replace every
    dependency with a mock version. As a result, we ran our tests against a live database,
    but with test data.
  prefs: []
  type: TYPE_NORMAL
- en: To ease the administrative burden of running tests, we used Docker to automate
    setting up and tearing down the test infrastructure. Just as Docker was useful
    in automating the deployment of the Notes application, it's also useful in automating
    test infrastructure deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we were able to test the Notes web user interface in a real web browser.
    We can't trust that unit testing will find every bug; some bugs will only show
    up in the web browser.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we've covered the full life cycle of Node.js development, from
    concept, through various stages of development, to deployment and testing. This
    will give you a strong foundation from which to start developing Node.js applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll explore another critical area – security. We'll start
    by using HTTPS to encrypt and authenticate user access to Notes. We'll use several
    Node.js packages to limit the chances of security intrusions.
  prefs: []
  type: TYPE_NORMAL
