- en: Using Spark SQL for Data Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce you to using Spark SQL for exploratory data
    analysis. We will introduce preliminary techniques to compute some basic statistics,
    identify outliers, and visualize, sample, and pivot data. A series of hands-on
    exercises in this chapter will enable you to use Spark SQL along with tools such
    as Apache Zeppelin for developing an intuition about your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we shall look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Exploratory Data Analysis (EDA)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is EDA important?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Spark SQL for basic data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing data with Apache Zeppelin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling data with Spark SQL APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Spark SQL for creating pivot tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Exploratory Data Analysis (EDA)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Exploratory Data Analysis** (**EDA**), or **Initial Data Analysis** (**IDA**),
    is an approach to data analysis that attempts to maximize insight into data. This
    includes assessing the quality and structure of the data, calculating summary
    or descriptive statistics, and plotting appropriate graphs. It can uncover underlying
    structures and suggest how the data should be modeled. Furthermore, EDA helps
    us detect outliers, errors, and anomalies in our data, and deciding what to do
    about such data is often more important than other, more sophisticated analysis.
    EDA enables us to test our underlying assumptions, discover clusters and other
    patterns in our data, and identify the possible relationships between various
    variables. A careful EDA process is vital to understanding the data and is sometimes
    sufficient to reveal such poor data quality that using a more sophisticated model-based
    analysis is not justified.'
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the graphical techniques used in EDA are simple, consisting of plotting
    the raw data and simple statistics. The focus is on the structures and models
    revealed by the data or best fit the data. EDA techniques include scatter plots,
    box plots, histograms, probability plots, and so on. In most EDA techniques, we
    use all of the data, without making any underlying assumptions. The analyst builds
    intuition, or gets a "feel", for the Dataset as a result of such exploration.
    More specifically, the graphical techniques allow us to efficiently select and
    validate appropriate models, test our assumptions, identify relationships, select
    estimators, detect outliers, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: EDA involves a lot of trial and error, and several iterations. The best way
    is to start simple and then build in complexity as you go along. There is a major
    trade-off in modeling between the simple and the more accurate ones. Simple models
    may be much easier to interpret and understand. These models can get you to 90%
    accuracy very quickly, versus a more complex model that might take weeks or months
    to get you an additional 2% improvement. For example, you should plot simple histograms
    and scatter plots to quickly start developing an intuition for your data.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark SQL for basic data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interactively, processing and visualizing large data is challenging as the queries
    can take a long time to execute and the visual interface cannot accommodate as
    many pixels as data points. Spark supports in-memory computations and a high degree
    of parallelism to achieve interactivity with large distributed data. In addition,
    Spark is capable of handling petabytes of data and provides a set of versatile
    programming interfaces and libraries. These include SQL, Scala, Python, Java and
    R APIs, and libraries for distributed statistics and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: For data that fits into a single computer, there are many good tools available,
    such as R, MATLAB, and others. However, if the data does not fit into a single
    machine, or if it is very complicated to get the data to that machine, or if a
    single computer cannot easily process the data, then this section will offer some
    good tools and techniques for data exploration.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will go through some basic data exploration exercises to
    understand a sample Dataset. We will use a Dataset that contains data related
    to direct marketing campaigns (phone calls) of a Portuguese banking institution.
    The marketing campaigns were based on phone calls to customers. We'll use the
    `bank-additional-full.csv` file that contains 41,188 records and 20 input fields,
    ordered by date (from May 2008 to November 2010). The Dataset has been contributed
    by S. Moro, P. Cortez, and P. Rita, and can be downloaded from [https://archive.ics.uci.edu/ml/datasets/Bank+Marketing](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing).
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, let''s define a schema and read in the CSV file to create
    a DataFrame. You can use `:paste` command to paste initial set of statements in
    your Spark shell session (use *Ctrl*+*D* to exit the paste mode), as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'After the DataFrame has been created, we first verify the number of records:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also define a `case` class called `Call` for our input records, and
    then create a strongly-typed Dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will begin our data exploration by identifying missing
    data in our Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Missing data can occur in Datasets due to reasons ranging from negligence to
    a refusal on the part of respondants to provide a specific data point. However,
    in all cases, missing data is a common occurrence in real-world Datasets. Missing
    data can create problems in data analysis and sometimes lead to wrong decisions
    or conclusions. Hence, it is very important to identify missing data and devise
    effective strategies to deal with it.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we analyze the numbers of records with missing data fields
    in our sample Dataset. In order to simulate missing data, we will edit our sample
    Dataset by replacing fields containing "unknown" values with empty strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we created a DataFrame/Dataset from our edited file, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following two statements give us a count of rows with certain fields having
    missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00052.gif)'
  prefs: []
  type: TYPE_IMG
- en: In [Chapter 4](part0057.html#1MBG20-e9cbc07f866e437b8aa14e841622275c), *Using
    Spark SQL for Data Munging*, we will look at effective ways of dealing with missing
    data. In the next section, we will compute some basic statistics for our sample
    Dataset to improve our understanding of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Computing basic statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Computing basic statistics is essential for a good preliminary understanding
    of our data. First, for convenience, we create a case class and a Dataset containing
    a subset of fields from our original DataFrame. In the following example, we choose
    some of the numeric fields and the outcome field, that is, the "term deposit subscribed"
    field:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Next, we use `describe()` compute the `count`, `mean`, `stdev`, `min`, and `max`
    values for the numeric columns in our Dataset. The `describe()` command gives
    a way to do a quick sense-check on your data. For example, the counts of rows
    of each of the columns selected matches the total number records in the DataFrame
    (no null or invalid rows),whether the average and range of values for the age
    column matching your expectations, and so on. Based on the values of the means
    and standard deviations, you can get select certain data elements for deeper analysis.
    For example, assuming normal distribution, the mean and standard deviation values
    for age suggest most values of age are in the range 30 to 50 years, for other
    columns the standard deviation values may be indicative of a skew in the data
    (as the standard deviation is greater than the mean).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Further, we can use the stat package to compute additional statistics such as
    covariance and Pearson's correlation coefficient. The covariance indicates the
    joint variability of two random variables. As we are in the EDA phase, these measures
    can give us indicators of how one variable varies vis-a-vis another one. For example,
    the sign of the covariance indicates the direction of variability between the
    two variables. In the following example, the covariance between age and duration
    of the last contact move in opposite directions, that is as age increases the
    duration decreases. Correlation gives a magnitude for the strength of this relationship
    between these two variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can create cross tabulations or crosstabs between two variables to evaluate
    interrelationships between them. For example, in the following example we create
    a crosstab between age and marital status represented as a 2x2 contingency table.
    From the table, we understand, for a given age the breakup of the total number
    of individuals across the various marital statuses. We can also extract items
    that occur most frequently in data columns of the DataFrame. Here, we choose the
    education level as the column and specify a support level of `0.3`, that is, we
    want to find education levels that occur with a frequency greater than `0.3` (observed
    30% of the time, at a minimum) in the DataFrame. Lastly, we can also compute approximate
    quantiles of the numeric columns in the DataFrame. Here, we compute the same for
    the age column with specified quantile probabilities of `0.25`, `0.5` and `0.75`,
    (a value of `0` is the minimum, `1` is the maximum, and `0.5` is the median).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we use the typed aggregation functions to summarize our data to understand
    it better. In the following statement, we aggregate the results by whether a term
    deposit was subscribed along with the total customers contacted, average number
    of calls made per customer, the average duration of the calls, and the average
    number of previous calls made to such customers. The results are rounded to two
    decimal points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00057.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, executing the following statement gives similar results by customer''s
    age:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: After getting a better understanding of our data by computing basic statistics,
    we shift our focus to identifying outliers in our data.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying data outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An outlier or an anomaly is an observation of the data that deviates significantly
    from other observations in the Dataset. These erroneous outliers can be due to
    errors in the data-collection or variability in measurement. They can impact the
    results significantly so it is imperative to identify them during the EDA process.
  prefs: []
  type: TYPE_NORMAL
- en: However, these techniques define outliers as points, which do not lie in clusters.
    The user has to model the data points using statistical distributions, and the
    outliers are identified depending on how they appear in relation to the underlying
    model. The main problem with these approaches is that during EDA, the user typically
    does not have enough knowledge about the underlying data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'EDA, using a modeling and visualizing approach, is a good way of achieving
    a deeper intuition of our data. Spark MLlib supports a large (and growing) set
    of distributed machine learning algorithms to make this task simpler.  For example,
    we can apply clustering algorithms and visualize the results to detect outliers
    in a combination columns. In the following example, we use the last contact duration,
    in seconds (duration), number of contacts performed during this campaign, for
    this client (campaign), number of days that have passed by after the client was
    last contacted from a previous campaign (pdays) and the previous: number of contacts
    performed before this campaign and for this client (prev) values to compute two
    clusters in our data by applying the k-means clustering algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Other distributed algorithms useful for EDA include classification, regression,
    dimensionality reduction, correlation, and hypothesis testing. More details on
    using Spark SQL and these algorithms are covered in [Chapter 6](part0103.html#3279U0-e9cbc07f866e437b8aa14e841622275c),
    *Using Spark SQL in Machine Learning Applications.*
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing data with Apache Zeppelin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typically, we will generate many graphs to verify our hunches about the data.
    A lot of these quick and dirty graphs used during EDA are, ultimately, discarded.
    Exploratory data visualization is critical for data analysis and modeling. However,
    we often skip exploratory visualization with large data because it is hard. For
    instance, browsers cannot typically cannot handle millions of data points. Hence,
    we have to summarize, sample, or model our data before we can effectively visualize
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, BI tools provided extensive aggregation and pivoting features
    to visualize the data. However, these tools typically used nightly jobs to summarize
    large volumes of data. The summarized data was subsequently downloaded and visualized
    on the practitioner's workstations. Spark can eliminate many of these batch jobs
    to support interactive data visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will explore some basic data visualization techniques using
    Apache Zeppelin. Apache Zeppelin is a web-based tool that supports interactive
    data analysis and visualization. It supports several language interpreters and
    comes with built-in Spark integration. Hence, it is quick and easy to get started
    with exploratory data analysis using Apache Zeppelin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download Appache Zeppelin from [https://zeppelin.apache.org/](https://zeppelin.apache.org/).
    Unzip the package on your hard drive and start Zeppelin using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You should be able to see the Zeppelin home page at: `http://localhost:8080/`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Click on the Create new note link and specify a path and name for your notebook,
    as shown:![](img/00061.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the next step, we paste the same code as in the beginning of this chapter
    to create a DataFrame for our sample Dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00062.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can execute typical DataFrame operations, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we create a table from our DataFrame and execute some SQL on it. The
    results of the SQL statements'' execution can be charted by clicking on the appropriate
    chart-type required. Here, we create bar charts as an illustrative example of
    summarizing and visualizing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can create a scatter plot, as shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can also read the coordinate values of each of the points plotted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Additionally, we can create a textbox that accepts input values to make the
    experience interactive. In the following figure, we create a textbox that can
    accept different values for the age parameter and the bar chart is updated accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can also create drop-down lists where the user can select the
    appropriate option:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And, the table of values or chart automatically gets updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We will explore more advanced visualizations using Spark SQL and SparkR in [Chapter
    8](part0149.html#4E33Q0-e9cbc07f866e437b8aa14e841622275c), *Using Spark SQL with
    SparkR**.* In the next section, we will explore the methods used to generate samples
    from our data.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling data with Spark SQL APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, we need to visualize individual data points to understand the nature
    of our data. Statisticians use sampling techniques extensively for data analysis.
    Spark supports both approximate and exact sample generation. Approximate sampling
    is faster and is often good enough in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore Spark SQL APIs used for generating samples.
    We will work through some examples of generating approximate and exact stratified
    samples, with and without replacement, using the DataFrame/Dataset API and RDD-based
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling with the DataFrame/Dataset API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use the `sampleBy` to create a stratified sample without replacement.
    We can specify the fractions for the percentages of each value to be selected
    in the sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'The size of the sample and the number of record of each type are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00070.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we create a sample with replacement that selects a fraction of rows (10%
    of the total records) using a random seed. Using `sample`  is not guaranteed to
    provide the exact fraction of the total number of records in the Dataset. We also
    print out the numbers of each type of records in the sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will explore sampling methods using RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling with the RDD API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we use RDDs for creating stratified samples with and without
    replacement.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create an RDD from our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00072.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can specify the fractions of each record-type in our sample, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following illustration, we use the `sampleByKey` and `sampleByKeyExact`
    methods to create our samples. The former is an approximate sample while the latter
    is an exact sample. The first parameter specifies whether the sample is generated
    with or without replacement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we print out the total number of records in the population and in each
    of the samples. You will notice that the `sampleByKeyExact` gives you exact numbers
    of records as per the specified fractions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00075.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The sample method can be used to create a random sample containing the specified
    fraction of records in the sample. Next, we create a sample with replacement,
    containing 10% of the total records:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00076.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Other statistical operations, such as hypothesis testing, random data generation,
    visualizing probability distributions, and so on, will be covered in the later
    chapters. In the next section, we will explore our data using Spark SQL for creating
    pivot tables.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark SQL for creating pivot tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pivot tables create alternate views of your data and are commonly used during
    data exploration. In the following example, we demonstrate pivoting using Spark
    DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00077.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following example pivots on housing loan taken and computes the numbers
    by marital status:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next example, we create a DataFrame with appropriate column names for
    the total and average number of calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00079.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following example, we create a DataFrame with appropriate column names
    for the total and average duration of calls for each job category:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00080.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following example, we show pivoting to compute average call duration
    for each job category, while also specifying a subset of marital status:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00081.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following example is the same as the preceding one, except that we split
    the average call duration values by the housing loan field as well in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00082.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we show how you can create a DataFrame of pivot table of term deposits
    subscribed by month, save it to disk, and read it back into a RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00083.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Further, we use the RDD in the preceding step to compute quarterly totals of
    customers who subscribed and did not subscribe to term loans:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00084.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We will introduce a detailed analysis of other types of data, including streaming
    data, large-scale graphs, time-series data, and so on, later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we demonstrated using Spark SQL for exploring Datasets, performing
    basic data quality checks, generating samples and pivot tables, and visualizing
    data with Apache Zeppelin.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will shift our focus to data munging/wrangling. We will
    introduce techniques to handle missing data, bad data, duplicate records, and
    so on. We will also use extensive hands-on sessions for demonstrating the use
    of Spark SQL for common data munging tasks.
  prefs: []
  type: TYPE_NORMAL
