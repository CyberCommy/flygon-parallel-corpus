- en: Chapter 3. Application Fingerprinting with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One important step during web application security assessment is fingerprinting.
    As a security researcher/pentester, we have to be well-versed at fingerprinting,
    which gives lot of information about underlying technology like software or framework
    version, web server info, OS and many more. This helps us to discover all the
    well-known vulnerabilities that are affecting the application and server.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: E-mail gathering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OS fingerprinting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EXIF data extraction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application fingerprinting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web scraping
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though some sites offer APIs, most websites are designed mainly for human
    eyes and only provide HTML pages formatted for humans. If we want a program to
    fetch some data from such a website, we have to parse the markup to get the information
    we need. Web scraping is the method of using a computer program to analyze a web
    page and get the data needed.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many methods to fetch the content from the site with Python modules:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Use `urllib`/`urllib2` to create an HTTP request that will fetch the webpage,
    and using `BeautifulSoup` to parse the HTML
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To parse an entire website we can use Scrapy ([http://scrapy.org](http://scrapy.org)),
    which helps to create web spiders
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use requests module to fetch and lxml to parse
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: urllib / urllib2 module
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Urllib is a high-level module that allows us to script different services such
    as HTTP, HTTPS, and FTP.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Useful methods of urllib/urllib2
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Urllib/urllib2 provide methods that can be used for getting resources from
    URLs, which includes opening web pages, encoding arguments, manipulating and creating
    headers, and many more. We can go through some of those useful methods as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a web page using `urlopen()`. When we pass a URL to `urlopen()` method,
    it will return an object, we can use the `read()` attribute to get the data from
    this object in string format, as follows:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The next method is parameter encoding: `urlencode()`. It takes a dictionary
    of fields as input and creates a URL-encoded string of parameters:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The other method is sending requests with parameters, for example, using a
    GET request: URL is crafted by appending the URL-encoded parameters:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Using the POST request method, the URL-encoded parameters are passed to the
    method `urlopen()` separately:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we use response headers then the HTTP response headers can be retrieved
    using the `info()` method, which will return a dictionary-like object:'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output will look as follows:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Useful methods of urllib/urllib2](img/image_03_001.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: 'We can also use `keys()` to get all the response header keys:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can access each entry as follows:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Urllib does not support cookies and authentication. Also, it only supports GET
    and POST requests. Urllib2 is built upon urllib and has many more features.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the status codes with the code method:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can modify the request headers with `urllib2` as follows:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Cookies can be used as follows:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Requests module
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can also use the requests module instead of `urllib`/`urllib2`, which is
    a better option as it supports a fully REST API and it simply takes a dictionary
    as an argument without any parameters encoded:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parsing HTML using BeautifulSoup
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding modules are only useful to fetch files. If we want to parse HTML
    obtained via `urlopen`, we have to use the `BeautifulSoup` module. `BeautifulSoup`
    takes raw HTML and XML files from `urlopen` and pulls data out of it. To run a
    parser, we have to create a parser object and feed it some data. It will scan
    through the data and trigger the various handler methods. Beautiful Soup 4 works
    on both Python 2.6+ and Python 3.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some simple examples:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'To prettify the HTML, use the following code:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output will be as follows:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Parsing HTML using BeautifulSoup](img/image_03_004.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: 'Some example ways to navigate through the HTML with `BeautifulSoup` are as
    follows:'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Some ways to search through the HTML for tags and properties are as follows:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Download all images on a page
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we can write a script to download all images on a page and save them in
    a specific location:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parsing HTML with lxml
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another powerful, fast, and flexible parser is the HTML Parser that comes with
    lxml. As lxml is an extensive library written for parsing both XML and HTML documents,
    it can handle messed up tags in the process.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with an example.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will use the requests module to retrieve the web page and parse it
    with lxml:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now the whole HTML is saved to `tree` in a nice tree structure that we can
    inspect in two different ways: XPath or CSS Select. XPath is used to navigate
    through elements and attributes to find information in structured documents such
    as HTML or XML.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use any of the page inspect tools, such as Firebug or Chrome developer
    tools, to get the XPath of an element:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![Parsing HTML with lxml](img/image_03_007.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: If we want to get the book names and prices from the  list, find the following
    section in the source.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'From this we can create Xpath as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then we can print the lists using the following code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learn more on lxml at [http://lxml.de](http://lxml.de).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scrapy is an open-source framework for web scraping and web crawling. This can
    be used to parse the whole website. As a framework, this helps to build spiders
    for specific requirements. Other than Scrapy, we can use mechanize to write scripts
    that can fill and submit forms.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: We can utilize the command line interface of Scrapy to create the basic boilerplate
    for new spidering scripts. Scrapy can be installed with `pip`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a new spider, we have to run the following command in the terminal
    after installing Scrapy:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will generate a project folder in the current working directory `testSpider`.
    This will also create a basic structure and files inside the folder for our spider:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '![Scrapy](img/image_03_010.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: 'Scrapy has CLI commands to create a spider. To create a spider, we have to
    enter the folder generated by the `startproject` command:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then we have to enter the generate spider command:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will generate another folder, named `spiders`, and create the required
    files inside that folder. Then, the folder structure will be as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![Scrapy](img/image_03_011.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'Now open the `items.py` file and define a new item in the subclass called `TestspiderItem`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Most of this crawling logic is given by Scrapy in the `pactpub` class inside
    the `spider` folder, so we can extend this to write our `spider`. To do this,
    we have to edit the `pactpub.py` file in the spider folder.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the `pactpub.py` file, first we import the required modules:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, we have to extend the spider class of the Scrapy to define our `pactpubSpider`
    class. Here we can define the domain and initial URLs for crawling:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: After that, we have to define the parse method, which will create an instance
    of `TestspiderItem()` that we defined in the `items.py` file, and assign this
    to the items variable.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Then we can add the items to extract, which can be done with XPATH or CSS style
    selectors.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are using XPATH selector:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now we are ready to run the `spider`. We can run it using the following command:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This will start Scrapy with the URLs we defined and the crawled URLs will be
    passed to the `testspiderItems` and a new instance is created for each item.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: E-mail gathering
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the Python modules discussed previously, we can gather e-mails and other
    information from the web.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: To get e-mail IDs from a website, we may have to write customized scraping scripts.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Here, we discuss a common method of extracting e-mails from a web page with
    Python.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through an example. Here, we are using `BeautifulSoup` and the requests
    module:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we will provide the list of URLs to crawl:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we store the processed URLs in a set so as not to process them twice:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Collected e-mails are also stored in a set:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'When we start scraping, we will take a URL from the queue and process it, and
    add it to the processed URLs. Also, we will do it until the queue is empty:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'With the `urlparse` module we will get the base URL. This will be used to convert
    relative links to absolute links:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The content of the URL will be available from try-catch. In case of error,
    it will go to the next URL:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Inside the response, we will search for the e-mails and add the e-mails found
    to the e-mails set:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'After scraping the page, we will get all the links to other pages and update
    the URL queue:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: OS fingerprinting
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A common process in pentesting is to identify the operating system used by
    the host. Usually, this involves tools like hping or Nmap, and in most cases these
    tools are quite aggressive to obtain such information and may generate alarms
    on the target host. OS fingerprinting mainly falls into two categories: active
    OS fingerprinting and passive OS fingerprinting.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Active fingerprinting is the method of sending packets to a remote host and
    analyzing corresponding responses. In passive fingerprinting, it analyzes packets
    from a host, so it does not send any traffic to the host and acts as a sniffer.
    In passive fingerprinting, it sniffs TCP/IP ports, so it avoids detection or being
    stopped by a firewall. Passive fingerprinting determines the target OS by analyzing
    the initial **Time to Live** (**TTL**) in IP headers packets, and with the TCP
    window size in the first packet of a TCP session. The first packet of TCP session
    is usually either a SYN (synchronize) or SYN/ACK (synchronize and acknowledge)
    packet.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the normal packet specifications for some operating systems:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '| **OS** | **Initial TTL** | **TCP window size** |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| Linux kernel 2.x | 64 milliseconds | 5,840 kilobytes |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: '| Android / Chrome OS | 64 milliseconds | 5,720 kilobytes |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '| Windows XP | 128 milliseconds | 65,535 kilobytes |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| Windows 7/ Server 2008 | 128 milliseconds | 8,192 kilobytes |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: '| Cisco routers (IOS 12.4) | 255 milliseconds | 4,128 kilobytes |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| FreeBSD | 64 milliseconds | 65,535 kilobytes |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: Passive OS fingerprinting is less accurate than the active method, but it helps
    the penetration tester avoid detection.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Another field that is interesting when fingerprinting systems is the **Initial
    Sequence Number** (**ISN**). In TCP, the members of a conversation keep track
    of what data has been seen and what data is to be sent next by using ISN. When
    establishing a connection, each member will select an ISN, and the following packets
    will be numbered by adding one to that number.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy can be used to analyze ISN increments to discover vulnerable systems.
    For that, we will collect responses from the target by sending a number of SYN
    packets in a loop.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the interactive Python interpreter with `sudo` permission and import
    Scrapy:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After collecting some responses, we can print the data for analysis:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This will print out the ISN values for analysis.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have installed Nmap, we can use the active fingerprinting database of
    Nmap with Scapy as follows; make sure we have configured the fingerprinting database
    of Nmap `conf.nmap_base`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Also, we can use `p0f` if it''s installed on our system to guess the OS with
    Scapy:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Get the EXIF data of an image
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can find a lot of information from an image posted online. For every photo
    we took with our smartphone or camera, it records the date, time, shutter speed,
    aperture setting, ISO setting, whether the flash was used, the focal length, and
    lots more. This is stored with the photo, and is referred to as *EXIF* data. When
    we copy an image, the EXIF data is copied as well, as a part of the image. It
    can pose a privacy issue. For instance, a photo taken with a GPS-enabled phone,
    it can reveal the location and time it was taken, as well as the unique ID number
    of the device:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: First we imported the modules `PIL` image and `PIL TAGS`. `PIL` is an image
    processing module in Python. It supports many file formats and has a powerful
    image-processing capability. Then we iterate through the results and print the
    values.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入了`PIL`图像和`PIL TAGS`模块。`PIL`是Python中的图像处理模块。它支持许多文件格式，并具有强大的图像处理能力。然后我们遍历结果并打印数值。
- en: There are many other modules which support EXIF data extraction, like `ExifRead`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他模块支持EXIF数据提取，比如`ExifRead`。
- en: Web application fingerprinting
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Web应用指纹识别
- en: Web application fingerprinting is the main part of the information gathering
    stage in security assessment. It helps us to accurately identify an application
    and to pinpoint known vulnerabilities. This also allows us to customize payload
    or exploitation techniques based on the information. The simplest method is to
    open the site in the browser and look at its source code for specific keywords.
    Similarly, with Python, we can download the page and then run some basic regular
    expressions, which can give you the results.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Web应用指纹识别是安全评估信息收集阶段的主要部分。它帮助我们准确识别应用程序并找出已知的漏洞。这也允许我们根据信息定制有效载荷或利用技术。最简单的方法是在浏览器中打开网站并查看其特定关键字的源代码。同样，使用Python，我们可以下载页面然后运行一些基本的正则表达式，这可以给你结果。
- en: We can download the website with the `urllib`/`requests` module in combination
    with BeautifulSoup or lxml, as we discussed in this chapter.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`urllib`/`requests`模块与BeautifulSoup或lxml结合下载网站，就像我们在本章讨论的那样。
- en: Summary
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed the possible methods of downloading and parsing
    a website. Using the basic methods discussed in this chapter, we can build our
    own scanners and web scrapers.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了下载和解析网站的可能方法。使用本章讨论的基本方法，我们可以构建自己的扫描器和网络爬虫。
- en: In the next chapter we will discuss more attack scripting techniques with Python.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论更多使用Python的攻击脚本技术。
