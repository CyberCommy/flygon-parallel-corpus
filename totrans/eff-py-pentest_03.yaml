- en: Chapter 3. Application Fingerprinting with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One important step during web application security assessment is fingerprinting.
    As a security researcher/pentester, we have to be well-versed at fingerprinting,
    which gives lot of information about underlying technology like software or framework
    version, web server info, OS and many more. This helps us to discover all the
    well-known vulnerabilities that are affecting the application and server.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: E-mail gathering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OS fingerprinting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EXIF data extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application fingerprinting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web scraping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though some sites offer APIs, most websites are designed mainly for human
    eyes and only provide HTML pages formatted for humans. If we want a program to
    fetch some data from such a website, we have to parse the markup to get the information
    we need. Web scraping is the method of using a computer program to analyze a web
    page and get the data needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many methods to fetch the content from the site with Python modules:'
  prefs: []
  type: TYPE_NORMAL
- en: Use `urllib`/`urllib2` to create an HTTP request that will fetch the webpage,
    and using `BeautifulSoup` to parse the HTML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To parse an entire website we can use Scrapy ([http://scrapy.org](http://scrapy.org)),
    which helps to create web spiders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use requests module to fetch and lxml to parse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: urllib / urllib2 module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Urllib is a high-level module that allows us to script different services such
    as HTTP, HTTPS, and FTP.
  prefs: []
  type: TYPE_NORMAL
- en: Useful methods of urllib/urllib2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Urllib/urllib2 provide methods that can be used for getting resources from
    URLs, which includes opening web pages, encoding arguments, manipulating and creating
    headers, and many more. We can go through some of those useful methods as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a web page using `urlopen()`. When we pass a URL to `urlopen()` method,
    it will return an object, we can use the `read()` attribute to get the data from
    this object in string format, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The next method is parameter encoding: `urlencode()`. It takes a dictionary
    of fields as input and creates a URL-encoded string of parameters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The other method is sending requests with parameters, for example, using a
    GET request: URL is crafted by appending the URL-encoded parameters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the POST request method, the URL-encoded parameters are passed to the
    method `urlopen()` separately:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If we use response headers then the HTTP response headers can be retrieved
    using the `info()` method, which will return a dictionary-like object:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Useful methods of urllib/urllib2](img/image_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also use `keys()` to get all the response header keys:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can access each entry as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Urllib does not support cookies and authentication. Also, it only supports GET
    and POST requests. Urllib2 is built upon urllib and has many more features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the status codes with the code method:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can modify the request headers with `urllib2` as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Cookies can be used as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Requests module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can also use the requests module instead of `urllib`/`urllib2`, which is
    a better option as it supports a fully REST API and it simply takes a dictionary
    as an argument without any parameters encoded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parsing HTML using BeautifulSoup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding modules are only useful to fetch files. If we want to parse HTML
    obtained via `urlopen`, we have to use the `BeautifulSoup` module. `BeautifulSoup`
    takes raw HTML and XML files from `urlopen` and pulls data out of it. To run a
    parser, we have to create a parser object and feed it some data. It will scan
    through the data and trigger the various handler methods. Beautiful Soup 4 works
    on both Python 2.6+ and Python 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some simple examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To prettify the HTML, use the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Parsing HTML using BeautifulSoup](img/image_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Some example ways to navigate through the HTML with `BeautifulSoup` are as
    follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Some ways to search through the HTML for tags and properties are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Download all images on a page
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we can write a script to download all images on a page and save them in
    a specific location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parsing HTML with lxml
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another powerful, fast, and flexible parser is the HTML Parser that comes with
    lxml. As lxml is an extensive library written for parsing both XML and HTML documents,
    it can handle messed up tags in the process.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will use the requests module to retrieve the web page and parse it
    with lxml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the whole HTML is saved to `tree` in a nice tree structure that we can
    inspect in two different ways: XPath or CSS Select. XPath is used to navigate
    through elements and attributes to find information in structured documents such
    as HTML or XML.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use any of the page inspect tools, such as Firebug or Chrome developer
    tools, to get the XPath of an element:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Parsing HTML with lxml](img/image_03_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we want to get the book names and prices from the  list, find the following
    section in the source.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'From this we can create Xpath as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can print the lists using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learn more on lxml at [http://lxml.de](http://lxml.de).
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scrapy is an open-source framework for web scraping and web crawling. This can
    be used to parse the whole website. As a framework, this helps to build spiders
    for specific requirements. Other than Scrapy, we can use mechanize to write scripts
    that can fill and submit forms.
  prefs: []
  type: TYPE_NORMAL
- en: We can utilize the command line interface of Scrapy to create the basic boilerplate
    for new spidering scripts. Scrapy can be installed with `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a new spider, we have to run the following command in the terminal
    after installing Scrapy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate a project folder in the current working directory `testSpider`.
    This will also create a basic structure and files inside the folder for our spider:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scrapy](img/image_03_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Scrapy has CLI commands to create a spider. To create a spider, we have to
    enter the folder generated by the `startproject` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we have to enter the generate spider command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate another folder, named `spiders`, and create the required
    files inside that folder. Then, the folder structure will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scrapy](img/image_03_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now open the `items.py` file and define a new item in the subclass called `TestspiderItem`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Most of this crawling logic is given by Scrapy in the `pactpub` class inside
    the `spider` folder, so we can extend this to write our `spider`. To do this,
    we have to edit the `pactpub.py` file in the spider folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the `pactpub.py` file, first we import the required modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we have to extend the spider class of the Scrapy to define our `pactpubSpider`
    class. Here we can define the domain and initial URLs for crawling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: After that, we have to define the parse method, which will create an instance
    of `TestspiderItem()` that we defined in the `items.py` file, and assign this
    to the items variable.
  prefs: []
  type: TYPE_NORMAL
- en: Then we can add the items to extract, which can be done with XPATH or CSS style
    selectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are using XPATH selector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready to run the `spider`. We can run it using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This will start Scrapy with the URLs we defined and the crawled URLs will be
    passed to the `testspiderItems` and a new instance is created for each item.
  prefs: []
  type: TYPE_NORMAL
- en: E-mail gathering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the Python modules discussed previously, we can gather e-mails and other
    information from the web.
  prefs: []
  type: TYPE_NORMAL
- en: To get e-mail IDs from a website, we may have to write customized scraping scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we discuss a common method of extracting e-mails from a web page with
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through an example. Here, we are using `BeautifulSoup` and the requests
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will provide the list of URLs to crawl:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we store the processed URLs in a set so as not to process them twice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Collected e-mails are also stored in a set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'When we start scraping, we will take a URL from the queue and process it, and
    add it to the processed URLs. Also, we will do it until the queue is empty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'With the `urlparse` module we will get the base URL. This will be used to convert
    relative links to absolute links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The content of the URL will be available from try-catch. In case of error,
    it will go to the next URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside the response, we will search for the e-mails and add the e-mails found
    to the e-mails set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'After scraping the page, we will get all the links to other pages and update
    the URL queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: OS fingerprinting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A common process in pentesting is to identify the operating system used by
    the host. Usually, this involves tools like hping or Nmap, and in most cases these
    tools are quite aggressive to obtain such information and may generate alarms
    on the target host. OS fingerprinting mainly falls into two categories: active
    OS fingerprinting and passive OS fingerprinting.'
  prefs: []
  type: TYPE_NORMAL
- en: Active fingerprinting is the method of sending packets to a remote host and
    analyzing corresponding responses. In passive fingerprinting, it analyzes packets
    from a host, so it does not send any traffic to the host and acts as a sniffer.
    In passive fingerprinting, it sniffs TCP/IP ports, so it avoids detection or being
    stopped by a firewall. Passive fingerprinting determines the target OS by analyzing
    the initial **Time to Live** (**TTL**) in IP headers packets, and with the TCP
    window size in the first packet of a TCP session. The first packet of TCP session
    is usually either a SYN (synchronize) or SYN/ACK (synchronize and acknowledge)
    packet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the normal packet specifications for some operating systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **OS** | **Initial TTL** | **TCP window size** |'
  prefs: []
  type: TYPE_TB
- en: '| Linux kernel 2.x | 64 milliseconds | 5,840 kilobytes |'
  prefs: []
  type: TYPE_TB
- en: '| Android / Chrome OS | 64 milliseconds | 5,720 kilobytes |'
  prefs: []
  type: TYPE_TB
- en: '| Windows XP | 128 milliseconds | 65,535 kilobytes |'
  prefs: []
  type: TYPE_TB
- en: '| Windows 7/ Server 2008 | 128 milliseconds | 8,192 kilobytes |'
  prefs: []
  type: TYPE_TB
- en: '| Cisco routers (IOS 12.4) | 255 milliseconds | 4,128 kilobytes |'
  prefs: []
  type: TYPE_TB
- en: '| FreeBSD | 64 milliseconds | 65,535 kilobytes |'
  prefs: []
  type: TYPE_TB
- en: Passive OS fingerprinting is less accurate than the active method, but it helps
    the penetration tester avoid detection.
  prefs: []
  type: TYPE_NORMAL
- en: Another field that is interesting when fingerprinting systems is the **Initial
    Sequence Number** (**ISN**). In TCP, the members of a conversation keep track
    of what data has been seen and what data is to be sent next by using ISN. When
    establishing a connection, each member will select an ISN, and the following packets
    will be numbered by adding one to that number.
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy can be used to analyze ISN increments to discover vulnerable systems.
    For that, we will collect responses from the target by sending a number of SYN
    packets in a loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the interactive Python interpreter with `sudo` permission and import
    Scrapy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'After collecting some responses, we can print the data for analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This will print out the ISN values for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have installed Nmap, we can use the active fingerprinting database of
    Nmap with Scapy as follows; make sure we have configured the fingerprinting database
    of Nmap `conf.nmap_base`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we can use `p0f` if it''s installed on our system to guess the OS with
    Scapy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Get the EXIF data of an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can find a lot of information from an image posted online. For every photo
    we took with our smartphone or camera, it records the date, time, shutter speed,
    aperture setting, ISO setting, whether the flash was used, the focal length, and
    lots more. This is stored with the photo, and is referred to as *EXIF* data. When
    we copy an image, the EXIF data is copied as well, as a part of the image. It
    can pose a privacy issue. For instance, a photo taken with a GPS-enabled phone,
    it can reveal the location and time it was taken, as well as the unique ID number
    of the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: First we imported the modules `PIL` image and `PIL TAGS`. `PIL` is an image
    processing module in Python. It supports many file formats and has a powerful
    image-processing capability. Then we iterate through the results and print the
    values.
  prefs: []
  type: TYPE_NORMAL
- en: There are many other modules which support EXIF data extraction, like `ExifRead`.
  prefs: []
  type: TYPE_NORMAL
- en: Web application fingerprinting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web application fingerprinting is the main part of the information gathering
    stage in security assessment. It helps us to accurately identify an application
    and to pinpoint known vulnerabilities. This also allows us to customize payload
    or exploitation techniques based on the information. The simplest method is to
    open the site in the browser and look at its source code for specific keywords.
    Similarly, with Python, we can download the page and then run some basic regular
    expressions, which can give you the results.
  prefs: []
  type: TYPE_NORMAL
- en: We can download the website with the `urllib`/`requests` module in combination
    with BeautifulSoup or lxml, as we discussed in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the possible methods of downloading and parsing
    a website. Using the basic methods discussed in this chapter, we can build our
    own scanners and web scrapers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we will discuss more attack scripting techniques with Python.
  prefs: []
  type: TYPE_NORMAL
