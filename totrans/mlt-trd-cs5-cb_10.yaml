- en: Chapter 10. Parallel Programming Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will review the common problems that a programmer often
    faces while trying to implement parallel workflow. You will learn about:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Lazy-evaluated shared states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Parallel Pipeline with BlockingCollection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Parallel Pipeline with TPL DataFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Map/Reduce with PLINQ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Patterns in programming mean a concrete and standard solution to a given problem.
    Usually, programming patterns are the result of people gathering experience, analyzing
    the common problems, and providing solutions to these problems.
  prefs: []
  type: TYPE_NORMAL
- en: Since parallel programming has existed for quite a long time, there are many
    different patterns for programming parallel applications. There are even special
    programming languages to make programming of specific parallel algorithms easier.
    However, this is where things start to become increasingly complicated. In this
    book, I will provide a starting point from where you will be able to study parallel
    programming further. We will review very basic, yet very useful, patterns that
    are quite helpful for many common situations in parallel programming.
  prefs: []
  type: TYPE_NORMAL
- en: First is about using a **shared-state object** from multiple threads. I would
    like to emphasize that you should avoid it as much as possible. As we have discussed
    in previous chapters, shared state is really bad when you write parallel algorithms,
    but in many occasions it is inevitable. We will find out how to delay an actual
    computation of an object until it is needed, and how to implement different scenarios
    to achieve thread safety.
  prefs: []
  type: TYPE_NORMAL
- en: The next two recipes will show how to create a structured parallel data flow.
    We will review a concrete case of a producer/consumer pattern, which is called
    as **Parallel Pipeline**. We are going to implement it by just blocking the collection
    first, and then see how helpful is another library from Microsoft for parallel
    programming—**TPL DataFlow**.
  prefs: []
  type: TYPE_NORMAL
- en: The last pattern that we will study is the **Map/Reduce** pattern. In the modern
    world, this name could mean very different things. Some people consider map/reduce
    not as a common approach to any problem but as a concrete implementation for large,
    distributed cluster computations. We will find out the meaning behind the name
    of this pattern and review some examples of how it might work in case of small
    parallel applications.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Lazy-evaluated shared states
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe shows how to program a Lazy-evaluated thread-safe shared state object.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start with this recipe, you will need a running Visual Studio 2012\. There
    are no other prerequisites. The source code for this recipe can be found at `BookSamples`
    `\Chapter10\Recipe1`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For implementing Lazy-evaluated shared states, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start Visual Studio 2012\. Create a new C# **Console Application** project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `Program.cs` file, add the following `using` directives:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following code snippet below the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following code snippet inside the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Run the program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first example show why it is not safe to use the `UnsafeState` object with
    multiple accessing threads. We see that the `Construct` method was called several
    times, and different threads use different values, which is obviously not right.
    To fix this, we can use a lock when reading the value, and if it is not initialized,
    create it first. It will work, but using a lock with every read operation is not
    efficient. To avoid using locks every time, there is a traditional approach called
    the **double-checked locking** pattern. We check the value for the first time,
    and if is not null, we avoid unnecessary locking and just use the shared object.
    However, if it was not constructed yet, we use the lock and then check the value
    for the second time, because it could be initialized between our first check and
    the lock operation. If it is still not initialized, only then we compute the value.
    We can clearly see that this approach works with the second example—there is only
    one call to the `Construct` method, and the first-called thread defines the shared
    object state.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that if the lazy- evaluated object implementation is thread-safe,
    it does not automatically mean that all its properties are thread-safe as well.
  prefs: []
  type: TYPE_NORMAL
- en: If you add, for example, an **int** public property to the `ValueToAccess` object,
    it will not be thread-safe; you still have to use interlocked constructs or locking
    to ensure thread safety.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern is very common, and that is why there are several classes in the
    Base Class Library to help us. First, we can use the `LazyInitializer.EnsureInitialized`
    method, which implements the double-checked locking pattern inside. However, the
    most comfortable option is to use the `Lazy<T>` class that allows us to have thread-safe
    Lazy-evaluated, shared state, out of the box. The next two examples show us that
    they are equivalent to the second one, and the program behaves the same. The only
    difference is that since `LazyInitializer` is a static class, we do not have to
    create a new instance of a class as we do in the case of `Lazy<T>`, and therefore
    the performance in the first case will be better in some scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The last option is to avoid locking at all, if we do not care about the `Construct`
    method. If it is thread-safe and has no side effects and/or serious performance
    impacts, we can just run it several times but use only the first constructed value.
    The last example shows the described behavior, and we can achieve this result
    by using another `LazyInitializer.EnsureInitialized` method overload.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Parallel Pipeline with BlockingCollection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will describe how to implement a specific scenario of a producer/consumer
    pattern, which is called Parallel Pipeline, using the standard `BlockingCollection`
    data structure.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To begin this recipe, you will need a running Visual Studio 2012\. There are
    no other prerequisites. The source code for this recipe can be found at `7644_Code\Chapter10\Recipe2`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand how to implement Parallel Pipeline using `BlockingCollection`,
    perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start Visual Studio 2012\. Create a new C# **Console Application** project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `Program.cs` file, add the following `using` directives:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following code snippet below the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following code snippet inside the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Run the program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding example, we implement one of the most common parallel programming
    scenarios. Imagine that we have some data that has to pass through several computation
    stages, which take a significant amount of time. The latter computation requires
    the results of the former, so we cannot run them in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: If we had only one item to process, there would not be many possibilities to
    enhance the performance. However, if we run many items through the set of same
    computation stages, we can use a Parallel Pipeline technique. This means that
    we do not have to wait until all items pass through the first computation stage
    to go to the next one. It is enough to have just one item that finishes the stage,
    we move it to the next stage, and meanwhile the next item is being processed by
    the previous stage, and so on. As a result, we almost have parallel processing
    shifted by a time required for the first item to pass through the first computation
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use four collections for each processing stage, illustrating that we
    can process every stage in parallel as well. The first step that we do is to provide
    a possibility to cancel the whole process by pressing the *C* key. We create a
    cancellation token and run a separate task to monitor the *C* key. Then, we define
    our pipeline. It consists of three main stages. The first stage is where we put
    the initial numbers on the first four collections that serve as the item source
    to the latter pipeline. This code is inside the `Parallel.For` loop, which in
    turn is inside the `Parallel.Invoke` statement, as we run all the stages in parallel;
    the initial stage runs in parallel as well.
  prefs: []
  type: TYPE_NORMAL
- en: The next stage is defining our pipeline elements. The logic is defined inside
    the `PipelineWorker` class. We initialize the worker with the input collection,
    provide a transformation function, and then run the worker in parallel with the
    other workers. This way we define two workers, or filters, because they filter
    the initial sequence. One of them turns an integer into a decimal value, and the
    second one turns a decimal to a string. Finally, the last worker just prints every
    incoming string to the console. Everywhere we provide a running thread ID to see
    how everything works. Besides this, we added artificial delays, so the items processing
    will be more natural, as we really use heavy computations.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, we see the exact expected behavior. First, some items are being
    created on the initial collections. Then, we see that the first filter starts
    to process them, and as they are being processed, the second filter starts to
    work, and finally the item goes to the last worker that prints it to the console.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Parallel Pipeline with TPL DataFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe shows how to implement a Parallel Pipeline pattern with the help
    of TPL DataFlow library.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start with this recipe, you will need a running Visual Studio 2012\. There
    are no other prerequisites. The source code for this recipe could be found at
    `7644_Code\Chapter10\Recipe3`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand how to implement Parallel Pipeline with TPL DataFlow, perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start Visual Studio 2012\. Create a new C# **Console Application** project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add references to the **Microsoft TPL DataFlow** NuGet package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Right-click on the **References** folder in the project and select the **Manage
    NuGet Packages...** menu option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now add your preferred references to the **Microsoft TPL DataFlow** NuGet package.
    You can use the search option in the **Manage NuGet Packages** dialog as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How to do it...](img/7644OT_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the `Program.cs` file, add the following `using` directives:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following code snippet below the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following code snippet inside the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Run the program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous recipe, we have implemented a Parallel Pipeline pattern to process
    items through sequential stages. It is quite a common problem, and one of the
    proposed ways to program such algorithms is using a TPL DataFlow library from
    Microsoft. It is distributed via **NuGet**, and is easy to install and use in
    your application.
  prefs: []
  type: TYPE_NORMAL
- en: The TPL DataFlow library contains different type of blocks that can be connected
    with each other in different ways and form complicated processes that can be partially
    parallel and sequential where needed. To see some of the available infrastructure,
    let's implement the previous scenario with the help of the TPL DataFlow library.
  prefs: []
  type: TYPE_NORMAL
- en: First, we define the different blocks that will be processing our data. Please
    note that these blocks have different options that can be specified during their
    construction; they can be very important. For example, we pass the cancellation
    token into every block we define, and when we signal the cancellation, all of
    them will stop working.
  prefs: []
  type: TYPE_NORMAL
- en: We start our process with `BufferBlock`. This block holds items to pass it to
    the next blocks in the flow. We restrict it to the five-items capacity, specifying
    the `BoundedCapacity` option value. This means that when there will be five items
    in this block, it will stop accepting new items until one of the existing items
    pass to the next blocks.
  prefs: []
  type: TYPE_NORMAL
- en: The next block type is `TransformBlock`. This block is intended for a data transformation
    step. Here we define two transformation blocks, one of them creates decimals from
    integers, and the second one creates a string from a decimal value. There is a
    `MaxDegreeOfParallelism` option for this block, specifying the maximum simultaneous
    worker threads.
  prefs: []
  type: TYPE_NORMAL
- en: The last block is the `ActionBlock` type. This block will run a specified action
    on every incoming item. We use this block to print our items to the console.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we link these blocks together with the help of the `LinkTo` methods. Here
    we have an easy sequential data flow, but it is possible to create schemes that
    are more complicated. Here we also provide `DataflowLinkOptions` with the `PropagateCompletion`
    property set to `true`. This means that when the step completes, it will automatically
    propagate its results and exceptions to the next stage. Then we start adding items
    to the buffer block in parallel, calling the block's `Complete` method, when we
    finish adding new items. Then we wait for the last block to complete. In case
    of a cancellation, we handle `OperationCancelledException` and cancel the whole
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Map/Reduce with PLINQ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will describe how to implement the **Map**/**Reduce** pattern while
    using PLINQ.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To begin with this recipe, you will need a running Visual Studio 2012\. There
    are no other prerequisites. The source code for this recipe can be found at `7644_Code\Chapter10\Recipe4`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand how to implement Map/Reduce with PLINQ, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start Visual Studio 2012\. Create a new C# **Console Application** project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `Program.cs` file, add the following `using` directives:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following code snippet below the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following code snippet inside the `Main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following code snippet after the `Program` class definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Run the program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Map`/`Reduce` functions are another important parallel programming pattern.
    It is suitable for a small program and large multi-server computations. The meaning
    of this pattern is that you have two special functions to apply to your data.
    The first of them is the `Map` function. It takes a set of initial data in a key/value
    list form and produces another key/value sequence, transforming the data to the
    comfortable format for further processing. Then we use another function called
    `Reduce`**.** The `Reduce` function takes the result of the `Map` function and
    transforms it to a smallest possible set of data that we actually need. To understand
    how this algorithm works, let's look through the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define a relatively large text in the string variable: `textToParse`.
    We need this text to run our queries on. Then we define our `Map`/`Reduce` implementation
    as a PLINQ extension method in the `PLINQExtensions` class. We use `SelectMany`
    to transform the initial sequence to the sequence we need by applying the `Map`
    function. This function produces several new elements from one sequence element.
    Then we choose how we group the new sequence with the `keySelector` function,
    and we use `GroupBy` with this key to produce an intermediate key/value sequence.
    The last thing we do is applying `Reduce` to the resulting grouped sequence to
    get the result.'
  prefs: []
  type: TYPE_NORMAL
- en: In our first example, we split the text into separate words, and then we chop
    each word into character sequences with the help of the `Map` function, and group
    the result by the character value. The `Reduce` function finally transforms the
    sequence into a key value pair, where we have a character and a number for the
    times it was used in the text ordered by the usage. Therefore, we are able to
    count each character appearance in the text in parallel (since we use PLINQ to
    query the initial data).
  prefs: []
  type: TYPE_NORMAL
- en: The next example is quite similar, but now we use PLINQ to filter the sequence
    leaving only the words containing our search pattern, and we then get all those
    words sorted by their usage in the text.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the last example uses file I/O. We save the sample text on the disk,
    splitting it into two files. Then we define the `Map` function as producing a
    number of strings from the directory name, which are all the words from all the
    lines in all text files in the initial directory. Then we group those words by
    the first letter (filtering out the empty strings) and use reduce to see which
    letter is most often used as the first word letter in the text. What is nice is
    that we can easily change this program to be distributed by just using other implementations
    of map and reduce functions, and we still are able to use PLINQ with them to make
    our program easy to read and maintain.
  prefs: []
  type: TYPE_NORMAL
