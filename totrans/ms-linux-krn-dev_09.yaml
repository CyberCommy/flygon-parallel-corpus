- en: Interrupts and Deferred Work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An **interrupt** is an electrical signal delivered to the processor indicating
    occurrence of a significant event that needs immediate attention. These signals
    can originate either from external hardware (connected to the system) or from
    circuits within the processor. In this chapter we will look into the kernel''s
    interrupt management subsystem and explore the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Programmable interrupt controllers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interrupt vector table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IRQs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IRQ chip and IRQ descriptors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registering and unregistering interrupt handlers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IRQ line-control operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IRQ stacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Need for deferred routines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softirqs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasklets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workqueues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interrupt signals and vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When an interrupt originates from an external device, it is referred to as a
    **hardware interrupt**. These signals are generated by external hardware to seek
    the attention of the processor on occurrence of a significant external event,
    for instance a key hit on the keyboard, a click on a mouse button, or moving the
    mouse trigger hardware interrupts through which the processor is notified about
    the availability of data to be read. Hardware interrupts occur asynchronously
    with respect to the processor clock (meaning they can occur at random times),
    and hence are also termed as **asynchronous interrupts**.
  prefs: []
  type: TYPE_NORMAL
- en: Interrupts triggered from within the CPU due to events generated by program
    instructions currently in execution are referred to as **software interrupts**.
    A software interrupt is caused either by an **exception** triggered by program
    instructions currently in execution or on execution of a privileged instruction
    that raises an interrupt. For instance, when a program instruction attempts to
    divide a number by zero, the arithmetic logic unit of the processor raises an
    interrupt called a divide-by-zero exception. Similarly, when a program in execution
    intends to invoke a kernel service call, it executes a special instruction (sysenter)
    that raises an interrupt to shift the processor into privileged mode, which paves
    the path for the execution of the desired service call. These events occur synchronously
    with respect to the processor's clock and hence are also called **synchronous
    interrupts**.
  prefs: []
  type: TYPE_NORMAL
- en: In response to the occurrence of an interrupt event, CPUs are designed to preempt
    the current instruction sequence or thread of execution, and execute a special
    function called **interrupt service routine** (**ISR**). To locate the appropriate
    ***ISR*** that corresponds to an interrupt event, **interrupt vector tables**
    are used. An **interrupt vector** is an address in memory that contains a reference
    to a software-defined **interrupt service** to be executed in response to an interrupt.
    Processor architectures define the total count of **interrupt vectors** supported,
    and describe the layout of each interrupt vector in memory. In general, for most
    processor architectures, all supported vectors are set up in memory as a list
    called an **interrupt vector table,** whose address is programmed into a processor
    register by the platform software.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider specifics of the *x86* architecture as an example for better
    understanding. The x86 family of processors supports a total of 256 interrupt
    vectors, of which the first 32 are reserved for processor exceptions and the rest
    used for software and hardware interrupts. Implementation of a vector table by
    x86 is referred to as an **interrupt descriptor table (IDT)**, which is an array
    of descriptors of either 8 byte (for 32-bit machines) or 16 byte (for 64-bit *x86*
    machines) sizes. During early boot, the architecture-specific branch of the kernel
    code sets up the **IDT** in memory and programs the **IDTR** register (special
    x86 register) of the processor with the physical start address and length of the
    **IDT**. When an interrupt occurs, the processor locates relevant vector descriptors
    by multiplying the reported vector number by the size of the vector descriptor
    (*vector number x 8* on x86_32 machines, and *vector no x 16* on x86_64 machines)
    and adding the result to the base address of the **IDT.** Once a valid *vector
    descriptor* is reached, the processor continues with the execution of actions
    specified within the descriptor.
  prefs: []
  type: TYPE_NORMAL
- en: On x86 platforms, each *vector descriptor* implements a *gate* (interrupt, task,
    or trap)*,* which is used to transfer control of execution across segments. Vector
    descriptors representing hardware interrupts implement an *interrupt gate,* which
    refers to the base address and offset of the segment containing interrupt handler
    code. An *interrupt gate* disables all maskable interrupts before passing control
    to a specified interrupt handler. Vector descriptors representing *exceptions*
    and software interrupts implement a *trap gate,* which also refers to the location
    of code designated as a handler for the event. Unlike an *interrupt gate*, a *trap
    gate* does not disable maskable interrupts, which makes it suitable for execution
    of soft interrupt handlers.
  prefs: []
  type: TYPE_NORMAL
- en: Programmable interrupt controller
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s focus on external interrupts and explore how processors identify
    the occurrence of an external hardware interrupt, and how they discover the vector
    number associated with the interrupt. CPUs are designed with a dedicated input
    pin (intr pin) used to signal external interrupts. Each external hardware device
    capable of issuing interrupt requests usually consists of one or more output pins
    called **Interrupt Request lines (IRQ)**, used to signal an interrupt request
    on the CPU. All computing platforms use a hardware circuit called a **programmable
    interrupt controller (PIC)** to multiplex the CPU''s interrupt pin across various
    interrupt request lines. All of the existing IRQ lines originating from on-board
    device controllers are routed to input pins of the interrupt controller, which
    monitors each IRQ line for an interrupt signal, and upon arrival of an interrupt,
    converts the request into a cpu-understandable vector number and relays the interrupt
    signal on to the CPU''s interrupt pin. In simple words, a programmable interrupt
    controller multiplexes multiple device interrupt request lines into a single interrupt
    line of the processor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Design and implementation of interrupt controllers is platform specific*.*
    Intel *x86* multiprocessor platforms use **Advanced Programmable Interrupt Controller**
    (**APIC**). The **APIC** design splits interrupt controller functionality into
    two distinct chipsets: the first component is an **I/O APIC** that resides on
    the system bus. All shared peripheral hardware IRQ lines are routed to the I/O
    APIC; this chip translates an interrupt request into vector code***.*** The second
    is a per-CPU controller called **Local APIC** (usually integrated into the processor
    core) which delivers hardware interrupts to specific CPU cores. **I/O APIC** routes
    the interrupt events to a **Local APIC** of the chosen CPU core. It is programmed
    with a redirection table, which is used for making interrupt routing decisions.
    CPU **Local APICs** manage all external interrupts for a specific CPU core; additionally,
    they deliver events from CPU local hardware such as timers and can also receive
    and generate **inter-processor interrupts** **(IPIs)** that can occur on an SMP
    platform.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts the split architecture of **APIC**. The flow
    of events now begins with individual devices raising IRQ on the **I/O APIC**,
    which routes the request to a specific **Local APIC**, which in turn delivers
    the interrupt to a specific CPU core:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Similar to the **APIC** architecture, multicore ARM platforms split the **generic
    interrupt controller** (**GIC**) implementation into two. The first component
    is called a **distributor,** which is global to the system and has several peripheral
    hardware interrupt sources physically routed to it. The second component is replicated
    per-CPU and is called the **cpu interface**. The *distributor* component is programmed
    with distribution logic of **shared peripheral interrupts**(***SPI)*** to known
    CPU interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Interrupt controller operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The architecture-specific branch of the kernel code implements interrupt controller
    specific operations for management of IRQ lines such as masking/unmasking individual
    interrupts, setting priorities, and SMP affinity. These operations are required
    to be invoked from architecture-independent code paths of the kernel for manipulation
    of individual IRQ lines, and to facilitate such calls, the kernel defines an architecture-independent
    abstraction layer through a structure called `struct irq_chip`. This structure
    can be found in the kernel header `<include/linux/irq.h>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The structure declares a set of function pointers to account for all peculiarities
    of IRQ chips found across various hardware platforms. Thus, a particular instance
    of the structure defined by board-specific code usually supports only a subset
    of possible operations. Following are x86 multicore platform versions of `irq_chip`
    instances defining operations of I/O APIC and LAPIC.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: IRQ descriptor table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another important abstraction is with respect to IRQ numbers associated with
    hardware interrupts. Interrupt controllers identify each IRQ source with a unique
    hardware IRQ number. The kernel's generic interrupt-management layer maps each
    hardware IRQ to a unique identifier called Linux IRQ; these numbers abstract hardware
    IRQs, thereby ensuring portability of kernel code. All of the peripheral device
    drivers are programmed to use the Linux IRQ number to bind or register their interrupt
    handlers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linux IRQs are represented by IRQ descriptor structure, which is defined by
    `struct irq_desc`; for each IRQ source, an instance of this structure is enumerated
    during early kernel boot. A list of IRQ descriptors is maintained in an array
    indexed by the IRQ number, called the IRQ descriptor table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`irq_data` is an instance of `struct irq_data`, and this contains low-level
    information that is relevant for interrupt management, such as Linux IRQ number,
    hardware IRQ number, and a pointer to interrupt controller operations (`irq_chip`)
    among other important fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `handle_irq` element of the `irq_desc` structure is a function pointer of
    type `irq_flow_handler_t`, which refers to a high-level function that deals with
    flow management on the line. The generic irq layer provides as set of predefined
    irq flow functions; an appropriate routine is assigned to each interrupt line
    based on its type.
  prefs: []
  type: TYPE_NORMAL
- en: '`handle_level_irq()`: Generic implementation for level-triggered interrupts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`handle_edge_irq()`: Generic implementation for edge-triggered interrupts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`handle_fasteoi_irq()`: Generic implementation for interrupts that only need
    an EOI at the end of the handler'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`handle_simple_irq()`: Generic implementation for simple interrupts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`handle_percpu_irq()`: Generic implementation for per-CPU interrupts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`handle_bad_irq()`: Used for spurious interrupts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `*action` element of the `irq_desc` structure is a pointer to one or a
    chain of action descriptors, which contain driver-specific interrupt handlers
    among other important elements. Each action descriptor is an instance of `struct
    irqaction` defined in the kernel header `<linux/interrupt.h>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: High-level interrupt-management interfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The generic IRQ layer provides a set of function interfaces for device drivers
    to grab IRQ descriptors and bind interrupt handlers, release IRQs, enable or disable
    interrupt lines, and so on. We will explore all of the generic interfaces in this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Registering an interrupt handler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`request_irq()` instantiates an `irqaction` object with values passed as parameters
    and binds it to the `irq_desc` specified as the first (`irq`) parameter. This
    call allocates interrupt resources and enables the interrupt line and IRQ handling.
    `handler` is a function pointer of type `irq_handler_t`, which takes the address
    of a driver-specific interrupt handler routine. `flags` is a bitmask of options
    related to interrupt management. Flag bits are defined in the kernel header `<linux/interrupt.h>:`'
  prefs: []
  type: TYPE_NORMAL
- en: '`IRQF_SHARED`: Used while binding an interrupt handler to a shared IRQ line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_PROBE_SHARED`: Set by callers when they expect sharing mismatches to
    occur.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_TIMER`: Flag to mark this interrupt as a timer interrupt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_PERCPU`: Interrupt is per CPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_NOBALANCING`: Flag to exclude this interrupt from IRQ balancing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_IRQPOLL`: Interrupt is used for polling (only the interrupt that is registered
    first in a shared interrupt is considered for performance reasons).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_NO_SUSPEND`: Do not disable this IRQ during suspend. Does not guarantee
    that this interrupt will wake the system from a suspended state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_FORCE_RESUME`: Force-enable it on resume even if `IRQF_NO_SUSPEND` is
    set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_EARLY_RESUME`: Resume IRQ early during syscore instead of at device resume
    time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_COND_SUSPEND`: If the IRQ is shared with a `NO_SUSPEND` user, execute
    this interrupt handler after suspending interrupts. For system wakeup devices,
    users need to implement wakeup detection in their interrupt handlers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since each flag value is a bit, a logical OR (that is, |) of a subset of these
    can be passed, and if none apply, then a value 0 for the `flags` parameter is
    valid. The address assigned to `dev` is considered as a unique cookie and serves
    as an identifier for the action instance in a shared IRQ case. The value of this
    parameter can be NULL while registering interrupt handlers without the `IRQF_SHARED`
    flag.
  prefs: []
  type: TYPE_NORMAL
- en: On success, `request_irq()` returns zero; a nonzero return value indicates failure
    to register the specified interrupt handler. The return error code `-EBUSY` denotes
    failure to register or bind the handler to a specified IRQ that is already in
    use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interrupt handler routines have the following prototype:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`irq` specifies the IRQ number, and `dev_id` is the unique cookie used while
    registering the handler. `irqreturn_t` is a typedef to an enumerated integer constant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The interrupt handler should return `IRQ_NONE` to indicate that the interrupt
    was not handled. It is also used to indicate that the source of the interrupt
    was not from its device in a shared IRQ case. When interrupt handling has completed
    normally, it must return `IRQ_HANDLED` to indicate success. `IRQ_WAKE_THREAD`
    is a special flag, returned to wake up the threaded handler; we elaborate on it
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deregistering an interrupt handler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A driver''s interrupt handlers can be deregistered through a call to the `free_irq()`
    routine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`dev_id` is the unique cookie (assigned while registering the handler) to identify
    the handler to be deregistered in a shared IRQ case; this argument can be NULL
    for other cases. This function is a potential blocking call, and must not be invoked
    from an interrupt context: it blocks calling context until completion of any interrupt
    handler currently in execution, for the specified IRQ line.'
  prefs: []
  type: TYPE_NORMAL
- en: Threaded interrupt handlers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Handlers registered through `request_irq()` are executed by the interrupt-handling
    path of the kernel. This code path is asynchronous, and runs by suspending scheduler
    preemption and hardware interrupts on the local processor, and so is referred
    to as a hard IRQ context. Thus, it is imperative to program the driver''s interrupt
    handler routines to be short (do as little work as possible) and atomic (non blocking),
    to ensure responsiveness of the system. However, not all hardware interrupt handlers
    can be short and atomic: there are a magnitude of convoluted devices generating
    interrupt events, whose responses involve complex variable-time operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Conventionally, drivers are programmed to handle such complications with a split-handler
    design for the interrupt handler, called **top half** and **bottom half**. Top
    half routines are invoked in hard interrupt context, and these functions are programmed
    to execute *interrupt critical* operations, such as physical I/O on the hardware
    registers, and schedule the bottom half for deferred execution. Bottom half routines
    are usually programmed to deal with the rest of the *interrupt non-critical* and
    *deferrable work*, such as processing of data generated by the top half, interacting
    with process context, and accessing user address space. The kernel offers multiple
    mechanisms for scheduling and execution of bottom half routines, each with a distinct
    interface API and policy of execution. We'll elaborate on the design and usage
    details of formal bottom half mechanisms in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an alternative to using formal bottom-half mechanisms, the kernel supports
    setting up interrupt handlers that can execute in a thread context, called **threaded
    interrupt handlers***.* Drivers can set up threaded interrupt handlers through
    an alternate interface routine called `request_threaded_irq()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The function assigned to `handler` serves as the primary interrupt handler
    that executes in a hard IRQ context. The routine assigned to `thread_fn` is executed
    in a thread context, and is scheduled to run when the primary handler returns
    `IRQ_WAKE_THREAD`. With this split handler setup, there are two possible use cases:
    the primary handler can be programmed to execute interrupt-critical work and defer
    non-critical work to the thread handler for later execution, similar to that of
    the bottom half. The alternative is a design that defers the entire interrupt-handling
    code into the thread handler and restricts the primary handler only for verification
    of the interrupt source and waking up thread routine. This use case might require
    the corresponding interrupt line to be masked until completion of the thread handler,
    to avoid the nesting of interrupts. This can be accomplished either by programming
    the primary handler to turn off the interrupt at source before waking up the thread
    handler or through a flag bit `IRQF_ONESHOT` assigned while registering the threaded
    interrupt handler.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are `irqflags` related to threaded interrupt handlers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`IRQF_ONESHOT`: The interrupt is not re-enabled after the hard IRQ handler
    is finished. This is used by threaded interrupts that need to keep the IRQ line
    disabled until the threaded handler has been run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_NO_THREAD`: The interrupt cannot be threaded. This is used in shared
    IRQs to restrict the use of threaded interrupt handlers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A call to this routine with NULL assigned to `handler` will cause the kernel
    to use the default primary handler, which simply returns `IRQ_WAKE_THREAD`. And
    a call to this function with NULL assigned to `thread_fn` is synonymous with `request_irq()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Another alternate interface for setting up an interrupt handler is `request_any_context_irq()`.
    This routine has a similar signature to that of `requeust_irq()` but slightly
    varies in its functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This function differs from `request_irq()` in that it looks into the IRQ descriptor
    for properties of the interrupt line as set up by the architecture-specific code,
    and decides whether to establish the function assigned as a traditional hard IRQ
    handler or as a threaded interrupt handler. On success, `IRQC_IS_HARDIRQ` is returned
    if the handler was established to run in hard IRQ context, or `IRQC_IS_NESTED`
    otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Control interfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The generic IRQ layer provides routines to carry out control operations on
    IRQ lines. Following is the list of functions for masking and unmasking specific
    IRQ lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This disables the specified IRQ line by manipulating the counter in the IRQ
    descriptor structure. This routine is a possible blocking call, as it waits until
    any running handlers for this interrupt complete. Alternatively, the function
    `disable_irq_nosync()` can also be used to *disable* the given IRQ line; this
    call does not check and wait for any running handlers for the given interrupt
    line to complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Disabled IRQ lines can be enabled with a call to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that IRQ enable and disable operations nest, that is, multiple calls to
    *disable* an IRQ line require the same number of *enable* calls for that IRQ line
    to be reenabled. This means that `enable_irq()` will enable the given IRQ only
    when a call to it matches the last *disable* operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'By choice, interrupts can also be disabled/enabled for the local CPU; the following
    pairs of macros can be used for the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '`local_irq_disable()`: To disable interrupts on the local processor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_irq_enable()`: Enables interrupts for the local processor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_irq_save(unsigned long flags)`: Disables interrupts on the local CPU
    by saving current interrupt state in *flags*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_irq_restore(unsigned long flags)`: Enables interrupts on the local CPU
    by restoring interrupts to a previous state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IRQ stacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Historically, for most architectures, interrupt handlers shared the kernel
    stack of the running process that was interrupted. As discussed in the first chapter,
    the process kernel stack is typically 8 KB for 32-bit architectures and 16 KB
    for 64-bit architectures. A fixed kernel stack might not always be enough for
    kernel work and IRQ processing routines, resulting in judicious allocation of
    data both by kernel code and interrupt handlers. To address this, the kernel build
    (for a few architectures) is configured by default to set up an additional per-CPU
    hard IRQ stack for use by interrupt handlers, and a per-CPU soft IRQ stack for
    use by software interrupt code. Following are the x86-64 bit architecture-specific
    stack declarations in kernel header `<arch/x86/include/asm/processor.h>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Apart from these, x86-64-bit builds also include special stacks; more details
    can be found in the kernel source documentation `<x86/kernel-stacks>`:'
  prefs: []
  type: TYPE_NORMAL
- en: Double fault stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debug stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NMI stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mce stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deferred work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As introduced in an earlier section, **bottom halves** are kernel mechanisms
    for executing deferred work, and can be engaged by any kernel code to defer execution
    of non-critical work until some time in the future. To support implementation
    and for management of deferred routines, the kernel implements special frameworks,
    called **softirqs**, **tasklets**, and **work queues**. Each of these frameworks
    constitute a set of data structures, and function interfaces, used for registering,
    scheduling, and queuing of the bottom half routines. Each mechanism is designed
    with a distinct *policy* for management and execution of bottom halfs. Drivers
    and other kernel services that require deferred execution will need to bind and
    schedule their BH routines through the appropriate framework.
  prefs: []
  type: TYPE_NORMAL
- en: Softirqs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term **softirq** loosely translates to **soft interrupt**, and as the name
    suggests, deferred routines managed by this framework are executed at a high priority
    but with hard interrupt lines enabled*.* Thus*,* softirq bottom halves (or softirqs)
    can preempt all other tasks except hard interrupt handlers. However, usage of
    softirqs is restricted to static kernel code and this mechanism is not available
    for dynamic kernel modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each softirq is represented through an instance of type `struct softirq_action`
    declared in the kernel header `<linux/interrupt.h>`. This structure contains a
    function pointer that can hold the address of the bottom half routine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Current versions of the kernel have 10 softirqs, each indexed through an enum
    in the kernel header `<linux/interrupt.h>`. These indexes serve as an identity
    and are treated as the relative priority of the softirq, and entries with lower
    indexes are considered higher in priority, with index 0 being the highest priority
    softirq:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The kernel source file `<kernel/softirq.c>` declares an array called `softirq_vec`
    of size `NR_SOFTIRQS`, with each offset containing a `softirq_action` instance
    of the corresponding softirq indexed in the enum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Framework provides a function `open_softriq()` used for initializing the softirq
    instance with the corresponding bottom-half routine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`nr` is the index of the softirq to be initialized and `*action` is a function
    pointer to be initialized with the address of the bottom-half routine. The following
    code excerpt is taken from the timer service, and shows the invocation of `open_softirq`
    to register a softirq:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Kernel services can signal the execution of softirq handlers using a function
    `raise_softirq()`. This function takes the index of the softirq as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code excerpt is from `<kernel/time/timer.c>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The kernel maintains a per-CPU bitmask for keeping track of softirqs raised
    for execution, and the function `raise_softirq()` sets the corresponding bit (index
    mentioned as argument) in the local CPUs softirq bitmask to mark the specified
    softirq as pending.
  prefs: []
  type: TYPE_NORMAL
- en: Pending softirq handlers are checked and executed at various points in the kernel
    code. Principally, they are executed in the interrupt context, immediately after
    the completion of hard interrupt handlers with IRQ lines enabled. This guarantees
    swift processing of softirqs raised from hard interrupt handlers, resulting in
    optimal cache usage. However, the kernel allows an arbitrary task to suspend execution
    of softirq processing on a local processor either through `local_bh_disable()`
    or `spin_lock_bh()` calls. Pending softirq handlers are executed in the context
    of an arbitrary task that re-enables softirq processing by invoking either `local_bh_enable()`
    or `spin_unlock_bh()` calls. And lastly, softirq handlers can also be executed
    by a per-CPU kernel thread `ksoftirqd`*,* which is woken up when a softirq is
    raised by any process-context kernel routine. This thread is also woken up from
    the interrupt context when too many softirqs accumulate due to high load.
  prefs: []
  type: TYPE_NORMAL
- en: Softirqs are most suitable for completion of priority work deferred from hard
    interrupt handlers since they run immediately on completion of hard interrupt
    handlers. However, softirqs handlers are reentrant, and must be programmed to
    engage appropriate protection mechanisms while accessing data structures, if any.
    The reentrant nature of softirqs may cause unbounded latencies, impacting the
    efficiency of the system as a whole, which is why their usage is restricted, and
    new ones are almost never added, unless it is absolute necessity for the execution
    of high-frequency threaded deferred work. For all other types of deferred work,
    tasklets and work queues are suggested.
  prefs: []
  type: TYPE_NORMAL
- en: Tasklets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **tasklet** mechanism is a sort of wrapper around the softirq framework;
    in fact, tasklet handlers are executed by softirqs. Unlike softirqs, tasklets
    are not reentrant, which guarantees that the same tasklet handler can never run
    concurrently. This helps minimize overall latencies, provided programmers examine
    and impose relevant checks to ensure that work done in a tasklet is non-blocking
    and atomic. Another difference is with respect to their usage: unlike softirqs
    (which are restricted), any kernel code can use tasklets, and this includes dynamically
    linked services.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each tasklet is represented through an instance of type `struct tasklet_struct`
    declared in kernel header `<linux/interrupt.h>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Upon initialization, `*func` holds the address of the handler routine and `data`
    is used to pass a data blob as a parameter to the handler routine during invocation.
    Each tasklet carries a `state`, which can be either `TASKLET_STATE_SCHED`, which
    indicates that it is scheduled for execution, or `TASKLET_STATE_RUN`, which indicates
    it is in execution. An atomic counter is used to *enable* or *disable* a tasklet;
    when `count` equals a *non-zero* value*,* it indicates that the tasklet is *disabled,*
    and *zero* indicates that it is *enabled*. A disabled tasklet cannot be executed
    even if scheduled, until it is enabled at some future time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kernel services can instantiate a new tasklet statically through any of the
    following macros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'New tasklets can be instantiated dynamically at runtime through the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The kernel maintains two per-CPU tasklet lists for queuing scheduled tasklets,
    and the definitions of these lists can be found in the source file `<kernel/softirq.c>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`tasklet_vec` is considered normal list, and all queued tasklets present in
    this list are run by `TASKLET_SOFTIRQ` (one of the 10 softirqs). `tasklet_hi_vec`
    is a high-priority tasklet list, and all queued tasklets present in this list
    are executed by `HI_SOFTIRQ`, which happens to be the highest priority softirq.
    A tasklet can be queued for execution into the appropriate list by invoking `tasklet_schedule()`
    or `tasklet_hi_scheudule()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the implementation of `tasklet_schedule()`; this function
    is invoked with the address of the tasklet instance to be queued as a parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The conditional construct checks if the specified tasklet is already scheduled;
    if not, it atomically sets the state to `TASKLET_STATE_SCHED` and invokes `__tasklet_shedule()`
    to enqueue the tasklet instance into the pending list. If the specified tasklet
    is already found to be in the `TASKLET_STATE_SCHED` state, it is not rescheduled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This function silently enqueues the specified tasklet to the tail of the `tasklet_vec`
    and raises the `TASKLET_SOFTIRQ` on the local processor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the code for the `tasklet_hi_scheudle()` routine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Actions executed in this routine are similar to that of `tasklet_schedule()`,
    with an exception that it invokes `__tasklet_hi_scheudle()` to enqueue the specified
    tasklet into the tail of `tasklet_hi_vec`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This call raises `HI_SOFTIRQ` on the local processor, which turns all tasklets
    queued in `tasklet_hi_vec` into the highest-priority bottom halves (higher in
    priority over the rest of the softirqs).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another variant is `tasklet_hi_schedule_first()`, which inserts the specified
    tasklet to the head of `tasklet_hi_vec` and raises `HI_SOFTIRQ`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Other interface routines exist that are used to enable, disable, and kill scheduled
    tasklets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This function disables the specified tasklet by incrementing its *disable count*.
    The tasklet may still be scheduled, but it is not executed until it has been enabled
    again. If the tasklet is currently running when this call is invoked, this function
    busy-waits until the tasklet completes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This attempts to enable a tasklet that had been previously disabled by decrementing
    its *disable count*. If the tasklet has already been scheduled, it will run soon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This function is called to kill the given tasklet, to ensure that the it cannot
    be scheduled to run again. If the tasklet specified is already scheduled by the
    time this call is invoked, then this function waits until its execution completes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This function is called to kill an already scheduled tasklet. It immediately
    removes the specified tasklet from the list even if the tasklet is in the `TASKLET_STATE_SCHED`
    state.
  prefs: []
  type: TYPE_NORMAL
- en: Workqueues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Workqueues** (**wqs**) are mechanisms for the execution of asynchronous process
    context routines. As the name aptly suggests, a workqueue (wq) is a list of *work*
    items*,* each containing a function pointer that takes the address of a routine
    to be executed asynchronously. Whenever some kernel code (that belongs to a subsystem
    or a service) intends to defer some work for asynchronous process context execution,
    it must initialize the *work* item with the address of the handler function, and
    enqueue it onto a workqueue. The kernel uses a dedicated pool of kernel threads,
    called *kworker* threads, to execute functions bound to each *work* item in the
    queue, sequentially.'
  prefs: []
  type: TYPE_NORMAL
- en: Interface API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The workqueue API offers two types of functions interfaces: first, a set of
    interface routines to instantiate and queue *work* items onto a global workqueue,
    which is shared by all kernel subsystems and services, and second, a set of interface
    routines to set up a new workqueue, and queue work items onto it. We will begin
    to explore workqueue interfaces with macros and functions related to the global
    shared workqueue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each *work* item in the queue is represented by an instance of type `struct
    work_struct`, which is declared in the kernel header `<linux/workqueue.h>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '`func` is a pointer that takes the address of the deferred routine; a new struct
    work object can be created and initialized through macro `DECLARE_WORK`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '`n` is the name of the instance to be created and `f` is the address of the
    function to be assigned. A work instance can be scheduled into the workqueue through
    `schedule_work()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This function enqueues the given *work* item on the local CPU workqueue, but
    does not guarantee its execution on it. It returns *true* if the given *work*
    is successfully enqueued, or *false* if the given *work* is already found in the
    workqueue. Once queued, the function associated with the *work* item is executed
    on any of the available CPUs by the relevant `kworker` thread. Alternatively,
    a *work* item can be marked for execution on a specific CPU, while scheduling
    it into the queue (which might yield better cache utilization); this can be done
    with a call to `scheudule_work_on()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '`cpu` is the identifier to which the *work* task is to be bound. For instance,
    to schedule a *work* task onto a local CPU, the caller can invoke:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '`smp_processor_id()` is a kernel macro (defined in `<linux/smp.h>`) that returns
    the local CPU identifier.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The interface API also offers a variant of scheduling calls, which allow the
    caller to queue *work* tasks whose execution is guaranteed to be delayed at least
    until a specified timeout. This is achieved by binding a *work* task with a timer,
    which can be initialized with an expiry timeout, until which time the *work* task
    is not scheduled into the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '`timer` is an instance of a dynamic timer descriptor, which is initialized
    with the expiry interval and armed while scheduling a *work* task. We''ll discuss
    kernel timers and other time-related concepts more in the next chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Callers can instantiate `delayed_work` and initialize it statically through
    a macro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to normal *work* tasks, delayed *work* tasks can be scheduled to run
    on any of the available CPUs or be scheduled to execute on a specified core. To
    schedule delayed *work* that can run on any of the available processors, callers
    can invoke `schedule_delayed_work()`, and to schedule delayed *work* onto specific
    CPUs, use the function `schedule_delayed_work_on()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Note that if the delay is zero, then the specified *work* item is scheduled
    for immediate execution.
  prefs: []
  type: TYPE_NORMAL
- en: Creating dedicated workqueues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Timing of the execution of *work* items scheduled onto the global workqueue
    is not predictable: one long-running *work* item can always cause indefinite delays
    for the rest. Alternatively, the workqueue framework allows the allocation of
    dedicated workqueues, which can be owned by a kernel subsystem or a service. Interface
    APIs used to create and schedule work into these queues provide control flags,
    through which owners can set special attributes such as CPU locality, concurrency
    limits, and priority, which have an influence on the execution of work items queued.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A new workqueue can be set up through a call to `alloc_workqueue()`; the following
    excerpt taken from `<fs/nfs/inode.c>` shows sample usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This call takes three arguments: the first is a string constant to `name` the
    workqueue. The second argument is the bitfield of `flags`, and the third an integer
    called `max_active`. The last two are used to specify control attributes of the
    queue. On success, this function returns the address of the workqueue descriptor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a list of flag options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`WQ_UNBOUND`: Workqueues created with this flag are managed by kworker-pools
    that are not bound to any specific CPU. This causes all *work* items scheduled
    to this queue to run on any available processor. *Work* items in this queue are
    executed as soon as possible by kworker pools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WQ_FREEZABLE`: A workqueue of this type is freezable, which means that it
    is affected by system suspend operations. During suspend, all current *work* items
    are drained and no new *work* item can run until the system is unfreezed or resumed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WQ_MEM_RECLAIM`: This flag is used to mark a workqueue that contains *work*
    items involved in memory reclaim paths. This causes the framework to ensure that
    there is always a *worker* thread available to run *work* items on this queue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WQ_HIGHPRI`: This flag is used to mark a workqueue as high priority. Work
    items in high-priority workqueues have a higher precedence over normal ones, in
    that these are executed by a high-priority pool of *kworker* threads. The kernel
    maintains a dedicated pool of high-priority kworker threads for each CPU, which
    are distinct from normal kworker pools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WQ_CPU_INTENSIVE`: This flag marks work items on this workqueue to be CPU
    intensive. This helps the system scheduler to regulate the execution of *work*
    items that are expected to hog the CPU for long intervals. This means runnable
    CPU-intensive *work* items will not prevent other work items in the same kworker-pool
    from starting. A runnable non-CPU-intensive *work* item can always delay the execution
    of *work* items marked as CPU intensive. This flag is meaningless for an unbound
    wq.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WQ_POWER_EFFICIENT`: Workqueues marked with this flag are per-CPU by default,
    but become unbound if the system was booted with the `workqueue.power_efficient`
    kernel param set. Per-CPU workqueues that are identified to contribute significantly
    to power consumption are identified and marked with this flag, and enabling the
    power_efficient mode leads to noticeable power savings at the cost of a slight
    performance penalty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final argument `max_active` is an integer, which must specify the count
    of *work* items that can be executed simultaneously from this workqueue on any
    given CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a dedicated workqueue is set up, *work* items can be scheduled through
    any of the following calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`wq` is a pointer to a queue; it enqueues the specified *work* item on the
    local CPU, but does not guarantee execution on local processor. This call returns
    *true* if the given work item is successfully queued, and *false* if the given
    work item is already scheduled.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, callers can enqueue a work item bound to a specific CPU with
    a call to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '*Once a work* item is enqueued into a workqueue of the specified `cpu`, it
    returns *true* if the given work item is successfully queued and *false* if the
    given work item is already found in the queue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to shared workqueue APIs, delayed scheduling options also are available
    for dedicated workqueues. The following calls are to be used for delayed scheduling
    of *work* items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Both calls delay scheduling of the given work until the timeout specified by
    the `delay` has elapsed, with the exception that `queue_delayed_work_on()` enqueues
    the given *work* item on the specified CPU and guarantees its execution on it.
    Note that if the delay specified is zero and the workqueue is idle, then the given
    *work* item is scheduled for immediate execution.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Through this chapter, we have touched base with interrupts, the various components
    that fabricate the whole infrastructure, and how the kernel manages it efficiently.
    We understood how the kernel engages abstraction to smoothly handle varied interrupt
    signals routed from various controllers. The kernel's effort in simplifying complex
    programming approaches is again brought to the fore through the high-level interrupt-management
    interfaces. We also stretched our understanding on all the key routines and important
    data structures of the interrupt subsystem. We also explored kernel mechanisms
    for handling deferred work.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the kernel's timekeeping subsystem to understand
    key concepts such as time measurement, interval timers, and timeout and delay
    routines.
  prefs: []
  type: TYPE_NORMAL
