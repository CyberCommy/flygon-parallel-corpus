- en: Building Our Own Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In previous chapters, we spent a lot of time working on individual pieces of
    infrastructure building up little isolated pieces here and there, but in this
    chapter, we will try to put as many concepts together and build a minimally-viable
    **Platform-as-a-Service** (**PaaS**). In the following sections, we will cover
    these topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Configuration Management **(**CM**) tooling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Web Service** (**AWS**) deployment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous integration/Continuous delivery (CI/CD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we build up the core of our services, we will see what it takes to take a
    small service and deploy it into the real cloud.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note here is that this chapter is provided as only a quick primer
    and a basic example on real deployments in the cloud since creating a full PaaS
    infrastructure with all the bells and whistles is something that is usually so
    complex that it takes large teams months or years sometime to work out all the
    problems. Compounding the issue, the solutions are usually very specifically tailored
    to the choices of services and orchestration tooling running on top of this and
    as such, consider things you see in this chapter as a sample of the current ecosystem
    that you could use in your own deployment but other tools may be better suited
    to your specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With every system that depends on a large number of similarly configured machines
    (regardless of whether they are physical or virtual ones), there always arises
    a need for simple and easy rebuild tooling to help automate the majority of the
    tasks that have in the past been done by hand. In the case of PaaS clusters, ideally,
    all pieces of the infrastructure are capable of being rebuilt with minimal user
    intervention into the exact state that is wanted. In the case of bare-metal PaaS
    server nodes, this is critically important as any operation that you have to do
    manually gets multiplied by the number of nodes you have, so streamlining this
    process should be of utmost importance for any kind of production-ready clustering
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you may ask yourself, "Why do we care about covering CM tooling?" and the
    truth of the matter is that if you do not have proper CM around your container
    infrastructure, you are guaranteeing yourself after-hour emergency calls due to
    various issues such as: the nodes never joining the cluster, mismatched configurations,
    unapplied changes, version incompatibilities, and many other problems that will
    make you pull your hair out. So to prevent this set of situations from happening
    to you, we will really dive deep into this ecosystem of supporting software.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With that explained and out of the way, we can see some of the options we have
    available to choose from for the CM tooling:'
  prefs: []
  type: TYPE_NORMAL
- en: Ansible ([https://www.ansible.com](https://www.ansible.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Puppet ([https://puppet.com](https://puppet.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chef ([https://www.chef.io/chef/](https://www.chef.io/chef/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SaltStack ([https://saltstack.com](https://saltstack.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A few others that are mostly far weaker in terms of functionality and stability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the fact that both Puppet and Chef require an agent-based deployment
    and SaltStack is trailing in Ansible popularity by a huge margin, for our work
    here, we will cover Ansible as the CM tooling of choice but as your needs will
    probably vary. Use your own requirements to select the most appropriate tool for
    the job.
  prefs: []
  type: TYPE_NORMAL
- en: As a relevant side note from my interactions with the DevOps online communities,
    it seems that at the time of writing this material, Ansible is becoming the de
    facto standard for CM tooling but it is not without its flaws. While I would love
    to recommend its use everywhere for a myriad of great features, expect complex
    edge cases of bigger modules to be marginally reliable and keep in mind that most
    bugs you will find are likely to have been already fixed by an unmerged pull request
    on GitHub that you might have to apply locally as needed.WARNING! Choice of configuration
    management tooling should not be taken lightly and you should weigh the pros and
    cons of each before committing to a single one as this tooling is the hardest
    to switch out once you have a few machines managed with it! While many IT and
    DevOps professionals treat this choice almost as a way of life (similar to polarization
    between `vim` and `emacs` users), make sure that you evaluate your options carefully
    and logically due to the high costs of switching to a different one down the road.
    I have personally never heard of a company switch CM tooling after running with
    one for a while though I am sure there are a few out there.
  prefs: []
  type: TYPE_NORMAL
- en: Ansible
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you have not worked with Ansible before, it is has the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: It is relatively easy to use (YAML/Ninja2 based)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It only needs an SSH connection to the target
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It contains a huge amount of pluggable modules to extend its functionality ([https://docs.ansible.com/ansible/latest/modules_by_category.html](https://docs.ansible.com/ansible/latest/modules_by_category.html)),
    many of which are in the base install so you usually do not have to worry about
    dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If this list doesn't sound good enough, the whole Ansible architecture is extensible,
    so if there are no available modules that satisfy your requirements, they are
    somewhat easy to write and integrate, and thus Ansible is able to accommodate
    almost any infrastructure you may have or want to build. Under the covers, Ansible
    uses Python and SSH to run commands directly on the target host but in a much
    higher-level **domain-specific language** (**DSL**) that makes it very easy and
    quick for someone to write a server configuration versus scripting SSH commands
    directly through something like Bash.
  prefs: []
  type: TYPE_NORMAL
- en: The current Ubuntu LTS version (16.04) comes with Ansible 2.0.0.2, which should
    be adequate for most purposes, but using versions that are closer to upstream
    ones is often advised for both bug fixes and for new module additions. If you
    choose the latter route, make sure to have the version pinned to ensure consistently
    working deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To install Ansible on most Debian-based distributions, generally the process
    is extremely simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The standard layout for a project is usually split into roles that define functionality
    slices with the rest of the configurations basically just supporting those roles.
    The basic file structure of Ansible projects looks something like this (though
    more complex setups are often needed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us break down the basic structure of this filesystem tree and see how each
    piece is used in the bigger picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '`group_vars/all`: This file is used to define variables that are used for all
    of your playbooks. These can be used in playbooks and templates with variable
    expansions (`"{{ variable_name }}"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hosts/`: This file or a directory lists hosts and groups that you want to
    manage and any specific connectivity details like protocol, username, SSH key,
    and so on. In documentation, this file is often called the inventory file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`roles/`: This holds a list of role definitions that can be applied in a hierarchical
    and layered way to a target machine. Usually, it is further subdivided into `tasks/`,
    `files/`, `vars/`, and other layout-sensitive structures within each role:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<role_name>/tasks/main.yml`: A YAML file that lists the main steps to execute
    as part of the role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<role_name>/files/...`: Here you would add static files that would be copied
    to target a machine that do not require any pre-processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<role_name>/templates/...`: In this directory, you would add template files
    for role-related tasks. These usually contain templates that will be copied to
    the target machine with variable substitutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<role_name>/vars/main.yml`: Just like the parent directory implies, this YAML
    file holds role-specific variable definitions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`playbooks/`: In this directory, you would add all top-level ancillary playbooks
    that do not fit well in role definitions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have been introduced to what Ansible looks like and how it operates,
    it is time to do something practical with it. What we will do at this point is
    make an Ansible deployment configuration to apply some of the system tweaks we
    covered in the previous chapter and have Docker ready for us on the machine after
    running the playbook.
  prefs: []
  type: TYPE_NORMAL
- en: This example is relatively simple but it should show pretty well the ease of
    use and power of a decent configuration management tooling. Ansible is also a
    massive topic that a small section like this just can not cover in as much detail
    as I would like to but the documentation is relatively good and you can find it
    at [https://docs.ansible.com/ansible/latest/index.html](https://docs.ansible.com/ansible/latest/index.html).This
    example (and others) can be found at [https://github.com/sgnn7/deploying_with_docker/tree/master/chapter_8/ansible_deployment](https://github.com/sgnn7/deploying_with_docker/tree/master/chapter_8/ansible_deployment)
    if you want to skip the manual typing; however, it might be good practice to do
    it once to get the hang of the Ansible YAML file structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will need to create our file structure for holding files. We will
    call our main role `swarm_node` and since our whole machine is just going to be
    a swarm node, we will name our top-level deployment playbook the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s add the following content to the top-level `swarm_node.yml`. This
    will be the main entry point for Ansible and it basically just defines target
    hosts and roles that we want to be run on them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: YAML files are whitespace structured so make sure that you do not omit any spacing
    when editing this file. In general, all nesting levels are two spaces farther
    than the parent, key/values are defined with colons, and lists are itemized with
    a `-` (minus) prefix. For more information, about the YAML structure go to  [https://en.wikipedia.org/wiki/YAML#Syntax](https://en.wikipedia.org/wiki/YAML#Syntax).
  prefs: []
  type: TYPE_NORMAL
- en: 'What we are doing here should be mostly obvious:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hosts: all`: Run this on all the defined servers in the inventory file. Generally,
    this would be just a DNS name but since we will only have a single machine target,
    `all` should be fine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`become: True`: Since we use SSH to run things on the target and the SSH user
    is usually not root, we need to tell Ansible that it needs to elevate permissions
    with `sudo` for the commands that we will run. If the user requires a password
    to use `sudo`, you can specify it when invoking the playbook with the `ansible-playbook
    -K` flag, but we will be using AWS instances later in the chapter which do not
    require one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`roles: swarm_mode`: This is a list of roles we want to apply to the targets
    which is for now just a single one called `swarm_node`. This name *must* match
    a folder name in `roles/`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next in line for defining will be our system tweaking configuration files that
    we covered in the previous chapter for things like increases in file descriptor
    maximum, ulimits, and a couple of others. Add the following files and their respective
    content to the `roles/swarm_node/files/` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '`conntrack.conf`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`file-descriptor-increase.conf`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`socket-buffers.conf`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`ulimit-open-files-increase.conf`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'With those added, our tree should look a bit more like this now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With most of the files in place, we can now finally move onto the main configuration
    file--`roles/swarm_mode/tasks/main.yml`. In it, we will lay out our configuration
    steps one by one using Ansible''s modules and DSL to:'
  prefs: []
  type: TYPE_NORMAL
- en: '`apt-get dist-upgrade` the image for security'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply various improvements to machine configuration files in order to perform
    better as a Docker host
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To simplify understanding the following Ansible configuration code, it would
    be good to also keep this structure in mind since it underpins each discrete step
    we will use and is pretty easy to understand after you see it a couple of times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can find all module documentation we use in the playbook below at the main
    Ansible website ([https://docs.ansible.com/ansible/latest/list_of_all_modules.html](https://docs.ansible.com/ansible/latest/list_of_all_modules.html)).
    We will avoid getting too deep in module documentation here due to the sheer volume
    of information that will generally be a distraction to the purpose of this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also find module-specific documentation that we used here too:'
  prefs: []
  type: TYPE_NORMAL
- en: '- [https://docs.ansible.com/ansible/latest/apt_module.html](https://docs.ansible.com/ansible/latest/apt_module.html)'
  prefs: []
  type: TYPE_NORMAL
- en: - [https://docs.ansible.com/ansible/latest/copy_module.html](https://docs.ansible.com/ansible/latest/copy_module.html)
  prefs: []
  type: TYPE_NORMAL
- en: - [https://docs.ansible.com/ansible/latest/lineinfile_module.html](https://docs.ansible.com/ansible/latest/lineinfile_module.html)
  prefs: []
  type: TYPE_NORMAL
- en: - [https://docs.ansible.com/ansible/latest/command_module.html](https://docs.ansible.com/ansible/latest/command_module.html)
  prefs: []
  type: TYPE_NORMAL
- en: - [https://docs.ansible.com/ansible/latest/apt_key_module.html](https://docs.ansible.com/ansible/latest/apt_key_module.html)
  prefs: []
  type: TYPE_NORMAL
- en: - [https://docs.ansible.com/ansible/latest/apt_repository_module.html](https://docs.ansible.com/ansible/latest/apt_repository_module.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see what that main installation playbook (`roles/swarm_mode/tasks/main.yml`)
    should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: WARNING! This configuration has *no* hardening for the image to be comfortably
    placed on the internet live so use care and add whatever securing steps and tooling
    you require into this playbook before doing your real deployment. At the absolute
    least I would suggest installing the `fail2ban` package but you may have alternative
    strategies (e.g. seccomp, grsecurity, AppArmor, etc).
  prefs: []
  type: TYPE_NORMAL
- en: In this file, we sequentially ordered the steps one by one to configure the
    machine from base to a system fully capable of running Docker containers by using
    some of the core Ansible modules and the configuration files we created earlier.
    One thing that might not be very obvious is our use of the `{{ ansible_distribution
    | lower }}` type variables but in those, we are using Ansible facts ([https://docs.ansible.com/ansible/latest/playbooks_variables.html](https://docs.ansible.com/ansible/latest/playbooks_variables.html))
    gathered about the system we are running on and passing them though a Ninja2 `lower()`
    filter to ensure that the variables are lowercase. By doing this for the repository
    endpoint, we can use the same configuration without problems on almost any deb-based
    server target without much trouble as the variables will be substituted to the
    appropriate values.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the only thing we would need to do in order to apply this configuration
    to a machine is to add our server IP/DNS to `hosts` file and run the playbook
    with `ansible-playbook <options> swarm_node.yml`. But since we want to run this
    on an Amazon infrastructure, we will stop here and see how we can take these configuration
    steps and from them create an **Amazon Machine Image** (**AMI**) on which we can
    start any number of **Elastic Compute Cloud** (**EC2**) instances that are identical
    and have already been fully configured.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Web Services setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To continue onto our Amazon Machine Image (AMI) building section, we cannot
    go any further without having a working AWS account and an associated API key
    so we will do that first before continuing further. To avoid ambiguity, be aware
    that almost all AWS services cost money to use and your use of the API may incur
    charges for you even for things that you might not readily expect (that is, bandwidth
    usage, AMI snapshot storage, and on ) so use it accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: AWS is a massively complex piece of machinery, exponentially more than Ansible,
    and covering everything that you might need to know about it is impossible to
    do within the scope of this book. But we will try here to provide you with enough
    relevant instructions for you to have a place to start from. If you decide you
    want to learn more about AWS, their documentation is generally pretty great and
    you can find it at [https://aws.amazon.com/documentation/](https://aws.amazon.com/documentation/).
  prefs: []
  type: TYPE_NORMAL
- en: Creating an account
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the process is pretty straightforward, it has changed a couple of times
    in very significant ways, so detailing the full process here with no way to update
    it would would end up being a disservice to you so to create the account, I will
    guide you to the link that has the most up-to-date information on how to do it,
    which is [https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/](https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/).
    In general, the start of the process is at [https://aws.amazon.com/](https://aws.amazon.com/)
    and you can begin it by clicking on the yellow Sign Up or Create an AWS Account
    button on the top right of the screen and following the instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ee360b0d-e6ef-4cf5-a704-a3c19f28462d.png)'
  prefs: []
  type: TYPE_IMG
- en: Getting API keys
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the AWS account created, we now need to get our API keys so that we can
    access and use our resources through the various tools we want to use:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sign in to your console by going to `https://<account_id or alias>.signin.aws.amazon.com/console`.
    Note that you may need to sign in as the root account initially to do this (small
    blue link below the Sign In button, as shown in the following screenshot) if you
    did not create a user when you registered the account:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/96e3d45c-e762-42df-8171-8ab7a21c4019.png)'
  prefs: []
  type: TYPE_IMG
- en: Navigate to the IAM page at [https://console.aws.amazon.com/iam/](https://console.aws.amazon.com/iam/)
    and click on the Users link on the left-hand side of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Add user to start the user creation process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/3e2af10f-fe45-4a06-9abe-37a6f98ca761.png)CAUTION! Make sure that
    the Programmatic access checkbox is ticked, or else your AWS API keys will not
    work for our examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the permissions, we will give this user full administrator access. For
    production services, you will want to limit this to only the needed level of access:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/b06d52c2-375d-47c6-b148-efebf348b7e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Follow the rest of the wizard and make a record of the key ID and key secret,
    as these will be your API credentials for AWS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/11fc7117-065e-4978-afc3-a37968ccbbcc.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the API keys
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use the API keys in the easiest way, you can export variables in your shell
    that will get picked up by the tooling; however, you will need to do this on every
    Terminal where you are working with AWS APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, if you have the `awscli` tool installed (`sudo apt-get install
    awscli`), you can just run `aws configure`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There are many other ways to set your credentials as well through things like
    profiles but it really all depends on your expected usage case. For more information
    on these options,  you can refer to the official documentation at [https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html).
  prefs: []
  type: TYPE_NORMAL
- en: So with the key available and configured for CLI use, we can now proceed onto
    building custom AMI images with Packer.
  prefs: []
  type: TYPE_NORMAL
- en: HashiCorp Packer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we previously implied, our CM scripts are really not that optimal if we have
    to run them every time on a new machine that we add to the cluster or the cloud
    infrastructure in general. While we can do that, we really shouldn't, since in
    a perfect world the cluster nodes should be a flexible group that can spawn and
    kill instances depending on the usage with minimal user intervention so requiring
    a manual setup of each new machine is simply untenable even at the smallest cluster
    scales. With AMI image creation we can pre-bake a templated base system image with
    Ansible a single time, when the image is being made. By doing that, we can launch
    any new machine with this same image and our interaction with a running system
    would be kept at a minimum since everything would ideally be already configured.
  prefs: []
  type: TYPE_NORMAL
- en: To make these machine images, HashiCorp Packer ([https://www.packer.io/](https://www.packer.io/))
    allows us to do exactly that by applying a provisioning run of our CM tool of
    choice (Ansible) and outputting a ready-to-use image for any of the big cloud
    providers. By doing this, you could have the desired state of the cluster nodes
    (or any other server configuration) permanently enshrined in an image, and for
    any node addition needs for the cluster all you would need to do is spawn more
    VM instances based on the same Packer image.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Due to the fact that Packer is written in Go programming language, to install
    Packer, you only need to download the binary from their website found at [https://www.packer.io/downloads.html](https://www.packer.io/downloads.html).
    You can usually do something like the following for a quick installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: CAUTION! Packer binaries only provide TLS authentication for their runner without
    any form of signature checking, so the assurance that the program was published
    by HashiCorp itself is orders of magnitude lower than a GPG-signed `apt` repository
    that Docker uses; so, exercise extra care when getting it this way or build it
    from source ([https://github.com/hashicorp/packer](https://github.com/hashicorp/packer)).
  prefs: []
  type: TYPE_NORMAL
- en: Usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using Packer is actually relatively easy as all you need in most cases is the
    Ansible setup code and a relatively small `packer.json` file. Add this content
    to `packer.json` in our Ansible deployment configuration from the earlier section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: If it is not obvious, what we have here in this configuration file is the `provisioners`
    and `builders` sections, which in general correspond to Packer inputs and outputs,
    respectively. In our preceding example, we first install Ansible through the `shell`
    provisioner since the next step requires it, and then run the `main.yml` playbook
    from our current directory with the `ansible-local` provisioner on a base AMI.
    After applying all the changes, we save the result as a new **Elastic Block Store**
    (**EBS**) optimized AMI image.
  prefs: []
  type: TYPE_NORMAL
- en: AWS **Elastic Block Store** (**EBS**) is a service that provides block device
    storage to EC2 instances (these instances are basically just VMs). To the machine,
    these look like regular hard disks and can be formatted to whatever filesystem
    you want and are used to persist data in a permanent manner in the Amazon Cloud.
    They have configurable size and levels of performance; however, as you might expect,
    the price goes up as those two settings increase. The only other thing to keep
    in mind is that while you can move the drive around EC2 instances just like you
    would move a physical disk, you cannot move an EBS volume across availability
    zones. A simple workaround is to copy the data over."AMI image" phrase expands
    into "Amazon Machine Image image", which is a really quirky way to phrase things,
    but just like the sister phrase "PIN number", it flows much better when used that
    way and will be intentionally referred to in that way in this section. If you're
    curious about this particularity of the English language, you should peruse the
    Wiki page for RAS syndrome at [https://en.wikipedia.org/wiki/RAS_syndrome](https://en.wikipedia.org/wiki/RAS_syndrome).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the builders section, it will be helpful to explain some of the parameters
    in more detail as they may not be obvious from reading the JSON file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You can find more information on this particular builder type and its available
    options at [https://www.packer.io/docs/builders/amazon-ebs.html](https://www.packer.io/docs/builders/amazon-ebs.html).
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right AMI base image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike selecting the base Docker image to extend that we covered in earlier
    chapters, choosing the correct AMI to use Packer on is sadly not a simple task.
    Some distributions are regularly updated, so the IDs change. The IDs are also
    unique per AWS region and you may want hardware or paravirtualization (`HVM` vs
    `PV`). On top of all this, you also have to chose the right one for your storage
    needs (`instance-store`, `ebs`, and `ebs-ssd` at the time of writing this book),
    creating an absolutely un-intuitive matrix of options.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have not worked with Amazon **Elastic Compute Cloud** (**EC2**) and
    EBS, the storage options are a bit confusing to newcomers but they mean the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`instance-store`: This type of storage is local to the EC2 VM that is running,
    has space varied depending on the VM type (usually very little though), and gets
    completely discarded anytime the VM is terminated (a stopped or rebooted VM retains
    its state though). Instance store is great for nodes that do not need to keep
    any state but should not be used for machines that you want to have data retained
    on; however, you can mount a separate EBS drive to an instance--store VM independently
    if you want to have persistent storage and also utilize the stateless storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ebs`: This storage type creates and associates an EBS volume backed by older
    magnetic spinning hard drives (relatively slow vs solid-state drives) anytime
    an EC2 instance is started with this specific image, so the data is always kept
    around. This option is good if you want to have your data persisted or the `instance-store`
    volumes are not big enough. As of today though, this option is being actively
    deprecated, so it is likely that it will disappear in the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ebs-ssd`: This option is pretty much the same as the preceding one, but using
    **Solid State Devices** (SSD) that are much faster but much more expensive per
    gigabyte of allocation as the backing store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another thing that we need to choose is the virtualization type:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paravirtualization / `pv`: This type of virtualization is older and uses software
    to chain load your image, so it was capable to run on a much more diverse hardware.
    While it was faster long time ago, today it is generally slower than the hardware
    virtualization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hardware virtualization / `hvm`: This type of virtualization uses CPU-level
    instructions to run your image in a completely isolated environment akin to running
    the image directly on bare-metal hardware. While it depends on specific Intel
    VT CPU technology implementations, it is generally much better performant than
    `pv` virtualization, so in most cases, you should probably use it over other options,
    especially if you are not sure which one to choose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With our new knowledge of the available options, we can now figure out what
    image we will use as the base. For our designated OS version (Ubuntu LTS), you
    can use the helper page at [https://cloud-images.ubuntu.com/locator/ec2/](https://cloud-images.ubuntu.com/locator/ec2/)
    to find the right one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c078e660-0b39-4cfc-a458-246a2aed150e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For our test builds, we will be using `us-west-1` region, Ubuntu 16.04 LTS
    version (`xenial`), 64-bit architecture (`amd64`), `hvm` virtualization, and `ebs-ssd` storage
    so we can use the filters at the bottom of the page to narrow things down:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a4fe9542-6e35-4fd1-a9f5-5aa1cfc6dd6d.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the list collapses to one choice and in our `packer.json` we
    will use `ami-1c1d217c`.
  prefs: []
  type: TYPE_NORMAL
- en: Since this list is updated with AMIs that have newer security patches, it is
    very likely that by the time you are reading this section the AMI ID will be something
    else on your end. Because of that, do not be alarmed if you see discrepancies
    between values we have found here and what you have available to you while reading
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Building the AMI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'WARNING! Running this Packer build will for sure incur some (albeit barely
    a couple of US dollars at the time of writing this book) charges on your AWS account
    due to usage of non-free instance type, snapshot use, and AMI use, some possibly
    recurring. Refer to the pricing documentation of AWS for those services to estimate
    the amount that you will be charged. As an additional note, it is also good practice
    to clean up everything either from the console or CLI after you finish working
    with AWS objects that will not be kept around since it will ensure that you do
    not get additional charges after working with this code.With the `packer.json` in
    place, we can now do a build of our image. We will first install the pre-requisites
    (`python-boto` and `awscli`), then check the access, and finally build our AMI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Success! With this new image ID that you can see at the end of the output (`ami-a694a8c6`),
    we can now launch instances in EC2 with this AMI and they will have all the tweaks
    we have applied as well as have Docker pre-installed!
  prefs: []
  type: TYPE_NORMAL
- en: Deployments to AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With just the bare images and no virtual machines to run them on, our previous
    Packer work has not gotten us yet fully into an automated working state. To really
    get there, we will now need to tie everything together with more Ansible glue
    to complete the deployment. The encapsulation hierarchy of the different stages
    should conceptually look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4620e596-f554-42e0-a087-4078996b65ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from the diagram, we will take a layered approach to deployments:'
  prefs: []
  type: TYPE_NORMAL
- en: In the innermost level, we have the Ansible scripts to take a bare machine,
    VM, or an AMI to the configuration state we want it to be in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packer encapsulates that process and produces static AMI images that are further
    usable on Amazon EC2 cloud offerings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ansible then finally encapsulates everything mentioned previously by deploying
    machines with those static, Packer-created images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The road to automated infrastructure deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know what we want, how can we do it? Luckily for us, as hinted in
    the previous list, Ansible can do that part for us; we just need to write a couple
    of configuration files. But AWS is very complex here so it will not be as simple
    as just starting an instance since we want an isolated VPC environment. However,
    since we will only manage one server, we don't really care much for inter-VPC
    networking, so that will make things a bit easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to consider all the steps that will be required. Some of these
    will be very foreign to most of you as AWS is pretty complex and most developers
    do not usually work on networking, but they are the minimum necessary steps to
    have an isolated VPC without clobbering the default settings of your account:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up the VPC for a specific virtual network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create and tie a subnet to it. Without this, our machines will not be able to
    use the network on it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up a virtual Internet gateway and attach it to the VPC for unresolvable
    addresses with a routing table. If we do not do this, the machines will not be
    able to use the Internet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up a security group (firewall) whitelist of ports that we want to be able
    to access our server (SSH and HTTP ports). By default all ports are blocked so
    this makes sure that the launched instances are reachable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, provision the VM instance using the configured VPC for networking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To tear down everything, we will need to do the same thing, but just in reverse.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need some variables that will be shared across both deploy and teardown playbooks.
    Create a `group_vars/all` file in the same directory as the big Ansible example
    that we have been working on in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can write our `deploy.yml` in the same directory that `packer.json`
    is in, using some of those variables:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The difficulties of this deployment is starting to scale up significantly from
    our previous examples and there is no good way to cover all the information that
    is spread between dozens of AWS, networking, and Ansible topics to describe it
    in a concise way, but here are some links to the modules we will use that, if
    possible, you should read before proceeding:'
  prefs: []
  type: TYPE_NORMAL
- en: - [https://docs.ansible.com/ansible/latest/ec2_vpc_net_module.html](https://docs.ansible.com/ansible/latest/ec2_vpc_net_module.html)
  prefs: []
  type: TYPE_NORMAL
- en: - [https://docs.ansible.com/ansible/latest/set_fact_module.html](https://docs.ansible.com/ansible/latest/set_fact_module.html)
  prefs: []
  type: TYPE_NORMAL
- en: - [https://docs.ansible.com/ansible/latest/ec2_vpc_subnet_module.html](https://docs.ansible.com/ansible/latest/ec2_vpc_subnet_module.html)
  prefs: []
  type: TYPE_NORMAL
- en: - [https://docs.ansible.com/ansible/latest/ec2_vpc_igw_module.html](https://docs.ansible.com/ansible/latest/ec2_vpc_igw_module.html)
  prefs: []
  type: TYPE_NORMAL
- en: - [https://docs.ansible.com/ansible/latest/ec2_vpc_route_table_module.html](https://docs.ansible.com/ansible/latest/ec2_vpc_route_table_module.html)
  prefs: []
  type: TYPE_NORMAL
- en: - [https://docs.ansible.com/ansible/latest/ec2_group_module.html](https://docs.ansible.com/ansible/latest/ec2_group_module.html)
  prefs: []
  type: TYPE_NORMAL
- en: - [https://docs.ansible.com/ansible/latest/ec2_module.html](https://docs.ansible.com/ansible/latest/ec2_module.html)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'What we are doing here closely matches our earlier plan but now we have concrete
    deployment code to match it up against:'
  prefs: []
  type: TYPE_NORMAL
- en: We set up the VPC with the `ec2_vpc_net` module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create our subnet and associate it to the VPC with the `ec2_vpc_subnet` module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Internet virtual gateway for our cloud is created with `ec2_vpc_igw`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Internet gateway is then made to resolve any addresses that are not within the
    same network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ec2_group` module is used to enable ingress and egress networking but only
    port `22` (SSH) and port `80` (HTTP) are allowed in.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, our EC2 instance is created within the newly configured VPC with the `ec2`
    module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, the tear-down should be very similar but in reverse
    and contain a lot more `state: absent` arguments. Let''s put the following in
    `destroy.yml` in the same folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If the deploy playbook was readable, then this playbook should be generally
    easy to understand and as we mentioned, it just runs the same steps in reverse,
    removing any infrastructure pieces we already created.
  prefs: []
  type: TYPE_NORMAL
- en: Running the deployment and tear-down playbooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you remember, earlier in our `group_vars` definition, we had a key variable
    (`ssh_key_name: swarm_key`) that at this point becomes relatively important as
    without a working key we can neither deploy nor start our VM, so let''s do that
    now. We will use `awscli` and `jq`--a JSON parsing tool that will reduce the amount
    of work we do, but it is possible to do without it as well through the GUI console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'With the key in place, we can finally run our deploy script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: If you see errors similar to `"No handler was ready to authenticate. 1 handlers
    were checked. ['HmacAuthV4Handler'] Check your credentials"`, ensure that you
    have your AWS credentials set properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looks like everything is working! At this point, we could literally deploy
    our previously built 3-tier application if we wanted to. As we are done with our
    example and since we have our mini PaaS working, we can go back and clean up things
    by running the `destroy.yml` playbook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: And with that, we have automated deployments and teardowns of our infrastructure
    with single commands. While the example is pretty limited in scope, it should
    give you some ideas on how to expand beyond that with auto-scaling groups, orchestration
    management AMIs, registry deployment, and data persistence that would turn this
    into a full-fledged PaaS.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration/Continuous delivery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you make more services, you will notice that manual deployments of changes
    from source control and builds are taking up more time due to the need to figure
    out which image dependencies belong where, which image actually needs rebuilding
    (if you run a mono-repo), if the service changed at all, and many other ancillary
    issues. In order to simplify and streamline our deployment process, we will need
    to find a way to make this whole system fully automated so that the only thing
    needed to deploy a new version of services is a commit of a change to a branch
    of your code repository.
  prefs: []
  type: TYPE_NORMAL
- en: As of today, the most popular automation server called Jenkins is generally
    used in such function to do this build automation and deployment of Docker images
    and infrastructure but others like Drone, Buildbot, Concoure, etc have been rising
    fast through the ranks of very capable software CI/CD tooling too but none have
    so far reached the same acceptance levels from the industry yet. Since Jenkins
    is also relatively easy to use, we can do a quick demonstration of its power,
    and while the example is a bit simplistic, it should make it obvious on how this
    can be used for much more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since Jenkins will need `awscli`, Ansible, and `python-boto`, we have to make
    a new Docker image based on the Jenkins that is available from Docker Hub. Create
    a new folder and add a `Dockerfile` with the following content in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we build and run our server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'While it is still running, let''s go to the main page and enter the installation
    password that we got a warning for during the image start. Go to `http://localhost:8080`
    and enter the password that was in your logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b92e7df8-9270-444e-ad56-3042aa188d7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on Install Suggested Plugins on the next window and then after the relevant
    downloads are finished, select Continue as admin on the last installer page, which
    should lead you to the main landing page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e82e77e8-f006-4cb2-ba62-bdcaee9d5aa5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on create new jobs, name it `redeploy_infrastructure`, and make it a
    Freestyle project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f90a1571-c515-4365-9c1d-80bdf33d8857.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will configure the job with our Git repository endpoint so that it
    builds on any commits to the master branch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6bb0475d-efd1-41d3-b071-a4f835c45f7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As our build step, when the repository trigger activates, we will destroy and
    deploy the infrastructure, effectively replacing it with a newer version. Add
    a new build step of **Execute Shell** type and add the following to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The job should look quite a bit similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c56d4689-5abb-4777-b187-a2dc96a5b7fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Save the changes with `Save`, which should take you to the build''s main page.
    Here, click on the `Build Now` button and once the build appears on the left side
    build list, click on its progress bar or the dropdown  next to its name and select
    `View Log`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f8092593-3a14-4fea-97d5-6050e1ba03a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Success! As you can see, with Jenkins and a small configuration, we just made
    an automated deployment of our simple infrastructure. It is crude but effective
    though normally you would not want to redeploy everything but just the pieces
    that have changed and have the Jenkins live in-cluster, but that are somewhat
    more-involved endeavors that will be left to the reader as possible points of
    improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Resource considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since Jenkins runs on top of a Java VM, it will eat up available RAM at an alarming
    rate and is usually the biggest hog of usage along with being the most frequent
    **out-of-memory** (**OOM**) culprit I have had experience with. In even the lightest
    use cases, plan to allocate at least 1 GB of RAM to Jenkins workers or risk various
    failures at the most inopportune stages of your build pipelines. As a general
    rule, most Jenkins installation at this time will not have many problems with
    2 GB of RAM allocated to them, but due to the price of RAM in VM instances, you
    can try to scale things back until you reach the acceptable levels of performance.
  prefs: []
  type: TYPE_NORMAL
- en: The last thing to also pay attention to is that the Jenkins image is also a
    bulky one relatively speaking, weighing in at about a hefty 800 MB, so keep in
    mind that moving this container is really not as easy nor fast as some other tooling
    that we have been using.
  prefs: []
  type: TYPE_NORMAL
- en: First-deploy circular dependency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When using Jenkins within your cluster as a Dockerized service to chain-build
    all other images, it is important for me to mention a common pitfall where you
    will inevitably have the issue with new deployments where Jenkins is not available
    initally since at the cluster initialization stage no images are usually available
    in the registry and the default Jenkins Docker image is not configured in any
    way. On top of all this, since you often need an already-running Jenkins instance
    to build a newer Jenkins image, you will be in the the classic Catch-22 situation.
    You may have a reflex to build Jenkins manually as a followup deployment step,
    but you must resist the urge to do so if you want to really have infrastructure
    that is mostly hands-off.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general workaround to this is problem of bootstrapping Jenkins on a clean
    cluster has generally been something as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b5ffcb96-c3ad-4d99-ac8f-840034f3ea73.png)'
  prefs: []
  type: TYPE_IMG
- en: The cluster deployment is done first to ensure that we have a way t build our
    bootstrap image, and the **Docker Registry** is used to store the image after
    it is built. Following this, we build the Jenkins image on any available Docker
    Engine node and push it to the registry so that the service will have the right
    image to start with. If needed, we then launch the mentioned service using the
    same configuration management tool (like Ansible) or the orchestration tooling
    and wait for the auto-start job that will build all other remaining images which
    should populate the registry with all the other images needed to run the full
    cluster. The basic idea here is to do the initial bootstrap through CM tooling
    and then let the Jenkins service rebuild all the other images and (re)start the
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In large-scale deployments, it is also possible to use your cluster orchestration
    to schedule and handle this bootstrap procedure instead of the CM tooling but
    due to the vast differences between each orchestration engine, these steps may
    vary wildly between them.
  prefs: []
  type: TYPE_NORMAL
- en: Further generic CI/CD uses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Good CI tooling like Jenkins can do much more than the things we covered here;
    they all require significant investment of time and effort to get working, but
    the benefits are pretty significant if you can get them implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Self-building**: As mentioned in the workaround previously, you can have
    Jenkins build its own image when the configuration changes and have it redeploy
    itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment of only changed Docker images**: If you use Docker caching, you
    can check whether the new build created a different image hash and only deploy
    if it did. Doing this will prevent pointless work and have your infrastructure
    always running the newest code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timed Docker pruning**: You can run cleanup jobs (or any other jobs similar
    to `cron`)  on Jenkins that will free up or manage your Docker nodes to avoid
    manual interactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This list can also include: automated releases, failure notifications, build
    tracking, and quite a few other things that can be gained as well but suffice it
    to say, you really want a working CI pipeline in any non-trivial deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: A rule of thumb is that if you need to do something manually that can be automated
    with some timers and shell script, most CI tooling (like Jenkins) is there to
    help you out, so don't be afraid to try different and creative uses for it. With
    a full array of options and other tooling we have covered in this chapter, you
    can go to sleep soundly knowing that your clusters are going to be fine for a
    little while without needing constant babysitting.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have covered more on how you would truly deploy a PaaS
    infrastructure and the following topics that were required for it were examined
    in depth: configuration Management tooling with Ansible, cloud image management
    with HashiCorp Packer, and continuous integration with Jenkins. With the knowledge
    gained here, you should now be able to use the various tooling we discussed and
    create your own mini-PaaS for your own service deployments, and with some additional
    work, you can turn it into a full-scale PaaS!'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a look at how we can take our current Docker
    and infrastructure work and take it even bigger. We will also cover what direction
    this field might be moving toward, so if you would like to gain insights into
    the largest of deployments in the world, stick around.
  prefs: []
  type: TYPE_NORMAL
