- en: '12: Docker overlay networking'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overlay networks are at the beating heart of most things we do with container-related
    networking. In this chapter we’ll cover the fundamentals of native Docker overlay
    networking, as implemented in a Docker Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Docker overlay networking on Windows has feature parity with Linux. This means
    the examples we’ll use in this chapter will all work on Linux and Windows.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll split this chapter into the usual three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The TLDR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deep dive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The commands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s do some networking magic!
  prefs: []
  type: TYPE_NORMAL
- en: Docker overlay networking - The TLDR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the real world, it’s vital that containers can communicate with each other
    reliably and securely, even when they’re on different hosts that are on different
    networks. This is where overlay networking comes in to play. It allows you to
    create a flat, secure, layer-2 network, spanning multiple hosts. Containers connect
    to this and can communicate directly.
  prefs: []
  type: TYPE_NORMAL
- en: Docker offers native overlay networking that is simple to configure and secure
    by default.
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, it’s built on top of `libnetwork` and drivers.
  prefs: []
  type: TYPE_NORMAL
- en: '`libnetwork`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drivers`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libnetwork is the canonical implementation of the Container Network Model (CNM),
    and drivers are pluggable components that implement different networking technologies
    and topologies. Docker offers native drivers such as the `overlay` driver, and
    third parties also offer drivers.
  prefs: []
  type: TYPE_NORMAL
- en: Docker overlay networking - The deep dive
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In March 2015, Docker, Inc. acquired a container networking startup called *Socket
    Plane*. Two of the reasons behind the acquisition were to bring *real networking*
    to Docker, and to make container networking simple enough that even developers
    could do it :-P
  prefs: []
  type: TYPE_NORMAL
- en: They’ve made immense progress on both fronts.
  prefs: []
  type: TYPE_NORMAL
- en: However, hiding behind the simple networking commands are a lot of moving parts.
    The kind of stuff you need understand before doing production deployments and
    attempting to troubleshoot issues!
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of this chapter will be broken into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 1: we’ll build and test a Docker overlay network in Swarm mode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 2: We’ll explain the theory behind how it works.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build and test a Docker overlay network in Swarm mode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the following examples, we’ll use two Docker hosts, on two separate Layer
    2 networks, connected by a router. See Figure 12.1, and note the different networks
    that each node is on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1](images/figure12-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1
  prefs: []
  type: TYPE_NORMAL
- en: You can follow along with either Linux or Windows Docker hosts. Linux should
    have at least a 4.4 Linux kernel (newer is always better) and Windows should be
    Windows Server 2016 with the latest hotfixes installed.
  prefs: []
  type: TYPE_NORMAL
- en: Build a Swarm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The first thing we’ll do is configure the two hosts into a two-node Swarm. We’ll
    run the `docker swarm init` command on **node1** to make it a *manager*, and then
    we’ll run the `docker swarm join` command on **node2** to make it a *worker*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Warning:** If you are following along in your own lab, you’ll need to swap
    the IP addresses, container IDs, tokens etc. with the correct values for your
    environment.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Run the following command on **node1**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`Run the next command on **node2**. For this to work on Windows Server, you
    may need to modify your Windows firewall rules to allow ports `2377/tcp`, `7946/tcp`
    and `7946/udp`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`We now have a two-node Swarm with **node1** as a manager and **node2** as
    a worker.'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new overlay network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now let’s create a new *overlay network* called **uber-net**.
  prefs: []
  type: TYPE_NORMAL
- en: Run the following command from **node1** (manager). For this to work on Windows
    you may need to add a rule for port `4789/udp` on your Windows Docker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`That’s it! You’ve just created a brand-new overlay network that is available
    to all hosts in the Swarm and has its control plane encrypted with TLS! If you
    want to encrypt the data plane, you just add the `-o encrypted` flag to the command.'
  prefs: []
  type: TYPE_NORMAL
- en: You can list all networks on each node with the `docker network ls` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`The output will look more like this on a Windows server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`The network we created is at the bottom of the list called **uber-net**. The
    other networks were automatically created when Docker was installed and when we
    initialized the Swarm.'
  prefs: []
  type: TYPE_NORMAL
- en: If you run the `docker network ls` command on **node2**, you’ll notice that
    it can’t see the **uber-net** network. This is because new overlay networks are
    only made available to worker nodes that are running containers attached to them.
    This lazy approach improves network scalability by reducing the amount of network
    gossip.
  prefs: []
  type: TYPE_NORMAL
- en: Attach a service to the overlay network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now that we have an overlay network, let’s create a new *Docker service* and
    attach it to it. We’ll create the service with two replicas (containers) so that
    one runs on **node1** and the other runs on **node2**. This will automatically
    extend the **uber-net** overlay to **node2**
  prefs: []
  type: TYPE_NORMAL
- en: Run the following commands from **node1**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linux example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`Windows example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`> **Note:** The Windows example uses the backtick character to split parameters
    over multiple lines to make the command more readable. The backtick is how PowerShell
    escapes line feeds.'
  prefs: []
  type: TYPE_NORMAL
- en: The command creates a new service called **test**, attaches it to the **uber-net**
    overlay network, and creates two replicas (containers) based on the image provided.
    In both examples, we issued a sleep command to the containers to keep them running
    and stop them from exiting.
  prefs: []
  type: TYPE_NORMAL
- en: Because we’re running two replicas (containers), and the Swarm has two nodes,
    one replica will be scheduled on each node.
  prefs: []
  type: TYPE_NORMAL
- en: Verify the operation with a `docker service ps` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`When Swarm starts a container on an overlay network, it automatically extends
    that network to the node the container is running on. This means that the **uber-net**
    network is now visible on **node2**.'
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You’ve created a new overlay network spanning two nodes on
    separate physical underlay networks. You’ve also attached two containers to it.
    How simple was that!
  prefs: []
  type: TYPE_NORMAL
- en: Test the overlay network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now let’s test the overlay network with the ping command.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Figure 12.2, we’ve got two Docker hosts on separate networks, with
    a single overlay plumbed into both. We’ve got one container connected to the overlay
    network on each node. Let’s see if they can ping each other.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2](images/figure12-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2
  prefs: []
  type: TYPE_NORMAL
- en: To perform the test, we’ll need the IP address of each container (for the purposes
    of this test, we’re ignoring the fact that containers on the same overlay can
    ping each other by name).
  prefs: []
  type: TYPE_NORMAL
- en: Run a `docker network inspect` to see the **Subnet** assigned to the overlay.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`The output above shows that **uber-net**’s subnet is `10.0.0.0/24`. Note that
    this does not match either of the physical underlay networks (`172.31.1.0/24`
    and `192.168.1.0/24`).'
  prefs: []
  type: TYPE_NORMAL
- en: Run the following two commands on **node1** and **node2**. These will get the
    container’s ID’s and IP addresses. Be sure to use the container ID’s from your
    own lab in the second command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`Make sure you run these commands on both nodes to get the IP addresses of
    both containers.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.3 shows the configuration so far. Subnet and IP addresses may be different
    in your lab.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3](images/Figure12-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, there is a Layer 2 overlay network spanning both hosts, and each
    container has an IP address on this overlay network. This means that the container
    on **node1** will be able to ping the container on **node2** using its `10.0.0.4`
    address from the overlay network. This works despite the fact that both *nodes*
    are on different Layer 2 underlay networks. Let’s prove it.
  prefs: []
  type: TYPE_NORMAL
- en: Log on to the container on **node1** and ping the remote container.
  prefs: []
  type: TYPE_NORMAL
- en: To do this on the Linux Ubuntu container you will need to install the `ping`
    utility. If you’re following along with the Windows PowerShell example the `ping`
    utility is already installed.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the container IDs will be different in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linux example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`Windows example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`Congratulations. The container on **node1** can ping the container on **node2**
    using the overlay network.'
  prefs: []
  type: TYPE_NORMAL
- en: You can also trace the route of the ping command from within the container.
    This will report a single hop, proving that the containers are communicating directly
    over the overlay network — blissfully unaware of any underlay networks that are
    being traversed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** For the `traceroute` to work on the Linux example, you will need
    to install the `traceroute` package.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Linux example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`Windows example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`So far, we’ve created an overlay network with a single command. We then added
    containers to it. The containers were scheduled on two hosts that were on two
    different Layer 2 underlay networks. Once we worked out the container’s IP addresses,
    we proved that they could talk directly over the overlay network.'
  prefs: []
  type: TYPE_NORMAL
- en: The theory of how it all works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that we’ve seen how to build and use a container overlay network, let’s
    find out how it’s all put together behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the detail in this section will be specific to Linux. However, the same
    overall principles apply to Windows.
  prefs: []
  type: TYPE_NORMAL
- en: VXLAN primer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: First and foremost, Docker overlay networking uses VXLAN tunnels to create virtual
    Layer 2 overlay networks. So, before we go any further, let’s do a quick VXLAN
    primer.
  prefs: []
  type: TYPE_NORMAL
- en: At the highest level, VXLANs let you create a virtual Layer 2 network on top
    of an existing Layer 3 infrastructure. The example we used earlier created a new
    10.0.0.0/24 Layer 2 network on top of a Layer 3 IP network comprising two Layer
    2 networks — 172.31.1.0/24 and 192.168.1.0/24\. This is shown in Figure 12.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4](images/figure12-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of VXLAN is that it’s an encapsulation technology that existing routers
    and network infrastructure just see as regular IP/UDP packets and handle without
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: To create the virtual Layer 2 overlay network, a VXLAN *tunnel* is created through
    the underlying Layer 3 IP infrastructure. You might hear the term *underlay network*
    used to refer to the underlying Layer 3 infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Each end of the VXLAN tunnel is terminated by a VXLAN Tunnel Endpoint (VTEP).
    It’s this VTEP that performs the encapsulation/de-encapsulation and other magic
    required to make all of this work. See Figure 12.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5](images/figure12-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5
  prefs: []
  type: TYPE_NORMAL
- en: Walk through our two-container example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the example we built earlier, we had two hosts connected via an IP network.
    Each host ran a single container, and we created a single VXLAN overlay network
    for the containers to connect to.
  prefs: []
  type: TYPE_NORMAL
- en: To accomplish this, a new *sandbox* (network namespace) was created on each
    host. As mentioned in the previous chapter, a *sandbox* is like a container, but
    instead of running an application, it runs an isolated network stack — one that’s
    sandboxed from the network stack of the host itself.
  prefs: []
  type: TYPE_NORMAL
- en: A virtual switch (a.k.a. virtual bridge) called **Br0** is created inside the
    sandbox. A VTEP is also created with one end plumbed into the **Br0** virtual
    switch, and the other end plumbed into the host network stack (VTEP). The end
    in the host network stack gets an IP address on the underlay network the host
    is connected to, and is bound to a UDP socket on port 4789\. The two VTEPs on
    each host create the overlay via a VXLAN tunnel as seen in Figure 12.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6](images/figure12-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.6
  prefs: []
  type: TYPE_NORMAL
- en: This is essentially the VXLAN overlay network created and ready for use.
  prefs: []
  type: TYPE_NORMAL
- en: Each container then gets its own virtual Ethernet (`veth`) adapter that is also
    plumbed into the local **Br0** virtual switch. The topology now looks like Figure
    12.7, and it should be getting easier to see how the two containers can communicate
    over the VXLAN overlay network despite their hosts being on two separate networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.7](images/figure12-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.7
  prefs: []
  type: TYPE_NORMAL
- en: Communication example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now that we’ve seen the main plumbing elements, let’s see how the two containers
    communicate.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we’ll call the container on node1 “**C1**” and the container
    on node2 “**C2**”. And let’s assume **C1** wants to ping **C2** like we did in
    the practical example earlier in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8](images/figure12-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8
  prefs: []
  type: TYPE_NORMAL
- en: '**C1** creates the ping requests and sets the destination IP address to be
    the `10.0.0.4` address of **C2**. It sends the traffic over its `veth` interface
    which is connected to the **Br0** virtual switch. The virtual switch doesn’t know
    where to send the packet as it doesn’t have an entry in its MAC address table
    (ARP table) that corresponds to the destination IP address. As a result, it floods
    the packet to all ports. The VTEP interface connected to **Br0** knows how to
    forward the frame, so responds with its own MAC address. This is a *proxy ARP*
    reply and results in the **Br0** switch *learning* how to forward the packet.
    So it updates its ARP table, mapping 10.0.0.4 to the MAC address of the local
    VTEP.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that the **Br0** switch has *learned* how to forward traffic to **C2**,
    all future packets for **C2** will be transmitted directly to the VTEP interface.
    The VTEP interface knows about **C2** because all newly started containers have
    their network details propagated to other nodes in the Swarm using the network’s
    built-in gossip protocol.
  prefs: []
  type: TYPE_NORMAL
- en: The switch then sends the packet to the VTEP interface, which encapsulates the
    frames so they can be sent over the underlay transport infrastructure. At a fairly
    high level, this encapsulation includes adding a VXLAN header to the Ethernet
    frame. The VXLAN header contains the VXLAN network ID (VNID) which is used to
    map frames from VLANs to VXLANs and vice versa. Each VLAN gets mapped to VNID
    so that the packet can be de-encapsulated on the receiving end and forwarded to
    the correct VLAN. This obviously maintains network isolation. The encapsulation
    also wraps the frame in a UDP packet with the IP address of the remote VTEP on
    node2 in the *destination IP field*, and the UDP port 4789 socket information.
    This encapsulation allows the data to be sent across the underlying networks without
    the underlying networks having to know anything about VXLAN.
  prefs: []
  type: TYPE_NORMAL
- en: When the packet arrives at node2, the kernel sees that it’s addressed to UDP
    port 4789\. The kernel also knows that it has a VTEP interface bound to this socket.
    As a result, it sends the packet to the VTEP, which reads the VNID, de-encapsulates
    the packet, and sends it on to its own local **Br0** switch on the VLAN that corresponds
    the VNID. From there it is delivered to container C2.
  prefs: []
  type: TYPE_NORMAL
- en: That’s the basics of how VXLAN technology is leveraged by native Docker overlay
    networking.
  prefs: []
  type: TYPE_NORMAL
- en: We’re only scratching the surface here, but it should be enough for you to be
    able to start the ball rolling with any potential production Docker deployments.
    It should also give you the knowledge required to talk to your networking team
    about the networking aspects of your Docker infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: One final thing. Docker also supports Layer 3 routing within the same overlay
    network. For example, you can create an overlay network with two subnets, and
    Docker will take care of routing between them. The command to create a network
    like this could be `docker network create --subnet=10.1.1.0/24 --subnet=11.1.1.0/24
    -d overlay prod-net`. This would result in two virtual switches, **Br0** and **Br1**,
    being created inside the *sandbox*, and routing happens by default.
  prefs: []
  type: TYPE_NORMAL
- en: Docker overlay networking - The commands
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`docker network create` is the command that we use to create a new container
    network. The `-d` flag lets you specify the driver to use, and the most common
    driver is the `overlay` driver. You can also specify *remote* drivers from 3rd
    parties. For overlay networks, the control plane is encrypted by default. Just
    add the `-o encrypted` flag to encrypt the data plane (performance overheads may
    be incurred).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker network ls` lists all of the container networks visible to a Docker
    host. Docker hosts running in *Swarm mode* only see overlay networks if they are
    hosting containers running on that particular network. This keeps network-related
    gossip to a minimum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker network inspect` shows you detailed information about a particular
    container network. This includes *scope*, *driver*, *IPv6*, *subnet configuration*,
    *VXLAN network ID*, and *encryption state*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker network rm` deletes a network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we saw how easy it is to create new Docker overlay networks
    with the `docker network create` command. We then learned how they are put together
    behind the scenes using VXLAN technology.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve only scratched the surface of what can be done with Docker overlay networking.[PRE14]`
  prefs: []
  type: TYPE_NORMAL
