- en: Containerize What You Know - Guidance for Implementing Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book I have used older .NET technologies for the sample applications
    to show you that Docker works just as well with them as it does with modern .NET
    Core apps. You can Dockerize a ten year old WebForms application and get many
    of the same benefits you get from running a greenfield ASP.NET Core web application
    in a container.
  prefs: []
  type: TYPE_NORMAL
- en: You've seen lots of examples of containerized applications and learned how to
    build, ship, and run production-grade apps with Docker. Now, you're ready to start
    working with Docker on your own projects, and this chapter gives you advice on
    how to get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ll cover some techniques and tools that will help you run a proof-of-concept
    project to move an application to Docker. I''ll also walk you through some case
    studies to show you how I''ve introduced Docker to existing projects:'
  prefs: []
  type: TYPE_NORMAL
- en: A small-scale .NET 2.0 WebForms app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A database integration service in a **Windows Communication Foundation** (**WCF**)
    app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A distributed IoT API app running in Azure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You'll see how to approach typical problems and how the move to Docker can help
    solve them.
  prefs: []
  type: TYPE_NORMAL
- en: Dockerizing what you know
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you move to a new application platform, you have to work with a new set
    of artifacts and new operational processes. If you currently use the Windows installer
    for deployment, your artifacts are Wix files and MSIs. Your deployment process
    is to copy the MSI to the target server, log on, and run the installer.
  prefs: []
  type: TYPE_NORMAL
- en: After the move to Docker, you will have Dockerfiles and images as the deployment
    artifacts. You push the image to a registry and run a container or update a service
    to deploy the app. The resources and activities are simpler in Docker, and they'll
    be consistent between projects, but there's still a learning curve when you start.
  prefs: []
  type: TYPE_NORMAL
- en: Containerizing an app that you know well is a great way to provide a solid basis
    to that learning experience. When you first run your app in a container, you may
    see errors or incorrect behavior, *but that will be in the domain of your own
    application*. When you're tracking down the issue, you'll be dealing with an area
    you understand well, so although the platform is new, the problem should be easy
    to identify.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a simple Proof-of-Concept app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker is ideally suited to distributed applications, where each component runs
    in a lightweight container, making efficient use of a minimal set of hardware.
    You can choose a distributed application for your first Docker deployment, but
    a simpler application will be faster to migrate and will give you a higher chance
    of success.
  prefs: []
  type: TYPE_NORMAL
- en: A monolithic app is a good choice. It doesn't have to be a small code base,
    but the fewer integrations with other components it has, the more quickly you
    will have it running in Docker. An ASP.NET application that stores state in SQL
    Server is a straightforward option. You can expect to have a **Proof-of-Concept**
    (**PoC**) running in a day or two with a simple application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with a compiled application rather than the source code is a good
    way to prove that the app can be Dockerized without having to be changed. There
    are a few factors to consider when you''re selecting your PoC application:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statefulness**: If your target app stores state in memory, you won''t be
    able to scale the PoC by running multiple containers. Each container will have
    its own state, and you''ll get inconsistent behavior as requests are handled by
    different containers unless you also run a reverse proxy with sticky session support.
    Consider stateless apps or apps that can use shared state, like using SQL Server
    as a session state provider for ASP.NET.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configuration**: .NET apps typically use XML configuration files in `Web.config`
    or `app.config`. You can set up your PoC to use an existing config file as the
    base and then swap out any values that don''t apply to the containerized environment.
    It is preferable to read config settings through Docker with environment variables
    and secrets, but staying with config files is easier for the PoC.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resilience**: Older applications typically make the assumption of availability—the
    web app expects the database to be always available and doesn''t handle failure
    conditions gracefully. If your app has no retry logic for external connections,
    your PoC will face an error if there are transient connection failures when containers
    are starting up. You can mitigate this in the Dockerfile with a check for dependencies
    on startup, and an ongoing health check.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Windows authentication**: Containers aren''t domain-joined. You can access
    **Active Directory** (**AD**) objects in containers if you create a Group Managed
    Service Account in AD, but that adds complexity. For the PoC, stick to simpler
    authentication schemes like basic authentication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: None of these are major restrictions. You should be able to work on the basis
    of containerizing an existing app without changing code, but you need to be aware
    that the functionality may not be perfect at the PoC stage.
  prefs: []
  type: TYPE_NORMAL
- en: Generating an initial Dockerfile with Image2Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image2Docker is an open source tool that you can use to generate a Dockerfile
    for an existing application. It's a PowerShell module that you can run on the
    local machine or against a remote machine or virtual machine disk file (in Hyper-V,
    the files are in `VHD` or `VHDX` format).
  prefs: []
  type: TYPE_NORMAL
- en: It's a very simple way to get started with Docker – you don't even need Docker
    installed on your machine to try it out and see what the Dockerfile would look
    like for your app. Image2Docker can work with different types of application (called
    **artifacts**), but the functionality is mature for ASP.NET apps running on IIS.
  prefs: []
  type: TYPE_NORMAL
- en: 'On my development machine, I have an ASP.NET application deployed to **Internet
    Information Services** (**IIS**). I can migrate that application to Docker by
    installing Image2Docker from the PowerShell gallery and importing the module to
    use it locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: PowerShell 5.0 is the minimum required version for `Image2Docker`, but the tool
    has no other dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'I can run the `ConvertTo-Dockerfile` cmdlet, specifying the IIS artifact to
    build a Dockerfile that contains all the IIS websites on my machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This creates a directory at `C:\i2d\iis`, and inside the folder I'll have a
    Dockerfile and sub directories for each of the websites. `Image2Docker` copies
    the website content from the source to the output location. The Dockerfile uses
    the most relevant base image for the applications it finds, that is, `microsoft/iis`,
    `microsoft/aspnet`, or `microsoft/aspnet:3.5`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If there are multiple websites or web applications on the source, `Image2Docker`
    extracts them all and builds a single Dockerfile that duplicates the original
    IIS setup, so there will be multiple apps in the Docker image. That''s not what
    I''m aiming for, since I want a single app in my Docker image, so I can run with
    a parameter instead to extract a single website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The process is the same, but this time, `Image2Docker` extracts only a single
    application from the source—the one that's named in the `ArtifactParam` parameter.
    The Dockerfile contains the steps to deploy the application, and you can run `docker
    image build` to create the image and run the app.
  prefs: []
  type: TYPE_NORMAL
- en: This could be your first step in Dockerizing your application, and then you
    would run a container and check the functionality of the app. There may be additional
    setup needed, which `Image2Docker` doesn't do for you, so you'll likely be iterating
    on that generated Dockerfile, but the tool is a good way to get started.
  prefs: []
  type: TYPE_NORMAL
- en: '`Image2Docker` is an open source project. The source is on GitHub – use the
    following short link to view it: [https://github.com/docker/communitytools-image2docker-win](https://github.com/docker/communitytools-image2docker-win).
    The tool has not been updated recently, as Docker now have a commercial alternative
    called Docker Application Convertor (DAC). DAC has a much greater feature set
    since it supports Linux and Windows applications. You can see it demonstrated
    in DockerCon sessions on YouTube: [https://is.gd/sLMOa1](https://is.gd/sLMOa1).'
  prefs: []
  type: TYPE_NORMAL
- en: Engaging other stakeholders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A successful PoC should be possible in just a few days. The output of that will
    be a sample application that runs in Docker, as well as a set of extra steps you
    need to productionize that PoC. If you're working in a DevOps environment where
    your team owns the delivery of your project, you can agree to make the investment
    to move to Docker for production.
  prefs: []
  type: TYPE_NORMAL
- en: 'For larger projects or larger teams, you''ll need to engage with other stakeholders
    to take your PoC further. The type of conversations you have will depend on the
    structure of your organization, but there are some themes that focus on the improvements
    you get with Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: The operations team often has friction in the handover from development when
    it's time to deploy the application. The Docker artifacts, Dockerfiles and Docker
    Compose files are a central point where dev and ops can work together. There's
    no risk that the ops team will be given an upgrade they can't deploy because the
    upgrade will be a Docker image that's already been tried and tested.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The security team in large companies often has to demonstrate provenance. They
    need to prove that the software running in production hasn't been tampered with
    and is actually running the code that's in SCM. This may be process-driven right
    now, but with image signing and Docker content trust, it can be explicitly proven.
    In some cases, security also need to demonstrate that a system will run only on
    certified hardware, and that's easy to do with secure labels and constraints in
    Docker Swarm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Product owners are often trying to balance large backlogs against long release
    schedules. Enterprise .NET projects are typically difficult to deploy—the upgrade
    process is slow, manual, and risky. There's a deployment phase and then a user
    testing phase, during which the application is offline to normal users. In contrast,
    deployments with Docker are fast, automated, and safe, which means you can deploy
    more frequently, adding features when they're ready instead of waiting months
    for the next scheduled release.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The management team will have a focus on the product and the cost of running
    the product. Docker helps reduce infrastructure costs through more efficient use
    of compute resources and lower licensing costs. It helps reduce project costs
    by letting the team work more efficiently, removing the gaps between environments
    so that deployments are consistent. It also helps increase product quality, as
    automated packaging and rolling updates mean you can deploy more often, adding
    features and fixing defects more quickly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can get started with Docker by running the **Community Edition** (**CE**)
    for your PoC, which you get with Docker Desktop on Windows 10\. Other stakeholders
    in your organization will want to understand the support that's available for
    applications running in containers. Docker Enterprise Engine is included in the
    Windows Server 2016 or 2019 license cost, so you have support from Microsoft and
    Docker, Inc. at no extra cost. Operations and security teams may see a lot of
    benefit in the full Docker Enterprise suite, which gives you **Universal Control
    Plane** (**UCP**) and **Docker Trusted Registry** (**DTR**).
  prefs: []
  type: TYPE_NORMAL
- en: Docker recently announced that they will be shipping Docker Desktop Enterprise
    for Mac and Windows. It will have the same great user experience as Docker Desktop,
    but with support on Windows 10 and the ability to run the same version of the
    Docker Enterprise Engine locally that your organization is running in production.
  prefs: []
  type: TYPE_NORMAL
- en: The Dockerfiles and Docker images from your PoC will work in the same way on
    all of these versions. Docker CE, Docker Enterprise Engine, and Universal Control
    Plane all share the same underlying platform.
  prefs: []
  type: TYPE_NORMAL
- en: Case studies for implementing Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I'm going to finish this chapter by looking at three real-life case studies,
    where I have brought Docker into existing solutions or prepared a roadmap to bring
    Docker into a project. These are production scenarios, ranging from a small company
    project with tens of users to a large enterprise project with over a million users.
  prefs: []
  type: TYPE_NORMAL
- en: Case study 1 – an in-house WebForms app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some years ago, I took on the support of a WebForms app for a vehicle hire company.
    The app was used by a team of about 30, and it was a small-scale deployment—they
    had one server hosting the database and one server running the web app. Although
    small, it was the core application for the business, and everything they did ran
    from this app.
  prefs: []
  type: TYPE_NORMAL
- en: 'The app had a very simple architecture: just one web application and an SQL
    Server database. Initially, I did a lot of work to improve the performance and
    quality of the application. After that, it became a caretaker role, where I would
    manage two or three releases a year, adding new features or fixing old bugs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These releases were always more difficult and time-consuming than they needed
    to be. The release usually consisted of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A Web Deploy package with the updated application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of SQL scripts with schema and data changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A manual testing guide to verify the new features and check for regressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deployment was done outside office hours to give us a window of time to
    fix any problems we found. I would access their services using the **Remote Desktop** **Protocol **(**RDP**),
    copy the artifacts, and manually run the Web Deploy package and the SQL scripts.
    It was usually months between releases, so I'd rely on the documentation that
    I'd written to remind me of the steps. Then, I'd walk through the testing guide
    and check the main features. Sometimes, there were problems because I was missing
    an SQL script or a dependency for the web application, and I'd need to try and
    track down an issue I hadn't seen earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Until recently, the application was running on Windows Server 2003, which has
    long been out of support. When the company wanted to upgrade Windows, I recommended
    the move to Windows Server 2016 Core and Docker. My suggestion was to use Docker
    to run the web application and leave SQL Server running natively on its own server,
    but use Docker as a distribution mechanism to deploy database upgrades.
  prefs: []
  type: TYPE_NORMAL
- en: 'The move to Docker was very simple. I used Image2Docker against the production
    server to produce an initial Dockerfile, and then I iterated on that by adding
    a health check and environment variables for configuration. I already had an SQL
    Server project in Visual Studio for the schema, so I added another Dockerfile
    to package the Dacpac with a deployment script for the database. It took only
    two days to finalize the Docker artifacts and have the new version running in
    a test environment. This was the architecture with Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/afda574a-1c76-4071-b661-d169f2139a59.png)'
  prefs: []
  type: TYPE_IMG
- en: '**1**: The web application runs in a Windows Docker container. In production,
    it connects to a separate SQL Server instance. In non-production environments,
    it connects to a local SQL Server instance running in a container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2**: The database is packaged into a Docker image based on SQL Server Express
    and deployed with the database schema in a Dacpac. In production, a task container
    is run from the image to deploy the schema to the existing database. In non-production
    environments, a background container is run to host the database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since then, deployments have been straightforward, and they always follow the
    same steps. We have a set of private repositories on Docker Hub, where the versioned
    application and database images are stored. I configure my local Docker CLI to
    work against their Docker Engine, and then I do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Stop the web application container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a container from the new database image to upgrade SQL Server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use Docker Compose to update the web application to the new image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The biggest benefits of moving to Docker have been fast and reliable releases
    and reduced infrastructure requirements. The company is currently looking at replacing
    their current number of large servers with more smaller servers so that they can
    run Docker Swarm  and have zero downtime upgrades.
  prefs: []
  type: TYPE_NORMAL
- en: An additional benefit is the simplicity of the release process. Because the
    deployment is already tried and tested, using the same Docker images that are
    going to be used in production, there's no need to have someone who understands
    the app available to track down issues. The company's IT support folks do the
    releases now, and they can do that without my help.
  prefs: []
  type: TYPE_NORMAL
- en: I'm working with the same company again to manage their upgrade to the latest
    Docker Enterprise on Windows Server 2019\. The plan is very simple—I've already
    built their application and database images on top of the latest Windows Server
    2019 Core images and verified that they work with a suite of end-to-end tests.
    Now, they can perform the server upgrades and deploy the new versions using the
    same tools and be confident of a successful release.
  prefs: []
  type: TYPE_NORMAL
- en: Case study 2 – a database integration service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I worked on a big, complex web application for a financial company. It was an
    internal-facing app that managed very large volumes of trades. The frontend was
    in ASP.NET MVC, but most of the logic was in the service tier, written in WCF.
    The service tier was also a facade over many third-party apps, isolating the integration
    logic in the WCF layer.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the third-party apps had XML web services or JSON REST APIs we could
    consume, but one of the older apps had no integration options. We only used it
    for reference data, so the facade was implemented as a database-level integration.
    The WCF service exposed nicely encapsulated endpoints, but the implementation
    connected directly to the external application database to provide the data.
  prefs: []
  type: TYPE_NORMAL
- en: Database integration is brittle because you have to rely on a private database
    schema instead of a public service contract, but sometimes there are no other
    options. In this case, the schema changed infrequently, and we could manage the
    disruption. Unfortunately, the release process was back-to-front. The operations
    team would release new versions of database in production first because the app
    only had support from the vendor in production. When it was all working, they
    would replicate the release in the dev and test environments.
  prefs: []
  type: TYPE_NORMAL
- en: One release had a database schema change that broke our integration. Any features
    that used the reference data from the third-party app stopped working, and we
    had to get a fix out as quickly as possible. The fix was straightforward, but
    the WCF app was a large monolith and it needed a lot of regression testing before
    we could be confident that this change didn't impact other areas. I was tasked
    with looking at Docker as a better way of managing the database dependency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The proposal was straightforward. I didn''t recommend moving the whole app
    to Docker—that was already on a longer-term roadmap—but just moving one service
    into Docker. The WCF endpoint for that was that the database app facade would
    run in Docker, isolated from the rest of the application. The web application
    was the only consumer of the service, so it would just be a case of changing the
    URL for the service in the consumer. This is the proposed architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/9b9125c5-a000-4378-8ddb-4f28f19f0ea8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**1**: The web application runs in IIS. The code is unchanged, but the configuration
    is updated to use the URL for the new integration component, running in a container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2**: The original WCF services continue to run in IIS but with the previous
    database integration component removed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3**: The new integration component uses the same WCF contract as earlier,
    but now it is hosted in a container, isolating access to the third-party application
    database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This approach has a lot of benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: If the database schema changes, we only need to change the Dockerized service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service changes can be released without a full application release just by updating
    the Docker image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a sandboxed introduction to Docker, so the dev and ops teams can use it
    for evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, the most important benefit was the reduced amount of testing effort.
    For the full monolithic app, a release needs several weeks of testing. By breaking
    out the services into Docker containers, only the services that have changed need
    testing for the release. This drastically reduces the amount of time and effort
    that's needed, which allows for more frequent releases, thus getting new features
    out to the business more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Case study 3 – an Azure IoT app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I was the API architect on a project for delivering backend services that were
    consumed by a mobile application. There were two main APIs. The configuration
    API was read-only, and the devices called it to check for updates to settings
    and software. The events API was write-only, and the devices posted anonymous
    events about user behavior, which the product team used to inform design decisions
    for the next generation of devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'The APIs supported over 1.5 million devices. The configuration APIs needed
    high availability; they had to respond quickly to device calls and scale to thousands
    of concurrent requests per second. The events APIs consumed data from the devices
    and pushed events to a message queue. Listening on the queue were two sets of
    handlers: one that stored all event data in Hadoop, for long-term analysis, and
    one that stored a subset of events to provide real-time dashboards.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All the components ran in Azure, and at the peak of the project, we were using
    cloud services, Event Hubs, SQL Azure, and HDInsight. The architecture looked
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/3994e8aa-c9c8-4b64-9aff-705bd5816469.png)'
  prefs: []
  type: TYPE_IMG
- en: '**1**: The events API was hosted in a cloud service with multiple instances.
    Devices posted events to the API, which does some pre-processing and posts them
    in batches to an Azure Event Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2**: The Configuration API was also hosted in a Cloud Service with multiple
    instances. Devices connect to the API to check software updates and configuration
    settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3**: Real-time analytics data, which is used for a subset of key performance
    indicators. This was stored in SQL Azure for fast access, as these are modest
    quantities of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4**: Batch analytics data for storing all the events that are posted by all
    devices. This was stored in HDInsight, the managed Hadoop service on Azure for
    long-running Big Data queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This system was expensive to run, but it gave the product team a lot of information
    on how the devices were used, which they fed into the design process for the next
    generation. Everyone was happy, but then the product roadmap was canceled and
    there weren't going to be any more devices, so we had to cut running costs.
  prefs: []
  type: TYPE_NORMAL
- en: I had the job of reducing the Azure bill from $50K per month to under $1K per
    month. I could lose some of the reporting features, but the events API and configuration
    API had to stay highly available.
  prefs: []
  type: TYPE_NORMAL
- en: 'This happened before Docker was available on Windows, so my first revision
    of the architecture used Linux containers running on a Docker Swarm in Azure.
    I replaced the analytics side of the system with Elasticsearch and Kibana and
    replaced the configuration API with static content served from Nginx. I left the
    custom .NET components running in cloud services for the events API feeding Azure
    Event Hubs with device data and the message handler pushing data to Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/97b221e5-2481-43c2-b9b2-1e18a1bc3581.png)'
  prefs: []
  type: TYPE_IMG
- en: '**1**: The Configuration API, now running as a static website in Nginx. Configuration
    data is served as JSON payloads, maintaining the original API contract.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2**: Kibana is used for real-time and historical analytics. By reducing the
    amount of data stored, we reduced the data storage requirements significantly,
    at the cost of losing detailed metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3**: Elasticsearch was used to store incoming event data. A .NET Cloud Service
    is still used to read from Event Hubs, but this version saves data in Elasticsearch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This first revision gave us the cost savings we needed, mainly by reducing the
    number of nodes needed for the APIs and the amount of data we stored from the
    devices. Instead of storing everything in Hadoop and real-time data in SQL Azure,
    I centralized on Elasticsearch and stored just a small subset of the data. Using
    Nginx to serve the configuration APIs, we lost the user-friendly features that
    the product team had for publishing configuration updates, but we could run with
    far smaller compute resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'I oversaw a second revision, when Windows Server 2016 launched and Docker on
    Windows was supported. I added Windows nodes to the existing Linux nodes in the
    Docker Swarm and migrated the events API and message handlers over to Windows
    Docker containers. At the time, I also moved the messaging system over to NATS,
    running in a Linux container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/3243f6ae-0f38-4885-a699-aee1112ae2f5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**1**: The Events API is now hosted in a Docker container, but the code hasn''t
    changed; this is still an ASP.NET web API project, running in a Windows container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2**: The messaging component is using NATS instead of Event Hubs. We lose
    the ability to store and reprocess messages, but the message queue now has the
    same availability as the Events API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3**: The message handler reads from NATS and saves data in Elasticsearch.
    The majority of the code is unchanged, but it now runs as a .NET console app in
    a Windows container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This second revision further reduced costs and complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: Every component is now running in Docker, so I can replicate the whole system
    in development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All components are built with Dockerfiles and packaged as Docker images, so
    everything uses the same artifacts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The whole solution has the same level of service, running efficiently on a single
    Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, the project is destined to wind down, and it will be easy to accommodate
    that with the new solution. Device usage is still recorded and shown with a Kibana
    dashboard. As fewer devices are used over time, the services need less compute,
    and we can remove nodes from the swarm. Ultimately, the project will run on a
    minimal infrastructure, possibly just a two-node swarm, running on small VMs in
    Azure, or it could move back into the company's data center.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large and small companies all over the world are moving to Docker on Windows
    and Linux. Some of the main drivers are efficiency, security, and portability.
    Many new projects are designed from the ground up using containers, but there
    are many more existing projects that would benefit from the move to Docker.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I've looked at migrating existing apps to Docker on Windows,
    recommending that you start with an application you know well. A short, time-boxed
    PoC for Dockerizing that app will quickly show you what your app looks like in
    Docker. The outcome of that PoC will help you understand what you need to do next
    and who you need to involve to get that PoC moved into production.
  prefs: []
  type: TYPE_NORMAL
- en: I finished with some very different cases studies, showing you how you can introduce
    Docker in existing projects. In one case, I used Docker primarily for the packaging
    benefits to run a monolithic app without changing it, but to power clean upgrades
    for future releases. In another case, I took one component from a monolithic app
    and extracted it to run in a container, in order to reduce the testing burden
    for releases. In the last case, I completely migrated an existing solution to
    Docker, making it cheaper to run, easier to maintain, and giving me the option
    to run it anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this chapter has helped you think about how you can introduce Docker
    into your own projects, and I hope the rest of this book has shown you what you
    can do with Docker and why it's such an exciting technology. Thanks for reading,
    make sure to check out my Pluralsight courses and follow me on Twitter, and good
    luck in your journey with Docker on Windows!
  prefs: []
  type: TYPE_NORMAL
