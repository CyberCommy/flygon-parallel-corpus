- en: What&#x27;s Next
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far we have gone through topics around carrying out DevOps'' tasks on Kubernetes
    across the board. Nevertheless, it''s always challenging to implement knowledge
    under real-world circumstances, hence you may wonder whether Kubernetes is able
    to solve particular problems that you are currently facing. In this chapter, we''ll
    learn the following topics to work out with challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Kubernetes features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes communities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other container orchestrator frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the possibilities of Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is evolving day by day, and it's at a pace where it is publishing
    one major version quarterly. Aside from the built-in functions that come with
    every new Kubernetes distribution, contributions from the community also play
    an important role in the ecosystem, and we'll have a tour around them in this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Mastering Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes' objects and resources are categorized into three API tracks, namely,
    alpha, beta, and stable to denote their maturity. The `apiVersion` field at the
    head of every resource indicates their level. If a feature has a versioning such
    as v1alpha1, it belongs to alpha-level API, and beta API is named in the same
    way. An alpha-level API is disabled by default and is subject to change without
    notice.
  prefs: []
  type: TYPE_NORMAL
- en: The beta-level API is enabled by default; it's well tested and considered to
    be stable, but the schema or object semantics could be changed as well. The rest
    of the parts are the stable, generally available ones. Once an API enters a stable
    stage, it's unlikely to be changed anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Even though we've discussed concepts and practices about Kubernetes extensively,
    there are still considerable features that we haven't mentioned, that deal with
    a variety of workload as well as scenarios, and make Kubernetes extremely flexible.
    They may or may not apply to everyone's needs and are not stable enough in particular
    cases. Let's take a brief look at the popular ones.
  prefs: []
  type: TYPE_NORMAL
- en: Job and CronJob
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'They are also high-level pod controllers, that allow us to run containers that
    will eventually terminate. A job ensures a certain number of pods run to completion
    with success; a CronJob ensures that a job is invoked at given times. If we have
    the need to run batch workloads or scheduled tasks, we''d know that there are
    built-in controllers that come into play. Related information can be found at:
    [https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/).'
  prefs: []
  type: TYPE_NORMAL
- en: Affinity and anti-affinity between pods and nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know a pod can be manually assigned to some nodes with the node selector,
    and a node can reject pods with taints. However, when it comes to more flexible
    circumstances, say, maybe we want some pods to be co-located, or we want pods
    to be distributed equally across availability zones, arranging our pods either
    by node selectors or by node taints may take a great effort. Thus, the affinity
    is designed to solve the case: [https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity).'
  prefs: []
  type: TYPE_NORMAL
- en: Auto-scaling of pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Almost all modern infrastructure supports auto-scaling an instance group that
    runs the application, so does Kubernetes. The pod horizontal scaler (`PodHorizontalScaler`)
    is able to scale pod replicas with CPU/memory metrics in a controller such as
    Deployment. Starting from Kubernetes 1.6, the scaler formally supports scaling
    based on custom metrics, say transactions-per-second. More information can be
    found at [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/).
  prefs: []
  type: TYPE_NORMAL
- en: Prevention and mitigation of pod disruptions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know pods are volatile, and they'd be terminated and relaunched across nodes
    as the cluster scales in and out. If too many pods of an application are destroyed
    simultaneously, it could result in lowering the service level or even the application
    fails. Especially when the application is stateful or quorum-based, it might barely
    tolerate, pod disruptions. To mitigate the disruption, we could leverage `PodDisruptionBudget`
    to inform Kubernetes of how many unavailable pods at any given time our application
    can tolerate so that Kubernetes is able to take proper actions with the knowledge
    of the applications on top of it. For more information, refer to [https://kubernetes.io/docs/concepts/workloads/pods/disruptions/](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, since `PodDisruptionBudget` is a managed object, it still
    cannot preclude disruptions caused by factors outside Kubernetes, such as hardware
    failures of a node, or node components being killed by the system due to insufficient
    memory. As such, we can incorporate tools such as node-problem-detector into our
    monitoring stack and properly configure the threshold on the resources of a node,
    to notify Kubernetes which begins to drain the node or evict excessive pods to
    prevent situations getting worse. For more detailed guides on node-problem-detector
    and resource thresholds, refer to the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/](https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/](https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes federation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A federation is a cluster of clusters. In other words, it's formed by multiple
    Kubernetes clusters and is accessible from a single control plane. The resources
    that are created on a federation will be synchronized across all connected clusters.
    As of Kubernetes 1.7, resources that can be federated include Namespace, ConfigMap,
    Secret, Deployment, DaemonSet, Service, and Ingress.
  prefs: []
  type: TYPE_NORMAL
- en: Capabilities of the federation to build a hybrid platform bring us another level
    of flexibility when architecting our software. For instance, we can federate clusters
    deployed in on-premise data centers and various public clouds together, to distribute
    workloads by cost, and utilize platform-specific features while keeping the elasticity
    to move around. Another typical use case is federating clusters scattered in different
    geographical locations to lower the edge latency to customers across the globe.
    Moreover, a single Kubernetes cluster backed by etcd3 supports 5,000 nodes while
    keeping the p99 of API response time less than 1 second (on version 1.6). If there
    is a need to have a cluster with thousands of nodes or beyond, we can surely federate
    clusters to get there.
  prefs: []
  type: TYPE_NORMAL
- en: 'The guide for a federation can be found at the following link: [https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/](https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/).'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster add-ons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster add-ons are programs, that are designed or configured to enhance a Kubernetes
    cluster, and they are considered to be inherent parts of Kubernetes. For instance,
    Heapster, which we used in [Chapter 6](part0138.html#43JDK0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Monitoring and Logging*, is one of the add-on components, and so is the node-problem-detector
    we mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'As cluster add-ons may be used in some critical functions, some hosted Kubernetes
    services such as GKE deploy the add-on manager to safeguard the state of the add-ons
    from being modified or deleted. Managed add-ons will be deployed with a label,
    `addonmanager.kubernetes.io/mode`, on the pod controller. If the mode is `Reconcile`,
    any modification to the specification will be rolled back to its initial state;
    the `EnsureExists` mode only checks whether the controller exists, but doesn''t
    check whether its specification is modified. For instance, the following Deployments
    are deployed on a 1.7.3 GKE cluster by default, and all of them are protected
    in the `Reconcile` mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00147.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you''d like to deploy add-ons in your own cluster, they can be found at:
    [https://github.com/kubernetes/kubernetes/tree/master/cluster/addons](https://github.com/kubernetes/kubernetes/tree/master/cluster/addons).'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes and communities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When choosing an open source tool to use, we definitely wonder how supportiveness
    it is after we begin to use it. The supportiveness includes factors such as who
    is leading the project, whether the project is opinionated, how is the project's
    popularity, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes originated from Google, and it's now backed by the **Cloud Native
    Computing Foundation** (**CNCF**, [https://www.cncf.io](https://www.cncf.io)).
    At the time when Kubernetes 1.0 was released, Google partnered with the Linux
    Foundation to form the CNCF, and donated Kubernetes as the seed project. The CNCF
    is meant to promote the development of containerized, dynamic orchestrated, and
    microservices-oriented applications.
  prefs: []
  type: TYPE_NORMAL
- en: Since all projects under the CNCF is container-based, they certainly could work
    fluently with Kubernetes. Prometheus, Fluentd, and OpenTracing, which we demonstrated
    and mentioned in [Chapter 6](part0138.html#43JDK0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Monitoring and Logging*, are all member projects of the CNCF.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes incubator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes incubator is a process to support projects for Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/kubernetes/community/blob/master/incubator.md](https://github.com/kubernetes/community/blob/master/incubator.md).'
  prefs: []
  type: TYPE_NORMAL
- en: Graduated projects might become a core function of Kubernetes, a cluster add-on,
    or an independent tool for Kubernetes. Throughout the book, we have already seen
    and used many of them, including the Heapster, cAdvisor, dashboard, minikube,
    kops, kube-state-metrics, and kube-problem-detector, whatever makes Kubernetes
    better and better. You can explore these projects under Kubernetes ([https://github.com/kubernetes](https://github.com/kubernetes)),
    or the Incubator ([https://github.com/kubernetes-incubator](https://github.com/kubernetes-incubator)).
  prefs: []
  type: TYPE_NORMAL
- en: Helm and charts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Helm ([https://github.com/kubernetes/helm](https://github.com/kubernetes/helm))
    is a package manager, that simplifies the day-0 through day-n operations of running
    software on Kubernetes. It's also a graduated project from the incubator.
  prefs: []
  type: TYPE_NORMAL
- en: As what we've learned in [Chapter 7](part0163.html#4REBM0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Continuous Delivery*, deploying a containerized software to Kubernetes is basically
    writing manifests. Nonetheless, an application may be built with dozens of Kubernetes
    resources. If we're going to deploy such an application many times, the task to
    rename the conflict parts could be cumbersome. If we introduce the idea of template
    engines to solve the renaming hell, we will soon realize that we should have a
    place to store the templates as well as the rendered manifests. Hence, the Helm
    is meant to solve such annoying chores.
  prefs: []
  type: TYPE_NORMAL
- en: 'A package in Helm is called a chart, and it''s a collection of configurations,
    definitions, and manifests to run an application. Charts contributed by the communities
    are published here: [https://github.com/kubernetes/charts](https://github.com/kubernetes/charts).
    Even if we are not going to use it, we can still find verified manifests for a
    certain package there.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Helm is quite simple. First get the Helm by running the official installation
    script here: [https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get](https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After getting the Helm binary working, it fetches our kubectl configurations
    to connect to the cluster. We''d need to have a manager `Tiller` inside our Kubernetes
    cluster to manage every deployment task from Helm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If we'd like to initialize the Helm client without installing the Tiller to
    our Kubernetes cluster, we can add the `--client-only` flag to `helm init`. Furthermore,
    using the `--skip-refresh` flag together allows us to initialize the client offline.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Helm client is able to search the available charts from the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s install a chart from the repository, say the last one, `wordpress`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The deployed chart in Helm is a release. Here, we have a release, `plinking-billygoat`,
    installed. Once the pods and the services are ready, we can connect to our site
    and check the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00148.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The teardown of a release also takes only one line of command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Helm leverages ConfigMap to store the metadata of a release, but deleting a
    release with `helm delete` won't delete its metadata. To wholly clear these metadata,
    we could either manually delete these ConfigMaps or add the `--purge` flag when
    executing `helm delete`.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to managing packages in our cluster, another value brought by Helm
    is it is established as a standard to share packages and so it allows us to install
    popular software directly, such as the Wordpress we installed, rather than rewriting
    manifests for every software we used.
  prefs: []
  type: TYPE_NORMAL
- en: Gravitating towards a future infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's always hard to tell whether a tool is a right fit or not, especially on
    opting for a cluster management software to underpin business missions, because
    the difficulties and challenges with which everyone is confronted varies. Apart
    from objective concerns such as performance, stability, availability, scalability,
    and usability, real circumstances also account for a significant portion of the
    decision. For instance, perspective on choosing a stack for developing greenfield
    projects and for building additional layers on top of bulky legacy systems could
    be diverse. Likewise, operating services by a highly cohesive DevOps team and
    by an organization working in the old day styles could also lead to distinct choices.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to Kubernetes, there are still other platforms, which also feature
    orchestrating containers, and they all provide some easy ways to getting started.
    Let's step back and take an overview over them to find out the best fit.
  prefs: []
  type: TYPE_NORMAL
- en: Docker swarm mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Swarm mode ([https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/))
    is Docker's native orchestrator integrated in the Docker engine since version
    1.12\. As such, it shares the same API and user interface with Docker itself,
    including the use of Docker Compose files. Such degrees of integration are considered
    to be advantages as well as disadvantages depending on if one is comfortable with
    working on a stack, where all the components are from the same vendor.
  prefs: []
  type: TYPE_NORMAL
- en: 'A swarm cluster consists of managers and workers, where the managers are part
    of a consensus group to maintain the state of a cluster while keeping high availability.
    Enabling the swarm mode is quite easy. Roughly speaking, it''s only two steps
    here: creating a cluster with `docker swarm init` and joining other managers and
    workers with `docker swarm join`. Additionally, Docker Cloud ([https://cloud.docker.com/swarm](https://cloud.docker.com/swarm))
    provided by Docker helps us bootstrap a swam cluster on various cloud providers.'
  prefs: []
  type: TYPE_NORMAL
- en: Features that come with the swarm mode are the ones we'd expect to have in a
    container platform, that is to say, container lifecycle managements, two scheduling
    strategies (replicated and global, which resemble to Deployment and DaemonSet
    in Kubernetes respectively), service discovery, secret managements, and so on.
    There is also an ingress network that works like the NodePort type service in
    Kubernetes, but we'll have to bring up something such as nginx or Traefik if we
    need a L7 layer LoadBalancer.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, the swarm mode proffers an option to orchestrate containerized applications
    that works out of the box once one begins to use Docker. Meanwhile, as it speaks
    the same language with Docker and simple architecture, it's also considered to
    be the easiest platform among all choices. Therefore, it's indeed reasonable to
    choose the swarm mode to get something done quickly. However, its simplicity sometimes
    leads to lack of flexibility. For example, in Kubernetes we are able to employ
    Blue/Green deployment strategy by merely manipulating selector and labels, but
    there is no easy way to do so in the swarm mode. Since the swarm mode is still
    under active development, such as the function to store configuration data, which
    is analogous to ConfigMap in Kubernetes is introduced in version 17.06, we definitely
    could look forward to the swarm mode becoming more powerful in the future while
    retaining its simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon EC2 container service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: EC2 container service (ECS, [https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/))
    is AWS' answer to the Docker upsurge. Unlike Google Cloud Platform and Microsoft
    Azure, which provides open source cluster managers such as Kubernetes, Docker
    Swarm, and DC/OS, AWS sticks to its own way in response to the need of container
    services.
  prefs: []
  type: TYPE_NORMAL
- en: ECS takes its Docker as its container runtime, and it also accepts Docker Compose
    files in syntax version 2\. Moreover, terminologies of ECS and Docker Swarm mode
    are pretty much the same thing, such as the idea of task and service. Yet the
    similarities stop here. Although the core functions of ECS is simple and even
    rudimentary, as a part of AWS, ECS fully utilizes other AWS products to enhance
    itself such as VPC for container networking, CloudWatch, and CloudWatch Logs for
    monitoring and logging, Application LoadBalancer and Network LoadBalancer with
    Target Groups for service discovering, Lambda with Route 53 for DNS-based service
    discovering, CloudWatch Events for CronJob, EBS and EFS for data persistence,
    ECR for docker registry, Parameter Store and KMS for storing configuration files
    and secrets, CodePipeline for CI/CD, and so forth. There is another AWS product,
    AWS Batch ([https://aws.amazon.com/batch/](https://aws.amazon.com/batch/)) that
    is built on top of ECS for processing batch workloads. Furthermore, an open source
    tool from AWS ECS team, Blox ([https://blox.github.io](https://blox.github.io)),
    augments the capabilities to customize the scheduling that are not shipped with
    ECS, such as the DaemonSet-like strategy, by wiring couples of AWS products up.
    From another perspective, if we take AWS as an integral whole to evaluate ECS,
    it's truly mighty.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting up an ECS cluster is easy: create an ECS cluster via the AWS console
    or API and join EC2 nodes with the ECS agent to the cluster. The good thing is
    that the master side is managed by AWS so that we are free from keeping wary eye
    on the master.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, ECS is easy to getting started, especially for people who are familiar
    with Docker as well as AWS products. On the other hand, if we aren't satisfied
    with the primitives currently provided, we have to do some handworks either with
    other AWS services mentioned earlier or third-party solutions to get things done,
    and this could result in undesired costs on those services and efforts on configurations
    and maintenances to make sure every component works together nicely. Besides,
    ECS is only available on AWS, which could also be one concern that people would
    take it seriously.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Mesos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Mesos* ([http://mesos.apache.org/)](http://mesos.apache.org/)) had been created
    long before Docker set off the trend of containers, and its goal is to solve the
    difficulties regarding management of resources in a cluster comprising general
    hardware while supporting diverse workloads. To build such a general platform,
    Mesos makes use of a two-tier architecture to divide the resource allocation and
    the task execution. As such, the execution part can theoretically extend to any
    kind of task, including orchestrating Docker containers.'
  prefs: []
  type: TYPE_NORMAL
- en: Even though we talked about only the name Mesos here, it is basically in charge
    of one tier of jobs as a matter of fact, and the execution part is done by other
    components called Mesos frameworks. For example, Marathon ([https://mesosphere.github.io/marathon/](https://mesosphere.github.io/marathon/))
    and Chronos ([https://mesos.github.io/chronos/](https://mesos.github.io/chronos/))
    were two popular frameworks to deploy long-running and batch-job tasks respectively,
    and both of them support the Docker container. In this way, when it comes to the
    term Mesos, it's referring to a stack such as Mesos/Marathon/Chronos or Mesos/Aurora.
    In fact, under Mesos' two-tier architecture, it's viable to run Kubernetes as
    a Mesos framework as well.
  prefs: []
  type: TYPE_NORMAL
- en: Frankly speaking, a properly organized Mesos stack and Kubernetes are pretty
    much the same in terms of capabilities except that Kubernetes requires that everything
    that is run on it should be containerized regardless of Docker, rkt, or a hypervisor
    container. On the other hand, as Mesos focuses on its generic scheduling and tends
    to keep its core small, some essential functions should be installed, tested,
    and operated separately, which could bring about extra efforts.
  prefs: []
  type: TYPE_NORMAL
- en: DC/OS ([https://dcos.io/](https://dcos.io/)) published by Mesosphere takes advantages
    of Mesos to build a full-stack cluster management platform, which is more comparable
    to Kubernetes with respect to capabilities. As a one-stop-shop for every solution
    built atop Mesos, it bundles couples of components to drive the whole system,
    Marathon for common workloads, Metronome for scheduled jobs, Mesos-DNS for service
    discovery, and so forth. Though these building blocks seem to be complicated,
    DC/OS greatly simplified the works on installations and configurations by CloudFormation/Terraform
    templates, and its package management system, Mesosphere Universe. Since DC/OS
    1.10, Kubernetes is officially integrated into DC/OS, and it can be installed
    via the Universe. Hosted DC/OS is also available on some cloud providers such
    as Microsoft Azure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot is the web console interface of DC/OS, which aggregates
    information from every component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00149.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So far we have discussed the community version of DC/OS, but some features are
    only available in the enterprise edition. They are mostly on security and compliance,
    and the list can be found at [https://mesosphere.com/pricing/](https://mesosphere.com/pricing/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have briefly discussed Kubernetes features that applies
    to certain more specific use cases, and guided where and how to leverage the strong
    communities, including the Kubernetes incubator and the package manager Helm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, we went back to the start and gave overview to three other popular
    alternatives for the same goal: orchestrating containers, so as to leave the conclusion
    in your mind for choosing your next generation infrastructure.'
  prefs: []
  type: TYPE_NORMAL
