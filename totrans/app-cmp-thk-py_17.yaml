- en: '*Chapter 14*: Using Computational Thinking and Python in Statistical Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use Python and the elements of computational thinking
    to solve problems that require statistical analysis algorithms. We will use **pandas
    DataFrames** to create statistical analysis algorithms within the Python environment.
    Additional packages in Python will be needed to create statistical analyses, such
    as **NumPy**, **pytz**, and more. We will use those packages when they are needed
    for the code we will work with and when learning what the libraries help us do,
    such as organizing data with pandas, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the problem and Python data selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing, analyzing, and summarizing data using visualizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to design algorithms that best
    fit the scenarios you are presented with. You will also be able to identify Python
    functions that best align with the problems presented and generalize your solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need the latest version of Python for running the code in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to have the **pandas**, **NumPy**, **SciPy**, and **Scikit-Learn**
    packages installed for the problems in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the full source used in this chapter here: [https://github.com/PacktPublishing/Applied-Computational-Thinking-with-Python/tree/master/Chapter14](https://github.com/PacktPublishing/Applied-Computational-Thinking-with-Python/tree/master/Chapter14)'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the problem and Python data selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we look at the pandas library, let's define what data analysis is. When
    we talk about data analysis, we are talking about the process of inspecting, cleansing,
    transforming, and modeling data with the intent of discovering useful data, notifying
    conclusions, and supporting decision-making. Decision-making is critical. We don't
    just want to see what the data says has happened in the past. We want to use data
    in order to make informed decisions for the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at some of the uses of data analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Business**: It helps when making decisions based on customer trends and behavior
    prediction, increasing business productivity, and driving effective decision-making.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weather forecasting**: Data about the atmosphere (temperature, humidity,
    wind, and more) is collected and analyzed to understand atmospheric processes
    (meteorology) to determine how the atmosphere will evolve in the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transportation**: Data can be used to determine trends, including traffic,
    accidents, and more, helping us make decisions about traffic patterns, traffic
    light durations, and much more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The aforementioned uses are only some of the possible applications, of course.
    Data analytics is used for a very wide range of things, and by businesses and
    educational organizations to make critical decisions, provide resources to the
    community, fund our schools and colleges, and so much more.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's take a look at what tools we have available to analyze that data.
    One of the main libraries used in data analysis in Python is the pandas package.
    The greatness of pandas lies in its ease of use, easy data structure, and high
    performance. Using pandas simplifies our work in data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Defining pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One thing that's important to note is that pandas is built on top of NumPy.
    **NumPy** is a package that helps us work with arrays. Python doesn't have arrays
    per se, so the packages allow us to create them, use them, and then build upon
    that capability.
  prefs: []
  type: TYPE_NORMAL
- en: Pandas provides a flexible and easy data structure to simplify your work in
    data analysis. It is a great tool for **big data**. When we talk about big data,
    we're talking about structured and unstructured datasets that are analyzed so
    that we can get better insights and aid in decision making or strategies for businesses
    and organizations. pandas can handle importing different formats, such as `.csv`
    files, **SQL**, and **JSON**, as well as all sorts of manipulation, such as selecting
    data, merging, reshaping, and cleaning the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two different ways to store data in pandas—series and DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Series** are one-dimensional arrays that hold any data type (integer, string,
    or float); series represent one column of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataFrames** are two-dimensional objects that can have multiple columns and
    data types. They take inputs such as dictionaries, series, lists, and other DataFrames.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now learn when to use pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Determining when to use pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pandas is extremely useful in general, but it is really a great tool when we
    are looking at large data and working with **comma-separated values** (**CSV**)
    files. These files are stored as tables, such as spreadsheets. The other thing
    is that we can establish *chunks* in pandas. Yes, there's a `chunksize` parameter
    in pandas that helps us break down our data. Let's say we have 5,000,000 rows.
    We can decide to use `chunksize` to break that down by 1,000,000 rows.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we sometimes have massive data files but only want to look at some
    of the components. Pandas allows us to identify the columns we want to include
    and those we want to ignore.
  prefs: []
  type: TYPE_NORMAL
- en: Working with pandas series
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned in the previous section—*Defining pandas*—series are one-dimensional.
    We can create an empty pandas series using some simple code. Note that we are
    importing the pandas library first, as is usual when working with packages and
    libraries, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The default series created will have a type of `float` because we didn''t establish
    any other type in our algorithm. However, the console prints a warning that in
    the future, empty series **dtypes** will be set as an object rather than `float`.
    `dtype` stands for data type; in this case, it''s a float. Take a look at the
    following screenshot, which shows the output when we run our algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Output when creating empty series in pandas without identifying
    dtype'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.01_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.1 – Output when creating empty series in pandas without identifying
    dtype
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there really isn't an error, just a warning about how the data
    was stored.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a series with defined elements. To do that, we''ll need
    to import both pandas and `numpy` so that we can create the arrays and then the
    series. Take a look at the following snippet of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the preceding code, we stored our array using `numpy`,
    then created a series using that array. Finally, we printed the series. The output
    is a table, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also get the exact same thing if we created a list first. Take a look
    at this snippet of code:'
  prefs: []
  type: TYPE_NORMAL
- en: ch14_seriesDemo2.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we created the series directly from the list. We're not going to
    show the output for this particular snippet of code because it's exactly the same
    as the previous snippet's output.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also get a series from a dictionary. Let''s take a look at that in the
    following snippet of code:'
  prefs: []
  type: TYPE_NORMAL
- en: ch14_seriesDemo3.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code is just a demo, but we do get a table series that contains
    the values of our dictionary when we run the algorithm. Let''s take a look at
    that output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have the two columns based on the key-value pairs, and the
    type, which is `object`. One thing that will become important is that series don't
    have column titles. For that, we'll need to use DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: '*What if we want to access a specific element in a series?* Well, to access
    the first element, let''s use the following snippet of code:'
  prefs: []
  type: TYPE_NORMAL
- en: ch14_demo4.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is simply `Miguel`. That''s because we used
    index 0 to identify what we wanted from that dictionary, so it''ll give us the
    value for the first key-value pair. If we wanted the value pairs for the first
    two key-value pair elements, we''d replace `(mySeries[0])` with `(mySeries[:2])`.
    Then, the output would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: There are many other things that can be done with series, so play around with
    the indexes and creating different types of series using lists, dictionaries,
    or NumPy arrays. For now, let's move on to DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Working with pandas DataFrames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at how we work with DataFrames. First, let''s take
    a look at a `.csv` file with pandas. We''re going to use the `demo.csv` file in
    the snippet of code that follows. Please replace the location of the file to match
    the location where you have saved the file, which can be found in the GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: ch14_csvDemo.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code does three things. It imports the pandas package so that
    we can use the data capabilities we need, it tells the program to open the data
    file we''ll be working with, and then it gives us the first few rows of data in
    the file so that we can see what we''re working with. The following screenshot
    shows the result, or output, from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – Output showing the first few rows of the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.02_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.2 – Output showing the first few rows of the dataset
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding screenshot, the table does not show all the
    values included in our file. While our file is not full of big data, it does include
    more rows of information. This is just a way for us to get a preview of our data.
  prefs: []
  type: TYPE_NORMAL
- en: But this is a *clean* dataset. *What happens if we have a dataset with rows
    or columns that are missing information?* Well, pandas allows us to work with
    the file to prepare it for us. DataFrames also prepare our data so that we can
    create visual representations. These visuals, or plots, will allow us to see trends,
    make predictions, determine what values we can use for training, and much more.
    The DataFrame is really just the backbone of everything else we can then do with
    a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about problems, how to work with pandas, and some
    of the capabilities of pandas series and DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: As a note, for the rest of this chapter, we'll be a lot more focused on DataFrames
    than series. But before we get into an application, let's take a look at how we
    can avoid errors and pitfalls by preprocessing our DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Preprocessing data** is a technique that transforms raw data into a useable
    and efficient format. It is, in fact, the most important step in the data mining
    and machine learning process.'
  prefs: []
  type: TYPE_NORMAL
- en: When we are preprocessing data, we are really cleaning it, transforming it,
    or doing a data reduction. In this section, we will take a look at what these
    all mean.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data cleaning** refers to the process of making our dataset more efficient.
    If we go through data cleaning in really large datasets, we can expedite the algorithm,
    avoid errors, and get better results. There are two things we deal with when data
    cleaning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing data**: This can be fixed by ignoring the data or manually entering
    a value for the missing data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noisy data**: This can be fixed/improved by using binning, regression, or
    clustering, among other processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We're going to look at each of these things in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Working with missing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s take a look at how we deal with missing data. First, we''re going to
    learn how to ignore missing data. We can use pandas to find rows with missing
    values. When we do that, we''re cleaning our dataset. Now, we''re not going to
    go through every method we can use, just one where we get rid of rows with missing
    values. As always, the dataset used is available in our GitHub repository and
    you''ll need to update your file location. Let''s look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: ch14_cleaningDemo1.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the first `print` statement is for our own sake so that
    we can see what our dataset looks like. *You''ll never want to do that with huge
    files!* The following screenshot shows the first `print` output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – First print statement, the original dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.03_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.3 – First print statement, the original dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the **1**, **Blue** column has a value of **NaN** under **Countries**
    and the next column (**2**, **Yellow**) has a missing value under the **Numbers**
    column. When we use `dropna()`, the algorithm will drop the rows with missing
    values. The following screenshot shows the printed statement with the altered
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – Printed clean dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.04_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.4 – Printed clean dataset
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding screenshot, the two rows that were missing
    values were eliminated in our new dataset. Now, we could run whatever analysis
    we wanted to for this data.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wanted to check only one column to verify whether missing values exist,
    you could use the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: ch14_cleaningDemo2.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice in the preceding algorithm that we used the `Countries` column heading
    to verify that particular column. When we run the algorithm, here''s what our
    output looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the second row in our dataset has a missing value in the `Countries`
    column.
  prefs: []
  type: TYPE_NORMAL
- en: While we're not going to go into every method, you can also remove columns.
    You can choose to remove rows and/or columns with a certain number of missing
    values. For example, you could choose to remove only rows or columns that have
    more than two missing values. If you did that, you'd still need to worry about
    the values that may still be missing in columns or rows that were not removed
    because there was only one missing value. For those, you may choose to replace
    the missing values with something.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say you want to replace a value given the column. To do so, let''s take
    a look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: ch14_cleaningDemo3.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, notice that we are filling each empty cell with the
    value `0`. When we run the algorithm, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.5 – Replaced missing values'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.05_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.5 – Replaced missing values
  prefs: []
  type: TYPE_NORMAL
- en: Notice the highlighted values in the preceding screenshot. Those are the values
    replaced by our algorithm. Now, let's take a look at how we deal with noisy data.
  prefs: []
  type: TYPE_NORMAL
- en: Working with noisy data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, let's define what we mean by **noisy data**. When we have a really large
    amount of data and some of it is not useful for our analysis, we say it is noisy
    data. Noisy data is also used to refer to data corruption. *Really, it's just
    useless data*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three of the ways that we deal with noisy data are binning, regression, and
    clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binning** uses neighboring data to smoothen a sorted data value. The sorted
    values go in bins, which are groups created within the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **clustering** method identifies and removes outliers in a dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **regression** method smoothens data by fitting it into regression functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The purpose of binning is to reduce some errors. In binning, data is divided
    into small buckets or bins. The data is then replaced with a calculated bin value.
    When we go through the binning process, we are smoothing the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example with a simple, numerical dataset in the algorithm. The following
    snippet of code will create bins with equal frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: ch14_binning1.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'When looking at the preceding code, you can see that the number of bins is
    defined as `5`, so the data will be binned into five lists. Binning really is
    a way to group information. We tell the algorithm we want to do it and how many
    bins we want, and it provides the data in those bins, or groups. In this case,
    we get those five lists. Take a look at the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the algorithm created five bins with three values in each of
    the bins.
  prefs: []
  type: TYPE_NORMAL
- en: 'Important Note:'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the binning process doesn't organize our data for us. So, if we reordered
    our values, they would still be binned in the order that the data was entered.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at a binning algorithm that uses equal width:'
  prefs: []
  type: TYPE_NORMAL
- en: ch14_binning2.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding snippet of code breaks down our data into three bins. The goal
    of equal width binning is to divide the dataset into bins of equal size, which
    in the case of equal width binning means equal range. The data will be split,
    but it is important to note that we''re talking about range here, so the bins
    won''t have the same number of elements in each of them for this particular dataset.
    The output for the preceding snippet of code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the binning produces an output that doesn't look quite as clean
    as the equal frequency output, but is actually more popular.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's talk about transforming data.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pandas allows us to transform data. Here are some of the ways that we can transform
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalization** transforms values into new range; the most popular is **min-max
    normalization**, given as follows:![](image/Formula_B15413_14_001.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribute selection** is the process of transforming data by replacing an
    attribute with a different attribute or attributes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concept hierarchy** is actually a transformation done by reducing data. It
    is done by replacing concepts such as numbers (*10*, *15*, *40*) with higher-level
    concepts, such as qualifiers (*short*, *lengthy*, *extremely lengthy*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will glance through the reduction of data.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data reduction** refers to a process that allows us to get similar or even
    the same results from a dataset but only after having reduced the representation
    of the data in volume.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We won''t go into too much depth with all the concepts here because they are
    much easier to look at in examples, but here are some of the ways that data reduction
    is done:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing invalid data from the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating summaries for the data at different levels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think of removing invalid data as taking care of any outliers. The data may
    have been entered incorrectly, conditions may have not been optimal, or similar.
    When we have a data point that doesn't fit the entirety of the dataset, especially
    where we have a large number of data points to compare it to, we can remove that
    data point as invalid or as an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: When creating summaries at different levels, we are aggregating the dataset
    and testing and producing summaries at each of those levels. Say you had 100 data
    points (datasets will often be in the thousands, but it's easier to explain with
    smaller numbers). We could create a summary for the first 20 data points. Then,
    we could do the same for the first 40, then the first 60, and so on. When compared,
    we could see the trends and use those smaller sub-sections of our dataset to make
    our predictions if the trends hold true. That's what data reduction helps with,
    simplifying our dataset while still getting accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to work with data that needs to be cleaned.
    We learned some of the ways that we can clean data, such as eliminating missing
    data or replacing missing points. We also learned about noisy data and how to
    address problems with noisy data in our datasets. Finally, we learned about data
    reduction and how we can get accurate results by removing invalid data and creating
    aggregate summaries of our data.
  prefs: []
  type: TYPE_NORMAL
- en: This is just an introduction to the types of things we do when we're working
    with data. So, let's look at an example so that we can put some of this into context.
  prefs: []
  type: TYPE_NORMAL
- en: Processing, analyzing, and summarizing data using visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're working in real estate now, and since we want to do well, we really want
    to build an algorithm that helps us analyze data and predict housing prices. But
    let's think about that for a second. We can define that problem very broadly or
    narrowly. We can do a pricing analysis for all houses in a state or houses with
    three bedrooms or more in a neighborhood. *Does performing the analysis matter?
    Maybe*. *But isn't that why we want to look at this problem?*
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at how we can process the data first.
  prefs: []
  type: TYPE_NORMAL
- en: Processing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start by gathering some data. For this problem, we''re using the `kv_house_data.csv`
    dataset, which is available in our GitHub repository. To look at this dataset,
    we''ll need quite a few libraries. We''ve been talking about pandas mostly, yes,
    but we want to also do visualizations and perform some analysis, so we''ll need
    **Seaborn**, **SciPy**, and **Scikit-Learn**. The full algorithm can be found
    in the `ch14_housePrice_prediction.py` file. We''ll look at it in snippets to
    discuss what we''re doing along the way:'
  prefs: []
  type: TYPE_NORMAL
- en: ch14_housePrice_prediction.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*So, what are all these libraries?* Pandas we know about. We''re using that
    for organizing our data. NumPy helps us with the arrays. Seaborn and Matplotlib
    are both used for visualizations. If we were taking this further to create models
    by training using the dataset, we''d also need Scikit-Learn. For this example,
    we''re going to stick with some of the plots we can get before training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s import our dataset. Remember from [*Chapter 12*](B15413_12_Final_SK_ePub.xhtml#_idTextAnchor159),
    *Using Python in Experimental and Data Analysis Problems*, that you can set the
    directory of your file directly. You can also provide the entire path to the data
    file, as we have done previously in this chapter, such as in the cleaning demo.
    You can use `os.chdir()` to establish the directory, adding the location of your
    file in parentheses, then use the following code snippet to read the `.csv` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We are using a pandas function here. *See that* `pd.read_csv()`*?* That `pd`
    is pandas, since we imported pandas as `pd`, and `read_csv()` is the function
    that allows the algorithm to get the information in the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you forget to enter your directory or include the wrong location for it,
    you will receive an error code, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.6 – Path to file error'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.06_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.6 – Path to file error
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, Python will make sure you know that you've made a mistake. *That
    can get aggravating at times, but it is certainly helpful.*
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our data, we'll need to check it and clean it. This is where
    all the content we shared previously in the chapter comes into play. Let's see
    what that looks like in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing and summarizing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, there''s something we haven''t really talked about much, and that''s Python''s
    variable explorer. More specifically, it''s **Spyder''s Python variable explorer**.
    Spyder is an integrated environment and is free of charge. It works with Python,
    running Python as usual, but also provides us with better editing tools. The following
    screenshot shows how it should look when you import your dataset from the variable
    explorer in Python. When we run the `ch14_housePrice_prediction.py` Python algorithm
    in Spyder, we can see our variables in the variable explorer. The following screenshot
    shows the data we get from the variable explorer when we run this algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.7 – Variable explorer view in Spyder'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.07_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.7 – Variable explorer view in Spyder
  prefs: []
  type: TYPE_NORMAL
- en: When we are dealing with a lot of data and larger algorithms, this tool becomes
    really critical. We can get a lot of information from just this tool. For example,
    let's look at the `housing_data` variable. In *Figure 14.7*, you can see that
    the type for this variable in our algorithm is **DataFrame** with a size of **(21613,
    21)**.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you double-click on the variable in the variable explorer, you get what''s
    shown in the following screenshot (please note that the screenshot may look different
    depending on the environment you are using. When running this code using environments
    such as Spyder or Jupyter, depending on your theme settings and choices, the table
    may look different, with different color schemes or no color schemes):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.8 – DataFrame variable view in Spyder'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.08_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.8 – DataFrame variable view in Spyder
  prefs: []
  type: TYPE_NORMAL
- en: This is only one way to get some of the information for our DataFrame. Spyder
    allows us to resize the window as well, so we can take a look at more columns,
    scroll through them to find values, and so on. It's not that easy if we're in
    the Python console. You can get the information, just not as easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the code that can give us some of the information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will show us the first five rows of our dataset. Take a
    look at the following screenshot; we have ellipses (**…**) between **price** and
    **long**. That''s because Python wants to let us know that there are additional
    columns between those two:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.9 – First few rows of the DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.09_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.9 – First few rows of the DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the rows help us see what our dataset looks like, but nothing
    else. So, we can also take a look at the size of our DataFrame by using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the preceding code, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, now we have the shape or size of our DataFrame. *What does it
    mean?* It means we have `21,613` rows of data in `21` columns. Regardless of whether
    you were in Spyder, the Python console, or another environment of your choice,
    you can see that your data was successfully imported.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the raw data imported, let''s see whether we can get more
    information. We can use the following code to get a summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `describe()` function generates a summary that includes details such as
    the mean, standard deviation, and percentile. This percentile is a part of your
    five-number summary for datasets, used to create **boxplots**. While we won''t
    create a boxplot in this problem, that visual representation can be helpful depending
    on our goals for our algorithms. Take a look at the following screenshot for the
    results of using the `describe()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.10 – Using the describe() function'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.10_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.10 – Using the describe() function
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we should note that the function analyzes numeric values from the DataFrame.
    It excludes `NaN` values. `NaN` stands for **Not a Number** values in Python and
    represents any value that is undefined or unrepresentable. `NaN` can also represent
    missing numbers. We talked about those missing numbers earlier in the *Working
    with missing data* section and some of the ways we could tackle them. Let''s look
    at that in context now. We want to find our missing values. For that, we can run
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding snippet will let us know if there are missing data points in
    each of the columns in our dataset. Then, it will aggregate the values as a sum.
    So, if we had two missing values under the `date` column, we''d expect to see
    a `2` there. Our results can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.11 – Results from running the isnull() and .sum() functions'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.11_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.11 – Results from running the isnull() and .sum() functions
  prefs: []
  type: TYPE_NORMAL
- en: '*We don''t have to clean this dataset!* *That''s a clean dataset*. However,
    learning how to identify whether or not we would need to do so is really important.
    Now that we know that we have a clean dataset, we can start working on building
    some visualizations.'
  prefs: []
  type: TYPE_NORMAL
- en: Using data visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Remember that this is still part of our initial algorithm file, `ch15_housePrice_prediction.py`.
    If you open that file, the code that follows starts at *line 25*. We have added
    comments for descriptions between the lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing we''re doing is identifying the columns that we''ll be using
    for our plot. After we do that, we''ll make a DataFrame and save it as `df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code snippet creates the correlations for our DataFrame. In the
    next code snippet from the file, we''ll create our figure; that is, we''ll work
    on the data visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we named our figure, added the subplot, and identified
    our colors. Next, we''ll have to set some of our properties, such as tick marks,
    the distance between tick marks, and labels for axes and tick marks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'After we''ve set some of the properties, we can ask for the plot to use `tight_layout()`.
    This will help us see all the details of the plot and labels. If we do not use
    `tight_layout()`, we''ll sometimes have some labels that will not be visible in
    our graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding snippet, we also created a save file and defined the figure''s
    size. Finally, we asked the algorithm to show us the correlations. The following
    screenshot shows us the result of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.12 – Correlations plot for housing data'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.12_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.12 – Correlations plot for housing data
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we just created a **correlation matrix** using Python. And it's
    a pretty great matrix. *But what is a correlation matrix?* A correlation matrix
    looks at all the values in our DataFrame and then calculates how closely they
    are correlated, giving a value between **-1** and **1**. The closer the value
    is to **1**, the more correlated the values are. Each value is compared to itself,
    which is a perfect correlation, of course, and seen as the diagonal in our matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the graph, where all the values are compared to each other, has
    to be looked at more closely. The closer to the yellow color, the closer the values
    are correlated. So, take a look at the *y*-axis **sqft_above** value and the corresponding
    value for **sqft_living** on the *x* axis. That value is close to the yellow value
    of **1**, but not quite. We''ve highlighted those values in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.13 – Highlighted correlation of sqft_above and sqft_living'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.13_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.13 – Highlighted correlation of sqft_above and sqft_living
  prefs: []
  type: TYPE_NORMAL
- en: There are other values that show some correlation, but not quite strong enough.
    This plot helps us then make decisions to maybe take a closer look at that correlation,
    find more information about it, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can also take a look at a pretty intense plot matrix that''s called
    *pair plotting*. A **pairs plot** shows us the distribution of single variables
    and the relationships between two variables. If we ran a pairs plot for the data
    that is included in our algorithm, we get a massive graph, as shown in the following
    screenshot (please note that this plot can take up to a few minutes to generate
    because of the amount of data being analyzed and plotted):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.14 – Full DataFrame pairs plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.14_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.14 – Full DataFrame pairs plot
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this is a fairly intense, hard-to-read, almost-impossible-to-analyze
    graph. You'd have to zoom in to each of the paired figures to make some determinations.
    It should also be mentioned that it takes time for the algorithm to run and produce
    this graphic. *It takes some processing power to create something so complex from
    such a large dataset!*
  prefs: []
  type: TYPE_NORMAL
- en: '*Do we ever want to see a big plot of graphs such as this one?* *Actually,
    yes*. *If we have a dataset with fewer variables, absolutely!* We can also make
    this more friendly by using other color schemes, for example. This may make identifying
    trends easier. But let''s be clear; this isn''t very helpful. What we can do is
    create a pairs plot of some of the variables we may be interested in. Remember
    we talked about `sqft_living` and`sqft_above` possibly having a strong positive
    correlation? We also really do want to compare things to pricing, right? So, let''s
    create a pairs plot using just `sqft_living`, `pricing`, and `sqft_above`. Take
    a look at the relevant code snippet from our file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we run this part of the algorithm, we get the graph shown in the
    following screenshot. This graph provides the correlations for these three values
    and we can definitely see some positive correlations happening:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.15 – Pairs plot of price, sqft_living, and sqft_above'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.15_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.15 – Pairs plot of price, sqft_living, and sqft_above
  prefs: []
  type: TYPE_NORMAL
- en: Notice in particular the graphs that pair **sqft_living** and **sqft_above**.
    The relationship between those two is fairly linear and positive. This confirms
    what we observed from *Figure 14.13*, where the correlation was closer to **1**
    than the other variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'But it would also help to analyze another three variables so that we can see
    what happens when the correlation is not strong. We''ll keep `price` and `sqft_living`
    so that we are only changing one of the variables for comparison purposes. Looking
    at *Figure 14.12*, **sqft_living** and **zipcode** don''t seem to have a strong
    positive correlation at all. So, let''s run the algorithm again, swapping `zipcode`
    with `sqft_above`. Let''s take a look at the result shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.16 – Pairs plot of price, sqft_living, and zipcode'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_14.16_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.16 – Pairs plot of price, sqft_living, and zipcode
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, **sqft_living** and **zipcode** show no correlation at all.
    They look more like bar graphs, with no diagonal in sight. Before we move on from
    these plots, it's worth mentioning that these pairs plots only provide scatterplots
    for the compared variables and histograms for each of the variables in the plots.
    If we wanted to go deeper, we could look at other visualization tools, such as
    those in **Seaborn**.
  prefs: []
  type: TYPE_NORMAL
- en: We're pausing this analysis here. We've used visualization to understand where
    our data has correlations. If we were to take this problem further, we could use
    the data to create training models and help us in predictions. Even with the data
    we have from the graphics, we can see our correlations and make predictions using
    that. If we had `sqft_living`, we could predict `sqft_above`, for example, because
    of their strong correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Python allows us to look at data in a wide variety of ways. That's one of the
    great assets of a tool such as this one.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've learned about how to work with data in Python and tackled
    a housing dataset using some of the concepts we learned about in the chapter.
    We learned about the pandas package and how it helps us organize and prepare data.
    We also learned about the need to preprocess datasets, especially in very large
    datasets. We worked through missing and noisy data, as well as data transformation
    and the reduction of data. We also learned how to use visualization, creating
    plots for our datasets that can aid us in identifying correlations and trends.
  prefs: []
  type: TYPE_NORMAL
- en: The topics in this chapter are pretty broad, with entire books written about
    them. But we felt it important to share some of the capabilities of the Python
    programming language before moving on to the next two chapters of the book.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapters, we will focus entirely on applications, using problem
    scenarios and topics to share some exciting applications of Python and computational
    thinking in designing algorithms.
  prefs: []
  type: TYPE_NORMAL
