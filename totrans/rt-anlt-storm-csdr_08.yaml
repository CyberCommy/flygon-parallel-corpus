- en: Chapter 8. Cassandra Management and Maintenance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about the gossip protocol of Cassandra. Thereafter,
    we will delve into Cassandra administration and management in terms of understanding
    scaling and reliability in action. This will equip you with the ability to handle
    situations that you would not like to come across but do happen in production,
    such as handling recoverable nodes, rolling restarts, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that will be covered in the chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra—gossip protocol
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cassandra scaling—adding a new node to a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing a node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replication factor changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node tool commands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rolling restarts and fault tolerance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cassandra monitoring tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, this chapter will help you understand the basics of Cassandra, as well as
    the various options required for the maintenance and management of Cassandra activities.
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra – gossip protocol
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gossip is a protocol wherein periodically the nodes exchange information with
    other nodes about the nodes they know; this way, all the nodes obtain information
    about each other via this peer-to-peer communication mechanism. It's very similar
    to real-world and social media world gossip.
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra executes this mechanism every second, and one node is capable of exchanging
    gossip information with up to three nodes in the cluster. All these gossip messages
    have a version associated with them to track the chronology, and the older gossip
    interaction updates are overwritten chronologically by newer ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know what Cassandra''s gossip is like at a very high level, let''s
    have a closer look at it and understand the purpose of this chatty protocol. Here
    are the two broad purposes served by having this in place:'
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Failure scenario handling—detection and recovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's understand what they mean in action and what their contribution is towards
    the well-being and stability of a Cassandra cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bootstrapping is a process that is triggered in a cluster when a node joins
    the ring for the first time. It's the seed nodes that we define under the `Cassandra.yaml`
    configuration file that help the new nodes obtain the information about the cluster,
    ring, keyset, and partition ranges. It's recommended that you keep the setting
    similar throughout the cluster; otherwise, you could run into partitions within
    the cluster. A node remembers which nodes it has gossiped with even after it restarts.
    One more point to remember about seed nodes is that their purpose is to serve
    the nodes at the time of bootstrap; beyond this, its neither a single point of
    failure, nor does it serve any other purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Failure scenario handling – detection and recovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Well, the gossip protocol is Cassandra's own efficient way of knowing when a
    failure has occurred; that is, the entire ring gets to know about a downed host
    through gossip. On a contrary, situation when a node joins the cluster, the same
    mechanism is employed to inform the all nodes in the ring.
  prefs: []
  type: TYPE_NORMAL
- en: Once Cassandra detects a failure of a nodes on the ring, it stops routing the
    client requests to it—failure definitely has some impact on the overall performance
    of the cluster. However, it's never a blocker until we have enough replicas for
    consistency to be served to the client.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting fact about gossip is that it happens at various levels—Cassandra
    gossip, like real-world gossip, could be secondhand or thirdhand and so on; this
    is the manifestation of indirect gossip.
  prefs: []
  type: TYPE_NORMAL
- en: Failure of a node could be actual or virtual. This means that either a node
    can actually fail due to system hardware giving away, or the failure could be
    virtual, wherein, for a while, network latency is so high that it would seem that
    the node is not responding. The latter scenarios, most of the time, are self-recoverable;
    that is, after a while, networks return to normalcy, and the nodes are detected
    in the ring once again. The live nodes keep trying to ping and gossip with the
    failed node periodically to see if they are up. If a node is to be declared as
    permanently departed from the cluster, we require some admin intervention to explicitly
    remove the node from the ring.
  prefs: []
  type: TYPE_NORMAL
- en: When a node is joined back to the cluster after quite a while, it's possible
    that it might have missed a couple of writes (inserts/updates/deletes), and thus,
    the data on the node is far from being accurate as per the latest state of data.
    It's advisable to run a repair using the `nodetool repair` command.
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra cluster scaling – adding a new node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cassandra scales very easily, and with zero downtime. This is one of the reasons
    why it is chosen over many other contenders. The steps are pretty straightforward
    and simple:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to set up Cassandra on the nodes to be added. Don''t start the Cassandra
    process yet; first, follow these steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the seed nodes in `Cassandra.yaml` under `seed_provider`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure the `tmp` folders are clean.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add `auto_bootstrap` to `Cassandra.yaml` and set it to `true`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update `cluster_name` in `Cassandra.yaml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update `listen_address`/`broadcast_address` in `Cassandra.yaml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start all the new nodes one by one, pausing for at least 5 minutes between two
    consecutive starts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the node is started, it will proclaim its share of data based on the token
    range it owns and start streaming that in. This could be verified using the `nodetoolnetstat`
    command, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After all the nodes are joined to the cluster, it''s strictly recommended that
    you run a `nodetool cleanup` command on all the nodes. This is recommended so
    that they relinquish the control of the keys that were formerly owned by them
    but now belong to the new nodes that have joined the cluster. Here is the command
    and the execution output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `NodeCmd` process is actually the cleanup process for the Cassandra
    daemon. The disk space reclaimed after the cleanup on the preceding node is shown
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Cassandra cluster – replacing a dead node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section captures the various situations and scenarios that can occur and
    cause failures in a Cassandra cluster. We will also equip you with the knowledge
    and talk about the steps to handle these situations. These situations are specific
    to version 1.1.6 but can be applied to others as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say, this is the problem: you''re running an n node, for example let''s say
    there are three node clusters and from that one node goes down; this will result
    in unrecoverable hardware failure. The solution is this: replace the dead nodes
    with new nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps to achieve the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Confirm the node failure using the `nodetool ring` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The dead node will be shown as `DOWN`; let''s assume `node3` is down:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Install and configure Cassandra on the replacement node. Make sure we remove
    the old installation, if any, from the replaced Cassandra node using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, `/var/lib/cassandra` is the path of the Cassandra data directory for Cassandra.
  prefs: []
  type: TYPE_NORMAL
- en: Configure `Cassandra.yaml` so that it holds the same non-default settings as
    that of the pre-existing Cassandra cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the `initial_token` range in the `cassandra.yaml` file of the replacement
    node to the value of the dead node's token 1, that is, `42535295865117307932921825928971026431`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Starting the new node will join the cluster at one place prior to the dead
    node in the ring:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We are almost done. Just run `nodetool repair` on each node on each keyspace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove the token of the dead node from the ring using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This command needs to be executed on all the remaining nodes to make sure all
    the live nodes know that the dead node is no longer available.
  prefs: []
  type: TYPE_NORMAL
- en: This removes the dead node from the cluster; now we are done.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The replication factor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Occasionally, there are instances when we come across situations where we make
    changes to the replication factor. For example, I started with a smaller cluster
    so I kept my replication factor as 2\. Later, I scaled out from 4 nodes to 8 nodes,
    and thus to make my entire setup more fail-safe, I increased my replication factor
    to 4\. In such situations, the following steps are to be followed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the command to update the replication factor and/or change
    the strategy. Execute these commands on the Cassandra CLI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the command has been updated, you have to execute the `nodetool` repair
    on each of the nodes one by one (in succession) so that all the keys are correctly
    replicated as per the new replication values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The following `compactionstats` command is used to track the progress of the
    `nodetool repair` command.
  prefs: []
  type: TYPE_NORMAL
- en: The nodetool commands
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `nodetool` command in Cassandra is the most handy tool in the hands of
    a Cassandra administrator. It has all the tools and commands that are required
    for all types of situational handling of various nodes. Let''s look at a few widely
    used ones closely:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Ring`: This command depicts the state of nodes (normal, down, leaving, joining,
    and so on). The ownership of the token range and percentage ownership of the keys
    along with the data centre and rack details is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`Join`: This is the option you can use with `nodetool`, which needs to be executed
    to add the new node to the cluster. When a new node joins the cluster, it starts
    streaming the data from other nodes until it receives all the keys as per its
    designated ownership based on the token in the ring. The status for this can be
    checked using the `netsat` commands:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`Info`: This `nodetool` option gets all the required information about the
    node specified in the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`Cleanup`: This is the option that is generally used when we scale the cluster.
    New nodes are added and thus the existing nodes need to relinquish the control
    of the keys that now belong to the new entrants in the cluster:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`Compaction`: This is one of the most useful tools. It''s used to explicitly
    issue the `compact` command to Cassandra. This can be done on the entire node,
    key space, or at the column family level:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Cassandra has two types of compactions: minor compaction and major compaction.
    The minor cycle of compaction gets executed whenever a new `sstable` data is created
    to remove all the tombstones (that is, the deleted entries).'
  prefs: []
  type: TYPE_NORMAL
- en: The major compaction is something that's triggered manually, using the preceding
    `nodetool` command. This can be applied to the node, keyspace, and a column family
    level.
  prefs: []
  type: TYPE_NORMAL
- en: '`Decommission`: This is, in a way, the opposite of bootstrap and is triggered
    when we want a node to leave the cluster. The moment a live node receives the
    command, it stops accepting new rights, flushes the `memtables`, and starts streaming
    the data from itself to the nodes that would be a new owner of the key range it
    currently owns:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`Removenode`: This command is executed when a node is dead, that is, physically
    unavailable. This informs the other nodes about the node being unavailable. Cassandra
    replication kicks into action to restore the correct replication by creating copies
    of data as per the new ring ownership:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`Repair`: This `nodetool repair` command is executed to fix the data on any
    node. This is a very important tool to ensure that there is data consistency and
    the nodes that join the cluster back after a period of time exist. Let''s assume
    a cluster with four nodes that are catering to continuous writes through a storm
    topology. Here, one of the nodes goes down and joins the ring again after an hour
    or two. Now, during this duration, the node might have missed some writes; to
    fix this data, we should execute a `repair` command on the node:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Cassandra fault tolerance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, one of the prime reasons for using Cassandra as a data store is its fault-tolerant
    capabilities. It's not driven by a typical master-slave architecture, where failure
    of the master becomes a single point of system breakdown. Instead, it harbors
    a concept of operating in a ring mode so that there is no single point of failure.
    Whenever required, we can restart the nodes without the dread of bringing the
    whole cluster down; there are various situations where this capability comes in
    handy.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are situations where we need to restart Cassandra, but Cassandra''s ring
    architecture equips the administrator to do this seamlessly with zero downtime
    for the cluster. This means that in situations such as the following that requires
    a Cassandra cluster to be restarted, a Cassandra administrator can restart the
    nodes one by one instead of bringing down the entire cluster and then starting
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting the Cassandra daemon with changes in the memory configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling JMX on an already running Cassandra cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes machines have routine maintenance and need restarts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cassandra monitoring systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have discussed the various management aspects of Cassandra, let's
    explore the various dashboarding and monitoring options for the Cassandra cluster.
    There are various free and licensed tools available that we'll discuss now.
  prefs: []
  type: TYPE_NORMAL
- en: JMX monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use a type of monitoring for Cassandra that is based on `jconsole`.
    Here are the steps to connect to Cassandra using `jconsole`:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Command Prompt, execute the `jconsole` command:![JMX monitoring](img/00051.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next step, you have to specify the Cassandra node IP and port for connectivity:![JMX
    monitoring](img/00052.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you are connected, JMX provides a variety of graphs and monitoring utilities:![JMX
    monitoring](img/00053.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The developers can monitor heap memory usage using the jconsole **Memory** tab.
    This will help you understand the utilization of node resources.
  prefs: []
  type: TYPE_NORMAL
- en: The limitation with jconsole is that it performs node-specific monitoring and
    not Cassandra-ring-based monitoring and dashboarding. Let's explore the other
    tools in the context.
  prefs: []
  type: TYPE_NORMAL
- en: Datastax OpsCenter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a datastax-provided utility with a graphical interface that lets the
    user monitor and execute administrative activities from one central dashboard.
    Note that a free version is available only for nonproduction usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Datastax Ops center provides a lot of graphical representations for various
    important system **Key Performance Indicators** (**KPIs**), such as performance
    trends, summary, and so on. Its UI also provides a historic data analysis and
    drill down capability on single data points. OpsCenter stores all its metrics
    in Cassandra itself. The key features of the OpsCenter utility are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: KPI-based monitoring for the entire cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerts and alarms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to set up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can install and set up OpsCenter using the following simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to get started:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Connect to OpsCenter in a web browser at `http://localhost:8888`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will get a welcome screen where you will have options to spawn a new cluster
    or connect to an existing one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, configure the agent; once this is done, OpsCenter is ready for use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is a screenshot from the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Datastax OpsCenter](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here we choose the metric to be executed and whether the operation is to be
    performed on a specific node or all the nodes. The following screenshot captures
    OpsCenter starting up and recognizing the various nodes in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Datastax OpsCenter](img/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot captures various KPIs in the aspects of read and writes
    to the cluster, the overall cluster latency, disk I/O, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Datastax OpsCenter](img/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Quiz time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q.1\. State whether the following statements are true or false.
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra has a single point of failure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A dead node is immediately detected in a Cassandra ring.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gossip is a data exchange protocol.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `decommission` and `removenode` commands are same.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Q.2\. Fill in the blanks.
  prefs: []
  type: TYPE_NORMAL
- en: _______________ is the command used to run compactions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: _______________ is the command to get the information about a live node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ___________ is the command that displays the entire cluster information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Q.3\. Execute the following use case to see Cassandra high availability and
    replications:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a 4-node Cassandra cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating a keyspace with a replication factor of 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bringing down a Cassandra daemon on one the nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executing `nestat` on each node to see the data streaming.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the concepts of the gossip protocol and adapted
    tools used for various scenarios such as scaling the cluster, replacing a dead
    node, compaction, and repair operations on Cassandra.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss storm cluster maintenance and operational
    aspects.
  prefs: []
  type: TYPE_NORMAL
