- en: Evading Intrusion Detection Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying intrusion detection systems is essential for every modern company,
    in order to defend against attackers. In the previous chapters, we learned how
    to build machine learning, based intrusion detection systems. Now, it is time
    to learn how to bypass these systems with adversarial learning; to defend your
    systems, you need to learn how to attack them first.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial machine learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning threat models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evading intrusion detection systems with adversarial network systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will need the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: PyYAML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SciPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CVXPY
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Progress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pathos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CVXOPT (optional, as a CVXPY solver)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jupyter Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code files atÂ [https://github.com/PacktPublishing/Mastering-Machine-Learning-for-Penetration-Testing/tree/master/Chapter08](https://github.com/PacktPublishing/Mastering-Machine-Learning-for-Penetration-Testing/tree/master/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial machine learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before studying adversarial machine learning, let''s explore two important
    terminologies: overfitting and underfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting and underfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overfitting is one of the biggest obstacles that machine learning practitioners
    face. Knowing how to spot overfitting is a required skill for building robust
    machine learning models, because achieving 99% accuracy is not the end of the
    story. In machine learning, we make predictions. By definition, the **fit** is
    how well we approximate the target function. As we saw in the first chapter, the
    aim of supervised learning is to map the function between the input data and the
    targets. Thus, a good fit is a good approximation of that function.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting happens when a model learns the details and noise in the training
    data, to the extent that it negatively impacts the performance of the model. In
    other words, noise is picked up and learned by the model, so it can no longer
    generalize well when it is fed new data. The following graph illustrates an overfitting
    situation. You will notice that the model has been trained too well, which makes
    it hard to achieve accuracy when we feed the model with data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another obstacle is underfitting. This occurs when a machine learning model
    does not fit the data well enough. In other words, when the model is too simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00191.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Overfitting and underfitting with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at a real-world demonstration of overfitting and underfitting,
    with scikit-learn. Import the required modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now build a small model and visualize the model, the samples, and the
    `true` function, to see overfitting and underfitting. We will use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'By running the previous script, we draw the following graphs that illustrate
    3 cases: under-fitting, Good fitting and over-fitting (from left to right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00192.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following table was created using the terms highlighted in the previous
    code, and the corresponding URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Modules** | **URL** |'
  prefs: []
  type: TYPE_TB
- en: '| `plt.subplot` | [https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `plt.setp` | [http://matplotlib.org/api/_as_gen/matplotlib.pyplot.setp.html#matplotlib.pyplot.setp](http://matplotlib.org/api/_as_gen/matplotlib.pyplot.setp.html#matplotlib.pyplot.setp)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `PolynomialFeatures` | [http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `LinearRegression` | [http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Pipeline` | [http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `np.newaxis` | [http://docs.scipy.org/doc/numpy-1.8.1/reference/arrays.indexing.html#numpy.newaxis](http://docs.scipy.org/doc/numpy-1.8.1/reference/arrays.indexing.html#numpy.newaxis)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `cross_val_score` | [http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `np.newaxis` | [http://docs.scipy.org/doc/numpy-1.8.1/reference/arrays.indexing.html#numpy.newaxis](http://docs.scipy.org/doc/numpy-1.8.1/reference/arrays.indexing.html#numpy.newaxis)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `np.linspace` | [http://docs.scipy.org/doc/numpy-1.8.1/reference/generated/numpy.linspace.html#numpy.linspace](http://docs.scipy.org/doc/numpy-1.8.1/reference/generated/numpy.linspace.html#numpy.linspace)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `plt.plot` | [http://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot](http://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `plt.scatter` | [http://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html#matplotlib.pyplot.scatter](http://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html#matplotlib.pyplot.scatter)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `plt.xlabel` | [http://matplotlib.org/api/_as_gen/matplotlib.pyplot.xlabel.html#matplotlib.pyplot.xlabel](http://matplotlib.org/api/_as_gen/matplotlib.pyplot.xlabel.html#matplotlib.pyplot.xlabel)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `plt.ylabel` | [http://matplotlib.org/api/_as_gen/matplotlib.pyplot.ylabel.html#matplotlib.pyplot.ylabel](http://matplotlib.org/api/_as_gen/matplotlib.pyplot.ylabel.html#matplotlib.pyplot.ylabel)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `plt.xlim` | [http://matplotlib.org/api/_as_gen/matplotlib.pyplot.xlim.html#matplotlib.pyplot.xlim](http://matplotlib.org/api/_as_gen/matplotlib.pyplot.xlim.html#matplotlib.pyplot.xlim)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `plt.ylim` | [http://matplotlib.org/api/_as_gen/matplotlib.pyplot.ylim.html#matplotlib.pyplot.ylim](http://matplotlib.org/api/_as_gen/matplotlib.pyplot.ylim.html#matplotlib.pyplot.ylim)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `plt.legend` | [http://matplotlib.org/api/legend_api.html#matplotlib.legend](http://matplotlib.org/api/legend_api.html#matplotlib.legend)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `plt.title` | [http://matplotlib.org/api/_as_gen/matplotlib.pyplot.title.html#matplotlib.pyplot.title](http://matplotlib.org/api/_as_gen/matplotlib.pyplot.title.html#matplotlib.pyplot.title)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `plt.show` | [http://matplotlib.org/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show](http://matplotlib.org/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show)
    |'
  prefs: []
  type: TYPE_TB
- en: Detecting overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To detect overfitting, it is highly recommended to split the initial dataset
    into a training set and a testing set. If the training set performs way better
    than the testing set, then we have a problem. Also, it is highly recommended to
    start with a simple algorithm and move on to more complex models later, checking
    whether upgrading the level of complexity was worth it. To defend against overfitting,
    we can use cross-validation. Cross-validation is the process of evaluating many
    machine learning techniques by training models with different subsets (*k* subsets).
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AdversarialÂ machine learning is the art of studying how to break and secure
    machine learning models. You can consider it an intersection between machine learning
    and information security. As a security professional, learning how to build defensive
    layers with machine learning is important, but knowing how to break them is also
    an amazing addition to your skill set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00193.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In 2006, Barreno, et al., proposed a taxonomy for the threat models against
    machine learning systems. The model is based on three axes:'
  prefs: []
  type: TYPE_NORMAL
- en: Influence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security violations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specificity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In 2011, the model was extended by Huang, et al., to include another axis,
    called **privacy**. In 2016, Papernot, McDaniel, Jha, Fredrikson, Celik, and Swami,
    introduced a new taxonomy that focuses on only two axes:'
  prefs: []
  type: TYPE_NORMAL
- en: Complexity of the attack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge of the attacker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the machine learning threat taxonomy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00194.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To attack machine learning models, attackers can perform many techniques, addressed
    in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Evasion attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To perform machine learning evasion attacks, cyber criminals try to bypass the
    learning outcomes by observing how the model works,Â especially the outcome, by
    trying many different samples simply by feeding the model with different inputs
    and trying to find the learning patterns. This technique is very popular. For
    example, if an attacker wants to bypass a machine learning spam filter, he needs
    to feed the system with different emails andÂ search for a pattern that makes a
    spam email goes through (Not detected as a spam email) and bypass detection by
    doing only a few modifications to previously detected emails.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following workflow illustrates how an evasion attack works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00195.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Poisoning attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In machine learning poisoning attacks, attackers poison the model in order
    to change the learning outcome, by adding malicious data in the model training
    phase. This method can be performed, for example, by sending and injecting carefully
    designed samples when data collection is occurring during network operations,
    to train a network intrusion detection system model. The following workflow illustrates
    how a poisoning attack occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00196.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Some of the greatest research conducted on adversarial machine learning was
    done in theÂ *Pattern recognition and applications lab Italy,Â *including *Poisoning
    Attacks Against Support Vector Machines,*Â when Battista Biggio and his team presented
    a great framework to attack support vector machine systems. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify a proper adversary's goal
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the adversary's knowledge
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Formulate the corresponding optimization problem
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Resample the collected (training and test) data accordingly
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the classifier's security on the resampled data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the evaluation for different levels of adversary knowledge
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are familiar with MATLAB, I highly recommend that you to try **ALFASVMLib**.
    It is a MATLAB library on adversarial label flip attacks on SVM. You can download
    it from [https://github.com/feuerchop/ALFASVMLib](https://github.com/feuerchop/ALFASVMLib).
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering techniques are widely used in many real-world applications. Attackers
    are coming up with new techniques to attack clustering models. One of them is
    adversarial clustering, wherein the attackers manipulate the input data (adding
    a small percentage of attack samples), so that the newly added sample can hide
    within the existing clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature selection is an important step in every machine learning project. Attackers
    are also using adversarial feature selection to attack models. I highly recommend
    that you read the research done by the same team (*Pattern recognition and applications
    lab Italy researchers*), presented in a paper called,Â *Is Feature Selection Secure
    Against Training Data Poisoning?*
  prefs: []
  type: TYPE_NORMAL
- en: The team showed that by poisoning the embedded feature selection algorithms,
    including LASSO, ridge regression, and the ElasticNet, they fooled a PDF malware
    detector.
  prefs: []
  type: TYPE_NORMAL
- en: There are many Python frameworks and open source projects that were developed
    by researchers to attack and evaluate machine learning models, like **CleverHans**,
    theÂ **Adversarial Machine Learning**Â (**AML**) library,Â and **EvadeML-Zoo**.
  prefs: []
  type: TYPE_NORMAL
- en: CleverHans
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CleverHans is under continual development; it is an adversarial example library
    for constructing attacks, building defenses, and benchmarking machine learning
    systems' vulnerability to adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can clone it fromÂ [https://github.com/tensorflow/cleverhans](https://github.com/tensorflow/cleverhans):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00197.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Or, you can install it by using theÂ `pip` utility, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00198.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The AML library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The AML library is a game-theoretic adversarial machine learning library, developed
    by the Computational Economics Research Lab at Vanderbilt University. By game
    theory we mean the study of mathematical models of cooperation between intelligent
    decision making agents. You can clone the library fromÂ [https://github.com/vu-aml/adlib](https://github.com/vu-aml/adlib).
  prefs: []
  type: TYPE_NORMAL
- en: EvadeML-Zoo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: EvadeML-Zoo is a benchmarking and visualization tool for adversarial machine
    learning, developed by the Machine Learning Group and the Security Research Group
    at the University of Virginia. You can download it fromÂ [https://github.com/mzweilin/EvadeML-Zoo](https://github.com/mzweilin/EvadeML-Zoo).
  prefs: []
  type: TYPE_NORMAL
- en: Evading intrusion detection systems with adversarial network systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you will have acquired a fair understanding of adversarial machine learning,
    and how to attack machine learning models. It's time to dive deep into more technical
    details, learning how to bypass machine learning based intrusion detection systems
    with Python. You will also learn how to defend against those attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this demonstration, you are going to learn how to attack the model with
    a poisoning attack. As discussed previously, we are going to inject malicious
    data, so that we can influence the learning outcome of the model. The following
    diagram illustrates how the poisoning attack will occur:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00199.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this attack, we are going to use a **Jacobian-Based Saliency Map Attack**
    (**JSMA**). This is done by searching for adversarial examples by modifying only
    a limited number of pixels in an input.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at how to attack a machine based intrusion detection system with
    Python. The code is a little bit long, so I am only going to include some important
    snippets; later, you can find the full code in the chapter's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: For this project, we need the NumPy, pandas, Keras, CleverHans, TensorFlow,
    scikit-learn, and matplotlib Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are some of the imported libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is pre-processing the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then load the data with pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, concatenate the training and testing sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the data and identify the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To identify DoS attacks, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Identify other attacks (**User-to-Root** (**U2R**), **Remote-to -Local** (**R2L**),
    and **Probe**) with the same technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate one-hot encoding, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the training and testing sets again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To scale the data, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'An example ofÂ `scale X_train`Â is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s suppose that we are going to attack a logistic regression model; we
    need to process data to train that model and generate label encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We have now completed the pre-processing phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the Jacobian-Based Saliency Map attack, we are going to use the following
    Python implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To build a `MultiLayer Perceptron` network, use the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00200.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For adversarial prediction, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to evaluate the model by feeding it with the adversarial testing
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00201.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If you get errors, check the chapter's GitHub repository. The code may be updated
    and enhanced after publishing this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we provided an overview of adversarial learning techniques,
    and described how attackers and cyber criminals perform attacks against machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will be a great complementary guide, exploring how to attack
    artificial neural networks and deep learning networks. You will learn how attackers
    can bypass modern anti-malware systems by using adversarial deep learning and
    reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can you briefly explain why overtraining a machine learning model is not a good
    idea?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between overfitting and underfitting?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between an evasion and poisoning attack?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does adversarial clustering work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What type of adversarial attack is used to avoid the intrusion detection system?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the preceding attack an evasion or poisoning attack?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and
    Mitigation*: [https://img1.wsimg.com/blobby/go/3d82daa4-97fe-4096-9c6b-376b92c619de/downloads/1c6q2kc4v_50335.pdf](https://img1.wsimg.com/blobby/go/3d82daa4-97fe-4096-9c6b-376b92c619de/downloads/1c6q2kc4v_50335.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attacking Machine Learning with Adversarial Examples*: [https://blog.openai.com/adversarial-example-research/](https://blog.openai.com/adversarial-example-research/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Awesome Adversarial Machine Learning*: [https://github.com/yenchenlin/awesome-adversarial-machine-learning](https://github.com/yenchenlin/awesome-adversarial-machine-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ensemble Adversarial Training: Attacks and Defenses*: [https://arxiv.org/pdf/1705.07204.pdf](https://arxiv.org/pdf/1705.07204.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introduction to Adversarial Machine Learning*: [https://mascherari.press/introduction-to-adversarial-machine-learning/](https://mascherari.press/introduction-to-adversarial-machine-learning/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adversarial Deep Learning Against Intrusion Detection Classifiers*: [http://www.diva-portal.org/smash/get/diva2:1116037/FULLTEXT01.pdf](http://www.diva-portal.org/smash/get/diva2:1116037/FULLTEXT01.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Is Feature Selection Secure Against Training Data Poisoning?* ([http://pralab.diee.unica.it/sites/default/files/biggio15-icml.pdf](http://pralab.diee.unica.it/sites/default/files/biggio15-icml.pdf)
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Security evaluation of learning algorithms*: [http://pralab.diee.unica.it/en/SecurityEvaluation](http://pralab.diee.unica.it/en/SecurityEvaluation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*General Framework for AI and Security Threats*:Â [https://img1.wsimg.com/blobby/go/3d82daa4-97fe-4096-9c6b-376b92c619de/downloads/1c6q2kc4v_50335.pdf](https://img1.wsimg.com/blobby/go/3d82daa4-97fe-4096-9c6b-376b92c619de/downloads/1c6q2kc4v_50335.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The challenge of verification and testing of machine learning*:Â [http://www.cleverhans.io/security/privacy/ml/2017/06/14/verification.html:](http://www.cleverhans.io/security/privacy/ml/2017/06/14/verification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attacks Against Intrusion Detection Networks: Evasion, Reverse Engineering,
    and Optimal Countermeasures* (PhD thesis):Â [http://www.seg.inf.uc3m.es/~spastran/phd/PhD_Thesis_Sergio_Pastrana.pdf](http://www.seg.inf.uc3m.es/~spastran/phd/PhD_Thesis_Sergio_Pastrana.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
