- en: Chapter 3. Input Formats and Schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The aim of this chapter is to demonstrate how to load data from its raw format
    onto different schemas, therefore enabling a variety of different kinds of downstream
    analytics to be run over the same data. When writing analytics, or even better,
    building libraries of reusable software, you generally have to work with interfaces
    of fixed input types. Therefore, having flexibility in how you transition data
    between schemas, depending on the purpose, can deliver considerable downstream
    value, both in terms of widening the type of analysis possible and the re-use
    of existing code.
  prefs: []
  type: TYPE_NORMAL
- en: Our primary objective is to learn about the data format features that accompany
    Spark, although we will also delve into the finer points of data management by
    introducing proven methods that will enhance your data handling and increase your
    productivity. After all, it is most likely that you will be required to formalize
    your work at some point, and an introduction to how to avoid the potential long-term
    pitfalls is invaluable when writing analytics, and long after.
  prefs: []
  type: TYPE_NORMAL
- en: With this is mind, we will use this chapter to look at the traditionally well
    understood area of *data schemas*. We will cover key areas of traditional database
    modeling and explain how some of these cornerstone principles are still applicable
    to Spark.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, while honing our Spark skills, we will analyze the GDELT data model
    and show how to store this large dataset in an efficient and scalable manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dimensional modeling: benefits and weaknesses in relation to Spark'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on the GDELT model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lifting the lid on schema-on-read
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avro object model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parquet storage model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with some best practice.
  prefs: []
  type: TYPE_NORMAL
- en: A structured life is a good life
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When learning about the benefits of Spark and big data, you may have heard discussions
    about *structured* data versus *semi-structured* data versus *unstructured* data.
    While Spark promotes the use of structured, semi-structured, and unstructured
    data, it also provides the basis for its consistent treatment. The only constraint
    being that it should be *record-based*. Providing they are record-based, datasets
    can be transformed, enriched and manipulated in the same way, regardless of their
    organization.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is worth noting that having unstructured data does not necessitate
    taking an unstructured *approach*. Having identified techniques for exploring
    datasets in the previous chapter, it would be tempting to dive straight into stashing
    data somewhere accessible and immediately commencing simple profiling analytics.
    In real life situations, this activity often takes precedence over due diligence.
    Once again, we would encourage you to consider several key areas of interest,
    for example, file integrity, data quality, schedule management, version management,
    security, and so on, before embarking on this exploration. These should not be
    ignored and many are large topics in their own right.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, while we have already covered many of these concerns in [Chapter
    2](ch02.xhtml "Chapter 2. Data Acquisition"), *Data Acquisition*, and will study
    more later, for example in [Chapter 13](ch13.xhtml "Chapter 13. Secure Data"),
    *Secure Data*, in this chapter we are going to focus on data input and output
    formats specifically, exploring some of the methods that we can employ to ensure
    better data handling and management.
  prefs: []
  type: TYPE_NORMAL
- en: GDELT dimensional modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have chosen to use GDELT for analysis purposes in this book, we will introduce
    our first example using this dataset. First, let's select some data.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two streams of data available: **Global Knowledge Graph** (**GKG**)
    and **Events**.'
  prefs: []
  type: TYPE_NORMAL
- en: For this chapter, we are going to use GKG data to create a time-series dataset
    queryable from Spark SQL. This will give us a great starting point to create some
    simple introductory analytics.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapters, [Chapter 4](ch04.xhtml "Chapter 4. Exploratory Data Analysis"),
    *Exploratory Data Analysis* and [Chapter 5](ch05.xhtml "Chapter 5. Spark for Geographic
    Analysis"), *Spark for Geographic Analysis*, we'll go into more detail but stay
    with GKG. Then, in [Chapter 7](ch07.xhtml "Chapter 7. Building Communities"), *Building
    Communities*, we will explore events by producing our own network graph of persons
    and using it in some cool analytics.
  prefs: []
  type: TYPE_NORMAL
- en: GDELT model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GDELT has been around for more than 20 years and, during that time, has undergone
    some significant revisions. For our introductory examples, to keep things simple,
    let's limit our range of data from 1st April 2013, when GDELT had a major file
    structure overhaul, introducing the GKG files. It's worth noting that the principles
    discussed in this chapter are applicable to all versions of GDELT data, however,
    the specific schemas and **Uniform Resource Identifiers** (**URIs**) prior to
    this date may be different to the ones described. The version we will use is GDELT
    v2.1, which is the latest version at the time of writing. But again, it's worth
    noting that this varies only slightly from GDELT 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two data tracks within GKG data:'
  prefs: []
  type: TYPE_NORMAL
- en: The entire knowledge graph, along with all of its fields.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The subset of the graph, which contains a set of predefined categories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll look at the first track.
  prefs: []
  type: TYPE_NORMAL
- en: First look at the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We discussed how to download GDELT data in [Chapter 2](ch02.xhtml "Chapter 2. Data
    Acquisition"), *Data Acquisition*, so if you already have a NiFi pipeline configured
    to download the GKG data, just ensure that it's available in HDFS. However, if
    you have not completed that chapter, then we would encourage you to do this first,
    as it explains why you should take a structured approach to obtaining data.
  prefs: []
  type: TYPE_NORMAL
- en: While we have gone to great lengths to discourage the use of ad hoc data downloading,
    the scope of this chapter is of course known and therefore, if you are interested
    in following the examples seen here, you can skip the use of NiFi and obtain the
    data directly (in order to get started as quickly as possible).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you do wish to download a sample, here''s a reminder of where to find the
    GDELT 2.1 GKG master file list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Make a note of a couple of the latest entries that match `.gkg.csv.zip`, copy
    them using your favorite HTTP tool, and upload them into HDFS. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now that you have unzipped your CSV file and loaded it into HDFS, let's get
    on and look at the data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is not actually necessary to unzip data before loading to HDFS. Spark's `TextInputFormat` class
    supports compressed types and will decompress transparently. However, as we unzipped
    the content in our NiFi pipeline in the previous chapter, decompression is performed
    here for consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Core global knowledge graph model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are some important principles to understand which will certainly save
    time in the long run, whether in terms of computing or human effort. Like many
    CSVs, this file is hiding some complexity that, if not understood well at this
    stage, could become a real problem during our large scale analytics later. The
    GDELT documentation describes the data. It can be found here: [http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf](http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'It indicates that each CSV line is newline delimite, and structured as in *Figure
    1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Core global knowledge graph model](img/image_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1 GDELT GKG v2.1
  prefs: []
  type: TYPE_NORMAL
- en: On the face of it, this appears to be a nice, simple model whereby we can simply
    query a field and use the enclosed data-exactly like the CSV files we import and
    export to Microsoft Excel every day. However, if we examine the fields in more
    detail, it becomes clear that some of the fields are actually references to external
    sources and others are flattened data, actually represented by other tables.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden complexity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The flattened data structures in a core GKG model represent hidden complexity.
    For example, looking at field V2GCAM in the documentation, it outlines the idea
    that this is a series of comma-delimited blocks containing colon-delimited key-value
    pairs, the pairs representing GCAM variables, and their respective counts. Like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If we reference the GCAM specification, [http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT](http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT)
    we can translate this to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden complexity](img/table.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are also other fields that work in the same way, such as `V2Locations`,
    `V2Persons`, `V2Organizations`, and so on. So, what's really going on here? What
    are all these nested structures and why would you choose to represent data in
    this way? Actually, it turns out that this is a convenient way to collapse a **dimensional
    model** so that it can be represented in single line records without any loss
    of data or cross-referencing. In fact, it's a frequently used technique, known
    as *denormalization*.
  prefs: []
  type: TYPE_NORMAL
- en: Denormalized models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traditionally, a dimensional model is a database table structure that comprises
    many fact and dimension tables. They are often referred to as having star or snowflake
    schemas due to their appearance in entity-relation diagrams. In such a model,
    a *fact* is a value that can be counted or summed and typically provides a measurement
    at a given point in time. As they are often based on transactions, or repeating
    events, the number of facts are prone to growing very large. A *dimension* on
    the other hand is a logical grouping of information whose purpose is to qualify
    or contextualize facts. They usually provide an entry point for interpreting facts
    by means of grouping or aggregation. Also, dimensions can be hierarchical and
    one dimension can reference another. We can see a diagram of the expanded GKG
    dimensional structure in *Figure 2*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our GCAM example, the facts are the entries found in the above table, and
    the dimension is the GCAM reference itself. While this may seem like a simple,
    logical abstraction, it does mean that we have an important area of concern that
    we should consider carefully: dimensional modeling is great for traditional databases
    where data can be split into tables–in this case, GKG and GCAM tables–as these
    types of databases, by their very nature, are optimized for that structure. For
    example, the operations for looking up values or aggregating facts are available
    natively. When using Spark, however, some of the operations that we take for granted
    can be very expensive. For example, if we wanted to average all of the GCAM fields
    for millions of entries, then we would have a very large computation to perform.
    We will discuss this in more detail in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denormalized models](img/image_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2 GDELT GKG 2.1 expanded
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with flattened data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having explored the GKG data schema, we now know that the taxonomy is a typical
    star schema with a single fact table referencing multiple dimension tables. With
    this hierarchical structure, we will certainly struggle should we need to slice-and-dice
    data in the same way a traditional database would allow.
  prefs: []
  type: TYPE_NORMAL
- en: But what makes it so difficult to process on Spark? Let's look at three different
    issues inherent with this type of organization.
  prefs: []
  type: TYPE_NORMAL
- en: Issue 1 - Loss of contextual information
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, there is the matter of the various arrays used within each record of
    the dataset. For example, `V1Locations`, `V1Organizations`, and `V1Persons` fields
    all contain a list of 0 or more objects. As we do not have the original body of
    the text used to derive this information (although we can sometimes obtain it
    if the source is WEB, JSTOR, and so on, since those will contain links to the
    source document), we lose the context of the relationships between the entities.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have [Barack Obama, David Cameron, Francois Hollande, USA,
    France, GB, Texaco, Esso, Shell] in our data, then we could make the assumption
    that the source article is related to a meeting between heads of state over an
    oil crisis. However, this is only an assumption and may not be the case, if we
    were truly objective, we could equally assume that the article was related to
    companies who had employees with famous names.
  prefs: []
  type: TYPE_NORMAL
- en: To help us to infer these relationships between entities, we can develop a time
    series model that takes all of the individual contents of a GDELT field, over
    a certain time period, and performs an expansion join. Thus, on a simple level,
    those pairs that are seen more often are more likely to actually relate to each
    other and we can start to make some more concrete assumptions. For example, if
    we see [Barack Obama, USA] 100,000 times in our timeseries and [Barack Obama,
    France] only 5000 times, then it is very likely that there is a strong relationship
    between the first pair, and a secondary relationship between the second. In other
    words, we can identify the tenuous relationships and remove them when needed.
    This method can be used at scale to identify relationships between apparently
    unrelated entities. In [Chapter 7](ch07.xhtml "Chapter 7. Building Communities"), *Building
    Communities*, we use this principle to identify relationships between some very
    unlikely people!
  prefs: []
  type: TYPE_NORMAL
- en: 'Issue 2: Re-establishing dimensions'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With any denormalized data it should be possible to reconstruct, or inflate,
    the original dimensional model. With this in mind, let''s look at a useful Spark
    function that will help us to expand our arrays and produce a flattened result;
    it''s called `DataFrame.explode`, and here''s an illustrative example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Using this method, we can easily expand arrays and then perform the grouping
    of our choice. Once expanded, the data is readily aggregated using the `DataFrame`
    methods and can even be done using SparkSQL. An example of this can be found in
    the Zeppelin notebooks in our repository.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand that, while this function is simple to implement,
    it is not necessarily performant and may hide the underlying processing complexity
    required. In fact, there is an example of the explode function using GKG data
    within the Zeppelin notebook that accompanies this chapter, whereby, if the explode
    functions are not reasonably scoped, then the function returns a heap space issue
    as it runs out of memory.
  prefs: []
  type: TYPE_NORMAL
- en: This function does not solve the inherent problem of consuming large amounts
    of system resources, and so you should still take care when using it. And while
    this general problem cannot be solved, it can be managed by performing only the
    groupings and joins necessary, or by calculating them ahead of time and ensuring
    they complete within the resources available. You may even wish to write an algorithm
    that splits a dataset and performs the grouping sequentially, persisting each
    time. We explore methods to help us with this problem, and other common processing
    issues, in [Chapter 14](ch14.xhtml "Chapter 14. Scalable Algorithms"), *Scalable
    Algorithms*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Issue 3: Including reference data'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For this issue, let''s look at the GDELT event data, which we have expanded
    in *Figure 3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Issue 3: Including reference data](img/image_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3 GDELT Events Taxonomy
  prefs: []
  type: TYPE_NORMAL
- en: 'This type of diagrammatic representation draws attention to the relationships
    in the data and gives an indication of how we might want to inflate it. Here,
    we see many fields that are just codes and would require translation back into
    their original descriptions in order to present anything meaningful. For example,
    in order to interpret the `Actor1CountryCode` (GDELT events), we will need to
    join the event data with one or more separate reference datasets that provide
    the translation text. In this case, the documentation tells us to reference the
    CAMEO dataset located here: [http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf](http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: This type of join has always presented a serious problem at data scale and there
    are various ways to handle it depending upon the given scenario - it is important
    at this stage to understand exactly how your data will be used, which joins may
    be required immediately, and which may be deferred until sometime in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case where we choose to completely denormalize, or flatten, the data
    before processing, then it makes sense to do the join upfront. In this case, follow-on
    analytics will certainly be more efficient, as the relevant joins have already
    been completed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For each code in the record, there is a join to the respective reference table,
    and the entire record becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is a simple change, but is one that uses a lot of disk space if performed
    across large numbers of rows. The trade-off is that the joins have to be performed
    at some point, perhaps at ingest or as a regular batch job after ingest; it is
    perfectly reasonable to ingest the data as is, and perform flattening of the dataset
    at a time that is convenient to the user. In any case, the flattened data can
    be consumed by any analytic and data analysts need not concern themselves with
    this potentially hidden issue.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, often, deferring the join until later in the processing can
    mean that there are fewer records to join with – as there may have been aggregation
    steps in the pipeline. In this case, joining to tables at the last possible opportunity
    pays off because, often, the reference or dimension tables are small enough to
    be broadcast joins, or map-side joins. As this is such an important topic, we
    will continue to look at different ways of approaching join scenarios throughout
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: Loading your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have outlined in previous chapters, traditional system engineering commonly
    adopts a pattern to move the data from its source to its destination, that is,
    ETL, whereas Spark tends to rely on schema-on-read. As it''s important to understand
    how these concepts relate to schemas and input formats, let''s describe this aspect
    in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading your data](img/B05261_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: On the face of it, the ETL approach seems to be sensible, and indeed has been
    implemented by just about every organization that stores and handles data. There
    are some very popular, feature-rich products out there that perform the ETL task
    very well - not to mention Apache's open source offering, Apache Camel [http://camel.apache.org/etl-example.html](http://camel.apache.org/etl-example.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this apparently straightforward approach belies the true effort required
    to implement even a simple data pipeline. This is because we must ensure that
    all data complies with a fixed schema before we can use it. For example, if we
    wanted to ingest some data from a starting directory, the minimal work is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure we are always looking at the pickup directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When data arrives, collect it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure the data is not missing anything and validate according to a predefined
    ruleset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the parts of the data that we are interested in, according to a predefined
    ruleset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform these selected parts according to a predefined schema.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the data to a repository (for example, a database) using the correct versioned
    schema.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deal with any failed records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can immediately see a number of formatting issues here that must be addressed:'
  prefs: []
  type: TYPE_NORMAL
- en: We have a predefined ruleset and, therefore, this must be version controlled.
    Any mistakes will mean bad data in the end database and a re-ingest of that data
    through the ETL process to correct it (very time and resource expensive). Any
    change to the format of the inbound dataset, and this ruleset must be changed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any change to the target schema will require very careful management. At the
    very least, a version control change in the ETL, and possibly even a reprocessing
    of some or all of the previous data (which could be a very time consuming and
    expensive backhaul).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any change to the end repository will result in at least a version control schema
    change, and perhaps even a new ETL module (again, very time and resource intensive).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inevitably, there will be some bad data that makes it through to the database.
    Therefore, an administrator will need set rules to monitor the referential integrity
    of tables to ensure damage is kept to a minimum and arrange for the re-ingestion
    of any corrupted data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we now consider these issues and massively increase the volume, velocity,
    variety, and veracity of the data, it is easy to see that our straightforward
    ETL system has quickly grown into a near unmanageable system. Any formatting,
    schema, and business rule changes will have a negative impact. In some cases,
    there may not be enough processor and memory resources to even keep pace, due
    to all the processing steps required. Data cannot be ingested until all of the
    ETL steps have been agreed and are in place. In large corporations it can take
    months to agree schema transforms before any implementation even commences, thus
    resulting in a large backlog, or even loss of data. All this results in a brittle
    system that is difficult to change.
  prefs: []
  type: TYPE_NORMAL
- en: Schema agility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To overcome this, schema-on-read encourages us to shift to a very simple principle:
    *apply schema to the data at runtime, as opposed to applying it on load (that
    is, at ingest)*. In other words, a schema is applied to the data when it is read
    in for processing. This simplifies the ETL process somewhat:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Schema agility](img/B05261_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Of course, it does not mean you eliminate the transform step entirely. You're
    simply *deferring* the act of validation, applying business rules, error handling,
    ensuring referential integrity, enriching, aggregating, and otherwise inflating
    the model until the point you are ready to use it. The idea is that, by this point,
    you should know more about the data and certainly about the way you wish to use
    it. Therefore, you can use this increased knowledge of the data to effect efficiencies
    in the loading method. Again, this is a trade-off. What you save in upfront processing
    costs, you may lose in duplicate processing and potential inconsistency. However,
    techniques such as persistence, indexing, memorization, and caching can all help
    here. As mentioned in the previous chapter, this process is commonly known as
    ELT due to the reversal in the order of processing steps.
  prefs: []
  type: TYPE_NORMAL
- en: One benefit of this approach is that it allows greater freedom to make appropriate
    decisions about the way you represent and model data for any given use case. For
    example, there are a variety of ways that data can be structured, formatted, stored,
    compressed, or serialized, and it makes sense to choose the most appropriate method
    given the set of specific requirements related to the particular problem you are
    trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important opportunities that this approach provides is that
    you can choose how to physically lay out the data, that is, decide on the directory
    structure where data is kept. It is generally not advised to store all your data
    in a single directory because, as the number of files grows, it takes punitively
    longer amounts of time for the underlying filesystem to address them. But, ideally,
    we want to be able to specify the smallest possible data split to fulfill the
    functionality and efficiently store and retrieve at the volumes required. Therefore,
    data should be logically grouped depending upon the analysis that is required
    and the amount of data that you expect to receive. In other words, data may be
    divided across directories based upon type, subtype, date, time, or some other
    relevant property, but it should be ensured that no single directory bears undue
    burden. Another important point to realize here is that, once the data is landed,
    it can always be reformatted or reorganized at a later date, whereas, in an ETL
    paradigm, this is usually far more difficult.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, ELT can also have a surprising benefit on **change management**
    and **version control**. For example, if external factors cause the data schema
    to change, you can simply load different data to a new directory in your data
    store and use a flexible schema tolerant serialization library, such as Avro or
    Parquet, which both support **schema evolution** (we will look at these later
    in this chapter); or, if the results of a particular job are unsatisfactory, we
    need only change the internals of that one job before rerunning it. This means
    that schema changes become something that can be managed on a per analytic basis,
    rather than on a per feed basis, and the impact of change is better isolated and
    managed.
  prefs: []
  type: TYPE_NORMAL
- en: By the way, it's worth considering a hybrid approach, particularly useful in
    streaming use cases, whereby some processing can be done during collection and
    ingest, and others during runtime. The decision around whether to use ETL or ELT
    is not necessarily a binary one. Spark provides features that give you control
    over your data pipelines. In turn, this affords you the flexibility to transform
    or persist data when it makes sense to do so, rather than adopting a one-size-fits-all
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to determine which approach to take is to learn from the actual
    day-to-day use of a particular dataset and adjust its processing accordingly,
    identifying bottlenecks and fragility as more experience is gained. There may
    also be corporate rules levied, such as virus scanning or data security, which
    will determine a particular route. We'll look more into this at the end of the
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Reality check
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with most things in computing, there's no silver bullet. ELT and schema-on-read
    will not fix all your data formatting problems, but they are useful tools in the
    toolbox and, generally speaking, the pros usually outweigh the cons. It is worth
    noting, however, that there are situations where you can actually introduce difficulties
    if you're not careful.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, it can be more involved to perform ad hoc analysis on complex
    data models (as opposed to in databases). For example, in the simple case of extracting
    a list of all of the names of the cities mentioned in news articles, in a SQL
    database you could essentially run `select CITY from GKG`, whereas, in Spark,
    you first need to understand the data model, parse and validate the data, and
    then create the relevant table and handle any errors on-the-fly, sometimes each
    time you run the query.
  prefs: []
  type: TYPE_NORMAL
- en: Again, this is a trade-off. With schema-on-read you lose the built-in data representation
    and inherent knowledge of a fixed schema, but you gain the flexibility to apply
    different models or views as required. As usual, Spark provides features designed
    to assist in exploiting this approach, such as, transformations, `DataFrames`,
    `SparkSQL`, and REPL, and when used properly, they allow you to maximize the benefits
    of schema-on-read. We'll learn more about this as we go furthur.
  prefs: []
  type: TYPE_NORMAL
- en: GKG ELT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As our NiFi pipeline writes data as is to HDFS, we can take full advantage of
    schema-on-read and immediately start to use it without having to wait for it to
    be processed. If you would like to be a bit more advanced, then you could load
    the data in a splittable and/or zipped format such as `bzip2` (native to Spark).
    Let's take a look at a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HDFS uses a block system to store data. In order to store and leverage data
    in the most efficient way, HDFS files should be splittable where possible. If
    the CSV GDELT files are loaded using `TextOutputFormat` class, for example, then
    files larger than the block size will be split across filesize/blocksize blocks.
    Partial blocks do not occupy a full block size on disk.
  prefs: []
  type: TYPE_NORMAL
- en: By using `DataFrames`, we can write SQL statements to explore the data or with
    datasets we can chain fluent methods, but in either case there is some initial
    preparation required.
  prefs: []
  type: TYPE_NORMAL
- en: 'The good news is that usually this can be done entirely by Spark, as it supports
    the transparent loading of data into Datasets via case classes, using **Encoders**
    and so most of the time you won''t need to worry too much about the inner workings.
    Indeed, when you have a relatively simple data model, it''s usually enough to
    define a case class, map your data onto it, and convert to a dataset using `toDS`
    method. However, in most real-world scenarios, where data models are more complex,
    you will be required to write your own custom parser. Custom parsers are nothing
    new in data engineering, but in a schema-on-read setting, they are often required
    to be used by data scientists, as the interpretation of data is done at runtime
    and not load time. Here''s an example of the use of the custom GKG parser to be
    found in our repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You can seen preceding that, once the data is parsed, it can be used in the
    full variety of Spark APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re more comfortable using SQL, you can define your own schema, register
    a table, and use SparkSQL. In either approach, you can choose how to load the
    data based on how it will be used, allowing for more flexibility over which aspects
    you spend time parsing. For example, the most basic schema for loading GKG is
    to treat every field as a String, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And now you can execute SQL queries, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: With this approach, you can start profiling the data straight away and it's
    useful for many data engineering tasks. When you're ready, you can choose other
    elements of the GKG record to expand. We'll see more about this in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have a DataFrame, you can convert it into a Dataset by defining a
    case class and casting, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Position matters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's worth noting here that, when loading data from CSV, Spark's schema matching
    is entirely *positional*. This means that, when Spark tokenizes a record based
    on the given separator, it assigns each token to a field in the schema using its
    position, even if a header is present. Therefore, if a column is omitted in the
    schema definition, or your dataset changes over time due to data drift or data
    versioning, you may get a misalignment that Spark will not necessarily warn you
    about!
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we recommend doing basic data profiling and data quality checks
    on a routine basis to mitigate these situations. You can use the built-in functions
    in `DataFrameStatFunctions` to assist with this. Some examples are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's explain a great way to put some structure around our code, and also
    reduce the amount of code written, by using Avro or Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: Avro
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how easy it can be to ingest some data and use Spark to analyze
    it without the need for any traditional ETL tools. While it is very useful to
    work in an environment where schemas are all but ignored, this is not realistic
    in the commercial world. There is, however, a good middle ground, which gives
    us some great advantages over both ETL and unbounded data processing-Avro.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Avro is serialization technology, similar in purpose to Google''s protocol
    buffers. Like many other serialization technologies, Avro uses a schema to describe
    data, but the key to its usefulness is that it provides the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**It stores the schema alongside the data**. This allows for efficient storage
    because the schema is only stored once, at the top of the file. It also means
    that data can be read even if the original class files are no longer available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It supports schema-on-read and schema evolution**. This means it can implement
    different schemas for the reading and writing of data, providing the advantages
    of schema versioning without the disadvantages of large administrative overhead
    every time we wish to make data amendments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It is language agnostic**. Therefore, it can be used with any tool or technology
    that allows custom serialization framework. It is particularly useful for writing
    directly to Hive, for example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As Avro stores the schema with the enclosed data, it is *self-describing*. So
    instead of struggling to read the data because you have no classes, or trying
    to guess which version of a schema applies, or in the worst case having to throw
    away the data altogether, we can simply interrogate the Avro file for the schema
    that the data was written with.
  prefs: []
  type: TYPE_NORMAL
- en: Avro also allows amendments to a schema in the form of additive changes, or
    appends, that can be accommodated thus making a specific implementation backwards
    compatible with older data.
  prefs: []
  type: TYPE_NORMAL
- en: As Avro represents data in a binary form, it can be transferred and manipulated
    more efficiently. Also, it takes up less space on disk due to its inherent compression.
  prefs: []
  type: TYPE_NORMAL
- en: For the reasons stated above, Avro is an incredibly popular serialization format,
    used by a wide variety of technologies and end-systems, and you will no doubt
    have cause to use it at some point. Therefore, in the next sections we will demonstrate
    two different ways to read and write Avro-formatted data. The first is an elegant
    and simple method that uses a third party, purpose-built library, called `spark-avro`,
    and the second is an under-the-covers method, useful for understanding how the
    mechanics of Avro work.
  prefs: []
  type: TYPE_NORMAL
- en: Spark-Avro method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To address the complexities of implementing Avro, the `spark-avro` library
    has been developed. This can be imported in the usual ways, using maven:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For this implementation, we will create the Avro schema using a `StructType`
    object, transform the input data using an `RDD`, and create a `DataFrame` from
    the two. Finally, the result can be written to file, in Avro format, using the
    `spark-avro` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `StructType` object is a variation on the `GkgCoreSchema` used above and
    in [Chapter 4](ch04.xhtml "Chapter 4. Exploratory Data Analysis"), *Exploratory
    Data Analysis*, and is constructed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We have used a number of custom `StructTypes`, which could be specified inline
    for `GkgSchema`, but which we have broken out for ease of reading.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, `GkgRecordIdStruct` is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we use this schema, we must first produce an `RDD` by parsing the input
    GDELT data into a `Row`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here you see a number of custom parsing functions, for instance, `createGkgRecordID`,
    that take raw data and contain the logic for reading and interpreting each field.
    As GKG fields are complex and often contain *nested data structures*, we need
    a way to embed them into the `Row`. To help us out, Spark allows us to treat them
    as `Rows` inside `Rows`. Therefore, we simply write parsing functions that return
    `Row` objects, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting the code together, we see the entire solution in just a few lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Reading the Avro files into a `DataFrame` is similarly simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This gives a neat solution for dealing with Avro files, but what's going on
    under the covers?
  prefs: []
  type: TYPE_NORMAL
- en: Pedagogical method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to explain how Avro works, let's take a look at a roll your own solution.
    In this case, the first thing we need to do is to create an Avro schema for the
    version or versions of data that we intend to ingest.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are Avro implementations for several languages, including Java. These
    implementations allow you to generate bindings for Avro so that you can serialize
    and deserialize your data objects efficiently. We are going to use a maven plugin
    to help us automatically compile these bindings using an Avro IDL representation
    of the GKG schema. The bindings will be in the form of a Java class that we can
    use later on to help us build Avro objects. Use the following imports in your
    project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now take a look at our Avro IDL schema created from a subset of the
    available Avro types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: val inputFile = new File("gkg.csv");
  prefs: []
  type: TYPE_NORMAL
- en: val outputFile = new File("gkg.avro");
  prefs: []
  type: TYPE_NORMAL
- en: val userDatumWriter = new
  prefs: []
  type: TYPE_NORMAL
- en: SpecificDatumWriter[Specification](classOf[Specification])
  prefs: []
  type: TYPE_NORMAL
- en: val dataFileWriter = new
  prefs: []
  type: TYPE_NORMAL
- en: DataFileWriter[Specification](userDatumWriter)
  prefs: []
  type: TYPE_NORMAL
- en: dataFileWriter.create(Specification.getClassSchema, outputFile)
  prefs: []
  type: TYPE_NORMAL
- en: for (line <- Source.fromFile(inputFile).getLines())
  prefs: []
  type: TYPE_NORMAL
- en: dataFileWriter.append(generateAvro(line))
  prefs: []
  type: TYPE_NORMAL
- en: dataFileWriter.close()
  prefs: []
  type: TYPE_NORMAL
- en: 'def generateAvro(line: String): Specification = {'
  prefs: []
  type: TYPE_NORMAL
- en: val values = line.split("\t",-1)
  prefs: []
  type: TYPE_NORMAL
- en: if(values.length == 27){
  prefs: []
  type: TYPE_NORMAL
- en: val specification = Specification.newBuilder()
  prefs: []
  type: TYPE_NORMAL
- en: .setGkgRecordId(createGkgRecordId(values{0}))
  prefs: []
  type: TYPE_NORMAL
- en: .setV21Date(values{1}.toLong)
  prefs: []
  type: TYPE_NORMAL
- en: .setV2SourceCollectionIdentifier(
  prefs: []
  type: TYPE_NORMAL
- en: createSourceCollectionIdentifier(values{2}))
  prefs: []
  type: TYPE_NORMAL
- en: .setV21SourceCommonName(values{3})
  prefs: []
  type: TYPE_NORMAL
- en: .setV2DocumentIdentifier(values{4})
  prefs: []
  type: TYPE_NORMAL
- en: .setV1Counts(createV1CountArray(values{5}))
  prefs: []
  type: TYPE_NORMAL
- en: .setV21Counts(createV21CountArray(values{6}))
  prefs: []
  type: TYPE_NORMAL
- en: .setV1Themes(createV1Themes(values{7}))
  prefs: []
  type: TYPE_NORMAL
- en: .setV2EnhancedThemes(createV2EnhancedThemes(values{8}))
  prefs: []
  type: TYPE_NORMAL
- en: .setV1Locations(createV1LocationsArray(values{9}))
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'def createSourceCollectionIdentifier(str: String) :    SourceCollectionIdentifier
    = {'
  prefs: []
  type: TYPE_NORMAL
- en: str.toInt match {
  prefs: []
  type: TYPE_NORMAL
- en: case 1 => SourceCollectionIdentifier.WEB
  prefs: []
  type: TYPE_NORMAL
- en: case 2 => SourceCollectionIdentifier.CITATIONONLY
  prefs: []
  type: TYPE_NORMAL
- en: case 3 => SourceCollectionIdentifier.CORE
  prefs: []
  type: TYPE_NORMAL
- en: case 4 => SourceCollectionIdentifier.DTIC
  prefs: []
  type: TYPE_NORMAL
- en: case 5 => SourceCollectionIdentifier.JSTOR
  prefs: []
  type: TYPE_NORMAL
- en: case 6 => SourceCollectionIdentifier.NONTEXTUALSOURCE
  prefs: []
  type: TYPE_NORMAL
- en: case _ => SourceCollectionIdentifier.WEB
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'def createV1LocationsArray(str: String): Array[Location] = {'
  prefs: []
  type: TYPE_NORMAL
- en: val counts = str.split(";")
  prefs: []
  type: TYPE_NORMAL
- en: counts map(createV1Location(_))
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: When to perform Avro transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to make best use of Avro, next, we need to decide when it is best
    to transform the data. Converting to Avro is a relatively expensive operation,
    so it should be done at the point when it makes most sense. Once again, it''s
    a tradeoff. This time, it''s between a flexible data model supporting unstructured
    processing, exploratory data analysis, ad hoc querying, and a structured type
    system. There are two main options to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convert as late as possible**: it is possible to perform Avro conversion
    in each and every run of a job. There are some obvious drawbacks here, so it''s
    best to consider persisting Avro files at some point, to avoid the recalculation.
    You could do this lazily upon the first time, but chances are this would get confusing
    quite quickly. The easier option is to periodically run a batch job over the data
    at rest. This job''s only task would be to create Avro data and write it back
    to disk. This approach gives us full control over when the conversion jobs are
    executed. In busy environments, jobs can be scheduled for quiet periods and priority
    can be allocated on an ad hoc basis. The downside is that we need to know how
    long the processing is going to take in order to ensure there is enough time for
    completion. If processing is not completed before the next batched data arrives,
    then a backlog builds and it can be difficult to catch up.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Convert as early as possible**: the alternative approach is to create an
    ingest pipeline, whereby the incoming data is converted to Avro on the fly (particularly
    useful in streaming scenarios). By doing this, we are in danger of approaching
    an ETL-style scenario, so it is really a judgment call as to which approach best
    suits the specific environment in use at the time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's look at a related technology that is used extensively throughout
    Spark, that is Apache Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: Parquet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Parquet is a columnar storage format specifically designed for the Hadoop
    ecosystem. Traditional row-based storage formats are optimized to work with one
    record at a time, meaning they can be slow for certain types of workload. Instead,
    Parquet serializes and stores data by column, thus allowing for optimization of
    storage, compression, predicate processing, and bulk sequential access across
    large datasets - exactly the type of workload suited to Spark!
  prefs: []
  type: TYPE_NORMAL
- en: As Parquet implements per column data compaction, it's particularly suited to
    CSV data, especially with fields of low cardinality, and file sizes can see huge
    reductions when compared to Avro.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parquet also integrates with Avro natively. Parquet takes an Avro in-memory
    representation of data and maps to its internal data types. It then serializes
    the data to disk using the Parquet columnar file format.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen how to apply Avro to the model, now we can take the next step
    and use this Avro model to persist data to disk via the Parquet format. Again,
    we will show the current method and then some lower-level code for demonstrative
    purposes. First, the recommended method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now for the detail behind how Avro and Parquet relate to each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As before, the lower-level code is quite verbose, although it does give some
    insight into the various steps required. You can find the full code in our repository.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a great model to store and retrieve our GKG data that uses Avro
    and Parquet and can easily be implemented using `DataFrames`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen why datasets should always be thoroughly understood
    before too much exploration work is undertaken. We have discussed the details
    of structured data and dimensional modeling, particularly with respect to how
    this applies to the GDELT dataset, and have expanded the GKG model to show its
    underlying complexity.
  prefs: []
  type: TYPE_NORMAL
- en: We have explained the difference between the traditional ETL and newer schema-on-read
    ELT techniques, and have touched upon some of the issues that data engineers face
    regarding data storage, compression, and data formats - specifically the advantages
    and implementations of Avro and Parquet. We have also demonstrated that there
    are several ways to explore data using the various Spark API, including examples
    of how to use SQL on the Spark shell.
  prefs: []
  type: TYPE_NORMAL
- en: We can conclude this chapter by mentioning that the code in our repository pulls
    everything together and is a full model for reading in raw GKG files (use the
    Apache NiFi GDELT data ingest pipeline from [Chapter 1](ch02.xhtml "Chapter 2. Data
    Acquisition"),* Data Acquisition* if you require some data).
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive deeper into the GKG model by exploring the
    techniques used to explore and analyze data at scale. We will see how to develop
    and enrich our GKG data model using SQL, and investigate how Apache Zeppelin notebooks
    can provide a richer data science experience.
  prefs: []
  type: TYPE_NORMAL
