- en: Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chapter 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is the idea behind concurrency, and why is it useful?**'
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency is about designing and structuring program commands and instructions
    so that different sections of the program can be executed in an efficient order,
    while sharing the same resources.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the differences between concurrent programming and sequential programming?**'
  prefs: []
  type: TYPE_NORMAL
- en: In sequential programming, the commands and instructions are executed one at
    the time, in a sequential order. In concurrent programming, some sections might
    be executed in an efficient way for better execution time.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the differences between concurrent programming and parallel programming?**'
  prefs: []
  type: TYPE_NORMAL
- en: In parallel programming, the separate sections of a program are independent
    of one another; they do not interact with one another, and therefore, they can
    be executed simultaneously. In concurrent programming, the separate tasks share
    the same resources, and some form of coordination between them is therefore required.
  prefs: []
  type: TYPE_NORMAL
- en: '**Can every program be made concurrent or parallel?**'
  prefs: []
  type: TYPE_NORMAL
- en: No.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are embarrassingly parallel tasks?**'
  prefs: []
  type: TYPE_NORMAL
- en: Embarrassingly parallel tasks can be divided into separate, independent sections,
    with little or no effort.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are inherently sequential tasks?**'
  prefs: []
  type: TYPE_NORMAL
- en: Tasks wherein the order of execution of individual sections is crucial to the
    results of the tasks, which cannot be made concurrent or parallel to obtain better
    execution time, are called inherently sequential.
  prefs: []
  type: TYPE_NORMAL
- en: '**What does I/O bound mean?**'
  prefs: []
  type: TYPE_NORMAL
- en: This is a condition in which the time it takes to complete a computation is
    determined mainly by the time spent waiting for input/output operations to be
    completed.
  prefs: []
  type: TYPE_NORMAL
- en: '**How is concurrent processing currently being used in the real world?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrency can be found almost everywhere: desktop and mobile applications,
    video games, web and internet development, artificial intelligence, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is Amdahl''s law? What problem does Amdahl''s law look to solve?**'
  prefs: []
  type: TYPE_NORMAL
- en: Amdahl's law provides an estimate of the theoretical speedup in latency of the
    execution of a task at fixed workload that can be expected of a system whose resources
    are improved.
  prefs: []
  type: TYPE_NORMAL
- en: '**Explain the formula of Amdahl''s Law, along with its components.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for Amdahl''s Law is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cf7ccda6-88e4-4f0c-99b5-d8c344bf900e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding formula, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*S* is the theoretical speedup in consideration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*B* is the portion of the whole task that is inherently sequential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*j* is the number of processors being utilized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**According to Amdahl''s Law, would speedup increase indefinitely as resources
    in the system improved?**'
  prefs: []
  type: TYPE_NORMAL
- en: No; as the number of processors becomes larger, the efficiency gained through
    the improvement decreases.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the relationship between Amdahl''s Law and the law of diminishing
    returns?**'
  prefs: []
  type: TYPE_NORMAL
- en: You have seen that in specific situations (namely, when only the number of processors
    increases), Amdahl's Law resembles the law of diminishing returns. Specifically,
    as the number of processors becomes larger, the efficiency gained through the
    improvement decreases, and the speedup curve flattens out.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is a thread? What are the core differences between a thread and a process?**'
  prefs: []
  type: TYPE_NORMAL
- en: A thread of execution is the smallest unit of programming commands. More than
    one thread can be implemented within a same process, usually executing concurrently
    and accessing/sharing the same resources, such as memory, while separate processes
    do not do this.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the API options provided by the `thread` module in Python?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main feature of the `thread` module is its fast and efficient method of
    creating new threads to execute functions: the `thread.start_new_thread()` function.
    Aside from this, the module only supports a number of low-level ways of working
    with multithreaded primitives and sharing their global data space. Additionally,
    simple lock objects (for example, mutexes and semaphores) are provided for synchronization
    purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the API options provided by the `threading`**** module in Python?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to all of the functionalities for working with threads that the
    `thread` module provides, the `threading` module also supports a number of extra
    methods, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`threading.activeCount()`: This function returns the number of currently active
    thread objects in the program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threading.currentThread()`: This function returns the number of thread objects
    in the current thread control from the caller.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threading.enumerate()`: This function returns a list of all of the currently
    active thread objects in the program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What are the processes of creating new threads via the `thread` and** `threading`
    **modules?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The processes for creating new threads using the `thread` and `threading` module
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the `thread` module, new threads are created to execute functions concurrently.
    The way to do this is by using the `thread.start_new_thread()` function: `thread.start_new_thread(function,
    args[, kwargs])`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To create and customize a new thread using the `threading` module, there are
    specific steps that need to be followed:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a subclass of the `threading.Thread` class in our program
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Override the default `__init__(self [,args])` method inside the subclass to
    add custom arguments for the class
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Override the default `run(self [,args])` method inside the subclass to customize
    the behavior of the thread class when a new thread is initialized and started
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**What is the idea behind thread synchronization using locks?**'
  prefs: []
  type: TYPE_NORMAL
- en: In a given program, when a thread is accessing/executing the critical section
    of the program, any other threads need to wait until that thread finishes executing.
    The typical goal of thread synchronization is to avoid any potential data discrepancies
    when multiple threads access their shared resource; allowing only one thread to
    execute the critical section at a time guarantees that no data conflicts can occur
    in our multithreaded applications. One of the most common ways to apply thread
    synchronization is through the implementation of a locking mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the process of implementing thread synchronization using locks in
    Python?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our `threading` module, the `threading.Lock` class provides a simple and
    intuitive approach to creating and working with locks. Its main usage includes
    the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`threading.Lock()`: This method initializes and returns a new lock object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`acquire(blocking)`: When this method is called, all threads will run synchronously
    (that is, only one thread can execute the critical section at a time).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`release()`: When this method is called, the lock is released.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What is the idea behind the queue data structure?**'
  prefs: []
  type: TYPE_NORMAL
- en: A queue is an abstract data structure that is a collection of different elements
    maintained in a specific order; these elements can be other objects in a program.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the main application of queuing in concurrent programming?**'
  prefs: []
  type: TYPE_NORMAL
- en: The concept of a queue is even more prevalent in the subfield of concurrent
    programming, as the order of elements maintained inside a queue plays an important
    role when a multithreaded program handles and manipulates its shared resources.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the core differences between a regular queue and a priority queue?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The priority queue abstract data structure is similar to the queue data structure,
    but each of the elements of a priority queue, as the name suggests, has a priority
    associated with it; in other words, when an element is added to a priority queue,
    its priority needs to be specified. Unlike in regular queues, the dequeuing principle
    of a priority queue relies on the priority of the elements: the elements with
    higher priority are processed before those with lower priority.'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is a file descriptor, and in what ways can it be handled in Python?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A file descriptor is used as a handle on an opened external file in a program.
    In Python, a file descriptor is handled by either using `open()` and `close()`
    functions or using the `with` statement; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`f = open(filename, ''r''); ... ; f.close()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`with open(filename, ''r'') as f: ...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What problem arises when file descriptors are not handled carefully?**'
  prefs: []
  type: TYPE_NORMAL
- en: Systems can only handle a certain number of opened external files in one running
    process. When that limit is passed, the handles on the opened files will be compromised
    and file descriptor leakage will occur.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is a lock, and in what ways can it be handled in Python?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A lock is a mechanism in concurrent and parallel programming that performs
    thread synchronization. In Python, a `threading.Lock` object can be handled by
    either using the `acquire()` and `release()` methods or using the `with` statement;
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`my_lock.acquire(); ... ; my_lock.release()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`with my_lock: ...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What problem arises when locks are not handled carefully?**'
  prefs: []
  type: TYPE_NORMAL
- en: When an exception occurs while a lock is acquired, the lock can never be released
    and acquired again if it is not handled carefully, causing a common problem in
    concurrent and parallel programming called deadlock.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the idea behind context managers?**'
  prefs: []
  type: TYPE_NORMAL
- en: Context managers are in charge of the context of resources within a program;
    they define and handle the interaction of other entities with those resources,
    and perform cleanup tasks after the program exits the context.
  prefs: []
  type: TYPE_NORMAL
- en: '**What options does the** `with` **statement in Python provide, in terms of
    context management?**'
  prefs: []
  type: TYPE_NORMAL
- en: The `with` statement in Python offers an intuitive and convenient way to manage
    resources while ensuring that errors and exceptions are handled correctly. Aside
    from better error handling and guaranteed cleanup tasks, the `with` statement
    also provides extra readability from your programs, which is one of the strongest
    features that Python offers to its developers.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is HTML?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**HTML** stands for **Hypertext Markup Language**, which is the standard and
    most common markup language for developing web pages and web applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What are HTTP requests?**'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the communication done via the internet (more specifically, the World Wide
    Web) utilizes HTTP. In HTTP, request methods are used to convey information on
    what data is being requested and should be sent back from a server.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are HTTP response status codes?**'
  prefs: []
  type: TYPE_NORMAL
- en: HTTP response status codes are three-digit numbers that signify the state of
    communication between a server and its client. They are sorted into five categories,
    each indicating a specific state of communication.
  prefs: []
  type: TYPE_NORMAL
- en: '**How does the **`requests`** module help with making web requests?**'
  prefs: []
  type: TYPE_NORMAL
- en: The `requests` module manages the communication between a Python program and
    a web server through HTTP requests.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is a ping test and how is one typically designed?**'
  prefs: []
  type: TYPE_NORMAL
- en: A ping test is a tool typically used by web administrators to make sure that
    their sites are still available to clients. A ping test does this by making requests
    to the websites under consideration and analyzes the returned response status
    codes
  prefs: []
  type: TYPE_NORMAL
- en: '**Why is concurrency applicable in making web requests?**'
  prefs: []
  type: TYPE_NORMAL
- en: Both the process of making different requests to a web server and the process
    of parsing and processing downloaded HTML source code are independent across separate
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the considerations that need to be made when developing web scraping
    applications?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following considerations should be made when developing applications that
    make concurrent web requests:'
  prefs: []
  type: TYPE_NORMAL
- en: The terms of service and data-collecting policies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error handling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating your program regularly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding over-scraping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is a process? What are the core differences between a process and a
    thread?**'
  prefs: []
  type: TYPE_NORMAL
- en: A process is an instance of a specific computer program or software that is
    being executed by the operating system. A process contains both the program code
    and its current activities and interactions with other entities. More than one
    thread can be implemented within the same process to access and share memory or
    other resources, while different processes do not interact in this way.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is multiprocessing? What are the core differences between multiprocessing
    and multithreading?**'
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocessing refers to the execution of multiple concurrent processes from
    an operating system, in which each process is executed on a separate CPU, as opposed
    to a single process at any given time. Multithreading, on the other hand, is the
    execution of multiple threads, which can be within the same process.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the API options provided by the multiprocessing module?**'
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing` module provides APIs to the `Process` class, which contains
    the implementation of a process while offering methods to spawn and interact with
    processes using an API similar to the `threading` module. The module also provides
    the `Pool` class, which is mainly used to implement a pool of processes, each
    of which will carry out the tasks submitted.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the core differences between the **`Process`** class and the **`Pool` **class
    from the multiprocessing module?**'
  prefs: []
  type: TYPE_NORMAL
- en: The `Pool` class implements a pool of processes, each of which will carry out
    tasks submitted to a `Pool` object. Generally, the `Pool` class is more convenient
    than the `Process` class, especially if the results returned from your concurrent
    application should be ordered.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the options to determine the current process in a Python program?**'
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing` module provides the `current_process()` method, which
    will return the `Process` object that is currently running at any point of a program. Another
    way to keep track of running processes in your program is to look at the individual
    process IDs through the `os` module.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are daemon processes? What are their purposes, in terms of waiting for
    processes in a multiprocessing program?**'
  prefs: []
  type: TYPE_NORMAL
- en: Daemon processes run in the background and do not block the main program from
    exiting. This specification is common when there is not an easy way for the main
    program to tell if it is appropriate to interrupt the process at any given time,
    or when exiting the main program without completing the worker does not affect
    the end result.
  prefs: []
  type: TYPE_NORMAL
- en: '**How can you terminate a process? Why is it sometimes acceptable to terminate
    processes?**'
  prefs: []
  type: TYPE_NORMAL
- en: The `terminate()` method from the `multiprocessing.Process` class offers a way
    to quickly terminate a process. If the processes in your program never interact
    with the shared resources, the `terminate()` method is considerably useful, especially
    if a process appears to be unresponsive or deadlocked.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the ways to facilitate interprocess communication in Python?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'While locks are one of the most common synchronization primitives used for
    communication among threads, pipes and queues are the main way to communicate
    between different processes. Specifically, they provide message passing options
    to facilitate communication between processes: pipes for connections between two
    processes, and queues for multiple producers and consumers.'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is a reduction operator? What conditions must be satisfied so that an
    operator can be a reduction operator?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'An operator is a reduction operator if it satisfies the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: The operator can reduce an array of elements into one scalar value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The end result (the scalar value) is obtained through creating and computing
    partial tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What properties do reduction operators have that are equivalent to the required
    conditions?**'
  prefs: []
  type: TYPE_NORMAL
- en: The communicative and associative properties are considered to be equivalent
    to the requirements for a reduction operator.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the connection between reduction operators and concurrent programming?**'
  prefs: []
  type: TYPE_NORMAL
- en: Reduction operators require communicative and associative properties. Consequently,
    their sub-tasks have to be able to be processed independently, which makes concurrency
    and parallelism applicable.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are some of the considerations that must be made when working with multiprocessing
    programs that facilitate interprocess communication in Python?**'
  prefs: []
  type: TYPE_NORMAL
- en: Some considerations include implementing the poison-pill technique, so that
    sub-tasks are distributed across all consumer processes; calling `task_done()`
    on the task queue each time the `get()` function is called, to ensure that the
    `join()` function will not block indefinitely; and avoiding using the `qsize()`
    method, which is unreliable and is not implemented on Unix operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are some real-life applications of concurrent reduction operators?**'
  prefs: []
  type: TYPE_NORMAL
- en: Some real-life applications include heavy number-crunching operators and complex
    programs that utilize logic operators.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is an image processing task?**'
  prefs: []
  type: TYPE_NORMAL
- en: Image processing is the task of analyzing and manipulating digital image files
    to create new versions of the images, or to extract important data from them.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the smallest unit of digital imaging? How is it represented in computers?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The smallest unit of digital imaging is a pixel, which typically contains an
    RGB value: a tuple of integers between 0 and 255.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is grayscaling? What purpose does the technique serve?**'
  prefs: []
  type: TYPE_NORMAL
- en: Grayscaling is the process of converting an image to gray colors by considering
    only the intensity information of each pixel, represented by the amount of light
    available. It reduces the dimensionality of the image pixel matrix by mapping
    traditional three-dimensional color data to one-dimensional gray data.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is thresholding? What purpose does the technique serve?**'
  prefs: []
  type: TYPE_NORMAL
- en: Thresholding replaces each pixel in an image with a white pixel if the pixel's
    intensity is greater than a previously specified threshold, and with a black pixel
    if the pixel's intensity is less than that threshold. After performing thresholding
    on an image, each pixel of that image can only hold two possible values, significantly
    reducing the complexity of image data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why should image processing be made concurrent?**'
  prefs: []
  type: TYPE_NORMAL
- en: Heavy computational number-crunching processes are typically involved when it
    comes to image processing, as each image is a matrix of integer tuples. However,
    these processes can be executed independently, which suggests that the whole task
    should be made concurrent.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are some good practices for concurrent image processing?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some good practices for concurrent image processing are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the correct method (out of many)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spawning an appropriate amount of processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing input/output concurrently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is the idea behind asynchronous programming?**'
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming is a model of programming that focuses on coordinating
    different tasks in an application with the goal that the application will use
    the least amount of time to finish executing those tasks. An asynchronous program
    switches from one task to another when it is appropriate to create overlap between
    the waiting and processing time, and therefore shorten the total time taken to
    finish the whole program.
  prefs: []
  type: TYPE_NORMAL
- en: '**How is asynchronous programming different from synchronous programming?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In synchronous programming, the instructions of a program are executed sequentially:
    a task has to finished executing before the next task in the program starts processing.
    With asynchronous programming, if the current task takes a significant amount
    of time to finish, you have the option to specify at one time during the task
    to switch the execution to another task.'
  prefs: []
  type: TYPE_NORMAL
- en: '**How is asynchronous programming different from threading and multiprocessing?**'
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming keeps all of the instructions of a program in the same
    thread and process. The main idea behind asynchronous programming is to have a
    single executor switch from one task to another if it is more efficient (in terms
    of execution time) to simply wait for the first task for a while, while processing
    the second.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is asynchronous programming? What advantages does it provide?**'
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming is a model of programming that takes advantage of coordinating
    computing tasks to overlap the waiting and processing times. If successfully implemented,
    asynchronous programming provides both responsiveness and an improvement in speed,
    as compared to synchronous programming.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the main elements in an asynchronous program? How do they interact
    with each other?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main components of an asynchronous program: the event loop,
    the coroutines, and the futures. The event loop is in charge of scheduling and
    managing coroutines by using its task queue; the coroutines are computing tasks
    that are to be executed asynchronously, and each coroutine has to specify, inside
    its function, exactly where it will give the execution flow back to the event
    loop (that is, the task-switching event); the futures are placeholder objects
    that contain the results obtained from the coroutines.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the **`async`** and **`await`** keywords? What purposes do they
    serve?**'
  prefs: []
  type: TYPE_NORMAL
- en: The `async` and `await` keywords are provided by the Python language as a way
    to implement asynchronous programming on a low level. The `async` keyword is placed
    in front of a function, in order to declare it as a coroutine, while the `await`
    keyword specifies the task-switching events.
  prefs: []
  type: TYPE_NORMAL
- en: '**What options does the **`asyncio`** module provide, in terms of the implementation
    of asynchronous programming?**'
  prefs: []
  type: TYPE_NORMAL
- en: The `asyncio` module provides an easy-to-use API and an intuitive framework
    to implement asynchronous programs; additionally, this framework makes the asynchronous
    code just as readable as synchronous code, which is generally quite rare in asynchronous
    programming.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the improvements, in regards to asynchronous programming, provided
    in Python 3.7?**'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3.7 comes with improvements in the API that initiates and runs the main
    event loop of asynchronous programs, while reserving `async` and `await` as official
    Python keywords.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are blocking functions? Why do they pose a problem for traditional asynchronous
    programming?**'
  prefs: []
  type: TYPE_NORMAL
- en: Blocking functions have non-stop execution, and therefore, they prevent any
    attempts to cooperatively switch tasks in an asynchronous program. If forced to
    release the execution flow back to the event loop, blocking functions will simply
    halt their execution until it is their turn to run again. While still achieving
    better responsiveness, in this case, asynchronous programming fails to improve
    the speed of the program; in fact, the asynchronous version of the program takes
    longer to finish executing than the synchronous version, most of the time, due
    to various overheads.
  prefs: []
  type: TYPE_NORMAL
- en: '**How does** `concurrent.futures`** provide a solution to blocking functions
    for asynchronous programming? What options does it provide?**'
  prefs: []
  type: TYPE_NORMAL
- en: The `concurrent.futures` module implements threading and multiprocessing for
    the execution of coroutines in an asynchronous program. It provides the `ThreadPoolExecutor`
    and `ProcessPoolExecutor` for asynchronous programming in separate threads and
    separate processes, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is a communication channel? What is its connection to asynchronous programming?**'
  prefs: []
  type: TYPE_NORMAL
- en: Communication channels are used to denote both the physical wiring connection
    between different systems and the logical communication of data that facilitates
    computer networks. The latter is related to computing, and is more relevant to
    the idea of asynchronous programming. Asynchronous programming can provide functionalities
    that complement the process of facilitating communication channels efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the two main parts of the ****Open Systems Interconnection (OSI)
    model protocol layers? What purposes do each of them serve?**'
  prefs: []
  type: TYPE_NORMAL
- en: The media layers contain fairly low-level operations that interact with the
    underlying process of the communication channel, while the host layers deals with
    high-level data communication and manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the transport layer? Why is it crucial to communication channels?**'
  prefs: []
  type: TYPE_NORMAL
- en: The transport layer is often viewed as the conceptual transition between the
    media layers and the host layers, responsible for sending data along end-to-end
    connections between different systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**How does **`asyncio`** facilitate the implementation of server-side communication
    channels?**'
  prefs: []
  type: TYPE_NORMAL
- en: Server-wise, the `asyncio` module combines the abstraction of transport with
    the implementation of an asynchronous program. Specifically, via its `BaseTransport`
    and `BaseProtocol` classes, `asyncio` provides different ways to customize the
    underlying architecture of a communication channel.
  prefs: []
  type: TYPE_NORMAL
- en: '**How does** `asyncio` **facilitate the implementation of client-side communication
    channels?**'
  prefs: []
  type: TYPE_NORMAL
- en: Together with the `aiohttp` module and, specifically, `aiohttp.ClientSession`,
    `asyncio` also offers efficiency and flexibility regarding client-side communication
    processes, via asynchronously making requests and reading the returned responses.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is** `aiofiles`**?**'
  prefs: []
  type: TYPE_NORMAL
- en: The `aiofiles` module, which can work in conjunction with `asyncio` and `aiohttp`,
    helps to facilitate asynchronous file reading/writing.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What can lead to a deadlock situation, and why is it undesirable?**'
  prefs: []
  type: TYPE_NORMAL
- en: A lack of (or mishandled) coordination between different lock objects can cause
    deadlock, in which no progress can be made and the program is locked in its current
    state.
  prefs: []
  type: TYPE_NORMAL
- en: '**How is the dining philosophers problem related to the problem of deadlock?**'
  prefs: []
  type: TYPE_NORMAL
- en: In the dining philosophers problem, as each philosopher is holding only one
    fork with their left hand, they cannot proceed to eat or put down the fork they
    are holding. The only way a philosopher gets to eat their food is for their neighbor
    philosopher to put their fork down, which is only possible if they can eat their
    own food; this creates a never-ending circle of conditions that can never be satisfied.
    This situation is, in essence, the nature of a deadlock, in which all elements
    of a system are stuck in place and no progress can be made.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the four Coffman conditions?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deadlock is also defined by the necessary conditions that a concurrent program
    needs to have at the same time, in order for deadlock to occur. These conditions
    were first proposed by the computer scientist Edward G. Coffman, Jr., and are
    therefore known as the Coffman conditions. The conditions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: At least one resource has to be in a non-shareable state. This means that that
    resource is being held by an individual process (or thread) and cannot be accessed
    by others; the resource can only be accessed and held by a single process (or
    thread) at any given time. This condition is also known as **mutual exclusion**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There exists one process (or thread) that is simultaneously accessing a resource
    and waiting for another held by other processes (or threads). In other words,
    this process (or thread) needs access to two resources in order to execute its
    instructions, one of which it is already holding, and the other of which it is
    waiting for from other processes (or threads). This condition is called **hold
    and wait**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources can only be released by a process (or a thread) holding them if there
    are specific instructions for the process (or thread) to do so. This is to say
    that unless the process (or thread) voluntarily and actively releases the resource,
    that resource remains in a non-shareable state. This is the **no preemption**
    condition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final condition is called **circular wait**. As suggested by the name, this
    condition specifies that there exists a set of processes (or threads) such that
    the first process (or thread) in the set is in a waiting state for a resource
    to be released by the second process (or thread), which, in turn, needs to be
    waiting for the third process (or thread); finally, the last process (or thread)
    in the set is waiting for the first one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How can resource ranking solve the problem of deadlock? What other problems
    occur when this is implemented?**'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of accessing the resources arbitrarily, if the processes (or threads)
    are to access them in a predetermined, static order, the circular nature of the
    way that they acquire and wait for the resources will be eliminated. However, if
    you place enough locks on the resources of your concurrent program, it will become
    entirely sequential in its execution, and, combined with the overhead of concurrent
    programming functionalities, it will have an even worse speed than the purely
    sequential version of the program.
  prefs: []
  type: TYPE_NORMAL
- en: '**How can ignoring locks solve the problem of deadlock? What other problems
    can occur when this is implemented?**'
  prefs: []
  type: TYPE_NORMAL
- en: By ignoring locks, our program resources effectively become shareable among
    different processes/threads in a concurrent program, thus eliminating the first
    of the four Coffman conditions, **mutual exclusion**. Doing this, however, can
    be seen as misunderstanding the problem completely. We know that locks are utilized
    so that processes and threads can access the shared resources in a program in
    a systematic, coordinated way, to avoid mishandling the data. Removing any locking
    mechanisms in a concurrent program means that the likelihood of the shared resources,
    which are now free from accessing limitations, being manipulated in an uncoordinated
    way (and therefore becoming corrupted) increases significantly.
  prefs: []
  type: TYPE_NORMAL
- en: '**How is livelock related to deadlock?**'
  prefs: []
  type: TYPE_NORMAL
- en: In a livelock situation, the processes (or threads) in the concurrent program
    are able to switch their states, yet they simply switch back and forth infinitely,
    and no progress can be made.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is starvation, and why is it undesirable in a concurrent program?**'
  prefs: []
  type: TYPE_NORMAL
- en: Starvation is a problem in concurrent systems in which a process (or a thread)
    cannot gain access to the necessary resources to proceed with its execution, and
    therefore, cannot make any progress.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the underlying causes of starvation? What are the common superficial
    causes of starvation that can manifest from the underlying cause?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the time, a poorly coordinated set of scheduling instructions is the
    main cause of starvation. Some high-level causes for starvation might include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Processes (or threads) with high priorities dominate the execution flow in the
    CPU, and thus, low-priority processes (or threads) are not given the opportunity
    to execute their own instructions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes (or threads) with high priorities dominate the usage of non-shareable
    resources, and thus, low-priority processes (or threads) are not given the opportunity
    to execute their own instructions. This situation is similar to the first one,
    but addresses the priority of accessing resources, instead of the priority of
    execution itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes (or threads) with low priorities are waiting for resources to execute
    their instructions, but as soon as the resources become available, other processes
    (or threads) with higher priorities are immediately given access to them, so the
    low-priority processes (or threads) wait infinitely.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What is the connection between deadlock and starvation?**'
  prefs: []
  type: TYPE_NORMAL
- en: Deadlock situations can also lead to starvation, as the definition of starvation
    states that if there exists a process (or a thread) that is unable to make any
    progress because it cannot gain access to the necessary process, the process (or
    thread) is experiencing starvation. This is also illustrated in the dining philosophers
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the readers-writers problem?**'
  prefs: []
  type: TYPE_NORMAL
- en: The readers-writers problem asks for a scheduling algorithm so that readers
    and writers can access the text file appropriately and efficiently, without mishandling/corrupting
    the data included.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the first approach to the readers-writers problem? Why does starvation
    arise in that situation?**'
  prefs: []
  type: TYPE_NORMAL
- en: The first approach allows for multiple readers to access the text file simultaneously, since
    readers simply read in the text file and do not alter the data in it. The problem
    with the first approach is that when a reader is accessing the text file and a
    writer is waiting for the file to be unlocked, if another reader starts its execution
    and wants to access the file, it will be given priority over the writer that has
    already been waiting. Additionally, if more and more readers keep requesting access
    to the file, the writer will be waiting infinitely.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the second approach to the readers-writers problem? Why does starvation
    arise in that situation?**'
  prefs: []
  type: TYPE_NORMAL
- en: This approach implements the specification that once a writer makes a request
    to access the file, no reader should be able to jump in line and access the file
    before that writer. As opposed to what we see in the first solution to the readers-writers
    problem, this solution is giving priority to writers and, as a consequence, the
    readers are starved.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the third approach to the readers-writers problem? Why does it successfully
    address starvation?**'
  prefs: []
  type: TYPE_NORMAL
- en: This approach implements a lock on both readers and writers. All threads will
    then be subject to the constants of the lock, and equal priority will thus be
    achieved among separate threads.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are some common solutions to starvation?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common solutions to starvation include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the priority of low-priority threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a first-in-first-out thread queue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A priority queue that also gives gradually increasing priority to threads that
    have been waiting in the queue for a long time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Or if a thread has been able to access the shared resource for many times, it
    will be given less priority
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is a critical section?**'
  prefs: []
  type: TYPE_NORMAL
- en: Critical sections indicate shared resources that are accessed by multiple processes
    or threads in a concurrent application, which can lead to unexpected, and even
    erroneous, behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is a race condition, and why is it undesirable in a concurrent program?**'
  prefs: []
  type: TYPE_NORMAL
- en: A race condition occurs when two or more threads/processes access and alter
    a shared resource simultaneously, resulting in mishandled and corrupted data.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the underlying cause of a race condition?**'
  prefs: []
  type: TYPE_NORMAL
- en: The root cause of a race condition is multiple threads/process reading in and
    altering a shared resource simultaneously; and, when all of the threads/processes
    finish their execution, only the result of the last thread/process is registered.
  prefs: []
  type: TYPE_NORMAL
- en: '**How can locks solve the problem of a race condition?**'
  prefs: []
  type: TYPE_NORMAL
- en: Since the race conditions arise when multiple threads or processes access and
    write to a shared resource simultaneously, the solution is to isolate the execution
    of different threads/processes, especially when interacting with the shared resource.
    With locks, we can turn a shared resource in a concurrent program into a critical
    section, whose integrity of data is guaranteed to be protected.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why are locks sometimes undesirable in a concurrent program?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of disadvantages to using locks: with enough locks implemented
    in a concurrent program, the whole program might become entirely sequential; locks
    don''t lock anything.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the problems race conditions raise in real-life systems and applications?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The problems race conditions raise in real-life systems and applications are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Security**: A race condition can be both exploited as a security vulnerability
    (to give external agents illegal access to a system) and used as random key generation,
    for security processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operating systems**: A race condition occurring when two agents (users and
    applications) interact with the same memory space can lead to unpredictable behaviors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Networking**: In networking, a race condition can lead to giving multiple
    users powerful privileges in a network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is the difference in memory management between Python and C++?**'
  prefs: []
  type: TYPE_NORMAL
- en: C++ associates a variable to its value by simply writing the value to the memory
    location of the variable; Python has its variables reference point to the memory
    location of the values that they hold. For this reason, Python needs to maintain
    a reference count for every value in its memory space.
  prefs: []
  type: TYPE_NORMAL
- en: '**What problem does the GIL solve for Python?**'
  prefs: []
  type: TYPE_NORMAL
- en: To avoid race conditions, and consequently, the corruption of value reference
    counts, the GIL is implemented so that only one thread can access and mutate the
    counts at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: '**What problem does the GIL create for Python?**'
  prefs: []
  type: TYPE_NORMAL
- en: The GIL effectively prevents multiple threads from taking advantage of the CPU
    and executing CPU-bound instructions at the same time. This means that if multiple
    threads that are meant to be executed concurrently are CPU-bound, they will actually
    be executed sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are some of the approaches to circumventing the GIL in Python programs?**'
  prefs: []
  type: TYPE_NORMAL
- en: There are a few ways to deal with the GIL in your Python applications; namely,
    implementing multiprocessing instead of multithreading, and utilizing other, alternative
    Python interpreters.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is the main approach to solving the problem that locks don''t lock anything?**'
  prefs: []
  type: TYPE_NORMAL
- en: The main approach is to have the locks internally implemented within the data
    structure's class attributes and methods, so that external functions and programs
    cannot bypass those locks and access a shared concurrent object simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: '**Describe the concept of scalability, in the context of concurrent programming.**'
  prefs: []
  type: TYPE_NORMAL
- en: By the scalability of a program, we mean the changes in performance when the
    amount of tasks to be processed by the program increases. Andre B. Bondi defines
    the term scalability as, *"the capability of a system, network, or process to
    handle a growing amount of work, or its potential to be enlarged to accommodate
    that growth."*
  prefs: []
  type: TYPE_NORMAL
- en: '**How does a naive locking mechanism affect the scalability of a concurrent
    program?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The scalability of a simple lock-based data structure is highly undesirable:
    as more threads are added to the program to execute more tasks, the performance
    of the program decreases somewhat linearly. Since only one thread can access and
    increment the shared counter at any given time, the more increments the program
    has to execute, the longer it will take to finish all of the incremented tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What are approximate counters, and how do they help with the problem of scalability
    in concurrent programming?**'
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea behind approximate counters is to distribute the work (incrementing
    the shared global counter) across other low-level counters. When an active thread
    executes and wants to increment the global counter; first, it has to increment
    its corresponding local counter. With one separate counter object for each thread,
    the threads can update their corresponding local counters independently and simultaneously,
    creating overlaps that will result in a better performance in speed for the programs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Are lock-free data structures possible in Python? Why, or why not?**'
  prefs: []
  type: TYPE_NORMAL
- en: The characteristic of being lock-free is impossible to implement in CPython,
    due to the existence of the **Global Interpreter Lock** **(GIL)**, which prevents
    more than one thread from executing in the CPU at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is a mutex-free concurrent data structure, and how is it different from
    a concurrent lock-based one?**'
  prefs: []
  type: TYPE_NORMAL
- en: The term mutex-free concurrent data structures indicates a lack of a locking
    mechanism and the use of other synchronization mechanisms to protect the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the RCU technique, and what problem does it solve for mutex-free
    concurrent data structures?**'
  prefs: []
  type: TYPE_NORMAL
- en: To protect the integrity of concurrent data structures, the RCU technique creates
    and maintains another version of the data structure when a thread or process is
    requesting reading or writing access to it. By isolating the interaction between
    the data structure and the threads/processes within a separate copy, RCU ensures
    that no conflicting data can occur.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What are the main components of the Python memory manager?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main components of the Python memory manager are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The raw memory allocator handles the allocation of memory at a low level by
    interacting with the memory manager of the operating system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object-specific memory allocators interact with the private heap of objects
    and values in Python. These allocators execute memory operations that are specific
    to given data and object types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system allocators from the standard C library are responsible for helping
    the raw memory allocator interact with the memory manager of the operating system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How does the Python memory model resemble a labeled directed graph?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The memory model keeps track of its data and variables via nothing but pointers:
    the value of every variable is a pointer, and this point can be pointing to a
    symbol, a number, or a subroutine. So, these pointers are the directed edges in
    the object graph, and the actual values (symbols, numbers, and subroutines) are
    the nodes in the graph.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the advantages and disadvantages of the Python memory model, in
    terms of developing concurrent applications in Python?**'
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning about the behaviors of a concurrent program can be significantly easier
    than doing the same in another programming language. However, the ease of understanding
    and debugging concurrent programs in Python also comes with a decrease in performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is an atomic operation, and why is it desirable in concurrent programming?**'
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations are instructions that cannot be interrupted during their execution.
    Atomicity is a desirable characteristic of concurrent operations, as it guarantees
    the safety of data shared across different threads.
  prefs: []
  type: TYPE_NORMAL
- en: '**Give three examples of innately atomic operations in Python.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Appending a predefined object to a list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending a list with another list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fetching an element from a list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popping from a list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sorting a list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning a variable to another variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning a variable to an attribute of an object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a new entry for a dictionary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating a dictionary with another dictionary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is a socket? How is it relevant in network programming?**'
  prefs: []
  type: TYPE_NORMAL
- en: Low-level network programming, more often than not, involves the manipulation
    and handling of sockets, which are defined as theoretical endpoints within the
    nodes of a specific computer network, responsible for receiving or sending data
    from the nodes that they are in.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the procedure of server-side communication when a potential client
    makes a request to connect?**'
  prefs: []
  type: TYPE_NORMAL
- en: To open a communication channel from the server side, a network programmer must
    first create a socket and bind it to a specific address. The server then begins
    to listen to any potential communication requests created by the clients in the
    network. Upon receiving a request to connect from a potential client, the server
    can now decide whether to accept that request. A connection is then established
    between the two systems in the network, which means that they can start to communicate
    and share data with each other. As the client sends a message to the server via
    the communication channel, the server then processes the message, and eventually
    sends a response back to the client through the same channel; this process continues
    until the connection between them ends, either by one of them quitting the connection
    channel or through some external factors.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are some methods provided by the socket module to facilitate low-level
    network programming on the server side?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the important methods are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`socket.bind()` binds the calling socket to the address that is passed to the
    method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`socket.listen()` allows the server that we create to accept connections from
    potential clients'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`socket.accept()` accepts a specific connection that the calling socket object
    has'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`socket.makefile()` returns a file object that is associated with the calling
    socket object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`socket.sendall()` sends the data passed as a parameter to the calling socket
    object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`socket.close()` marks the calling socket object as closed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What are generators? What is their advantage over Python lists?**'
  prefs: []
  type: TYPE_NORMAL
- en: Generators are functions that return iterators and are able to be paused and
    resumed dynamically. Generator iterators are lazy, and only produce results when
    specifically asked. For this reason, generator iterators are more efficient in
    terms of memory management, and are therefore often preferred over lists when
    large amounts of data are involved.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are asynchronous generators? How can they be applied in order to build
    a non-blocking server?**'
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous generators allow for the execution flow to switch between generating
    tasks. Combined with using callbacks that can be run at a later time, a server
    can read and handle data coming in from multiple clients at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is APScheduler? Why isn''t it a scheduling service?**'
  prefs: []
  type: TYPE_NORMAL
- en: APScheduler is an external Python library that supports scheduling Python code
    to be executed later. APScheduler is not, in itself, a scheduling service that
    has a built-in GUI or command-line interface. It is still a Python library that
    has to be imported and utilized inside existing applications. However, APScheduler
    comes with numerous functionalities that can be leveraged in order to build an
    actual scheduling service.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the main scheduling functionalities of APScheduler?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'It offers three different scheduling mechanisms: cron-style scheduling, interval-based
    execution, and delayed execution. Furthermore, APScheduler allows for storing
    the jobs to be executed in various backend systems, and working with common Python
    concurrency frameworks, such as AsyncIO, Gevent, Tornado, and Twisted. Finally,
    APScheduler provides different options to actually execute the scheduled code,
    by specifying the appropriate executor(s).'
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the differences between APScheduler and another scheduling tool
    in Python, Celery?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'While Celery is a distributed task queue with basic scheduling capabilities,
    APScheduler is quite the opposite: a scheduler with basic task queuing options
    and advanced scheduling functionalities. Users have reported that APScheduler
    is easier to set up and implement than Celery.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the purpose of testing in programming? How is it different in concurrent
    programming?**'
  prefs: []
  type: TYPE_NORMAL
- en: Testing evokes errors that indicate the existence of bugs in our programs. Testing
    concurrent programs is typically difficult, as non-determinism allows for a concurrency
    bug to be detected in one run of the test and become invisible in another. We
    call the concurrency bugs that might become invisible from test to test non-reproducible,
    and they are the main reason why we cannot reply on testing to detect all concurrency
    bugs consistently.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the methods of testing that were discussed in this chapter?**'
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing is applied to individual units of the program under consideration,
    where a unit is the smallest testable part of the program. Static code analysis,
    on the other hand, looks at the actual code itself without executing it. Static
    code analysis scans for visual errors in the code structure and usage of variables
    and functions.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the purpose of debugging in programming? How is it different in concurrent
    programming?**'
  prefs: []
  type: TYPE_NORMAL
- en: Debugging is the process by which programmers attempt to identify and resolve
    problems or defects that would otherwise cause the computer applications that
    they reside in to produce incorrect results, or even stop functioning. Similar
    to the problem of testing concurrent programs, debugging, when applied to concurrency,
    can become increasingly complex and difficult, as shared resources can interact
    with (and be altered by) multiple agents simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the methods of debugging that were discussed in this chapter?**'
  prefs: []
  type: TYPE_NORMAL
- en: General debugging methods include print debugging, logging, tracing, and using
    a debugger. The process of debugging concurrent programs can utilize minimization,
    single-threading/processing, and manipulating scheduling in order to amplify potential
    bugs.
  prefs: []
  type: TYPE_NORMAL
