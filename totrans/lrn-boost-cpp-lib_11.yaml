- en: Chapter 11. Network Programming Using Boost Asio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today's networked world, Internet servers handling thousands of requests
    per second have a tough mandate to fulfill—of maintaining responsiveness and not
    slowing down even with increasing volumes of requests. Building reliable processes
    that efficiently handle network I/O and scale with the number of connections is
    challenging because it often requires the application programmer to understand
    the underlying protocol stack and exploit it in ingenious ways. What adds to the
    challenge is the variance in the programming interfaces and models for network
    programming across platforms, and the inherent difficulties of using low-level
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Boost Asio (pronounced ay-see-oh) is a portable library for performing efficient
    network I/O using a consistent programming model. The emphasis is on performing
    asynchronous I/O (hence the name Asio), where the program initiates I/O operations
    and gets on with its other jobs, without blocking for the OS to return with the
    results of the operation. When the operation is complete in the underlying OS,
    the program is notified by the Asio library and takes an appropriate action. The
    problems Asio helps solve and the consistent, portable interfaces it uses to do
    so, make Asio compellingly useful. But the asynchronous nature of interactions
    also makes it more complex and less straightforward to reason about. This is the
    reason we will study Asio in two parts: to first understand its interaction model
    and then use it to perform network I/O:'
  prefs: []
  type: TYPE_NORMAL
- en: Task execution with Asio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network programming using Asio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asio provides a toolkit for performing and managing arbitrary tasks, and the
    focus of the first part of this chapter is to understand this toolkit. We apply
    this understanding in the second part of this chapter, when we look specifically
    at how Asio helps write programs that communicate with other programs over the
    network, using protocols from the Internet Protocol (IP) suite.
  prefs: []
  type: TYPE_NORMAL
- en: Task execution with Asio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At its core, Boost Asio provides a task execution framework that you can use
    to perform operations of any kind. You create your tasks as function objects and
    post them to a task queue maintained by Boost Asio. You enlist one or more threads
    to pick these tasks (function objects) and invoke them. The threads keep picking
    up tasks, one after the other till the task queues are empty at which point the
    threads do not block but exit.
  prefs: []
  type: TYPE_NORMAL
- en: IO Service, queues, and handlers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the heart of Asio is the type `boost::asio::io_service`. A program uses the
    `io_service` interface to perform network I/O and manage tasks. Any program that
    wants to use the Asio library creates at least one instance of `io_service` and
    sometimes more than one. In this section, we will explore the task management
    capabilities of `io_service`, and defer the discussion of network I/O to the latter
    half of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the IO Service in action using the obligatory "hello world" example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.1: Asio Hello World**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We include the convenience header `boost/asio.hpp`, which includes most of the
    Asio library that we need for the examples in this chapter (line 1). All parts
    of the Asio library are under the namespace `boost::asio`, so we use a shorter
    alias for this (line 3). The program itself just prints `Hello, world!` on the
    console but it does so through a task.
  prefs: []
  type: TYPE_NORMAL
- en: The program first creates an instance of `io_service` (line 6) and *posts* a
    function object to it, using the `post` member function of `io_service`. The function
    object, in this case defined using a lambda expression, is referred to as a **handler**.
    The call to `post` adds the handler to a queue inside `io_service`; some thread
    (including that which posted the handler) must *dispatch* them, that is, remove
    them off the queue and call them. The call to the `run` member function of `io_service`
    (line 14) does precisely this. It loops through the handlers in the queue inside
    `io_service`, removing and calling each handler. In fact, we can post more handlers
    to the `io_service` before calling `run`, and it would call all the posted handlers.
    If we did not call `run`, none of the handlers would be dispatched. The `run`
    function blocks until all the handlers in the queue have been dispatched and returns
    only when the queue is empty. By itself, a handler may be thought of as an independent,
    packaged task, and Boost Asio provides a great mechanism for dispatching arbitrary
    tasks as handlers. Note that handlers must be nullary function objects, that is,
    they should take no arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Asio is a header-only library by default, but programs using Asio need to link
    at least with `boost_system`. On Linux, we can use the following command line
    to build this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Most examples in this chapter would require you to link to additional libraries.
    You can use the following command line to build all the examples in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you did not install Boost from a native package, and for installation on
    Windows, refer to [Chapter 1](ch01.html "Chapter 1. Introducing Boost"), *Introducing
    Boost*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this program prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that `Greetings:` is printed from the main function (line 13) before the
    call to `run` (line 14). The call to `run` ends up dispatching the sole handler
    in the queue, which prints `Hello, World!`. It is also possible for multiple threads
    to call `run` on the same I/O object and dispatch handlers concurrently. We will
    see how this can be useful in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Handler states – run_one, poll, and poll_one
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the `run` function blocks until there are no more handlers in the queue,
    there are other member functions of `io_service` that let you process handlers
    with greater flexibility. But before we look at this function, we need to distinguish
    between pending and ready handlers.
  prefs: []
  type: TYPE_NORMAL
- en: The handlers we posted to the `io_service` were all ready to run immediately
    and were invoked as soon as their turn came on the queue. In general, handlers
    are associated with background tasks that run in the underlying OS, for example,
    network I/O tasks. Such handlers are meant to be invoked only once the associated
    task is completed, which is why in such contexts, they are called **completion
    handlers**. These handlers are said to be **pending** until the associated task
    is awaiting completion, and once the associated task completes, they are said
    to be **ready**.
  prefs: []
  type: TYPE_NORMAL
- en: The `poll` member function, unlike `run`, dispatches all the ready handlers
    but does not wait for any pending handler to become ready. Thus, it returns immediately
    if there are no ready handlers, even if there are pending handlers. The `poll_one`
    member function dispatches exactly one ready handler if there be one, but does
    not block waiting for pending handlers to get ready.
  prefs: []
  type: TYPE_NORMAL
- en: The `run_one` member function blocks on a nonempty queue waiting for a handler
    to become ready. It returns when called on an empty queue, and otherwise, as soon
    as it finds and dispatches a ready handler.
  prefs: []
  type: TYPE_NORMAL
- en: post versus dispatch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A call to the `post` member function adds a handler to the task queue and returns
    immediately. A later call to `run` is responsible for dispatching the handler.
    There is another member function called `dispatch` that can be used to request
    the `io_service` to invoke a handler immediately if possible. If `dispatch` is
    invoked in a thread that has already called one of `run`, `poll`, `run_one`, or
    `poll_one`, then the handler will be invoked immediately. If no such thread is
    available, `dispatch` adds the handler to the queue and returns just like `post`
    would. In the following example, we invoke `dispatch` from the `main` function
    and from within another handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.2: post versus dispatch**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The first call to `dispatch` (line 8) adds a handler to the queue without invoking
    it because `run` was yet to be called on `io_service`. We call this the Hello
    Handler, as it prints `Hello`. This is followed by the two calls to `post` (lines
    10, 18), which add two more handlers. The first of these two handlers prints `Hello,
    world!` (line 12), and in turn, calls `dispatch` (line 13) to add another handler
    that prints the Spanish greeting, `Hola, mundo!` (line 14). The second of these
    handlers prints the German greeting, `Hallo, Welt` (line 18). For our convenience,
    let''s just call them the English, Spanish, and German handlers. This creates
    the following entries in the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, when we call `run` on the `io_service` (line 19), the Hello Handler is
    dispatched first and prints `Hello`. This is followed by the English Handler,
    which prints `Hello, World!` and calls `dispatch` on the `io_service`, passing
    the Spanish Handler. Since this executes in the context of a thread that has already
    called `run`, the call to `dispatch` invokes the Spanish Handler, which prints
    `Hola, mundo!`. Following this, the German Handler is dispatched printing `Hallo,
    Welt!` before `run` returns.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if the English Handler called `post` instead of `dispatch` (line 13)?
    In that case, the Spanish Handler would not be invoked immediately but would queue
    up after the German Handler. The German greeting `Hallo, Welt!` would precede
    the Spanish greeting `Hola, mundo!`. The output would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Concurrent execution via thread pools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `io_service` object is thread-safe and multiple threads can call `run`
    on it concurrently. If there are multiple handlers in the queue, they can be processed
    concurrently by such threads. In effect, the set of threads that call `run` on
    a given `io_service` form a **thread pool**. Successive handlers can be processed
    by different threads in the pool. Which thread dispatches a given handler is indeterminate,
    so the handler code should not make any such assumptions. In the following example,
    we post a bunch of handlers to the `io_service` and then start four threads, which
    all call `run` on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.3: Simple thread pools**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We post twenty handlers in a loop (line 18). Each handler prints its identifier
    (line 19), and then sleeps for a second (lines 19-20). To run the handlers, we
    create a group of four threads, each of which calls run on the `io_service` (line
    21) and wait for all the threads to finish (line 24). We define the macro `PRINT_ARGS`
    which writes output to the console in a thread-safe way, tagged with the current
    thread ID (line 7-10). We will use this macro in other examples too.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build this example, you must also link against `libboost_thread`, `libboost_date_time`,
    and in Posix environments, with `libpthread` too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'One particular run of this program on my laptop produced the following output
    (with some lines snipped):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the different handlers are executed by different threads (each
    thread ID marked differently).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If any of the handlers threw an exception, it would be propagated across the
    call to the `run` function on the thread that was executing the handler.
  prefs: []
  type: TYPE_NORMAL
- en: io_service::work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes, it is useful to keep the thread pool started, even when there are
    no handlers to dispatch. Neither `run` nor `run_one` blocks on an empty queue.
    So in order for them to block waiting for a task, we have to indicate, in some
    way, that there is outstanding work to be performed. We do this by creating an
    instance of `io_service::work`, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.4: Using io_service::work to keep threads engaged**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we create an object of `io_service::work` wrapped in a `unique_ptr`
    (line 18). We associate it with an `io_service` object by passing to the `work`
    constructor a reference to the `io_service` object. Note that, unlike listing
    11.3, we create the worker threads first (lines 24-27) and then post the handlers
    (lines 33-39). However, the worker threads stay put waiting for the handlers because
    of the calls to `run` block (line 26). This happens because of the `io_service::work`
    object we created, which indicates that there is outstanding work in the `io_service`
    queue. As a result, even after all handlers are dispatched, the threads do not
    exit. By calling `reset` on the `unique_ptr,` wrapping the `work` object, its
    destructor is called, which notifies the `io_service` that all outstanding work
    is complete (line 42). The calls to `run` in the threads return and the program
    exits once all the threads are joined (line 43). We wrapped the `work` object
    in a `unique_ptr` to destroy it in an exception-safe way at a suitable point in
    the program.
  prefs: []
  type: TYPE_NORMAL
- en: We omitted the definition of `PRINT_ARGS` here, refer to listing 11.3.
  prefs: []
  type: TYPE_NORMAL
- en: Serialized and ordered execution via strands
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thread pools allow handlers to be run concurrently. This means that handlers
    that access shared resources need to synchronize access to these resources. We
    already saw examples of this in listings 11.3 and 11.4, when we synchronized access
    to `std::cout`, which is a global object. As an alternative to writing synchronization
    code in handlers, which can make the handler code more complex, we can use **strands**.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Think of a strand as a subsequence of the task queue with the constraint that
    no two handlers from the same strand ever run concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scheduling of other handlers in the queue, which are not in the strand,
    is not affected by the strand in any way. Let us look at an example of using strands:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.5: Using strands**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we create two handler functions: `workFuncStrand` (line 21)
    and `workFunc` (line 28). The lambda `workFuncStrand` captures a counter `on_strand`,
    increments it, and prints a message `Hello, from strand!`, prefixed with the value
    of the counter. The function `workFunc` captures another counter `regular`, increments
    it, and prints `Hello, World!`, prefixed with the counter. Both pause for 2 seconds
    before returning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To define and use a strand, we first create an object of `io_service::strand`
    associated with the `io_service` instance (line 17). Thereafter, we post all handlers
    that we want to be part of that strand by wrapping them using the `wrap` member
    function of the `strand` (line 36). Alternatively, we can post the handlers to
    the strand directly by using either the `post` or the `dispatch` member function
    of the strand, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `wrap` member function of strand returns a function object, which in turn
    calls `dispatch` on the strand to invoke the original handler. Initially, it is
    this function object rather than our original handler that is added to the queue.
    When duly dispatched, this invokes the original handler. There are no constraints
    on the order in which these wrapper handlers are dispatched, and therefore, the
    actual order in which the original handlers are invoked can be different from
    the order in which they were wrapped and posted.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, calling `post` or `dispatch` directly on the strand avoids
    an intermediate handler. Directly posting to a strand also guarantees that the
    handlers will be dispatched in the same order that they were posted, achieving
    a deterministic ordering of the handlers in the strand. The `dispatch` member
    of `strand` blocks until the handler is dispatched. The `post` member simply adds
    it to the strand and returns.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `workFuncStrand` increments `on_strand` without synchronization (line
    22), while `workFunc` increments the counter `regular` within the `PRINT_ARGS`
    macro (line 29), which ensures that the increment happens in a critical section.
    The `workFuncStrand` handlers are posted to a strand and therefore are guaranteed
    to be serialized; hence no need for explicit synchronization. On the flip side,
    entire functions are serialized via strands and synchronizing smaller blocks of
    code is not possible. There is no serialization between the handlers running on
    the strand and other handlers; therefore, the access to global objects, like `std::cout`,
    must still be synchronized.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a sample output of running the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'There were three distinct threads in the pool and the handlers from the strand
    were picked up by two of these three threads: initially, by thread ID `b73b6b40`,
    and later on, by thread ID `b63b4b40`. This also dispels a frequent misunderstanding
    that all handlers in a strand are dispatched by the same thread, which is clearly
    not the case.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different handlers in the same strand may be dispatched by different threads
    but will never run concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Network I/O using Asio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We want to use Asio to build scalable network services that perform I/O over
    the network. Such services receive requests from clients running on remote machines
    and send them information over the network. The data transfer between processes
    across machine boundaries, happening over the wire, is done using certain protocols
    of network communication. The most ubiquitous of these protocols is IP or the
    **Internet Protocol** and a **suite of protocols** layered above it. Boost Asio
    supports TCP, UDP, and ICMP, the three popular protocols in the IP protocol suite.
    We do not cover ICMP in this book.
  prefs: []
  type: TYPE_NORMAL
- en: UDP and TCP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**User Datagram Protocol** or UDP is used to transmit **datagrams** or message
    units from one host to another over an IP network. UDP is a very basic protocol
    built over IP and is stateless in the sense that no context is maintained across
    multiple network I/O operations. The reliability of data transfer using UDP depends
    on the reliability of the underlying network, and UDP transfers have the following
    caveats:'
  prefs: []
  type: TYPE_NORMAL
- en: A UDP datagram may not be delivered at all
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A given datagram may be delivered more than once
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two datagrams may not be delivered to the destination in the order in which
    they were dispatched from the source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UDP will detect any data corruption in the datagrams and drop such messages
    without any means of recovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For these reasons, UDP is considered to be an unreliable protocol.
  prefs: []
  type: TYPE_NORMAL
- en: 'If an application requires stronger guarantees from the protocol, we choose
    **Transmission Control Protocol** or TCP. TCP deals in terms of byte streams rather
    than messages. It uses a handshake mechanism between two endpoints of the network
    communication to establish a durable **connection** between the two points and
    maintains state during the life of the connection. All communications between
    the two endpoints happen over such a connection. At the cost of a somewhat higher
    latency than UDP, TCP guarantees the following:'
  prefs: []
  type: TYPE_NORMAL
- en: On a given connection, the receiving application receives the stream of bytes
    sent by the sender in the order they were sent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any data lost or corrupted on the wire can be retransmitted, greatly improving
    the reliability of deliveries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time applications that can handle unreliability and data loss for themselves
    often use UDP. In addition, a lot of higher level protocols are run on top of
    UDP. TCP is more frequently used, where correctness concerns supersede real-time
    performance, for example, e-mail and file transfer protocols, HTTP, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: IP addresses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IP addresses are numeric identifiers used to uniquely identify interfaces connected
    to an IP network. The older IPv4 protocol uses 32-bit IP addresses in an address
    space of 4 billion (2^(32)) addresses. The emergent IPv6 protocol uses 128-bit
    IP addresses in an address space of 3.4 × 10^(38) (2^(128)) unique addresses,
    which is practically inexhaustible. You can represent IP addresses of both types
    using the class `boost::asio::ip::address`, while version-specific addresses can
    be represented using `boost::asio::ip::address_v4` and `boost::asio::ip::address_v6`.
  prefs: []
  type: TYPE_NORMAL
- en: IPv4 addresses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The familiar IPv4 addresses, such as 212.54.84.93, are 32-bit unsigned integers
    expressed in the *dotted-quad notation*; four 8-bit unsigned integers or *octets*
    representing the four bytes in the address, the most significant on the left to
    the least significant on the right, separated by dots (period signs). Each octet
    can range from 0 through 255\. IP addresses are normally interpreted in network
    byte order, that is, Big-endian.
  prefs: []
  type: TYPE_NORMAL
- en: Subnets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Larger computer networks are often divided into logical parts called **subnets**.
    A subnet consists of a set of nodes that can communicate with each other using
    broadcast messages. A subnet has an associated pool of IP addresses that have
    a common prefix, usually, called the *routing prefix* or *network address*. The
    remaining part of the IP address field is called the *host part*.
  prefs: []
  type: TYPE_NORMAL
- en: Given an IP address *and* the length of the prefix, we can compute the prefix
    using the **netmask**. The netmask of a subnet is a 4-byte bitmask, whose bitwise-AND
    with an IP address in the subnet yields the routing prefix. For a subnet with
    a routing prefix of length N, the netmask has the most significant N bits set
    and the remaining 32-N bits unset. The netmask is often expressed in a dotted-quad
    notation. For example, if the address 172.31.198.12 has a routing prefix that
    is 16 bits long, then its netmask would be 255.255.0.0 and the routing prefix
    would be 172.31.0.0.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the length of the routing prefix must be explicitly specified. The
    **Classless Interdomain Routing** (**CIDR) notation** augments the dotted-quad
    notation with a trailing slash and a number between 0 and 32 that represents the
    prefix length. Thus, 10.209.72.221/22 represents an IP address with a prefix length
    of 22\. An older scheme of classification, referred to as the *classful scheme*,
    divided the IPv4 address space into ranges and assigned a *class* to each range
    (see the following table). Addresses belonging to each range were said to be of
    the corresponding class, and the length of the routing prefix was determined based
    on the class, without being specified as, with the CIDR notation.
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Address range | Prefix length | Netmask | Remarks |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Class A | 0.0.0.0 – 127.255.255.255 | 8 | 255.0.0.0 |   |'
  prefs: []
  type: TYPE_TB
- en: '| Class B | 128.0.0.0 – 191.255.255.255 | 16 | 255.255.0.0 |   |'
  prefs: []
  type: TYPE_TB
- en: '| Class C | 192.0.0.0 – 223.255.255.255 | 24 | 255.255.255.0 |   |'
  prefs: []
  type: TYPE_TB
- en: '| Class D | 224.0.0.0 – 239.255.255.255 | Not specified | Not specified | Multicast
    |'
  prefs: []
  type: TYPE_TB
- en: '| Class E | 240.0.0.0 – 255.255.255.255 | Not specified | Not specified | Reserved
    |'
  prefs: []
  type: TYPE_TB
- en: Special addresses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some IPv4 addresses have special meanings. For example, an IP address with all
    bits set in the host part is known as the **broadcast address** for the subnet
    and is used to broadcast messages to all hosts in the subnet. For example, the
    broadcast address in the network 172.31.0.0/16 is 172.31.255.255.
  prefs: []
  type: TYPE_NORMAL
- en: Applications listening for incoming requests use the **unspecified address**
    0.0.0.0 (`INADDR_ANY`) to listen on all available network interfaces, without
    the need to know addresses plumbed on the system.
  prefs: []
  type: TYPE_NORMAL
- en: The **loopback address** 127.0.0.1 is commonly associated with a virtual network
    interface that is not associated with any hardware and does not require the host
    to be connected to a network. Data sent over the loopback interface immediately
    shows up as received data on the sender host itself. Often used for testing networked
    applications within a box, you can configure additional loopback interfaces and
    associate loopback addresses from the range 127.0.0.0 through 127.255.255.255.
  prefs: []
  type: TYPE_NORMAL
- en: Handling IPv4 addresses with Boost
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let us now look at a code example of constructing IPv4 addresses and glean
    useful information from them, using the type `boost::asio::ip::address_v4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.6: Handling IPv4 addresses**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This example highlights a few basic operations on IPv4 addresses. We create
    a vector of `boost::asio::ip::address` objects (not just `address_v4`) and push
    IPv4 addresses constructed from their string representations using the `address_v4::from_string`
    static function (line 32). We use the two-argument overload of `from_string`,
    which takes the address string, and a non-const reference to an `error_code` object
    that is set if it is unable to parse the address string. A one-argument overload
    exists, which throws if there is an error. Note that you can implicitly convert
    or assign `address_v4` instances to `address` instances. Default constructed instances
    of `address_v4` are equivalent to the unspecified address 0.0.0.0 (line 28), which
    is also returned by `address_v4::any()` (line 29).
  prefs: []
  type: TYPE_NORMAL
- en: To print the properties of the address, we have written the `printAddrProperties`
    function (line 9). We print IP addresses by streaming them to `std::cout` (line
    10). We check whether an address is an IPv4 or IPv6 address using the `is_v4`
    and `is_v6` member functions (lines 12, 14), print the netmask for an IPv4 address
    using the `address_v4::netmask` static function (line 13), and also check whether
    the address is an unspecified address, loopback address, or IPv4 multicast address
    (class D) using appropriate member predicates (lines 16-18). Note that the `address_v4::from_string`
    function does not recognize the CIDR format (as of Boost version 1.57), and the
    netmask is computed based on the classful scheme.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, following a brief overview of IPv6 addresses, we will augment
    the `printAddrProperties` (line 14) function to print IPv6 specific properties
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: IPv6 addresses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In its most general form, an IPv6 address is represented as a sequence of eight
    2-byte unsigned hexadecimal integers, separated by colons. Digits `a` through
    `f` in the hexadecimal integers are written in lowercase by convention and leading
    zeros in each 16-bit number are omitted. Here is an example of an IPv6 address
    in this notation:'
  prefs: []
  type: TYPE_NORMAL
- en: 2001:0c2f:003a:01e0:0000:0000:0000:002a
  prefs: []
  type: TYPE_NORMAL
- en: One sequence of two or more zero terms can be collapsed completely. Thus, the
    preceding address can be written as 2001:c2f:3a:1e0::2a. All leading zeros have
    been removed and the contiguous zero terms between bytes 16 and 63 have been collapsed,
    leaving the colon pair (::). If there be multiple zero-term sequences, the longest
    one is collapsed, and if there is a tie, the one that is leftmost is collapsed.
    Thus, we can abbreviate this 2001:0000:0000:01e0:0000:0000:001a:002a to this 2001::1e0:0:0:1a:2a.
    Note that the leftmost sequence of two zero-terms is collapsed, while the other
    between bits 32 and 63 are not collapsed.
  prefs: []
  type: TYPE_NORMAL
- en: In environments transitioning from IPv4 to IPv6, software frequently supports
    both IPv4 and IPv6\. *IPv4-mapped IPv6 addresses* are used to enable communication
    between IPv6 and IPv4 interfaces. IPv4 addresses are mapped to an IPv6 address
    with the ::ffff:0:0/96 prefix and the last 32-bits same as the IPv4 address. For
    example, 172.31.201.43 will be represented as ::ffff:172.31.201.43/96.
  prefs: []
  type: TYPE_NORMAL
- en: Address classes, scopes, and subnets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are three classes of IPv6 addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unicast addresses**: These addresses identify a single network interface'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multicast addresses**: These addresses identify a group of network interfaces
    and are used to send data to all the interfaces in the group'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anycast addresses**: These addresses identify a group of network interfaces,
    but data sent to an **anycast** address is delivered to one or more interfaces
    that are topologically closest to the sender and not to all the interfaces in
    the group'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In unicast and anycast addresses, the least significant 64-bits of the address
    represent the host ID. In general, the higher order 64-bits represent the network
    prefix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each IPv6 address also has a **scope**, which identifies the segment of the
    network in which it is valid:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Node-local** addresses, including loopback addresses are used for communication
    within the node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global** addresses are routable addresses reachable across networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Link-local** addresses are automatically assigned to each and every IPv6-enabled
    interface and are accessible only within a network, that is, routers do not route
    traffic headed for link-local addresses. Link-local addresses are assigned to
    interfaces even when they have routable addresses. Link-local addresses have a
    prefix of fe80::/64.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Special addresses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The IPv6 **loopback address** analogous to 127.0.0.1 in IPv4 is ::1\. The **unspecified
    address** (all zeros) in IPv6 is written as :: (`in6addr_any`). There are no broadcast
    addresses in IPv6, and multicast addresses are used to define groups of recipient
    interfaces, a topic that is outside the scope of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: Handling IPv6 addresses with Boost
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the following example, we construct IPv6 addresses and query properties
    of these addresses using the `boost::asio::ip::address_v6` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.7: Handling IPv6 addresses**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This example augments listing 11.6 with IPv6-specific checks. The function `printAddrProperties`
    (line 15) is the same as that from listing 11.6, so it is not repeated in full.
    The `printAddr6Properties` function (line 8) checks whether the address is an
    IPv4-mapped IPv6 address (line 9) and whether it is a link-local address (line
    11). Other relevant checks are already performed through version-agnostic members
    of `address` in `printAddrProperties` (see listing 11.6).
  prefs: []
  type: TYPE_NORMAL
- en: We create a vector of `boost::asio::ip::address` objects (not just `address_v6`)
    and push IPv6 addresses constructed from their string representations, using the
    `address_v6::from_string` static function (line 24), which returns `address_v6`
    objects, which are implicitly convertible to `address`. Notice that we have the
    loopback address, the unspecified address, IPv4-mapped address, a regular IPv6
    unicast address, and a link-local address (lines 20-21).
  prefs: []
  type: TYPE_NORMAL
- en: Endpoints, sockets, and name resolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Applications **bind** to IP addresses when providing network services, and multiple
    applications initiate outbound communication with other applications, starting
    from an IP address. Multiple applications can bind to the same IP address using
    different **ports**. A port is an unsigned 16-bit integer which, along with the
    IP address and protocol (TCP, UDP, etc.), uniquely identifies a communication
    **endpoint**. Data communication happens between two such endpoints. Boost Asio
    provides distinct endpoint types for UDP and TCP, namely, `boost::asio::ip::udp::endpoint`
    and `boost::asio::ip::tcp::endpoint`.
  prefs: []
  type: TYPE_NORMAL
- en: Ports
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many standard and widely used network services use fixed, well-known ports.
    Ports 0 through 1023 are assigned to well-known system services, including the
    likes of FTP, SSH, telnet, SMTP, DNS, HTTP, and HTTPS. Widely used applications
    may register standard ports between 1024 and 49151 with the **Internet Assigned
    Numbers Authority** (**IANA**). Ports above 49151 can be used by any application,
    without the need for registration. The mapping of well-known ports to services
    is often maintained on a disk file, such as `/etc/services` on POSIX systems and
    `%SYSTEMROOT%\system32\drivers\etc\services` on Windows.
  prefs: []
  type: TYPE_NORMAL
- en: Sockets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **socket** represents an endpoint in use for network communication. It represents
    one end of a communication channel and provides the interface for performing all
    data communication. Boost Asio provides distinct socket types for UDP and TCP,
    namely, `boost::asio::ip::udp::socket` and `boost::asio::ip::tcp::socket`. Sockets
    are always associated with a corresponding local endpoint object. The native network
    programming interfaces on all modern operating systems use some derivative of
    the Berkeley Sockets API, which is a C API for performing network communications.
    The Boost Asio library provides type-safe abstractions built around this core
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Sockets are an example of **I/O objects**. In Asio, I/O objects are the class
    of objects that are used to initiate I/O operations. The operations are dispatched
    to the underlying operating system by an **I/O service** object, which is an instance
    of `boost::asio::io_service`. Earlier in this chapter, we saw the I/O service
    objects in action as task managers. But their primary role is as an interface
    for operations on the underlying operating system. Each I/O object is constructed
    with an associated I/O service instance. In this way, high-level I/O operations
    are initiated on the I/O object, but the interactions between the I/O object and
    the I/O service remain encapsulated. In the following sections, we will see examples
    of using UDP and TCP sockets for network communication.
  prefs: []
  type: TYPE_NORMAL
- en: Hostnames and domain names
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Identifying hosts in a network by names rather than numeric addresses is often
    more convenient. The Domain Name System (DNS) provides a hierarchical naming system
    in which hosts in a network are each identified by a hostname qualified with a
    unique name identifying the network, known as the **fully-qualified domain name**
    or simply **domain name**. For example, the imaginary domain name `elan.taliesyn.org`
    could be mapped to the IP address 140.82.168.29\. Here, `elan` would identify
    the specific host and `taliesyn.org` would identify the domain that the host is
    part of. It is quite possible for different groups of machines in a single network
    to report to different domains and even for a given machine to be part of multiple
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: Name resolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A hierarchy of DNS servers across the world, and within private networks, maintain
    name-to-address mappings. Applications ask a configured DNS server to resolve
    a fully-qualified domain name to an address. The DNS server either resolves the
    request to an IP address or forwards it to another DNS server higher up in the
    hierarchy if there is one. The resolution fails if none of the DNS servers, all
    the way up to the root of the hierarchy, has an answer. A specialized program
    or a library that initiates such name resolution requests is called a **resolver**.
    Boost Asio provides protocol-specific resolvers: `boost::asio::ip::tcp::resolver`
    and `boost::asio::ip::udp::resolver` for performing such name resolutions. We
    query for services on hostnames and obtain one or more endpoints for that service.
    The following example shows how to do this, given a hostname, and optionally,
    a service name or port:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.8: Looking up IP addresses of hosts**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You run this program by passing it a hostname and an optional service name
    on the command line. This program resolves these to an IP address and a port,
    and prints them to the standard output (lines 21-22). The program creates an instance
    of `io_service` (line 14), which would be the conduit for operations on the underlying
    operating system, and an instance of `boost::asio::ip::tcp::resolver` (line 15)
    that provides the interface for requesting name resolution. We create a name lookup
    request in terms of the hostname and service name, encapsulated in a `query` object
    (line 16), and call the `resolve` member function of the `resolver`, passing the
    `query` object as an argument (line 18). The `resolve` function returns an **endpoint
    iterator** to a sequence of `endpoint` objects resolved by the query. We iterate
    through this sequence, printing the address and port number for each endpoint.
    This would print IPv4 as well as IPv6 addresses if any. If we wanted IP addresses,
    specific to one version of IP, we would need to use the three-argument constructor
    for `query` and specify the protocol in the first argument. For example, to look
    up only IPv6 addresses, we can use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'On lookup failure, the `resolve` function throws an exception unless we use
    the two-argument version that takes a non-const reference to `error_code`, as
    a second argument and sets it on error. In the following example, we perform the
    reverse lookup. Given an IP address and a port, we look up the associated hostname
    and service name:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.9: Looking up hosts and service names**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We pass the IP address and the port number to the program from the command line,
    and using them, we construct the `endpoint` (lines 16-17). We then pass the `endpoint`
    to the `resolve` member function of the `resolver` (line 19), and iterate through
    the results. The iterator in this case points to `boost::asio::ip::tcp::query`
    objects, and we print the host and service name for each, using the appropriate
    member functions (lines 22-23).
  prefs: []
  type: TYPE_NORMAL
- en: Buffers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data is sent or received over the network as a byte stream. A contiguous byte
    stream can be represented using a pair of values: the starting address of the
    sequence and the number of bytes in the sequence. Boost Asio provides two abstractions
    for such sequences, `boost::asio::const_buffer` and `boost::asio::mutable_buffer`.
    The `const_buffer` type represents a read-only sequence that is typically used
    as a data source when sending data over the network. The `mutable_buffer` represents
    a read-write sequence that is used when you need to add or update data in your
    buffer, for example, when you receive data from a remote host:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.10: Using const_buffer and mutable_buffer**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we show how a char array is wrapped in a `mutable_buffer` and
    a `const_buffer` (lines 8-9). While constructing a buffer, you specify the starting
    address of the memory region and the length of the region in number of bytes.
    A `const char` array can only be wrapped in a `const_buffer`, not in a `mutable_buffer`.
    These buffer wrappers *do not* allocate storage, manage any heap-allocated memory,
    or perform any data copying.
  prefs: []
  type: TYPE_NORMAL
- en: The function `boost::asio::buffer_size` returns the length of the buffer in
    bytes (lines 11-12). This is the length you passed while constructing the buffer,
    and it is not dependent on the data present in the buffer. Default-initialized
    buffers have zero length.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function template `boost::asio::buffer_cast<>` is used to obtain a pointer
    to the underlying byte array of a buffer (lines 14-15). Note that we get a compilation
    error if we try to use `buffer_cast` to get a mutable array from a `const_buffer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Finally, you can create a buffer from an offset into another buffer, using the
    `operator+` (line 19). The length of the resultant buffer would be less than that
    of the original buffer by the length of the offset (line 22).
  prefs: []
  type: TYPE_NORMAL
- en: Buffer sequences for vectored I/O
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes, it is convenient to send data from a series of buffers or split
    the received data across a series of buffers. Calling network I/O functions once
    per sequence would be inefficient, because these calls ultimately translate to
    system calls and there is an overhead in making each such call. An alternative
    is to use network I/O functions that can process a **sequence of buffers** passed
    to it as an argument. This is often called **vectored I/O** or **gather-scatter
    I/O**. All of Boost Asio''s I/O functions deal in buffer sequences, and so they
    must be passed buffer sequences rather than single buffers. A valid buffer sequence
    for use with Asio I/O functions satisfies the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: Has a member function `begin` that returns a bidirectional iterator, which points
    to a `mutable_buffer` or `const_buffer`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has a member function `end` that returns an iterator pointing to the end of
    the sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is copyable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a buffer sequence to be useful, it must either be a sequence of `const_buffer`s
    or a sequence of `mutable_buffer`s. Formally, these requirements are summarized
    in the **ConstBufferSequence** and **MutableBufferSequence** concepts. This is
    a slightly simplified set of conditions, but is good enough for our purposes.
    We can make such sequences using Standard Library containers, such as `std::vector`,
    `std::list`, and so on, as well as Boost containers. However, since we frequently
    deal with only a single buffer, Boost provides the `boost::asio::buffer` function
    that makes it easy to adapt a single buffer as a buffer sequence of length one.
    Here is a short example illustrating these ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.11: Using buffers**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we create a mutable buffer sequence as a `vector` of two `mutable_buffer`s
    (line 14). The two mutable buffers wrap a `vector` of `char`s (lines 16-17) and
    an array of `char`s (line 18). Using the `buffers_begin` (line 20) and `buffers_end`
    functions (line 21), we determine the entire range of bytes encapsulated by the
    buffer sequence `bufseq` and iterate through it, setting each byte to a random
    character (line 22). As these get written to the underlying vector or array, we
    construct strings using the underlying vector or array and print their contents
    (lines 27-28).
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous and asynchronous communications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following sections, we put together our understanding of IP addresses,
    endpoints, sockets, buffers, and other Asio infrastructure we learned so far to
    write network client and server programs. Our examples use the **client-server
    model** of interaction, in which a **server** program services incoming requests,
    and a **client** program initiates such requests. Such clients are referred to
    as the **active endpoints**, while such servers are referred to as **passive endpoints**.
  prefs: []
  type: TYPE_NORMAL
- en: Clients and servers may communicate **synchronously**, blocking on each network
    I/O operation until the request has been handled by the underlying OS, and only
    then proceeding to the next step. Alternatively, they can use **asynchronous I/O**,
    initiating network I/O without waiting for them to complete, and being notified
    later upon their completion. With asynchronous I/O, unlike the synchronous case,
    programs do not wait idly if there are I/O operations to perform. Thus, asynchronous
    I/O scales better with larger numbers of peers and higher volumes of data. We
    will look at both synchronous and asynchronous models of communication. While
    the programming model for asynchronous interactions is event-driven and more complex,
    the use of Boost Asio coroutines can keep it very manageable. Before we write
    UDP and TCP servers, we will take a look at the Asio deadline timer to understand
    how we write synchronous and asynchronous logic using Asio.
  prefs: []
  type: TYPE_NORMAL
- en: Asio deadline timer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Asio provides the `basic_deadline_timer` template, using which you can wait
    for a specific duration to elapse or for an absolute time point. The specialization
    `deadline_timer` is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'It uses `boost::posix_time::ptime` and `boost::posix_time::time_duration` as
    the time point and duration type respectively. The following example illustrates
    how an application can use `deadline_timer` to wait for a duration to elapse:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.12: Waiting synchronously**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We create an object of `io_service` (line 6), which acts as the conduit for
    operations on the underlying OS. We create an instance of `deadline_timer` associated
    with the `io_service` (line 7). We specify a 5 second duration to wait for using
    the member function `expires_from_now` of `deadline_timer` (line 12). We then
    call the `wait` member function to block until the duration elapses. Notice that
    we do not need to call `run` on the `io_service` instance. We can instead use
    the `expires_at` member function to wait until a specific time point, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes, programs do not want to block waiting for the timer to go off, or
    in general, for any future event it is interested in. In the meantime, it can
    finish off other valuable work and therefore be more responsive than if it were
    to block, waiting on the event. Instead of blocking on an event, we just want
    to tell the timer to notify us when it goes off, and proceed to do other work
    meanwhile. For this purpose, we call the `async_wait` member function and pass
    it a *completion handler*. A completion handler is a function object we register
    using `async_wait` to be called once the timer expires:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.13: Waiting asynchronously**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two essential changes in listing 11.13 compared to listing 11.12\.
    We call the `async_wait` member function of `deadline_timer` instead of `wait`,
    passing it a pointer to the completion handler function `on_timer_expiry`. We
    then call `run` on the `io_service` object. When we run this program, it prints
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The call to `async_wait` does not block (line 24) and therefore the first two
    lines are printed in quick succession. Following this, the call to `run` (line
    27) blocks until the timer expires, and the completion handler for the timer is
    dispatched. Unless some error occurred, the completion handler prints `Timer expired`.
    Thus, there is a time lag between the appearance of the first two messages and
    the third message, which is from the completion handler.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous logic using Asio coroutines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `async_wait` member function of `deadline_timer` initiates an asynchronous
    operation. Such a function returns before the operation it initiates is completed.
    It registers a completion handler, and the completion of the asynchronous event
    is notified to the program through a call to this handler. If we have to run such
    asynchronous operations in a sequence, the control flow becomes complex. For example,
    let us suppose we want to wait for 5 seconds, print `Hello`, then wait for 10
    more seconds, and finally, print `world`. Using synchronous `wait`, it is as easy
    as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In many real-life scenarios, especially with network I/O, blocking on synchronous
    operations is just not an option. In such cases, the code becomes considerably
    more complex. Using `async_wait` as a model asynchronous operation, the following
    example illustrates the complexity of asynchronous code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.14: Asynchronous operations**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The move from synchronous to asynchronous logic for the same functionality incurs
    more than double the lines of code and a complex control flow. We register the
    function `print_hello` (line 10) as the completion handler for the first 5-second
    wait (lines 22, 24). `print_hello` in turn starts a 10-second wait using the same
    timer, and registers the function `print_world` (line 6), as the completion handler
    for this wait (lines 14-15).
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we use `boost::bind` to generate the completion handler for the
    first 5-second wait, passing the `timer` from the `main` function to the `print_hello`
    function. The `print_hello` function thus uses the same timer. Why did we need
    to do it this way? First of all, `print_hello` needs to use the same `io_service`
    instance to initiate the 10-second wait operation and the earlier 5-second wait.
    The `timer` instance refers to this `io_service` instance and is used by both
    completion handlers. Moreover, creating a local `deadline_timer` instance in `print_hello`
    would be problematic because `print_hello` would return before the timer would
    go off, and the local timer object would be destroyed, so it would never go off.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11.14 illustrates the problem of *inversion of control flow*, which
    is a source of significant complexity in asynchronous programming models. We can
    no longer string together a sequence of statements, and assume that each initiates
    an operation only once the operation initiated by the preceding statement is completed—a
    safe assumption for the synchronous model. Instead, we depend on notifications
    from `io_service` to determine the right time to run the next operation. The logic
    is fragmented across functions, and any data that needs to be shared across these
    functions requires more effort to manage.
  prefs: []
  type: TYPE_NORMAL
- en: Asio simplifies asynchronous programming using a thin wrapper around the Boost
    Coroutine library. Like with Boost Coroutine, it is possible to use stackful as
    well as stackless coroutines. In this book, we only look at stackful coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `boost::asio::spawn` function template, we can launch tasks as coroutines.
    If a coroutine is dispatched and it calls an asynchronous function, the coroutine
    is suspended. Meanwhile, the `io_service` dispatches other tasks, including other
    coroutines. Once an asynchronous operation is completed, the coroutine that initiated
    it is resumed, and it proceeds to the next step. In the following listing, we
    rewrite listing 11.14 using coroutines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.15: Asynchronous programming using coroutines**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `wait_and_print` function is the coroutine and takes two arguments: an
    object of type `boost::asio::yield_context` and a reference to an `io_service`
    instance (line 7). `yield_context` is a thin wrapper around Boost Coroutine. We
    must use `boost::asio::spawn` to dispatch a coroutine, and such a coroutine must
    have the signature `void (boost::asio::yield_context)`. Thus, we adapt the `wait_and_print`
    function using `boost::bind` to make it compatible with the coroutine signature
    expected by `spawn`. We bind the second argument to a reference to the `io_service`
    instance (lines 24-26).'
  prefs: []
  type: TYPE_NORMAL
- en: The `wait_and_print` coroutine creates a `deadline_timer` instance on the stack,
    and starts a 5-second asynchronous wait, passing its `yield_context` to the `async_wait`
    function in place of a completion handler. This suspends the `wait_and_print`
    coroutine, and it is resumed only once the wait is completed. In the meantime,
    other tasks if any can be processed from the `io_service` queue. Once the wait
    is over and `wait_and_print` is resumed, it prints `Hello` and starts a 10-second
    wait. Once again, the coroutine suspends, and it resumes only after the 10 seconds
    elapse, thereafter printing `world`. Coroutines make the asynchronous logic as
    simple and readable as the synchronous one, with very little overhead. In the
    following sections, we will use coroutines to write TCP and UDP servers.
  prefs: []
  type: TYPE_NORMAL
- en: UDP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The UDP I/O model is relatively simple and the distinction between client and
    server is blurred. For network I/O using UDP, we create a UDP socket, and use
    the `send_to` and `receive_from` functions to send datagrams to specific endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous UDP client and server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we write a UDP client (listing 11.16) and a synchronous UDP
    server (listing 11.17). The UDP client tries to send some data to a UDP server
    on a given endpoint. The UDP server blocks waiting to receive data from one or
    more UDP clients. After sending data, the UDP client blocks waiting to receive
    a response from the server. The server, after receiving the data, sends some response
    back before proceeding to handle more incoming messages.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.16: Synchronous UDP client**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We run the client by passing it the server hostname and the service (or port)
    to connect to on the command line. It resolves them to an endpoint (IP address
    and port number) for UDP (lines 13-17), creates a UDP socket for IPv4 (line 18),
    and calls the `send_to` member function on it. We pass to `send_to`, a `const_buffer`
    containing the data to be sent and the destination endpoint (line 23).
  prefs: []
  type: TYPE_NORMAL
- en: Each and every program that performs network I/O using Asio uses an *I/O service*,
    which is an instance of the type `boost::asio::io_service`. We have already seen
    `io_service` in action as a task manager. But the primary role of the I/O service
    is that of an interface for operations on the underlying operating system. Asio
    programs use *I/O objects* that are responsible for initiating I/O operations.
    Sockets, for example, are I/O objects.
  prefs: []
  type: TYPE_NORMAL
- en: We call the `send_to` member function on the UDP socket to send a predefined
    message string to the server (line 23). Note that we wrap the message array in
    a buffer sequence of length one constructed using the `boost::asio::buffer` function,
    as shown earlier in this chapter, in the section on buffers. Once `send_to` completes,
    the client calls `recv_from` on the same socket, passing a mutable buffer sequence
    constructed out of a writable character array using `boost::asio::buffer` (lines
    25-26). The second argument to `receive_from` is a non-const reference to a `boost::asio::ip::udp::endpoint`
    object. When `receive_from` returns, this object contains the address and port
    number of the remote endpoint, which sent the message (lines 28-29).
  prefs: []
  type: TYPE_NORMAL
- en: The calls to `send_to` and `receive_from` are **blocking calls**. The call to
    `send_to` does not return until the buffer passed to it has been written to the
    underlying UDP buffer in the system. Dispatching the UDP buffer over the wire
    to the server may happen later. The call to `receive_from` does not return until
    some data has been received.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use a single UDP socket to send data to multiple other endpoints, and
    we can receive data from multiple other endpoints on a single socket. Thus, each
    call to `send_to` takes the destination endpoint as input. Likewise, each call
    to `receive_from` takes a non-const reference to an endpoint, and on return, sets
    it to the sender''s endpoint. We will now write the corresponding UDP server using
    Asio:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.17: Synchronous UDP server**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The synchronous UDP server creates a single UDP endpoint of type `boost::asio::ip::udp::endpoint`
    on the port 55000, keeping the address unspecified (line 14). Notice that we use
    a two-argument `endpoint` constructor, which takes *the protocol* and port as
    arguments. The server creates a single UDP socket of type `boost::asio::ip::udp::socket`
    for this endpoint (line 15), and spins in a loop, calling `receive_from` on the
    socket per iteration, waiting until a client sends some data. The data is received
    in a `char` array called `msg`, which is passed to `receive_from` wrapped in a
    mutable buffer sequence of length one. The call to `receive_from` returns the
    number of bytes received, which is used to add a terminating null character in
    `msg` so that it can be used like a C-style string (line 22). In general, UDP
    presents the incoming data as a message containing a sequence of bytes and its
    interpretation is left to the application. Each time the server receives data
    from a client, it echoes back the data sent, preceded by a fixed greeting string.
    It does so by calling the `send_to` member function on the socket twice, passing
    the buffer to send, and the endpoint of the recipient (lines 26-27, 28).
  prefs: []
  type: TYPE_NORMAL
- en: The calls to `send_to` and `receive_from` are synchronous and return only once
    the data is passed completely to the OS (`send_to`) or received completely by
    the application (`receive_from`). If many instances of the client send messages
    to the server at the same time, the server can still only process one message
    at a time, and therefore the clients queue up waiting for a response. Of course,
    if the clients did not wait for a response, they could all have sent messages
    and exited but the messages would still be received by the server serially.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous UDP server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An asynchronous version of the UDP server can significantly improve the responsiveness
    of the server. A traditional asynchronous model can entail a more complex programming
    model, but coroutines can significantly improve the situation.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous UDP server using completion handler chains
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For asynchronous communication, we use the `async_receive_from` and `async_send_to`
    member functions of `socket`. These functions do not wait for the I/O request
    to be handled by the operating system but return immediately. They are passed
    a function object, which is to be called when the underlying operation is completed.
    This function object is queued in the task queue of the `io_service` and is dispatched
    when the actual operation on the operating system returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The signature for both the read handler passed to `async_receive_from` and
    the write handler passed to `async_send_to` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The handlers expect to be passed a non-const reference to an `error_code` object,
    indicating the status of the completed operation and the number of bytes read
    or written. The handlers can call other asynchronous I/O operations and register
    other handlers. Thus, the entire I/O operation is defined in terms of a chain
    of handlers. We now look at a program for an asynchronous UDP server:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.18: Asynchronous UDP server**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The UDP server is encapsulated in the class `UDPAsyncServer` (line 8). To start
    the server, we first create the obligatory `io_service` object (line 42), followed
    by an instance of `UDPAsyncServer` (line 43) that is passed the `io_service` instance
    and the port number it should use. Finally, a call to the `run` member function
    of `io_service` starts the processing of incoming requests (line 44). So how does
    `UDPAsyncServer` work?
  prefs: []
  type: TYPE_NORMAL
- en: The constructor of `UDPAsyncServer` initializes the member UDP `socket` with
    a local endpoint (lines 12-13). It then calls the member function `waitForReceive`
    (line 14), which in turn calls `async_receive_from` on the socket (line 18), to
    start waiting for any incoming messages. We call `async_receive_from,` passing
    a mutable buffer made from the `buffer` member variable (line 17), a non-const
    reference to the `remote_peer` member variable (line 18), and a lambda expression
    that defines a completion handler for the receive operation (lines 19-31). `async_receive_from`
    initiates an I/O operation, adds the handler to the task queue in `io_service`,
    and returns. The call to `run` on the `io_service` (line 43) blocks as long as
    there are I/O tasks in the queue. When a UDP message comes along, the data is
    received by the OS, and it invokes the handler to take further action. To understand
    how the UDP server keeps handling more and more messages ad infinitum, we need
    to understand what the handlers do.
  prefs: []
  type: TYPE_NORMAL
- en: The *receive handler* is invoked when the server receives a message. It prints
    the message received and the details of the remote sender (lines 22-23), and then
    issues a call to `waitForReceive`, thus restarting the receive operation. It then
    sends a message `hello from server` (line 21) back to the sender identified by
    the `remote_peer` member variable. It does so by calling the `async_send_to` member
    function of the UDP `socket`, passing the message buffer (line 27), the destination
    endpoint (line 28), and another handler in the form of a lambda (lines 29-32),
    which does nothing.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we capture the `this` pointer in the lambdas to be able to access
    the member variables from the surrounding scope (line 20, 29). Also, neither handler
    does error checking using the `error_code` argument, which is a must in real-world
    software.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous UDP server using coroutines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Handler chaining fragments the logic across a set of handlers and sharing state
    across handlers becomes particularly complex. It is the price for better performance,
    but it is a price we can avoid, as we saw earlier using Asio coroutines to handle
    asynchronous waits on `boost::asio::deadline_timer` in listing 11.15\. We will
    now use Asio coroutines to write an asynchronous UDP server:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.19: Asynchronous UDP server using Asio coroutines**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'With the use of coroutines, the structure of the asynchronous UDP server changes
    considerably from listing 11.18 and is closer to the synchronous model of listing
    11.17\. The function `udp_server` contains the core logic for the UDP server (line
    23). It is meant to be used as a coroutine; hence, one of its arguments is of
    type `boost::asio::yield_context` (line 23). It takes two additional arguments:
    a reference to an `io_service` instance (line 24) and the UDP server port (line
    25).'
  prefs: []
  type: TYPE_NORMAL
- en: In the main function, we create an instance of `io_service` (line 48), and then
    add a task to run `udp_server` as a coroutine, using the `boost::asio::spawn`
    function template (lines 49-50). We bind the service and port arguments of `udp_server`
    appropriately. We then call `run` on the `io_service` instance to start processing
    I/O operations. The call to `run` dispatches the `udp_server` coroutine (line
    51).
  prefs: []
  type: TYPE_NORMAL
- en: The `udp_server` coroutine creates a UDP socket associated with the unspecified
    IPv4 address (0.0.0.0) and the specific port passed as an argument (lines 27-29).
    The socket is wrapped in a `shared_ptr`, the reasons for which will become clear
    in a bit. There are additional variables on the coroutine stack to hold the data
    received from clients (line 31) and to identify the client endpoint (line 32).
    The `udp_server` function then spins in a loop calling `async_receive_from` on
    the socket, passing the `yield_context` for the receive handler (lines 36-37).
    This suspends the execution of the `udp_server` coroutine until `async_receive_from`
    completes. In the meantime, the call to `run` resumes and processes other tasks
    if any. Once a call to `async_receive_from` function completes, the `udp_server`
    coroutine resumes execution and proceeds to the next iteration of its loop.
  prefs: []
  type: TYPE_NORMAL
- en: For each completed receive operation, `udp_server` sends a fixed greeting string
    ("Hello from server") in response to the client. The task to send this greeting
    is also encapsulated in a coroutine, `udp_send_to` (line 14), which the `udp_server`
    coroutine adds to the task queue using `spawn` (line 40). We pass the UDP socket
    and the endpoint identifying the client as arguments to this coroutine. Notice
    that the local variable called `remote_peer` is passed by value to the `udp_send_to`
    coroutine (line 42). This is used inside `udp_send_to`, as an argument to `async_send_to`,
    to specify the recipient of the response (lines 19-20). We pass a copy rather
    than a reference to `remote_peer` because when the call to `async_send_to` is
    issued, another call to `async_receive_from` can be active and can overwrite the
    `remote_peer` object, before it is used by `async_send_to`. We also pass the socket
    wrapped in a `shared_ptr`. Sockets are not copyable unlike endpoints. If the socket
    object was on automatic storage in the `udp_server` function, and `udp_server`
    exited while there were still a pending `udp_send_to` task, the reference to the
    socket inside `udp_send_to` would be invalid and possibly lead to crashes. For
    this reason, the `shared_ptr` wrapper is the correct choice.
  prefs: []
  type: TYPE_NORMAL
- en: If you noticed, the handler to `async_receive_from` is written as `yield[ec]`
    (line 37). The `yield_context` class has an overloaded subscript operator using
    which we can specify a mutable reference to a variable of type `error_code`. When
    the asynchronous operation completes, the variable passed as the argument of the
    subscript operator is set to the error code if any.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prefer using coroutines over handler-chaining, when writing asynchronous servers.
    Coroutines enable simpler code and a more intuitive control flow.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and concurrency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We claimed that the asynchronous mode of communication improves responsiveness
    of the server. Let us understand exactly what factors contribute to this improvement.
    In the synchronous model of listing 11.17, a call to `receive_from` could not
    be issued unless the `send_to` function returned. In the asynchronous code of
    listing 11.18, `waitForReceive` is called as soon as a message is received and
    consumed (lines 23-25), and it does not wait for the `async_send_to` to complete.
    Likewise, in listing 11.19 which illustrates the use of coroutines in asynchronous
    models, coroutines help suspend a function waiting for an asynchronous I/O operation
    to complete, and to continue processing other tasks in the queue meanwhile. This
    is the principal source of improvement in the responsiveness of the asynchronous
    servers.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that in listing 11.18, all I/O happens on a single thread.
    This means that at any given point in time, our program handles only one incoming
    UDP message. This allows us to reuse the `buffer` and `remote_peer` member variables,
    without worrying about synchronization. We must still ensure that we print the
    received buffer (lines 22-23) before calling `waitForReceive` again (line 24).
    If we inverted that order, the buffer could potentially be overwritten by a new
    incoming message before it could be printed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider what would have happened if we called `waitForReceive` inside the
    receive handler rather than the send handler like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the receive would be started only after the send completed; so
    even with asynchronous calls it would be no better than the synchronous example
    in listing 11.17.
  prefs: []
  type: TYPE_NORMAL
- en: 'In listing 11.18, we do not need the buffer received from the remote peer while
    sending content back, so we do not need to hold on to that buffer till the send
    is complete. This allows us to start the asynchronous receive (line 24) without
    waiting for the send to complete. The receive can complete first and overwrite
    the buffer, but as long as the send operation does not use the buffer, everything
    is fine. Too often in the real world, this is not the case, so let us see how
    to fix this without delaying the receive till after the send. Here is a modified
    implementation of the handlers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now, instead of relying on a buffer that is a shared member variable, we allocate
    a buffer for receiving each new message (line 18). This obviates the need for
    the `buffer` member variable in listing 11.18\. We use the `boost::shared_array`
    wrapper because this buffer needs to be passed from the `waitForReceive` call
    to the receive handler and further; it should be released only when the last reference
    to it is gone. Likewise, we remove the `remote_peer` member variable that represented
    the remote endpoint, and use a `shared_ptr`-wrapped endpoint for each new request.
  prefs: []
  type: TYPE_NORMAL
- en: We pass the underlying array to `async_receive_from` (line 21), and make sure
    it survives long enough by capturing its `shared_array` wrapper in the completion
    handler for `async_receive_from` (line 23). For the same reason, we also capture
    the endpoint wrapper `epPtr`. The receive handler calls `waitForReceive` (line
    25), and then prints the message received from the client, prefixed with the thread
    ID of the current thread (with an eye on the future). It then calls `async_send_to`,
    passing the buffer received instead of some fixed message (line 34). Once again,
    we need to ensure that the buffer and remote endpoint survive till the send completes;
    so we capture the `shared_array` wrapper of the buffer and the `shared_ptr` wrapper
    of the remote endpoint in the send completion handler (line 36).
  prefs: []
  type: TYPE_NORMAL
- en: 'The changes for the coroutine-based asynchronous UDP server (listing 11.19)
    are on the same lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: As the data received from the client needs to be echoed back, the `udp_send_to`
    coroutine must have access to it. Thus, it takes the buffer containing the received
    data and the number of bytes read as arguments (line 17). In order to make sure
    that this data is not overwritten by a subsequent receive, we must allocate buffers
    for receiving the data in each iteration of the loop in `udp_server` (line 39).
    We pass this buffer, and also the number of bytes read as returned by `async_receive_from`
    (line 40) to `udp_send_to` (line 47). With these changes, our asynchronous UDP
    servers can now maintain the context of each incoming request until it has responded
    to that peer, without the need to delay the handling of newer requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'These changes also make the handlers thread-safe because essentially, we removed
    any shared data across handlers. While the `io_service` is still shared, it is
    a thread-safe object. We can easily turn the UDP server into a multithreaded server.
    Here is how we do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: This would create four worker threads that handle incoming UDP messages concurrently.
    The same would work with coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: TCP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In terms of network I/O, the programming model for UDP is about as simple as
    it gets—you either send a message, or receive a message, or do both. TCP is a
    fairly complex beast in comparison and its interaction model has a few additional
    details to understand.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to reliability guarantees, TCP implements several nifty algorithms
    to ensure that an overeager sender does not swamp a relatively slow receiver with
    lots of data (**flow control**), and all senders get a fair share of the network
    bandwidth (**congestion control**). There is a fair amount of computation that
    happens at the TCP layer for all of this, and TCP needs to maintain some state
    information to perform these computations. For this TCP uses **connections** between
    endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a TCP connection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **TCP connection** consists of a pair of TCP sockets, potentially on different
    hosts connected by an IP network and some associated state data. Relevant connection
    state information is maintained at each end of the connection. A **TCP server**
    typically starts *listening for incoming connections* and is said to constitute
    the **passive end** of the connection. A **TCP client** initiates a request to
    connect to a TCP server and is said to be the *active end* of the connection.
    A well-defined mechanism known as the **TCP 3-way handshake** is used for establishing
    TCP connections. Similar mechanisms exist for coordinated connection termination.
    Connections can also be unilaterally reset or terminated, like in case of applications
    or hosts going down for various reasons or in case of an irrecoverable error of
    some sort.
  prefs: []
  type: TYPE_NORMAL
- en: Client- and server-side calls
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For a TCP connection to be set up, a server process must be listening on an
    endpoint, and a client process must actively initiate a connection to that endpoint.
    The server performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a TCP listener socket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a local endpoint for listening to incoming connections and bind the TCP
    listener socket to this endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start listening for incoming connections on the listener.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accept any incoming connections, and open a server-side endpoint (different
    from the listener endpoint) to serve that connection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform communication on that connection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Handle the termination of the connection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue to listen for other incoming connections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The client in turn performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a TCP socket and, optionally, bind it to a local endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect to a remote endpoint serviced by a TCP server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once connection is established, perform communication on that connection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Handle termination of the connection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Synchronous TCP client and server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now write a TCP client which connects to a TCP server on a specified
    host and port, sends some text to the server, and then receives some messages
    back from the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.20: Synchronous TCP client**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The TCP client resolves the host and port (or service name) passed to it on
    the command line (lines 16-19) and creates an endpoint representing the server
    to connect to (line 21). It creates an IPv4 socket (line 23) and calls the `connect`
    member function on it to initiate a connection to the remote server (line 25).
    The `connect` call blocks until a connection is established, or throws an exception
    if the attempt to connect fails. Once the connection is successful, we use the
    `boost::asio::write` function to send the text `Hello from client` to the server
    (lines 27-28). We call the `shutdown` member function of the socket with the argument
    `shutdown_send` (line 29) to close the write channel to the server. This shows
    up as an EOF on the server-side. We then use the `read` function to receive any
    message sent by the server (lines 33-34). Both `boost::asio::write` and `boost::asio::read`
    are blocking calls. The call to `write` would throw an exception on failure, for
    example, if the connection was reset or the send timed out because of a busy server.
    We call a non-throwing overload of `read`, and on failure, it sets the non-const
    reference to the error code we pass to it.
  prefs: []
  type: TYPE_NORMAL
- en: The function `boost::asio::read` tries to read as many bytes as it can to fill
    the buffer passed, and blocks until either all the data has arrived, or an end-of-file
    is received. Although an end-of-file is flagged as an error condition by `read`,
    it could simply indicate that the server was done sending data, and we would be
    interested in whatever data was received. For this reason, we specifically use
    a non-throwing overload of `read`, and in case an error was set in the `error_code`
    reference, we distinguish between end-of-file and other errors (line 35). For
    the same reason, we called `shutdown` to close the write channel on this connection
    (line 29) so that the server did not wait for more input.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike UDP, TCP is stream-oriented and does not define message boundaries. An
    application must define its own mechanism to identify message boundaries. Some
    strategies include prefixing the length of the message to the message, using character
    sequences as message end-markers, or using messages of a fixed length. In the
    examples in this book, we use the `shutdown` member function of `tcp::socket`,
    which causes an end-of-file to be read by the receiver, indicating that we are
    done sending messages. This keeps the examples simple, but in practice, this is
    not the most flexible strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now write the TCP server, which will handle requests from this client:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.21: Synchronous TCP server**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The first thing that a TCP server does is to create a listener socket and bind
    it to a local endpoint. With Boost Asio, you do this by creating an instance of
    `asio::ip::tcp::acceptor` and passing it the endpoint to bind to (line 14). We
    create an IPv4 endpoint specifying only the port and not the address so that it
    uses the unspecified address 0.0.0.0 (line 13). We bind the endpoint to the listener
    by passing it to the constructor of the `acceptor` (line 14). We then spin in
    a loop waiting for incoming connections (line 16). We create a new socket as we
    need a distinct socket to serve as the server-side endpoint for each new connection
    (line 17). We then call the `accept` member function on the acceptor (line 18),
    passing it the new socket. The call to `accept` blocks until a new connection
    is established. When `accept` returns, the socket passed to it represents the
    server-side endpoint of the connection established.
  prefs: []
  type: TYPE_NORMAL
- en: We create a new thread to serve each new connection established (line 19). We
    generate the initial function for this thread using a lambda (line 19-44), capturing
    the `shared_ptr`-wrapped server-side `socket` for this connection (line 19). Within
    the thread, we call the `read` function to read data sent by the client (lines
    31-32), and then write data back using `write` (line 37). To show how it is done,
    we send data from a multi-buffer sequence set up from two character strings (lines
    22-26). The network I/O in this thread is done inside a try-block to make sure
    that no exception escapes the thread. Note that we call `close` on the socket
    after the call to `write` returns (line 38). This closes the connection from the
    server-side, and the client reads an end-of-file in the received stream.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency and performance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The TCP server handles each connection independently. But creating a new thread
    for each new connection scales badly, and the server''s resources could be overrun
    if a large number of connections hit it over a very short interval. One way to
    handle this is to limit the number of threads. Earlier, we modified the UDP server
    example from listing 11.18 to use a thread pool and limit the total number of
    threads. We can do the same with our TCP server from listing 11.21\. Here is an
    outline for how this can be done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: First, we create a pool of a fixed number of threads (lines 15-20), and make
    sure they do not exit by posting a dummy work to the `io_service`'s task queue
    (lines 13-14). Instead of creating a thread for each new connection, we post a
    handler for the connection to the task queue of the `io_service` (line 28). This
    handler can be exactly the same as the initial function of the per-connection
    thread in listing 11.21\. The threads in the pool then dispatch the handlers on
    their own schedule. The number of threads represented by `max_threads` can be
    tweaked easily based on the number of processors in the system.
  prefs: []
  type: TYPE_NORMAL
- en: While using the thread pool limits the number of threads, it does little to
    improve the responsiveness of the server. In the event of a large influx of new
    connections, handlers of the newer connections would form a big backlog in the
    queue, and these clients would be kept waiting while the server services earlier
    connections. We have already addressed similar concerns in our UDP server by using
    asynchronous I/O. In the next section, we will use the same strategy to scale
    our TCP servers better.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous TCP server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The synchronous TCP server is inefficient mainly because the read and write
    operations on the sockets block for a finite amount of time, waiting for the operations
    to complete. During this time, even with thread pools around, the thread serving
    the connection just waits idly for an I/O operation to go through, before it can
    proceed to handle the next available connection.
  prefs: []
  type: TYPE_NORMAL
- en: We can eliminate these idle waits using asynchronous I/O. Just as we saw with
    the asynchronous UDP server, we could either use chains of handlers or coroutines
    to write the asynchronous TCP server. While handler chains make the code complex,
    and therefore error-prone, coroutines make it far more readable and intuitive.
    We will first write an asynchronous TCP server using coroutines, and then use
    the more traditional handler-chaining, just to put the difference between the
    two approaches in perspective. You can skip the handler-chaining implementations
    on first reading.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous TCP server using coroutines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following is the complete code for a TCP server employing asynchronous
    I/O via coroutines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.22: Asynchronous TCP server using coroutines**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We use two coroutines: `accept_connections` handles incoming connection requests
    (line 42), while `handle_connection` performs I/O on each new connection (line
    14). The `main` function calls the `spawn` function template to add the `accept_connections`
    task to the `io_service` queue, to be run as a coroutine (line 63). The spawn
    function template is available through the header `boost/asio/spawn.hpp` (line
    2). The call to the `run` member function of the `io_service` invokes the `accept_connections`
    coroutine, which spins in a loop awaiting new connection requests (line 65).'
  prefs: []
  type: TYPE_NORMAL
- en: The `accept_connections` function takes two arguments in addition to the obligatory
    `yield_context`. These are a reference to the `io_service` instance, and the port
    to listen on for new connections—values bound by the `main` function when it spawns
    this coroutine (lines 63-64). The `accept_connections` function creates an endpoint
    for the unspecified IPv4 address and the specific port it is passed (lines 46-47),
    and creates an acceptor for that endpoint (line 48). It then calls the `async_accept`
    member function of the acceptor in each iteration of the loop, passing a reference
    to a TCP socket, and the local `yield_context` as the completion handler (line
    53). This suspends the `accept_connections` coroutine until a new connection is
    accepted. Once a new connection request is received, `async_accept` accepts it,
    sets the socket reference passed to it to the server-side socket for the new connection,
    and resumes the `accept_connections` coroutine. The `accept_connections` coroutine
    adds the `handle_connection` coroutine to the `io_service` queue for handling
    the I/O on this specific connection (lines 56-57). In the next iteration of the
    loop, it again waits for new incoming connections.
  prefs: []
  type: TYPE_NORMAL
- en: The `handle_connection` coroutine takes a TCP socket wrapped in a `shared_ptr`,
    as a parameter in addition to `yield_context`. The `accept_connections` coroutine
    creates this socket, and passes it to `handle_connection`, wrapped in the `shared_ptr`.
    The `handle_connection` function receives any data sent by the client using `async_read`
    (lines 23-24). If the receive is successful, it sends back a response string `Hello
    from server`, and then echoes back the received data, using a buffer sequence
    of length 2 (lines 28-30).
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous TCP server without coroutines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We now look at how to write an asynchronous TCP server without coroutines. This
    involves a more complex handshake between handlers, and hence, we want to split
    the code into appropriate classes. We define two classes in two separate header
    files. The class `TCPAsyncServer` (listing 11.23) represents the server instance
    that listens for incoming connections. It goes in the `asyncsvr.hpp` header file.
    The class `TCPAsyncConnection` (listing 11.25) represents the processing context
    of a single connection. It goes in the `asynconn.hpp` header file.
  prefs: []
  type: TYPE_NORMAL
- en: '`TCPAsyncServer` creates a new instance of `TCPAsyncConnection` for each new
    incoming connection. The `TCPAsyncConnection` instance reads incoming data from
    the client and sends back messages to the client until the client closes the connection
    to the server.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the server, you create an instance of `TCPAsyncServer`, passing the
    instance of `io_service` and a port number, and then call the `run` member function
    of the `io_service` to start processing new connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.23: Asynchronous TCP server (asyncsvr.hpp)**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The `TCPAsyncServer` class has an acceptor member variable of type `boost::asio::ip::tcp::acceptor`,
    which is used to listen for and accept incoming connections (line 39). The constructor
    initializes the acceptor with a local TCP endpoint on the unspecified IPv4 address
    and a specific port (lines 17-19), and then calls the `waitForConnection` member
    function (line 20).
  prefs: []
  type: TYPE_NORMAL
- en: The `waitForConnection` function creates a new instance of `TCPAsyncConnection`
    wrapped in a `shared_ptr` called `connectionPtr` (lines 24-25) to handle each
    new connection from a client. We have included our own header file `asynconn.hpp`
    to access the definition of `TCPAsyncConnection` (line 7), which we will look
    at shortly. It then calls the `async_accept` member function on the acceptor to
    listen for new incoming connections and accept them (line 26-27). We pass to `async_accept`,
    a non-const reference to a `tcp::socket` object that is a member of `TCPAsyncConnection`,
    and a completion handler that is called each time a new connection is established
    (lines 27-35). It is an asynchronous call and returns immediately. But each time
    a new connection is established, the socket reference is set to the server-side
    socket for serving that connection, and the completion handler gets called.
  prefs: []
  type: TYPE_NORMAL
- en: The completion handler for `async_accept` is written as a lambda, and it captures
    the `this` pointer pointing to the `TCPAsyncServer` instance and the `connectionPtr`
    (line 27). This allows the lambda to call member functions on both the `TCPAsyncServer`
    instance, and on the `TCPAsyncConnection` instance serving this specific connection.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The lambda expression generates a function object and the captured `connectionPtr`
    is copied to a member of it. Since `connectionPtr` is a `shared_ptr`, its reference
    count is bumped up in the process. The `async_accept` function pushes this function
    object into the task handler queue of `io_service`, so the underlying instance
    of `TCPAsyncConnection` survives, even after `waitForConnection` returns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon connection establishment, when the completion handler is called, it does
    two things. If there were no errors, it initiates I/O on the new connection by
    calling the `waitForReceive` function on the `TCPAsyncConnection` object (line
    32). It then restarts the wait for the next connection by calling `waitForConnection`
    on the `TCPAsyncServer` object, via the captured `this` pointer (line 33). In
    case of an error, it prints a message (lines 29-30). The `waitForConnection` call
    is asynchronous, and we will soon find out that so is the `waitForReceive` call
    because both call asynchronous Asio functions. Once the handler returns, the server
    proceeds to handle I/O on existing connections or accepts new connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.24: Running the async server**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the server, we simply instantiate it with the `io_service` and port
    number (line 12), and then call the `run` method on `io_service` (line 13). The
    server we are building will be thread-safe, so we can as well call `run` from
    each of the pool of threads to introduce some concurrency in the processing of
    incoming connections. We will now see how I/O on each connection is handled:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 11.25: Per-connection I/O Handler class (asynconn.hpp)**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We saw in listing 11.23 how an instance of `TCPAsyncConnection` gets created,
    wrapped in a `shared_ptr`, to handle each new connection, and I/O is initiated
    on it by a call to the `waitForReceive` member function. Let us now understand
    its implementation. `TCPAsyncConnection` has two public members for performing
    asynchronous I/O on the connection: `waitForReceive` to perform asynchronous receives
    (line 23) and `startSend` to perform asynchronous sends (line 40).'
  prefs: []
  type: TYPE_NORMAL
- en: The `waitForReceive` function initiates a receive by calling the `­async­_read`
    function on the socket (line 25). The data is received into the `buf` member (line
    57). The completion handler for this call (line 26-37) is invoked when the data
    is completely received. If there were no errors, it calls `startSend`, which asynchronously
    sends a message to the client (line 28), and then calls `waitForReceive` again,
    provided an end-of-file was not encountered by the previous receive (line 32).
    Thus, as long as there was no read error, the server keeps waiting to read more
    data on the connection. If there was an error, it prints a diagnostic message
    (lines 34-35).
  prefs: []
  type: TYPE_NORMAL
- en: The `startSend` function uses the function `async_write` to send the text `Hello
    from server` to the client. Its handler does not do anything on success but prints
    a diagnostic message on failure (lines 49-50). For EOF write errors, it closes
    the socket (line 47).
  prefs: []
  type: TYPE_NORMAL
- en: Lifetime of TCPAsyncConnection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each instance of `TCPAsyncConnection` needs to survive as long as the client
    remains connected to the server. This makes it difficult to bind the scope of
    this object to any function in the server. This is the reason we create the `TCPAsyncConnection`
    object wrapped in a `shared_ptr`, and then capture it in handler lambdas. The
    `TCPAsyncConnection` member functions for performing I/O on the connection, `waitForReceive`
    and `startSend`, are both asynchronous. So they push a handler into the `io_service`'s
    task queue before returning. These handlers capture the `shared_ptr` wrapped instance
    of `TCPAsyncConnection` to keep the instance alive across calls.
  prefs: []
  type: TYPE_NORMAL
- en: In order for the handlers to have access to the `shared_ptr`-wrapped instance
    of the `TCPAsyncConnection` object from within `waitForReceive` and `startSend`,
    it is required that these member functions of `TCPAsyncConnection` have access
    to the `shared_ptr` wrapped instance on which they are called. The *enable shared
    from this* idiom, which we learned in [Chapter 3](ch03.html "Chapter 3. Memory
    Management and Exception Safety"), *Memory Management and Exception Safety*, is
    tailor-made for such purposes. This is the reason we derive `TCPAsyncConnection`
    from `enable_shared_from_this<TCPAsyncConnection>`. By virtue of this, `TCPAsyncConnection`
    inherits the `shared_from_this` member function, which returns the `shared_ptr`-wrapped
    instance we need. This means that `TCPAsyncConnection` should always be allocated
    dynamically and wrapped in a `shared_ptr`, and any other way would result in undefined
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: This is the reason we call `shared_from_this` in both `waitForReceive` (line
    24) and `startSend` (line 42), and it is captured by the respective handlers (lines
    26, 44). As long as the `waitForReceive` member function keeps getting called
    from the completion handler for `async_read` (line 32), the `TCPAsyncConnection`
    instance survives. If an error is encountered in receive, either because the remote
    endpoint closed the connection or for another reason, then this cycle breaks.
    The `shared_ptr` wrapping the `TCPAsyncConnection` object is no longer captured
    by any handler and is destroyed at the end of the scope, closing the connection.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and concurrency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Notice that both implementations of TCP asynchronous server, with and without
    coroutines, are single-threaded. However, there are no thread-safety issues in
    either implementation, so we could have as well employed a thread pool, each of
    whose threads would call `run` on the `io_service`.
  prefs: []
  type: TYPE_NORMAL
- en: Inversion of control flow
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The most significant difficulty with programming asynchronous systems is the
    inversion of control flow. To write the code for a synchronous server, we know
    we have to call the operations in the following sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: Call `accept` on the acceptor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call `read` on the socket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call `write` on the socket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We know that `accept` returns only when the connection has been established,
    so it is safe to call `read`. Also, `read` returns only after it has read the
    number of bytes asked for, or encountered an end-of-file. So it is safe for a
    `write` call to follow. This made writing code incredibly easy compared to the
    asynchronous model, but introduced waits that affected our ability to handle other
    waiting connections, while our requests were being serviced.
  prefs: []
  type: TYPE_NORMAL
- en: We eliminated that wait with asynchronous I/O, but lost the simplicity of the
    model when we used handler chaining. As we cannot deterministically tell at which
    point an asynchronous I/O operation is completed, we ask the `io_service` to run
    specific handlers on completion of our requests. We still know which operation
    to perform after which, but we no longer know when. So we tell the `io_service`
    *what* to run, and it uses the appropriate notifications from the OS to know *when*
    to run them. The biggest challenge in this model is to maintain object states
    and managing object lifetimes across handlers.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines eliminate this *inversion of control flow* by allowing the sequence
    of asynchronous I/O operations to be written in a single coroutine, which is *suspended*
    instead of waiting for an asynchronous operation to complete, and *resumed* when
    the operation is completed. This allows for wait-free logic without the inherent
    complexities of handler chaining.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Always prefer coroutines over handler chaining when writing asynchronous servers.
  prefs: []
  type: TYPE_NORMAL
- en: Self-test questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For multiple choice questions, choose all options that apply:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between `io_service::dispatch` and `io_service::post`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. `dispatch` returns immediately while `post` runs the handler before returning
  prefs: []
  type: TYPE_NORMAL
- en: b. `post` returns immediately while `dispatch` may run the handler on the current
    thread if it can, or it behaves like post
  prefs: []
  type: TYPE_NORMAL
- en: c. `post` is thread-safe while `dispatch` is not
  prefs: []
  type: TYPE_NORMAL
- en: d. `post` returns immediately while `dispatch` runs the handler
  prefs: []
  type: TYPE_NORMAL
- en: What happens if a handler throws an exception when it is dispatched?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. It is undefined behavior
  prefs: []
  type: TYPE_NORMAL
- en: b. It terminates the program with a call to `std::terminate`
  prefs: []
  type: TYPE_NORMAL
- en: c. The call to run, on the `io_service` that dispatched the handler, will throw
  prefs: []
  type: TYPE_NORMAL
- en: d. The `io_service` is stopped
  prefs: []
  type: TYPE_NORMAL
- en: What is the role of the unspecified address 0.0.0.0 (IPv4) or ::/1 (IPv6)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. It is used to communicate with local services on a system
  prefs: []
  type: TYPE_NORMAL
- en: b. Packets sent to this address are echoed back to the sender
  prefs: []
  type: TYPE_NORMAL
- en: c. It is used to broadcast to all connected hosts in the network
  prefs: []
  type: TYPE_NORMAL
- en: d. It is used to bind to all available interfaces without the need to know addresses
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following statements about TCP are true?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. TCP is faster than UDP
  prefs: []
  type: TYPE_NORMAL
- en: b. TCP detects data corruption but not data loss
  prefs: []
  type: TYPE_NORMAL
- en: c. TCP is more reliable than UDP
  prefs: []
  type: TYPE_NORMAL
- en: d. TCP retransmits lost or corrupted data
  prefs: []
  type: TYPE_NORMAL
- en: What do we mean when we say that a particular function, for example, `async_read`,
    is asynchronous?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. The function returns before the requested action is complete
  prefs: []
  type: TYPE_NORMAL
- en: b. The function starts the operation on a different thread and returns immediately
  prefs: []
  type: TYPE_NORMAL
- en: c. The requested action is queued for processing by the same or another thread
  prefs: []
  type: TYPE_NORMAL
- en: d. The function performs the action if it immediately can, or returns an error
    if it cannot immediately perform the action
  prefs: []
  type: TYPE_NORMAL
- en: How can we ensure that an object created just before calling an asynchronous
    function would still be available in the handler?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Make the object global.
  prefs: []
  type: TYPE_NORMAL
- en: b. Copy/capture the object wrapped in a `shared_ptr` in the handler.
  prefs: []
  type: TYPE_NORMAL
- en: c. Allocate the object dynamically and wrap it in a `shared_ptr`.
  prefs: []
  type: TYPE_NORMAL
- en: d. Make the object a member of the class.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Asio is a well-designed library that can be used to write fast, nimble network
    servers that utilize the most optimal mechanisms for asynchronous I/O available
    on a system. It is an evolving library and is the basis for a Technical Specification
    that proposes to add a networking library to a future revision of the C++ Standard.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use the Boost Asio library as a task queue
    manager and leverage Asio's TCP and UDP interfaces to write programs that communicate
    over the network. Using Boost Asio, we were able to highlight some of the general
    concerns of network programming, the challenges to scaling for a large number
    of concurrent connections, and the advantages and complexity of asynchronous I/O.
    In particular, we saw how using stackful coroutines makes writing asynchronous
    servers a breeze, compared to the older model of chaining handlers. While we did
    not cover stackless coroutines, the ICMP protocol, and serial port communications
    among other things, the topics covered in this chapter should provide you with
    a solid foundation for understanding these areas.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Thinking Asynchronously in C++* (blog), *Christopher Kohlhoff*: [http://blog.think-async.com/](http://blog.think-async.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Networking Library Proposal*, *Christopher Kohlhoff*: [http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4332.html](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4332.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
