- en: 15\. Monitoring and Autoscaling in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will introduce you to how Kubernetes enables you to monitor your
    cluster and workloads, and then use the data collected to automatically drive
    certain decisions. You will learn about the Kubernetes Metric Server, which aggregates
    all cluster runtime information, allowing you to use this information to drive
    application runtime scaling decisions. We will walk you through setting up monitoring
    using the Kubernetes Metrics server and Prometheus and then use Grafana to visualize
    those metrics. By the end of this chapter, you will also have learned how to automatically
    scale up your application to completely utilize the resources on the provisioned
    infrastructure, as well as automatically scale your cluster infrastructure as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a moment to reflect on our progress through this series of chapters
    beginning from *Chapter 11*, *Build Your Own HA Cluster*. We started by setting
    up a Kubernetes cluster using kops to configure AWS infrastructure in a highly
    available manner. Then, we used Terraform and some scripting to improve the stability
    of our cluster and deploy our simple counter app. After this, we began hardening
    the security and increasing the availability of our app using Kubernetes/cloud-native
    principles. Finally, we learned how to run a stateful database responsible for
    using transactions to ensure that we always get a series of increasing numbers
    from our application.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to explore how to leverage the data that already
    exists in Kubernetes about our applications to drive and automate decision-making
    processes around scaling them so that they are always the right size for our load.
    Because it takes time to observe application metrics, schedule and start containers,
    and bootstrap nodes from scratch, this scaling is not instantaneous but will eventually
    (usually within minutes) balance the number of pods and nodes needed to perform
    the work of the load on the cluster. To achieve this, we need a way of getting
    this data, understanding/interpreting this data, and feeding back instructions
    to Kubernetes with this data. Luckily, there are already tools in Kubernetes that
    will help us do this. These are the **Kubernetes Metric Server**, **HorizontalPodAutoscalers**
    (**HPAs**), and the **ClusterAutoscaler**.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes has built-in support for providing useful monitoring information
    about infrastructure components as well as various Kubernetes objects. The Kubernetes
    Metrics server is a component (which does not come built-in) that gathers and
    exposes the metrics data at an API endpoint on the API server. Kubernetes uses
    this data to manage the scaling of Pods, but this data can also be scraped by
    a third-party tool such as Prometheus for use by cluster operators. Prometheus
    has a few very basic data visualization functions and primarily serves as a metric-gathering
    and storage tool, so you can use a more powerful and useful data visualization
    tool such as Grafana. Grafana allows cluster admins to create useful dashboards
    to monitor their clusters. You can learn more about how monitoring in Kubernetes
    is architected at this link: [https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/monitoring_architecture.md](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/monitoring_architecture.md).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how this will look for us in a diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1: An overview of the monitoring pipeline that'
  prefs: []
  type: TYPE_NORMAL
- en: we will implement in this chapter
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.1: An overview of the monitoring pipeline that we will implement
    in this chapter'
  prefs: []
  type: TYPE_NORMAL
- en: 'This diagram represents how the monitoring pipeline is going to be implemented
    through various Kubernetes objects. In summary, the monitoring pipeline will work
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The various components of Kubernetes are already instrumented to provide various
    metrics. The Kubernetes Metrics server will fetch these metrics from the components.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Kubernetes Metrics server will then expose these metrics on an API endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prometheus will access this API endpoint, scrape these metrics, and add it to
    its special database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Grafana will query the Prometheus database to gather these metrics and present
    it in a neat dashboard with graphs and other visual representations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's look at each of the previously mentioned components to understand
    them better.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Metrics API/Metrics Server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Kubernetes Metrics server (formerly known as Heapster) gathers and exposes
    metric data on the running state of all Kubernetes components and objects in Kubernetes.
    Nodes, control plane components, running pods, and really any Kubernetes objects
    are all observable via the Metrics server. Some examples of the metrics that it
    collects are the number of pods that are desired in a Deployment/ReplicaSet, the
    number of pods posting a `Ready` status in that Deployment, and the CPU and memory
    utilization of each container.
  prefs: []
  type: TYPE_NORMAL
- en: We will mostly be using the default exposed metrics while gathering the information
    relevant to the Kubernetes objects that we are orchestrating our application.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prometheus is a metric collector, a time-series database, and an alert manager
    for just about anything. It makes use of a scraping function to pull metrics from
    running processes that expose those metrics in Prometheus format at a defined
    interval. Those metrics are then stored in their own time-series database and
    you can run queries on this data to get a snapshot of the state of your running
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: It also comes with an alert manager function, which allows you to set up triggers
    to alert your on-call admins. As an example, you can configure the alert manager
    to automatically trigger an alert if the CPU utilization on one of your nodes
    is above 90% for 15 minutes. The alert manager can interface with several third-party
    services to send the alert via various means, such as email, chat messages, or
    SMS phone alerts.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to learn more about Prometheus, you can refer to this book: [https://www.packtpub.com/virtualization-and-cloud/hands-infrastructure-monitoring-prometheus](https://www.packtpub.com/virtualization-and-cloud/hands-infrastructure-monitoring-prometheus).'
  prefs: []
  type: TYPE_NORMAL
- en: Grafana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Grafana is an open-source tool that can be used to visualize data and create
    useful dashboards. Grafana will query the Prometheus database for metrics and
    graph them on dashboard charts that are easier for humans to understand and spot
    trends or discrepancies. These tools are indispensable when running a production
    cluster as they help us spot issues in the infrastructure quickly and resolve
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Your Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While application monitoring is beyond the scope of this book, we will provide
    some rough guidelines so that you can explore more on this topic. We would recommend
    that you expose your application's metrics in Prometheus format and use Prometheus
    to scrape them; there are many libraries for most languages that can help with
    this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way is to use Prometheus exporters that are available for various applications.
    Exporters gather the metrics from an application and expose them to an API endpoint
    so that Prometheus can scrape it. You can find several open-source exporters for
    common applications at this link: [https://prometheus.io/docs/instrumenting/exporters/](https://prometheus.io/docs/instrumenting/exporters/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For your custom applications and frameworks, you can create your own exporters
    using the libraries provided by Prometheus. You can find the relevant guidelines
    at this link: [https://prometheus.io/docs/instrumenting/writing_exporters/](https://prometheus.io/docs/instrumenting/writing_exporters/).'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have exposed and scraped the metrics from your applications, you can
    present them in a Grafana dashboard, similar to the one we will create for monitoring
    Kubernetes components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 15.01: Setting up the Metrics Server and Observing Kubernetes Objects'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we are going to be setting up monitoring for Kubernetes objects
    in our cluster and running a few queries and creating visualizations to see what''s
    going on. We''re going to be installing Prometheus, Grafana, and the Kubernetes
    Metrics server:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, we will recreate your EKS cluster from the Terraform file in
    *Exercise 12.02*, *Creating a Cluster with EKS Using Terraform*. If you already
    have the `main.tf` file, you can work with it. Otherwise, you can run the following
    command to get it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, use the following two commands one after the other to get your cluster
    resources up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need `jq` for the following command. `jq` is a simple tool to manipulate
    JSON data. If you don''t already have it installed, you can do so by using this
    command: `sudo apt install jq`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up the Kubernetes Metrics server in our cluster, we need to run the
    following in sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.2: Deploying all the objects required for the Metrics server'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.2: Deploying all the objects required for the Metrics server'
  prefs: []
  type: TYPE_NORMAL
- en: 'To test this, let''s run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are getting a `ServiceUnavailable` error, please check whether your firewall
    rules are allowing the API server to communicate with the node running the Metrics
    server.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have been frequently using the `kubectl get` commands by naming the object.
    We have seen in *Chapter 4*, *How to Communicate with Kubernetes (API Server)*,
    that Kubectl interprets the request, points the request to the appropriate endpoint,
    and formats the results in a readable format. But here, since we have created
    a custom endpoint at our API server, we have to point toward it using the `--raw`
    flag. You should see a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.3: Response from the Kubernetes Metrics server'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.3: Response from the Kubernetes Metrics server'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see here, the response contains JSON blobs that define a metric namespace,
    metric values, and metric metadata, such as a node name and availability zones.
    However, these metrics are not very readable. We will make use of Prometheus to
    aggregate them and then use Grafana to present the aggregated metrics in a concise
    dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have metric data being aggregated. Let''s start scraping and visualizing
    with Prometheus and Grafana. For this, we will install Prometheus and Grafana
    using Helm. Run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are installing and running helm for the first time, you will need to
    run the following command to get stable repos:'
  prefs: []
  type: TYPE_NORMAL
- en: '`help repo add stable https://kubernetes-charts.storage.googleapis.com/`'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.4: Installing the Helm chart for Prometheus'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.4: Installing the Helm chart for Prometheus'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s install Grafana in a similar fashion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.5: Installing the Helm chart for Grafana'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.5: Installing the Helm chart for Grafana'
  prefs: []
  type: TYPE_NORMAL
- en: In this screenshot, notice the `NOTES:` section, which lists two steps. Follow
    these steps to get your Grafana admin password and your endpoint to access Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are running the first command that Grafana showed in the output of
    the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Please use the version of the commands that you got; the command will be customized
    for your instance. This command gets your password, which is stored in a Secret,
    decodes it, and echoes it in your terminal output so that you can copy it for
    use in further steps. You should see a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s run the next two commands that Grafana asked us to run, as seen
    in *Figure 15.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, use the command that you obtain for your instance as this will be customized.
    These commands find the Pod that Grafana is running on and then map a port from
    our local machine to it so that we can easily access it. You should see the following
    response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'At this step, if you are facing any issues with getting the proper Pod name,
    you can simply run `kubectl get pods` to find the name of the Pod running Grafana
    and use that name instead of the shell (`$POD_NAME`) variable. So, your command
    will look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl --namespace default port-forward grafana-1591658222-7cd4d8b7df-b2hlm
    3000`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, open your browser and visit `http://localhost:3000` to access Grafana.
    You should see the following landing page:![Figure 15.6: The log-in page for the
    Grafana dashboard'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B14870_15_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.6: The log-in page for the Grafana dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: The default username is `admin` and the password is the value echoed in the
    output of *step 6*. Use that to log in.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a successful login, you should see this page:![Figure 15.7: The Grafana
    Home dashboard'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B14870_15_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.7: The Grafana Home dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a dashboard for Kubernetes metrics. To do so, we need to
    set up Prometheus as a data source for Grafana. On the left sidebar, click on
    `Configuration` and then on `Data Sources`:![Figure 15.8: Selecting Data Sources
    from the Configuration menu'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B14870_15_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.8: Selecting Data Sources from the Configuration menu'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see this page:![Figure 15.9: The Add data source option'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B14870_15_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.9: The Add data source option'
  prefs: []
  type: TYPE_NORMAL
- en: Now, click on the `Add data source` button.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see this page with several database options. Prometheus should be
    on top. Click on that:![Figure 15.10: Choosing Prometheus as our data source for
    the Grafana dashboard'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B14870_15_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.10: Choosing Prometheus as our data source for the Grafana dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: Now, before we move on to the next screen, here, we need to get the URL that
    Grafana will use to access the Prometheus database from inside the cluster. We
    will do that in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new terminal window and run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.11: Getting the list of all services'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.11: Getting the list of all services'
  prefs: []
  type: TYPE_NORMAL
- en: Copy the name of the service that starts with `prometheus` and ends in `server`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After *step 12*, you will have arrived at the screen shown in the following screenshot:![Figure
    15.12: Entering the address of our Prometheus service in Grafana'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B14870_15_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.12: Entering the address of our Prometheus service in Grafana'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `URL` field of the `HTTP` section, enter the following value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that you should see `Data source is working`, as shown in the preceding
    screenshot. Then, click on the `Save and Test` button at the bottom. The reason
    we have added `.default` to our URL is that we deployed this Helm chart to the
    `default` Kubernetes namespace. If you deployed it to another namespace, you should
    replace `default` with the name of your namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s set up the dashboard. Back on the Grafana home page (`http://localhost:3000`),
    click on the `+` symbol on the left sidebar, and then click on `Import`, as shown
    here:![Figure 15.13: Navigating to import Dashboard option'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B14870_15_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.13: Navigating to import Dashboard option'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the next page, you should see the `Grafana.com Dashboard` field, as shown
    here:![Figure 15.14: Entering the source to import the dashboard from'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B14870_15_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.14: Entering the source to import the dashboard from'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paste the following link into the `Grafana.com Dashboard` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This is an officially supported Kubernetes dashboard. Once you click anywhere
    outside the file, you should automatically advance to the next screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous step should lead you to this screen:![Figure 15.15: Setting Prometheus
    as the data source for the imported dashboard'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B14870_15_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.15: Setting Prometheus as the data source for the imported dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: Where you see the `prometheus`, click on the drop-down list next to it, select
    `Prometheus`, and hit `Import`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result should look like this:![Figure 15.16: The Grafana dashboard to monitor
    our cluster'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B14870_15_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.16: The Grafana dashboard to monitor our cluster'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have a concise dashboard for monitoring workloads in Kubernetes.
    In this exercise, we deployed our Metric Server to collect and expose Kubernetes
    object metrics, then we deployed Prometheus to store those metrics and Grafana
    to help us visualize the collected metrics in Prometheus, which will inform us
    as to what's going on in our cluster at any point in time. Now, it's time to use
    that information to scale things.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes allows you to automatically scale your workloads to adapt to changing
    demands on your applications. The information gathered from the Kubernetes Metrics
    server is the data that is used for driving the scaling decisions. In this book,
    we will be covering two types of scaling action—one that impacts the number of
    running pods in a Deployment and another that impacts the number of running nodes
    in a cluster. Both are examples of horizontal scaling. Let''s briefly gain an
    intuition for what both the horizontal scaling of pods and the horizontal scaling
    of nodes would entail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pods**: Assuming that you filled out the `resources:` section of `podTemplate`
    when creating a Deployment in Kubernetes, each container within that pod will
    have the `requests` and `limits` fields, as designated by the corresponding `cpu`
    and `memory` fields. When the resources needed to process a workload exceed that
    which you have allocated, then by adding additional replicas of a pod to the Deployment,
    you are horizontally scaling to add capacity to your Deployment. By letting a
    software process decide the number of replicas of a Pod in a Deployment for you
    based on load, you are *autoscaling* your deployment to keep the number of replicas
    consistent with the metric you have defined to express your application''s load.
    One such metric for application load could be the percentage of the allocated
    CPU that is currently being consumed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nodes**: Every node has a certain amount of CPU (typically expressed by the
    number of cores) and memory (typically expressed in gigabytes) that it has available
    for consumption by Pods. When the total capacity of all worker nodes is exhausted
    by all running pods (meaning that the CPU and memory requests/limits for all the
    Pods are equal to or greater than that of the whole cluster), then we have saturated
    the resources of our cluster. In order to allow more Pods to be run on the cluster,
    or to allow more autoscaling to take place in the cluster, we need to add capacity
    in the form of additional worker nodes. When we allow a software process to make
    this decision for us, we are considered to be *autoscaling* the total capacity
    of our cluster. In Kubernetes, this is handled by the ClusterAutoscaler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When you increase the number of pod replicas of an application, it is known
    as horizontal scaling and is handled by the **HorizontalPodAutoscaler**. If, instead,
    you were to increase the resource limits for your replicas, that would be called
    vertical scaling. Kubernetes also offers **VerticalPodAutoscaler**, but we are
    leaving it out for brevity, and due to the fact that it is not yet generally available
    and safe for use in production.
  prefs: []
  type: TYPE_NORMAL
- en: Using both HPAs and ClusterAutoscalers in conjunction with each other can be
    an effective way for companies to ensure that they always have the right amount
    of application resources deployed for their load and that they aren't paying too
    much for it at the same time. Let's examine both of them in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: HorizontalPodAutoscaler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HPAs are responsible for making sure that the number of replicas of your application
    in a Deployment match whatever the current demand as measured by a metric. This
    is useful because we can use real-time metric data, which is already gathered
    by Kubernetes, to always ensure that our application is meeting the demands we
    have set forth in our thresholds. This may be a new concept to some application
    owners who are not used to running applications using data, but once you begin
    to leverage tools that can right-size your deployments, you will never want to
    go back.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has an API resource in the `autoscaling/v1` and `autoscaling/v2beta2`
    groups to provide a definition of autoscaling triggers that can run against another
    Kubernetes resource, which is most often a Kubernetes Deployment object. In the
    case of `autoscaling/v1`, the only supported metric is the current CPU consumption,
    and in the case of `autoscaling/v2beta2`, there is support for any custom metrics.
  prefs: []
  type: TYPE_NORMAL
- en: HPA queries the Kubernetes Metric Server to look at the metrics for the particular
    deployment. Then, the autoscaling resource will determine whether or not the currently
    observed metric is beyond the threshold for a scaling target. If it is, then it
    will change the number of Pods desired by the deployment to be higher or lower
    depending on the load.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider a shopping cart microservice hosted by an e-commerce
    company. The shopping cart service experiences a heavy load during the coupon
    code-entry process because it must traverse all items in the cart and search for
    active coupons on them before validating a coupon code. On a random Tuesday morning,
    there are many shoppers online using the service and they all want to use coupons.
    Normally, the service would become overwhelmed and requests would start to fail.
    However, if you were able to use an HPA, Kubernetes would use the spare computing
    power of your cluster to ensure that there are enough Pods of this shopping cart
    service to be able to handle the load.
  prefs: []
  type: TYPE_NORMAL
- en: Note that simply autoscaling a Deployment is not a "one-size-fits-all" solution
    to performance problems in an application. There are many places in modern applications
    where slowdowns can occur, so careful consideration should be made about your
    application architecture to see where you can identify other bottlenecks not solved
    by simple autoscaling. One such example would be slow query performance on a database.
    However, for this chapter, we will be focusing on application problems that can
    be solved by autoscaling in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the structure of an HPA to understand a bit better:'
  prefs: []
  type: TYPE_NORMAL
- en: with_autoscaler.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find the full code at this link: [https://packt.live/3bE9v28](https://packt.live/3bE9v28).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this spec, observe the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scaleTargetRef`: This is the reference to the object that is being scaled.
    In this case, it is a pointer to the Deployment of a shopping-cart microservice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minReplicas`: The minimum replicas in the Deployment, regardless of scaling triggers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxReplicas`: The maximum number of replicas in the Deployment, regardless
    of scaling triggers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`targetCPUUtilizationPercentage`: The goal percentage of average CPU utilization
    across all Pods in this deployment. Kubernetes will re-evaluate this metric constantly
    and increase and decrease the number of pods so that the actual average CPU utilization
    matches this target.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To simulate stress on our application, we will use **wrk**, because it is simple
    to configure and has a Docker container already made for us. wrk is an HTTP load-testing
    tool. It is simple to use and only has a few options; however, it will be able
    to generate large amounts of load by making requests over and over using multiple
    simultaneous HTTP connections against a specified endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find out more about wrk at this link: [https://github.com/wg/wrk](https://github.com/wg/wrk).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the following exercise, we will use a modified version of the application
    we''ve been running to help drive scaling behavior. In this revision of our application,
    we have modified it such that the application will perform a Fibonacci sequence
    calculation in a naïve way out to the 10,000,000th entry so that it will be slightly
    more computationally expensive and exceed our CPU autoscaling trigger. If you
    examine the source code, you can see that we have added this function:'
  prefs: []
  type: TYPE_NORMAL
- en: main.go
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find the full code at this link: [https://packt.live/3h5wCEd](https://packt.live/3h5wCEd).'
  prefs: []
  type: TYPE_NORMAL
- en: Other than this, we will be using an Ingress, which we learned about in *Chapter
    12*, *Your Application and HA*, and the same SQL database that we built in the
    previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now, with all of that said, let's dig into the implementation of these autoscalers
    in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 15.02: Scaling Workloads in Kubernetes'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we''re going to be putting together a few different pieces
    from before. Since our application has several moving parts at this point, we
    need to lay out some steps that we''re going to take so that you understand where
    we''re headed:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to have our EKS cluster set up as we have in *Exercise 12.02*, *Creating
    a Cluster with EKS Using Terraform*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to have the required components for the Kubernetes Metrics server set up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Considering these two points, you need to complete the previous exercise successfully
    to be able to perform this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: We need to install our counter application using a modification so that it will
    be a computationally intensive exercise to get the next number in a sequence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to install the HPA and set a metric target for the CPU percentage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to install the ClusterAutoscaler and give it the permissions to change
    the **Autoscaling Group** (**ASG**) size in AWS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to stress test our application by generating enough load to be able
    to scale the application out and cause the HPA to trigger a cluster-scaling action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will use a Kubernetes Ingress resource to load test using traffic external
    to our cluster so that we can create an even more realistic simulation.
  prefs: []
  type: TYPE_NORMAL
- en: 'After doing this, you''ll be a Kubernetes captain, so let''s dive in:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s deploy the `ingress-nginx` setup by running the following commands
    one after the other:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.17: Deploying the nginx Ingress controller'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.17: Deploying the nginx Ingress controller'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s fetch the manifest for our application with HA MySQL, Ingress,
    and an HPA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we apply it, let''s look at our autoscaling trigger:'
  prefs: []
  type: TYPE_NORMAL
- en: with_autoscaler.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The full code can be found at this link: [https://packt.live/3bE9v28](https://packt.live/3bE9v28).'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are starting with two replicas of this deployment and allowing ourselves
    to grow up to 1000 replicas while trying to keep the CPU at a constant 10% utilization.
    Recall from our Terraform template that we are using m4.large EC2 instances to
    run these Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s deploy this application by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.18: Deploying our application'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.18: Deploying our application'
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we are ready to load test. Before we begin, let''s check on the
    number of Pods in our deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This may take up to 5 minutes to show a percentage, after which you should
    see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.19: Getting details about our HPA'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.19: Getting details about our HPA'
  prefs: []
  type: TYPE_NORMAL
- en: The `Deployment pods:` field shows `2 current / 2 desired`, meaning our HPA
    has changed the desired replica count from 3 to 2 because we have a CPU utilization
    of 0%, which is below the target of 10%.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we need to get some load going. We're going to run a load test from our
    computer to the cluster using wrk as a Docker container. But first, we need to
    get the Ingress endpoint to access our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to first get your Ingress endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.20: Checking our Ingress endpoint'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.20: Checking our Ingress endpoint'
  prefs: []
  type: TYPE_NORMAL
- en: 'In another terminal session, run a `wrk` load test using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s quickly understand these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-t10`: The number of threads to use for this test, which is 10 in this case.'
  prefs: []
  type: TYPE_NORMAL
- en: '`-c1000`: The total number of connections to hold open. In this case, each
    thread is handling 1,000 connections each.'
  prefs: []
  type: TYPE_NORMAL
- en: '`-d600`: The number of seconds to run this test (which in this case is 600
    seconds or 10 minutes).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should get output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.21: Running a load test to our Ingress endpoint'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.21: Running a load test to our Ingress endpoint'
  prefs: []
  type: TYPE_NORMAL
- en: 'In another session, let''s keep an eye on the pods for our application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You should be able to see a response similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.22: Watching pods backing our application'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.22: Watching pods backing our application'
  prefs: []
  type: TYPE_NORMAL
- en: In this terminal window, you should see the number of Pods increasing. Note
    that we can also check the same in our Grafana dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Here, it is increased by 1; but soon, these pods will exceed all the available
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'In yet another terminal session, you can again set up port forwarding to Grafana
    to observe the dashboard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, access the dashboard on your browser at `localhost:3000`:![Figure 15.23:
    Observing our cluster in the Grafana dashboard'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B14870_15_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.23: Observing our cluster in the Grafana dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to see the number of Pods increasing here as well. Thus,
    we have successfully deployed an HPA that is automatically scaling up the number
    of Pods as the load on our application increases.
  prefs: []
  type: TYPE_NORMAL
- en: ClusterAutoscaler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the HPA ensures that there are always the right number of Pods running in
    a Deployment, then what happens when we run out of capacity on the cluster for
    all of those Pods? We need more of them, but we also don't want to be paying for
    that additional cluster capacity when we don't need it. This is where the ClusterAutoscaler
    comes in.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ClusterAutoscaler will work inside your cluster to ensure that the number
    of nodes running in the ASG (in the case of AWS) always has enough capacity to
    run the currently deployed application components of your cluster. So, if 10 pods
    in a Deployment can fit on 2 nodes, then when you need an 11th Pod, the ClusterAutoscaler
    will ask AWS to add a 3rd node to your Kubernetes cluster to get that Pod scheduled.
    When that Pod is no longer needed, that Node goes away, too. Let''s look at a
    brief architecture diagram to understand how the ClusterAutoscaler works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.24: Cluster with nodes at full capacity'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.24: Cluster with nodes at full capacity'
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this example, we have an EKS cluster running two worker nodes and
    all available cluster resources are taken up. So, here's what the ClusterAutoscaler
    does.
  prefs: []
  type: TYPE_NORMAL
- en: When a request for a Pod that won't fit arrives at the control plane, it remains
    in a `Pending` state. When the ClusterAutoscaler observes this, it will communicate
    with the AWS EC2 API and request for the ASG, which has our worker nodes deployed
    in them, to scale up by another node. This requires the ClusterAutoscaler to be
    able to communicate with the API for the cloud provider it is running in in order
    to change worker node count. In the case of AWS, this also means that we will
    either have to generate IAM credentials for the ClusterAutoscaler or allow it
    to use the IAM role of the machine to access the AWS APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'A successful scaling action should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.25: Additional node provisioned to run the additional pods'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.25: Additional node provisioned to run the additional pods'
  prefs: []
  type: TYPE_NORMAL
- en: We will implement the ClusterAutoscaler in the following exercise, and then
    load test it in the activity after that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 15.03: Configuring the ClusterAutoscaler'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So, now that we''ve seen our Kubernetes Deployment scale, it''s time to see
    it scale to the point where it needs to add more node capacity to the cluster.
    We will be continuing where the last lesson left off and run the exact same application
    and load test but let it run a little longer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a ClusterAutoscaler, first, we need to create an AWS IAM account
    and give it the permissions to manage our ASGs. Create a file called `permissions.json`
    with the following contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s run the following command to create an AWS IAM policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.26: Creating an AWS IAM policy'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.26: Creating an AWS IAM policy'
  prefs: []
  type: TYPE_NORMAL
- en: Note down the value of the `Arn:` field from the output that you get.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to create an IAM user and then attach a policy to it. First, let''s
    create the user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see this response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.27: Creating an IAM user to use our policy'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.27: Creating an IAM user to use our policy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s attach the IAM policy to the user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Use the ARN value that you obtained in *step 2*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need the secret access key for this IAM user. Run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get this response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.28: Fetching the secret access key for the created IAM user'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.28: Fetching the secret access key for the created IAM user'
  prefs: []
  type: TYPE_NORMAL
- en: In the output of this command, note `AccessKeyId` and `SecretAccessKey`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, get the manifest file for ClusterAutoscaler that we have provided:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to create a Kubernetes Secret to expose these credentials to the ClusterAutoscaler.
    Open the `cluster_autoscaler.yaml` file. In the first entry, you should see the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: cluster_autoscaler.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find the full code at this link: [https://packt.live/2DCDfzZ](https://packt.live/2DCDfzZ).'
  prefs: []
  type: TYPE_NORMAL
- en: You need to replace `YOUR_AWS_ACCESS_KEY_ID` and `YOUR_AWS_SECRET_ACCESS_KEY`
    with the Base64-encoded versions of the values returned by AWS in *step 5*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To encode in Base64 format, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Run this twice, using `AccessKeyID` and `SecretAccessKey` in place of `<YOUR_VALUE>`
    to get the corresponding Base64-encoded version that you need to enter into the
    secret fields. Here''s what it should look like when complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in the same `cluster_autoscaler.yaml` file, go to line 188\. You will
    need to replace the value of `YOUR_AWS_REGION` with the value of the region you
    deployed your EKS cluster into, such as `us-east-1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: cluster_autoscaler.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find the entire code at this link: [https://packt.live/2F8erkb](https://packt.live/2F8erkb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, apply this file by running the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.29: Deploying our ClusterAutoscaler'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.29: Deploying our ClusterAutoscaler'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we need to now modify our ASG in AWS to allow for a scale-up; otherwise,
    the ClusterAutoscaler will not attempt to add any nodes. To do this, we have provided
    a modified `main.tf` file that has only one line changed: `max_size = 5` (*line
    299*). This will allow the cluster to add up a maximum of five EC2 nodes to itself.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the same location where you downloaded the previous Terraform file,
    and then run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see this response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.30: Downloading the modified Terraform file'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.30: Downloading the modified Terraform file'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, apply the modifications to the Terraform file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that the changes are only applied to the ASG max capacity, and then
    type `yes` when prompted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.31: Applying our Terraform modifications'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_31.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.31: Applying our Terraform modifications'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will test this ClusterAutoscaler in the following activity. Hence, do not
    delete your cluster and API resources for now.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have deployed our ClusterAutoscaler and configured it to access
    the AWS API. Thus, we should be able to scale the number of nodes as required.
  prefs: []
  type: TYPE_NORMAL
- en: Let's proceed to the following activity, where we will load test our cluster.
    You should plan to do this activity as soon as possible in order to keep costs
    down.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 15.01: Autoscaling Our Cluster Using ClusterAutoscaler'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, we are going to run another load test and this time, we are
    going to run it for longer and observe the changes to the infrastructure as the
    cluster expands to meet demands. This activity should repeat the previous steps
    (as shown in *Exercise 15.02, Scaling Workloads in Kubernetes*) to run the load
    test but this time, it should be done with the ClusterAutoscaler installed so
    that when your cluster runs out of capacity for the Pods, it will scale the number
    of nodes to fit the new Pods. The goal of this is to see a load test increase
    the node count.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these guidelines to complete your activity:'
  prefs: []
  type: TYPE_NORMAL
- en: We will use the Grafana dashboard to observe the cluster metrics, paying close
    attention to the number of running Pods and the number of nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our HPA should be set up so that when our application receives more load, we
    can scale the number of Pods to meet the demand.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure that your ClusterAutoscaler has been successfully set up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To fulfill the three aforementioned requirements, you will need to have successfully
    completed all the exercises in this chapter. We will be using the resources created
    in those exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Run a load test, as shown in *step 2* of *Exercise 15.02*. You may choose a
    longer or more intense test if you wish.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By the end of this activity, you should be able to observe an increase in the
    number of nodes by describing the AWS ASG like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.32: Increase in the number of nodes observed'
  prefs: []
  type: TYPE_NORMAL
- en: by describing the AWS scaling group
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_32.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.32: Increase in the number of nodes observed by describing the AWS
    scaling group'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should also be able to observe the same in your Grafana dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.33: Increase in the number of nodes observed in the Grafana dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_15_33.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.33: Increase in the number of nodes observed in the Grafana dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to this activity can be found at the following address: [https://packt.live/304PEoD](https://packt.live/304PEoD).
    Make sure you delete the EKS cluster by running the command `terraform destroy`.'
  prefs: []
  type: TYPE_NORMAL
- en: Deleting Your Cluster Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the last chapter where we will use our EKS cluster. Hence, we recommend
    that you delete your cluster resources using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This should stop the billing for the EKS cluster that we created using Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's reflect a bit on how far we've come from *Chapter 11*, *Build Your Own
    HA Cluster*, when we started to talk about running Kubernetes in a highly available
    manner. We covered how to set up a production cluster that was secure in the cloud
    and created using infrastructure as code tools such as Terraform, as well as secured
    the workloads that it runs. We also looked at necessary modifications to our applications
    in order to scale them well—both for the stateful and stateless versions of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in this chapter, we looked at how we can extend the management of our
    application runtimes using data specifically when introducing Prometheus, Grafana,
    and the Kubernetes Metrics server. We then used that information to leverage the
    HPA and the ClusterAutoscaler so that we can rest assured that our cluster is
    always appropriately sized and ready to respond to spikes in demand automatically
    without having to pay for hardware that is overprovisioned.
  prefs: []
  type: TYPE_NORMAL
- en: In the following series of chapters, we will explore some advanced concepts
    in Kubernetes, starting with admission controllers in the next chapter.
  prefs: []
  type: TYPE_NORMAL
