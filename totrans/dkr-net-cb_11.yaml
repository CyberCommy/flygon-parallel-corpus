- en: Chapter 11. Troubleshooting Docker Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Using tcpdump to verify network paths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verifying VETH pairs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verifying published ports and outbound masquerading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verifying name resolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a test container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resetting the local Docker network database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we've seen in earlier chapters, Docker leverages a combination of relatively
    well-known Linux networking constructs to deliver container networking. Throughout
    this book, we've looked at many different ways you can configure, consume, and
    validate Docker networking configuration. What we haven't done is outline a troubleshooting
    and validation methodology you can use when you run into issues. When troubleshooting
    container networking, it is important to understand and be able to troubleshoot
    each specific networking component used in delivering end-to-end connectivity.
    The goal of this chapter is to provide specific steps you can take when you need
    to validate or troubleshoot a Docker networking issue.
  prefs: []
  type: TYPE_NORMAL
- en: Using tcpdump to verify network paths
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we glanced over its usage in previous chapters, anyone working with
    networking on a Linux-based system should be comfortable with `tcpdump`. `tcpdump`
    allows you to capture network traffic on one or more interfaces on the host. In
    this recipe, we'll walk through how we can use `tcpdump` to verify container network
    traffic in a couple of different Docker networking scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll be using a single Docker host. It is assumed that Docker
    is installed and in its default configuration. You''ll also need root-level access
    in order to inspect and change the hosts networking and firewall configuration.
    You''ll also need the `tcpdump` utility installed. If you don''t have it on your
    system, you can install it with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`tcpdump` is an amazing troubleshooting tool. When used properly, it can give
    you a detailed view of packets as they traverse interfaces on a Linux host. To
    demonstrate, let''s start a single container on our Docker host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since we didn't specify any network parameters, this container will run on the
    `docker0` bridge and have any exposed ports published to the hosts interfaces.
    Traffic generated from the container will also be hidden behind the hosts IP interfaces
    as the traffic heads toward the outside network. Using `tcpdump`, we can see this
    traffic at every stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first examine inbound traffic that''s coming into the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, this container was exposing port `80`, which has now been published
    to the host''s interfaces on port `32768`. Let''s first ensure that the traffic
    is coming into the host on the right port. To do this, we can capture on the hosts
    `eth0` interface for traffic destined to port `32768`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To use `tcpdump` to capture this inbound traffic, we used a couple of different
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`q`: This tells `tcpdump` to be quiet, or not generate as much output. Since
    we really only want to see the layer 3 and layer 4 information this cleans up
    the output quite nicely'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nn`: This tells `tcpdump` not to attempt to resolve IPs to DNS names. Again,
    we want to see the IP address here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`i`: This specifies the interface we want to capture on, in this case, `eth0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src port`: Tell `tcpdump` to filter on traffic that has a destination port
    of `32768`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `dst` parameter could be removed from this command. Doing so would filter
    on any traffic with a port of `32768` thus showing you the entire flow including
    the return traffic.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding code, we can see the host receiving traffic on its
    physical interface (`10.10.10.101`) on port `32768` coming from a remote source
    of `10.20.30.41`. In this case, `10.20.30.41` is a test server that is originating
    traffic toward the container's published port.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve seen the traffic get to the host, let''s look at it as it traverses
    the `docker0` bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we can see the traffic by just filtering on the `docker0` bridge
    interface. As expected, we see the same traffic, with the same source, but now
    reflecting the accurate destination IP and port of the service running in the
    container thanks to the published port functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this is certainly the easiest means to capture traffic, it''s not very
    effective if you have multiple containers running on the `docker0` bridge. The
    current filtering would provide you all of the traffic traversing the bridge rather
    than just the specific container you were looking for. In these cases, you can
    also specify the IP address in the filter like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We're specifying the destination IP as a filter here. If we wished to see both
    traffic source and destined to that IP address, we could replace `dst` with `host`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This sort of packet capture is essential to validating that features like port
    publication are working as expected. Captures can be done on the majority of interface
    types including those that don''t have an IP address associated with them. A good
    example of such an interface is the host side of a VETH pair used to connect a
    containers namespace back to the default namespace. When troubleshooting container
    connectivity, it might be handy to be able to correlate traffic arriving on the
    `docker0` bridge with a specific host-side VETH interface. We can do this by correlating
    data from multiple places. For instance, assume that we do the following `tcpdump`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that in this case we passed the `e` parameter to `tcpdump`. This tells
    `tcpdump` to display the source and destination MAC address for each frame. In
    this case, we can see that we have two MAC addresses. One of these will be the
    MAC address associated with the `docker0` bridge, and the other will be the MAC
    address associated with the container. We can look at the `docker0` bridge information
    to determine what its MAC address is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This leaves the address `02:42:ac:11:00:02`. Using the bridge command that
    comes as part of the `iproute2` toolset, we can determine on which interface this
    MAC address lives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that the MAC address of the container is accessible through
    the interface named `vetha431055`. Doing a capture on that interface will confirm
    that we''re looking at the right interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`tcpdump` can be a vital tool in verifying container communication. It is wise
    to spend some time understanding the tool and the different ways you can filter
    on traffic using its different parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Verifying VETH pairs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of all the Linux network constructs we've reviewed in this book, VETH pairs
    are likely the most essential. Being namespace aware they allow you to connect
    a container in a unique namespace to any other namespace including the default.
    And while Docker handles all of this for you, it is useful to be able to determine
    where the ends of a VETH pair live and correlate them to determine what purpose
    a VETH pair is serving. In this recipe, we'll review in depth how to find and
    correlate the ends of a VETH pair.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we'll be using a single Docker host. It is assumed that Docker
    is installed and in its default configuration. You'll also need root-level access
    in order to inspect and change the hosts networking and firewall configuration.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main use case for VETH pairs in Docker is to connect a containers network
    namespace back to the default network namespace. It does this by placing one of
    the VETH pair on the `docker0` bridge and the other end in the container. The
    container side of the VETH pair gets an IP address assigned to it and then renamed
    to `eth0`.
  prefs: []
  type: TYPE_NORMAL
- en: When looking to match up ends of a VETH pair for a container, there are two
    scenarios. The first is when you start with the end in the default namespace,
    and the second is when you start the end in the container namespace. Let's walk
    through each case and how to correlate them together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first start with knowing the host end of the interface. For instance,
    let''s say we''re looking for the container end of this interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a couple things to point out here. First, passing the `-d` parameter
    to the `ip link` subcommand displays extra detail about the interface. In this
    case, it confirms that the interface is a VETH pair. Second, VETH pair naming
    generally follows the `<end1>@<end2>` naming convention. In this case, we can
    see that the end `vetha431055` is the local interface and the `if5` is the other
    end. `if5` stands for interface 5 or the index ID of the 5th interface on the
    host. Since VETH interfaces are always created in pairs, it''s fair to assume
    that the end of this VETH pair with index 6 is very likely index 5 or 7\. In this
    case, the naming is indicating that it''s 5, but we can confirm that using the
    `ethtool` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the other end of this VETH pair has an interface index of 5
    as the name indicated. Now finding which container has 5 is the hard part. To
    do this, we need to inspect each container for a specific interface number. If
    you''re running a lot of containers, this can be a challenge. Instead of inspecting
    each container manually, you can loop through them using Linux `xargs`. For instance,
    look at this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'What we''re doing here is returning a list of the container IDs for all running
    containers and then passing that list to `xargs`. In turn, `xargs` is using those
    container IDs to run a command inside the container with `docker exec`. That command
    happens to be the `ip link` command, which will return a list of all interfaces
    and their associated index numbers. If any of that information returned starts
    with a `5:`, indicating an interface index of 5, we''ll print it to the screen.
    In order to see which container has the interface in question, we have to run
    the `xargs` command in verbose mode (`--verb`), which will show us each command
    as it runs. The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there were six containers running on this host. We didn't find
    the interface ID we were looking for until the last container. Given the container
    ID, we can tell which container has the other end of the VETH interface.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You could confirm this by running the `docker exec -it` `ea32565ece0c ip link`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s try another example of starting with the container end of the VETH
    pair. This is slightly easier since the naming of the interface tells us the index
    of the host-side matching interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then validate that the interface on the host with index 6 is a match
    to the interface with an index 5 in the container by once again using the `ethtool`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Verifying published ports and outbound masquerading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the more difficult pieces involved in Docker networking is `iptables`.
    The `iptables`/netfilter integration plays a key role in providing functionality
    like port publication and outbound masquerading. However, `iptables` can be difficult
    to understand and troubleshoot if you're not already familiar with it. In this
    recipe, we'll review how to examine the `iptables` configuration in detail and
    verify that connectivity is working as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we'll be using a single Docker host. It is assumed that Docker
    is installed and in its default configuration. You'll also need root-level access
    in order to inspect the `iptables` rule set.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we've seen in earlier chapters, Docker does an outstanding job of managing
    host firewall rules on your behalf. There will likely be very few instances in
    which you need to view or modify the `iptables` rules as they relate to Docker.
    However, it's always a good idea to be able to validate the configuration to rule
    out `iptables` as a possible issue when you're troubleshooting container networking.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate walking through the `iptables` rule set, we''ll examine an example
    container that''s publishing a port. The steps we perform to do this are easily
    transferable to examining rules for any other Docker-integrated `iptables` use
    cases. To do this, we''ll run a simple container that exposes port `80` for publishing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we told Docker to publish any exposed ports, we know that this container
    should have its exposed port of `80` published to the host. To verify that the
    port is actually being published, we can check the `iptables` rule set. The first
    thing we''d want to do is to make sure that the destination NAT required for port
    publication is in place. To examine an `iptables` table, we can use the `iptables`
    command and pass the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n`: Tells `iptables` to use numeric information in the output for things such
    as addresses and ports'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`L`: Tells `iptables` that you want to output a list of rules'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`v`: Tells `iptables` to provide verbose output, so we can see all of the rule
    information as well as rule counters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t`: Tells `iptables` to only show information from a specific table'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Putting that all together, we can use the command `sudo iptables –nL –t nat`
    to view the rules in the NAT table of the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that all the default table and chain policies we'll examine in this recipe
    are `ACCEPT`. If the default chain policy is `ACCEPT`, it means that even if we
    don't get a rule match, the flow will still be allowed. Docker will create the
    rules regardless of what the default policy is set to.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re not comfortable with `iptables`, interpreting this output can be
    a bit daunting. Even though we''re looking at the NAT table, we need to know what
    chain is being processed for inbound communication to the host. In our case, since
    the traffic is coming into the host, the chain we''re interested with is the `PREROUTING`
    chain. Let''s walk through how the table is processed:'
  prefs: []
  type: TYPE_NORMAL
- en: The first line in the `PREROUTING` chain looks for traffic destined for `LOCAL`
    or the host itself. Since the traffic is destined to an IP address on one of the
    host's interfaces, we match on this rule and perform the action that references
    jumping to a new chain named `DOCKER`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the `DOCKER` chain, we hit the first rule that is looking for traffic coming
    into the `docker0` bridge. Since this traffic isn't coming into the `docker0`
    bridge, the rule is passed over and we move to the next rule in the chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second rule in the `DOCKER` chain is looking for traffic that's not coming
    into the `docker0` bridge and has a destination port of TCP `32768`. We match
    this rule and perform the action to perform a destination NAT to `172.17.0.2`
    port `80`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The processing in the table looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The arrows in the preceding image indicate the traffic flow as the traffic traverses
    the NAT table. In this example, we only have one container running on the host,
    so it's pretty easy to see which rules are being processed.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can couple this sort of output with the `watch` command to get a live output
    of the counters, for instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''ve traversed the NAT table, the next thing we need to worry about
    is the filter table. We can view the filter table in much the same way that we
    viewed the NAT table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'At first glance, we can see that this table is laid out slightly different
    than the NAT table was. For instance, we have different chains in this table than
    we did with the NAT table. In our case, the chain we''re interested in for inbound
    published port communication would be the forward chain. This is because the host
    is forwarding, or routing, the traffic to the container. The traffic will traverse
    this table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first line in the forward chain sends the traffic directly to the `DOCKER-ISOLATION`
    chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, the only rule in the `DOCKER-ISOLATION` chain is a rule to send
    the traffic back, so we resume reviewing rules in the `FORWARD` table.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second rule in the forward table says that if the traffic is going out the
    `docker0` bridge to send the traffic to the `DOCKER` chain. Since our destination
    (`172.17.0.20`) lives out the `docker0` bridge, we match on this rule and jump
    to the `DOCKER` chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the `DOCKER` chain, we inspect the first rule and determine that it's looking
    for traffic that is destined to the container IP address on port TCP `80` and
    is going out, but not in, the `docker0` bridge. We match on this rule and the
    flow is accepted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The processing in the table looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Passing the filter table is the last step published port traffic has to take
    in order to reach the container. However, we've now only reached the container.
    We still need to account for the return traffic from the container back to the
    host talking to the published port. So now, we need to talk about how traffic
    originated from the container is handled by `iptables`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first table we''ll encounter with outbound traffic is the filter table.
    Traffic originating from the container will once again use the forward chain of
    the filter table. The flow would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: The first line in the forward chain sends the traffic directly to the `DOCKER-ISOLATION`
    chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, the only rule in the `DOCKER-ISOLATION` chain is a rule to send
    the traffic back, so we resume reviewing rules in the FORWARD table.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second rule in the forward table says that if the traffic is going from
    the `docker0` bridge, send the traffic to the `DOCKER` chain. Since our traffic
    is going into the `docker0` bridge rather than out, this rule is passed over and
    we move to the next rule in the chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third rule in the forward table says that if the traffic is going out from
    the `docker0` bridge and its connection state is `RELATED` or `ESTABLISHED` that
    the flow should be accepted. This traffic is going into the `docker0` bridge,
    so we won't match this rule either. However, it is worth pointing out that this
    rule is used to allow return traffic for flows initiated from the container. It's
    just not hit as part of the initial outbound connection since that represents
    a new flow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fourth rule in the forward table says that if the traffic is going in the
    `docker0` bridge, but not out the `docker0` bridge, to accept it. Because our
    traffic is going into the `docker0` bridge, we match on this rule and the traffic
    is accepted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The processing in the table looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next table we''d hit for outbound traffic is the NAT table. This time,
    we want to look at the `POSTROUTING` chain. In this case, we match the first rule
    of the chain which is looking for traffic that is not going out the `docker0`
    bridge and is sourced from the `docker0` bridge subnet (`172.17.0.0/16`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_11_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The action for this rule is to `MASQUERADE`, which will hide the traffic behind
    one of the hosts interfaces based on the hosts routing table.
  prefs: []
  type: TYPE_NORMAL
- en: Taking this same approach, you can easily validate other `iptables` flows related
    to Docker. Granted, as the number of containers scale, this becomes a harder task.
    However, since the majority of rules are written on a per container basis, the
    hit counters will be unique to each container, making it easier to narrow the
    scope.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on the order in which `iptables` tables and chains are
    processed, take a look at this `iptables` web page and the associated flow charts
    at [http://www.iptables.info/en/structure-of-iptables.html](http://www.iptables.info/en/structure-of-iptables.html).
  prefs: []
  type: TYPE_NORMAL
- en: Verifying name resolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DNS resolution for containers has always been rather straightforward. The container
    received the same DNS configuration as the host. However, with the advent of user-defined
    networks and the embedded DNS server, this has now become a little trickier. A
    common problem in many of the DNS issues I've seen is not understanding how the
    embedded DNS server works and how to validate that it's working correctly. In
    this recipe, we'll step through a container DNS configuration to validate which
    DNS server it is using to resolve specific namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we'll be using a single Docker host. It is assumed that Docker
    is installed and in its default configuration. You'll also need root-level access
    in order to inspect and change the host's networking and firewall configuration.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The standard DNS configuration for Docker without user-defined networks is
    to simply copy the DNS configuration from the host into the container. In these
    cases, the DNS resolution is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In these cases, all DNS requests will go straight to the defined DNS server.
    This means that our container can resolve any DNS records that our host can:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Coupled with the fact that Docker will masquerade this traffic to the IP address
    of the host itself makes this a simple and easily maintainable solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this gets a little trickier when we start using user-defined networks.
    This is because user-defined networks provide for container name resolution. That
    is, one container can resolve the name of another container without the use of
    static or manual host file entries and linking. This is a great feature, but it
    can cause some confusion if you don''t understand how the container receives its
    DNS configuration. For instance, let''s now create a new user-defined network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now start a new container named `web2` on this network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if we connect our existing `web1` container to this bridge, we should find
    that `web1` can resolve the container `web2` by name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The problem here is that in order to facilitate this, Docker had to change
    the DNS configuration of the `web1` container. In doing so, it injects the embedded
    DNS server in the middle of a containers DNS request. So before, when we were
    talking directly to the hosts DNS server, we are now talking to the embedded DNS
    server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This is required for DNS resolution to containers to work, but it has an interesting
    side effect. The embedded DNS server reads the host's `/etc/resolv.conf` file
    and uses any name servers defined in that file as forwarders for the embedded
    DNS server. The net effect of this is that you don't notice the embedded DNS server
    since it's still forwarding requests it can't answer to the host's DNS server.
    However, it only programs these forwarders if they are defined. If they don't
    exist or are set to `127.0.0.1`, then Docker programs the forwarders to be Google's
    public DNS server (`8.8.8.8` and `8.4.4.4`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Although this makes good sense, there are rare circumstances in which your
    local DNS server happens to be `127.0.0.1`. For instance, you happen to be running
    some type of local DNS resolver on the same host or using a DNS forwarder application
    such as **DNSMasq**. In those cases, there are some complications that can be
    caused by Docker forwarding the container''s DNS requests off to the aforementioned
    external DNS servers instead of the one locally defined. Namely, internal DNS
    zones will no longer be resolvable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This can also cause general resolution issues because it is common to block
    DNS traffic to external DNS servers preferring instead to force internal endpoints
    to use internal DNS servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In these scenarios, there are a couple of ways to address this. You can either
    run the container with a specific DNS server by passing the DNS flag at container
    runtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Otherwise, you can set the DNS server at the Docker service level, which the
    embedded DNS server will then use as the forwarder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In either case, if you're having container resolution issues, always check and
    see what the container has configured in its `/etc/resolv.conf` file. If it's
    `127.0.0.11`, that indicates you're using the Docker embedded DNS server. If you
    are, and you're still having issues, make sure that you validate the host DNS
    configuration to determine what the embedded DNS server is consuming for a forwarder.
    If there isn't one defined or it's `127.0.0.1`, then make sure that you tell the
    Docker service what DNS server it should be passing to containers in one of the
    two ways defined earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Building a test container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the tenants of building Docker containers is to keep them small and lean.
    In some cases, this can limit your troubleshooting options as the containers won't
    have many of the common Linux networking tools as part of their image. While not
    ideal, it is sometimes nice to have a container image that has these tools installed
    so that you can troubleshoot the network from the container perspective. In this
    chapter, we'll review how to build a Docker image specifically for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we'll be using a single Docker network host. It is assumed that
    Docker is installed and in its default configuration. You'll also need root-level
    access in order to inspect and change the hosts networking and firewall configuration.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A Docker image is built by defining a Dockerfile. The Dockerfile defines what
    base image to use as well as commands to run inside of the container. In my example,
    I''ll define the Dockerfile as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The goal of this image is twofold. First, I wanted to be able to run the container
    in detached mode and have it offer a service. This would allow me to define the
    container and verify that things such as port publishing were working off the
    host. This container image provides me with a known good container that will publish
    a service on port `80`. For this purpose, we're using Apache to host a simple
    index page.
  prefs: []
  type: TYPE_NORMAL
- en: 'The index file is pulled into the image at build time and can be customized
    by you. I use a simple HTML page, `index.html`, that shows big red font such as
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Second, the image has a lot of network tools installed as part of the image.
    You''ll notice that I''m installing the following packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`net-tools`: This provides network utilities to view and configure interfaces'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inetutils-ping`: This provides ping functionality'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`curl`: This is to pull files from other network endpoints'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dnsutils`: This is to resolve DNS names and other DNS tracing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ethtool`: This is to get information and stats from interfaces'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tcpdump`: This is to do packet capture from within the container'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you define this Dockerfile, as well as it''s required supporting files (an
    index page), you can build the image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a lot of options you can define when building an image. Take a look
    at `docker build --help` for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Docker will then process the Dockerfile and, if successful, it will generate
    a `docker image` file, which you can then push to your container registry of choice
    for consumption on other hosts with `docker pull`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once built, you can run it and verify that the tools are working as expected.
    Having `ethtool` within the container means that we can easily determine the host-side
    VETH end of the VETH pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also perform local `tcpdump` actions to verify traffic reaching the
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As your use cases change, you can modify the Dockerfile to make it more specific
    to your own use cases. Being able to troubleshoot from within the container can
    be a big help when diagnosing connectivity issues.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This image is just an example. There are many ways that this can be made more
    lightweight. I decided to use Ubuntu as the base image just for the sake of familiarity.
    The image described earlier is fairly heavy because of this.
  prefs: []
  type: TYPE_NORMAL
- en: Resetting the local Docker network database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the advent of user-defined networks, users are able to define custom network
    types for their containers. Once defined, these networks persist through system
    reboots until they are deleted by an administrator. In order for this persistence
    to work, Docker needs some place to store information related to your user-defined
    networks. The answer is a database file that's local to the host. In some rare
    cases, this database can get out of sync with the current state of the containers
    on the host or become corrupt. This can cause issues related to deleting containers,
    deleting networks, and starting the Docker service. In this recipe, we'll show
    you how to remove the database to restore Docker back to its default network configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we'll be using a single Docker network host. It is assumed that
    Docker is installed and in its default configuration. You'll also need root-level
    access in order to inspect and change the host's networking and firewall configuration.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker stores information related to user-defined networks in a database stored
    on the local host. This database is written to when networks are defined and read
    from when the service starts. In the rare case that this database gets out of
    sync or becomes corrupt, you can delete the database and restart the Docker service
    in order to reset the Docker user-defined networks and restore the three default
    network types (bridge, host, and none).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Warning: Deleting this database deletes any and all Docker user-defined networks
    on the host. It is wise to only do this as a last resort and if you have the capability
    of recreating the networks that were previously defined. All other troubleshooting
    options should be pursued before attempting this and you should create a backup
    of the file before deleting it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The database is named `local-kv.db` and is stored in the path `/var/lib/network/files/`.
    Accessing and or deleting the file requires root-level access. For the purpose
    of this example, we''ll switch to the root user in order to make browsing this
    protected directory easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'To demonstrate what happens when we delete this file, let''s first create a
    new user-defined network and attach a container to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now delete the file `local-db.kv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'While this has no immediate effect on the running container, it does prevent
    us from adding, removing, or starting new containers associated with this user-defined
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'After deleting the `boltdb` database file, `local-kv.db`, you''ll need to restart
    the Docker service in order for Docker to recreate it with the default settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the file is recreated, you''ll once again be able to create user-defined
    networks. However, any containers that were attached to previously configure user-defined
    network will now fail to start:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This is expected behavior since Docker still believes that the container should
    have an interface on that network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: To remedy this problem, you have two options. First, you can recreate the user-defined
    network named `mybridge` using the same configuration options from when it was
    initially provisioned. If this doesn't work, your only alternative is to delete
    the container and restart a new instance referencing a newly created or default
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There has been some discussion on GitHub of newer versions of Docker supporting
    a `--force` flag when using the `docker network disconnect` subcommand. In version
    1.10, this parameter exists, but still doesn't like that the user-defined network
    does not exist. If you're running a newer version, this might be worth trying
    as well.
  prefs: []
  type: TYPE_NORMAL
