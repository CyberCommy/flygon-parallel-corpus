- en: Bypassing Machine Learning Malware Detectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned that you can break into machine learning
    models and make them perform malicious activities by using adversarial machine
    learning techniques. In this chapter, we are going to explore further techniques,
    like how to fool artificial neural networks and deep learning networks. We are
    going to look at anti-malware system evasion as a case study.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to bypass next generation malware detectors with generative adversarial
    networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bypassing machine learning with reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code files for this chapter at [https://github.com/PacktPublishing/Mastering-Machine-Learning-for-Penetration-Testing/tree/master/Chapter09](https://github.com/PacktPublishing/Mastering-Machine-Learning-for-Penetration-Testing/tree/master/Chapter09).
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Information security professionals are doing their best to come up with novel
    techniques to detect malware and malicious software. One of the trending techniques
    is using the power of machine learning algorithms to detect malware. On the other
    hand, attackers and cyber criminals are also coming up with new approaches to
    bypass next-generation systems. In the previous chapter, we looked at how to attack
    machine learning models and how to bypass intrusion detection systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Malware developers use many techniques to bypass machine learning malware detectors.
    Previously, we explored an approach to build malware classifiers by training the
    system with grayscale image vectors. In a demonstration done by the **Search And
    RetrieVAl of Malware** (**SARVAM**) research unit, at the Vision Research Lab,
    UCSB, the researchers illustrated that, by changing a few bytes, a model can classify
    a malware as a goodware. This technique can be performed by attackers to bypass
    malware classifiers, through changing a few bytes and pixels. In the demonstration,
    the researchers used a variant of the NETSTAT program, which is a command-line
    network utility tool that displays network connections. In the following image,
    the left-hand side is a representation of the `NETSTAT.EXE` malware, and the second
    is detected as a goodware. As you can see, the difference between the two programs
    is unnoticeable (88 bytes out of 36,864 bytes: 0.78%), after converting the two
    types of files into grayscale images and checking the differences between the
    two of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00202.gif)'
  prefs: []
  type: TYPE_IMG
- en: This technique is just the beginning; in this chapter, we are going to dive
    deep into how we can trick them (the machine learning model, in our case the malware
    classifier) into performing malicious activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous chapter was an overview of adversarial machine learning. We learned
    how machine learning can be bypassed by attackers. In this chapter, we are going
    to go deeper, discovering how to bypass malware machine learning based detectors;
    before that, we are going to learn how to fool artificial neural networks and
    avoid deep learning networks with Python, open source libraries, and open source
    projects. Neural networks can be tricked by **adversarial samples**. Adversarial
    samples are used as inputs to the neural network, to influence the learning outcome.
    A pioneering research project, called *Explaining and Harnessing Adversarial Networks,* conducted
    by Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy (at Google), showed
    that a small amount of carefully constructed noise can fool the neural network
    into thinking that the entered image is an image of a gibbon and not a panda,
    with 99.3% confidence. The neural network originally thought that the provided
    image was a panda, with 57.7% confidence, which is true; but it is not the case
    in the second example, after fooling the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00203.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Many electronic devices and systems rely on deep learning as a protection mechanism,
    including face recognition; imagine what attackers can do to attack them and gain
    unauthorized access to critical systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s try to fool a neural network. We are going to fool a handwritten
    digit detector system by using the famous MNIST dataset. In [Chapter 4](part0081.html#2D7TI0-49a67f1d6e7843d3b2296f38e3fe05f5),
    *Malware Detection with Deep Learning*, we learned how to build one. For the demonstration,
    we are going to fool a pretrained neural network by Michael Nielsen. He used 50,000
    training images and 10,000 test images. Or, you can simply use your own neural
    network. You can find the training information in the GitHub repository of this
    chapter. The file is called `trained_network.pkl`; you will also find the MNIST
    file (`mnist.pkl.gz`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check whether the model is well-trained. Load the `pickle` file. Load
    the data with `pickle.load()`, and identify training,validation, and testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To check digit 2, for example, we are going to select `test_data[1][0]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot illustrates the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00204.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Plot the result to check further by using `matplotlib.pyplot (plt)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see we generated the digit **2** so the model was trained well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00205.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Everything is set up correctly. Now, we are going to attack the neural network
    with two types of attacks: **targeted** and **non-targeted.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a non-targeted attack, we are going to generate an adversarial sample and
    make the network give a certain output, for example, *6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00206.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'In this attack, we want the neural network to think that the image entered
    is *6*. The target image (let''s call it *X)* is a *784* dimensional vector, because
    the image dimension is *28×28* pixels. Our goal is to find a vector  `*⃗x*` that
    minimizes the cost *C,* resulting in an image that the neural network predicts
    as our goal label. The cost function, *C,* is defined as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00207.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code block is an implementation of a derivative function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To generate the adversarial sample, we need to set the goal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a random image for gradient descent initialization, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the gradient descent, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can generate the sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the adversarial sample, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00208.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'In targeted attacks, we use the same technique and the same code, but we add
    a new term to the cost function. So, it will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00209.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Foolbox
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Foolbox is a Python toolbox to benchmark the robustness of machine learning
    models. It is supported by many frameworks, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theano
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lasagne
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MXNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To install Foolbox, use the `pip` utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00210.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are some Foolbox attacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient-Based Attacks**: By linearizing the loss around an input, *x*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient Sign Attack (FGSM)**: By computing the gradient, *g(x0)*, once,
    and then seeking the minimum step size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative Gradient Attack**: By maximizing the loss along small steps in
    the gradient direction, *g(x)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative Gradient Sign Attack**: By maximizing the loss along small steps
    in the ascent direction, *sign(g(x))*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepFool L2Attack**: By computing, for each class, the minimum distance,
    *d(ℓ, ℓ0)*, that it takes to reach the class boundary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepFool L∞Attack**: Like L2Attack, but minimizes the *L∞-norm* instead'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jacobian-Based Saliency Map Attack**: By computing a saliency score for each
    input feature'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single Pixel Attack**: By setting a single pixel to white or black'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To implement an attack with Foolbox, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If you receive the error, `ImportError('`load_weights` requires h5py.')`, solve
    it by installing the **h5py** library (`pip install h5py`).
  prefs: []
  type: TYPE_NORMAL
- en: 'To plot the result, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00211.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Deep-pwning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep-pwning is a lightweight framework for experimenting with machine learning
    models, with the goal of evaluating their robustness against a motivated adversary.
    It is called the **metasploit of machine learning**. You can clone it from the
    GitHub repository at [https://github.com/cchio/deep-pwning](https://github.com/cchio/deep-pwning).
  prefs: []
  type: TYPE_NORMAL
- en: 'Don''t forget to install all of the requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the Python libraries required to work with Deep-pwning:'
  prefs: []
  type: TYPE_NORMAL
- en: Tensorflow 0.8.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib >= 1.5.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numpy >= 1.11.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pandas >= 0.18.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Six >= 1.10.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EvadeML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: EvadeML ([https://evademl.org](https://evademl.org/) ) is an evolutionary framework
    based on genetic programming, for automatically finding variants that evade detection
    by machine learning based malware classifiers. It was developed by the Machine
    Learning Group and the Security Research Group at the University of Virginia.
  prefs: []
  type: TYPE_NORMAL
- en: To download EvadeML, clone it from [https://github.com/uvasrg/EvadeML](https://github.com/uvasrg/EvadeML).
  prefs: []
  type: TYPE_NORMAL
- en: 'To install EvadeML, you need to install these required tools:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A modified version of pdfrw for parsing PDFs: [https://github.com/mzweilin/pdfrw](https://github.com/mzweilin/pdfrw)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cuckoo Sandbox v1.2, as the oracle: [https://github.com/cuckoosandbox/cuckoo/releases/tag/1.2](https://github.com/cuckoosandbox/cuckoo/releases/tag/1.2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The target classifier PDFrate-Mimicus: [https://github.com/srndic/mimicus](https://github.com/srndic/mimicus)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The target classifier Hidost: [https://github.com/srndic/hidost](https://github.com/srndic/hidost)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To configure the project, copy the template, and configure it with an editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Before running the main program, `./gp.py` , run the centralized detection
    agent with predefined malware signatures, as indicated in the documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Select several benign PDF files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: To add a new classifier to evade, just add a wrapper in `./classifiers/`.
  prefs: []
  type: TYPE_NORMAL
- en: Bypassing next generation malware detectors with generative adversarial networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2014, Ian Goodfellow, Yoshua Bengio, and their team, proposed a framework
    called the **generative adversarial network (GAN)**. Generative adversarial networks
    have the ability to generate images from a random noise. For example, we can train
    a generative network to generate images for handwritten digits from the MNIST
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative adversarial networks are composed of two major parts: a **generator**
    and a **discriminator**.'
  prefs: []
  type: TYPE_NORMAL
- en: The generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The generator takes latent samples as inputs; they are randomly generated numbers,
    and they are trained to generate images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00212.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, to generate a handwritten digit, the generator will be a fully
    connected network that takes latent samples and generates `784` data points, reshaping
    them into *28x28* pixel images (MNIST digits). It is highly recommended to use
    `tanh` as an activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The discriminator is simply a classifier trained with supervised learning techniques
    to check if the image is real (`1`) or fake (`0`). It is trained by both the MNIST
    dataset and the generator samples. The discriminator will classify the MNIST data
    as real, and the generator samples as fake:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'By connecting the two networks, the generator and the discriminator, we produce
    a generative adversarial network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a high-level representation of a generative adversarial network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00213.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To train the GAN, we need to train the generator (the discriminator is set
    as non-trainable in further steps); in the training, the back-propagation updates
    the generator''s weights to produce realistic images. So, to train a GAN, we use
    the following steps as a loop:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the discriminator with the real images (the discriminator is trainable
    here)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the discriminator as non-trainable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training loop will occur until both of the networks cannot be improved any
    further.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a GAN with Python, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00214.gif)'
  prefs: []
  type: TYPE_IMG
- en: To build a GAN with Python, we are going to use NumPy and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: MalGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To generate malware samples to attack machine learning models, attackers are
    now using GANs to achieve their goals. Using the same techniques we discussed
    previously (a generator and a discriminator), cyber criminals perform attacks
    against next-generation anti-malware systems, even without knowing the machine
    learning technique used (black box attacks). One of these techniques is MalGAN,
    which was presented in a research project called, *Generating Adversarial Malware
    Examples for Black Box Attacks Based on GAN,* conducted by Weiwei Hu and Ying
    Tan from the Key Laboratory of Machine Perception (MOE) and the Department of
    Machine Intelligence. The architecture of MalGAN is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00215.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The generator creates adversarial malware samples by taking malware (feature
    vector *m*) and a noise vector, *z,* as input. The substitute detector is a multilayer,
    feed-forward neural network, which takes a program feature vector, *X,* as input.
    It classifies the program between a benign program and malware.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the generative adversarial network, the researchers used this algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Many of the samples generated may not be valid PE files. To preserve mutations
    and formats, the systems required a sandbox to ensure that functionality was preserved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative adversarial network training cannot simply produce great results;
    that is why many hacks are needed to achieve better results. Some tricks were
    introduced by Soumith Chintala, Emily Denton, Martin Arjovsky, and Michael Mathieu,
    to obtain improved results:'
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing the images between *-1* and *1*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a max log, *D,* as a loss function, to optimize *G* instead of min (*log
    1-D*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling from a Gaussian distribution, instead of a uniform distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing different mini-batches for real and fake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding ReLU and MaxPool, and using LeakyReLU and Average Pooling instead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **Deep Convolutional GAN** (**DCGAN**), if possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `ADAM` optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bypassing machine learning with reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous technique, we noticed that if we are generating adversarial
    samples, especially if the outcomes are binaries, we will face some issues, including
    generating invalid samples. Information security researchers have come up with
    a new technique to bypass machine learning anti-malware systems with reinforcement
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Previously (especially in the first chapter), we explored the different machine
    learning models: supervised, semi-supervised, unsupervised, and reinforcement
    models. Reinforcement machine learning models are important approaches to building
    intelligent machines. In reinforcement learning, an agent learns through experience,
    by interacting with an environment; it chooses the best decision based on a state
    and a reward function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00216.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A famous example of reinforcement learning is the AI-based Atari Breakout.
    In this case, the environment includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The ball and the bricks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The moving paddle (left or right)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward for eliminating the bricks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure illustrates a high overview of the reinforcement model
    used to teach the model how to play Atari Breakout:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00217.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'With the Atari Breakout environment as an analogy to learn how to avoid anti-malware
    systems, our environment will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00218.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For the agent, it takes the environment state (general file information, header
    information, imported and exported functions, strings, and so on) to optimize
    its performance and  the reward input  from the antivirus reports, and result
    actions (creating entry points and new sections, modifying sections and so on).
    In other words, to perform and learn the agent is taking two inputs (States and
    rewards).
  prefs: []
  type: TYPE_NORMAL
- en: As an implementation of the concepts we've discussed, information security professionals
    worked on an OpenAI environment, to build a malware that can escape detection
    using reinforcement learning techniques. One of these environments is **Gym-malware**.
    This great environment was developed by endgame.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI gym contains an open source Python framework, developed by a nonprofit
    AI research company called OpenAI ([https://openai.com/](https://openai.com/))
    to develop and evaluate reinforcement learning algorithms. To install OpenAI Gym,
    use the following code (you''ll need to have Python 3.5+ installed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenAI Gym is loaded pre-made environments. You can check all of the available
    environments at [http://gym.openai.com/envs/](http://gym.openai.com/envs/):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00219.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the Gym-malware environment, you will need to install Python 3.6 and
    a library called Instrument Executable Formats, aptly named `LIEF`. You can add
    it by typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Download Gym-malware from [https://github.com/endgameinc/gym-malware](https://github.com/endgameinc/gym-malware). [Move
    the installed Gym-malware environment to `gym_malware/gym_malware/envs/utils/samples/`.](https://github.com/endgameinc/gym-malware)
  prefs: []
  type: TYPE_NORMAL
- en: 'To check whether you have the samples in the correct directory, type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The actions available in this environment are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`append_zero`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`append_random_ascii`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`append_random_bytes`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remove_signature`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upx_pack`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upx_unpack`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`change_section_names_from_list`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`change_section_names_to random`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modify_export`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remove_debug`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`break_optional_header_checksum`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we continued our journey of learning how to bypass machine
    learning models. In the previous chapter, we discovered adversarial machine learning;
    in this continuation, we explored adversarial deep learning and how to fool deep
    learning networks. We looked at some real-world cases to learn how to escape anti-malware
    systems by using state of the art techniques. In the next and last chapter, we
    are going to gain more knowledge, learning how to build robust models.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the components of generative adversarial networks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between a generator and a discriminator?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we make sure that the malware adversarial samples are still valid when
    we are generating them?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do a bit of research, then briefly explain how to detect adversarial samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What distinguishes reinforcement learning from deep learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between supervised and reinforcement learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does an agent learn in reinforcement learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following resources include a great deal of information:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Explaining and Harnessing Adversarial Samples*: [https://arxiv.org/pdf/1412.6572.pdf](https://arxiv.org/pdf/1412.6572.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Delving Into Transferable Adversarial Examples and Black Box Attacks*: [https://arxiv.org/pdf/1611.02770.pdf](https://arxiv.org/pdf/1611.02770.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Foolbox - a Python toolbox to benchmark the robustness of machine learning
    models*: [https://arxiv.org/pdf/1707.04131.pdf](https://arxiv.org/pdf/1707.04131.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Foolbox *GitHub: [https://github.com/bethgelab/foolbox](https://github.com/bethgelab/foolbox)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN*: [https://arxiv.org/pdf/1702.05983.pdf](https://arxiv.org/pdf/1702.05983.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Malware Images: Visualization and Automatic Classification*: [https://arxiv.org/pdf/1702.05983.pdf](https://arxiv.org/pdf/1702.05983.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SARVAM: Search And RetrieVAl of Malware*: [http://vision.ece.ucsb.edu/sites/vision.ece.ucsb.edu/files/publications/2013_sarvam_ngmad_0.pdf](http://vision.ece.ucsb.edu/sites/vision.ece.ucsb.edu/files/publications/2013_sarvam_ngmad_0.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SigMal: A Static Signal Processing Based Malware Triage*: [http://vision.ece.ucsb.edu/publications/view_abstract.cgi?416](http://vision.ece.ucsb.edu/publications/view_abstract.cgi?416)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
