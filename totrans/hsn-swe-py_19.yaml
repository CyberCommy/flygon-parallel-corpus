- en: Multiprocessing and HPC in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**High-performance computing** (**HPC**), quite simply, is the use of parallel
    processing during the execution of an application to spread the computational
    load across multiple processors, often across multiple machines. There are several
    MPC strategies to choose from, ranging from custom applications that leverage
    local multiprocessor computer architecture through to dedicated MPC systems, such
    as Hadoop or Apache Spark.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore and apply different Python capabilities, building
    from executing a baseline algorithm against elements in a dataset one element
    at a time, and look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Building parallel processing approaches that exploit locally available multiprocessor
    architectures, and the limitations of those approaches using Python's `multiprocessing`
    module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining and implementing an approach across multiple machines to parallelize
    the baseline serial process—essentially creating a basic computational cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring how to use Python code in dedicated, industry-standard HPC clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common factors to consider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Code that executes in a parallel manner has a few additional factors to consider
    as it's being developed. The first consideration is the input to the program.
    If the primary operations against any set of data are wrapped in a function or
    method, then the data is handed off to the function. The function does whatever
    it needs to do, and control is handed back to the code where the function was
    called. In a parallel processing scenario, that same function might be called
    any number of times, with different data, with control passing back to the calling
    code in a different order than their execution started in. As the datasets get
    larger, or more processing power is made available to parallelize the function,
    more control has to be exerted over how that function is called, as well as when
    (under what circumstances), in order to reduce or eliminate that possibility.
    There may also be a need to control how much data is being worked on at any given
    time, if only to avoid overwhelming the machine that the code is running on.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of this scenario seems to be in order. Consider three calls to the
    same function, all within a few milliseconds, where the first and third call complete
    in one second, but the second call, for whatever reason, takes ten seconds. The
    order of calls to the function would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Call #1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Call #2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Call #3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The order that those return in, though, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Call #1 (in one second)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Call #3 (also in one second)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Call #2 (in *ten* seconds)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The potential concern is that if the returns from the function are expected
    to come back in the same order they were called, even if it''s only implicitly,
    with dependencies on Call #2 needed by Call #3, the expected data won''t be present,
    and Call #3 will fail, probably in a very confusing manner.'
  prefs: []
  type: TYPE_NORMAL
- en: This collection of controls over the input data, and as a result over when,
    how, and how often the parallelized process is executed, has several names, but
    we'll use the term orchestration here. Orchestration can take many forms, from
    simple loops over a small dataset, launching parallel processes for each element
    in the dataset, to large-scale, over-the-wire message-based process request-and-response
    mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: The output from a set of parallel processes also has to be considered in some
    detail. Some of the parallelization methods available in Python simply do not
    allow the results of a function call to be directly returned to the calling code
    (at least not yet). Others may allow it, but only when the active process is complete
    and the code actively attaches to the process, blocking access to any other processes
    until the targeted one has completed. One of the more common strategies for dealing
    with output is to create the processes to be parallelized so that they are fire-and-forget
    calls—calling the function deals with the actual processing of the data and with
    sending the results to some common destination. Destinations can include multiprocess-aware
    queues (provided by the multiprocessing module as a `Queue` class), writing data
    to files, storing the results to a database, or sending some sort of asynchronous
    message to somewhere that stores the results independent of the orchestration
    or execution of those processes. There may be several different terms for these
    processes, but we'll use dispatch in our exploration here. Dispatch may also be
    controlled to some extent by whatever orchestration processes are in play, or
    might have their own independent orchestration, depending on the complexity of
    the processes.
  prefs: []
  type: TYPE_NORMAL
- en: The processes themselves, and any post-dispatch use of their results, also need
    to be given some additional thought, at least potentially. Since the goal, ultimately,
    is to have some number of independent processes working on multiple elements of
    the dataset at the same time, and there is no sure way to anticipate how long
    any individual process might take to complete, there is a very real possibility
    that two or more processes will resolve and dispatch their data at different rates.
    That may be true even if the expected runtime for the relevant data elements is
    the same. There is no guarantee, then, for any given sequence of elements to be
    processed, that the results will be dispatched in the same sequence that the processes
    against those elements were started. This is particularly true in distributed
    processing architectures, since the individual machines that are actually doing
    the work may have other programs consuming their available CPU cycles, memory,
    or other resources that are needed to run the process.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping the processes and the dispatch of their results independent, as much
    as possible, will go a long way toward mitigating that particular concern. Independent
    processes won't interact with or depend on any other processes, eliminating any
    potential for cross-process conflicts, and independent dispatches eliminate the
    potential for cross-results data contamination. If there is a need for processes
    that have dependencies, those can still be implemented, but additional effort
    (most likely in the form of dispatch-focused orchestration) may be needed to prevent
    conflicts from arising as results from parallel processes become available.
  prefs: []
  type: TYPE_NORMAL
- en: A simple but expensive algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to solve a problem. In order to keep the focus on the various
    mechanisms for parallel processing, that domain of that problem needs to be easily
    understood. At the same time, it needs to allow for processing of arbitrarily
    large datasets, preferably with unpredictable runtimes per element in the dataset,
    and with results that are unpredictable. To that end, the problem we''re going
    to solve is determining all of the factors of every number in some range of integer
    values. That is, for any given positive integer value, `x`, we want to be able
    to calculate and return a list of all the integer values that `x` is evenly divisible
    by. The function to calculate and return the list of factors for a single number
    (`factors_of`) is relatively simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Although this function, by itself, only deals with a single number, a process
    that calls it over and over again for any set of numbers can be scaled out to
    any number of numbers to process, giving us the arbitrarily large dataset capabilities
    when needed. The runtimes are somewhat predictable—it should be possible to get
    a reasonable runtime estimate for numbers across various ranges, though they will
    vary based on how large the number is. If a truly unpredictable runtime simulation
    is needed, we'd be able to pre-generate the list of numbers to be processed, then
    randomly select them, one at a time. Finally, the results on a number-by-number
    basis aren't predictable.
  prefs: []
  type: TYPE_NORMAL
- en: Some testing setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It may be useful to capture some runtime information for a sample set of numbers,
    say from `10,000,000` to `10,001,000`, capturing both the total runtime and the
    average time per number. A simple script (`serial_baseline.py`), executing the
    `factors_of` function against each of those numbers one at a time (serially),
    is easily assembled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming that any/all machines involved in the calculation processes are essentially
    identical in terms of processing power, the output from this script gives a reasonable
    estimate for how long it takes to perform the `factors_of` calculation against
    a number near a value of `10,000,000`. The output from a fairly new powerful laptop,
    where this code was initially tested, looked like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ea12fc7a-a77f-4e4f-84b3-4a5e5f7c2d43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For testing purposes further down the line, we''ll also create a constant list
    of test numbers (`TEST_NUMBERS`), chosen to provide a fairly wide range of processing
    times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: These seven numbers were chosen to provide a good range of larger and smaller
    numbers, varying the individual runtimes for calls of the `factors_of` function.
    Since there are only seven numbers, any test runs that make use of them (instead
    of the 1,000 numbers used in the preceding code) will take substantially less
    time to execute, while still providing some insight into the individual runtimes
    if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Local parallel processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The primary focus for the local parallelization of processing will be on the
    `multiprocessing` module. There are a couple of other modules that might be usable
    for some parallelization efforts (and those will be discussed later), but `multiprocessing`
    provides the best combination of flexibility and power with the least potential
    for restrictions from the Python interpreter or other OS-level interference.
  prefs: []
  type: TYPE_NORMAL
- en: As might be expected from the module's name, `multiprocessing` provides a class
    (`Process`) that facilitates the creation of child processes. It also provides
    a number of other classes that can be used to make working with child processes
    easier, including `Queue` (a multiprocess-aware queue implementation that can
    be used as a data destination), and `Value` and `Array`, which allow single and
    multiple values (of a single type) to be stored in a memory space that is shared
    across multiple processes, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full life cycle of a `Process` object involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating the `Process` object, defining what function or method will be executed
    when it is started, and any arguments that should be passed to it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Starting the `Process`, which begins its execution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Joining the `Process`, which waits for the process to complete, blocking further
    execution from the calling process until it is complete
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For comparison purposes, a multiprocessing-based baseline timing test script,
    equivalent to the `serial_baseline.py` script, was created. The significant differences
    between the two scripts start with the import of the multiprocessing module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Because there are multiple processes being created, and because they will need
    to be polled after all of them have been created, we create a list of `processes`,
    and append each new `process` as it''s created. As the process objects are being
    created, we''re specifying a `name` as well—that has no bearing or impact on the
    functionality, but does make things a bit more convenient for display purposes,
    should it be needed in testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As soon as `process.start()` is called for each `process`, it launches and
    runs in the background until it''s complete. The individual processes don''t terminate
    once they''re complete, though: that happens when `process.join()` is called and
    the process that has been joined has completed. Since we want all of the processes
    to start executing before joining to any of them (which blocks the continuation
    of the loop), we handle all of the joins separately—which also gives every process
    that has started some time to run until they''re complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from this test script on the same machine that the previous script
    was run on, and with the same programs running in the background, shows some significant
    improvement in the raw runtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d1227bae-c30a-45fa-b940-83ab838083b0.png)'
  prefs: []
  type: TYPE_IMG
- en: This is an improvement, even without any sort of orchestration driving it other
    than whatever is managed by the underlying OS (it just throws the same 1,000 numbers
    at `Process` instances that call the `factors_of` function): the total runtime
    is about 55% of the time that the serial processing took.
  prefs: []
  type: TYPE_NORMAL
- en: Why only 55%? Why not 25%, or at least close to that? Without some sort of orchestration
    to control how many processes were being run, this created a 1,000 processes,
    with all the attendant overhead at the operating system level, and had to give
    time to each of them in turn, so there was a lot of context shifting going on.
    A more carefully tuned orchestration process should be able to reduce that runtime
    more, but might not reduce it by much.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step toward a useful multiprocessing solution would be to actually
    be able to retrieve the results of the child process operations. In order to provide
    some visibility into what''s actually happening, we''re going to print several
    items through the process as well. We''ll also randomize the sequence of test
    numbers so that each run will execute them in a different order, which will (often)
    show how the processes are interwoven:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re going to use `TEST_NUMBERS` we set up earlier, and randomly arrange
    them into a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to actually capture the results, we''ll need somewhere that they can
    be sent when they are calculated: an instance of `multiprocessing.Queue`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The resultant `queue` object, as noted earlier, lives in memory that is shared
    by and accessible to the top-level process (the `multiprocessing_tests.py` script)
    and by all of the child `Process` objects' processes when they execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we''re going to be storing results in the `queue` object as they are
    calculated, we need to modify the `factors_of` function to handle that. We''ll
    also add in some `print()` calls to display when the function is called, and when
    it''s done with its work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The type and value checking remains unchanged:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The actual calculation of the factors of `number` remains unchanged, though
    we''re assigning the results to a variable instead of returning them so that we
    can deal with them differently as the function completes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of returning the calculated values, we''re going to use `queue.put()`
    to add them to the results that `queue` is keeping track of. The `queue` object
    doesn''t particularly care what data gets added to it—any object will be accepted—but
    for consistency''s sake, and to assure that each result that gets sent back has
    both the number and the factors of that number, we''ll `put` a `tuple` with both
    of those values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'With all of that prepared, we can start the main body of the test script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to keep track of the starting time for the calculation of the runtime
    later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Creating and starting the processes that call `factors_of` is the same basic
    structure that we used earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we have a set of started but possibly incomplete child processes
    running in the background. If the first few that were created and started were
    for the smaller numbers, they may have already completed, and are just waiting
    for a `join()` to finish their execution and terminate. If, on the other hand,
    one of the *larger* numbers was the first to be executed against, that first child
    process may well still be running for some time, while the others, with shorter
    individual runtimes, may be idling in the background, waiting for a `join()`.
    In any event, we can simply iterate over the list of process items, and `join()`
    each one in turn until they''re all done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Once all of the `join()` calls have completed, the `queue` will have all of
    the results for all of the numbers, in an arbitrary order. The heavy lifting of
    the child processes is all complete, so we can calculate the final runtime and
    show the relevant information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Actually accessing the results, which in this case is just for display purposes,
    requires calling the `get` method of the queue object—each `get` call fetches
    and removes one item that was put into the queue earlier, and for now we can simply
    print `queue.get()` until the `queue` is empty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'There are several noteworthy items that appear in the results of the test run,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2b828037-44b1-4a8f-b54b-9f2546cfa4cd.png)'
  prefs: []
  type: TYPE_IMG
- en: All of the lines that begin with `==>` show where the calls to the `factors_of`
    function occurred during the run. Unsurprisingly, they are all near the beginning
    of the process. The lines beginning with `***` show where the processes were joined—one
    of which happened in the middle of a run of `Process` creation events. Lines beginning
    with `<==` show where the `factors_of` calls were completed, after which they
    remained idle until the corresponding `process.join()` was called.
  prefs: []
  type: TYPE_NORMAL
- en: The randomized sequence of test numbers, judging by the calls to `factors_of`,
    was `11, 101, 102`, `1000000001`, `16`, `1000001`, and `1001`. The sequence of
    calls completed was 11, `101`, `102`, `16`, `1001`, `1000001`, and `100000000`—a
    slightly different sequence, and the *joins* sequence (and thus the sequence of
    the final **results**) was slightly different from that as well. All of these
    confirm that the various processes were starting, executing, and completing independently
    of the main process (the `for number in TEST_NUMBERS` loop).
  prefs: []
  type: TYPE_NORMAL
- en: 'With the `Queue` instance in place, and a way established for accessing the
    results of the child processes, that''s everything really needed for basic local
    multiprocess-based parallelization. There are a few things that could be tweaked
    or enhanced, if there were functional needs for them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If throttling of the number of active child processes were needed, or any finer
    control over how or when they were created, started, and joined, a more structured Orchestrator
    of some sort could be constructed:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of processes allowed could be limited based on the number of available
    CPUs on the machine, which can be retrieved with `multiprocessing.cpu_count()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of how the number of processes allowed was determined, limiting the
    number of active processes could be managed in several ways, including a `Queue`
    for pending requests, another for results, and a third for requests that were
    ready to be joined. Overriding each `Queue` object's `put` so that it would check
    the other queues' status, and trigger whatever actions/code was appropriate in
    those other queues, could allow a single queue to control the entire process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orchestration functionality could, itself, be wrapped in a `Process`, as could
    whatever data handling was needed after the dispatch of the child process data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The multiprocessing module also provides other object types that might prove
    useful for certain multiprocessing scenarios, including the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `multiprocessing.pool.Pool` class—objects that provide/control a pool of
    Worker processes to which jobs can be submitted, with support for asynchronous
    results, timeouts and callbacks, and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A variety of manager-object options that provide ways to create data that can
    be shared between different processes—including sharing over a network between
    processes running on different machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python has another local parallelization library—`thread`. The `thread` objects
    it provides are created and used in much the same way that `multiprocessing.Process`
    objects are, but thread-based processes run in the same memory space as the parent
    process, while `Process` objects, when they are started, actually create a new
    Python interpreter instance (with some connection capabilities to the parent Python
    interpreter).
  prefs: []
  type: TYPE_NORMAL
- en: Because threads run in the same interpreter and memory space, they are not capable
    of accessing multiple processors the same way that a `Process` can.
  prefs: []
  type: TYPE_NORMAL
- en: A thread's access to multiple CPUs on a machine is a function of the Python
    interpreter that's used to run the code. The standard interpreter that ships with
    Python (Cpython) and the alternative PyPy interpreter both share this limitation.
    IronPython, an interpreter that runs under/in the .NET framework, and Jython,
    which runs in a Java runtime environment, do not have that limitation.
  prefs: []
  type: TYPE_NORMAL
- en: Thread-based parallelization is also far more likely to encounter conflicts
    with Python's **global interpreter lock** (**GIL**). The GIL actively prevents
    multiple threads from executing or altering the same Python bytecode at the same
    time. There are some potentially long-running processes that happen outside the
    GIL's control—I/O, networking, some image processing functionality, and various
    libraries such as NumPy—but outside those exceptions, any multithreaded Python
    program that spends a lot of its execution time interpreting or manipulating Python
    bytecode will eventually hit a GIL bottleneck, losing its parallelization in the
    process.
  prefs: []
  type: TYPE_NORMAL
- en: More information about the GIL, why it exists, what it does, and so on, can
    be found on the Python wiki at [https://wiki.python.org/moin/GlobalInterpreterLock](https://wiki.python.org/moin/GlobalInterpreterLock).
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing across multiple machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another common parallelization strategy is to spread the workload of computational
    processes across multiple machines (physical or virtual). Where local parallelization
    is limited, ultimately, by the number of CPUs, or the number of cores, or the
    combination of both on a single machine, machine-level parallelization is limited
    by the number of machines that can be thrown at a problem. In this day and age,
    with immense reservoirs of virtual machines able to be made available in public
    clouds and private data centers, it's relatively easy to scale the number of available
    machines to match the computational needs of a problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic design for this kind of horizontally scalable solution is more complicated
    than the design for a local solution—it has to accomplish the same tasks, but
    separate the ability to do those tasks so that they can be made available on any
    number of machines, and provide mechanisms for executing processes and accepting
    the results from the remote tasks as they complete. In order to be reasonably
    fault-tolerant, there also needs to be more visibility into the status of the
    remote process machines, and those, in turn, have to be proactive about sending
    notifications to the central controller when something occurs that will disrupt
    their ability to do their jobs. A typical logical architecture, at a high level,
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d8c021c1-d206-4497-858d-fe2755a42279.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: The Orchestrator is a process running on one machine that is responsible for
    taking bits of the Process dataset, and sending them to the next available Worker.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It also keeps track of what Worker nodes are available, and probably what each Worker's
    capacity is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to accomplish that, the Orchestrator would have to be capable of registering
    and unregistering Worker nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Orchestrator should probably also keep track of the general health/availability
    of each of its Worker nodes, and be able to associate tasks with those nodes—if
    one becomes unavailable, and still has pending tasks, it can then reassign those
    tasks to other, available Worker nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each Worker node is a process running on an individual machine that, while running,
    accepts process instructions in incoming message items, executes the process(es)
    necessary to generate the results, and sends a results message to the Dispatcher
    when complete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each Worker node would also have to announce to the Orchestrator when it becomes
    available, in order to be registered, and when it is shutting down normally so
    that the Orchestrator could unregister it accordingly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If processing an incoming message wasn't possible because of an error, a Worker
    should also be able to relay that information back to the Orchestrator, allowing
    it to reassign the task to another Worker when it can.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Dispatcher is a process running on one machine that is responsible for accepting
    result message data, and doing whatever needs to be done with it—storing it in
    a database, writing it to a file, and so on. The Dispatcher could, conceivably,
    be the same machine, or even the same process as the Orchestrator—so long as dispatch-related
    message items get handled appropriately and without bogging down the orchestration
    processes, where it lives is a matter of preference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The basic structure of this kind of system could be implemented with the code
    that was already shown in [Chapter 16](9e235ce2-5611-4e7d-a16b-3332561fe85b.xhtml),
    *The Artisan Gateway Service*:'
  prefs: []
  type: TYPE_NORMAL
- en: The Orchestrator and Worker nodes could be implemented as a daemon, similar
    to `ArtisanGatewayDaemon`. If it were determined that the Dispatcher needed to
    be independent, it, too, could be a similar daemon.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The messaging between them could be handled with a variant of `DaemonMessage`
    objects, providing the same signed message security, transmitted over a RabbitMQ
    message system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That message transmission process could leverage the `RabbitMQSender` class
    that was already defined (also from [Chapter 16](9e235ce2-5611-4e7d-a16b-3332561fe85b.xhtml),
    *The Artisan Gateway Service*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A complete implementation of this approach is outside the scope of this book,
    but the critical aspects of it can be examined in enough detail to write an implementation
    if the reader so desires.
  prefs: []
  type: TYPE_NORMAL
- en: Common functionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The existing `DaemonMessage` class would need to be altered or overridden to
    accept different operations at the Orchestrator, Worker, and Dispatcher levels,
    creating new `namedtuple` constants that are applicable for each. Initially, the
    Worker node would only be concerned with accepting calls to its `factors_of` method,
    and its allowed operations would reflect this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding change to the setter method for the operation property could
    use the appropriate `namedtuple` constant to control accepted values (for example,
    replacing `_OPERATIONS` with `WORKER_OPERATIONS`, in some fashion, for a Worker
    node''s implementation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, all three components would potentially need to know about all the
    possible `origin` values, in order to be able to assign message origins appropriately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `main` method of any of the individual daemons would remain essentially
    unchanged from how `ArtisanGatewayDaemon` implemented it.
  prefs: []
  type: TYPE_NORMAL
- en: In this approach, there are only a few distinct variations of a few class members
    for each of the daemon classes (Worker Node, Orchestrator, and Dispatcher), but
    they are worth noting because of their distinct nature. The bulk of the differences
    is in the `_handle_message` methods of each daemon class, and each would have
    to implement its own instance methods for the operations they map process requests
    to as well.
  prefs: []
  type: TYPE_NORMAL
- en: The Worker nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All of the operations that were defined in the previous section for a hypothetical
    Worker daemon would have to be handled in the class'' `_handle_message` method—to
    start with, that''s nothing more than the `factors_of` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation of the `factors_of` method would not be substantially different
    from the original `factors_of` function, as defined at the beginning of this chapter,
    except that it would have to send a results message to the Dispatcher''s message
    queue rather than returning a value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The Worker node daemons, which need to notify the Orchestrator when they become
    available and are becoming unavailable, can do so in their `preflight` and `cleanup`
    methods, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: They would also have to implement the `dispatcher_queue`, `worker_id`, and `orchestrator_queue`
    properties that these methods use, providing a unique identifier of the worker
    node (which could be as simple as a random `UUID`) and the common Orchestrator
    and Dispatcher queue names (probably from a configuration file that's common to
    all Worker instances).
  prefs: []
  type: TYPE_NORMAL
- en: The Orchestrator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Orchestrator would be concerned with registration, unregistration, and
    pulse operations (allowing the Workers to send messages to the Orchestrator, essentially
    saying "I''m still alive"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The Orchestrator''s `_handle_message` would have to map each operation to the
    appropriate method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The Dispatcher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Initially, the Dispatcher, if it were an independent process and not folded
    into the Orchestrator, would be concerned with dispatch result operations only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Its `_handle_message` method would be constructed accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Integrating Python with large-scale, cluster computing frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Large-scale, cluster computing frameworks, in order to provide as much compatibility
    with custom written operations as possible, will probably accept input in only
    two different ways: as command-line arguments, or using standard input, with the
    latter being more common for systems that are targeted for big data operations.
    In either case, what''s needed to allow a custom process to be executed at and
    scaled to a clustered environment is a self-contained, command-line executable
    that usually returns its data to standard output.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A minimal script that accepts standard input—whether by passing data into it
    with a pipe, or by reading the contents of a file and using that—could be implemented
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Standard input is available through Python''s `sys` module as `sys.stdin`.
    It''s a file-like object, and can be both read and iterated over on a line-by-line
    basis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `factors_of` function should probably be included directly in the script
    code, if only so that the entire script is totally self-contained, and won''t
    require any custom software installation to be usable. For the sake of keeping
    the code shorter and easier to walk through, though, we''ll just import it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'If the script is executed directly—`python factors_stdin.py`—then we''ll actually
    execute the process, starting with acquiring all of the numbers from `stdin`.
    They may come in as multiple lines, each of which could have multiple numbers,
    so the first step is to extract all of them so that we end up with one list of
    numbers to process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'With all of the numbers ready, we can iterate over them, convert each value
    from the string value that was in the input into an actual `int`, and process
    them. If a value in the input can''t be converted to an `int`, we''ll simply skip
    it for now, though depending on the calling cluster framework, there may be specific
    ways to handle—or at least log—any bad values as errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The script can be tested by echoing a list of numbers, and piping that into
    `python factors_stdin.py`. The results are printed, one result per line, which
    would be accepted by a calling program as standard output, ready to be passed
    to some other process that accepted standard input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/86edbbca-29df-41ae-a84a-1510adbec05b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the source numbers are in a file (`hugos_numbers.txt`, in the chapter code),
    those can be used just as easily, and generate the same results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d218df9b-3663-413d-88e0-8db471ab13be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the cluster environment expects command-line arguments to be passed, a script
    can be written to accommodate that as well. It starts with much the same code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Where it deviates is in acquiring the numbers to be processed. Since they are
    passed as command-line values, they will be part of the `argv` list (another item
    provided by Python''s `sys` module), after the script name. The balance of this
    process is identical to the `stdin` based script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, as with the previous script, is simply printed to the console,
    and would be accepted as standard input by any other processes that it was handed
    off to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ed182f45-e7e2-4955-8673-7123a4fdb861.png)'
  prefs: []
  type: TYPE_IMG
- en: Python, Hadoop, and Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's likely that the most common or popular of the large-scale, cluster computing
    frameworks available is Hadoop. Hadoop is a collection of software that provides
    cluster computing capabilities across networked computers, as well as a distributed
    storage mechanism that can be thought of as a network-accessible filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Among the utilities it provides is Hadoop Streaming ([https://hadoop.apache.org/docs/r1.2.1/streaming.html](https://hadoop.apache.org/docs/r1.2.1/streaming.html)),
    which allows for the creation and execution of Map/Reduce jobs using any executable
    or script as a mapper and/or reducer. Hadoop's operational model, at least for
    processes that can use Streaming, is file-centric, so processes written in Python
    and executed under Hadoop will tend to fall into the `stdin` based category that
    we discussed earlier more often than not.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark is another option in the large-scale, cluster computing frameworks
    arena. Spark is a distributed, general-purpose framework, and has a Python API
    (`pyspark`, [http://spark.apache.org/docs/2.2.0/api/python/pyspark.html](http://spark.apache.org/docs/2.2.0/api/python/pyspark.html))
    available for installation with `pip`, allowing for more direct access to its
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered all of the basic permutations (serial and parallel,
    local and remote/distributed) of multiprocessing in Python, as it would apply
    to custom HPC operations. The basics needed for integrating a process written
    in Python to be executed by a large-scale cluster computing system such as Hadoop
    are quite basic—simple executable scripts—and the integration prospects with those
    system are as varied as the systems themselves.
  prefs: []
  type: TYPE_NORMAL
