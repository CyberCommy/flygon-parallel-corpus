- en: Markers and Parametrization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After learning the basics of writing and running tests, we will delve into
    two important pytest features: **marks** and **parametrization**.'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we will learn about marks, which allow us to selectively run tests
    based on applied marks, and to attach general data to test functions, which can
    be used by fixtures and plugins. In the same topic, we will take a look at built-in
    marks and what they have to offer.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, we will learn about **test parametrization**, which allows us to easily
    apply the same test function to a set of input values. This greatly avoids duplicating
    test code and makes it easy to add new test cases that may appear as our software
    evolves.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, here is what we will be covering in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Mark basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Built-in marks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parametrization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mark basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pytest allows you to mark functions and classes with metadata. This metadata
    can be used to selectively run tests, and is also available for fixtures and plugins,
    to perform different tasks. Let's take a look at how to create and apply marks
    to test functions, and later on jump into built-in pytest marks.
  prefs: []
  type: TYPE_NORMAL
- en: Creating marks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Marks are created with the `@pytest.mark` decorator. It works as a factory,
    so any access to it will automatically create a new mark and apply it to a function.
    This is easier to understand with an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: By using the `@pytest.mark.slow` decorator, you are applying a mark named `slow` to
    `test_long_computation`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Marks can also receive **parameters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `@pytest.mark.timeout` used in the last example was taken from the pytest-timeout
    plugin; for more details go to [https://pypi.org/project/pytest-timeout/](https://pypi.org/project/pytest-timeout/).
    With this, we define that `test_topology_sort` should not take more than 10 seconds,
    in which case it should be terminated using the `thread` method. Assigning arguments
    to marks is a very powerful feature, providing plugins and fixtures with a lot
    of flexibility. We will explore those capabilities and the `pytest-timeout` plugin in
    the next chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can add more than one mark to a test by applying the `@pytest.mark` decorator
    multiple times— for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are applying the same mark over and over, you can avoid repeating yourself
    by assigning it to a variable once and applying it over tests as needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If this mark is used in several tests, you can move it to a testing utility
    module and import it as needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Running tests based on marks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can run tests by using marks as selection factors with the `-m` flag. For
    example, to run all tests with the `slow` mark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `-m` flag also accepts expressions, so you can do a more advanced selection.
    To run all tests with the `slow` mark but not the tests with the `serial` mark
    you can use::'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The expression is limited to the `and`, `not`, and `or` operators.
  prefs: []
  type: TYPE_NORMAL
- en: Custom marks can be useful for optimizing test runs on your CI system. Oftentimes,
    environment problems, missing dependencies, or even some code committed by mistake
    might cause the entire test suite to fail. By using marks, you can choose a few
    tests that are fast and/or broad enough to detect problems in a good portion of
    the code and then run those first, before all the other tests. If any of those
    tests fail, we abort the job and avoid wasting potentially a lot of time by running
    all tests that are doomed to fail anyway.
  prefs: []
  type: TYPE_NORMAL
- en: We start by applying a custom mark to those tests. Any name will do, but a common
    name used is `smoke`, as in *smoke detector*, to detect problems before everything
    bursts into flames.
  prefs: []
  type: TYPE_NORMAL
- en: 'You then run smoke tests first, and only after they pass do, you run the complete
    test suite:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If any smoke test fails, you don't have to wait for the entire suite to finish
    to obtain this feedback.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can add to this technique by creating layers of tests, from simplest to
    slowest, creating a test hierarchy of sorts. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`smoke`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unittest`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`integration`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<all the rest>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This would then be executed like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Make sure to include the fourth step; otherwise, tests without marks will never
    run.
  prefs: []
  type: TYPE_NORMAL
- en: Using marks to differentiate tests in different pytest runs can also be used
    in other scenarios. For instance, when using the `pytest-xdist` plugin to run
    tests in parallel, we have a parallel session that executes most test suites in
    parallel but might decide to run a few tests in a separate pytest session serially
    because they are sensitive or problematic when executed together.
  prefs: []
  type: TYPE_NORMAL
- en: Applying marks to classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can apply the `@pytest.mark` decorator to a class. This will apply that
    same mark to all tests methods in that class, avoiding have to copy and paste
    the mark code over all test methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code is essentially the same as the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'However, there is one difference: applying the `@pytest.mark` decorator to
    a class means that all its subclasses will inherit the mark. Subclassing test
    classes is not common, but it is sometimes a useful technique to avoid repeating
    test code, or to ensure that implementations comply with a certain interface.
    We will see more examples of this later in this chapter and in [Chapter 4](bf8b3438-83e6-4ce5-9df4-4da086636ef7.xhtml),
    *Fixtures*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like test functions, decorators can be applied multiple times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Applying marks to modules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also apply a mark to all test functions and test classes in a module.
    Just declare a **global variable** named `pytestmark`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the equivalent to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use a `tuple` or `list` of marks to apply multiple marks as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Custom marks and pytest.ini
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Being able to declare new marks on the fly just by applying the `pytest.mark`
    decorator is convenient. It makes it a breeze to quickly start enjoying the benefits
    of using marks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This convenience comes at a price: it is possible for a user to make a typo
    in the mark name, for example `@pytest.mark.solw`, instead of `@pytest.mark.slow`.
    Depending on the project under testing, this typo might be a mere annoyance or
    a more serious problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s go back to our previous example, where a test suite is executed
    in layers on CI based on marked tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '`smoke`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unittest`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`integration`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<all the rest>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'A developer could make a typo while applying a mark to one of the tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This means the test will execute at the last step, instead of on the first step
    with the other `smoke` tests. Again, this might vary from a small nuisance to
    a terrible problem, depending on the test suite.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mature test suites that have a fixed set of marks might declare them in the
    `pytest.ini` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `markers` option accepts a list of markers in the form of `<name>: description`,
    with the description part being optional (`slow` and `serial` in the last example
    don''t have a description).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A full list of marks can be displayed by using the `--markers` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `--strict` flag makes it an error to use marks not declared in the `pytest.ini`
    file. Using our previous example with a typo, we now obtain an error, instead
    of pytest silently creating the mark when running with `--strict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Test suites that want to ensure that all marks are registered in `pytest.ini` should
    also use `addopts`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Built-in marks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having learned the basics of marks and how to use them, let's now take a look
    at some built-in pytest marks. This is not an exhaustive list of all built-in
    marks, but the more commonly used ones. Also, keep in mind that many plugins introduce
    other marks as well.
  prefs: []
  type: TYPE_NORMAL
- en: '@pytest.mark.skipif'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might have tests that should not be executed unless some condition is met.
    For example, some tests might depend on certain libraries that are not always
    installed, or a local database that might not be online, or are executed only
    on certain platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Pytest provides a built-in mark, `skipif`, that can be used to *skip* tests
    based on specific conditions. Skipped tests are not executed if the condition
    is true, and are not counted towards test suite failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you can use the `skipif` mark to always skip a test when executing
    on Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The first argument to `@pytest.mark.skipif` is the condition: in this example,
    we are telling pytest to skip this test in Windows. The `reason=` keyword argument
    is mandatory and is used to display why the test was skipped when using the `-ra`
    flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: It is good style to always write descriptive messages, including ticket numbers
    when applicable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can write the same condition as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The latter version checks whether the actual feature is available, instead of
    making assumptions based on the platform (Windows currently does not have an `os.fork`
    function, but perhaps in the future Windows might support the function). The same
    thing is common when testing features of libraries based on their version, instead
    of checking whether some functions exist. I suggest that so this reads: some functions
    exist. I suggest that when possible, prefer to check whether a function actually
    exists, instead of checking for a specific version of the library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Checking capabilities and features is usually a better approach, instead of
    checking platforms and library version numbers.The following is the full `@pytest.mark.skipif`
    signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: pytest.skip
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `@pytest.mark.skipif` decorator is very handy, but the mark must evaluate
    the condition at `import`/`collection` time, to determine whether the test should
    be skipped. We want to minimize test collection time, because, after all, we might
    end up not even executing all tests if the `-k` or `--lf` flags are being used,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, it might even be almost impossible (without some gruesome hack) to
    check whether a test should be skipped during import time. For example, you can
    make the decision to skip a test based on the capabilities of the graphics driver only
    after initializing the underlying graphics library, and initializing the graphics
    subsystem is definitely not something you want to do at import time.
  prefs: []
  type: TYPE_NORMAL
- en: 'For those cases, pytest lets you skip tests imperatively in the test body by
    using the `pytest.skip` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`pytest.skip` works by raising an internal exception, so it follows normal
    Python exception semantics, and nothing else needs to be done for the test to
    be skipped properly.'
  prefs: []
  type: TYPE_NORMAL
- en: pytest.importorskip
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is common for libraries to have tests that depend on a certain library being
    installed. For example, pytest's own test suite has some tests for `numpy` arrays,
    which should be skipped if `numpy` is not installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to handle this would be to manually try to import the library and skip
    the test if it is not present:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This can get old quickly, so for this reason, pytest provides the handy `pytest.importorskip`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`pytest.importorskip` will import the module and return the module object,
    or skip the test entirely if the module could not be imported.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If your test requires a minimum version of the library, `pytest.importorskip`
    also supports a `minversion` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '@pytest.mark.xfail'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can use `@pytest.mark.xfail` decorator to indicate that a test is *`expected
    to fail`*. As usual, we apply the mark decorator to the test function or method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This mark supports some parameters, all of which we will see later in this
    section; but one in particular warrants discussion now: the `strict` parameter.
    This parameter defines two distinct behaviors for the mark:'
  prefs: []
  type: TYPE_NORMAL
- en: With `strict=False` (the default), the test will be counted separately as an
    **XPASS** (if it passes) or an **XFAIL** (if it fails), and will **not fail the
    test suite**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With `strict=True`, the test will be marked as **XFAIL** if it fails, but if
    it unexpectedly passes, it will **fail the test suite**, as a normal failing test
    would
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But why would you want to write a test that you expect to fail anyway, and in
    which occasions is this useful? This might seem weird at first, but there are
    a few situations where this comes in handy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first situation is when a test always fails, and you want to be told (loudly)
    if it suddenly starts passing. This can happen when:'
  prefs: []
  type: TYPE_NORMAL
- en: You found out that the cause of a bug in your code is due to a problem in a
    third-party library. In this situation, you can write a failing test that demonstrates
    the problem, and mark it with `@pytest.mark.xfail(strict=True)`. If the test fails,
    the test will be marked as **XFAIL** in the test session summary, but if the test
    **passes**, it will **fail the test suite**. This test might start to pass when
    you upgrade the library that was causing the problem, and this will alert you
    that the issue has been fixed and needs your attention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have thought about a new feature, and design one or more test cases that
    exercise it even before your start implementing it. You can commit the tests with
    the `@pytest.mark.xfail(strict=True)` mark applied, and remove the mark from the
    tests as you code the new feature. This is very useful in a collaborative environment,
    where one person supplies tests on how they envision a new feature/API, and another
    person implements it based on the test cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You discover a bug in your application and write a test case demonstrating the
    problem. You might not have the time to tackle it right now or another person
    would be better suited to work in that part of the code. In this situation, marking
    the test as `@pytest.mark.xfail(strict=True)` would be a good approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All the cases above share one characteristic: you have a failing test and want
    to know whether it suddenly starts passing. In this case, the fact that the test
    passes warns you about a fact that requires your attention: a new version of a
    library with a bug fix has been released, part of a feature is now working as
    intended, or a known bug has been fixed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other situation where the `xfail` mark is useful is when you have tests
    that fail *sometimes*, also called **flaky** **tests**. Flaky tests are tests
    that fail on occasion, even if the underlying code has not changed. There are
    many reasons why tests fail in a way that appears to be random; the following
    are a few:'
  prefs: []
  type: TYPE_NORMAL
- en: Timing issues in multi threaded code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intermittent network connectivity problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tests that don't deal with asynchronous events correctly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relying on non-deterministic behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That is just to list a few possible causes. This non-determinism usually happens
    in tests with broader scopes, such as integration or UI. The fact is that you
    will almost always have to deal with flaky tests in large test suites.
  prefs: []
  type: TYPE_NORMAL
- en: Flaky tests are a serious problem, because the test suite is supposed to be
    an indicator that the code is working as intended and that it can detect real
    problems when they happen. Flaky tests destroy that image, because often developers
    will see flaky tests failing that don't have anything to do with recent code changes.
    When this becomes commonplace, people begin to just run the test suite again in
    the hope that this time the flaky test passes (and it often does), but this erodes
    the trust in the test suite as a whole, and brings frustration to the development
    team. You should treat flaky tests as a menace that should be contained and dealt
    with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some suggestions regarding how to deal with flaky tests within a development
    team:'
  prefs: []
  type: TYPE_NORMAL
- en: First, you need to be able to correctly identify flaky tests. If a test fails
    that apparently doesn't have anything to do with the recent changes, run the tests
    again. If the test that failed previously now `passes`, it means the test is flaky.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an issue to deal with that particular flaky test in your ticket system.
    Use a naming convention or other means to label that issue as related to a flaky
    test (for example GitHub or JIRA labels).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Apply the `@pytest.mark.xfail(reason="flaky test #123", strict=False)` mark,
    making sure to include the issue ticket number or identifier. Feel free to add
    more information to the description, if you like.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure to periodically assign issues about flaky tests to yourself or other
    team members (for example, during sprint planning). The idea is to take care of
    flaky tests at a comfortable pace, eventually reducing or eliminating them altogether.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These practices take care of two major problems: they allow you to avoid eroding
    trust in the test suite, by getting flaky tests out of the way of the development
    team, and they put a policy in place to deal with flaky tests in their due time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having covered the situations where the `xfail` mark is useful, let''s take
    a look at the full signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`condition`: the first parameter, if given, is a `True`/`False` condition,
    similar to the one used by `@pytest.mark.skipif`: if `False`, the `xfail` mark
    is ignored. It is useful to mark a test as `xfail` based on an external condition,
    such as the platform, Python version, library version, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`reason`: a string that will be shown in the short test summary when the `-ra`
    flag is used. It is highly recommended to always use this parameter to explain
    the reason why the test has been marked as `xfail` and/or include a ticket number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`raises`: given an exception type, it declares that we expect the test to raise
    an instance of that exception. If the test raises another type of exception (even
    `AssertionError`), the test will `fail` normally. It is especially useful for
    missing functionality or to test for known bugs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '`run`: if `False`, the test will not even be executed and will fail as XFAIL.
    This is particularly useful for tests that run code that might crash the test-suite
    process (for example, C/C++ extensions causing a segmentation fault due to a known
    problem).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '`strict`: if `True`, a passing test will fail the test suite. If `False`, the
    test will not fail the test suite regardless of the outcome (the default is `False`).
    This was discussed in detail at the start of this section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The configuration variable `xfail_strict` controls the default value of the
    `strict` parameter of `xfail` marks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Setting it to `True` means that all xfail-marked tests without an explicit `strict`
    parameter are considered an actual failure expectation instead of a flaky test.
    Any `xfail` mark that explicitly passes the `strict` parameter overrides the configuration
    value.
  prefs: []
  type: TYPE_NORMAL
- en: pytest.xfail
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, you can imperatively trigger an XFAIL result within a test by calling
    the `pytest.xfail` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Similar to `pytest.skip`, this is useful when you can only determine whether
    you need to mark a test as `xfail` at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Parametrization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common testing activity is passing multiple values to the same test function
    and asserting the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have an application that allows the user to define custom mathematical
    formulas, which will be parsed and evaluated at runtime. The formulas are given
    as strings, and can make use of mathematical functions such as `sin`, `cos`, `log`,
    and so on. A very simple way to implement this in Python would be to use the `eval`
    built-in ([https://docs.python.org/3/library/functions.html#eval](https://docs.python.org/3/library/functions.html#eval)),
    but because it can execute arbitrary code, we opt to use a custom tokenizer and
    evaluator for safety, instead..
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s not delve into the implementation details but rather focus on a test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Here, we create a `Tokenizer` class, which is used by our implementation to
    break the formula string into internal tokens for later processing. Then, we pass
    the formula string and tokenizer to `Formula.from_string`, to obtain a formula
    object. With the formula object on our hands, we pass the input values to `formula.eval` and
    check that the returned value matches our expectation.
  prefs: []
  type: TYPE_NORMAL
- en: But we also want to test other math operations, to ensure we are covering all
    the features of our `Formula` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'One approach is to expand our test by using multiple assertions to check other
    formulas and input values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This works, but if one of the assertions fails, the following assertions within
    the test function will not be executed. If there are multiple failures, we will
    have to run the test multiple times to see all of them and eventually fix all
    the issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see multiple failures per test run, we might decide to explicitly write
    a separate test for each assertion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: But now we are duplicating code all over the place, which will make maintenance
    more difficult. Suppose in the future `FormulaTokenizer` is updated to explicitly
    receive a list of functions that can be used in formulas. This means that we would
    have to update the creation of `FormulaTokenzier` in several places.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid repeating ourselves, we might decide to write this instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: This solves the problem of duplicating code, but now we are back to the initial
    problem of seeing only one failure at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Enter @pytest.mark.parametrize
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To solve all the above problems, pytest provides the much-loved `@pytest.mark.parametrize`
    mark. With this mark, you are able to provide a list of input values to the test,
    and pytest automatically generates multiple test functions for each input value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows this in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The `@pytest.mark.parametrize` mark automatically generates multiple test functions,
    parametrizing them with the arguments given to the mark. The call receives two
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`argnames`: a comma-separated string of argument names that will be passed
    to the test function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`argvalues`: a sequence of tuples, with each tuple generating a new test invocation.
    Each item in the tuple corresponds to an argument name, so the first tuple `("C0
    * x + 10", dict(x=1.0, C0=2.0), 12.0)` will generate a call to the test function
    with the arguments:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`formula` = `"C0 * x + 10"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs` = `dict(x=1.0, C0=2.0)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`expected` = `12.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using this mark, pytest will run `test_formula_parsing` three times, passing
    one set of arguments given by the `argvalues` parameter each time. It will also
    automatically generate a different node ID for each test, making it easy to distinguish
    between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: It is also important to note that the body of the function is just as compact
    as our starting test at the beginning of this section, but now we have multiple
    tests which allows us to see multiple failures when they happen.
  prefs: []
  type: TYPE_NORMAL
- en: Test parametrization not only avoids repeating test code and makes maintenance
    easier, it also invites you and the next developer who comes along to add more
    input values as the code matures. It encourages developers to cover more cases,
    because people are more eager to add a single line to the `argvalues` of a parametrized
    test than to copy and paste an entire new test to cover another input value.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, `@pytest.mark.parametrize` will make you cover more input cases,
    with very little overhead. It is definitely a very useful feature and should be
    used whenever you have multiple input values to be tested in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: Applying marks to value sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, in parametrized tests, you find the need to apply one or more marks to
    a set of parameters as you would to a normal test function. For example, you want
    to apply a `timeout` mark to one set of parameters because it takes too long to
    run, or `xfail` a set of parameters, because it has not been implemented yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'In those cases, use `pytest.param` to wrap the set of values and apply the
    marks you want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The signature of `pytest.param` is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '`*values` is the parameter set: `"hypot(x, y)", dict(x=3, y=4), 5.0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**kw` are options as keyword arguments: `marks=pytest.mark.xfail(reason="not
    implemented: #102")`. It accepts a single mark or a sequence of marks. There is
    another option, `ids`, which will be shown in the next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behind the scenes, every tuple of parameters passed to `@pytest.mark.parametrize`
    is converted to a `pytest.param` without extra options, so, for example,in the
    following the first code snippet is equivalent to the second code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Customizing test IDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'As we have seen, pytest automatically creates custom test IDs, based on the
    parameters used in a parametrized call. Running `pytest -v` will generate these
    test IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don''t like the automatically generated IDs, you can use `pytest.param`
    and the `id` option to customize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'This is also useful because it makes selecting tests significantly easier when
    using the `-k `flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'versus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Testing multiple implementations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well-designed systems usually make use of abstractions provided by interfaces,
    instead of being tied to specific implementations. This makes a system more resilient
    to future changes because, to extend it, you need to implement a new extension
    that complies with the expected interface, and integrate it into the system.
  prefs: []
  type: TYPE_NORMAL
- en: One challenge that often comes up is how to make sure existing implementations
    comply with all the details of a specific interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose our system needs to be able to serialize some internal
    classes into a text format to save and load to disk. The following are some of
    the internal classes in our system:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Quantity`: represents a value and a unit of measure. For example `Quantity(10,
    "m")` means *10 meters*. `Quantity` objects have addition, subtraction, and multiplication—basically,
    all the operators that you would expect from a native `float`, but taking the
    unit of measure into account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pipe`: represents a duct where some fluid can flow through. It has a `length`
    and `diameter`, both `Quantity` instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Initially, in our development, we only need to save those objects in **JSON**
    format, so we go ahead and implement a straightforward serializer class, that
    is able to serialize and de-serialize our classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we should write some tests to ensure everything is working:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: This works well and is a perfectly valid approach, given our requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some time passes, and new requirements arrive: now we need to serialize our
    objects into other formats, namely `XML` and [`YAML`(](http://yaml.org/)[http://yaml.org/](http://yaml.org/)[)](http://yaml.org/).
    To keep things simple, we create two new classes, `XMLSerializer` and `YAMLSerializer`,
    which implement the same `serialize`/`deserialize` methods. Because they comply
    with the same interface as `JSONSerializer`, we can use the new classes interchangeably
    in our system, which is great.'
  prefs: []
  type: TYPE_NORMAL
- en: But how do we test the different implementations?
  prefs: []
  type: TYPE_NORMAL
- en: 'A naive approach would be to loop over the different implementations inside
    each test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: This works, but it is not ideal, because we have to copy and paste the loop
    definition in each test, making it harder to maintain. Also, if one of the serializers
    fails, the next ones in the list will never be executed.
  prefs: []
  type: TYPE_NORMAL
- en: Another, horrible approach, would be to copy and paste the entire test functions,
    replacing the serializer class each time, but we won't be showing that here.
  prefs: []
  type: TYPE_NORMAL
- en: 'A much better solution is to use `@pytest.mark.parametrize` at class level.
    Observe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'With a small change, we have multiplied our existing tests to cover all the
    new implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The `@pytest.mark.parametrize` decorator also makes it very clear that new implementations
    should be added to the list and that all existing tests must pass. New tests added
    to the class also need to pass for all implementations.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, `@pytest.mark.parametrize` can be a very powerful tool to ensure
    that different implementations comply with the specifications of an interface.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use marks to organize our code and help us
    run the test suite in flexible ways. We then looked at how to use the `@pytest.mark.skipif`
    to conditionally skip tests, and how to use the `@pytest.mark.xfail` mark to deal
    with expected failures and flaky tests. Then we discussed ways of dealing with
    flaky tests in a collaborative environment. Finally, we discussed the benefits
    of using `@pytest.mark.parametrize` to avoid repeating our testing code and to
    make it easy for ourselves and others to add new input cases to existing tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will finally get to one of pytest''s most loved and
    powerful features: **fixtures**.'
  prefs: []
  type: TYPE_NORMAL
