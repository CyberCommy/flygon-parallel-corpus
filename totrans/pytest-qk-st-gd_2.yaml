- en: Markers and Parametrization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After learning the basics of writing and running tests, we will delve into
    two important pytest features: **marks** and **parametrization**.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we will learn about marks, which allow us to selectively run tests
    based on applied marks, and to attach general data to test functions, which can
    be used by fixtures and plugins. In the same topic, we will take a look at built-in
    marks and what they have to offer.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, we will learn about **test parametrization**, which allows us to easily
    apply the same test function to a set of input values. This greatly avoids duplicating
    test code and makes it easy to add new test cases that may appear as our software
    evolves.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, here is what we will be covering in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Mark basics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Built-in marks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parametrization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mark basics
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pytest allows you to mark functions and classes with metadata. This metadata
    can be used to selectively run tests, and is also available for fixtures and plugins,
    to perform different tasks. Let's take a look at how to create and apply marks
    to test functions, and later on jump into built-in pytest marks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Creating marks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Marks are created with the `@pytest.mark` decorator. It works as a factory,
    so any access to it will automatically create a new mark and apply it to a function.
    This is easier to understand with an example:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: By using the `@pytest.mark.slow` decorator, you are applying a mark named `slow` to
    `test_long_computation`.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Marks can also receive **parameters**:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `@pytest.mark.timeout` used in the last example was taken from the pytest-timeout
    plugin; for more details go to [https://pypi.org/project/pytest-timeout/](https://pypi.org/project/pytest-timeout/).
    With this, we define that `test_topology_sort` should not take more than 10 seconds,
    in which case it should be terminated using the `thread` method. Assigning arguments
    to marks is a very powerful feature, providing plugins and fixtures with a lot
    of flexibility. We will explore those capabilities and the `pytest-timeout` plugin in
    the next chapters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'You can add more than one mark to a test by applying the `@pytest.mark` decorator
    multiple times— for example:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you are applying the same mark over and over, you can avoid repeating yourself
    by assigning it to a variable once and applying it over tests as needed:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If this mark is used in several tests, you can move it to a testing utility
    module and import it as needed:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Running tests based on marks
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can run tests by using marks as selection factors with the `-m` flag. For
    example, to run all tests with the `slow` mark:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `-m` flag also accepts expressions, so you can do a more advanced selection.
    To run all tests with the `slow` mark but not the tests with the `serial` mark
    you can use::'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The expression is limited to the `and`, `not`, and `or` operators.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Custom marks can be useful for optimizing test runs on your CI system. Oftentimes,
    environment problems, missing dependencies, or even some code committed by mistake
    might cause the entire test suite to fail. By using marks, you can choose a few
    tests that are fast and/or broad enough to detect problems in a good portion of
    the code and then run those first, before all the other tests. If any of those
    tests fail, we abort the job and avoid wasting potentially a lot of time by running
    all tests that are doomed to fail anyway.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: We start by applying a custom mark to those tests. Any name will do, but a common
    name used is `smoke`, as in *smoke detector*, to detect problems before everything
    bursts into flames.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'You then run smoke tests first, and only after they pass do, you run the complete
    test suite:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If any smoke test fails, you don't have to wait for the entire suite to finish
    to obtain this feedback.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'You can add to this technique by creating layers of tests, from simplest to
    slowest, creating a test hierarchy of sorts. For example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '`smoke`'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unittest`'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`integration`'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<all the rest>`'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This would then be executed like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Make sure to include the fourth step; otherwise, tests without marks will never
    run.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Using marks to differentiate tests in different pytest runs can also be used
    in other scenarios. For instance, when using the `pytest-xdist` plugin to run
    tests in parallel, we have a parallel session that executes most test suites in
    parallel but might decide to run a few tests in a separate pytest session serially
    because they are sensitive or problematic when executed together.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Applying marks to classes
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can apply the `@pytest.mark` decorator to a class. This will apply that
    same mark to all tests methods in that class, avoiding have to copy and paste
    the mark code over all test methods:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The previous code is essentially the same as the following code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'However, there is one difference: applying the `@pytest.mark` decorator to
    a class means that all its subclasses will inherit the mark. Subclassing test
    classes is not common, but it is sometimes a useful technique to avoid repeating
    test code, or to ensure that implementations comply with a certain interface.
    We will see more examples of this later in this chapter and in [Chapter 4](bf8b3438-83e6-4ce5-9df4-4da086636ef7.xhtml),
    *Fixtures*.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Like test functions, decorators can be applied multiple times:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Applying marks to modules
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also apply a mark to all test functions and test classes in a module.
    Just declare a **global variable** named `pytestmark`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following is the equivalent to this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can use a `tuple` or `list` of marks to apply multiple marks as well:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Custom marks and pytest.ini
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Being able to declare new marks on the fly just by applying the `pytest.mark`
    decorator is convenient. It makes it a breeze to quickly start enjoying the benefits
    of using marks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'This convenience comes at a price: it is possible for a user to make a typo
    in the mark name, for example `@pytest.mark.solw`, instead of `@pytest.mark.slow`.
    Depending on the project under testing, this typo might be a mere annoyance or
    a more serious problem.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s go back to our previous example, where a test suite is executed
    in layers on CI based on marked tests:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '`smoke`'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unittest`'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`integration`'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<all the rest>`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'A developer could make a typo while applying a mark to one of the tests:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This means the test will execute at the last step, instead of on the first step
    with the other `smoke` tests. Again, this might vary from a small nuisance to
    a terrible problem, depending on the test suite.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Mature test suites that have a fixed set of marks might declare them in the
    `pytest.ini` file:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `markers` option accepts a list of markers in the form of `<name>: description`,
    with the description part being optional (`slow` and `serial` in the last example
    don''t have a description).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'A full list of marks can be displayed by using the `--markers` flag:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `--strict` flag makes it an error to use marks not declared in the `pytest.ini`
    file. Using our previous example with a typo, we now obtain an error, instead
    of pytest silently creating the mark when running with `--strict`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Test suites that want to ensure that all marks are registered in `pytest.ini` should
    also use `addopts`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Built-in marks
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having learned the basics of marks and how to use them, let's now take a look
    at some built-in pytest marks. This is not an exhaustive list of all built-in
    marks, but the more commonly used ones. Also, keep in mind that many plugins introduce
    other marks as well.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '@pytest.mark.skipif'
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might have tests that should not be executed unless some condition is met.
    For example, some tests might depend on certain libraries that are not always
    installed, or a local database that might not be online, or are executed only
    on certain platforms.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Pytest provides a built-in mark, `skipif`, that can be used to *skip* tests
    based on specific conditions. Skipped tests are not executed if the condition
    is true, and are not counted towards test suite failures.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you can use the `skipif` mark to always skip a test when executing
    on Windows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The first argument to `@pytest.mark.skipif` is the condition: in this example,
    we are telling pytest to skip this test in Windows. The `reason=` keyword argument
    is mandatory and is used to display why the test was skipped when using the `-ra`
    flag:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: It is good style to always write descriptive messages, including ticket numbers
    when applicable.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can write the same condition as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The latter version checks whether the actual feature is available, instead of
    making assumptions based on the platform (Windows currently does not have an `os.fork`
    function, but perhaps in the future Windows might support the function). The same
    thing is common when testing features of libraries based on their version, instead
    of checking whether some functions exist. I suggest that so this reads: some functions
    exist. I suggest that when possible, prefer to check whether a function actually
    exists, instead of checking for a specific version of the library.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'Checking capabilities and features is usually a better approach, instead of
    checking platforms and library version numbers.The following is the full `@pytest.mark.skipif`
    signature:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: pytest.skip
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `@pytest.mark.skipif` decorator is very handy, but the mark must evaluate
    the condition at `import`/`collection` time, to determine whether the test should
    be skipped. We want to minimize test collection time, because, after all, we might
    end up not even executing all tests if the `-k` or `--lf` flags are being used,
    for example.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, it might even be almost impossible (without some gruesome hack) to
    check whether a test should be skipped during import time. For example, you can
    make the decision to skip a test based on the capabilities of the graphics driver only
    after initializing the underlying graphics library, and initializing the graphics
    subsystem is definitely not something you want to do at import time.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'For those cases, pytest lets you skip tests imperatively in the test body by
    using the `pytest.skip` function:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`pytest.skip` works by raising an internal exception, so it follows normal
    Python exception semantics, and nothing else needs to be done for the test to
    be skipped properly.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: pytest.importorskip
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is common for libraries to have tests that depend on a certain library being
    installed. For example, pytest's own test suite has some tests for `numpy` arrays,
    which should be skipped if `numpy` is not installed.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to handle this would be to manually try to import the library and skip
    the test if it is not present:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This can get old quickly, so for this reason, pytest provides the handy `pytest.importorskip`
    function:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`pytest.importorskip` will import the module and return the module object,
    or skip the test entirely if the module could not be imported.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'If your test requires a minimum version of the library, `pytest.importorskip`
    also supports a `minversion` argument:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '@pytest.mark.xfail'
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can use `@pytest.mark.xfail` decorator to indicate that a test is *`expected
    to fail`*. As usual, we apply the mark decorator to the test function or method:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This mark supports some parameters, all of which we will see later in this
    section; but one in particular warrants discussion now: the `strict` parameter.
    This parameter defines two distinct behaviors for the mark:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: With `strict=False` (the default), the test will be counted separately as an
    **XPASS** (if it passes) or an **XFAIL** (if it fails), and will **not fail the
    test suite**
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With `strict=True`, the test will be marked as **XFAIL** if it fails, but if
    it unexpectedly passes, it will **fail the test suite**, as a normal failing test
    would
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But why would you want to write a test that you expect to fail anyway, and in
    which occasions is this useful? This might seem weird at first, but there are
    a few situations where this comes in handy.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'The first situation is when a test always fails, and you want to be told (loudly)
    if it suddenly starts passing. This can happen when:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: You found out that the cause of a bug in your code is due to a problem in a
    third-party library. In this situation, you can write a failing test that demonstrates
    the problem, and mark it with `@pytest.mark.xfail(strict=True)`. If the test fails,
    the test will be marked as **XFAIL** in the test session summary, but if the test
    **passes**, it will **fail the test suite**. This test might start to pass when
    you upgrade the library that was causing the problem, and this will alert you
    that the issue has been fixed and needs your attention.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你发现你的代码中的一个bug的原因是第三方库中的问题。在这种情况下，你可以编写一个演示问题的失败测试，并用`@pytest.mark.xfail(strict=True)`标记它。如果测试失败，测试将在测试会话摘要中标记为**XFAIL**，但如果测试**通过**，它将**失败测试套件**。当你升级导致问题的库时，这个测试可能会开始通过，这将提醒你问题已经解决，并需要你的注意。
- en: You have thought about a new feature, and design one or more test cases that
    exercise it even before your start implementing it. You can commit the tests with
    the `@pytest.mark.xfail(strict=True)` mark applied, and remove the mark from the
    tests as you code the new feature. This is very useful in a collaborative environment,
    where one person supplies tests on how they envision a new feature/API, and another
    person implements it based on the test cases.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你想到了一个新功能，并设计了一个或多个在你开始实施之前就对其进行测试的测试用例。你可以使用`@pytest.mark.xfail(strict=True)`标记提交测试，并在编写新功能时从测试中删除该标记。这在协作环境中非常有用，其中一个人提供了关于他们如何设想新功能/API的测试，另一个人根据测试用例实现它。
- en: You discover a bug in your application and write a test case demonstrating the
    problem. You might not have the time to tackle it right now or another person
    would be better suited to work in that part of the code. In this situation, marking
    the test as `@pytest.mark.xfail(strict=True)` would be a good approach.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你发现应用程序中的一个bug，并编写一个演示问题的测试用例。你可能现在没有时间解决它，或者另一个人更适合在代码的那部分工作。在这种情况下，将测试标记为`@pytest.mark.xfail(strict=True)`是一个很好的方法。
- en: 'All the cases above share one characteristic: you have a failing test and want
    to know whether it suddenly starts passing. In this case, the fact that the test
    passes warns you about a fact that requires your attention: a new version of a
    library with a bug fix has been released, part of a feature is now working as
    intended, or a known bug has been fixed.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有情况都有一个共同点：你有一个失败的测试，并想知道它是否突然开始通过。在这种情况下，测试通过的事实警告你需要注意的事实：一个带有bug修复的库的新版本已发布，部分功能现在按预期工作，或者已修复了一个已知的bug。
- en: 'The other situation where the `xfail` mark is useful is when you have tests
    that fail *sometimes*, also called **flaky** **tests**. Flaky tests are tests
    that fail on occasion, even if the underlying code has not changed. There are
    many reasons why tests fail in a way that appears to be random; the following
    are a few:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`xfail`标记有用的另一种情况是当你有*有时*失败的测试，也称为**不稳定**的**测试**。不稳定的测试是指有时会失败的测试，即使基础代码没有更改。测试失败看起来是随机的原因有很多；以下是其中一些：'
- en: Timing issues in multi threaded code
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多线程代码中的时间问题
- en: Intermittent network connectivity problems
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 间歇性的网络连接问题
- en: Tests that don't deal with asynchronous events correctly
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有正确处理异步事件的测试
- en: Relying on non-deterministic behavior
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于不确定的行为
- en: That is just to list a few possible causes. This non-determinism usually happens
    in tests with broader scopes, such as integration or UI. The fact is that you
    will almost always have to deal with flaky tests in large test suites.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是列举了一些可能的原因。这种不确定性通常发生在范围更广的测试中，比如集成或UI。事实上，你几乎总是需要处理大型测试套件中的不稳定测试。
- en: Flaky tests are a serious problem, because the test suite is supposed to be
    an indicator that the code is working as intended and that it can detect real
    problems when they happen. Flaky tests destroy that image, because often developers
    will see flaky tests failing that don't have anything to do with recent code changes.
    When this becomes commonplace, people begin to just run the test suite again in
    the hope that this time the flaky test passes (and it often does), but this erodes
    the trust in the test suite as a whole, and brings frustration to the development
    team. You should treat flaky tests as a menace that should be contained and dealt
    with.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 不稳定的测试是一个严重的问题，因为测试套件应该是代码按预期工作并在发生真正问题时能够检测到的指标。不稳定的测试破坏了这一形象，因为开发人员经常会看到与最近的代码更改无关的不稳定的测试失败。当这种情况变得司空见惯时，人们开始再次运行测试套件，希望这次不稳定的测试通过（它经常会通过），但这会侵蚀对整个测试套件的信任，并给开发团队带来挫折。你应该把不稳定的测试视为一个应该被遏制和处理的威胁。
- en: 'Here are some suggestions regarding how to deal with flaky tests within a development
    team:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于如何处理开发团队中的不稳定测试的一些建议：
- en: First, you need to be able to correctly identify flaky tests. If a test fails
    that apparently doesn't have anything to do with the recent changes, run the tests
    again. If the test that failed previously now `passes`, it means the test is flaky.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你需要能够正确识别不稳定的测试。如果一个测试失败，显然与最近的更改无关，再次运行测试。如果之前失败的测试现在**通过**，这意味着测试是不稳定的。
- en: Create an issue to deal with that particular flaky test in your ticket system.
    Use a naming convention or other means to label that issue as related to a flaky
    test (for example GitHub or JIRA labels).
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的工单系统中创建一个处理特定不稳定测试的问题。使用命名约定或其他方式标记该问题与不稳定测试相关（例如GitHub或JIRA标签）。
- en: 'Apply the `@pytest.mark.xfail(reason="flaky test #123", strict=False)` mark,
    making sure to include the issue ticket number or identifier. Feel free to add
    more information to the description, if you like.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '应用`@pytest.mark.xfail(reason="flaky test #123", strict=False)`标记，确保包括问题票号或标识。如果愿意，可以在描述中添加更多信息。'
- en: Make sure to periodically assign issues about flaky tests to yourself or other
    team members (for example, during sprint planning). The idea is to take care of
    flaky tests at a comfortable pace, eventually reducing or eliminating them altogether.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These practices take care of two major problems: they allow you to avoid eroding
    trust in the test suite, by getting flaky tests out of the way of the development
    team, and they put a policy in place to deal with flaky tests in their due time.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'Having covered the situations where the `xfail` mark is useful, let''s take
    a look at the full signature:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`condition`: the first parameter, if given, is a `True`/`False` condition,
    similar to the one used by `@pytest.mark.skipif`: if `False`, the `xfail` mark
    is ignored. It is useful to mark a test as `xfail` based on an external condition,
    such as the platform, Python version, library version, and so on.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`reason`: a string that will be shown in the short test summary when the `-ra`
    flag is used. It is highly recommended to always use this parameter to explain
    the reason why the test has been marked as `xfail` and/or include a ticket number.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`raises`: given an exception type, it declares that we expect the test to raise
    an instance of that exception. If the test raises another type of exception (even
    `AssertionError`), the test will `fail` normally. It is especially useful for
    missing functionality or to test for known bugs.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`run`: if `False`, the test will not even be executed and will fail as XFAIL.
    This is particularly useful for tests that run code that might crash the test-suite
    process (for example, C/C++ extensions causing a segmentation fault due to a known
    problem).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`strict`: if `True`, a passing test will fail the test suite. If `False`, the
    test will not fail the test suite regardless of the outcome (the default is `False`).
    This was discussed in detail at the start of this section.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The configuration variable `xfail_strict` controls the default value of the
    `strict` parameter of `xfail` marks:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Setting it to `True` means that all xfail-marked tests without an explicit `strict`
    parameter are considered an actual failure expectation instead of a flaky test.
    Any `xfail` mark that explicitly passes the `strict` parameter overrides the configuration
    value.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: pytest.xfail
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, you can imperatively trigger an XFAIL result within a test by calling
    the `pytest.xfail` function:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Similar to `pytest.skip`, this is useful when you can only determine whether
    you need to mark a test as `xfail` at runtime.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Parametrization
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common testing activity is passing multiple values to the same test function
    and asserting the outcome.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have an application that allows the user to define custom mathematical
    formulas, which will be parsed and evaluated at runtime. The formulas are given
    as strings, and can make use of mathematical functions such as `sin`, `cos`, `log`,
    and so on. A very simple way to implement this in Python would be to use the `eval`
    built-in ([https://docs.python.org/3/library/functions.html#eval](https://docs.python.org/3/library/functions.html#eval)),
    but because it can execute arbitrary code, we opt to use a custom tokenizer and
    evaluator for safety, instead..
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s not delve into the implementation details but rather focus on a test:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Here, we create a `Tokenizer` class, which is used by our implementation to
    break the formula string into internal tokens for later processing. Then, we pass
    the formula string and tokenizer to `Formula.from_string`, to obtain a formula
    object. With the formula object on our hands, we pass the input values to `formula.eval` and
    check that the returned value matches our expectation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: But we also want to test other math operations, to ensure we are covering all
    the features of our `Formula` class.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'One approach is to expand our test by using multiple assertions to check other
    formulas and input values:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This works, but if one of the assertions fails, the following assertions within
    the test function will not be executed. If there are multiple failures, we will
    have to run the test multiple times to see all of them and eventually fix all
    the issues.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'To see multiple failures per test run, we might decide to explicitly write
    a separate test for each assertion:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: But now we are duplicating code all over the place, which will make maintenance
    more difficult. Suppose in the future `FormulaTokenizer` is updated to explicitly
    receive a list of functions that can be used in formulas. This means that we would
    have to update the creation of `FormulaTokenzier` in several places.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid repeating ourselves, we might decide to write this instead:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This solves the problem of duplicating code, but now we are back to the initial
    problem of seeing only one failure at a time.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Enter @pytest.mark.parametrize
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To solve all the above problems, pytest provides the much-loved `@pytest.mark.parametrize`
    mark. With this mark, you are able to provide a list of input values to the test,
    and pytest automatically generates multiple test functions for each input value.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows this in action:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The `@pytest.mark.parametrize` mark automatically generates multiple test functions,
    parametrizing them with the arguments given to the mark. The call receives two
    parameters:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '`argnames`: a comma-separated string of argument names that will be passed
    to the test function.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`argvalues`: a sequence of tuples, with each tuple generating a new test invocation.
    Each item in the tuple corresponds to an argument name, so the first tuple `("C0
    * x + 10", dict(x=1.0, C0=2.0), 12.0)` will generate a call to the test function
    with the arguments:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`formula` = `"C0 * x + 10"`'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs` = `dict(x=1.0, C0=2.0)`'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`expected` = `12.0`'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using this mark, pytest will run `test_formula_parsing` three times, passing
    one set of arguments given by the `argvalues` parameter each time. It will also
    automatically generate a different node ID for each test, making it easy to distinguish
    between them:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: It is also important to note that the body of the function is just as compact
    as our starting test at the beginning of this section, but now we have multiple
    tests which allows us to see multiple failures when they happen.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Test parametrization not only avoids repeating test code and makes maintenance
    easier, it also invites you and the next developer who comes along to add more
    input values as the code matures. It encourages developers to cover more cases,
    because people are more eager to add a single line to the `argvalues` of a parametrized
    test than to copy and paste an entire new test to cover another input value.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: In summary, `@pytest.mark.parametrize` will make you cover more input cases,
    with very little overhead. It is definitely a very useful feature and should be
    used whenever you have multiple input values to be tested in the same way.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Applying marks to value sets
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, in parametrized tests, you find the need to apply one or more marks to
    a set of parameters as you would to a normal test function. For example, you want
    to apply a `timeout` mark to one set of parameters because it takes too long to
    run, or `xfail` a set of parameters, because it has not been implemented yet.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'In those cases, use `pytest.param` to wrap the set of values and apply the
    marks you want:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The signature of `pytest.param` is this:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Where:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '`*values` is the parameter set: `"hypot(x, y)", dict(x=3, y=4), 5.0`.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**kw` are options as keyword arguments: `marks=pytest.mark.xfail(reason="not
    implemented: #102")`. It accepts a single mark or a sequence of marks. There is
    another option, `ids`, which will be shown in the next section.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behind the scenes, every tuple of parameters passed to `@pytest.mark.parametrize`
    is converted to a `pytest.param` without extra options, so, for example,in the
    following the first code snippet is equivalent to the second code snippet:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Customizing test IDs
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider this example:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'As we have seen, pytest automatically creates custom test IDs, based on the
    parameters used in a parametrized call. Running `pytest -v` will generate these
    test IDs:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'If you don''t like the automatically generated IDs, you can use `pytest.param`
    and the `id` option to customize it:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This produces the following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This is also useful because it makes selecting tests significantly easier when
    using the `-k `flag:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'versus:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Testing multiple implementations
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well-designed systems usually make use of abstractions provided by interfaces,
    instead of being tied to specific implementations. This makes a system more resilient
    to future changes because, to extend it, you need to implement a new extension
    that complies with the expected interface, and integrate it into the system.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: One challenge that often comes up is how to make sure existing implementations
    comply with all the details of a specific interface.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose our system needs to be able to serialize some internal
    classes into a text format to save and load to disk. The following are some of
    the internal classes in our system:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '`Quantity`: represents a value and a unit of measure. For example `Quantity(10,
    "m")` means *10 meters*. `Quantity` objects have addition, subtraction, and multiplication—basically,
    all the operators that you would expect from a native `float`, but taking the
    unit of measure into account.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pipe`: represents a duct where some fluid can flow through. It has a `length`
    and `diameter`, both `Quantity` instances.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Initially, in our development, we only need to save those objects in **JSON**
    format, so we go ahead and implement a straightforward serializer class, that
    is able to serialize and de-serialize our classes:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now we should write some tests to ensure everything is working:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: This works well and is a perfectly valid approach, given our requirements.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Some time passes, and new requirements arrive: now we need to serialize our
    objects into other formats, namely `XML` and [`YAML`(](http://yaml.org/)[http://yaml.org/](http://yaml.org/)[)](http://yaml.org/).
    To keep things simple, we create two new classes, `XMLSerializer` and `YAMLSerializer`,
    which implement the same `serialize`/`deserialize` methods. Because they comply
    with the same interface as `JSONSerializer`, we can use the new classes interchangeably
    in our system, which is great.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: But how do we test the different implementations?
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'A naive approach would be to loop over the different implementations inside
    each test:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: This works, but it is not ideal, because we have to copy and paste the loop
    definition in each test, making it harder to maintain. Also, if one of the serializers
    fails, the next ones in the list will never be executed.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Another, horrible approach, would be to copy and paste the entire test functions,
    replacing the serializer class each time, but we won't be showing that here.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'A much better solution is to use `@pytest.mark.parametrize` at class level.
    Observe:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'With a small change, we have multiplied our existing tests to cover all the
    new implementations:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The `@pytest.mark.parametrize` decorator also makes it very clear that new implementations
    should be added to the list and that all existing tests must pass. New tests added
    to the class also need to pass for all implementations.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: In summary, `@pytest.mark.parametrize` can be a very powerful tool to ensure
    that different implementations comply with the specifications of an interface.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use marks to organize our code and help us
    run the test suite in flexible ways. We then looked at how to use the `@pytest.mark.skipif`
    to conditionally skip tests, and how to use the `@pytest.mark.xfail` mark to deal
    with expected failures and flaky tests. Then we discussed ways of dealing with
    flaky tests in a collaborative environment. Finally, we discussed the benefits
    of using `@pytest.mark.parametrize` to avoid repeating our testing code and to
    make it easy for ourselves and others to add new input cases to existing tests.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用标记来组织我们的代码，并帮助我们以灵活的方式运行测试套件。然后我们看了如何使用`@pytest.mark.skipif`来有条件地跳过测试，以及如何使用`@pytest.mark.xfail`标记来处理预期的失败和不稳定的测试。然后我们讨论了在协作环境中处理不稳定测试的方法。最后，我们讨论了使用`@pytest.mark.parametrize`的好处，以避免重复我们的测试代码，并使自己和其他人能够轻松地向现有测试添加新的输入案例。
- en: 'In the next chapter, we will finally get to one of pytest''s most loved and
    powerful features: **fixtures**.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将终于介绍pytest最受喜爱和强大的功能之一：**fixtures**。
