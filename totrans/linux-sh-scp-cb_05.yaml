- en: Chapter 5. Tangled Web? Not At All!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading from a web page
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading a web page as formatted plain text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A primer on cURL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing unread Gmail mails from the command line
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing data from a website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an image crawler and downloader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a web photo album generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Twitter command-line client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define utility with Web backend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding broken links in a website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking changes to a website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Posting to a web page and reading response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Web is becoming the face of technology. It is the central access point for
    data processing. Though shell scripting cannot do everything that languages like
    PHP can do on the Web, there are still many tasks to which shell scripts are ideally
    suited. In this chapter we will explore some recipes that can be used to parse
    website content, download and obtain data, send data to forms, and automate website
    usage tasks and similar activities. We can automate many activities that we perform
    interactively through a browser with a few lines of scripting. Access to the functionalities
    provided by the HTTP protocol with command-line utilities enables us to write
    scripts that are suitable to solve most of the web-automation utilities. Have
    fun while going through the recipes of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading from a web page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Downloading a file or a web page from a given URL is simple. A few command-line
    download utilities are available to perform this task.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`wget` is a file download command-line utility. It is very flexible and can
    be configured with many options.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A web page or a remote file can be downloaded using `wget` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to specify multiple download URLs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'A file can be downloaded using `wget` using the URL as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Usually, files are downloaded with the same filename as in the URL and the download
    log information or progress is written to `stdout`.
  prefs: []
  type: TYPE_NORMAL
- en: You can specify the output file name with the `-O` option. If the file with
    the specified filename already exists, it will be truncated first and the downloaded
    file will be written to the specified file.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also specify a different logfile path rather than printing logs to
    `stdout` by using the `-o` option as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: By using the above command, nothing will be printed on screen. The log or progress
    will be written to `log` and the output file will be `dloaded_file.img`.
  prefs: []
  type: TYPE_NORMAL
- en: There is a chance that downloads might break due to unstable Internet connections.
    Then we can use the number of tries as an argument so that once interrupted, the
    utility will retry the download that many times before giving up.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to specify the number of tries, use the `-t` flag as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `wget` utility has several additional options that can be used under different
    problem domains. Let's go through a few of them.
  prefs: []
  type: TYPE_NORMAL
- en: Restricted with speed downloads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we have a limited Internet downlink bandwidth and many applications sharing
    the internet connection, if a large file is given for download, it will suck all
    the bandwidth and may cause other process to starve for bandwidth. The `wget`
    command comes with a built-in option to specify the maximum bandwidth limit the
    download job can possess. Hence all the applications can simultaneously run smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can restrict the speed of `wget` by using the `--limit-rate` argument as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this command `k` (kilobyte) and `m` (megabyte) specify the speed limit.
  prefs: []
  type: TYPE_NORMAL
- en: We can also specify the maximum quota for the download. It will stop when the
    quota is exceeded. It is useful when downloading multiple files limited by the
    total download size. This is useful to prevent the download from accidently using
    too much disk space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `--quota` or `–Q` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Resume downloading and continue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If a download using `wget` gets interrupted before it is completed, we can
    resume the download where we left off by using the `-c` option as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Using cURL for download
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: cURL is another advanced command-line utility. It is much more powerful than
    `wget`.
  prefs: []
  type: TYPE_NORMAL
- en: 'cURL can be used to download as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Unlike `wget`, `curl` writes the downloaded data into standard output (`stdout`)
    rather than to a file. Therefore, we have to redirect the data from `stdout` to
    the file using a redirection operator.
  prefs: []
  type: TYPE_NORMAL
- en: Copying a complete website (mirroring)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`wget` has an option to download the complete website by recursively collecting
    all the URL links in the web pages and downloading all of them like a crawler.
    Hence we can completely download all the pages of a website.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to download the pages, use the `--mirror` option as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Or use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`-l` specifies the `DEPTH` of web pages as levels. That means it will traverse
    only that much number of levels. It is used along with `–r` (recursive). The `-N`
    argument is used to enable time stamping for the file. `URL` is the base URL for
    a website for which the download needs to be initiated.'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing pages with HTTP or FTP authentication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some web pages require authentication for HTTP or FTP URLs. This can be provided
    by using the `--user` and `--password` arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: It is also possible to ask for a password without specifying the password inline.
    In order to do that use `--ask-password` instead of the `--password` argument.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading a web page as formatted plain text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web pages are HTML pages containing a collection of HTML tags along with other
    elements, such as JavaScript, CSS, and so on. But the HTML tags define the base
    of a web page. We may need to parse the data in a web page while looking for specific
    content, and this is something Bash scripting can help us with. When we download
    a web page, we receive an HTML file. In order to view formatted data, it should
    be viewed in a web browser. However, in most of the circumstances, parsing a formatted
    text document will be easier than parsing HTML data. Therefore, if we can get
    a text file with formatted text similar to the web page seen on the web browser,
    it is more useful and it saves a lot of effort required to strip off HTML tags.
    Lynx is an interesting command-line web browser. We can actually get the web page
    as plain text formatted output from Lynx. Let's see how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s download the webpage view, in ASCII character representation, in a text
    file using the `–dump` flag with the `lynx` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This command will also list all the hyper-links (`<a href="link">`) separately
    under a heading **References** as the footer of the text output. This would help
    us avoid parsing of links separately using regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the plain text version of text by using the `cat` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: A primer on cURL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: cURL is a powerful utility that supports many protocols including HTTP, HTTPS,
    FTP, and much more. It supports many features including POST, cookie, authentication,
    downloading partial files from a specified offset, referers, user agent strings,
    extra headers, limit speed, maximum file size, progress bars, and so on. cURL
    is useful for when we want to play around with automating a web page usage sequence
    and to retrieve data. This recipe is a list of the most important features of
    cURL.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: cURL doesn't come with any of the main Linux distros by default, so you may
    have to install it using the package manager. By default, most distributions ship
    with `wget`.
  prefs: []
  type: TYPE_NORMAL
- en: cURL usually dumps downloaded files to `stdout` and progress information to
    `stderr`. To avoid progress information from being shown, we always use the`--silent`
    option.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `curl` command can be used to perform different activities such as downloading,
    sending different HTTP requests, specifying HTTP headers, and so on. Let's see
    how to perform different tasks with cURL.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The above command dumps the downloaded file into the terminal (the downloaded
    data is written to `stdout`).
  prefs: []
  type: TYPE_NORMAL
- en: The `--silent` option is used to prevent the `curl` command from displaying
    progress information. If progress information is required, remove `--silent`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `-O` option is used to write the downloaded data into a file with the filename
    parsed from the URL rather than writing into the standard output.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`index.html` will be created.'
  prefs: []
  type: TYPE_NORMAL
- en: It writes a web page or file to the filename as in the URL instead of writing
    to `stdout`. If filenames are not there in the URL, it will produce an error.
    Hence, make sure that the URL is a URL to a remote file. `curl http://slynux.org
    -O --silent` will display an error since the filename cannot be parsed from the
    URL.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `-o` option is used to download a file and write to a file with a specified
    file name.
  prefs: []
  type: TYPE_NORMAL
- en: In order to show the `#` progress bar while downloading, use `–-progress` instead
    of `–-silent`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections we have learned how to download files and dump HTML
    pages to the terminal. There several advanced options that come along with cURL.
    Let's explore more on cURL.
  prefs: []
  type: TYPE_NORMAL
- en: Continue/Resume downloading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: cURL has advanced resume download features to continue at a given offset unlike
    `wget`. It helps to download portions of files by specifying an offset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The offset is an integer value in bytes.
  prefs: []
  type: TYPE_NORMAL
- en: 'cURL doesn''t require us to know the exact byte offset if we want to resume
    downloading a file. If you want cURL to figure out the correct resume point, use
    the `-C -` option, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: cURL will automatically figure out where to restart the download of the specified
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Set referer string with cURL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Referer is a string in the HTTP header used to identify the page from which
    the user reaches the current web page. When a user clicks on a link from web page
    A and it reaches web page B, the referer header string in the page B will contain
    a URL of page A.
  prefs: []
  type: TYPE_NORMAL
- en: Some dynamic pages check the referer string before returning HTML data. For
    example, a web page shows a Google logo attached page when a user navigates to
    a website by searching on Google, and shows a different page when they navigate
    to the web page by manually typing the URL.
  prefs: []
  type: TYPE_NORMAL
- en: The web page can write a condition to return a Google page if the referer is
    [www.google.com](http://www.google.com) or else return a different page.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use `--referer` with the `curl` command to specify the referer string
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Cookies with cURL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using `curl` we can specify as well as store cookies encountered during HTTP
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: In order to specify cookies, use the `--cookie "COOKIES"` option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cookies should be provided as `name=value`. Multiple cookies should be delimited
    by a semicolon ";". For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to specify a file to which cookies encountered are to be stored, use
    the `--cookie-jar` option. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Setting a user agent string with cURL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some web pages that check the user-agent won't work if there is no user-agent
    specified. You may have noticed that certain websites work well only in Internet
    Explorer (IE). If a different browser is used, the website will show a message
    that it will work only on IE. This is because the website checks for a user agent.
    You can set the user agent as IE with `curl` and see that it returns a different
    web page in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using cURL it can be set using `--user-agent` or `–A` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Additional headers can be passed with cURL. Use `–H "Header"` to pass multiple
    additional headers. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Specifying bandwidth limit on cURL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When the available bandwidth is limited and multiple users are sharing the
    Internet, in order to perform the sharing of bandwidth smoothly, we can limit
    the download rate to a specified limit from `curl` by using the `--limit-rate`
    option as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In this command `k` (kilobyte) and `m` (megabyte) specify the download rate
    limit.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying the maximum download size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The maximum download file size for cURL can be specified using the `--max-filesize`
    option as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: It will return a non-zero exit code if the file size exceeds. It will return
    zero if it succeeds.
  prefs: []
  type: TYPE_NORMAL
- en: Authenticating with cURL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HTTP authentication or FTP authentication can be done using cURL with the `-u`
    argument.
  prefs: []
  type: TYPE_NORMAL
- en: The username and password can be specified using `-u username:password`. It
    is possible to not provide a password such that it will prompt for password while
    executing.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you prefer to be prompted for the password, you can do that by using only
    `-u username`. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to be prompted for the password use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Printing response headers excluding data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is useful to print only response headers to apply many checks or statistics.
    For example, to check whether a page is reachable or not, we don't need to download
    the entire page contents. Just reading the HTTP response header can be used to
    identify if a page is available or not.
  prefs: []
  type: TYPE_NORMAL
- en: An example usage case for checking the HTTP header is to check the file size
    before downloading. We can check the `Content-Length` parameter in the HTTP header
    to find out the length of a file before downloading. Also, several useful parameters
    can be retrieved from the header. The `Last-Modified` parameter enables to know
    the last modification time for the remote file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `–I` or `–head` option with `curl` to dump only HTTP headers without
    downloading the remote file. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Posting to a web page and reading response*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing Gmail from the command line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gmail is a widely-used free e-mail service from Google[: http://mail.google.com/](http://:
    http://mail.google.com/). Gmail allows you to read your mail via authenticated
    RSS feeds. We can parse the RSS feeds with the sender''s name and an e-mail with
    subject. It will help to have a look at unread mails in the inbox without opening
    the web browser.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s go through the shell script to parse the RSS feeds for Gmail to display
    the unread mails:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The script uses cURL to download the RSS feed by using user authentication.
    User authentication is provided by the `-u username:password` argument. You can
    use `-u user` without providing the password. Then while executing cURL it will
    interactively ask for the password.
  prefs: []
  type: TYPE_NORMAL
- en: Here we can split the piped commands into different blocks to illustrate how
    they work.
  prefs: []
  type: TYPE_NORMAL
- en: '`tr -d ''\n''` removes the newline character so that we restructure each mail
    entry with `\n` as the delimiter. `sed ''s:</entry>:\n:g''` replaces every `</entry>`
    with a newline so that each mail entry is delimited by a newline and hence mails
    can be parsed one by one. Have a look at the source of [https://mail.google.com/mail/feed/atom](https://mail.google.com/mail/feed/atom)
    for XML tags used in the RSS feeds. `<entry> TAGS </entry>` corresponds to a single
    mail entry.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next block of script is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This script matches the substring title using `<title>\(.*\)<\/title`, the
    sender name using `<author><name>\([^<]*\)<\/name>`, and e-mail using `<email>\([^<]*\)`.
    Then back referencing is used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Author: \2 [\3] \nSubject: \1\n` is used to replace an entry for a mail with
    the matched items in an easy-to-read format. `\1` corresponds to the first substring
    match, `\2` for the second substring match, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `SHOW_COUNT=5` variable is used to take the number of unread mail entries
    to be printed on terminal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head` is used to display only `SHOW_COUNT*3` lines from the first line. `SHOW_COUNT`
    is used three times in order to show three lines of the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*A primer on cURL*, explains the curl command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Basic sed primer* of[Chapter 4](ch04.html "Chapter 4. Texting and Driving"),
    explains the sed command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing data from a website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is often useful to parse data from web pages by eliminating unnecessary details.
    `sed` and `awk` are the main tools that we will use for this task. You might have
    come across a list of access rankings in a grep recipe in the previous chapter
    *Texting and driving*; it was generated by parsing the website page [http://www.johntorres.net/BoxOfficefemaleList.html](http://www.johntorres.net/BoxOfficefemaleList.html).
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to parse the same data using text-processing tools.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s go through the command sequence used to parse details of actresses from
    the website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lynx is a command-line web browser; it can dump the text version of the website
    as we would see in a web browser rather than showing us the raw code. Hence it
    avoids the job of removing the HTML tags. We parse the lines starting with Rank,
    using `sed` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: These lines could be then sorted according to the ranks. `awk` is used here
    to keep the spacing between rank and the name uniform by specifying the width.
    `%-4s` specifies a four-character width. All the fields except the first field
    are concatenated to form a single string as `$2`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Basic sed primer* of[Chapter 4](ch04.html "Chapter 4. Texting and Driving"),
    explains the sed command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Basic awk primer* of[Chapter 4](ch04.html "Chapter 4. Texting and Driving"),
    explains the awk command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Downloading a web page as formatted plain text*, explains the lynx command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image crawler and downloader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image crawlers are very useful when we need to download all the images that
    appear in a web page. Instead of going through the HTML sources and picking all
    the images, we can use a script to parse the image files and download them automatically.
    Let's see how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s write a Bash script to crawl and download the images from a web page
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'An example usage is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The above image downloader script parses an HTML page, strips out all tags except
    `<img>`, then parses `src="img/URL"` from the `<img>` tag and downloads them to
    the specified directory. This script accepts a web page URL and the destination
    directory path as command-line arguments. The first part of the script is a tricky
    way to parse command-line arguments. The `[ $# -ne 3 ]` statement checks whether
    the total number of arguments to the script is three, else it exits and returns
    a usage example.
  prefs: []
  type: TYPE_NORMAL
- en: 'If it is 3 arguments, then parse the URL and the destination directory. In
    order to do that a tricky hack is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: A `for` loop is iterated four times (there is no significance to the number
    four, it is just to iterate a couple of times to run the case statement).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `case` statement will evaluate the first argument (`$1`), and matches `-d`
    or any other string arguments that are checked. We can place the `-d` argument
    anywhere in the format as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Or:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`shift` is used to shift arguments such that when `shift` is called `$1` will
    be assigned with `$2`, when again called `$1=$3` and so on as it shifts `$1` to
    the next arguments. Hence we can evaluate all arguments through `$1` itself.'
  prefs: []
  type: TYPE_NORMAL
- en: When `-d` is matched ( `-d)` ), it is obvious that the next argument is the
    value for the destination directory. `*)` corresponds to default match. It will
    match anything other than `-d`. Hence while iteration `$1=""` or `$1=URL` in the
    default match, we need to take `$1=URL` avoiding `""` to overwrite. Hence we use
    the `url=${url:-$1}` trick. It will return a URL value if already not `""` else
    it will assign `$1`.
  prefs: []
  type: TYPE_NORMAL
- en: '`egrep -o "<img src=[^>]*>"` will print only the matching strings, which are
    the `<img>` tags including their attributes. `[^>]*` used to match all characters
    except the closing `>`, that is, `<img src="img/image.jpg" …. >`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`sed ''s/<img src=\"\([^"]*\).*/\1/g''` parses `src="img/url"` so that all
    image URLs can be parsed from the `<img>` tags already parsed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of image source paths: relative and absolute. Absolute
    paths contain full URLs that start with `http://` or `https://`. Relative URLs
    starts with `/` or `image_name` itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of an absolute URL is: [http://example.com/image.jpg](http://example.com/image.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a relative URL is: `/image.jpg`'
  prefs: []
  type: TYPE_NORMAL
- en: For relative URLs the starting `/` should be replaced with the base URL to transform
    it to [http://example.com/image.jpg](http://example.com/image.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: For that transformation, we initially find out `baseurl sed` by parsing.
  prefs: []
  type: TYPE_NORMAL
- en: Then replace every occurrence of the starting `/` with `baseurl sed` as `sed
    -i "s|^/|$baseurl/|" /tmp/$$.list`.
  prefs: []
  type: TYPE_NORMAL
- en: Then a `while` loop is used to iterate the list line by line and download the
    URL using `curl`. The `--silent` argument is used with `curl` to avoid other progress
    messages from being printed on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*A primer on cURL*, explains the curl command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Basic sed primer* of[Chapter 4](ch04.html "Chapter 4. Texting and Driving"),
    explains the sed command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Searching and mining "text" inside a file with grep* of[Chapter 4](ch04.html
    "Chapter 4. Texting and Driving"), explains the grep command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web photo album generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web developers commonly design photo album pages for websites that consist of
    a number of image thumbnails on the page. When thumbnails are clicked, a large
    version of the picture will be displayed. But when many images are required, copying
    the `<img>` tag every time, resizing the image to create a thumbnail, placing
    them in the thumbs directory, testing the links, and so on are real hurdles. It
    takes a lot of time and repeats the same task. It can be automated easily by writing
    a simple Bash script. By writing a script, we can create thumbnails, place them
    in exact directories, and generate the code fragment for `<img>` tags automatically
    in few seconds. This recipe will teach you how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can perform this task with a `for` loop that iterates every image in the
    current directory. The usual Bash utilities such as `cat` and `convert` (image
    magick) are used. These will generate an HTML album, using all the images, to
    `index.html`. In order to use `convert`, make sure you have Imagemagick installed.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s write a Bash script to generate a HTML album page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the script as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The initial part of the script is to write the header part of the HTML page.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following script redirects all the contents up to EOF (excluding) to the
    `index.html`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The header includes the HTML and stylesheets.
  prefs: []
  type: TYPE_NORMAL
- en: '`for img in *.jpg;` will iterate through names of each file and will perform
    actions.'
  prefs: []
  type: TYPE_NORMAL
- en: '`convert "$img" -resize "100x" "thumbs/$img"` will create images of 100px width
    as thumbnails.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following statement will generate the required `<img>` tag and appends
    it to the `index.html`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the footer HTML tags are appended with `cat` again.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Playing with file descriptors and redirection* of[Chapter 1](ch01.html "Chapter 1. Shell
    Something Out"), explains EOF and stdin redirection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Twitter command-line client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Twitter is the hottest micro blogging platform as well as the latest buzz of
    online social media. Tweeting and reading tweets is fun. What if we can do both
    from command line? It is pretty simple to write a command-line Twitter client.
    Twitter has RSS feeds and hence we can make use of them. Let's see how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use cURL to authenticate and send twitter updates as well as download
    the RSS feed pages to parse the tweets. Just four lines of code can do it. Let's
    do it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s write a Bash script using the `curl` command to manipulate twitter APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the script as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s see the working of above script by splitting it into two parts. The
    first part is about reading tweets. To read tweets the script downloads the RSS
    information from [http://twitter.com/statuses/friends_timeline.rss](http://twitter.com/statuses/friends_timeline.rss)
    and parses the lines containing the `<title>` tag. Then it strips off the `<title>`
    and `</title>` tags using `sed` to form the required tweet text. Then a `COUNT`
    variable is used to remove all other text except the number of recent tweets by
    using the `head` command. `tail –n +2` is used to remove an unnecessary header
    text "Twitter: Timeline of friends".'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the sending tweet part, the `-d` status argument of `curl` is used to post
    data to Twitter using their API: [http://twitter.com/statuses/update.xml](http://twitter.com/statuses/update.xml).'
  prefs: []
  type: TYPE_NORMAL
- en: '`$1` of the script will be the tweet in the case of sending a tweet. Then to
    obtain the status we take `$@` (list of all arguments of the script) and remove
    the word "tweet" from it.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*A primer on cURL*, explains the curl command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*head and tail - printing the last or first 10 lines* of[Chapter 3](ch03.html
    "Chapter 3. File In, File Out"), explains the commands head and tail'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: define utility with Web backend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google provides Web definitions for any word by using the search query `define:WORD`.
    We need a GUI web browser to fetch the definitions. However, we can automate it
    and parse the required definitions by using a script. Let's see how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use `lynx`, `sed`, `awk`, and `grep` to write the define utility.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s go through the code for the define utility script to fetch definitions
    from Google search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the script as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will look into the core part of the definition parser. Lynx is used to obtain
    the plain text version of the web page. [http://www.google.co.in/search?q=define:$word](http://www.google.co.in/search?q=define:$word)
    is the URL for the web definition web page. Then we reduce the text between "Definitions
    on web" and "Find definitions". All the definitions are occurring in between these
    lines of text (`awk '/Defini/,/Find defini/'`).
  prefs: []
  type: TYPE_NORMAL
- en: '`''s:*:\n*:''` is used to replace * with * and newline in order to insert a
    newline in between each definition, and `s:^[ ]*::` is used to remove extra spaces
    in the start of lines. Hyperlinks are marked as [number] in lynx output. Those
    lines are removed by `grep -v`, the invert match lines option. Then `awk` is used
    to replace the * occurring at start of the line with a number so that each definition
    can assign a serial number. If we have read a `-n` count in the script, it has
    to output only a few definitions as per count. So `awk` is used to print the definitions
    with number 1 to count (this makes it easier since we replaced * with the serial
    number).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Basic sed primer* of[Chapter 4](ch04.html "Chapter 4. Texting and Driving"),
    explains the sed command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Basic awk primer* of[Chapter 4](ch04.html "Chapter 4. Texting and Driving"),
    explains the awk command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Searching and mining "text" inside a file with grep* of[Chapter 4](ch04.html
    "Chapter 4. Texting and Driving"), explains the grep command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Downloading a web page as formatted plain text*, explains the lynx command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding broken links in a website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have seen people manually checking each and every page on a site to search
    for broken links. It is possible only for websites having very few pages. When
    the number of pages become large, it will become impossible. It becomes really
    easy if we can automate finding broken links. We can find the broken links by
    using HTTP manipulation tools. Let's see how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to identify the links and find the broken ones from the links, we can
    use `lynx` and `curl`. It has an option `-traversal`, which will recursively visit
    pages in the website and build the list of all hyperlinks in the website. We can
    use cURL to verify whether each of the links are broken or not.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s write a Bash script with the help of the `curl` command to find out
    the broken links on a web page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`lynx -traversal URL` will produce a number of files in the working directory.
    It includes a file `reject.dat` which will contain all the links in the website.
    `sort -u` is used to build a list by avoiding duplicates. Then we iterate through
    each link and check the header response by using `curl -I`. If the header contains
    first line `HTTP/1.0 200 OK` as the response, it means that the target is not
    broken. All other responses correspond to broken links and are printed out to
    `stdout`.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Downloading a web page as formatted plain text*, explains the lynx command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A primer on cURL*, explains the curl command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking changes to a website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tracking changes to a website is helpful to web developers and users. Checking
    a website manually in intervals is really hard and impractical. Hence we can write
    a change tracker running at repeated intervals. When a change occurs, it can play
    a sound or send a notification. Let's see how to write a basic tracker for the
    website changes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tracking changes in terms of Bash scripting means fetching websites at different
    times and taking the difference using the `diff` command. We can use `curl` and
    `diff` to do this.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s write a Bash script by combining different commands to track changes
    in a web page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the output of the `track_changes.sh` script when changes are
    made to the web page and when the changes are not made to the page:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First run:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Second Run:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Third run after making changes to the web page:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The script checks whether the script is running for the first time using `[
    ! -e "last.html" ];`. If `last.html` doesn't exist, that means it is the first
    time and hence the webpage must be downloaded and copied as `last.html`.
  prefs: []
  type: TYPE_NORMAL
- en: If it is not the first time, it should download the new copy (`recent.html`)
    and check the difference using the `diff` utility. If changes are there, it should
    print the changes and finally it should copy `recent.html` to `last.html`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*A primer on cURL*, explains the curl command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Posting to a web page and reading response
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: POST and GET are two types of requests in HTTP to send information to or retrieve
    information from a website. In a GET request, we send parameters (name-value pairs)
    through the web page URL itself. In the case of POST, it won't be attached with
    the URL. POST is used when a form needs to be submitted. For example, a username,
    the password to be submitted, and the login page to be retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: POSTing to pages comes as frequent use while writing scripts based on web page
    retrievals. Let's see how to work with POST. Automating the HTTP GET and POST
    request by sending POST data and retrieving output is a very important task that
    we practice while writing shell scripts that parse data from websites.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both cURL and `wget` can handle POST requests by arguments. They are to be passed
    as name-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s see how to POST and read HTML response from a real website using `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a website ([http://book.sarathlakshman.com/lsc/mlogs/](http://book.sarathlakshman.com/lsc/mlogs/))
    and it is used to submit the current user information such as hostname and username.
    Assume that, in the home page of the website there are two fields HOSTNAME and
    USER, and a SUBMIT button. When the user enters a hostname, a user name, and clicks
    on the SUBMIT button, the details will be stored in the website. This process
    can be automated using a single line of `curl` command by automating the POST
    request. If you look at the website source (use the view source option from the
    web browser), you can see an HTML form defined similar to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Here, [http://book.sarathlakshman.com/lsc/mlogs/submit.php](http://book.sarathlakshman.com/lsc/mlogs/submit.php)
    is the target URL. When the user enters the details and clicks on the Submit button.
    The host and user inputs are sent to `submit.php` as a POST request and the response
    page is returned on the browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can automate the POST request as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Now `curl` returns the response page.
  prefs: []
  type: TYPE_NORMAL
- en: '`-d` is the argument used for posting. The string argument for `-d` is similar
    to the GET request semantics. `var=value` pairs are to be delimited by `&`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `-d` argument should always be given in quotes. If quotes are not used,
    `&` is interpreted by the shell to indicate this should be a background process.
  prefs: []
  type: TYPE_NORMAL
- en: There's more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's see how to perform POST using cURL and `wget`.
  prefs: []
  type: TYPE_NORMAL
- en: POST in curl
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can POST data in `curl` by using `-d` or `–data` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'If multiple variables are to be sent, delimit them with `&`. Note that when
    `&` is used the name-value pairs should be enclosed in quotes, else the shell
    will consider `&` as a special character for background process. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: POST data using wget
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can POST data using `wget` by using `-–post-data "string"`. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Use the same format as cURL for name-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*A primer on cURL*, explains the curl command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Downloading from a web page* explains the wget command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
