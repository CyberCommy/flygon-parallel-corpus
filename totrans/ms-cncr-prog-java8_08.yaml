- en: Chapter 7. Processing Massive Datasets with Parallel Streams – The Map and Reduce
    Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Undoubtedly, the most important innovations introduced in Java 8 are **lambda**
    expressions and **stream** API. A stream is a sequence of elements that can be
    processed in a sequential or parallel way. We can transform the stream applying
    the intermediate operations and then perform a final computation to get the desired
    result (a list, an array, a number, and so on). In this chapter, we will cover
    the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first example – a numerical summarization application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second example – an information retrieval search tool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A stream is a sequence of data (it is not a data structure) that allows you
    to apply a sequence of operations in a sequential or concurrent way to filter,
    convert, sort, reduce, or organize those elements to obtain a final object. For
    example, if you have a stream with the data of your employees, you can use a stream
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: Count the total number of employees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the average salary of all employees who live in a particular place
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain a list of the employees who haven't met their objectives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any operation that implies work with all or part of the employees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streams are greatly influenced by functional programming (the **Scala** programming
    language provides a very similar mechanism), and they were thought to work with
    lambda expressions. A stream API resembles **LINQ** (short for **Language-Integrated
    Query**) queries available in C# language and, to some extent, could be compared
    with SQL queries.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will explain the basic characteristics of streams
    and the parts you will find in a stream.
  prefs: []
  type: TYPE_NORMAL
- en: Basic characteristics of streams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main characteristics of a stream are:'
  prefs: []
  type: TYPE_NORMAL
- en: A stream does not store its elements. The stream takes the elements from its
    source and sends them across all the operations that form the pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can work with streams in parallel without any extra work. When you create
    a stream, you can use the `stream()` method to create a sequential stream or `parallelStream()`
    to create a concurrent one. The `BaseStream` interface defines the `sequential()`
    methods to obtain a sequential version of the stream and `parallel()` to obtain
    a concurrent version of the stream. You can convert a sequential stream to parallel
    and a parallel to sequential as many times as you want. Take into account that
    when the terminal stream operation is performed, all the stream operations will
    be processed according to the last setting. You cannot instruct a stream to perform
    some operations sequentially and other operations concurrently. Internally, parallel
    streams in Oracle JDK 8 and Open JDK 8 use an implementation of the Fork/Join
    framework to execute the concurrent operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streams are greatly influenced by the functional programming and the Scala programming
    language. You can use the new lambda expressions as a way to define the algorithm
    to be executed in an operation over a stream.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streams can't be reusable. When you obtain a stream, for example, from a list
    of values, you can use that stream only once. If you want to perform another operation
    on the same data, you have to create one more stream.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streams make a lazy processing of data. They don't obtain the data until it's
    necessary. As you will learn later, a stream has an origin, some intermediate
    operations, and a terminal operation. The data isn't processed until the terminal
    operation needs it, so the stream processing doesn't begin until the terminal
    operation is executed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can't access the elements of the stream in a different way. When you have
    a data structure, you can access one determined element stored in it, for example,
    indicating its position or its key. Stream operations usually process the elements
    uniformly, so the only thing you have is the element itself. You don't know the
    position of the element in the stream and the neighbor elements. In the case of
    parallel streams, the elements can be processed in any order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stream operations don't allow you to modify the stream source. For example,
    if you use a list as the stream source, you can store the processing result into
    the new list, but you cannot add, remove, or replace the elements of the original
    list. Although this sounds restrictive, it's a very useful feature as you can
    return the stream created from your internal collection without a fear that the
    list will be modified by the caller.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sections of a stream
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A stream has three different sections:'
  prefs: []
  type: TYPE_NORMAL
- en: A **source** that generates the data consumed by the stream.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero or more **intermediate operations**, which generate another stream as an
    output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One **terminal operation** that generates an object, which can be a simple object
    or a collection as an array, a list, or a hash table. There can be also terminal
    operations that don't produce any explicit result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sources of a stream
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The source of the stream generates the data that will be processed by the `Stream`
    object. You can create a stream from different sources. For example, the `Collection`
    interface has included the `stream()` methods in Java 8 to generate a sequential
    stream and `parallelStream()` to generate a parallel one. This allows you to generate
    a stream to process all the data from almost all the data structures implemented
    in Java as lists (`ArrayList`, `LinkedList`, and so on), sets (`HashSet`, `EnumSet`),
    or concurrent data structures (`LinkedBlockingDeque`, `PriorityBlockingQueue`,
    and so on). Another data structure that can generate streams is an array. The
    `Array` class includes four versions of the `stream()` method to generate a stream
    from the array. If you pass an array of `int` numbers to the method, it will generate
    `IntStream`. This is a special kind of stream implemented to work with integer
    numbers (you can still use `Stream<Integer>` instead of `IntStream`, but performance
    might be significantly worse). Similarly, you can create `LongStream` or `DoubleStream`
    from the `long[]` or `double[]` array.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if you pass an array of object to the `stream()` method, you will
    obtain a generic stream of the same type. In this case, there is no `parallelStream()`
    method, but once you have obtained the stream, you can call the `parallel()` method
    defined in the `BaseStream` interface to convert the sequential stream into a
    concurrent one.
  prefs: []
  type: TYPE_NORMAL
- en: Other interesting functionality provided by the `Stream` API is that you can
    generate and stream to process the content of a directory or a file. The `Files`
    class provides different methods to work with files using streams. For example,
    the `find()` method returns a stream with the `Path` objects of the files in a
    file tree that meet certain conditions. The `list()` method returns a stream of
    the `Path` objects with the contents of a directory. The `walk()` method returns
    a stream of the `Path` objects processing all the objects in a directory tree
    using a depth-first algorithm. But the most interesting method is the `lines()`
    method that creates a stream of `String` objects with the lines of a file, so
    you can process its content using a stream. Unfortunately, all of the methods
    mentioned here parallelize badly unless you have many thousands of elements (files
    or lines).
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, you can create a stream using two methods provided by the `Stream` interface:
    the `generate()` and `iterate()` methods. The `generate()` method receives a `Supplier`
    parameterized with an object type as a parameter and generates an infinite sequential
    stream of objects of that type. The `Supplier` interface has the `get()` method.
    Every time the stream needs a new object it will call this method to obtain the
    next value of the stream. As we mentioned earlier, streams process the data in
    a lazy way, so there is no problem with the infinite nature of the stream. You
    will use other methods that will convert that stream in finite way. The `iterate()`
    method is similar, but in this case, the method receives a seed and a `UnaryOperator`.
    The first value is the result of apply the `UnaryOperator` to the seed; the second
    value is the result of apply the `UnaryOperator` to the first result, and so on.
    This method should be avoided as much as possible in concurrent applications because
    of their performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s also more stream sources as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`String.chars()`: This returns a `IntStream` with the `char` values of the
    `String`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Random.ints()`, `Random.doubles()`, or `Random.longs()`: This returns `IntStream`,
    `DoubleStream`, and `LongStream`, respectively with pseudorandom values. You can
    specify the range of numbers between the random numbers or the number or random
    values that you want to obtain. For example, you can generate pseudorandom numbers
    between 10 and 20 using `new Random.ints(10,20)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `SplittableRandom` class: This class provides the same methods as the `Random`
    class to generate pseudorandom `int`, `double`, and `long` values, but is more
    suitable for parallel processing. You can check the Java API documentation to
    get the details of this class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Stream.concat()` method: This receives two streams as parameters and creates
    a new stream with the elements of the first stream followed by the elements of
    the second stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can generate streams from other sources, but we think they are not significant.
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most important characteristic of an intermediate operation is that they
    return another stream as their result. The objects of the input and output stream
    can be of a different type, but an intermediate operation always will generate
    a new stream. You can have zero or more intermediate operations in a stream. The
    most important intermediate operations provided by the `Stream` interface are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`distinct()`: This method returns a stream with unique values. All the repeated
    elements will be eliminated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter()`: This method returns a stream with the elements that meet certain
    criteria'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flatMap()`: This method is used to convert a stream of streams (for example,
    a stream of list, sets, and so on) in a single stream'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limit()`: This method returns a stream that contains at the most the specified
    number of the original elements in the encounter order starting from the first
    element'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map()`: This method is used to transform the elements of a stream for one
    type to another'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peek()`: This method returns the same stream, but it executes some code; normally,
    it is used to write log messages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip()`: This method ignores the first elements (the concrete number is passed
    as parameter) of the stream'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sorted()`: This method sorts the elements of the stream'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terminal operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A terminal operation returns an object as a result. It never returns a stream.
    In general, all streams will end with a terminal operation that returns the final
    result of all the sequence of operations. The most important terminal operations
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`collect()`: This method provides a way to reduce the number of elements of
    the source stream organizing the elements of the stream in a data structure. For
    example, you want to group the elements of your stream by any criterion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count()`: This returns the number of elements of the stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max()`: This returns the maximum element of the stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min()`: This returns the minimum element of the stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduce()`: This method transforms the elements of the stream in a unique object
    that represents the stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forEach()`/`forEachOrdered()`: This methods apply an action to every element
    in the stream. The second method uses the order of the elements of the stream
    if the stream has a defined order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`findFirst()`/`findAny()`: This returns `1` or the first element of the stream,
    respectively, if they exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`anyMatch()`/`allMatch()`/`noneMatch()`: They receive a predicate as a parameter
    and return a Boolean value to indicate if any, all, or none elements of the stream
    match the predicate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`toArray()`: This method returns an array with the elements of the stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MapReduce versus MapCollect
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MapReduce is a programming model to process very large datasets in distributed
    environments with a lot of machines working in a cluster. It has two steps generally
    implemented by two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Map**: This filters and transforms the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduce**: This applies a summary operation in the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make this operation in a distributed environment, we have to split the data
    and then distribute over the machines of the cluster. This programming model has
    been used for a long time in the functional programming world. Google recently
    developed a framework based on this principle, and in the **Apache Foundation**,
    the **Hadoop** project is very popular as an open source implementation of this
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Java 8 with streams allows programmers to implement something very similar to
    this. The `Stream` interface defines intermediate operations (`map()`, `filter()`,
    `sorted()`, `skip()`, and so on) that can be considered as map functions, and
    it provides the `reduce()` method as a terminal operation whose main objective
    is to make a reduction of the elements of the stream as the reduction of the MapReduce
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea of the `reduce` operation is to create a new intermediate result
    based on a previous intermediate result and a stream element. The alternative
    way of reduction (also named mutable reduction) is to incorporate the new resulting
    item into the mutable container (for example, adding it into `ArrayList`). This
    kind of reduction is performed by the `collect()` operation, and we will name
    it as a **MapCollect** model.
  prefs: []
  type: TYPE_NORMAL
- en: We will see how to work with the MapReduce model in this chapter and how to
    work with the MapCollect model in [Chapter 8](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 8. Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model*.
  prefs: []
  type: TYPE_NORMAL
- en: The first example – a numerical summarization application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common needs when you have a big set of data is to process its
    elements to measure certain characteristics. For example, if you have a set with
    the products purchased in a shop, you can count the number of products you have
    sold, the number of units per product you have sold, or the average amount that
    each customer spent on it. We have named that process **numerical summarization**.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to use streams to obtain some measures of the
    **Bank Marketing** dataset of the **UCI Machine Learning Repository** that you
    can download from [http://archive.ics.uci.edu/ml/datasets/Bank+Marketing](http://archive.ics.uci.edu/ml/datasets/Bank+Marketing).
    Specifically, we have used the `bank-additional-full.csv` file. This dataset stores
    information about marketing campaigns of a Portuguese banking institution.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other chapters, in this case, we explain the concurrent version using
    streams and then how to implement a serial equivalent version to verify that concurrency
    improves performance with streams too. Take into account that concurrency is transparent
    for the programmer, as we mentioned in the introduction of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The concurrent version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our numerical summarization application is very simple. It has the following
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Record`: This class defines the internal structure of every record of the
    file. It defines the 21 attributes of every record and the corresponding `get()`
    and `set()` method to establish their values. Its code is very simple, so it won''t
    be included in the book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ConcurrentDataLoader`: This class will load the `bank-additional-full.csv`
    file with the data and convert it to a list of `Record` objects. We will use streams
    to load the data and make the conversion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ConcurrentStatistics`: This class implements the operations that we will use
    to make the calculations over the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ConcurrentMain`: This class implements the `main()` method to call the operations
    of the `ConcurrentStatistics` class and measure its execution time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's describe the last three classes in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The ConcurrentDataLoader class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `ConcurrentDataLoader` class implements the `load()` method that loads
    the file with the Bank Marketing dataset and converts it to a list of `Record`
    objects. First, we use the method `readAllLines()` of the `Files` method to load
    the file and convert its contents in a list of `String` objects. Every line of
    the file will be converted in an element of the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we apply the necessary operations to the stream to get the list of `Record`
    objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The operations we use are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`parallelStream()`: We create a parallel stream to process all the lines of
    the file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip(1)`: We ignore the first item of the stream; in this case, the first
    line of the file, which contains the headers of the file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map (l → l.split(";"))`: We convert each string in a `String[]` array dividing
    the line by the `;` character. We use a lambda expression where `l` represents
    the input parameter and `l.split()` will generate the array of strings. We call
    this method in a stream of strings, and it will generate a stream of `String[]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map(t → new Record(t))`: We convert each array of string in a `Record` object
    using the constructor of the `Record` class. We use a lambda expression where
    `t` represents the array of strings. We call this method in a stream of `String[]`,
    and we generate a stream of `Record` objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`collect(Collectors.toList())`: This method converts the stream into a list.
    We will work with the `collect` method in more detail in [Chapter 8](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 8. Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see, we have made the transformation in a compact, elegant, and
    concurrent way without the utilization of any thread, task, or framework. Finally,
    we return the list of `Record` objects, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The ConcurrentStatistics class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ConcurrentStatistics` class implements the methods that make the calculus
    over the data. We have seven different operations to obtain information about
    the dataset. Let's describe each of them.
  prefs: []
  type: TYPE_NORMAL
- en: Job information from subscribers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main objective of this method is to obtain the number of people per job
    type (field job) for those people who have subscribed a bank deposit (field subscribe
    equals to `yes`).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the source code of this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The method receives the list of `Record` objects as input parameters. First,
    we use a stream to obtain a `ConcurrentMap<String, List<Record>>` object where
    there are different job types and the list includes the records of each job type.
    This stream starts with the `parallelStream()`method to create a parallel stream.
    Then, we use the `filter()` method to select those `Record` objects with the subscribe
    attribute to `yes`. Finally, we use the `collect()` method passing the `Collectors.groupingByConcurrent()`method
    to group the actual elements of the stream by the values of the job attribute.
    Take into account that the `groupingByConcurrent()` method is an unordered collector.
    The records collected into the list can be in arbitrary order, not in the original
    order (unlike the simple `groupingBy()` collector).
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the `ConcurrentMap` object, we use the `forEach()` method to write
    the information on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Age data from subscribers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main objective of this method is to obtain statistical information (maximum,
    minimum, and average values) from the age of the subscribers of a bank deposit
    (field subscribe equals to `yes`).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the source code of the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This method receives the list of `Record` objects as input parameters and uses
    a stream to get a `DoubleSummaryStatistics` object with the statistical information.
    First, we use the `parallelStream()` method to get a parallel stream. Then, we
    use the `filter()` method to obtain the subscribers of a bank deposit. Finally,
    we use the `collect()` method with the `Collectors.summarizingDouble()` parameter
    to obtain the `DoubleSummaryStatistics` object. This class implements the `DoubleConsumer`
    interface and collects statistical data of the values it receives in the `accept()`
    method. This `accept()` method is called internally by the `collect()` method
    of the stream. Java provides the `IntSummaryStatistics` and `LongSummaryStatistics`
    classes also to obtain statistical data from the `int` and `long` values. In this
    case, we use the `max()`, `min()`, and `average()`methods to obtain the maximum,
    minimum, and average values, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Marital data from subscribers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main objective of this method is to obtain the different marital status
    (field marital) of the subscribers of a bank deposit (field subscribe equals to
    `yes`).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the source code of the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The method receives the list of `Record` objects as input parameters and uses
    the `parallelStream()` method to get a parallel stream. Then, we use the `filter()`
    method to only get the subscribers of a bank deposit. Then, we use the `map()`
    method to obtain a stream of `String` objects with the marital status of all the
    subscribers. With the `distinct()` method, we take only the unique values, and
    with the `sorted()` method, we sort those values alphabetically. Finally, we are
    using `forEachOrdered()` to print the result. Be careful not to use `forEach()`
    here as it will print the results in no particular order, which would make the
    `sorted()` step useless. The `forEach()` operation is useful when the elements
    order is not important and may work much faster for parallel streams than `forEachOrdered()`.
  prefs: []
  type: TYPE_NORMAL
- en: Campaign data from nonsubscribers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the most common mistakes we have when we use streams is to try to reuse
    a stream. We will show you the consequences of this mistake with this method,
    the main objective of which is to obtain the maximum number of contacts (attribute
    campaign).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first version of the method is to try to reuse a stream. This is its source
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The method receives the list of `Record` objects as input parameters. First,
    we create an `IntStream` object using that list. With the `parallelStream()` method,
    we create a parallel stream. Then, we use the `filter()` method to get the nonsubscribers
    and the `mapToInt()` method to convert a stream of `Record` objects in an `IntStream`
    object replacing each object by the value of the `getCampaign()` method.
  prefs: []
  type: TYPE_NORMAL
- en: We try to use that stream to get the maximum value, with the `max()` method,
    and the minimum value, with the `min()` method. If we execute this method, we
    will obtain `IllegalStateException` in the second call with the message **the
    stream has already been operated upon or closed**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can solve this problem creating two different streams, one to obtain the
    maximum and the other to obtain the minimum. This is the source code of this option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Another option is to use the `summaryStatistics()` method to obtain an `IntSummaryStatistics`
    object, as we showed in a previous method.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple data filter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The main objective of this method is to obtain the number of records that meet
    at most one of the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: The `defaultCredit` attribute takes the value `true`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `housing` attribute takes the value `false`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `loan` attribute takes the value `false`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One solution to implement this method is to implement a filter that checks
    whether the elements meet one of these conditions. You can implement other solutions
    with the `concat()` method provided by the `Stream` interface. This is the source
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This method receives the list of `Record` objects as input parameters. First,
    we create three streams with the elements that meet each of the conditions and
    then we use the `concat()` method to generate a single stream. The `concat()`
    method only creates a stream with the elements of the first stream followed by
    the elements of the second stream. For this reason, with the final stream, we
    use the `parallel()` method to convert the final stream in a parallel once, the
    `unordered()` method to get an unordered stream that will give us better performance
    in the `distinct()` method with parallel streams and the `distinct()` method to
    get only unique values, and the `count()` method to obtain the number of elements
    in the stream.
  prefs: []
  type: TYPE_NORMAL
- en: This is not the most optimal solution. We have used it to show you how the `concat()`
    and `distinct()` methods work. You can implement the same in a more optimal way
    using the following code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We create a stream of three predicates and reduce them via the `Predicate::or`
    operation to create the compound predicate, which is `true` when either of the
    input predicates is `true`. You can also use the `Predicate::and` reduction operation
    to create a predicate, which is `true` when all the input predicates are `true`.
  prefs: []
  type: TYPE_NORMAL
- en: Duration data from nonsubscribers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main objective of this method is to obtain the 10 longest phone calls (duration
    attribute) to people that finally didn't subscribe a bank deposit (field subscribe
    equals to `no`).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the source code of this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The method receives the list of `Record` objects as input parameters and uses
    the `parallelStream()`method to get a parallel stream. We use the `filter()` method
    to get the nonsubscribers. Then, we use the `sorted()` method and we pass a comparator.
    The comparator is created using the `Comparator.comparingInt()` static method.
    As we need to sort in reverse order (biggest duration first), we simply add the
    `reversed()` method to the created comparator. The `sorted()` method uses that
    comparator to compare and sort the elements of the stream, so we can obtain the
    elements sorted as we want.
  prefs: []
  type: TYPE_NORMAL
- en: Once the elements have been sorted, we get the first 10 results using the `limit()`
    method and print the result using the `forEachOrdered()` method.
  prefs: []
  type: TYPE_NORMAL
- en: People aged between 25 and 50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main objective of this method is to obtain the number of people in the file
    with an age between 25 and 50.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the source code of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The method receives the list of `Record` objects as input parameters and uses
    the `parallelStream()`method to get a parallel stream. Then, we use the `map()`
    method to transform the stream of `Record` objects into a stream of `int` values
    replacing each object by the value of its age attribute. Then, we use the `filter()`
    method to select only the people with an age of 25 and 50 and the `map()` method
    again to convert each value into `1`. Finally, we use the `reduce()` method to
    sum all those 1s and obtain the total number of people between 25 and 50\. The
    first parameter of the `reduce()` method is the identity value, and the second
    parameter is the operation that is used to obtain a single value from all the
    elements of the stream. In this case, we use the `Integer::sum` operation. The
    first sum is between the initial and the first value of the stream, the second
    sum between the result of the first sum and the second value of the stream, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: The ConcurrentMain class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `ConcurrentMain` class implements the `main()` method to test the `ConcurrentStatistic`
    class. First, we implement the `measure()` method that measures the execution
    time of a task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We use a map to store all the execution times of every method. We are going
    to execute each method 10 times to see how the execution time decreases after
    the first execution. Then, we include the `main()` method code. It uses the `measure()`
    method to measure the execution time of every method and repeats this process
    10 times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we write in the console all the execution times and the average execution
    time, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The serial version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this case, the serial version is almost equal to the concurrent one. We only
    replace all the calls to the `parallelStream()` method by calls to the `stream()`
    method to obtain a sequential stream instead of a parallel stream. We also have
    to delete the call to the `parallel()` method we use in one of the samples and
    change the call to the `groupingByConcurrent()` method to `groupingBy()`.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the two versions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have executed both versions of the operations to test if the use of parallel
    streams provides us with a better performance. We have executed them using the
    JMH framework ([http://openjdk.java.net/projects/code-tools/jmh/](http://openjdk.java.net/projects/code-tools/jmh/))
    that allows you to implement micro benchmarks in Java. Using a framework for benchmarking
    is a better solution that simply measures time using methods such as `currentTimeMillis()`
    or `nanoTime()`. We have executed them 10 times in a computer with a four-core
    processor and calculated the medium execution time of those 10 times. Take into
    account that we have implemented a special class to execute the JMH tests. You
    can find these classes in the `com.javferna.packtpub.mastering.numericalSummarization.benchmark`
    package of the source code. These are the results in milliseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Operation | Sequential streams | Parallel streams |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Job info | 13.704 | 9.550 |'
  prefs: []
  type: TYPE_TB
- en: '| Age info | 7.218 | 5.512 |'
  prefs: []
  type: TYPE_TB
- en: '| Marital info | 8.551 | 6.783 |'
  prefs: []
  type: TYPE_TB
- en: '| Multiple filter | 27.002 | 23.668 |'
  prefs: []
  type: TYPE_TB
- en: '| Multiple filter with predicate | 9.413 | 6.963 |'
  prefs: []
  type: TYPE_TB
- en: '| Duration data | 41.762 | 23.641 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of contacts | 22.148 | 13.059 |'
  prefs: []
  type: TYPE_TB
- en: '| People between 25 and 50 | 9.102 | 6.014 |'
  prefs: []
  type: TYPE_TB
- en: 'We can see how parallel streams always get a better performance than the serial
    streams. This is the speed-up for all the examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Operation | Speed-up |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Job info | 1.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Age info | 1.25 |'
  prefs: []
  type: TYPE_TB
- en: '| Marital info | 1.16 |'
  prefs: []
  type: TYPE_TB
- en: '| Multiple filter | 1.08 |'
  prefs: []
  type: TYPE_TB
- en: '| Duration data | 1.51 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of contacts | 1.64 |'
  prefs: []
  type: TYPE_TB
- en: '| People between 25 and 50 | 1.37 |'
  prefs: []
  type: TYPE_TB
- en: The second example – an information retrieval search tool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'According to Wikipedia ([https://en.wikipedia.org/wiki/Information_retrieval](https://en.wikipedia.org/wiki/Information_retrieval)),
    **information retrieval** is:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The activity obtaining information resources relevant to an information need
    from a collection of information resources."'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Usually, the information resources are a collection of documents and the information
    need is a set of words, which summarizes our need. To make a fast search over
    the document collection, we use a data structure named **inverted index**. It
    stores all the words of the document collection, and for each word, a list of
    the documents that contains that word. In [Chapter 4](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 4. Getting Data from the Tasks – The Callable and Future Interfaces"),
    *Getting Data from the Tasks – The Callable and Future Interfaces*, you constructed
    an inverted index of a document collection constructed with the Wikipedia pages
    with information about movies to construct a set of 100,673 documents. We have
    converted each Wikipedia page in a text file. This inverted index is stored in
    a text file where each line contains the word, its document frequency, and all
    the documents in which the word appears with the `tfxidf` attribute of the word
    in the document. The documents are sorted by the value of the `tfxidf` attribute.
    For example, a line of the file looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This line contains the `velankanni` word with a DF of `4`. It appears in the
    `18005302.txt` document with a `tfxidf` value of `10.13`, in the `20681361.txt`
    document with a `tfxidf` value of `10.13`, in the document `45672176.txt` with
    a `tfxidf` value of `10.13`, and in the `6592085.txt` document with a `tfxidf`
    value of `10.13`.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use the stream API to implement different versions
    of our search tool and obtain information about the inverted index.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to the reduction operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned earlier in this chapter, the `reduce` operation applies a summary
    operation to the elements of a stream to generate a single summary result. This
    single result can be of the same type as the elements of the stream of an other
    type. A simple example of the `reduce` operation is calculating the sum of a stream
    of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The stream API provides the `reduce()` method to implement reduction operations.
    This method has the following three different versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`reduce(accumulator)`: This version applies the `accumulator` function to all
    the elements of the stream. There is no initial value in this case. It returns
    an `Optional` object with the final result of the `accumulator` function or an
    empty `Optional` object if the stream is empty. This `accumulator` function must
    be an `associative` function. It implements the `BinaryOperator` interface. Both
    parameters could be either the stream elements or the partial results returned
    by previous accumulator calls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduce(identity, accumulator)`: This version must be used when the final result
    and the elements of the stream has the same type. The identity value must be an
    identity value for the `accumulator` function. That is to say, if you apply the
    `accumulator` function to the identity value and any value `V`, it must return
    the same value `V: accumulator(identity,V)=V`. That identity value is used as
    the first result for the accumulator function and is the returned value if the
    stream has no elements. As in the other version, the accumulator must be an `associative`
    function that implements the `BinaryOperator` interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduce(identity, accumulator, combiner)`: This version must be used when the
    final result has a different type than the elements of the stream. The identity
    value must be an identity for the `combiner` function, that is to say, `combiner(identity,v)=v`.
    A `combiner` function must be compatible with the `accumulator` function, that
    is to say, `combiner(u,accumulator(identity,v))=accumulator(u,v)`. The `accumulator`
    function takes a partial result and the next element of the stream to generate
    a partial result, and the combiner takes two partial results to generate another
    partial result. Both functions must be associative, but in this case, the `accumulator`
    function is an implementation of the `BiFunction` interface and the `combiner`
    function is an implementation of the `BinaryOperator` interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `reduce()` method has a limitation. As we mentioned before, it must return
    a single value. You shouldn't use the `reduce()` method to generate a collection
    or a complex object. The first problem is performance. As the documentation of
    the stream API specifies, the `accumulator` function returns a new value every
    time it processes an element. If your `accumulator` function works with collections,
    every time it processes an element it creates a new collection, which is very
    inefficient. Another problem is that, if you work with parallel streams, all the
    threads will share the identity value. If this value is a mutable object, for
    example, a collection, all the threads will be working over the same collection.
    This does not comply with the philosophy of the `reduce()` operation. In addition,
    the `combiner()` method will always receive two identical collections (all the
    threads are working over only one collection), that also doesn't comply the philosophy
    of the `reduce()` operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to make a reduction that generates a collection or a complex object,
    you have the following two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Apply a mutable reduction with the `collect()` method. [Chapter 8](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 8. Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model*, explains in detail how to use this method in different situations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create the collection and use the `forEach()` method to fill the collection
    with the required values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, we will use the `reduce()` method to obtain information about
    the inverted index and the `forEach()` method to reduce the index to the list
    of relevant documents for a query.
  prefs: []
  type: TYPE_NORMAL
- en: The first approach – full document query
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our first approach, we will use all the documents associated with a word.
    The steps of this implementation of our search process are:'
  prefs: []
  type: TYPE_NORMAL
- en: We select in the inverted index the lines corresponding with the words of the
    query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We group all the document lists in a single list. If a document appears associated
    with two or more different words, we sum the `tfxidf` value of those words in
    the document to obtain the final `tfxidf` value of the document. If a document
    is only associated with one word, the `tfxidf` value of that word will be the
    final `tfxidf` value for that document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We sort the documents using its `tfxidf` value, from high to low.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We show to the user the 100 documents with a higher value of `tfxidf`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We have implemented this version in the `basicSearch()`method of the `ConcurrentSearch`
    class. This is the source code of the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive an array of string objects with the words of the query. First, we
    transform that array in a set. Then, we use a *try-with-resources* stream with
    the lines of the `invertedIndex.txt` file, which is the file that contains the
    inverted index. We use a *try-with-resources* so we don''t have to worry about
    opening or closing the file. The aggregate operations of the stream will generate
    a `QueryResult` object with the relevant documents. We use the following methods
    to obtain that list:'
  prefs: []
  type: TYPE_NORMAL
- en: '`parallel()`: First, we obtain a parallel stream to improve the performance
    of the search process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter()`: We select the lines that associated the word in the set with the
    words in the query. The `Utils.getWord()`method obtains the word of the line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flatMap()`: We convert the stream of strings where each string is a line of
    the inverted index in a stream of `Token` objects. Each token contains the `tfxidf`
    value of a word in a file. Of every line, we will generate as many tokens as files
    containing that word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forEach()`: We generate the `QueryResult` object adding every token with the
    `add()` method of that class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once we have created the `QueryResult` object, we create other streams to obtain
    the final list of results using the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`getAsList()`: The `QueryResult` object returns a list with the relevant documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stream()`: To create a stream to process the list'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sorted()`: To sort the list of documents by its `tfxidf` value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limit()`: To get the first 100 results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forEach()`: To process the 100 results and write the information in the screen'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's describe the auxiliary classes and methods used in the example.
  prefs: []
  type: TYPE_NORMAL
- en: The basicMapper() method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This method converts a stream of strings into a stream of `Token` objects.
    As we will describe later in detail, a token stores the `tfxidf` value of a word
    in a document. This method receives a string with a line of the inverted index.
    It splits the line into the tokens and generates as many `Token` objects as documents
    containing the word. This method is implemented in the `ConcurrentSearch` class.
    This is the source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: First, we create a `ConcurrentLinkedDeque` object to store the `Token` objects.
    Then, we split the string using the `split()` method and use the `stream()` method
    of the `Arrays` class to generate a stream. Skip the first element (which contains
    the information of the word) and process the rest of the tokens in parallel. For
    each element, we create a new `Token` object (we pass to the constructor the word
    and the token that has the `file:tfxidf` format) and add it to the stream. Finally,
    we return a stream using the `stream()` method of the `ConcurrenLinkedDeque` object.
  prefs: []
  type: TYPE_NORMAL
- en: The Token class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, this class stores the `tfxidf` value of a word in
    a document. So, it has three attributes to store this information, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor receives two strings. The first one contains the word, and
    the second one contains the file and the `tfxidf` attribute in the `file:tfxidf`
    format, so we have to process it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have added methods to obtain (not to set) the values of the three
    attributes and to convert an object to a string, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The QueryResult class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This class stores the list of documents relevant to a query. Internally, it
    uses a map to store the information of the relevant documents. The key is the
    name of the file that stores the document, and the value is a `Document` object
    that also contains the name of the file and the total `tfxidf` value of that document
    to the query, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the constructor of the class to indicate the concrete implementation
    of the `Map` interface we will use. We use a `ConcurrentHashMap` to the concurrent
    version and a `HashMap` in the serial version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The class includes the `append` method that inserts a token in the map, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We use the `computeIfAbsent()` method to create a new `Document` object if there
    is no `Document` object associated with the file or to obtain the corresponding
    one if already exists and add the `tfxidf` value of the token to the total `tfxidf`
    value of the document using the `addTfxidf()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we have included a method to obtain the map as a list, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `Document` class stores the name of the file as a string and the total `tfxidf`
    value as `DoubleAdder`. This class is a new feature of Java 8 and allows us to
    sum values to the variable from different threads without worrying about synchronization.
    It implements the `Comparable` interface to sort the documents by its `tfxidf`
    value, so the documents with biggest `tfxidf` will be first. Its source code is
    very simple, so it is not included.
  prefs: []
  type: TYPE_NORMAL
- en: The second approach – reduced document query
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first approach creates a new `Token` object per word and file. We have noted
    that common words, for example, `the`, take a lot of documents associated and
    a lot of them have low values of `tfxidf`. We have changed our mapper method to
    take into account only 100 files per word, so the number of `Token` objects generated
    will be smaller.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have implemented this version in the `reducedSearch()` method of the `ConcurrentSearch`
    class. This method is very similar to the `basicSearch()` method. It only changes
    the stream operations that generate the `QueryResult` object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now, we use the `limitedMapper()` method as function in the `flatMap()` method.
  prefs: []
  type: TYPE_NORMAL
- en: The limitedMapper() method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This method is similar to the `basicMapper()` method, but, as we mentioned
    earlier, we only take into account the first 100 documents associated with every
    word. As the documents are sorted by its `tfxidf` value, we are using the 100
    documents in which the word is more important, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The only difference with the `basicMapper()` method is the `limit(100)` call
    that takes the first 100 elements of the stream.
  prefs: []
  type: TYPE_NORMAL
- en: The third approach – generating an HTML file with the results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While working with a search tool using a web search engine (for example, Google),
    when you make a search, it returns you the results of your search (the 10 most
    important) and for every result the title of the document and a fragment of the
    document where the words you have searched for appears.
  prefs: []
  type: TYPE_NORMAL
- en: Our third approach to the search tool is based on the second approach but by
    adding a third stream to generate an HTML file with the results of the search.
    For every result, we will show the title of the document and three lines where
    one of the words introduced in the query appears. To implement this, you need
    access to the files that appears in the inverted index. We have stored them in
    a folder named `docs`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This third approach is implemented in the `htmlSearch()`method of the `ConcurrentSearch`
    class. The first part of the method to construct the `QueryResult` object with
    the 100 results is equal to the `reducedSearch()` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create the file to write the output and the HTML headers in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we include the stream that generates the results in the HTML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We have used the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`getAsList()` to get the list of relevant documents for the query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stream()` to generate a sequential stream. We can''t parallelize this stream.
    If we try to do it, the results in the final file won''t be sorted by the `tfxidf`
    value of the documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sorted()` to sort the results by its `tfxidf` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map()` to convert a `Result` object into a string with the HTML code for each
    result using the `ContentMapper` class. We will explain the details of this class
    later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forEach()` to write the `String` objects returned by the `map()` method in
    the file. The methods of the `Stream` object can''t throw a checked exception,
    so we have to include the try-catch block that will throw.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see the details of the `ContentMapper` class.
  prefs: []
  type: TYPE_NORMAL
- en: The ContentMapper class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ContentMapper` class is an implementation of the `Function` interface that
    converts a `Result` object in an HTML block with the title of the document and
    three lines that include one or more words of the query.
  prefs: []
  type: TYPE_NORMAL
- en: 'The class uses an internal attribute to store the query and implements a constructor
    to initialize that attribute, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The title of the document is stored in the first line of the file. We use a
    try-with-resources instruction and the `lines()` method of the `Files` class to
    create and stream `String` objects with the lines of the file and take the first
    one with the `findFirst()` to obtain the line as a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use a similar structure, but in this case, we use the `filter()` method
    to get only the lines that contain one or more words of the query, the `limit()`
    method to take three of those lines. Then, we use the `map()` method to add the
    HTML tags for a paragraph (`<p>`) and the `reduce()` method to complete the HTML
    code with the selected lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The fourth approach – preloading the inverted index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The three previous solutions have a problem when they are executed in parallel.
    As we mentioned earlier, parallel streams are executed using the common Fork/Join
    pool provided by the Java concurrency API. In [Chapter 6](part0041_split_000.html#173722-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 6. Optimizing Divide and Conquer Solutions – The Fork/Join Framework"),
    *Optimizing Divide and Conquer Solutions – The Fork/Join Framework*, you learned
    that you shouldn't use I/O operations as read or write data in a file inside the
    tasks. This is because when a thread has blocked reading or writing data from
    or to a file, the framework doesn't use the work-stealing algorithm. As we use
    a file as the source of our streams, we are penalizing our concurrent solution.
  prefs: []
  type: TYPE_NORMAL
- en: One solution to this problem is to read the data to a data structure and then
    create our streams from that data structure. Obviously, the execution time of
    this approach will be smaller when we compare it with the other approaches, but
    we want to compare the serial and concurrent versions to see (as we expect) that
    the concurrent version gives us better performance than the serial version. The
    bad part of this approach is that you need to have your data structure in memory,
    so you will need a big amount of memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'This fourth approach is implemented in the `preloadSearch()` method of the
    `ConcurrentSearch` class. This method receives the query as an `Array` of `String`
    and an object of the `ConcurrentInvertedIndex` class (we will see the details
    of this class later) with the data of the inverted index as parameters. This is
    the source code of this version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The `ConcurrentInvertedIndex` class has `List<Token>` to store all the `Token`
    objects read from the file. It has two methods, `get()` and `set()` for this list
    of elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in other approaches, we use two streams: the first one to get a `ConcurrentLinkedDeque`
    of `Result` objects with the whole list of results and the second one to write
    the results in the console. The second one doesn''t change over other versions,
    but the first one changes. We use the following methods in this stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '`getIndex()`: First, we obtain the list of `Token` objects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parallelStream()`: Then, we create a parallel stream to process all the elements
    of the list'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter()`: We select the token associated with the words in the query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forEach()`: We process the list of tokens adding them to the `QueryResult`
    object using the `append()` method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ConcurrentFileLoader class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `ConcurrentFileLoader` class loads into memory the contents of the `invertedIndex.txt`
    file with the information of the inverted index. It provides a static method named
    `load()` that receives a path with the route of the file where the inverted index
    is stored and returns an `ConcurrentInvertedIndex` object. We have the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We open the file using a *try-with-resources* structure and create a stream
    to process all the lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the following methods in the stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '`parallel()`: We convert the stream into a parallel one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flatMap()`: We convert the line into a stream of `Token` objects using the
    `limitedMapper()` method of the `ConcurrentSearch` class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forEach()`: We process the list of `Token` objects adding them to a `ConcurrentLinkedDeque`
    object using the `add()` method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we convert the `ConcurrentLinkedDeque` object into `ArrayList` and
    set it in the `InvertedIndex` object using the `setIndex()` method.
  prefs: []
  type: TYPE_NORMAL
- en: The fifth approach – using our own executor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To go further with this example, we''re going to test another concurrent version.
    As we mentioned in the introduction of this chapter, parallel streams use the
    common Fork/Join pool introduced in Java 8\. However, we can use a trick to use
    our own pool. If we execute our method as a task of the Fork/Join pool, all the
    operations of the stream will be executed in the same Fork/Join pool. To test
    this functionality, we have added the `executorSearch()` method to the `ConcurrentSearch`
    class. This method receives the query as an array of `String` objects as a parameter,
    the `InvertedIndex` object, and a `ForkJoinPool` object. This is the source code
    of this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We execute the content of the method, with its two streams, as a task in the
    Fork/Join pool using the `submit()` method, and wait for its finalization using
    the `join()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Getting data from the inverted index – the ConcurrentData class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have implemented some methods to get information about the inverted index
    using the `reduce()` method in the `ConcurrentData` class.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the number of words in a file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first method calculates the number of words in a file. As we mentioned
    earlier in this chapter, the inverted index stores the files in which a word appears.
    If we want to know the words that appear in a file, we have to process all the
    inverted index. We have implemented two versions of this method. The first one
    is implemented in `getWordsInFile1()`. It receives the name of the file and the
    `InvertedIndex` object as parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we get the list of `Token` objects using the `getIndex()` method
    and create a parallel stream using the `parallelStream()` method. Then, we filter
    the tokens associated with the file using the `filter()` method, and finally,
    we count the number of words associated with that file using the `count()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have implemented another version of this method using the `reduce()` method
    instead of the `count()` method. It''s the `getWordsInFile2()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The start of the sequence of operations is the same as the previous one. When
    we have obtained the stream of `Token` objects with the words of the file, we
    use the `mapToInt()` method to convert that stream into a stream of `1` and then
    the `reduce()` method to sum all the `1` numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the average tfxidf value in a file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have implemented the `getAverageTfxidf()` method that calculates the average
    `tfxidf` value of the words of a file in the collection. We have used here the
    `reduce()` method to show how it works. You can use other methods here with better
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We use two streams. The first one calculates the number of words in a file
    and has the same source code as the `getWordsInFile2()` method. The second one
    calculates the total `tfxidf` value of all the words in the file. We use the same
    methods to get the stream of `Token` objects with the words in the file and then
    we use the `reduce` method to sum the `tfxidf` value of all the words. We pass
    the following three parameters to the `reduce()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '`O`: This is passed as the identity value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(n,t) -> n+t.getTfxidf()`: This is passed as the `accumulator` function. It
    receives a `double` number and a `Token` object and calculates the sum of the
    number and the `tfxidf` attribute of the token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(n1,n2) -> n1+n2`: This is passed as the `combiner` function. It receives
    two numbers and calculates their sum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the maximum and minimum tfxidf values in the index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have also used the `reduce()` method to calculate the maximum and minimum
    `tfxidf` values of the inverted index in the `maxTfxidf()` and `minTfxidf()` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The method receives the `ConcurrentInvertedIndex` as a parameter. We use the
    `getIndex()` to obtain the list of `Token` objects. Then, we use the `parallelStream()`
    method to create a parallel stream over the list and the `reduce()` method to
    obtain the `Token` with the biggest `tfxidf`. In this case, we use the `reduce()`
    method with two parameters: an identity value and an `accumulator` function. The
    identity value is a `Token` object. We don''t care about the word and the file
    name, but we initialize its `tfxidf` attribute with the value `0`. Then, the `accumulator`
    function receives two `Token` objects as parameters. We compare the `tfxidf` attribute
    of both objects and return the one with greater value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `minTfxidf()` method is very similar, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The main difference is that in this case, the identity value is initialized
    with a very high value for the `tfxidf` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: The ConcurrentMain class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To test all the methods explained in the previous sections, we have implemented
    the `ConcurrentMain` class that implements the `main()` method to launch our tests.
    In these tests, we have used the following three queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`query1`, with the words `james` and `bond`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query2`, with the words `gone`, `with`, `the`, and `wind`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query3`, with the words `rocky`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have tested the three queries with the three versions of our search process
    measuring the execution time of each test. All the tests have a code similar to
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'To load the inverted index from a file to an `InvertedIndex` object, you can
    use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To create the `Executor` to use in the `executorSearch()` method, you can use
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The serial version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have implemented a serial version of this example with the `SerialSearch`,
    `SerialData`, `SerialInvertendIndex`, `SerialFileLoader`, and `SerialMain` classes.
    To implement that version, we have made the following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: Use sequential streams instead of parallel ones. You have to delete the use
    of the `parallel()` method to convert the streams in parallel or replace the method
    `parallelStream()` to create a parallel stream for the `stream()` method to create
    a sequential one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the `SerialFileLoader` class, use `ArrayList` instead of `ConcurrentLinkedDeque`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing the solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's compare the solutions of the serial and concurrent versions of all the
    methods we have implemented. We have executed them using the JMH framework ([http://openjdk.java.net/projects/code-tools/jmh/](http://openjdk.java.net/projects/code-tools/jmh/)),
    which allows you to implement micro benchmarks in Java. Using a framework for
    benchmarking is a better solution that simply measures time using methods such
    as `currentTimeMillis()` or `nanoTime()`. We have executed them 10 times in a
    computer with a four-core processor so a concurrent algorithm can become theoretically
    four times faster than a serial one. Take into account that we have implemented
    a special class to execute the JMH tests. You can find these classes in the `com.javferna.packtpub.mastering.irsystem.benchmark`
    package of the source code.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first query, with the words `james` and `bond`, these are the execution
    times obtained in milliseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | **Serial** | **Concurrent** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Basic search | 3516.674 | 3301.334 |'
  prefs: []
  type: TYPE_TB
- en: '| Reduced search | 3458.351 | 3230.017 |'
  prefs: []
  type: TYPE_TB
- en: '| HTML search | 3298.996 | 3298.632 |'
  prefs: []
  type: TYPE_TB
- en: '| Preload search | 153.414 | 105.195 |'
  prefs: []
  type: TYPE_TB
- en: '| Executor search | 154.679 | 102.135 |'
  prefs: []
  type: TYPE_TB
- en: 'For the second query, with the words `gone`, `with`, `the`, and `wind`, these
    are the execution times obtained in milliseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | **Serial** | **Concurrent** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Basic search | 3446.022 | 3441.002 |'
  prefs: []
  type: TYPE_TB
- en: '| Reduced search | 3249.930 | 3260.026 |'
  prefs: []
  type: TYPE_TB
- en: '| HTML search | 3299.625 | 3379.277 |'
  prefs: []
  type: TYPE_TB
- en: '| Preload search | 154.631 | 113.757 |'
  prefs: []
  type: TYPE_TB
- en: '| Executor search | 156.091 | 106.418 |'
  prefs: []
  type: TYPE_TB
- en: 'For the third query, with the words `rocky`, these are the execution times
    obtained in milliseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Serial | Concurrent |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Basic search | 3271.308 | 3219.990 |'
  prefs: []
  type: TYPE_TB
- en: '| Reduced search | 3318.343 | 3279.247 |'
  prefs: []
  type: TYPE_TB
- en: '| HTML search | 3323.345 | 3333.624 |'
  prefs: []
  type: TYPE_TB
- en: '| Preload search | 151.416 | 97.092 |'
  prefs: []
  type: TYPE_TB
- en: '| Executor search | 155.033 | 103.907 |'
  prefs: []
  type: TYPE_TB
- en: 'Finally, these are the average execution times in milliseconds for the methods
    that return information about the inverted index:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Serial | Concurrent |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `getWordsInFile1` | 131.066 | 81.357 |'
  prefs: []
  type: TYPE_TB
- en: '| `getWordsInFile2` | 132.737 | 84.112 |'
  prefs: []
  type: TYPE_TB
- en: '| `getAverageTfxidf` | 253.067 | 166.009 |'
  prefs: []
  type: TYPE_TB
- en: '| `maxTfxidf` | 90.714 | 66.976 |'
  prefs: []
  type: TYPE_TB
- en: '| `minTfxidf` | 84.652 | 68.158 |'
  prefs: []
  type: TYPE_TB
- en: 'We can draw the following conclusions:'
  prefs: []
  type: TYPE_NORMAL
- en: When we read the inverted index to obtain the list of relevant documents, we
    obtain worse execution times. In this case, the execution times between the concurrent
    and serial versions are very similar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we work with a preload version of the inverted index, concurrent versions
    of the algorithms give us better performance in all cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the methods that give us information about the inverted index, concurrent
    versions of the algorithms always give us better performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can compare the parallel and sequential streams for the three queries in
    this end using the speed-up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing the solutions](img/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, in our third approach, we generate an HTML web page with the results
    of the queries. These are the first results with the query `james bond`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing the solutions](img/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the query `gone with the wind`, these are the first results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing the solutions](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, these are the first results for the query `rocky`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing the solutions](img/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we were introduced to streams, a new feature introduced in
    Java 8 inspired by functional programming and got ready to work with the new lambda
    expressions. A stream is a sequence of data (it is not a data structure) that
    allows you to apply a sequence of operations in a sequential or concurrent way
    to filter, convert, sort, reduce, or organize those elements to obtain a final
    object.
  prefs: []
  type: TYPE_NORMAL
- en: You also learned the main characteristics of the streams that we have to take
    into account when we use streams in our sequential or concurrent applications.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we used streams in two samples. In the first sample, we used almost
    all the methods provided by the `Stream` interface to calculate statistical data
    of a big dataset. We used the Bank Marketing dataset of the UCI Machine Learning
    Repository with its 45,211 records. In the second sample, we implemented different
    approaches to a search application in an inverted index to obtain the most relevant
    documents to a query. This is one of the most common tasks in the information
    retrieval field. For this purpose, we used the `reduce()` method as the terminal
    operation of our streams.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue working with streams, but with more focus
    on the `collect()` terminal operation.
  prefs: []
  type: TYPE_NORMAL
