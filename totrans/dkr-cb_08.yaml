- en: Chapter 8. Docker Orchestration and Hosting Platforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Running applications with Docker Compose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Cluster with Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up CoreOS for Docker orchestration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a Project Atomic host
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing atomic update/rollback with Project Atomic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding more storage for Docker in Project Atomic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Cockpit for Project Atomic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling up and down in a Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up WordPress with a Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Running Docker on a single host may be good for the development environment,
    but the real value comes when we span multiple hosts. However, this is not an
    easy task. You have to orchestrate these containers. So, in this chapter, we'll
    cover some of the orchestration tools and hosting platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Inc. announced two such tools:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose ([https://docs.docker.com/compose](https://docs.docker.com/compose))
    to create apps consisting of multiple containers and Docker Swarm ([https://docs.docker.com/swarm/](https://docs.docker.com/swarm/))
    to cluster multiple Docker hosts. Docker Compose was previously called Fig ([http://www.fig.sh/](http://www.fig.sh/)).
  prefs: []
  type: TYPE_NORMAL
- en: CoreOS ([https://coreos.com/](https://coreos.com/)) created etcd ([https://github.com/coreos/etcd](https://github.com/coreos/etcd))
    for consensus and service discovery, fleet ([https://coreos.com/using-coreos/clustering](https://coreos.com/using-coreos/clustering))
    to deploy containers in a cluster, and flannel ([https://github.com/coreos/flannel](https://github.com/coreos/flannel))
    for overlay networking.
  prefs: []
  type: TYPE_NORMAL
- en: Google started Kubernetes ([http://kubernetes.io/](http://kubernetes.io/)) for
    Docker orchestration. Kubernetes provides mechanisms for application deployment,
    scheduling, updating, maintenance, and scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Red Hat launched a container-specific operating system called Project Atomic
    ([http://www.projectatomic.io/](http://www.projectatomic.io/)), which can leverage
    the orchestration capabilities of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Even Microsoft announced a specialized operating system for Docker ([http://azure.microsoft.com/blog/2015/04/08/microsoft-unveils-new-container-technologies-for-the-next-generation-cloud/](http://azure.microsoft.com/blog/2015/04/08/microsoft-unveils-new-container-technologies-for-the-next-generation-cloud/)).
  prefs: []
  type: TYPE_NORMAL
- en: Apache Mesos ([http://mesos.apache.org/](http://mesos.apache.org/)), which provides
    resource management and scheduling across entire datacenter and cloud environments,
    also added support for Docker ([http://mesos.apache.org/documentation/latest/docker-containerizer/](http://mesos.apache.org/documentation/latest/docker-containerizer/)).
  prefs: []
  type: TYPE_NORMAL
- en: VMware also launched the container-specific host VMware Photon ([http://vmware.github.io/photon/](http://vmware.github.io/photon/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is definitely a very interesting space, but the policy management tools
    of many orchestration engines do not make the lives of developers and operators
    easy. They have to learn different tools and formats when they move from one platform
    to another. It would be great if we could have a standard way to build and launch
    composite, multicontainer apps. The Project Atomic community seems to be working
    on one such platform-neutral specification called Nulecule ([https://github.com/projectatomic/nulecule/](https://github.com/projectatomic/nulecule/)).
    A good description about Nulecule is available at [http://www.projectatomic.io/blog/2015/05/announcing-the-nulecule-specification-for-composite-applications/](http://www.projectatomic.io/blog/2015/05/announcing-the-nulecule-specification-for-composite-applications/):'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Nulecule defines a pattern and model for packaging complex multi-container
    applications, referencing all their dependencies, including orchestration metadata,
    in a single container image for building, deploying, monitoring, and active management.
    Just create a container with a Nulecule file and the app will ''just work''. In
    the Nulecule spec, you define orchestration providers, container locations and
    configuration parameters in a graph, and the Atomic App implementation will piece
    them together for you with the help of Providers. The Nulecule specification supports
    aggregation of multiple composite applications, and it''s also container and orchestration
    agnostic, enabling the use of any container and orchestration engine."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: AtomicApp is a reference implementation ([https://github.com/projectatomic/atomicapp/](https://github.com/projectatomic/atomicapp/))
    of the Nulecule specification. It can be used to bootstrap container applications
    and to install and run them. AtomicApp currently has a limited number of providers
    (Docker, Kubernetes, OpenShift), but support for others will be added soon.
  prefs: []
  type: TYPE_NORMAL
- en: On a related note, the CentOS community is building a CI environment, which
    will take advantage of Nulecule and AtomicApp. For further information, visit
    [http://wiki.centos.org/ContainerPipeline](http://wiki.centos.org/ContainerPipeline).
  prefs: []
  type: TYPE_NORMAL
- en: All of the preceding tools and platforms need separate chapters for themselves.
    In this chapter, we'll explore Compose, Swarm, CoreOS, Project Atomic, and Kubernetes
    briefly.
  prefs: []
  type: TYPE_NORMAL
- en: Running applications with Docker Compose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Compose ([http://docs.docker.com/compose/](http://docs.docker.com/compose/))
    is the native Docker tool to run the interdependent containers that make up an
    application. We define a multicontainer application in a single file and feed
    it to Docker Compose, which sets up the application. At the time of writing, Compose
    is still not production-ready. In this recipe, we'll once again use WordPress
    as a sample application to run.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Make sure you have Docker Version 1.3 or later installed on the system. To
    install Docker Compose, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create a directory for the application, and within it create `docker-compose.yml`
    to define the app:![How to do it…](../Images/image00378.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have taken the preceding example from the official WordPress Docker repo
    on Docker Hub ([https://registry.hub.docker.com/_/wordpress/](https://registry.hub.docker.com/_/wordpress/)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Within the app directory, run the following command to build the app:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once the build is complete, access the WordPress installation page from `http://localhost:8080`
    or `http://<host-ip>:8080`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker Compose downloads both the `mariadb` `wordpress` images, if not available
    locally from the official Docker registry. First, it starts the `db` container
    from the `mariadb` image; then it starts the `wordpress` container. Next, it links
    with the `db` container and exports the port to the host machine.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can even build images from the Dockerfile during the compose and then use
    it for the app. For example, to build the `wordpress` image, we can get the corresponding
    Dockerfile and other supporting file from within the application''s Compose directory
    and update the `docker-compose.yml` file in a similar manner as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more…](../Images/image00379.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can start, stop, rebuild, and get the status of the app. Visit its documentation
    on the Docker website.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Docker Compose YAML file reference at [http://docs.docker.com/compose/yml/](http://docs.docker.com/compose/yml/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Docker Compose command-line reference at [http://docs.docker.com/compose/cli/](http://docs.docker.com/compose/cli/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Docker Compose GitHub repository at [https://github.com/docker/compose](https://github.com/docker/compose)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up cluster with Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Swarm ([http://docs.docker.com/swarm/](http://docs.docker.com/swarm/))
    is native clustering to Docker. It groups multiple Docker hosts into a single
    pool in which one can launch containers. In this recipe, we'll use Docker Machine
    ([http://docs.docker.com/machine/](http://docs.docker.com/machine/)) to set up
    a Swarm cluster. At the time of writing, Swarm is still not production-ready.
    If you recall, we used Docker Machine to set up a Docker host on Google Compute
    Engine in [Chapter 1](part0015.xhtml#aid-E9OE2 "Chapter 1. Introduction and Installation"),
    *Introduction and Installation*. To keep things simple, here we'll use VirtualBox
    as the backend for Docker Machine to configure hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Install VirtualBox on your system ([https://www.virtualbox.org/](https://www.virtualbox.org/)).
    Instructions to configure VirtualBox are outside the scope of this book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download and set up Docker Machine. In Fedora x86_64, run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the Swarm discovery service, we first need to create a Swarm token to
    identify our cluster uniquely. Other than the default hosted discovery service,
    Swarm supports different types of discovery services such as etcd, consul, and
    zookeeper. For more details, please visit [https://docs.docker.com/swarm/discovery/](https://docs.docker.com/swarm/discovery/).
    To create a token using the default hosted discovery service, we''ll first set
    up the Docker host using Docker Machine on a VM and then get the token:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To access the Docker we just created from your local Docker client, run the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the token, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the token created in the preceding step, set up Swarm master:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, let''s create two Swarm nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, connect to Docker Swarm from your local Docker client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Swarm APIs are compatible with Docker client APIs. Let''s run the `docker info`
    command to see Swarm''s current configuration/setup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it…](../Images/image00380.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we have three nodes in the cluster: one master and two nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the unique token we got from the hosted discovery service, we registered
    the master and nodes in a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding `docker info` output, we also scheduled policy (strategy) and
    filters. More information on these can be found at [https://docs.docker.com/swarm/scheduler/strategy/](https://docs.docker.com/swarm/scheduler/strategy/)
    and [https://docs.docker.com/swarm/scheduler/filter/](https://docs.docker.com/swarm/scheduler/filter/).
    These define where the container will run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is active development happening to integrate Docker Swarm and Docker Compose
    so that we point and compose the app to the Swarm cluster. The app will then start
    on the cluster. Visit [https://github.com/docker/compose/blob/master/SWARM.md](https://github.com/docker/compose/blob/master/SWARM.md)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Swarm documentation on the Docker website at [https://docs.docker.com/swarm/](https://docs.docker.com/swarm/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swarm's GitHub repository at [https://github.com/docker/swarm](https://github.com/docker/swarm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up CoreOS for Docker orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CoreOS ([https://coreos.com/](https://coreos.com/)) is a Linux distribution
    that has been rearchitected to provide the features needed to run modern infrastructure
    stacks. It is Apache 2.0 Licensed. It has a product called CoreOS Managed Linux
    ([https://coreos.com/products/managed-linux/](https://coreos.com/products/managed-linux/))
    for which the CoreOS team provides commercial support.
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, CoreOS provides platforms to host a complete applications stack.
    We can set up CoreOS on different cloud providers, bare metal, and in the VM environment.
    Let''s look at the building blocks of CoreOS:'
  prefs: []
  type: TYPE_NORMAL
- en: etcd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container runtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Systemd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fleet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s discuss each in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**etcd**: From the GitHub page of etcd ([https://github.com/coreos/etcd/#etcd](https://github.com/coreos/etcd/#etcd)).
    `etcd` is a highly available key-value store for shared configuration and service
    discovery. It is inspired by Apache ZooKeeper and doozer with a focus on being:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simple**: Curl-able user-facing API (HTTP plus JSON)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secure**: Optional SSL client certificate authentication'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fast**: Benchmark of 1,000s of writes per instance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reliable**: Proper distribution using Raft'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is written in Go and uses the Raft consensus algorithm ([https://raftconsensus.github.io/](https://raftconsensus.github.io/))
    to manage a highly available replicated log. etcd can be used independent of CoreOS.
    We can:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up a single or multinode cluster. More information on this can be found
    at [https://github.com/coreos/etcd/blob/master/Documentation/clustering.md](https://github.com/coreos/etcd/blob/master/Documentation/clustering.md).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access using CURL and different libraries, found at [https://github.com/coreos/etcd/blob/master/Documentation/libraries-and-tools.md](https://github.com/coreos/etcd/blob/master/Documentation/libraries-and-tools.md).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In CoreOS, `etcd` is meant for the coordination of clusters. It provides a mechanism
    to store configurations and information about services in a consistent way.
  prefs: []
  type: TYPE_NORMAL
- en: '**Container runtime**: CoreOS supports Docker as a container runtime environment.
    In December 2014, CoreOS announced a new container runtime Rocket ([https://coreos.com/blog/rocket/](https://coreos.com/blog/rocket/)).
    Let''s restrict our discussion to Docker, which is currently installed on all
    CoreOS machines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**systemd**: `systemd` is an init system used to start, stop, and manage processes.
    In CoreOS, it is used to:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launch Docker containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Register services launched by containers to etcd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Systemd manages unit files. A sample unit file looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This unit file starts the Docker daemon with the command mentioned in `ExecStart`
    on Fedora 21\. The Docker daemon will start after the `network target` and `docker
    socket` services. `docker socket` is a prerequisite for the Docker daemon to start.
    Systemd targets are ways to group processes so that they can start at the same
    time. `multi-user` is one of the targets with which the preceding unit file is
    registered. For more details, you can look at the upstream documentation of Systemd
    at [http://www.freedesktop.org/wiki/Software/systemd/](http://www.freedesktop.org/wiki/Software/systemd/).
  prefs: []
  type: TYPE_NORMAL
- en: '**Fleet**: Fleet ([https://coreos.com/using-coreos/clustering/](https://coreos.com/using-coreos/clustering/))
    is the cluster manager that controls `systemd` at the cluster level. systemd unit
    files are combined with some Fleet-specific properties to achieve the goal. From
    the Fleet documentation ([https://github.com/coreos/fleet/blob/master/Documentation/architecture.md](https://github.com/coreos/fleet/blob/master/Documentation/architecture.md)):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*"Every system in the fleet cluster runs a single `fleetd` daemon. Each daemon
    encapsulates two roles: the *engine* and the *agent*. An engine primarily makes
    scheduling decisions while an agent executes units. Both the engine and agent
    use the *reconciliation model*, periodically generating a snapshot of ''current
    state'' and ''desired state'' and doing the necessary work to mutate the former
    towards the latter."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`etcd` is the sole datastore in a `fleet` cluster. All persistent and ephemeral
    data is stored in `etcd`; unit files, cluster presence, unit state, and so on.
    `etcd` is also used for all internal communication between fleet engines and agents.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we know of all the building blocks of CoreOS. Let's try out CoreOS on our
    local system/laptop. To keep things simple, we will use Vagrant to set up the
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Install VirtualBox on the system ([https://www.virtualbox.org/](https://www.virtualbox.org/))
    and Vagrant ([https://www.vagrantup.com/](https://www.vagrantup.com/)). The instructions
    to configure both of these things are outside the scope of this book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clone the `coreos-vagrant` repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy the sample file `user-data.sample` to `user-data` and set up the token
    to bootstrap the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When we configure the CoreOS cluster with more than one node, we need a token
    to bootstrap the cluster to select the initial etcd leader. This service is provided
    free by the CoreOS team. We just need to open `https://discovery.etcd.io/new`
    in the browser to get the token and update it within the `user-data` file as follows:![Getting
    ready](../Images/image00381.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Copy `config.rb.sample` to `config.rb` and make changes to the following line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It should now look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This will ask Vagrant to set up three node clusters. By default, Vagrant is
    configured to get the VM images from the alpha release. We can change it to beta
    or stable by updating the `$update_channel` parameter in Vagrantfile. For this
    recipe, I chose stable.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Run the following command to set up the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, check the status, using the command shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](../Images/image00382.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Log in to one of the VMs using SSH, look at the status of services, and list
    the machines in the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it…](../Images/image00383.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Create a service unit file called `myapp.service` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now submit the service for scheduling and start the service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it…](../Images/image00384.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, our service has started on one of the nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vagrant uses the cloud configuration file (`user-data`) to boot the VMs. As
    they have the same token to bootstrap the cluster, they select the leader and
    start operating. Then, with `fleetctl`, which is the fleet cluster management
    tool, we submit the unit file for scheduling, which starts on one of the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the cloud configuration file in this recipe, we can start `etcd` and `fleet`
    on all the VMs. We can choose to run `etcd` just on selected nodes and then configure
    worker nodes running `fleet` to connect to etcd servers. This can be done by setting
    the cloud configuration file accordingly. For more information, please visit [https://coreos.com/docs/cluster-management/setup/cluster-architectures/](https://coreos.com/docs/cluster-management/setup/cluster-architectures/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With `fleet`, we can configure services for high availability. For more information,
    take a look at [https://coreos.com/docs/launching-containers/launching/fleet-unit-files/](https://coreos.com/docs/launching-containers/launching/fleet-unit-files/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though your service is running on the host, you will not be able to reach it
    from the outside world. You will need to add some kind of router and wildcard
    DNS configuration to reach your service from the outside world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CoreOS documentation for more details at [https://coreos.com/docs/](https://coreos.com/docs/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The visualization of RAFT consensus algorithm at [http://thesecretlivesofdata.com/raft](http://thesecretlivesofdata.com/raft)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to configure the cloud config file at [https://coreos.com/docs/cluster-management/setup/cloudinit-cloud-config/](https://coreos.com/docs/cluster-management/setup/cloudinit-cloud-config/)
    and [https://coreos.com/validate/](https://coreos.com/validate/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation on systemd at [https://coreos.com/docs/launching-containers/launching/getting-started-with-systemd/](https://coreos.com/docs/launching-containers/launching/getting-started-with-systemd/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to launch containers with fleet at [https://coreos.com/docs/launching-containers/launching/launching-containers-fleet/](https://coreos.com/docs/launching-containers/launching/launching-containers-fleet/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a Project Atomic host
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Project Atomic facilitates application-centric IT architecture by providing
    an end-to-end solution to deploy containerized applications quickly and reliably,
    with atomic update and rollback for the application and host alike.
  prefs: []
  type: TYPE_NORMAL
- en: This is achieved by running applications in containers on a Project Atomic host,
    which is a lightweight operating system specially designed to run containers.
    The hosts can be based on Fedora, CentOS, or Red Hat Enterprise Linux.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will elaborate on the building blocks of the Project Atomic host.
  prefs: []
  type: TYPE_NORMAL
- en: '**OSTree and rpm-OSTree**: OSTree ([https://wiki.gnome.org/action/show/Projects/OSTree](https://wiki.gnome.org/action/show/Projects/OSTree))
    is a tool to manage bootable, immutable, and versioned filesystem trees. Using
    this, we can build client-server architecture in which the server hosts an OSTree
    repository and the client subscribed to it can incrementally replicate the content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: rpm-OSTree is a system to decompose RPMs on the server side into the OSTree
    repository to which the client can subscribe and perform updates. With each update,
    a new root is created, which is used for the next reboot. During updates, `/etc`
    is rebased and `/var` is untouched.
  prefs: []
  type: TYPE_NORMAL
- en: '**Container runtime**: As of now Project Atomic only supports Docker as container
    runtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**systemd**: As we saw in earlier recipes, systemd is a new init system. It
    also helps to set up SELinux policies to containers for complete multitenant security
    and to control Cgroups policies, which we looked in at [Chapter 1](part0015.xhtml#aid-E9OE2
    "Chapter 1. Introduction and Installation"), *Introduction and Installation*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project Atomic uses Kubernetes ([http://kubernetes.io/](http://kubernetes.io/))
    for application deployment over clusters of container hosts. Project Atomic can
    be installed on bare metal, cloud providers, VMs, and so on. In this recipe, let's
    see how we can install it on a VM using virt-manager on Fedora.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Download the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: I have downloaded the beta image for Fedora 22 Cloud image *For Containers*.
    You should look for the latest cloud image *For Containers* at [https://getfedora.org/en/cloud/download/](https://getfedora.org/en/cloud/download/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Uncompress this image by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We downloaded the cloud image that does not have any password set for the default
    user `fedora`. While booting the VM, we have to provide a cloud configuration
    file through which we can customize the VM. To do this, we need to create two
    files, `meta-data` and `user-data`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we need to provide the complete SSH public key. We then
    need to create an ISO image consisting of these files, which we will use to boot
    to the VM. As we are using a cloud image, our setting will be applied to the VM
    during the boot process. This means the hostname will be set to `atomichost`,
    the password will be set to `atomic`, and so on. To create the ISO, run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Start virt-manager.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **New Virtual Machine** and then import the existing disk image. Enter
    the image path of the Project Atomic image we downloaded earlier. Select **OS
    type** as **Linux** and **Version** as **Fedora 20/Fedora 21 (or later)**, and
    click on **Forward**. Next, assign CPU and Memory and click on **Forward**. Then,
    give a name to the VM and select **Customize configuration** before install. Finally,
    click on **Finish** and review the details.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click on **Add Hardware**, and after selecting **Storage**, attach the
    ISO (`init.iso`) file we created to the VM and select **Begin Installation**:![How
    to do it…](../Images/image00385.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once booted, you can see that its hostname is correctly set and you will be
    able to log in through the password given in the cloud init file. The default
    user is `fedora` and password is `atomic` as set in the `user-data` file.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we took a Project Atomic Fedora cloud image and booted it using
    `virt-manager` after supplying the cloud init file.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After logging in, if you do file listing at `/`, you will see that most of the
    traditional directories are linked to `/var` because it is preserved across upgrades.![There's
    more…](../Images/image00386.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After logging in, you can run the Docker command as usual:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The virtual manager documentation at [https://virt-manager.org/documentation/](https://virt-manager.org/documentation/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More information on package systems, image systems, and RPM-OSTree at [https://github.com/projectatomic/rpm-ostree/blob/master/doc/background.md](https://github.com/projectatomic/rpm-ostree/blob/master/doc/background.md)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The quick-start guide on the Project Atomic website at [http://www.projectatomic.io/docs/quickstart/](http://www.projectatomic.io/docs/quickstart/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resources on cloud images at [https://www.technovelty.org//linux/running-cloud-images-locally.html](https://www.technovelty.org//linux/running-cloud-images-locally.html)
    and [http://cloudinit.readthedocs.org/en/latest/](http://cloudinit.readthedocs.org/en/latest/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to set up Kubernetes with an Atomic host at [http://www.projectatomic.io/blog/2014/11/testing-kubernetes-with-an-atomic-host/](http://www.projectatomic.io/blog/2014/11/testing-kubernetes-with-an-atomic-host/)
    and [https://github.com/cgwalters/vagrant-atomic-cluster](https://github.com/cgwalters/vagrant-atomic-cluster)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing atomic update/rollback with Project Atomic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get to the latest version or to roll back to the older version of Project
    Atomic, we use the `atomic host` command, which internally calls rpm-ostree.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Boot and log in to the Atomic host.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just after the boot, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: You will see details about one deployment that is in use now.
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](../Images/image00387.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To upgrade, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](../Images/image00388.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This changes and/or adds new packages. After the upgrade, we will need to reboot
    the system to use the new update. Let's reboot and see the outcome:![How to do
    it…](../Images/image00389.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can see, the system is now booted with the new update. The `*`, which
    is at the beginning of the first line, specifies the active build.
  prefs: []
  type: TYPE_NORMAL
- en: 'To roll back, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We will have to reboot again if we want to use older bits.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For updates, the Atomic host connects to the remote repository hosting the newer
    build, which is downloaded and used from the next reboot onwards until the user
    upgrades or rolls back. In the case rollback older build available on the system
    used after the reboot.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The documentation Project Atomic website, which can be found at [http://www.projectatomic.io/docs/os-updates/](http://www.projectatomic.io/docs/os-updates/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding more storage for Docker in Project Atomic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Atomic host is a minimal distribution and, as such, is distributed on a
    6 GB image to keep the footprint small. This is very less amount of storage to
    build and store lots of Docker images, so it is recommended to attach external
    storage for those operations.
  prefs: []
  type: TYPE_NORMAL
- en: By default, Docker uses `/var/lib/docker` as the default directory where all
    Docker-related files, including images, are stored. In Project Atomic, we use
    direct LVM volumes via the devicemapper backend to store Docker images and metadata
    in `/dev/atomicos/docker-data` and `/dev/atomicos/docker-meta` respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to add more storage, Project Atomic provides a helper script called `docker-storage-helper`
    to add an external disk into the existing LVM thin pool. Let''s look at the current
    available storage to Docker with the `docker info` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding more storage for Docker in Project Atomic](../Images/image00390.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the total data space is 2.96 GB and the total metadata space
    is 8.38 MB.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stop the VM, if it is running.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add an additional disk of the size you want to the Project Atomic VM. I have
    added 8 GB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Boot the VM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check whether the newly attached disk is visible to the VM or not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Check if the additional disk is available to the Atomic host VM:![How to do
    it…](../Images/image00391.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can see, the newly created 8 GB disk is available to the VM.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the newly attached disk is `/dev/sdb`, create a file called `/etc/sysconfig/docker-storage-setup`
    with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `docker-storage-setup` command to add `/dev/sdb` to the existing volume:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it…](../Images/image00392.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's look at the current available storage to Docker once again with the
    `docker info` command:![How to do it…](../Images/image00393.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can see, both the total data space and metadata space have increased.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The procedure is the same as extending any other LVM volume. We create a physical
    volume on the added disk, add that physical volume to the volume group, and then
    extend the LVM volumes. Since we are directly accessing the thin pool within Docker,
    we won't need to create or extend a filesystem or mount the LVM volumes.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the `DEVS` option, you can also add the `VG` option to the `/etc/sysconfig/docker-storage-setup`
    file to use a different volume group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can add more than one disk with the `DEVS` option.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a disk that is already part of the Volume Group has been mentioned with the
    `DEVS` option, then the `docker-storage-setup` script will exit, as the existing
    device has a partition and physical volume already created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `docker-storage-setup` script reserves 0.1 percent of the size for `meta-data`.
    This is why we saw an increase in the Metadata Space as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The documentation on the Project Atomic website at [http://www.projectatomic.io/docs/docker-storage-recommendation/](http://www.projectatomic.io/docs/docker-storage-recommendation/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supported filesystems with Project Atomic at [http://www.projectatomic.io/docs/filesystems/](http://www.projectatomic.io/docs/filesystems/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Cockpit for Project Atomic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cockpit ([http://cockpit-project.org/](http://cockpit-project.org/)) is a server
    manager that makes it easy to administer your GNU/Linux servers via a web browser.
    It can be used to manage the Project Atomic host as well. More than one host can
    be managed through one Cockpit instance. Cockpit does not come by default with
    the latest Project Atomic, and you will need to start it as a **Super Privileged
    Container** (**SPC**). SPCs are specially built containers that run with security
    turned off (`--privileged`); they turn off one or more of the namespaces or "volume
    mounts in" parts of the host OS into the container. For more details on SPC, refer
    to [https://developerblog.redhat.com/2014/11/06/introducing-a-super-privileged-container-concept/](https://developerblog.redhat.com/2014/11/06/introducing-a-super-privileged-container-concept/)
    and [https://www.youtube.com/watch?v=eJIeGnHtIYg](https://www.youtube.com/watch?v=eJIeGnHtIYg).
  prefs: []
  type: TYPE_NORMAL
- en: Because Cockpit runs as an SPC, it can access the resources needed to manage
    the Atomic host within the container.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Set up the Project Atomic host and log in to it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Run the following command to start the Cockpit container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it…](../Images/image00394.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Open the browser (`http://<VM IP>:9090`) and log in with the default user/password
    `fedora/atomic`. Once logged in, you can select the current host to manage. You
    will see a screen as shown here:![How to do it…](../Images/image00395.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we used the `atomic` command instead of the `docker` command to start
    the container. Let''s look at the Cockpit Dockerfile ([https://github.com/fedora-cloud/Fedora-Dockerfiles/blob/master/cockpit-ws/Dockerfile](https://github.com/fedora-cloud/Fedora-Dockerfiles/blob/master/cockpit-ws/Dockerfile))
    to see why we did that. In the Dockerfile you will see some instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: If you recall from [Chapter 2](part0022.xhtml#aid-KVCC1 "Chapter 2. Working
    with Docker Containers"), *Working with Docker Containers* and [Chapter 3](part0038.xhtml#aid-147LC1
    "Chapter 3. Working with Docker Images"), *Working with Docker Images*, we could
    assign metadata to images and containers using labels. `INSTALL`, `UNINSTALL`,
    and `RUN` are labels here. The `atomic` command is a command specific to Project
    Atomic, which reads those labels and performs operations. As the container is
    running as an SPC, it does not need port forwarding from host to container. For
    more details on the `atomic` command, please visit [https://developerblog.redhat.com/2015/04/21/introducing-the-atomic-command/](https://developerblog.redhat.com/2015/04/21/introducing-the-atomic-command/).
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can perform almost all administrator tasks from the GUI for the given system.
    You can manage Docker images/containers through this. You can perform operations
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Pulling an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting/stopping the containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also add other machines to the same Cockpit instance so that you manage
    them from one central location.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Cockpit documentation at [http://files.cockpit-project.org/guide/](http://files.cockpit-project.org/guide/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a Kubernetes cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes is an open source container orchestration tool across multiple nodes
    in the cluster. Currently, it only supports Docker. It was started by Google,
    and now developers from other companies are contributing to it. It provides mechanisms
    for application deployment, scheduling, updating, maintenance, and scaling. Kubernetes''
    auto-placement, auto-restart, auto-replication features make sure that the desired
    state of the application is maintained, which is defined by the user. Users define
    applications through YAML or JSON files, which we''ll see later in the recipe.
    These YAML and JSON files also contain the API Version (the `apiVersion` field)
    to identify the schema. The following is the architectural diagram of Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up a Kubernetes cluster](../Images/image00396.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/docs/architecture.png](https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/docs/architecture.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at some of the key components and concepts of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pods**: A pod, which consists of one or more containers, is the deployment
    unit of Kubernetes. Each container in a pod shares different namespaces with other
    containers in the same pod. For example, each container in a pod shares the same
    network namespace, which means they can all communicate through localhost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node/Minion**: A node, which was previously known as a minion, is a worker
    node in the Kubernetes cluster and is managed through master. Pods are deployed
    on a node, which has the necessary services to run them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: docker, to run containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kubelet, to interact with master
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: proxy (kube-proxy), which connects the service to the corresponding pod
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Master**: Master hosts cluster-level control services such as the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API server**: This has RESTful APIs to interact with master and nodes. This
    is the only component that talks to the etcd instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scheduler**: This schedules jobs in clusters, such as creating pods on nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replication controller**: This ensures that the user-specified number of
    pod replicas is running at any given time. To manage replicas with replication
    controller, we have to define a configuration file with the replica count for
    a pod.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Master also communicates with etcd, which is a distributed key-value pair. etcd
    is used to store the configuration information, which is used by both master and
    nodes. The watch functionality of etcd is used to notify the changes in the cluster.
    etcd can be hosted on master or on a different set of systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Services**: In Kubernetes, each pod gets its own IP address, and pods are
    created and destroyed every now and then based on the replication controller configuration.
    So, we cannot rely on a pod''s IP address to cater an app. To overcome this problem,
    Kubernetes defines an abstraction, which defines a logical set of pods and policies
    to access them. This abstraction is called a service. Labels are used to define
    the logical set, which a service manages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Labels**: Labels are key-value pairs that can be attached to objects like,
    using which we select a subset of objects. For example, a service can select all
    pods with the label `mysql`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volumes**: A volume is a directory that is accessible to the containers in
    a pod. It is similar to Docker volumes but not the same. Different types of volumes
    are supported in Kubernetes, some of which are EmptyDir (ephemeral), HostDir,
    GCEPersistentDisk, and NFS. Active development is happening to support more types
    of volumes. More details can be found at [https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/volumes.md](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/volumes.md).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes can be installed on VMs, physical machines, and the cloud. For the
    complete matrix, take a look at [https://github.com/GoogleCloudPlatform/kubernetes/tree/master/docs/getting-started-guides](https://github.com/GoogleCloudPlatform/kubernetes/tree/master/docs/getting-started-guides).
    In this recipe, we'll see how to install it on VMs, using Vagrant with VirtualBox
    provider. This recipe and the following recipes on Kubernetes, were tried on v0.17.0
    of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Install latest Vagrant >= 1.6.2 from [http://www.vagrantup.com/downloads.html](http://www.vagrantup.com/downloads.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the latest VirtualBox from [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads).
    Detailed instructions on how to set this up are outside the scope of this book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Run the following command to set up Kubernetes on Vagrant VMs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The bash script downloaded from the `curl` command, first downloads the latest
    Kubernetes release and then runs the `./kubernetes/cluster/kube-up.sh` bash script
    to set up the Kubernetes environment. As we have specified Vagrant as `KUBERNETES_PROVIDER`,
    the script first downloads the Vagrant images and then, using Salt ([http://saltstack.com/](http://saltstack.com/)),
    configures one master and one node (minion) VM. Initial setup takes a few minutes
    to run.
  prefs: []
  type: TYPE_NORMAL
- en: Vagrant creates a credential file in `~/.kubernetes_vagrant_auth` for authentication.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to `./cluster/kube-up.sh`, there are other helper scripts to perform
    different operations from the host machine itself. Make sure you are in the `kubernetes`
    directory, which was created with the preceding installation, while running the
    following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the list of nodes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the list of pods:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the list of services:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the list of replication controllers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Destroy the vagrant cluster:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then bring back the vagrant cluster:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: You will see some `pods`, `services`, and `replicationControllers` listed, as
    Kubernetes creates them for internal use.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting up the Vagrant environment at [https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/getting-started-guides/vagrant.md](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/getting-started-guides/vagrant.md)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kubernetes user guide at [https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/user-guide.md](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/user-guide.md)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes API conventions at [https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/api-conventions.md](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/api-conventions.md)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling up and down in a Kubernetes cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we mentioned that the replication controller ensures
    that the user-specified number of pod replicas is running at any given time. To
    manage replicas with the replication controller, we have to define a configuration
    file with the replica count for a pod. This configuration can be changed at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Make sure the Kubernetes setup is running as described in the preceding recipe
    and that you are in the `kubernetes` directory, which was created with the preceding
    installation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the `nginx` container with a replica count of 3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it…](../Images/image00397.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This will start three replicas of the `nginx` container. List the pods to get
    the status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the replication controller configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it…](../Images/image00398.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have a `my-nginx` controller, which has a replica count of
    3\. There is a replication controller for `kube-dns`, which we will explore in
    next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Request the replication controller service to scale down to replica of 1 and
    update the replication controller:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it…](../Images/image00399.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Get the list of pods to verify; you should see only one pod for `nginx`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We request the replication controller service running on master to update the
    replicas for a pod, which updates the configuration and requests nodes/minions
    to act accordingly to honor the resizing.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Get the services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![There''s more…](../Images/image00400.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we don't have any service defined for our `nginx` containers
    started earlier. This means that though we have a container running, we cannot
    access them from outside because the corresponding service is not defined.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting up the Vagrant environment at [https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/getting-started-guides/vagrant.md](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/getting-started-guides/vagrant.md)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kubernetes user guide at [https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/user-guide.md](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/user-guide.md)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up WordPress with a Kubernetes cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the WordPress example given in the Kubernetes GitHub
    ([https://github.com/GoogleCloudPlatform/kubernetes/tree/master/examples/mysql-wordpress-pd](https://github.com/GoogleCloudPlatform/kubernetes/tree/master/examples/mysql-wordpress-pd)).
    The given example requires some changes, as we'll be running it on the Vagrant
    environment instead of the default Google Compute engine. Also, instead of using
    the helper functions (for example, `<kubernetes>/cluster/kubectl.sh`), we'll log
    in to master and use the `kubectl` binary.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Make sure the Kubernetes cluster has been set up as described in the previous
    recipe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the `kubernetes` directory that was downloaded during the setup, you will
    find an examples directory that contains many examples. Let''s go to the `mysql-wordpress-pd`
    directory:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: These `.yaml` files describe pods and services for `mysql` and `wordpress` respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the pods files (`mysql.yaml` and `wordpress.yaml`), you will find the section
    on volumes and the corresponding `volumeMount` file. The original example assumes
    that you have access to Google Compute Engine and that you have the corresponding
    storage setup. For simplicity, we will not set up that and instead use ephemeral
    storage with the `EmptyDir` volume option. For reference, our `mysql.yaml` will
    look like the following:![Getting ready](../Images/image00401.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make the similar change to `wordpress.yaml`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With SSH, log in to the master node and look at the running pods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it…](../Images/image00402.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `kube-dns-7eqp5` pod consists of three containers: `etcd`, `kube2sky`,
    and `skydns`, which are used to configure an internal DNS server for service name
    to IP resolution. We''ll see it in action later in this recipe.'
  prefs: []
  type: TYPE_NORMAL
- en: The Vagrantfile used in this example is created so that the `kubernetes` directory
    that we created earlier is shared under `/vagrant` on VM, which means that the
    changes we made to the host system will be visible here as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the master node, create the `mysql` pod and check the running pods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it…](../Images/image00403.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, a new pod with the `mysql` name has been created and it is running
    on host `10.245.1.3`, which is our node (minion).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s create the service for `mysql` and look at all the services:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it…](../Images/image00404.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, a service named `mysql` has been created. Each service has a
    Virtual IP. Other than the `kubernetes` services, we see a service named `kube-dns`,
    which is used as the service name for the `kube-dns` pod we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to `mysql`, let''s create a pod for `wordpress`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'With this command, there are a few things happening in the background:'
  prefs: []
  type: TYPE_NORMAL
- en: The `wordpress` image gets downloaded from the official Docker registry and
    the container runs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, whenever a pod starts, information about all the existing services
    is exported as environment variables. For example, if we log in to the `wordpress`
    pod and look for `MYSQL`-specific environment variables, we will see something
    like the following:![How to do it…](../Images/image00405.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the WordPress container starts, it runs the `/entrypoint.sh` script, which
    looks for the environment variables mentioned earlier to start the service. [https://github.com/docker-library/wordpress/blob/master/docker-entrypoint.sh](https://github.com/docker-library/wordpress/blob/master/docker-entrypoint.sh).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the `kube-dns` service, PHP scripts of `wordpress` are able to the reserve
    lookup to proceed forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After starting the pod, the last step here is to set up the `wordpress` service.
    In the default example, you will see an entry like the following in the service
    file `(/vagrant/examples/mysql-wordpress-pd/mysql-service.yaml`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This has been written to keep in mind that this example will run on the Google
    Compute Engine. So it is not valid here. In place of that, we will need to make
    an entry like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We have replaced the load-balancer entry with the public IP of the node, which
    in our case is the IP address of the node (minion). So, the `wordpress` file would
    look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](../Images/image00406.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To start the `wordpress` service, run the following command from the master
    node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it…](../Images/image00407.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see here that our service is also available through the node (minion)
    IP.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify if everything works fine, we can install the links package on master
    by which we can browse a URL through the command line and connect to the public
    IP we mentioned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: With this, you should see the `wordpress` installation page.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we first created a `mysql` pod and service. Later, we connected
    it to a `wordpress` pod, and to access it, we created a `wordpress` service. Each
    YAML file has a `kind` key that defines the type of object it is. For example,
    in pod files, the `kind` is set to pod and in service files, it is set to service.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example setup, we have only one Node (minion). If you log in to it,
    you will see all the running containers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we have not configured replication controllers. We can extend
    this example by creating them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting up the Vagrant environment at [https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/getting-started-guides/vagrant.md](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/getting-started-guides/vagrant.md)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kubernetes User Guide at [https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/user-guide.md](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/user-guide.md)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The documentation on kube-dns at [https://github.com/GoogleCloudPlatform/kubernetes/tree/master/cluster/addons/dns](https://github.com/GoogleCloudPlatform/kubernetes/tree/master/cluster/addons/dns)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
