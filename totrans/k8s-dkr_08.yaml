- en: '*Chapter 6*: Services, Load Balancing, and External DNS'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you deploy an application to a Kubernetes cluster, your pods are assigned
    ephemeral IP addresses. Since the assigned addresses are likely to change as pods
    are restarted, you should never target a service using a pod IP address; instead,
    you should use a service object, which will map a service IP address to backend
    pods based on labels. If you need to offer service access to external requests,
    you can deploy an Ingress controller, which will expose your service to external
    traffic on a per-URL basis. For more advanced workloads, you can deploy a load
    balancer, which provides your service with an external IP address, allowing you
    to expose any IP-based service to external requests.
  prefs: []
  type: TYPE_NORMAL
- en: We will explain how to implement each of these by deploying them on our KinD
    cluster. To help us understand how the Ingress works, we will deploy a NGINX Ingress
    controller to the cluster and expose a web server. Since Ingress rules are based
    on the incoming URL name, we need to be able to provide stable DNS names. In an
    enterprise environment, this would be accomplished using standard DNS. Since we
    are using a development environment without a DNS server, we will use a popular
    service from nip.io.
  prefs: []
  type: TYPE_NORMAL
- en: To end the chapter, we will explain how you can dynamically register service
    names using an ETCD-integrated DNS zone with the Kubernetes incubator project,
    external-dns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exposing workloads to requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to load balancers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layer 7 load balancers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layer 4 load balancers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making service names available externally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter has the following technical requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: A new Ubuntu 18.04 server with a minimum of 4 GB of RAM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A KinD cluster configured using the configuration from [*Chapter 4*](B15514_04_Final_ASB_ePub.xhtml#_idTextAnchor083),
    Deploying Kubernetes using KinD.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can access the code for this chapter at GitHub repository [https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide](https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide).
  prefs: []
  type: TYPE_NORMAL
- en: Exposing workloads to requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Three of the most misunderstood objects in Kubernetes are services, Ingress
    controllers, and load balancers. In order to expose your workloads, you need to
    understand how each object works and the options that are available to you. Let's
    look at these in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how services work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned in the introduction, any pod that is running a workload is assigned
    an IP address at pod startup. Many events will cause a deployment to restart a
    pod, and when the pod is restarted, it will likely receive a new IP address. Since
    the addresses that are assigned to pods may change, you should never target a
    pod's workload directly.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most powerful features that Kubernetes offers is the ability to scale
    your deployments. When a deployment is scaled, Kubernetes will create additional
    pods to handle any additional resource requirements. Each pod will have an IP
    address, and as you may know, most applications only target a single IP address
    or name. If your application were to scale from a single pod to ten pods, how
    would you utilize the additional pods?
  prefs: []
  type: TYPE_NORMAL
- en: 'Services use Kubernetes labels to create a dynamic mapping between the service
    itself and the pods running the workload. The pods that are running the workload
    are labeled when they start up. Each pod has the same label that is defined in
    the deployment. For example, if we were using a NGINX web server in our deployment,
    we would create a deployment with the following manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: apps/v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Deployment'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'creationTimestamp: null'
  prefs: []
  type: TYPE_NORMAL
- en: 'labels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'run: nginx-frontend'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: nginx-frontend'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'replicas: 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'selector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'matchLabels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'run: nginx-frontend'
  prefs: []
  type: TYPE_NORMAL
- en: 'strategy: {}'
  prefs: []
  type: TYPE_NORMAL
- en: 'template:'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'labels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'run: nginx-frontend'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '- image: bitnami/nginx'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: nginx-frontend'
  prefs: []
  type: TYPE_NORMAL
- en: This deployment will create three NGINX servers and each pod will be labeled
    with **run=nginx-frontend**. We can verify whether the pods are labeled correctly
    by listing the pods using kubectl, adding **the --show-labels** option, **kubectl
    get pods --show-labels.**
  prefs: []
  type: TYPE_NORMAL
- en: 'This will list each pod and any associated labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '**nginx-frontend-6c4dbf86d4-72cbc           1/1     Running            0          19s    pod-template-hash=6c4dbf86d4,run=nginx-frontend**'
  prefs: []
  type: TYPE_NORMAL
- en: '**nginx-frontend-6c4dbf86d4-8zlwc           1/1     Running            0          19s    pod-template-hash=6c4dbf86d4,run=nginx-frontend**'
  prefs: []
  type: TYPE_NORMAL
- en: '**nginx-frontend-6c4dbf86d4-xfz6m           1/1     Running            0          19s    pod-template-hash=6c4dbf86d4,run=nginx-frontend**'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding output, each pod has a label, **run=nginx-frontend**.
    You will use this label when you create your service for the application, configuring
    the service to use the label to create the endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you know how a service will use labels to create endpoints, let's discuss
    the service options we have in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: This section will introduce each service type and show you how to create a service
    object. Each type will be detailed in its own section after the general introduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes services can be created using one of four types:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 6.1: Kubernetes service types'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Table_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6.1: Kubernetes service types'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a service, you need to create a service object that includes the
    **kind**, a **selector**, a **type**, and any **ports** that will be used to connect
    to the service. For our NGINX deployment, we want to expose the service on ports
    80 and 443\. We labeled the deployment with **run=nginx-frontend**, so when we
    create a manifest, we will use that name as our selector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Service'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'labels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'run: nginx-frontend'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: nginx-frontend'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'selector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'run: nginx-frontend'
  prefs: []
  type: TYPE_NORMAL
- en: 'ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: http'
  prefs: []
  type: TYPE_NORMAL
- en: 'port: 80'
  prefs: []
  type: TYPE_NORMAL
- en: 'protocol: TCP'
  prefs: []
  type: TYPE_NORMAL
- en: 'targetPort: 80'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: https'
  prefs: []
  type: TYPE_NORMAL
- en: 'port: 443'
  prefs: []
  type: TYPE_NORMAL
- en: 'protocol: TCP'
  prefs: []
  type: TYPE_NORMAL
- en: 'targetPort: 443'
  prefs: []
  type: TYPE_NORMAL
- en: 'type: ClusterIP'
  prefs: []
  type: TYPE_NORMAL
- en: If a type is not defined in a service manifest, Kubernetes will assign a default
    type of **ClusterIP**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that a service has been created, we can verify that it was correctly defined
    using a few **kubectl** commands. The first check we will perform is to verify
    that the service object was created. To check our service, we use the **kubectl
    get services** command:'
  prefs: []
  type: TYPE_NORMAL
- en: NAME                   TYPE          CLUSTER-IP    EXTERNAL-IP   PORT(S)                  AGE
    nginx-frontend   ClusterIP   10.43.142.96  <none>            80/TCP,443/TCP   3m49s
  prefs: []
  type: TYPE_NORMAL
- en: 'After verifying that the service has been created, we can verify that the endpoints
    were created. Using kubectl, we can verify the endpoints by executing **kubectl
    get ep <service name>**:'
  prefs: []
  type: TYPE_NORMAL
- en: NAME                  ENDPOINTS                                                                                            AGE
  prefs: []
  type: TYPE_NORMAL
- en: nginx-frontend   10.42.129.9:80,10.42.170.91:80,10.42.183.124:80 + 3 more...   7m49s
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the service shows three endpoints, but it also shows a **+3
    more** in the endpoint list. Since the output is truncated, the output from a
    get is limited and it cannot show all of the endpoints. Since we cannot see the
    entire list, we can get a more detailed list if we describe the endpoints. Using
    kubectl, you can execute the **kubectl describe ep <service name>** command:'
  prefs: []
  type: TYPE_NORMAL
- en: Name:         nginx-frontend
  prefs: []
  type: TYPE_NORMAL
- en: Namespace:    default
  prefs: []
  type: TYPE_NORMAL
- en: Labels:       run=nginx-frontend
  prefs: []
  type: TYPE_NORMAL
- en: 'Annotations:  endpoints.kubernetes.io/last-change-trigger-time: 2020-04-06T14:26:08Z'
  prefs: []
  type: TYPE_NORMAL
- en: 'Subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: Addresses:          10.42.129.9,10.42.170.91,10.42.183.124
  prefs: []
  type: TYPE_NORMAL
- en: NotReadyAddresses:  <none>
  prefs: []
  type: TYPE_NORMAL
- en: 'Ports:'
  prefs: []
  type: TYPE_NORMAL
- en: Name   Port  Protocol
  prefs: []
  type: TYPE_NORMAL
- en: '----         ----    --------'
  prefs: []
  type: TYPE_NORMAL
- en: http      80      TCP
  prefs: []
  type: TYPE_NORMAL
- en: https  443   TCP
  prefs: []
  type: TYPE_NORMAL
- en: Events:  <none>
  prefs: []
  type: TYPE_NORMAL
- en: 'If you compare the output from our **get** and **describe** commands, it may
    appear that there is a mismatch in endpoints. The **get** command showed a total
    of six endpoints: it showed three IP endpoints and because it was truncated, it
    also listed a **+3**, for a total of six endpoints. The output from the **describe**
    command shows only three IP addresses, not six. Why do the two outputs appear
    to show different results?'
  prefs: []
  type: TYPE_NORMAL
- en: The **get** command will list each endpoint and port in the list of addresses.
    Since our service is defined to expose two ports, each address will have two entries,
    one for each exposed port. The address list will always contain every socket for
    the service, which may list the endpoint addresses multiple times, once for each
    socket.
  prefs: []
  type: TYPE_NORMAL
- en: The **describe** command handles the output differently, listing the addresses
    on one line with all of the ports listed below the addresses. At first glance,
    it may look like the **describe** command is missing three address, but since
    it breaks the output into multiple sections, it will only list the addresses once.
    All ports are broken out below the address list; in our example, it shows port
    80 and 443.
  prefs: []
  type: TYPE_NORMAL
- en: Both commands show the same data, but it's presented in a different format.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the service is exposed to the cluster, you could use the assigned service
    IP address to connect to the application. While this would work, the address may
    change if the service object is deleted and recreated. Rather than target an IP
    address, you should use the DNS that was assigned to the service when it was created.
    In the next section, we will explain how to use internal DNS names to resolve
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Using DNS to resolve services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the world of physical machines and virtual servers, you have probably targeted
    a DNS record to communicate with a server. If the IP address of the server changed,
    then assuming you had dynamic DNS enabled, it would not have any effect on the
    application. This is the advantage of using names rather than IP addresses as
    endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you create a service, an internal DNS record is created that can be queried
    by other workloads in the cluster. If all pods are in the same namespace, then
    we can target the services using a simple, short name like, **mysql-web**; however,
    you may have some services that will be used by multiple namespaces, and when
    workloads need to communicate to a service outside of their own namespace, you
    must target the service using the full name. The following is an example table
    showing how a service may be targeted from namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 6.2: Internal DNS examples'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Table_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6.2: Internal DNS examples'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding table, you can target a service that is in
    another namespace by using a standard naming convention, *.<namespace>.svc.<cluster
    name>*. In most cases, when you are accessing a service in a different namespace,
    you do not need to add the cluster name since it should be appended automatically.
  prefs: []
  type: TYPE_NORMAL
- en: To build on the general services concept, let's get into the details of each
    of the types and how we can use them to access our workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding different service types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you create a service, you need to specify a service type. The service type
    that is assigned will configure how the service is exposed to either the cluster
    or external traffic.
  prefs: []
  type: TYPE_NORMAL
- en: The ClusterIP service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most commonly used, and misunderstood, service type is ClusterIP. If you
    look back at our table, you can see that the description for the ClusterIP type
    states that the service allows connectivity to the service from within the cluster.
    The ClusterIP type does not allow any external traffic to the exposed service.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of exposing a service to only internal cluster workloads can be a confusing
    concept. Why would you expose a service that can only be used by workloads in
    the cluster?
  prefs: []
  type: TYPE_NORMAL
- en: For a minute, let's forget about external traffic entirely. We need to concentrate
    on our current deployment and how each component interacts to create our application.
    Using the NGINX example, we will expand the deployment to include a backend database
    that services the web server.
  prefs: []
  type: TYPE_NORMAL
- en: Our application will have two deployments, one for the NGINX servers and one
    for the database server. The NGINX deployment will create five replicas while
    the database server will consist of a single replica. The NGINX servers need to
    connect to the database server to pull data for the web pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, this is a simple application: we have our deployments created, a service
    for the NGINX servers called the web frontend, and a database service called **mysql-web**.
    To configure the database connection from the web servers, we have decided to
    use a ConfigMap that will target the database service. What do we use in the ConfigMap
    as the destination for the database?'
  prefs: []
  type: TYPE_NORMAL
- en: You may be thinking that since we are using a single database server, we could
    simply use the IP address. While this would initially work, any restarts to the
    pod would change the address and the web servers would fail to connect to the
    database. A service should always be used, even if you are only targeting a single
    pod. Since the database deployment is called mysql-web, our ConfigMap should use
    that name as the database server.
  prefs: []
  type: TYPE_NORMAL
- en: By using the service name, we will not run into issues when the pod is restarted
    since the service targets the labels rather than an IP address. Our web servers
    will simply query the Kubernetes DNS server for the service name, which will contain
    the endpoints of any pod that has a matching label.
  prefs: []
  type: TYPE_NORMAL
- en: The NodePort service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A NodePort service will expose your service internally to the cluster, as well
    as externally to the network. At a first glance, this may look like the go-to
    service when you want to expose a service. It exposes your service to everybody,
    but it does this by using something called a NodePort, and using it for external
    service access can become difficult to maintain. It is also very confusing for
    users to use a NodePort or remember when they need to access a service over the
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a service that uses the NodePort type, you just need to set the type
    to NodePort in your manifest. We can use the same manifest that we used earlier
    to expose a NGINX deployment from the ClusterIP example, only changing the **type**
    to **NodePort**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Service'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'labels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'run: nginx-frontend'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: nginx-frontend'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'selector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'run: nginx-frontend'
  prefs: []
  type: TYPE_NORMAL
- en: 'ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: http'
  prefs: []
  type: TYPE_NORMAL
- en: 'port: 80'
  prefs: []
  type: TYPE_NORMAL
- en: 'protocol: TCP'
  prefs: []
  type: TYPE_NORMAL
- en: 'targetPort: 80'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: https'
  prefs: []
  type: TYPE_NORMAL
- en: 'port: 443'
  prefs: []
  type: TYPE_NORMAL
- en: 'protocol: TCP'
  prefs: []
  type: TYPE_NORMAL
- en: 'targetPort: 443'
  prefs: []
  type: TYPE_NORMAL
- en: 'type: NodePort'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can view the endpoints in the same way that we did for a ClusterIP service,
    using kubectl. Running a **kubectl get services** will show you the newly created
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: NAME                    TYPE           CLUSTER-IP         EXTERNAL-IP   PORT(S)                                         AGE
  prefs: []
  type: TYPE_NORMAL
- en: nginx-frontend    NodePort   10.43.164.118   <none>            80:31574/TCP,443:32432/TCP   4s
  prefs: []
  type: TYPE_NORMAL
- en: The output shows that the type is NodePort and that we have exposed the service
    IP address and the ports. If you look at the ports, you will notice that unlike
    a ClusterIP service, a NodePort service shows two ports rather than one. The first
    port is the exposed port that the internal cluster services can target and the
    second port number is the randomly generated port that is accessible from outside
    of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we exposed both ports 80 and 443 for the service, we will have two NodePorts
    assigned. If someone needs to target the service from outside of the cluster,
    they can target any worker node with the supplied port to access the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – NGINX service using NodePort'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.1_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 – NGINX service using NodePort
  prefs: []
  type: TYPE_NORMAL
- en: Each node maintains a list of the NodePorts and their assigned services. Since
    the list is shared with all nodes, you can target any functioning node using the
    port and Kubernetes will route it to a running pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize the traffic flow, we have created a graphic showing the web request
    to our NGINX pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – NodePort traffic flow overview'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.2_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – NodePort traffic flow overview
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some issues to consider when using a NodePort to expose a service:'
  prefs: []
  type: TYPE_NORMAL
- en: If you delete and recreate the service, the assigned NodePort will change.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you target a node that is offline or having issues, your request will fail.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using NodePort for too many services may get confusing. You need to remember
    the port for each service, and remember that there are no *external* names associated
    with the service. This may get confusing for users that are targeting services
    in the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of the limitations listed here, you should limit using NodePort services.
  prefs: []
  type: TYPE_NORMAL
- en: The LoadBalancer service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many people starting out in Kubernetes read about services and discover that
    the LoadBalancer type will assign an external IP address to a service. Since an
    external IP address can be addressed directly by any machine on the network, this
    is an attractive option for a service, which is why many people try to use it
    first. Unfortunately, since many users start by using an on-premise Kubernetes
    cluster, they run into headaches trying to create a LoadBalancer service.
  prefs: []
  type: TYPE_NORMAL
- en: The LoadBalancer service relies on an external component that integrates with
    Kubernetes to create the IP address assigned to the service. Most on-premise Kubernetes
    installations do not include this type of service. When you try to use a LoadBalancer
    service without the support infrastructure, you will find that your service shows
    **<pending>** in the **EXTERNAL-IP** status column.
  prefs: []
  type: TYPE_NORMAL
- en: We will explain the LoadBalancer service and how to implement it later in the
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The ExternalName service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ExternalName service is a unique service type with a specific use case.
    When you query a service that uses an ExternalName type, the final endpoint is
    not a pod that is running in the cluster, but an external DNS name.
  prefs: []
  type: TYPE_NORMAL
- en: To use an example that you may be familiar with outside of Kubernetes, this
    is similar to using **c-name** to alias a host record. When you query a **c-name**
    record in DNS, it resolves to a host record rather than an IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Before using this service type, you need to understand potential issues that
    it may cause for your application. You may run into issues if the target endpoint
    is using SSL certificates. Since the hostname you are querying may not be the
    same as the name on the destination server's certificate, your connection may
    not succeed because of the name mismatch. If you find yourself in this situation,
    you may be able to use a certificate that has **subject alternative names** (**SAN**)
    added to the certificate. Adding alternative names to a certificate allow you
    to associate multiple names with a certificate.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain why you may want to use an ExternalName service, let''s use the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image/Table_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Based on the requirements, using an ExternalName service is the perfect solution.
    So, how would we accomplish the requirements? (This is a theoretical exercise;
    you do not need to execute anything on your KinD cluster)
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to create a manifest that will create the ExternalName service
    for the database server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Service'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: sql-db'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: finance'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'type: ExternalName'
  prefs: []
  type: TYPE_NORMAL
- en: 'externalName: sqlserver1.foowidgets.com'
  prefs: []
  type: TYPE_NORMAL
- en: With the service created, the next step is to configure the application to use
    the name of our new service. Since the service and the application are in the
    same namespace, you can configure the application to target the name **sql-db**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, when the application queries for **sql-db**, it will resolve to **sqlserver1.foowidgets.com**,
    and, ultimately, the IP address of 192.168.10.200.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This accomplishes the initial requirement, connecting the application to the
    external database server using only the Kubernetes DNS server.
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering why we didn't simply configure the application to use the
    database server name directly. The key is the second requirement, limiting any
    reconfiguration when the SQL server is migrated to a container.
  prefs: []
  type: TYPE_NORMAL
- en: Since we cannot reconfigure the application after the SQL server is migrated
    to the cluster, we will not be able to change the name of the SQL server in the
    application settings. If we configured the application to use the original name,
    **sqlserver1.foowidgets.com**, the application would not work after the migration.
    By using the ExternalName service, we have the ability to change the internal
    DNS service name by replacing the ExternalHost service name with a standard Kubernetes
    service that points to the SQL server.
  prefs: []
  type: TYPE_NORMAL
- en: 'To accomplish the second goal, go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Delete the **ExternalName** service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new service using the name **ext-sql-db** that uses **app=sql-app**
    as the selector. The manifest would look like the one shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Service'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'labels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'app: sql-db'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: sql-db'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: finance'
  prefs: []
  type: TYPE_NORMAL
- en: 'ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '- port: 1433'
  prefs: []
  type: TYPE_NORMAL
- en: 'protocol: TCP'
  prefs: []
  type: TYPE_NORMAL
- en: 'targetPort: 1433'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: sql'
  prefs: []
  type: TYPE_NORMAL
- en: 'selector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'app: sql-app'
  prefs: []
  type: TYPE_NORMAL
- en: 'type: ClusterIP'
  prefs: []
  type: TYPE_NORMAL
- en: Since we are using the same service name for the new service, no changes need
    to be made to the application. The app will still target the name **sql-db**,
    which will now use the SQL server deployed in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know about services, we can move on to load balancers, which will
    allow you to expose services externally using standard URL names and ports.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to load balancers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before discussing different types of load balancers, it's important to understand
    the **Open Systems Interconnection** (**OSI**) model. Understanding the different
    layers of the OSI model will help you to understand how different solutions handle
    incoming requests.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the OSI model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you hear about different solutions to expose an application in Kubernetes,
    you will often here a reference to layer 7 or layer 4 load balancing. These designations
    refer to where each operates in the OSI model. Each layer offers different functionality;
    a component that runs at layer 7 offers different functionality than a component
    in layer 4.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, let''s look at a brief overview of the seven layers and a description
    of each. For this chapter, we are interested in the two highlighted sections,
    **layer 4 and layer 7**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image/Table_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 6.3 OSI model layers
  prefs: []
  type: TYPE_NORMAL
- en: You don't need to be an expert in the OSI layers, but you should understand
    what a layer 4 and layer 7 load balancer provide and how each may be used with
    a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go deeper into the details of layer 4 and 7:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Layer 4**: As the description states in the chart, layer 4 is responsible
    for the communication traffic between devices. Devices that run at layer 4 have
    access to TCP/UPD information. Load balancers that are layer-4 based provide your
    applications with the ability to service incoming requests for any TCP/UDP port.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer 7**: Layer 7 is responsible for providing network services to applications.
    When we say application traffic, we are not referring to applications such as
    Excel or Word; instead, we are referring to the protocols that support the applications,
    such as HTTP and HTTPS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will explain each load balancer type and how to use
    them in a Kubernetes cluster to expose your services.
  prefs: []
  type: TYPE_NORMAL
- en: Layer 7 load balancers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes provides layer 7 load balancers in the form of an Ingress controller.
    There are a number of solutions to provide Ingress to your clusters, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: NGINX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Envoy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traefik
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haproxy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, a layer 7 load balancer is limited in the functions it can perform.
    In the Kubernetes world, they are implemented as Ingress controllers that can
    route incoming HTTP/HTTPS requests to your exposed services. We will go into detail
    on implementing NGINX as a Kubernetes Ingress controller in the *Creating Ingress
    rules* section.
  prefs: []
  type: TYPE_NORMAL
- en: Name resolution and layer 7 load balancers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To handle layer 7 traffic in a Kubernetes cluster, you deploy an Ingress controller.
    Ingress controllers are dependent on incoming names to route traffic to the correct
    service. In a legacy server deployment model, you would create a DNS entry and
    map it to an IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Applications that are deployed on a Kubernetes cluster are no different—the
    user will use a DNS name to access the application.
  prefs: []
  type: TYPE_NORMAL
- en: Oftentimes, you will create a new wildcard domain that will target the Ingress
    controller via an external load balancer, such as an F5, HAproxy, or SeeSaw.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that our company is called FooWidgets and we have three Kubernetes
    clusters, fronted by an external load balancer with multiple Ingress controller
    endpoints. Our DNS server would have entries for each cluster, using a wildcard
    domain that points to the load balancer''s virtual IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image/Table_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 6.4 Example wildcard domain names for Ingress
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the entire flow of the request:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Multiple-name Ingress traffic flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.3_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 – Multiple-name Ingress traffic flow
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the steps in diagram 6.3 are detailed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a browser, the user requests the URL [https://timesheets.cluster1.foowidgets.com](https://timesheets.cluster1.foowidgets.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The DNS query is sent to a DNS server. The DNS server looks up the zone details
    for **cluster1.foowidgets.com**. There is a single entry in the DNS zone that
    resolves to the VIP assigned on the load balancer for the domain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The load balancer's VIP for **cluster1.foowidgets.com** has three backend servers
    assigned, pointing to three worker nodes where we have deployed Ingress controllers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using one of the endpoints, the request is sent to the Ingress controller.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Ingress controller will compare the requested URL to a list of Ingress rules.
    When a matching request is found, the Ingress controller will forward the request
    to the service that was assigned to the Ingress rule.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To help reinforce how Ingress works, it will help to create Ingress rules on
    a cluster to see them in action. Right now, the key takeaways are that Ingress
    uses the requested URL to direct traffic to the correct Kubernetes services.
  prefs: []
  type: TYPE_NORMAL
- en: Using nip.io for name resolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most personal development clusters, such as our KinD installation, may not have
    enough access to add records to a DNS server. To test Ingress rules, we need to
    target unique host names that are mapped to Kubernetes services by the Ingress
    controller. Without a DNS server, you need to create a local host file with multiple
    names pointing to the IP address of the Ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you deployed four web servers, you need to add all four names
    to your local hosts. An example of this is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**192.168.100.100 webserver1.test.local**'
  prefs: []
  type: TYPE_NORMAL
- en: '**192.168.100.100 webserver2.test.local**'
  prefs: []
  type: TYPE_NORMAL
- en: '**192.168.100.100 webserver3.test.local**'
  prefs: []
  type: TYPE_NORMAL
- en: '**192.168.100.100 webserver4.test.local**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This can also be represented on a single line rather than multiple lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**192.168.100.100 webserver1.test.local webserver2.test.local webserver3.test.local
    webserver4.test.local**'
  prefs: []
  type: TYPE_NORMAL
- en: If you use multiple machines to test your deployments, you will need to edit
    the host file on every machine that you plan to use for testing. Maintaining multiple
    files on multiple machines is an administrative nightmare and will lead to issues
    that will make testing a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, there are free services available that provide DNS services that we
    can use without configuring a complex DNS infrastructure for our KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Nip.io is the service that we will use for our KinD cluster name resolution
    requirements. Using our previous web server example, we will not need to create
    any DNS records. We still need to send the traffic for the different servers to
    the NGINX server running on 192.168.100.100 so that Ingress can route the traffic
    to the appropriate service. Nip.io uses a naming format that includes the IP address
    in the hostname to resolve the name to an IP. For example, say that we have four
    web servers that we want to test called webserver1, webserver2, webserver3, and
    webserver4, with Ingress rules on an Ingress controller running on 192.168.100.100.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, we do not need to create any records to accomplish
    this. Instead, we can use the naming convention to have nip.io resolve the name
    for us. Each of the web servers would use a name with the following naming standard:'
  prefs: []
  type: TYPE_NORMAL
- en: '**<desired name>.<INGRESS IP>.nip.io**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The names for all four web servers are listed in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image/Table_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 6.5 – Nip.io example domain names
  prefs: []
  type: TYPE_NORMAL
- en: 'When you use any of the preceding names, nip.io will resolve them to 192.168.100.100\.
    You can see an example ping for each name in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Example name resolution using nip.io'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.4_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – Example name resolution using nip.io
  prefs: []
  type: TYPE_NORMAL
- en: This may look like it has very little benefit, since you are supplying the IP
    address in the name. Why would you need to bother using nip.io if you know the
    IP address?
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the Ingress rules require a unique name to route traffic to the
    correct service. While the name may not be required for you to know the IP address
    of the server, the name is required for the Ingress rules. Each name is unique,
    using the first part of the full name—in our example, that is **webserver1**,
    **webserver2**, **webserver3**, and **webserver4**.
  prefs: []
  type: TYPE_NORMAL
- en: By providing this service, nip.io allows you to use any name for Ingress rules
    without the need to have a DNS server in your development cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to use nip.io to resolve names for your cluster, let's
    explain how to use a nip.io name in an Ingress rule.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Ingress rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Remember, Ingress rules use names to route the incoming request to the correct
    service. The following is a graphical representation of an incoming request, showing
    how Ingress routes the traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Ingress traffic flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.5_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – Ingress traffic flow
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.5 shows a high-level overview of how Kubernetes handles incoming Ingress
    requests. To help explain each step in more depth, let''s go over the five steps
    in greater detail. Using the graphic provided in Figure 6.5, we will explain each
    numbered step in detail to show how Igress processes the request:'
  prefs: []
  type: TYPE_NORMAL
- en: The user requests a URL in their browser named webserver1.192.168.200.20.nio.io.
    A DNS request is sent to the local DNS server, which is ultimately sent to the
    nip.io DNS server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The nip.io server resolves the domain name to the IP address of 192.168.200.20,
    which is returned to the client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The client sends the request to the Ingress controller, which is running on
    192.168.200.20\. The request contains the complete URL name, **webserver1.192.168.200.20.nio.io**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Ingress controller looks up the requested URL name in the configured rules
    and matches the URL name to a service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The service endpoint(s) will be used to route traffic to the assigned pods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The request is routed to an endpoint pod running the web server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using the preceding example traffic flow, let''s go over the Kubernetes objects
    that need to be created:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need a simple webserver running in a namespace. We will simply deploy
    a base NGINX webserver in the default namespace. Rather than create a manifest
    manually, we can create a deployment quickly using the following **kubectl run**
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**kubectl run nginx-web --image bitnami/nginx**'
  prefs: []
  type: TYPE_NORMAL
- en: Using the **run** option is a shortcut that will create a deployment called
    **nginx-web** in the default namespace. You may notice that the output will give
    you a warning that the run is being deprecated. This is just a warning; it will
    still create our deployment, although using **run** to create a deployment may
    not work in future Kubernetes versions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we need to create a service for the deployment. Again, we will create
    a service using a kubectl command, **kubectl expose**. The Bitnami NGINX image
    runs on port 8080, so we will use the same port to expose the service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**kubectl expose deployment nginx-web --port 8080 --target-port 8080**'
  prefs: []
  type: TYPE_NORMAL
- en: This will create a new service called nginx-web for our deployment, called nginx-web.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our deployment and service created, the last step is to create
    the Ingress rule. To create an Ingress rule, you create a manifest using the object
    type **Ingress**. The following is an example Ingress rule that assumes that the
    Ingress controller is running on 192.168.200.20\. If you are creating this rule
    on your host, you should use the **IP address of your Docker host**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a file called **nginx-ingress.yaml** with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: networking.k8s.io/v1beta1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Ingress'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: nginx-web-ingress'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '- host: webserver1.192.168.200.20.nip.io'
  prefs: []
  type: TYPE_NORMAL
- en: 'http:'
  prefs: []
  type: TYPE_NORMAL
- en: 'paths:'
  prefs: []
  type: TYPE_NORMAL
- en: '- path: /'
  prefs: []
  type: TYPE_NORMAL
- en: 'backend:'
  prefs: []
  type: TYPE_NORMAL
- en: 'serviceName: nginx-web'
  prefs: []
  type: TYPE_NORMAL
- en: 'servicePort: 8080'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the Ingress rune using **kubectl apply**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**kubectl apply -f nginx-ingress.yaml**'
  prefs: []
  type: TYPE_NORMAL
- en: You can test the deployment from any client on your internal network by browsing
    to the Ingress URL, **http:// webserver1.192.168.200.20.nip.io**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If everything was created successfully, you should see the NGINX welcome page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.6 – NGINX web server using nip.io for Ingress'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.6_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 – NGINX web server using nip.io for Ingress
  prefs: []
  type: TYPE_NORMAL
- en: Using the information in this section, you can create Ingress rules for multiple
    containers using different hostnames. Of course, you aren't limited to using a
    service like nip.io to resolve names; you can use any name resolution method that
    you have available in your environment. In a production cluster, you will have
    an enterprise DNS infrastructure, but in a lab environment, such as our KinD cluster,
    nip.io is the perfect tool to test scenarios that require proper naming conventions.
  prefs: []
  type: TYPE_NORMAL
- en: We will use nip.io naming standards throughout the book, so it's important to
    understand the naming convention before moving on to the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Layer 7 load balancers, such as NGINX Ingress, are used by many standard workloads,
    such as like web servers. There will be deployments that will require a more complex
    load balancer, one that runs at a lower layer of the OIS model. As we move down
    the model, we gain lower-level features. In our next section, we will discuss
    layer 4 load balancers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you deployed the NGINX example on your cluster, you should delete the service
    and the Ingress rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '• To delete the Ingress rule, execute the following: **kubectl delete ingress
    nginx-web-ingress**'
  prefs: []
  type: TYPE_NORMAL
- en: '• To delete the service, execute the following: **kubectl delete service nginx-web**'
  prefs: []
  type: TYPE_NORMAL
- en: You can leave the NGINX deployment running for the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Layer 4 load balancers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Layer 4 of the OSI model is responsible for protocols such as TCP and UDP. A
    load balancer that is running in layer 4 accepts incoming traffic based on the
    only IP address and port. The incoming request is accepted by the load balancer,
    and based on a set of rules, the traffic is sent to the destination IP address
    and port.
  prefs: []
  type: TYPE_NORMAL
- en: There are lower-level networking operations in the process that are out of the
    scope of this book. HAproxy has a good summary of the terminology and example
    configurations on their website at [https://www.haproxy.com/fr/blog/loadbalancing-faq/](https://www.haproxy.com/fr/blog/loadbalancing-faq/).
  prefs: []
  type: TYPE_NORMAL
- en: Layer 4 load balancer options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple options available to you if you want to configure a layer
    4 load balancer for a Kubernetes cluster. Some of the options include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: HAproxy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NGINX Pro
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SeeSaw
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F5 Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MetalLB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And more…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each option provides layer 4 load balancing, but for the purpose of this book,
    we felt that MetalLB was the best choice.
  prefs: []
  type: TYPE_NORMAL
- en: Using MetalLB as a layer 4 load balancer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Remember that in [*Chapter 4*](B15514_04_Final_ASB_ePub.xhtml#_idTextAnchor083)
    *Deploying Kubernetes using KinD* we had a diagram showing the flow of traffic
    between a workstation and the KinD nodes. Because KinD was running in a nested
    Docker container, a layer 4 load balancer would have had certain limitations when
    it came to networking connectivity. Without additional network configuration on
    the Docker host, you will not be able to target the services that use the LoadBalancer
    type outside of the Docker host itself.
  prefs: []
  type: TYPE_NORMAL
- en: If you deploy MetalLB to a standard Kubernetes cluster running on a host, you
    will not be limited to accessing services outside of the host itself.
  prefs: []
  type: TYPE_NORMAL
- en: MetalLB is a free, easy to configure layer 4 load balancer. It includes powerful
    configuration options that give it the ability to run in a development lab or
    an enterprise cluster. Since it is so versatile, it has become a very popular
    choice for clusters requiring layer 4 load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will focus on installing MetalLB in layer 2 mode. This is
    an easy installation and works for development or small Kubernetes clusters. MetalLB
    also offers the option to deploy using BGP mode, which allows you to establish
    peering partners to exchange networking routes. If you would like to read about
    MetalLB's BGP mode, you can read about it on MetalLB's site at  [https://metallb.universe.tf/concepts/bgp/](https://metallb.universe.tf/concepts/bgp/).
  prefs: []
  type: TYPE_NORMAL
- en: Installing MetalLB
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To deploy MetalLB on your KinD cluster, use the manifests from MetalLB''s GitHub
    repository. To install MetalLB, go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following will create a new namespace called **metallb-system** with a
    label of **app: metallb**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This will deploy MetalLB to your cluster. It will create all required Kubernetes
    objects, including **PodSecurityPolicies**, **ClusterRoles**, **Bindings**, **DaemonSet**,
    and a **deployment**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last command will create a secret in the **metalb-system** namespace that
    has a randomly generated value. This secret is used by MetalLB to encrypt communications
    between speakers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl
    rand -base64 128)"**'
  prefs: []
  type: TYPE_NORMAL
- en: Now that MetalLB has been deployed to the cluster, you need to supply a configuration
    file to complete the setup.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MetalLB's configuration file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MetalLB is configured using a ConfigMap that contains the configuration. Since
    we will be using MetalLB in layer 2 mode, the required configuration file is fairly
    simple and only requires one piece of information: the IP range that you want
    to create for services.'
  prefs: []
  type: TYPE_NORMAL
- en: To keep the configuration simple, we will use a small range from the Docker
    subnet in which KinD is running. If you were running MetalLB on a standard Kubernetes
    cluster, you could assign any range that is routable in your network, but we are
    limited with our KinD clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the subnet that Docker is using, we can inspect the default bridge network
    that we are using:'
  prefs: []
  type: TYPE_NORMAL
- en: '**docker network inspect bridge**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the output, you will see the assigned subnet, similar to following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**"Subnet": "172.17.0.0/16"**'
  prefs: []
  type: TYPE_NORMAL
- en: This is an entire class-B address range. We know that we will not use all of
    the IP addresses for running containers, so we will use a small range from the
    subnet in our MetalLB configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a new file called **metallb-config.yaml** and add the following
    to the file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: ConfigMap'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: metallb-system'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: config'
  prefs: []
  type: TYPE_NORMAL
- en: 'data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'config: |'
  prefs: []
  type: TYPE_NORMAL
- en: 'address-pools:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: default'
  prefs: []
  type: TYPE_NORMAL
- en: 'protocol: layer2'
  prefs: []
  type: TYPE_NORMAL
- en: 'addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '- 172.17.200.100-172.17.200.125'
  prefs: []
  type: TYPE_NORMAL
- en: The manifest will create a ConfigMap in the **metallb-system** namespace called
    **config**. The configuration file will set MetalLB's mode to layer 2 with an
    IP pool called **default**, using the range of 172.16.200-100 through 172.16.200.125
    for LoadBalancer services.
  prefs: []
  type: TYPE_NORMAL
- en: You can assign different addresses based on the configuration names. We will
    show this when we explain how to create a LoadBalancer service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, deploy the manifest using kubectl:'
  prefs: []
  type: TYPE_NORMAL
- en: '**kubectl apply -f metallb-config.yaml**'
  prefs: []
  type: TYPE_NORMAL
- en: To understand how MetalLB works, you need to know the installed components and
    how they interact to assign IP addresses to services.
  prefs: []
  type: TYPE_NORMAL
- en: MetalLB components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second manifest in our deployment is what installs the MetalLB components
    to the cluster. It deploys a DaemonSet that includes the speaker image and a DaemonSet
    that includes the controller image. These components communicate with each other
    to maintain a list of services and assigned IP addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: The speaker
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The speaker component is what MetaLB uses to announce the LoadBalancer services
    on the node. It is deployed as a DaemonSet since the deployments can be on any
    worker node, and therefore, each worker node needs to announce the workloads that
    are running. As services are created using a LoadBalancer type, the speaker will
    announce the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the speaker log from a node, we can see the announcements following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**{"caller":"main.go:176","event":"startUpdate","msg":"start of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.437231123Z"}**'
  prefs: []
  type: TYPE_NORMAL
- en: '**{"caller":"main.go:189","event":"endUpdate","msg":"end of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.437516541Z"}**'
  prefs: []
  type: TYPE_NORMAL
- en: '**{"caller":"main.go:176","event":"startUpdate","msg":"start of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.464140524Z"}**'
  prefs: []
  type: TYPE_NORMAL
- en: '**{"caller":"main.go:246","event":"serviceAnnounced","ip":"10.2.1.72","msg":"service
    has IP, announcing","pool":"default","protocol":"layer2","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.464311087Z"}**'
  prefs: []
  type: TYPE_NORMAL
- en: '**{"caller":"main.go:249","event":"endUpdate","msg":"end of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.464470317Z"}**'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding announcement is for Grafana. After the announcement, you can see
    that it has been assigned an IP address of 10.2.1.72.
  prefs: []
  type: TYPE_NORMAL
- en: The controller
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The controller will receive the announcements from the speaker on each worker
    node. Using the same service announcement shown previously, the controller log
    shows the announcement and the IP address that the controller assigned to the
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '**{"caller":"main.go:49","event":"startUpdate","msg":"start of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.437701161Z"}**'
  prefs: []
  type: TYPE_NORMAL
- en: '**{"caller":"service.go:98","event":"ipAllocated","ip":"10.2.1.72","msg":"IP
    address assigned by controller","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.438079774Z"}**'
  prefs: []
  type: TYPE_NORMAL
- en: '**{"caller":"main.go:96","event":"serviceUpdated","msg":"updated service object","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.467998702Z"}**'
  prefs: []
  type: TYPE_NORMAL
- en: In the second line of the log, you can see that the controller assigned the
    IP address of 10.2.1.72.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a LoadBalancer service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have installed MetalLB and understand how the components create
    the services, let's create our first LoadBalancer service on our KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the layer 7 load balancer section, we created a deployment running NGINX
    that we exposed by creating a service and an Ingress rule. At the end of the section,
    we deleted the service and the Ingress rule, but we kept the NGINX deployment
    for this section. If you followed the steps in the Ingress section and have not
    deleted the service and Ingress rule, please do so before creating the LoadBalancer
    service. If you did not create the deployment at all, you will need an NGINX deployment
    for this section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create a quick NGINX deployment by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**kubectl run nginx-web --image bitnami/nginx**'
  prefs: []
  type: TYPE_NORMAL
- en: To create a new service that will use the LoadBalancer type, you can create
    a new manifest or you can expose the deployment using only kubectl.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To create a manifest, create a new file called **nginx-lb.yaml** and add the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Service'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: nginx-lb'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '- port: 8080'
  prefs: []
  type: TYPE_NORMAL
- en: 'targetPort: 8080'
  prefs: []
  type: TYPE_NORMAL
- en: 'selector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'run: nginx-web'
  prefs: []
  type: TYPE_NORMAL
- en: 'type: LoadBalancer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the file to the cluster using kubectl:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**kubectl apply -f nginx-lb.yaml**'
  prefs: []
  type: TYPE_NORMAL
- en: To verify that the service was created correctly, list the services using **kubectl
    get services**:![Figure 6.7 – Kubectl service output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/Fig_6.7_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – Kubectl service output
  prefs: []
  type: TYPE_NORMAL
- en: You will see that a new service was created using the LoadBalancer type and
    that MetalLB assigned an IP address from the configured pool we created earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick look at the controller log will verify that the MetalLB controller
    assigned the service the IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '**{"caller":"service.go:114","event":"ipAllocated","ip":"172.16.200.100","msg":"IP
    address assigned by controller","service":"default/nginx-lb","ts":"2020-04-25T23:54:03.668948668Z"}**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can test the service by using **curl** on the Docker host. Using the
    IP address that was assigned to the service and port 8080, enter the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**curl 172.17.200.100:8080**'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Curl output to the LoadBalancer service running NGINX'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.8_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.8 – Curl output to the LoadBalancer service running NGINX
  prefs: []
  type: TYPE_NORMAL
- en: Adding MetalLB to a cluster allows you to expose applications that otherwise
    could not be exposed using a layer 7 balancer. Adding both layer 7 and layer 4
    services to your clusters allows you to expose almost any application type you
    can think of, including databases. What if you wanted to offer different IP pools
    to services? In the next section, we will explain how to create multiple IP pools
    that can be assigned to services using an annotation, allowing you to assign an
    IP range to services.
  prefs: []
  type: TYPE_NORMAL
- en: Adding multiple IP pools to MetalLB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There may be scenarios where you need to provide different subnets to specific
    workloads on a cluster. One scenario may be that when you created a range on the
    network for your services, you underestimated how many services would be created
    and you ran out of IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the original range that you used, you may be able to just increase
    the range on your configuration. If you cannot extend the existing range, you
    will need to create a new range before any new LoadBalancer services can be created.
    You can also add additional IP ranges to the default pool, but for this example,
    we will create a new pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can edit the configuration file and add the new range information to the
    file. Using the original YAML file, **metallb-config.yaml**, we need to add the
    text in bold in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: ConfigMap'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: metallb-system'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: config'
  prefs: []
  type: TYPE_NORMAL
- en: 'data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'config: |'
  prefs: []
  type: TYPE_NORMAL
- en: 'address-pools:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: default'
  prefs: []
  type: TYPE_NORMAL
- en: 'protocol: layer2'
  prefs: []
  type: TYPE_NORMAL
- en: 'addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '- 172.17.200.100-172.17.200.125'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: subnet-201'
  prefs: []
  type: TYPE_NORMAL
- en: 'protocol: layer2'
  prefs: []
  type: TYPE_NORMAL
- en: 'addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '- 172.17.201.100-172.17.201.125'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the updated ConfigMap using **kubectl**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**kubectl apply -f metallb-config.yaml**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The updated ConfigMap will create a new pool called subnet-201\. MetalLB now
    has two pools that can be used to assign IP addresses to services: the default
    and subnet-201\.'
  prefs: []
  type: TYPE_NORMAL
- en: If a user creates a LoadBalancer service and does not specify a pool name, Kubernetes
    will attempt to use the default pool. If the requested pool is out of address,
    the service will sit in a pending state until an address is available.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a new service from the second pool, you need to add an annotation
    to your service request. Using our NGINX deployment, we will create a second service
    called **nginx-web2** that will request an IP address from the subnet-201 pool:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new file called **nginx-lb2.yaml** with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Service'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: nginx-lb2'
  prefs: []
  type: TYPE_NORMAL
- en: 'annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'metallb.universe.tf/address-pool: subnet-201'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '- port: 8080'
  prefs: []
  type: TYPE_NORMAL
- en: 'targetPort: 8080'
  prefs: []
  type: TYPE_NORMAL
- en: 'selector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'run: nginx-web'
  prefs: []
  type: TYPE_NORMAL
- en: 'type: LoadBalancer'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the new service, deploy the manifest using kubectl:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**kubectl apply -f nginx-lb2.yaml**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify that the service was created with an IP address from the subnet-201
    pool, list all of the services:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**kubectl get services**'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Example services using LoadBalancer'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.9_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.9 – Example services using LoadBalancer
  prefs: []
  type: TYPE_NORMAL
- en: The last service in the list is our newly created **nginx-lb2** service. We
    can confirm that it has been assigned an external IP address of 172.17.20.100,
    which is from the subnet-201 pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, we can test the service by using a **curl** command on the Docker
    host, to the assigned IP address on port 8080:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Curl NGINX on a LoadBalancer using a second IP pool'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.10_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.10 – Curl NGINX on a LoadBalancer using a second IP pool
  prefs: []
  type: TYPE_NORMAL
- en: Having the ability to offer different address pools allows you to assign a known
    IP address block to services. You may decide that address pool 1 will be used
    for web services, address pool 2 for databases, address pool 3 for file transfers,
    and so on. Some organizations do this to identify traffic based on the IP assignment,
    making it easier to track communication.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a layer 4 load balancer to your cluster allows you to migrate applications
    that may not work with simple layer 7 traffic.
  prefs: []
  type: TYPE_NORMAL
- en: As more applications are migrated or refactored for containers, you will run
    into many applications that require multiple protocols for a single service. Natively,
    if you attempt to create a service with both TCP and UDP port mapping, you will
    receive an error that multiple protocols are not supported for the service object.
    This may not affect many applications, but why should you be limited to a single
    protocol for a service?
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple protocols
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All of our examples so far have used a TCP as the protocol. Of course, MetalLB
    supports using UDP as the service protocol as well, but what if you had a service
    that required you to use both protocols?
  prefs: []
  type: TYPE_NORMAL
- en: Multiple protocol issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Not all service types support assigning multiple protocols to a single service.
    The following table shows the three service types and their support for multiple
    protocols:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image/Table_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 6.6 – Service type protocol support
  prefs: []
  type: TYPE_NORMAL
- en: 'If you attempt to create a service that uses both protocols, you will receive
    an error message. We have highlighted the error in the following error message:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Service "kube-dns-lb" is invalid: spec.ports: Invalid value: []core.ServicePort{core.ServicePort{Name:"dns",
    Protocol:"UDP", Port:53, TargetPort:intstr.IntOrString{Type:0, IntVal:53, StrVal:""},
    NodePort:0}, core.ServicePort{Name:"dns-tcp", Protocol:"TCP", Port:53, TargetPort:intstr.IntOrString{Type:0,
    IntVal:53, StrVal:""}, NodePort:0}}: **cannot create an external load balancer
    with mix protocols**'
  prefs: []
  type: TYPE_NORMAL
- en: The service we were attempting to create would expose our CoreDNS service to
    an external IP using a LoadBalancer service. We need to expose the service on
    port 50 for both TCP and UDP.
  prefs: []
  type: TYPE_NORMAL
- en: 'MetalLB includes support for multiple protocols bound to a single IP address.
    The configuration requires the creation of two different services rather than
    a single service, which may seem a little odd at first. As we have shown previously,
    the API server will not allow you to create a service object with multiple protocols.
    The only way to work around this limitation is to create two different services:
    one that has the TCP ports assigned and another that has the UDP ports assigned.'
  prefs: []
  type: TYPE_NORMAL
- en: Using our CoreDNS example, we will go through the steps to create an application
    that requires multiple protocols.
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple protocols with MetalLB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To enable support for an application that requires both TCP and UDP you need
    to create two separate services. If you have been paying close attention to how
    services are created, you may have noticed that each service receives an IP address.
    Logically, this means that when we create two services for our application, we
    would receive two different IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we want to expose CoreDNS as a LoadBalancer service, which requires
    both TCP and UDP protocols. If we created two standard services, one with each
    protocol defined, we would receive two different IP address. How would you configure
    a system to use a DNS server that requires two different IP addresses for a connection?
  prefs: []
  type: TYPE_NORMAL
- en: The simple answer is, **you can't**.
  prefs: []
  type: TYPE_NORMAL
- en: But we just told you that you MetalLB supports this type of configuration. Stay
    with us—we are building up to explaining this by first explaining the issues that
    MetalLB will solve for us.
  prefs: []
  type: TYPE_NORMAL
- en: When we created the NGINX service that pulled from the subnet-201 IP pool earlier,
    we did so by adding an annotation to the load-balancer manifest. MetalLB has added
    support for multiple protocols by adding an annotation for **shared-IPs.**
  prefs: []
  type: TYPE_NORMAL
- en: Using shared-IPs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you understand the limitations around multiple protocol support in
    Kubernetes, let's use MetalLB to expose our CoreDNS service to external requests,
    using both TCP and UDP.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, Kubernetes will not allow you to create a single service
    with both protocols. To have a single load-balanced IP use both protocols, you
    need to create a service for both protocols, one for TCP and another for UDP.
    Each of the services will need an annotation that MetalLB will use to assign the
    same IP to both services.
  prefs: []
  type: TYPE_NORMAL
- en: For each service, you need to set the same value for the **metallb.universe.tf/allow-shared-ip**
    annotation. We will cover a complete example to expose CoreDNS to explain the
    entire process.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Most Kubernetes distributions use CoreDNS as the default DNS provider, but some
    of them still use the service name from when kube-dns was the default DNS provider.
    KinD is one of the distributions that may confuse you at first, since the service
    name is kube-dns, but rest assured, the deployment is using CoreDNS.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: First, look at the services in the **kube-system** namespace:![Figure 6.11 –
    Default service list for kube-system
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/Fig_6.11_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.11 – Default service list for kube-system
  prefs: []
  type: TYPE_NORMAL
- en: The only service we have is the default **kube-dns** service, using the ClusterIP
    type, which means that it is only accessible internally to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: You might have noticed that the service has multiple protocol support, having
    both port UDP and TCP assigned. Remember that, unlike the LoadBalancer service,
    a ClusterIP service **can** be assigned multiple protocols.
  prefs: []
  type: TYPE_NORMAL
- en: The first step to add LoadBalancer support to our CoreDNS server is to create
    two manifests, one for each protocol.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will create the TCP service first. Create a file called **coredns-tcp.yaml**
    and add the content from the following example manifest. Note that the internal
    service for CoreDNS is using the **k8s-app: kube-dns** selector. Since we are
    exposing the same service, that''s the selector we will use in our manifests:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Service'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: coredns-tcp'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: kube-system'
  prefs: []
  type: TYPE_NORMAL
- en: 'annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'metallb.universe.tf/allow-shared-ip: "coredns-ext"'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'selector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'k8s-app: kube-dns'
  prefs: []
  type: TYPE_NORMAL
- en: 'ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: dns-tcp'
  prefs: []
  type: TYPE_NORMAL
- en: 'port: 53'
  prefs: []
  type: TYPE_NORMAL
- en: 'protocol: TCP'
  prefs: []
  type: TYPE_NORMAL
- en: 'targetPort: 53'
  prefs: []
  type: TYPE_NORMAL
- en: 'type: LoadBalancer'
  prefs: []
  type: TYPE_NORMAL
- en: This file should be familiar by now, with the one exception in the annotations
    being the addition of the **metallb.universe.tf/allow-shared-ip** value. The use
    for this value will become clear when we create the next manifest for the UDP
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Create a file called **coredns-udp.yaml** and add the content from the following
    example manifest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Service'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: coredns-udp'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: kube-system'
  prefs: []
  type: TYPE_NORMAL
- en: 'annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'metallb.universe.tf/allow-shared-ip: "coredns-ext"'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'selector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'k8s-app: kube-dns'
  prefs: []
  type: TYPE_NORMAL
- en: 'ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: dns-tcp'
  prefs: []
  type: TYPE_NORMAL
- en: 'port: 53'
  prefs: []
  type: TYPE_NORMAL
- en: 'protocol: UDP'
  prefs: []
  type: TYPE_NORMAL
- en: 'targetPort: 53'
  prefs: []
  type: TYPE_NORMAL
- en: 'type: LoadBalancer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we used the same annotation value from the TCP service manifest,
    **metallb.universe.tf/allow-shared-ip: "coredns-ext"**. This is the value that
    MetalLB will use to create a single IP address, even though two separate services
    are being requested.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can deploy the two services to the cluster using **kubectl apply**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**kubectl apply -f coredns-tcp.yaml kubectl apply -f coredns-udp.yaml**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once deployed, get the services in the **kube-system** namespace to verify
    that our services were deployed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Multiple protocols assigned using MetalLB'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.12_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.12 – Multiple protocols assigned using MetalLB
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see that two new services were created: the **coredns-tcp** and
    **coredns-udp** services. Under the **EXTERNAL-IP** column, you can see that both
    services have been assigned the same IP address, which allows the service to accept
    both protocols on the same IP address.'
  prefs: []
  type: TYPE_NORMAL
- en: Adding MetalLB to a cluster gives your users the ability to deploy any application
    that they can containerize. It uses IP pools that dynamically assign an IP address
    for the service so that it is instantly accessible for servicing external requests.
  prefs: []
  type: TYPE_NORMAL
- en: One issue is that MetalLB does not provide name resolution for the service IPs.
    Users prefer to target an easy-to-remember name rather than random IP addresses
    when they want to access a service. Kubernetes does not provide the ability to
    create externally accessible names for services, but it does have an incubator
    project to enable this feature.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to use CoreDNS to create service name
    entries in DNS using an incubator project called external-dns.
  prefs: []
  type: TYPE_NORMAL
- en: Making service names available externally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may have been wondering why we were using the IP addresses to test the NGINX
    services that we created while we used domain names for our Ingress tests.
  prefs: []
  type: TYPE_NORMAL
- en: While a Kubernetes load balancer provides a standard IP address to a service,
    it does not create an external DNS name for users to connect to the service. Using
    IP addresses to connect to applications running on a cluster is not very efficient,
    and manually registering names in DNS for each IP assigned by MetalLB would be
    an impossible method to maintain. So how would you provide a more cloud-like experience
    to adding name resolution to our LoadBalancer services?
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the team that maintains KinD, there is a Kubernetes SIG that is working
    on this feature to Kubernetes called **external-dns**. The main project page is
    found on the SIG's Github at [https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns).
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, the **external-dns** project supports a long list of
    compatible DNS servers, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Google's cloud DNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon's Route 53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AzureDNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloudflare
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreDNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RFC2136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And more…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you know, our Kubernetes cluster is running CoreDNS to provide cluster DNS
    name resolution. Many people are not aware that CoreDNS is not limited to providing
    only internal cluster DNS resolution. It can also provide external name resolution,
    resolving names for any DNS zone that is managed by a CoreDNS deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up external-dns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Right now, our CoreDNS is only resolving names for internal cluster names, so
    we need to set up a zone for our new DNS entries. Since FooWidgets wanted all
    applications to go into **foowidgets.k8s**, we will use that as our new zone.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating external-dns and CoreDNS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final step to providing dynamic service registration to our cluster is to
    deploy and integrate **external-dns** with CoreDNS.
  prefs: []
  type: TYPE_NORMAL
- en: To configure **external-dns** and CoreDNS to work in the cluster, we need to
    configure each to use ETCD for the new DNS zone. Since our clusters are running
    KinD with a preinstalled ETCD, we will deploy a new ETCD pod dedicated to **external-dns**
    zones.
  prefs: []
  type: TYPE_NORMAL
- en: The quickest method to deploy a new ETCD service is to use the official ETCD
    operator Helm chart. Using the following single command, we can install the operator
    and a three-node ETCD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need install the Helm binary. We can install Helm quickly using the
    script provided by the Helm team:'
  prefs: []
  type: TYPE_NORMAL
- en: curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
  prefs: []
  type: TYPE_NORMAL
- en: chmod 700 get_helm.sh
  prefs: []
  type: TYPE_NORMAL
- en: ./get_helm.sh
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, using Helm, we can create the ETCD cluster that we will integrate with
    CoreDNS. The following command will deploy the ETCD operator and create the ETCD
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: helm install etcd-dns --set customResources.createEtcdClusterCRD=true stable/etcd-operator
    --namespace kube-system
  prefs: []
  type: TYPE_NORMAL
- en: 'It will take a few minutes to deploy the operator and the ETCD nodes. You can
    check on the status by looking at the pods in the **kube-system** namespace. Once
    fully installed, you will see three ETCD operator pods and three ETCD cluster
    pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – ETCD operator and nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.13_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.13 – ETCD operator and nodes
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the deployment has completed, view the services in the **kube-system**
    namespace to get the IP address of the new ETCD service called **etcd-cluster-client**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – ETCD service IP'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.14_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.14 – ETCD service IP
  prefs: []
  type: TYPE_NORMAL
- en: We will need the assigned IP address to configure **external-dns** and the CoreDNS
    zone file in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Adding an ETCD zone to CoreDNS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**external-dns** requires the CoreDNS zone to be stored on an ETCD server.
    Earlier, we created a new zone for foowidgets, but that was just a standard zone
    that would require manually adding new records for new services. Users do not
    have time to wait to test their deployments, and using an IP address may cause
    issues with proxy servers or internal policies. To help the users speed up their
    delivery and testing of application, we need to provide dynamic name resolution
    for their services. To enable an ETCD-integrated zone for foowidgets, edit the
    CoreDNS configmap, and add the following bold lines.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may need to change the **endpoint** to the IP address of the new ETCD service
    that was retrieved on the previous page:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Corefile: |'
  prefs: []
  type: TYPE_NORMAL
- en: .:53 {
  prefs: []
  type: TYPE_NORMAL
- en: errors
  prefs: []
  type: TYPE_NORMAL
- en: health {
  prefs: []
  type: TYPE_NORMAL
- en: lameduck 5s
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ready
  prefs: []
  type: TYPE_NORMAL
- en: kubernetes cluster.local in-addr.arpa ip6.arpa {
  prefs: []
  type: TYPE_NORMAL
- en: pods insecure
  prefs: []
  type: TYPE_NORMAL
- en: fallthrough in-addr.arpa ip6.arpa
  prefs: []
  type: TYPE_NORMAL
- en: ttl 30
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: prometheus :9153
  prefs: []
  type: TYPE_NORMAL
- en: forward . /etc/resolv.conf
  prefs: []
  type: TYPE_NORMAL
- en: '**etcd foowidgets.k8s {**'
  prefs: []
  type: TYPE_NORMAL
- en: '**stubzones**'
  prefs: []
  type: TYPE_NORMAL
- en: '**path /skydns**'
  prefs: []
  type: TYPE_NORMAL
- en: '**endpoint http://10.96.181.53:2379**'
  prefs: []
  type: TYPE_NORMAL
- en: '**}**'
  prefs: []
  type: TYPE_NORMAL
- en: cache 30
  prefs: []
  type: TYPE_NORMAL
- en: loop
  prefs: []
  type: TYPE_NORMAL
- en: reload
  prefs: []
  type: TYPE_NORMAL
- en: loadbalance
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: ConfigMap'
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to deploy **external-dns** to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We have provided a manifest in the GitHub repository in the **chapter6** directory
    that will patch the deployment with your ETCD service endpoint. You can deploy
    **external-dns** using this manifest by executing the following command, from
    the **chapter6** directory. The following command will query the service IP for
    the ETCD cluster and create a deployment file using that IP as the endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'The newly created deployment will then install **external-dns** in your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: ETCD_URL=$(kubectl -n kube-system get svc etcd-cluster-client -o go-template='{{
    .spec.clusterIP }}')
  prefs: []
  type: TYPE_NORMAL
- en: cat external-dns.yaml | sed -E "s/<ETCD_URL>/${ETCD_URL}/" > external-dns-deployment.yaml
  prefs: []
  type: TYPE_NORMAL
- en: kubectl apply -f external-dns-deployment.yaml
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy **external-dns** to your cluster manually, create a new manifest
    called **external-dns-deployment.yaml** with the following content, using your
    ETCD service IP address on the last line:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: rbac.authorization.k8s.io/v1beta1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: ClusterRole'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: external-dns'
  prefs: []
  type: TYPE_NORMAL
- en: 'rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '- apiGroups: [""]'
  prefs: []
  type: TYPE_NORMAL
- en: 'resources: ["services","endpoints","pods"]'
  prefs: []
  type: TYPE_NORMAL
- en: 'verbs: ["get","watch","list"]'
  prefs: []
  type: TYPE_NORMAL
- en: '- apiGroups: ["extensions"]'
  prefs: []
  type: TYPE_NORMAL
- en: 'resources: ["ingresses"]'
  prefs: []
  type: TYPE_NORMAL
- en: 'verbs: ["get","watch","list"]'
  prefs: []
  type: TYPE_NORMAL
- en: '- apiGroups: [""]'
  prefs: []
  type: TYPE_NORMAL
- en: 'resources: ["nodes"]'
  prefs: []
  type: TYPE_NORMAL
- en: 'verbs: ["list"]'
  prefs: []
  type: TYPE_NORMAL
- en: '---'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: rbac.authorization.k8s.io/v1beta1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: ClusterRoleBinding'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: external-dns-viewer'
  prefs: []
  type: TYPE_NORMAL
- en: 'roleRef:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiGroup: rbac.authorization.k8s.io'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: ClusterRole'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: external-dns'
  prefs: []
  type: TYPE_NORMAL
- en: 'subjects:'
  prefs: []
  type: TYPE_NORMAL
- en: '- kind: ServiceAccount'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: external-dns'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: kube-system'
  prefs: []
  type: TYPE_NORMAL
- en: '---'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: ServiceAccount'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: external-dns'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: kube-system'
  prefs: []
  type: TYPE_NORMAL
- en: '---'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: apps/v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Deployment'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: external-dns'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: kube-system'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'type: Recreate'
  prefs: []
  type: TYPE_NORMAL
- en: 'selector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'matchLabels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'app: external-dns'
  prefs: []
  type: TYPE_NORMAL
- en: 'template:'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'labels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'app: external-dns'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'serviceAccountName: external-dns'
  prefs: []
  type: TYPE_NORMAL
- en: 'containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: external-dns'
  prefs: []
  type: TYPE_NORMAL
- en: 'image: registry.opensource.zalan.do/teapot/external-dns:latest'
  prefs: []
  type: TYPE_NORMAL
- en: 'args:'
  prefs: []
  type: TYPE_NORMAL
- en: '- --source=service'
  prefs: []
  type: TYPE_NORMAL
- en: '- --provider=coredns'
  prefs: []
  type: TYPE_NORMAL
- en: '- --log-level=info'
  prefs: []
  type: TYPE_NORMAL
- en: 'env:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: ETCD_URLS'
  prefs: []
  type: TYPE_NORMAL
- en: 'value: http://10.96.181.53:2379'
  prefs: []
  type: TYPE_NORMAL
- en: Remember, if your ETCD server's IP address is not 10.96.181.53, change it before
    deploying the manifest.
  prefs: []
  type: TYPE_NORMAL
- en: Deploy the manifest using **kubectl apply -f external-dns-deployment.yaml**.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a LoadBalancer service with external-dns integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You should still have the NGINX deployment from the beginning of this chapter
    running. It has a few services tied to it. We will add another one to show you
    how to create a dynamic registration for the deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a dynamic entry in the CoreDNS zone, you need to add an annotation
    in your service manifest. Create a new file called **nginx-dynamic.yaml** with
    the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Service'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '**annotations:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**external-dns.alpha.kubernetes.io/hostname: nginx.foowidgets.k8s**'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: nginx-ext-dns'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: default'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '- port: 8080'
  prefs: []
  type: TYPE_NORMAL
- en: 'protocol: TCP'
  prefs: []
  type: TYPE_NORMAL
- en: 'targetPort: 8080'
  prefs: []
  type: TYPE_NORMAL
- en: 'selector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'run: nginx-web'
  prefs: []
  type: TYPE_NORMAL
- en: 'type: LoadBalancer'
  prefs: []
  type: TYPE_NORMAL
- en: Note the annotation in the file. To instruct **external-dns** to create a record,
    you need to add an annotation that has the key **external-dns.alpha.kubernetes.io/hostname**
    with the desired name for the service—in this example, **nginx.foowidgets.k8s**.
  prefs: []
  type: TYPE_NORMAL
- en: Create the service using **kubectl apply -f nginx-dynamic.yaml**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It takes about a minute for the **external-dns** to pick up on DNS changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify that the record was created, check the **external-dns** pod logs
    using **kubectl logs -n kube-system -l app=external-dns**. Once the record has
    been picked up by **external-dns**, you will see an entry similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**time="2020-04-27T18:14:38Z" level=info msg="Add/set key /skydns/k8s/foowidgets/nginx/03ebf8d8
    to Host=172.17.201.101, Text=\"heritage=external-dns,external-dns/owner=default,external-dns/resource=service/default/nginx-lb\",
    TTL=0"**'
  prefs: []
  type: TYPE_NORMAL
- en: The last step to confirm that external-dns is fully working is to test a connection
    to the application. Since we are using a KinD cluster, we must test this from
    a pod in the cluster. We will use a Netshoot container, as we have been doing
    throughout this book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this section, we will show the steps to integrate a Windows DNS
    server with our Kubernetes CoreDNS servers. The steps are being provided to provide
    you with a complete understanding of how you fully integrate the enterprise DNS
    server with delegation to our CoreDNS service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run a Netshoot container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**kubectl run --generator=run-pod/v1 tmp-shell --rm -i --tty --image nicolaka/netshoot
    -- /bin/bash**'
  prefs: []
  type: TYPE_NORMAL
- en: To confirm that the entry has been created successfully, execute a **nslookup**
    for the host in a Netshoot shell:![Figure 6.15 – Nslookup for new record
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/Fig_6.15_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.15 – Nslookup for new record
  prefs: []
  type: TYPE_NORMAL
- en: We can confirm that the DNS server in use is CoreDNS, based on the IP address,
    which is the assigned IP to the **kube-dns** service. (Again, the service is **kube-dns**,
    but the pods are running CoreDNS).
  prefs: []
  type: TYPE_NORMAL
- en: 'The 172.17.201.101 address is the IP that was assigned to the new NGINX service;
    we can confirm this by listing the services in the default namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – NGINX external IP address'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.16_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.16 – NGINX external IP address
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s confirm that the connection to NGINX works by connecting to
    the container using the name. Using a **curl** command in the Netshoot container,
    curl to the DNS name on port 8080:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Curl test using the external-dns name'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.17_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.17 – Curl test using the external-dns name
  prefs: []
  type: TYPE_NORMAL
- en: The **curl** output confirms that we can use the dynamically created service
    name to access the NGINX web server.
  prefs: []
  type: TYPE_NORMAL
- en: We realize that some of these tests aren't very exciting, since you can test
    them using a standard browser. In the next section, we will integrate the CoreDNS
    running in our cluster with a Windows DNS server.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating CoreDNS with an enterprise DNS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section will show you how to forward the name resolution of the **foowidgets.k8s**
    zone to a CoreDNS server running on a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This section has been included to provide an example of integrating an enterprise
    DNS server with a Kubernetes DNS service.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the external requirements and additional setup, the steps provided
    are for reference and **should not be executed** on your KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: For this scenario, the main DNS server is running on a Windows 2016 server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The components deployed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Windows 2016 Server running DNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bitnami NGINX deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LoadBalancer service created, assigned IP 10.2.1.74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreDNS service configured to use hostPort 53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployed add-ons, using the configuration from this chapter such as external-dns,
    ETCD cluster for CoreDNS, CoreDNS ETCD zone added, and MetalLB using an address
    pool of 10.2.1.60-10.2.1.80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's go through the configuration steps to integrate our DNS servers.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the primary DNS server
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first step is to create a conditional forwarder to the node running the
    CoreDNS pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the Windows DNS host, we need to create a new conditional forwarder for
    **foowidgets.k8s** pointing to the host that is running the CoreDNS pod. In our
    example, the CoreDNS pod has been assigned to the host 10.240.100.102:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Windows conditional forwarder setup'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.18_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.18 – Windows conditional forwarder setup
  prefs: []
  type: TYPE_NORMAL
- en: This configures the Windows DNS server to forward any request for a host in
    the **foowidgets.k8s** domain to CoreDNS pod.
  prefs: []
  type: TYPE_NORMAL
- en: Testing DNS forwarding to CoreDNS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To test the configuration, we will use a workstation on the main network that
    has been configured to use the Windows DNS server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first test we will run is a **nslookup** of the NGINX record that was created
    by the MetalLB annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From a command prompt, we execute a **nslookup nginx.foowidgets.k8s**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Nslookup confirmation for registered name'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.19_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.19 – Nslookup confirmation for registered name
  prefs: []
  type: TYPE_NORMAL
- en: Since the query returned the IP address we expected for the record, we can confirm
    that the Windows DNS server is forwarding requests to CoreDNS correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do one more additional NGINX test from the laptop''s browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Success browsing from an external workstation using CoreDNS'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.20_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.20 – Success browsing from an external workstation using CoreDNS
  prefs: []
  type: TYPE_NORMAL
- en: One test confirms that the forwarding works, but we aren't comfortable that
    the system is fully working.
  prefs: []
  type: TYPE_NORMAL
- en: To test a new service, we deploy a different NGINX server called microbot, with
    a service that has an annotation assigning the name **microbot.foowidgets.k8s**.
    MetalLB has assigned the service the IP address of 10.2.1.65.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like our previous test, we test the name resolution using nslookup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.21 – Nslookup confirmation for an additional registered name'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.21_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.21 – Nslookup confirmation for an additional registered name
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm that the web server is running correctly, we browse to the URL from
    a workstation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Successful browsing from an external workstation using CoreDNS'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_6.22_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.22 – Successful browsing from an external workstation using CoreDNS
  prefs: []
  type: TYPE_NORMAL
- en: Success! We have now integrated an enterprise DNS server with a CoreDNS server
    running on a Kubernetes cluster. This integration provides users with the ability
    to register service names dynamically by simply adding an annotation to the service.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about two important objects in Kubernetes that
    expose your deployments to other cluster resources and users.
  prefs: []
  type: TYPE_NORMAL
- en: We started the chapter by going over services and the multiple types that can
    be assigned. The three major service types are ClusterIP, NodePort, and LoadBalancer.
    Selecting the type of service will configure how your application is accessed.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, services alone are not the only objects that are used to provide
    access to applications running in the cluster. You will often use a ClusterIP
    service along with an Ingress controller to provide access to services that use
    layer 7\. Some applications may require additional communication, that is not
    provided by a layer-7 load balancer. These applications may need a layer-4 load
    balancer to expose their services to the users. In the load balancing section,
    we demonstrated the installation and use of MetalLB, a commonly used open source
    layer-7 load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: In the last section, we explained how to integrate a dynamic CoreDNS zone with
    an external enterprise DNS server using conditional forwarding. Integrating the
    two naming systems provides a method to allow the dynamic registration of any
    layer-4 load-balanced service in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to expose services on the cluster to users, how do we
    control who has access to the cluster to create a new service? In the next chapter,
    we will explain how to integrate authentication with your cluster. We will deploy
    an OIDC provider into our KinD clusters and connect with an external SAML2 lab
    server for identities.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does a service know what pods should be used as endpoints for the service?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. By the service port
  prefs: []
  type: TYPE_NORMAL
- en: B. By the namespace
  prefs: []
  type: TYPE_NORMAL
- en: C. By the author
  prefs: []
  type: TYPE_NORMAL
- en: D. By the selector label
  prefs: []
  type: TYPE_NORMAL
- en: What kubectl command helps you to troubleshoot services that may not be working
    properly?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. **kubectl get services <service name>**
  prefs: []
  type: TYPE_NORMAL
- en: B. **kubectl get ep <service name>**
  prefs: []
  type: TYPE_NORMAL
- en: C. **kubectl get pods <service name>**
  prefs: []
  type: TYPE_NORMAL
- en: D. **kubectl get servers <service name>**
  prefs: []
  type: TYPE_NORMAL
- en: All Kubernetes distributions include support for services that use the **LoadBalancer**
    type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  prefs: []
  type: TYPE_NORMAL
- en: B. False
  prefs: []
  type: TYPE_NORMAL
- en: Which load balancer type supports all TCP/UDP ports and accepts traffic regardless
    of the packet's contents?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Layer 7
  prefs: []
  type: TYPE_NORMAL
- en: B. Cisco layer
  prefs: []
  type: TYPE_NORMAL
- en: C. Layer 2
  prefs: []
  type: TYPE_NORMAL
- en: D. Layer 4
  prefs: []
  type: TYPE_NORMAL
- en: Without any added components, you can use multiple protocols using which of
    the following service types?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. **NodePort** and **ClusterIP**
  prefs: []
  type: TYPE_NORMAL
- en: B. **LoadBalancer** and **NodePort**
  prefs: []
  type: TYPE_NORMAL
- en: C. **NodePort**, **LoadBalancer**, and **ClusterIP**
  prefs: []
  type: TYPE_NORMAL
- en: D. **LoadBalancer** and **ClusterIP**
  prefs: []
  type: TYPE_NORMAL
