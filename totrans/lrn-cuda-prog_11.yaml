- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CUDA is a parallel programming platform. Learning CUDA means not only learning
    the language, but also having some engineering prowess with the GPU. That engineering
    area can be monitoring, environment settings, performance understanding, containerization,
    and so on. This chapter provides some tips to help engineers use GPUs. We could
    cover even more topics, but the following topics will be helpful for those of
    you who want to learn about CUDA and its GPU operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Useful `nvidia-smi` commands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WDDM/TCC mode in Windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring container-based development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Useful nvidia-smi commands
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover the monitoring features and management operations
    of `nvidia-smi`. `nvidia-smi` is the **command-line interface** (**CLI**) of the
    **NVIDIA Management Library** (**NVML**). This library enables the management
    and monitoring of NVIDIA devices. `nvidia-smi` also provides direct queries and
    commands to the device through the library. The data is presented in either plain
    text or XML format via `stdout` or a file. It provides several management tools
    for changing device statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '`nvidia-smi` is a CLI application that wraps NVML C/C++ APIs. It obtains requested
    information from the `NVIDIA` driver via NVML. NVML also provides APIs to work
    with other languages, such as Python and Perl.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, `nvidia-smi` reports the following installed GPU stats for the user:'
  prefs: []
  type: TYPE_NORMAL
- en: The first row reports the driver version, and the CUDA version supported
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second row shows the GPU stats format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each consecutive row contains each GPU''s stats, including the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Operation mode:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistence mode (ON/OFF)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tesla Compute Cluster** (**TCC**)/**Windows Display Driver Model** (**WDDM**)
    mode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan speed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU temperature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Power usage and capacity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bus-ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory usage and installed memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counted **error-correcting code** (**ECC**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU utilization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basically, `nvidia-smi` can handle all NVIDIA GPU cards, including Tesla, Quadro,
    and GeForce. Enabled features can vary in terms of the model and type. For example,
    the ECC error count is available in Tesla and Quadro cards, while it isn't in
    GeForce because it doesn't provide the ECC feature in its device memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The format of `nvidia-smi` reports is the same across operating systems. The
    following screenshot shows the output of Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c002293-324c-4ee6-b99d-346e3295257c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the output of Linux:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbf9fd23-5ea3-458b-9e02-82b2b9882431.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, we can read the reports and set GPU operations in the same format.
    Now, let''s move on and look at the commands that are frequently used. The default
    `nvidia-smi` CLI''s usage is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To begin with, the following options are frequently used depending on the monitoring
    purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-i`, `--id=`: For selecting the targeting GPU'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-l`, `--loop=`: Reports the GPU''s status at a specified second interval'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-f`, `--filename=`: For logging in to a specified file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This list covers `nvidia-smi` options that can help us to obtain detailed information
    from the GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the GPU's information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`nvidia-smi` reports structured output when we use the `--query` (`-q`) option.
    Therefore, we can learn about which information is collected. We can obtain GPU
    information such as utilization, power, memory, and clock speed stats. On the
    other hand, this format is not helpful if we wish to monitor the GPU''s status
    continuously.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting formatted information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The basic GPU stats that we need to monitor are power, temperature, core utilization,
    and memory usage. This can easily be done with the `--query-gpu` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command shows some options that we can use to detect performance
    draw reasons for clock throttling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The reasons for GPU clock throttling can be power brake, overheating, and sync
    boost. Power brake means that the GPU's power consumption is limited by the user's
    setting or the power supplier's performance in the system. Overheating is also
    a frequent throttling reason due to a poor cooling environment.
  prefs: []
  type: TYPE_NORMAL
- en: Power management mode settings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find out the maximum power consumption per GPU using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Setting the GPU's clock speed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, the GPU's clock speed changes based on demand and saves power consumption
    to maximize power efficiency. To maximize your GPU's performance and reduce latency,
    especially in a benchmark situation, we can ensure that the GPU has a maximum
    clock speed and disable the GPU driver.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to set the GPU in persistence mode. Doing this means that the
    GPU driver module is always loaded to the kernel and reduces the initial response
    time. This option is only available on Linux since Windows does not unload the
    GPU driver. The persistent mode setting command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can set the maximum supported clocks. This value will vary based on
    the GPU you are using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, the Tesla V100 card can be set with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: GPU device monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This command probes the selected GPU''s device status every second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the result of the previous command. The device
    we are monitoring states that it has a GPU device status of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb906ac4-8892-4d60-a69a-13552a6b92ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The collected information can be specified with the `-s` option, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`p`: Power usage and temperature'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`u`: Utilization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`c`: Proc and mem clocks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`v`: Power and thermal violations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`m`: FB and Bar1 memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`e`: ECC errors and PCIe replay errors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t`: PCIe Rx and Tx throughput'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring GPU utilization along with multiple processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you''re using multiple process operations on a single GPU, you may consider
    using this command. This command collects GPU stats, along with the process they
    are being used on. This means you can determine which process has been throttled
    by GPU sharing, the room for memory timings, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the output of `nvidia-smi` with **Process ID** (**PID**),
    which helps in determining which process is using what GPU resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b31adbb3-a103-4fe1-9f65-b36c37da194e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each column in the preceding screenshot shows each GPU''s computing unit utilization
    or memory usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sm%`: CUDA core utilization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mem%`: Sampled time ratio for memory operations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enc%`/`dec%`: HW encoder''s utilization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fb`: FB memory usage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting GPU topology information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a multi-GPU system, it is useful to use `nvidia-smi` to obtain GPU topology
    information. The following command is an `nvidia-smi` command that shows the GPU
    topology of a multi-GPU system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the output of `nvidia-smi` showing the system''s
    topology. The result of DGX Station is that we have four NVLink-enabled V100 GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c16a6ff8-8869-4136-8fc4-3b8ddd1b30db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Following this result, we can confirm that the system''s GPU topology is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2c9864c-3133-47d2-bf3e-12687a9e7fa7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following command identifies the peer-to-peer accessibility between GPUs.
    We used this command in [Chapter 6](ba3092b0-9a57-4137-8ec9-229253c98552.xhtml),
    *Scalable Multi-GPU Programming*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the `nvidia-smi` topology, which has four GPUs
    in a system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/332cf62f-5f61-45cf-a65a-66b005de6d18.png)'
  prefs: []
  type: TYPE_IMG
- en: Peer-to-peer access is an important factor for scalability or operations. This
    command helps you confirm that the GPUs and your system can support peer-to-peer
    access between GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: WDDM/TCC mode in Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On the Windows platform, the NVIDIA GPU has two modes: WDDM and TCC. WDDM is
    the graphics driver for video cards so that it can render desktops and applications.
    If the installed GPU is only used for computing, display rendering is useless
    overhead. In this situation, the NVIDIA GPU can switch to a mode that only focuses
    on computing. This mode is known as TCC mode.'
  prefs: []
  type: TYPE_NORMAL
- en: WDDM allows the NVIDIA GPU to cooperate with Windows' WDDM driver, which serves
    displays. Supporting WDDM mode is a requirement for Windows graphics. On the other
    hand, TCC mode only works toward computing. Depending on your GPU products and
    configuration, the GPU's mode can be changed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Operation mode follows four NVIDIA product classes, and its default mode can
    vary, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GeForce**:WDDM mode only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quadro/Titan**: WDDM mode by default, but can be used in TCC mode too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tesla**: Typically defaults to TCC mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tegra**: Supports Linux only. No WDDM/TCC issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WDDM mode supports CUDA operations and debugging CUDA applications with Nsight,
    while also supporting the display. As a single host machine, you can do everything
    that the GPU can. However, TCC mode disables graphics on the graphics driver and
    enables GPU as a computing accelerator. In other words, this should be used when
    the graphics card does not have to serve displays.
  prefs: []
  type: TYPE_NORMAL
- en: 'TCC mode has some benefits over WDDM mode in CUDA processing, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Serves large-scale computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ignores Windows' display timeout interval (typically two seconds) to enable
    kernel operations that are longer than two seconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduces CUDA's kernel launch overhead on Windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports CUDA processing with Windows remote desktop service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enables the use of NVIDIA GPUs with non-NVIDIA integrated graphics so that you
    can save global memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, TCC mode brings optimal configuration for GPUs as accelerators if
    they do not serve displays.
  prefs: []
  type: TYPE_NORMAL
- en: Setting TCC/WDDM mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To change TCC or WDDM mode, use the `nvidia-smi` utility, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`0` means WDDM mode, and `1` means TCC mode.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to set TCC mode for the selected GPUs, use the `-g` option to specify
    target GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This option is useful when you want to separate the purpose of GPU use for display
    and compute. After you've applied these settings, you may want to *reboot* your
    machine to apply these changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can identify that TCC mode is enabled by using `nvidia-smi`. The following
    screenshot shows GPU operation mode in TCC:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/596401fd-59e4-4156-a3af-22ed06088a29.png)'
  prefs: []
  type: TYPE_IMG
- en: By looking at the right-hand side of the GPU name in the first column, we can
    confirm that TCC mode is enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Performance modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to understand the characteristics of the application/algorithm
    and the GPU hardware to set realistic speedup targets. This can be achieved by
    adding parallelism. We also need to determine whether there's room to optimize
    the GPU when we optimize an application.
  prefs: []
  type: TYPE_NORMAL
- en: One simple approach is to apply Amdahl's law. We can predict that achievable
    performance gain in an application is limited by the sequential portion of the
    code. For example, only 50% of the code can be made parallel, while the rest is
    sequential in nature (such as reading from a file). If this is the case, the maximum
    speedup that can be achieved is 2x; that is, the program can only run twice as
    fast. However, this performance modeling only shows the maximum speedup. We can't
    help but assume that we can completely parallelize and eliminate the execution
    time in the parallel portion of the code.
  prefs: []
  type: TYPE_NORMAL
- en: Another performance modeling practice is to perform analysis based on the target
    architecture's performance bounding factors. In practice, we have hardware specifications,
    and its operations introduce inevitable performance limitations. By analyzing
    these limitations, we can establish whether there is room to perform optimization
    and look at the next set of optimization strategies.
  prefs: []
  type: TYPE_NORMAL
- en: The Roofline model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Every kernel function can be classified into one of the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute bound**: The kernel function does more arithmetic operations for
    every byte of data that is read or written. These applications demand more compute
    FLOPS from the hardware.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory bound**: The application spends most of its time reading and writing
    from memory and less computation. The application gets affected most by the memory
    bandwidth of the system rather than the FLOP rating of the hardware.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latency bound**: The kernel function''s CUDA threads spend most of their
    time in waiting rather than executing. There are many reasons for this scenario
    to occur. The primary reason is the sub-optimal level of parallelism or non-optimal
    usage of memory and computes resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since all of these bindings are introduced by the hardware, we can graph the
    target hardware''s peak performance and memory bandwidth along with their arithmetic
    intensity. The performance curve is bounded by the hardware''s peak performance. We
    briefly touched on this in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml),
    *CUDA Thread Programming*, to determine the next optimization strategy. The following
    illustration was used in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml), *CUDA
    Thread Programming*, and shows an example of the Roofline model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47a31ebb-4282-499a-bcd0-c43c007e024f.png)'
  prefs: []
  type: TYPE_IMG
- en: For any computation to happen, the data needs to be transported from memory
    to the arithmetic units. It traverses a different level of the memory hierarchy,
    and the peak memory bandwidth varies depending on the memory type. The algorithm's
    peak performance can be categorized following its arithmetic intensity. This intensity
    is determined by the amount of computing data versus loaded data. Also, there
    is a computational ceiling introduced by these latency-bound factors. By measuring
    the performance and analysis against the hardware specification, we can confirm
    that the target algorithm achieved peak performance or was bound by memory or
    latency. In either case, we can determine the next step. In [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml),
    *CUDA Thread Programming*, we explored this in depth. In this section, we will
    focus on the Roofline model by looking at an example and seeing how useful it
    is.
  prefs: []
  type: TYPE_NORMAL
- en: The Roofline model takes into consideration the operational intensity of the
    application. In simple terms, this means that operations are done per byte from
    the main memory (DRAM). While there are more complicated models that also consider
    the cache to processor transfers, the Roofline model focuses more on the data
    transfer from DRAM to cache and, hence, focuses on the DRAM bandwidth that's needed
    by the CUDA kernel on a particular GPU architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Roofline model states the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Attainable performance ( GFLOP/s) = min (Peak Floating-Point Performance,
    Peak Memory Bandwidth * Operational Intensity)"'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the Jacobi method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s try to understand this formula and get the Roofline model for the V100
    GPU card. The V100 GPU has the following specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: 80 SMs, each with 32 FP64 cores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 900 GB/s aggregate bandwidth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L2 cache: 6 MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L1 Cache: 10 MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Register: 62 KB/SM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s try to analyze a simple Jacobi method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s analyze the data transfer for the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory load of the vector (`Anew`, `rhs`, `Aref`): *I[Load]* *= NoRow * NoCol
    * 3 * 8 Bytes (double precision)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store for the vector (`Anew`): *I[store] = NoRow * NoCol * 8 Bytes*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Floating-point operations: *I[FP] = NoRow * NoCol * 6 FLOP*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following graph shows the Roofline analysis of the Tesla V100 card and
    the Jacobi method''s arithmetic intensity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1660d18-bfa6-463b-8ed3-569519b1db7a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The arithmetic intensity of Jacobi on the V100 will be *I[FP]/(I[Load]+I[Strore])
    = 0.18 FLOP/byte.*
  prefs: []
  type: TYPE_NORMAL
- en: The Roofline model clearly shows that the algorithm is memory bound and that
    the maximum attainable performance is 0.18 FLOP/byte only, and so will not be
    able to reach the peak FLOP rating of the V100, which is 7.8 TFLOPS. However,
    we can also predict the attainable performance after optimization by reusing the
    fetched data.
  prefs: []
  type: TYPE_NORMAL
- en: The Roofline model helps in defining the upper-performance limit for algorithms
    based on their hardware characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Jacobi method**'
  prefs: []
  type: TYPE_NORMAL
- en: This is an iterative algorithm for finding solutions for a system of linear
    equations. Its basic operations and GPU optimization are explained at [https://www.olcf.ornl.gov/wp-content/uploads/2016/01/Introduction-to-Accelerated-Computing-with-OpenACC-Jeff-Larkin.pdf](https://www.olcf.ornl.gov/wp-content/uploads/2016/01/Introduction-to-Accelerated-Computing-with-OpenACC-Jeff-Larkin.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring container-based development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the key challenges that developers and IT administrators who are maintaining
    the cluster face is the complexity of the software stack. Each and every application/framework
    has many dependencies. More complexity gets added when these dependencies are
    of different versions. For example, in DL, Caffe has different requirements of
    versions of cuDNN and Python than TensorFlow. In a particular organization/institute,
    there are many users, and each and every user may use different versions of the
    same framework. Installing all the right dependencies and setting up the right
    environment results in the loss of productivity. More time is spent on installation
    rather than doing the work. Another challenge that's faced is that it is almost
    impossible to reproduce the result/performance numbers by different individuals,
    even though they might run on the same system, due to dependency mismatch. For
    example, the GROMACS Molecular Dynamics framework has many settings, such as compile
    with multithreading or **Message Passing Interface** (**MPI**) support, the version
    on MPI, and the MPI type. Another challenge, especially in AI, is that every software
    framework that you can possibly think of is moving very fast, and new patches
    are added frequently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers provide a solution to these problems. The key advantages of using
    containers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Isolation**: Containers provide isolation of the environment for applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Run anywhere**: Containers provide an easy way to share and test applications
    in different environments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lightweight**: Containers are lightweight compared to using virtual machine-based
    solutions and provide almost negligible latency and overhead'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two of the most famous container environments are Docker and Singularity. Both
    have their advantages and disadvantages. Note, however, that this section is not
    an extensive guide to Docker or Singularity.
  prefs: []
  type: TYPE_NORMAL
- en: Developers usually create containers and publish them online for others to use.
    We will be explaining one such repository that's maintained by NVIDIA called **Nvidia
    GPU Cloud** (**NGC**) in detail. NGC is like a repository that hosts containers
    for popular **Deep Learning** (**DL**), **High-Performance Computing** (**HPC**),
    and **Virtual Reality** (**VR**) frameworks. NVIDIA tests these applications for
    different environments of GPU and goes through an extensive QA process before
    being made available to the public. This means that performance is guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: An analogy of NGC is the Android App Store, which provides a repository for
    different applications that can run on different mobiles running the Android OS.
    These applications get verified and go through a QA process. The NGC name sometimes
    confuses people and developers think that it is a cloud. It should be made clear
    that it is a repository of containers that can be pulled into a system with a
    GPU and run locally. The container can run on different systems with GPUs, just
    like it can run on the desktop with NVIDIA Titan cards, servers with Tesla V100
    cards, or the NVIDIA AI supercomputer DGX. NGC containers can also be run on cloud
    platforms such as AWS and Azure.
  prefs: []
  type: TYPE_NORMAL
- en: NGC configuration for a host machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following steps cover how to configure an NGC working environment and find available
    images in NGC:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Basic installation**: To make use of containers on a GPU system, you need
    to have installed the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nvidia drivers
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nvidia-docker`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nvidia-docker` is an open source project that loads the NVIDIA components
    and modules into a container. It is basically a wrapper around Docker. You can
    download and see the installation instructions at [https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)](https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visit the NGC website**: Now, you can go to the NGC website to choose a container
    ([nvidia.com/ngc](https://www.nvidia.com/en-us/gpu-cloud/)), as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6605ca00-405e-43a5-b447-9a99c2a2f8b3.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, there are six categories for containers. Choose the one that's
    relevant to you. The earlier version of NGC required users to register, but this
    requirement was removed recently.
  prefs: []
  type: TYPE_NORMAL
- en: Basic usage of the NGC container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover how to pull containers from the NGC registry
    and how to customize our own. It's no different from using Docker, except we can
    access the NGC registry, `nvcr.io`. If you are already familiar with Docker commands,
    you can skip this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps explain how to obtain and launch an NGC container on your
    local Linux machine in a Terminal session:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the software you want to use and copy the command from the NGC site.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, pull the container image by pasting the command into the Terminal. The
    following screenshot shows the `pull` commands and their Docker operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9f5c6c17-d493-45bb-b8fc-7972af831865.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, Docker uses a layer-based approach. The CUDA container is built
    over the basic layer of Ubuntu. Also, the Docker images command showed us the
    locally pulled container on our machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to launch the pulled container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The GPUs are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85548425-3f21-4f14-afbc-a011124d86e3.png)'
  prefs: []
  type: TYPE_IMG
- en: As soon as we run Docker, the shell login changes and we are logged into the
    container that's running as root. Due to this, we were able to run the `nvidia-smi` command
    inside the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use the container to access the host resources by using its additional
    options. The most frequently used options are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`-v`: For mounting the volume'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-p`: For port forwarding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-u`: For user forwarding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basic usage of `nvidia-docker` is similar to normal Docker usage, except
    we can use the GPUs. This means you can also get the added benefits of Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and saving a new container from the NGC container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can also add layers to the existing container and save them for future
    use. Let''s learn how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `Dockerfile` and create some layers over the base image. For example,
    we can update APEX ([https://github.com/nvidia/apex](https://github.com/nvidia/apex))
    in the NGC PyTorch container so that we can use its latest version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You can add your desired Ubuntu packages or Python package installation code
    to that file too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can build a customized container with the `docker build` command.
    The following command shows the basic format of the Docker image `build` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This command will find the `Dockerfile` we created and launch each command line
    by line. Each line of the `Dockerfile` will create a Docker layer, so it is recommended
    to write a `RUN` command to cover a single objective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you need to back up your Docker images into your private registry or create
    a file. After you''ve finalized the container, you may want to propagate or reuse
    the container in other systems. In that case, you can push the Docker image into
    your registry. For instance, Docker provides a free registry if you have an account
    on `DockerHub`. You can push your container into the registry with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also create backup files and copy them over your local filesystem.
    The following command shows you how to create a container backup with compression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can load that image using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You can create a local backup image without compression, but the output file
    is too large to deliver to the other systems in general.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have covered some basic operations of Docker. However, Docker
    provides other plentiful functions and benefits too. Although Linux is only available
    for the use of CUDA in the Docker container, Docker will save you time when it
    comes to building the working environment and help you focus on your code development.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the default runtime as NVIDIA Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With some modifications to the `nvidia-docker` configuration, we can launch
    GPU containers without notifying the GPU about this use. Because we can set the
    GPU runtime option to `nvidia-docker`, we can adopt Docker''s runtime design.
    To do that, you need to insert `default-runtime": "nvidia",` as an option into
    `/etc/docker/daemon.json`. Then, the `daemon.json` file can be configured as follows
    if there is no other Docker configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After doing this, reboot the system or restart the Docker daemon with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can enjoy GPU containers without the GPU command option in Docker commands.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to `nvidia-docker` is provided in the NVIDIA development blog,
    which can be found at [https://devblogs.nvidia.com/gpu-containers-runtime](https://devblogs.nvidia.com/gpu-containers-runtime/).
    Here, you will learn not only about its configuration, but also how to integrate
    it with Docker compose or **Linux Containers** (**LXC**). It even allows GPU containers
    to work with Kubernetes via its GPU device plugin.
  prefs: []
  type: TYPE_NORMAL
