- en: Synchronization Primitives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed the potential pitfalls of parallel programming.
    One of these was synchronization overheads. As we break down work into tasks to
    be processed by multiple work items, there arises a need to synchronize the results
    from each thread. We discussed the concept of thread-local-storage and partition-local-storage,
    which can be used to work around this synchronization issue to a certain extent.
    However, it is still necessary to synchronize threads so that we can write data
    to a shared memory location and so that we can perform I/O operations.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the synchronization primitives that are provided
    by the .NET Framework and the TPL.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization primitives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interlocked operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locking primitives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Signaling primitives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lightweight synchronization primitives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barriers and countdown events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a good understanding of the various
    locking and signaling primitives that are provided by .NET Framework, including
    some lightweight synchronization primitives that should be used as much as possible
    wherever there are synchronization needs.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete this chapter, you should have a good understanding of TPL, primarily
    parallel loops. The source code for this chapter is available on GitHub at [https://github.com/PacktPublishing/Hands-On-Parallel-Programming-with-C-8-and-.NET-Core-3/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Parallel-Programming-with-C-8-and-.NET-Core-3/tree/master/Chapter05).
  prefs: []
  type: TYPE_NORMAL
- en: What are synchronization primitives?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before understanding synchronization primitives, we need to understand critical
    section. Critical section is part of the execution path of a thread that must
    be protected from concurrent access in order to maintain some invariants. Critical
    section is not a synchronization primitive in itself but relies on synchronization
    primitives.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization primitives are simple software mechanisms that are provided
    by the underlying platform (the OS). They help in multithreading the kernel. Synchronization
    primitives internally use low-level atomic operations, as well as memory barriers.
    This means that users of synchronization primitives don't have to worry about
    implementing locks and memory barriers themselves. Some common examples of synchronization
    primitives are locks, mutexes, conditional variables, and semaphores. The monitor
    is a higher-level synchronization tool that makes use of other synchronization
    primitives internally.
  prefs: []
  type: TYPE_NORMAL
- en: 'The .NET Framework provides a range of synchronization primitives to deal with
    the interaction among threads, as well as to avoid potential race conditions.
    Synchronization primitives can be broadly divided into five categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Interlocked operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Signaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lightweight synchronization types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SpinWait`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will discuss each category and their respective
    low-level primitives.
  prefs: []
  type: TYPE_NORMAL
- en: Interlocked operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The interlocked class encapsulates synchronization primitives and is used to
    provide atomic operations to variables that are shared across threads. It provides
    methods such as `Increment`, `Decrement`, `Add`, `Exchange`, and `CompareExchange`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following code, which tries to increment a counter inside a parallel
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this code, we will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02d70e3d-6df9-4fb4-824e-222489691f08.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the expected value and the actual value do not match. This is
    because of the race condition among the threads, which has arisen because the
    thread wants to read a value from a variable to which the value has been written
    but not yet committed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can modify the preceding code using the `Interlocked` class to make it thread-safe,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3406d7f-f496-46c5-a762-0cc384cd4b01.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, we can use `Interlocked.Decrement(ref _counter)` to decrement the
    value in a thread-safe manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the complete list of operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Apart from the preceding methods, two new methods were added in .NET Framework
    4.5: `Interlocked.MemoryBarrier()` and `Interlocked.MemoryBarrierProcessWide()`.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn more about memory barriers in .NET.
  prefs: []
  type: TYPE_NORMAL
- en: Memory barriers in .NET
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threading models work differently on single-core versus multicore processors.
    On single-core processors, only one thread gets a CPU slice while other threads
    wait for their turn. This ensures that whenever a thread accesses the memory (for
    loading and storing), it is in the right order. This model is also known as a
    **sequential consistency model**. In the case of multicore processor systems,
    multiple threads run concurrently. Sequential consistency is not guaranteed in
    these systems since either the hardware or the **Just in Time** (**JIT**) compiler
    might reorder the memory instructions to improve performance. The memory instructions
    may also be reordered for performance purposes for caching, load speculations,
    or delaying store operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a load speculation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'An example of a store operation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Load and store statements, when encountered by the compiler, are not always
    executed in the same order as they are written. Compilers do some reordering for
    performance benefits. Let's try to understand more about reordering.
  prefs: []
  type: TYPE_NORMAL
- en: What is reordering?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For a given sequence of code statements, the compiler can choose to either
    execute them in the same order as they are received or reorder them to gain performance
    if multiple threads are working on the same code. For example, take a look at
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code can be reordered and executed in the following order for
    another thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Code reordering is a problem for multicore processors with weak memory models,
    such as Intel Itanium processors. It has no impact on single-core processors,
    however, due to the sequential consistency model. The code is restructured so
    that another thread can take advantage or store an instruction that is already
    in the memory. Code reordering can be done either by hardware or by a JIT compiler.
    To guarantee code reordering, we need some sort of **memory barrier**.
  prefs: []
  type: TYPE_NORMAL
- en: Types of memory barriers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Memory barriers ensure that any code statements above or below the barrier
    will not cross the barrier, thereby enforcing the order of the code. There are
    three types of memory barrier:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Store (write) memory barrier: **A store memory barrier ensures that no store
    operations are allowed to move across the barrier. It has no effect on load operations;
    these can still be reordered. The equivalent CPU instruction to achieve this effect
    is **SFENCE**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d03a60bd-ad17-46a1-afdc-8157aeae929d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Load (read) memory barrier: **A load barrier ensures that no load operations
    are allowed to move across the barrier but places no such enforcement on store
    operations. The equivalent CPU instruction to achieve this effect is **LFENCE**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/6ad7473e-6976-40c6-8282-08b4300a9bf5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Full memory barrier: **A full memory barrier ensures ordering by not allowing
    store or load operations to move across the memory barrier. The equivalent CPU
    instruction to achieve this effect is **MFENCE**. The behavior of the full memory
    barrier is often implemented by .NET synchronization constructs such as the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Task.Start`, `Task.Wait`, and `Task.Continuation`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Thread.Sleep`, `Thread.Join`, `Thread.SpinWait`, `Thread.VolatileRead`, and `Thread.VolatileWrite`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Thread.MemoryBarrier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Lock`, `Monitor.Enter`, and `Monitor.Exit`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Interlocked` class operations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Half barriers are provided by the `Volatile` keyword and the `Volatile` class
    methods. The .NET Framework provides some built-in patterns using volatile fields
    in classes such as `Lazy<T>` and `LazyInitializer`. We will discuss these further
    in [Chapter 7](584edc9a-7c38-480b-a280-b6c17008ae94.xhtml), *Improving Performance
    with Lazy Initialization*.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding code reordering using constructs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can avoid reordering using `Thread.MemoryBarrier`, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`Thread.MemoryBarrier` creates a full barrier that doesn’t allow load or store
    operations to pass. It has been wrapped inside `Interlocked.MemoryBarrier`, so
    the same code can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to create a process- and system-wide barrier, we can make use of
    `Interlocked.MemoryBarrierProcessWide`, which was introduced in .NET Core 2.0\.
    This is a wrapper over the `FlushProcessWriteBuffer` Windows API or `sys_membarrier` on
    a Linux kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example shows us how we can create a process-wide barrier. Now,
    let's look at what locking primitives are.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to locking primitives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Locks can be used to limit access to a protected resource to only a single thread
    or group of threads. To be able to implement locking efficiently, we need to identify
    appropriate critical sections that can be protected via locking primitives.
  prefs: []
  type: TYPE_NORMAL
- en: How locking works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we apply a lock to a shared resource, the following steps are performed:'
  prefs: []
  type: TYPE_NORMAL
- en: A thread or group of threads access a shared resource by acquiring a lock.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Other threads that cannot get access to a lock go into a wait state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As soon as the lock is freed by one of the threads, it is acquired by another
    thread, which starts its execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To understand locking primitives, we need to understand various thread states,
    as well as concepts such as blocking and spinning.
  prefs: []
  type: TYPE_NORMAL
- en: Thread state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At any point during the thread''s life cycle, we can query a thread state using
    the `ThreadState` property of the thread. A thread can be in any one of the following
    states:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Unstarted`: The thread has been created by CLR but the `System.Threading.Thread.Start`
    method hasn''t been invoked on the thread yet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Running`: The thread has been started via a call to `Thread.Start`. It is
    not waiting for any pending operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WaitSleepJoin`: The thread is in a blocked state as a result of invoking the
    `Wait()`, `Sleep()`, or `Join()` methods by calling the thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StopRequested`: The thread has been requested to stop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Stopped`: The thread has stopped executing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AbortRequested`: The `Abort()` method has been called on the thread, but the
    thread hasn''t been aborted yet as it is waiting for `ThreadAbortException`, which
    will try to terminate it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Aborted`: The thread has been aborted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SuspendRequested`: The thread is requested to suspend as a result of calling
    the `Suspend` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Suspended`: The thread has been suspended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Background`: The thread is being executed in the background.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s try to explore the journey of a thread from its initial state, `UnStarted`,
    to its final state, `Stopped`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77801852-0e65-4a02-853e-ac10b7ad47c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When a thread is created by CLR, it is in an `Unstarted` state. It makes a
    transition from `Unstarted` to `Running` when the external thread calls the `Thread.Start()`
    method on it. From the `Running` state, a thread can transition to the following
    states:'
  prefs: []
  type: TYPE_NORMAL
- en: '`WaitSleepJoin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AbortRequested`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Stopped`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A thread is said to be blocked when it is in the `WaitSleepJoin` state. The
    execution of a blocked thread is paused since it is waiting for some external
    conditions to be met, which may be the result of some CPU-bound I/O operation
    or some other thread. Once blocked, the thread immediately yields the CPU time
    slice and doesn't use the processor slice until the blocked condition is satisfied.
    At this point, the thread is unblocked. Blocking and unblocking constitutes a
    performance overhead as this requires the CPU to carry out context switching.
  prefs: []
  type: TYPE_NORMAL
- en: 'A thread can be unblocked in any of the following events:'
  prefs: []
  type: TYPE_NORMAL
- en: If the blocking condition is satisfied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By calling `Thread.Interrupt` on the blocked thread
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By aborting a thread using `Thread.Abort`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the specified timeout is reached
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blocking versus spinning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A blocked thread relinquishes the processor slice for a specified amount of
    time. This improves performance by making it available for other threads but incurs
    the overhead of context switching. It is good in a scenario where the thread has
    to be blocked for a considerable amount of time. If the waiting time is less,
    it makes sense to go for spinning without relinquishing the processor slice. For
    example, the following code simply loops infinitely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is just an empty `while` loop that checks for a Boolean variable. When
    the wait is over, the variable will be set to false and the loop can break. Although
    this is a waste of processor time, it can significantly improve performance if
    the wait isn't very long. The .NET Framework provides some special constructs,
    which we will discuss later in this chapter, such as `SpinWait` and `SpinLock`.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to understand some locking primitives with code examples.
  prefs: []
  type: TYPE_NORMAL
- en: Lock, mutex, and semaphore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lock and mutex are locking constructs that allow only one thread to access a
    protected resource. Lock is a shortcut implementation that uses another higher-level
    synchronization class called `Monitor`.
  prefs: []
  type: TYPE_NORMAL
- en: Semaphore is a locking construct that allows a specified number of threads to
    access a protected resource. Lock can only synchronize access inside a process,
    but if we need to access a system-level resource or shared memory, we need to
    actually synchronize access across multiple processes. A mutex allows us to synchronize
    access to resources across processes by providing a kernel-level lock.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table provides a comparison of the capabilities of these constructs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3333def0-bb0b-4805-b86c-ca1607865494.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, **Lock** and **Mutex** only allow single-thread access to shared
    resources, whereas **Semaphore** and **SemaphoreSlim** can be used to allow access
    to resources that have been shared by multiple threads. Also, where **Lock** and
    **SemaphoreSlim** only work inside a process, **Mutex** and **Semaphore** have
    a process-wide lock.
  prefs: []
  type: TYPE_NORMAL
- en: Lock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider the following code, which tries to write a number to a text
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output when we run the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e55e8a1f-df02-483a-80bc-a2ae13ed28d3.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the task is composed of 1,000 work items and each work item
    takes approximately 10 milliseconds to execute. The time that's taken by the task
    is 1,000 multiplied by 10, which is 10,000 milliseconds. We also have to take
    into consideration the time taken to perform I/O, so the total time turns out
    to be 11,949.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to parallelize this task using the `AsParallel()` and `AsOrdered()`
    clauses, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When we try to run this code, we get the following `System.IO.IOException**:**
    'The process cannot access the file …\test.txt' because it is being used by another
    process.'`.
  prefs: []
  type: TYPE_NORMAL
- en: What actually happened here is that the file is a shared resource with a critical
    section and therefore only allows atomic operations. With the parallel code, we
    have a situation where multiple threads are actually trying to write to the file
    and causing an exception. We need to make sure that the code runs in parallel
    as fast as possible but also maintains atomicity while writing to the file. We
    need to modify the preceding code using a lock statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, declare a `static` reference type variable. In our case, we take a variable
    of the `object` type. We need a reference type variable since the lock can only
    be applied on the heap memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we modify the code inside the `ForAll()` method to include a `lock`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we run this code, we won''t get any exceptions, but the time that
    the task took was actually more than the sequential execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae6f865f-a773-43e8-80d5-9907b0703fd5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'What went wrong here? Lock ensures atomicity by making sure that only one thread
    is allowed to access the vulnerable code, but this comes with the overhead of
    blocking the thread that is waiting for the lock to be freed. We call this a dumb
    lock. We can modify the program slightly to only lock the critical section to
    improve performance while maintaining atomicity, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63589a04-4827-411e-a907-dd302daa543f.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we achieved significant gains by mixing synchronization along
    with parallelization. We can achieve similar results using another locking primitive,
    that is, the `Monitor` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lock is actually a shorthand syntax for achieving `Monitor.Enter()` and `Monitor.Exit()`
    wrapped inside a `try`-`catch` block. The same code can, therefore, be written
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5da8d712-da99-4097-804c-d9174d3951ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Mutex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding code works well for a single instance application since tasks
    run inside a process and the lock actually locks a memory barrier inside the process.
    If we run multiple instances of the application, both applications will have their
    own copy of the static data members and will, therefore, lock their own memory
    barriers. This will allow one thread per process to actually enter the critical
    section and try to write the file. This causes the following `System.IO.IOException**:**
    'The process cannot access the file …\test.txt' because it is being used by another
    process.'`.
  prefs: []
  type: TYPE_NORMAL
- en: To be able to apply locks to shared resources, we can apply a lock at the kernel
    level using the `mutex` class. Like lock, mutex allows only one thread to access
    a protected resource but can work across processes as well, thereby allowing only
    one thread per system to access a protected resource, irrespective of the number
    of processes that are executing.
  prefs: []
  type: TYPE_NORMAL
- en: A mutex can be named or unnamed. An unnamed mutex works like a lock and cannot
    work across processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll create an unnamed `Mutex`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll modify the preceding parallel code so that we can use `Mutex`
    like a lock:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/063ee14e-1885-46c5-b045-59ecacdb5a61.png)'
  prefs: []
  type: TYPE_IMG
- en: With a `Mutex` class, we can call the `WaitHandle.WaitOne()` method to lock
    the critical section and `ReleaseMutex()` to unlock the critical sections. Closing
    or disposing of a mutex automatically releases it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding program works well, but if we try to run it on multiple instances,
    it will throw an `IOException`. For this, we can create a `namedMutex`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Optionally, we can specify a timeout while calling `WaitOne()` on the mutex
    so that it waits for a signal for a specified amount of time before unblocking
    itself. This is shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The preceding mutex will wait for three seconds before unblocking itself if
    it doesn't receive a signal.
  prefs: []
  type: TYPE_NORMAL
- en: Lock and mutex can only be released from the thread that obtained them.
  prefs: []
  type: TYPE_NORMAL
- en: Semaphore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lock, mutex, and monitor allow only one thread to access a protected resource.
    Sometimes, however, we need to allow multiple threads to be able to access a shared
    resource. Examples of these include resource pooling scenarios and throttling
    scenarios. A `semaphore`, unlike lock or mutex, is thread-agnostic, which means
    that any thread can call a release of `semaphore`. Just like a mutex, it works
    across processes as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical `semaphore` constructor is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c903a4bf-5953-4af1-a3f2-7d0ae9503f5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, it accepts two parameters: the `initialCount`, which specifies
    how many threads are initially allowed to enter, and `maximumCount`, which specifies
    the total number of threads that can enter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we have a remote service that only allows three concurrent connections
    per client and takes one second to process a request, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a method that has 1,000 work items that need to call the service with
    parameters. We need to process a task in parallel but also make sure that there
    are no more than three calls to the service at any time. We can achieve this by
    creating a `semaphore` with a max count of `3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can write some code that can simulate making 1,000 requests in parallel,
    but only three at a time, using the following `semaphore`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34020bb8-f6f0-4ee6-8496-f03b46823fb5.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, three threads enter and call the service while other threads
    wait for the lock to be released. As soon as a thread releases the lock, another
    thread enters but only if three threads are inside the critical section at any
    one time.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of semaphores: local and global. We will discuss these
    next.'
  prefs: []
  type: TYPE_NORMAL
- en: Local semaphore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A local `semaphore` is local to the application where it''s used. Any `semaphore`
    that is created without a name will be created as a local `semaphore`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Global semaphore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A global `semaphore` is global to the operating system as it applies kernel-
    or system-level locking primitives. Any `semaphore` that is created with a name
    will be created as a global `semaphore`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If we create a `semaphore` with only one thread, it will act like a lock.
  prefs: []
  type: TYPE_NORMAL
- en: ReaderWriterLock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `ReaderWriterLock` class defines a lock that supports multiple readers and
    a single writer at a time. This is handy in scenarios where a shared resource
    is read frequently by many threads but updated infrequently. There are two reader-writer
    lock classes that are provided by the .NET Framework: `ReaderWriterLock` and `ReaderWriterLockSlim`.
    `ReaderWriterLock` is almost outdated now since it can incur potential deadlocks,
    reduced performance, complex recursion rules, and upgrading or downgrading of
    locks. We will discuss `ReaderWriterLockSlim` in more detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to signaling primitives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An important aspect of parallel programming is task coordination. While creating
    tasks, you may come across a producer/consumer scenario where a thread (the consumer)
    is waiting for a shared resource to be updated by another thread (the producer).
    Since the consumer doesn't know when the producer is going to update the shared
    resource, it keeps on polling the shared resource, which can lead to race conditions.
    Polling is highly inefficient in dealing with these scenarios. It is better to
    use the signaling primitives that are provided by the .NET Framework. With signaling
    primitives, the consumer thread is paused until it receives a signal from the
    producer thread. Let's discuss some common signaling primitives, such as `Thread.Join`,
    `WaitHandles`, and `EventWaitHandlers`.
  prefs: []
  type: TYPE_NORMAL
- en: Thread.Join
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the simplest way in which we can make a thread wait for a signal from
    another thread. `Thread.Join` is blocking in nature, which means that the caller
    thread is blocked until the joined thread is complete. Optionally, we can specify
    a timeout that allows the blocked thread to come out of its blocking state once
    the timeout has been reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will create a child thread that simulates a long-running
    task. Once complete, it will update the output in the local variable, which is
    called `result`. The program is supposed to print the result `10` to the console.
    Let''s try to run the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5be7850-e091-49c4-8be9-8d495b974ed5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We expected the result to be `10`, but it has come out as `0`. This happened
    because the main thread that was supposed to write the value runs before the child
    thread has finished execution. We can achieve the desired behavior by blocking
    the main thread until the child thread completes. This can be done by calling
    `Join()` on the child thread, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the code again now, we will see the desired output after a wait of
    five seconds, during which the main thread is blocked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29f82897-6ec0-4906-b64e-f59cccc732df.png)'
  prefs: []
  type: TYPE_IMG
- en: EventWaitHandle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `System.Threading.EventWaitHandle` class represents a synchronization event
    for a thread. It serves as a base class for the `AutoResetEvent` and `ManualResetEvent`
    classes. We can signal an `EventWaitHandle` by calling `Set()` or `SignalAndWait()`.
    The `EventWaitHandle` class doesn't have any thread affinity, so it can be signaled
    by any thread. Let's learn more about `AutoResetEvent` and `ManualResetEvent`.
  prefs: []
  type: TYPE_NORMAL
- en: AutoResetEvent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This refers to `WaitHandle` classes that are automatically reset. Once they
    are reset, they allow one thread to pass through the barrier that is created.
    As soon as the thread is passed, they are set again, thereby blocking threads
    until the next signal.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we are trying to find out the sum of 10 numbers in
    a thread-safe manner, without using locks.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create an `AutoResetEvent` with the initial state as non-signaled, or
    `false`. This means that all the threads should wait until a signal is received.
    If we set the initial state to signaled, or `true`, the first thread will go through
    while the others wait for a signal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a signaling task that fires a signal 10 times per second using
    the `autoResetEvent.Set()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare a variable sum and initialize it to `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a parallel `for` loop that creates 10 tasks. Each task will start immediately
    and wait for a signal to enter, thereby blocking at the `autoResetEvent.WaitOne()`
    statement. After every second, a signal will be sent by the signaling task and
    one thread will enter and update the `sum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3369c0d3-6e1a-4209-a1d0-a27351c60e85.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, all 10 tasks blocked initially and released one per second after
    receiving the signal.
  prefs: []
  type: TYPE_NORMAL
- en: ManualResetEvent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This refers to wait handles that need to be reset manually. Unlike `AutoResetEvent`,
    which only allows one thread to pass per signal, `ManualResetEvent` allows threads
    to keep passing through until it is set again. Let's try to understand this using
    a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we need to make 15 service calls in batches of 5
    in parallel, with a 2-second delay between each batch. While making the service
    call, we need to make sure that the system is connected to the network. To simulate
    the network status, we will create two tasks: one that signals the network off
    and one that signals the network on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll create a manual reset event with the initial state *off*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll create two tasks that simulate the network turning on and off
    by firing the network *off* event every two seconds (which blocks all the network
    calls) and the network *on* event every five seconds (which allows all the network
    calls to go through):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the preceding code, we have signaled a manual reset event
    every five seconds using `manualResetEvent.Set()`. We turn it off every two seconds
    using `manualResetEvent.Reset()`. The following code makes the actual service
    calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding code, we have created a `for` loop that creates
    five tasks in each iteration with a sleep interval of two seconds between iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Before making service calls, we wait for the network to be up by calling `manualResetEvent.WaitOne();`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the preceding code, we''ll receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bab3114-6be3-4e91-8980-4779dac5779e.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, five tasks are started and blocked immediately to wait for the
    network to be up. After five seconds, when the network is up, we signal using
    the `Set()` method and all five threads pass through to make the service call.
    This is repeated with each iteration of the `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: WaitHandles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`System.Threading.WaitHandle` is a class that inherits from the `MarshalByRefObject`
    class and is used to synchronize threads that are running in an application. Blocking
    and signaling are used to synchronize threads using wait handles. Threads can
    be blocked by calling any of the methods of the `WaitHandle` class. They are released,
    depending on the type of signaling construct that is selected. The methods of
    the `WaitHandle` class are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`WaitOne`: Blocks the calling thread until it receives a signal from the wait
    handles that it''s waiting for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WaitAll`: Blocks the calling thread until it receives a signal from all of
    the wait handles it''s waiting for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an example that shows us how `WaitAll` works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example that makes use of two threads to simulate two different
    service calls. Both threads will execute in parallel but will wait at `WaitHandle.WaitAll(waitHandles)`
    before printing the sum to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8381fd4-8d56-4b68-aeca-cd815f15d19e.png)'
  prefs: []
  type: TYPE_IMG
- en: '`WaitAny`: Blocks the calling thread until it receives a signal from any of
    the wait handles it''s waiting for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the signature of the `WaitAny` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Here is an example that makes use of two threads to perform an item search.
    Both threads will execute in parallel and the program waits for any of the threads
    to finish execution at the `WaitHandle.WaitAny(waitHandles)` method before printing
    the item index to the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two methods, binary search and linear search, that perform a search
    using binary and linear algorithms. We want to get a result as soon as possible
    from either of these methods. We can achieve this via signaling using `AutoResetEvent` 
    and store the results in the `findIndex` and `winnerAlgo` global variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code calls both algorithms in parallel using `ThreadPool`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '`SignalAndWait`: This method is used to call `Set()` on a wait handle and calls `WaitOne`
    for another wait handle. In a multithreaded environment, this method can be utilized
    to release one thread at a time and then resets to wait for the next thread:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Lightweight synchronization primitives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The .NET Framework also provides lightweight synchronization primitives, which
    are better in performance than their counterparts. They avoid dependency on kernel
    objects such as wait handles wherever possible, so they only work inside the process.
    These primitives should be used when the thread's wait time is short. We can divide
    them into two categories, both of which we'll look at in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Slim locks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Slim locks are slim implementations of legacy synchronization primitives that
    can improve performance by reducing overheads.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the legacy synchronization primitives and their slim
    counterparts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/254c3dd1-bfc8-4e51-b3fd-0d025205ff9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's try to learn more about slim locks.
  prefs: []
  type: TYPE_NORMAL
- en: ReaderWriterLockSlim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`ReaderWriterLockSlim` is a lightweight implementation of `ReaderWriterLock`.
    It represents a lock that can be used to manage protected resources in a way that
    allows multiple threads to share read access while allowing only one thread write
    access.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example uses `ReaderWriterLockSlim` to protect access on a list
    that is shared by three reader threads and one writer thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8c8ea67-cbde-4075-92ef-5fba32c0e226.png)'
  prefs: []
  type: TYPE_IMG
- en: SemaphoreSlim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`SemaphoreSlim` is a lightweight implementation of `semaphore`. It throttles
    access to a protected resource to a number of threads.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a slim version of the `semaphore` program that we showed earlier in
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The difference we can see here, apart from replacing the `Semaphore` class with `SemaphoreSlim`,
    is that we now have the `Wait()` method instead of `WaitOne()`. This makes much
    more sense as we are allowing more than one thread to pass through.
  prefs: []
  type: TYPE_NORMAL
- en: Another important difference is that `SemaphoreSlim` is always created as a
    local `semaphore`, unlike `semaphore`, which can be created globally as well.
  prefs: []
  type: TYPE_NORMAL
- en: ManualResetEventSlim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`ManualResetEventSlim` is a lightweight implementation of `ManualResetEvent`.
    It has better performance and less overhead than `ManualResetEvent`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create an object using the following syntax, just like `ManualResetEvent`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Just like other slim counterparts, one major difference here is that we have
    replaced the `WaitOne()` method with `Wait()`.
  prefs: []
  type: TYPE_NORMAL
- en: You can try running some `ManualResetEvent` demonstration code by making the
    preceding changes and see if it works.
  prefs: []
  type: TYPE_NORMAL
- en: Barrier and countdown events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The .NET Framework has some built-in signaling primitives that help us synchronize
    multiple threads without us having to write lots of synchronization logic. All
    the synchronization is handled internally by the provided data structures. In
    this section, let''s discuss two very important signaling primitives: `CountDownEvent`
    and `Barrier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CountDownEvent**: The `System.Threading.CountDownEvent` class refers to an
    event that''s signaled when its count becomes 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Barrier**: The `Barrier` class allows multiple threads to run without having
    the master thread controlling them. It creates a barrier that participating threads
    must wait in until all the threads have arrived. `Barrier` works well for cases
    where work needs to be carried out in parallel and in phases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A case study using Barrier and CountDownEvent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an example, let's say we need to fetch data from two services that are dynamically
    hosted. Before fetching the data from service one, we need to host it. Once the
    data has been fetched, it needs to be closed down. Only when service one has been
    closed down can we start service two and fetch data from it. The data needs to
    be fetched as quickly as possible. Let's create some code to meet the requirements
    of this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `Barrier` with `5` participants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Create two `CountdownEvents` that will trigger the start or close of services
    when six threads have passed through it. Five worker tasks will participate, along
    with a task that will manage the start or close of services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, create another `CountdownEvent` with a count of `5`. This refers to
    the number of threads that can pass through before the event is signaled. `CountdownEvent`
    will trigger when all the worker tasks finish executing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is our `serviceManagerTask` implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the method that is executed by the worker tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67c1de2f-4302-412d-817c-3fad0b113ae1.png)'
  prefs: []
  type: TYPE_IMG
- en: In this section, we have looked at various built-in signaling primitives that
    help make code synchronization easier without the need to lock ourselves as a
    developer. Blocking still comes at a performance cost as it involves context switching.
    In the next section, we will look at some spinning techniques that can help remove
    that context switching overhead.
  prefs: []
  type: TYPE_NORMAL
- en: SpinWait
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of this chapter, we mentioned that spinning is much more efficient
    than blocking for smaller waits. Spinning has fewer kernel overheads related to
    context switching and transitioning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a `SpinWait` object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, wherever we need to make a `spin`, we can just call the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: SpinLock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Locks and interlocking primitives can significantly slow down performance if
    the wait time to get a lock is very low. `SpinLock` provides a lightweight, low-level
    alternative to locking. `SpinLock` is a value type, so if we want to use the same
    object in multiple places, we need to pass it by a reference. For performance
    reasons, even when `SpinLock` hasn't even acquired the lock, it yields the time
    slice of the thread so that the garbage collector can work efficiently. By default,
    `SpinLock` doesn't support thread tracking, which refers to determining which
    thread has acquired the lock. However, this feature can be turned on. This is
    only recommended for debugging and not for production as it reduces performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `SpinLock` object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a method that will be called by various threads and update a global
    static list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the lock is acquired using `_spinLock.Enter(ref lockTaken)`
    and released via `_spinLock.Exit(false)`**.** Everything between these two statements
    will be executed as synchronized between all the threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s call this method in a parallel loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the synchronized output if we had used locking primitives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66d8c9c2-ca57-40d0-bd38-97c542db9ed7.png)'
  prefs: []
  type: TYPE_IMG
- en: As a rule of thumb, if we have small tasks, context switching can be completely
    avoided by using spinning.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about the synchronization primitives that are
    provided by .NET Core. Synchronized primitives are a must if you want to write
    parallel code and ensure that it is correct, even when multiple threads are working
    on it. Synchronization primitives come with performance overheads and the use
    of their slim counterparts is advised wherever possible.
  prefs: []
  type: TYPE_NORMAL
- en: We learned about signaling primitives as well, which can come in very handy
    when threads need to work on some external events. We also discussed the barrier
    and countdown events, which help us avoid code synchronization issues without
    the need to write additional logic. Finally, we introduced some spinning techniques,
    which take away performance overheads that arise from blocking code, that is, `SpinLock`
    and `SpinWait`.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the various data structures provided
    by .NET Core. These are synchronized automatically and are parallel at the same
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which of these can be used for cross-process synchronization?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Lock`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Interlocked.Increment`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Interlocked.MemoryBarrierProcessWide`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of these is not a valid memory barrier?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read memory barrier
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Half memory barrier
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Full memory barrier
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read and execute memory barrier
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From which of the following states can we not resume a thread?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`WaitSleepJoin`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Suspended`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Aborted`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An unnamed `semaphore` can provide synchronization where?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within process
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Across process
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of these constructs support tracking threads?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`SpinWait`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`SpinLock`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
